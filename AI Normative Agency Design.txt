AI Normative Agency Design
Open sidebar
AI Normative Agency Design
GPT-4
Summarize: Normative Agency Design 
Research Statement 
Silviu Pitis, November 2020 
I���m interested in the normative design of general purpose artif i cial agency: how should we design AIs that 
solve general tasks and contribute positively to society? The i rst part of this document motivates normative 
agency design and summarizes my past work. The second part outlines my current work and related ideas. 
INormative Agency Design and Past Work 
Imitating Human Intelligence Is Not Enough.To def i ne intelligence, we often use the human as a 
prototype. Animals exhibiting more human capabilities are said to be more intelligent. Similarly, we compare 
the task-specif i c performance of artif i cial agents (Deep Blue, DQN, AlphaGo, etc.) to human performance. 
For value alignment and reward learning, we typically consider the human as the ���gold standard��� [5,13,17,22]. 
We i nd this human-centric approach attractive because we are human���we know how we think and what 
makes us intelligent. Or do we? For all our achievements, the inner workings of the brain and our true 
motivations remain a mystery, we regularly exhibit undesirable behaviors and thought patterns [18], and our 
individual values and beliefs continue to evolve (e.g., just two centuries ago, slavery was commonly accepted). 
While we should aspire to create agents that possess human capabilities, any approach to intelligence that is 
modeled too closely after the ���empirical human��� will suf f er from the same shortcomings. To the extent we 
have irrational or anti-social humans, such approach will result in irrational or anti-social AIs (e.g., Tay [44]). 
Normative Intelligence.To separate our study of agency from the empirical human, we can describe 
the qualities we seek in an ���ideal agent���, and design our agents so as to satisfy them. I call this normative 
agency design because it specif i es how agents should think and behave.I now describe two styles of 
normative design���axiomatic and capability/constraint���together with a bullet summarizing my past work. 
The Axiomatic Approach.The axiomatic approach to normative design starts with a set of basic, 
universally appealing axioms and uses them to derive powerful conclusions about agents that satisfy the 
axioms. The axioms can be understood as def i ning ���rationality��� and are usually simple, intuitive statements. 
For instance, we may think an ideal agent should exhibit transitive preference: if the agent prefers A to B, 
and B to C, it should prefer A to C. This transitivity axiom is one of four VNM axioms that can be used 
to conclude that ���rational��� agents are expected utility maximizers [23,42], which is fundamental to our use 
of value functions in reinforcement learning (RL) [37]. For an agent with multiple principals (e.g., any pro- 
social agent), we should further consider axioms from social choice [4]. For instance, the Pareto axiom���if 
all principals prefer A to B, so too should the agent���is used in both Arrow���s Impossibility Theorem [3] and 
Harsanyi���s aggregation theorem [15]. These two results provide strong normative guidance on agent design: 
if we accept their axioms, our agents should have a way to compare the utilities of their human principals. 
��� The work I am most proud of is my work on discounting [28], in which I show that agents satisfying a 
Dynamic Consistency axiom [19,21], the VNM axioms and a few other axioms must satisfy a generalized 
Bellman relation with state-action dependent discounting. This provides a normative justif i cation for 
White���s unifying RL task formalism [43], and allows agents to represent a wider range of objectives than 
the standard Bellman relation, while still precluding inconsistencies. If we accept my axiomatization, it 
suggests there are human preferences that i xed discount agents cannot represent (i.e., reward learning is 
not enough���we must also learn time preference). I plan to empirically verify this in future work. 
The Capability/Constraint Approach.The capability/constraint approach to normative design asks 
���what capabilities should agents possess?��� and similarly, ���what constraints should we place on agents?���, 
and proceeds to construct agents with these capabilities and constraints. Work in this category has a more 
empirical l avor than axiomatic work, because capabilities are often def i ned in less formal terms, and much like 
written law, some interpretation is required. Nevertheless, by cataloging desired capabilities and constraints, 
we can start to form a more clear picture of the ideal agent. For instance, the fact that a general purpose 
agent must explore, communicate and adapt through time, has motivated my focus on the RL setting [40]. 
��� I take this approach in my works on Source Traces (grants agents a model of potential causes) [27], 
Maximum Entropy Gain Exploration (MEGA) (grants agents the ability to systematically explore an 
environment) [31], Prototype Goal Encodings (ProtoGE) (grants agents the ability to pursue goals from 
multiple goal spaces) [29], and Counterfactual Data Augmentation (CoDA) (grants agents the ability 
to generate counterfactuals by mixing independent subsamples of observed transitions) [32] and Neural 
Metrics (architecturally constrains an agent���s value function to satisfy the triangle inequality) [30]. While 
I like this research and think it is valuable, I i nd it somewhat less satisfying than the axiomatic approach. 
The solutions are mathematically motivated, but they exemplify only one, not necessarily best solution.
1 / 1
ChatGPT
ChatGPT
Silviu Pitis's research statement from November 2020 focuses on the normative design of artificial agents that contribute positively to society. He distinguishes two approaches to normative agency design: axiomatic and capability/constraint.
Axiomatic Approach
: This begins with universally appealing axioms, like rationality, to derive strong conclusions about agent design. Pitis highlights his work on discounting, which connects with reinforcement learning, providing normative justifications for specific relations and suggesting future empirical verification.
Capability/Constraint Approach
: This considers the desired abilities and limitations for agents, constructing them accordingly. Pitis's works in this area include developing agents with abilities to explore, communicate, adapt, and generate counterfactuals.
He emphasizes that imitating human intelligence is insufficient because humans have irrational or antisocial traits, and suggests designing agents according to the qualities of an "ideal agent" to avoid these shortcomings. While valuing both methods, Pitis expresses a preference for the axiomatic approach, as it offers potentially more robust solutions.
1 / 1
IIResearch Topics I���m Interested In (November 2020) 
One of my long run goals is to obtain a clear normative account of intelligence before we develop a general 
purpose, potentially superhuman, AI. My immediate interests, reward aggregation and l exible multi-goal 
specif i cation, primarily relate to principled abstraction���a critical piece of the larger puzzle. As it is likely 
our agents, regardless of compute, will possess but bounded rationality [39], normative design demands 
principled approximation. Yet perhaps the most common modern approach is to throw a neural network at 
the problem and approximate everything���entire observations, entire states, even entire trajectories [6,12,14]. 
This produces representations and behaviors that are entangled in subtle, often obscure ways with the data 
distribution, optimization process, and hyperparameters [1,9,16]. Even more so than the empirical human, 
the resulting agents are black boxes with unclear motivations, possibly inconsistent beliefs [20], and poor 
generalizability [7]. I don���t yet have a clear picture of what the correct approach is, but I believe it should 
involve some form of compositional, context-conditioned reasoning over basic abstractions. This motivates 
my interest in multi-goal RL [26,29,30,31,32,34,38], which allows agents to plan over a range of goals, 
typically def i ned as abstractions (sets) over the state space. Below, I outline some of my current ideas. 
Decomposing Individual Intelligence.We typically characterize individual choice as the result of a 
monolithic set of preferences. Consider, however, the corporation or national government. We can treat 
them as individual agents, but we can also view their actions as the result of a negotiation between several 
human principals (I used both lenses to analyze corporate and national policy from a law and economics 
angle in law school [24,25]). This perspective also applies to individual action, as a negotiation between 
competing objectives [36], and so we can apply the axioms and tools of social choice to analyze individual 
choice. At present, I���m using this approach to work toward an universal basis for reward functions. My 
current theoretical work shows that under the assumptions of my work on discounting [28] (including the 
VNM and Dynamic Consistency axioms), the Pareto axiom, and the presence of varying time preference [8], it 
is impossible to consistently combine reward functions without forgoing the Markov property (cf. [45]). This 
suggests that to faithfully represent compound objectives, we must treat rewards as non-Markovian [41]. Can 
this analysis be applied to design agents that can successfully navigate conf l icting objectives and principals? 
Aggregating Imperfect Principals.The prior paragraph considers an agent with multiple fully observed, 
cardinal principals. But how should an agent integrate noisy feedback signals? [35]. My recent project on 
Objective Social Choice [33] formalizes the noisy aggregation problem in the bandits setting by framing 
voting as an estimation problem. We found that auxiliary information can be used to improve outcomes and 
concluded that we should not assume Anonymity���which treats all principals equally���as an axiom. Can 
this be extended to the sequential setting, where current approaches (e.g., [5]) are Anonymous? Relatedly, 
what should we do when signals are a mix of ordinal and cardinal values? 
Axiomatizing Abstraction.A normative approach to abstraction could be of tremendous value. For 
instance, my labmate and I recently designed a neural network architecture that is guaranteed to respect 
the triangle inequality [30]. The intent was to use these networks to represent asymmetric metrics in multi- 
goal RL, thereby providing our agents with consistency guarantees on their value estimates. We discovered, 
however, that the underlying idea���representing distances in RL using an asymmetric metric���can be applied 
naively only in the non-abstracted case, where goals are completely specif i ed states. In the abstracted case, 
the triangle inequality may fail and we need a more nuanced approach, which we are now working toward. 
If we had normative guidance, e.g., on how to represent and/or compute distances between abstractions, it 
might simplify our problem signif i cantly. Can we come up with an axiomatic approach to abstraction? 
Enabling Dynamic Abstraction.One of our most remarkable human capabilities is that we can view 
the same underlying state from a range of diverse perspectives. In a legal trial, for instance, the lawyers for 
each side attempt to persuade the judge and jury that their characterization of the facts is the ���better��� one. 
I believe this ability to reason in terms of multiple abstractions is one of the most important capabilities for 
an agent to possess: it is necessary for empathy, compromise, and communication with others whose basic 
assumptions dif f er. As the right low-dimensional abstraction can also greatly simplify a variety of tasks, 
including exploration [30,31], the ability to represent abstractions dynamically [10,11], as well as translate 
between abstractions [29], would be invaluable. How can we grant our agents the ability to reason about the 
world from multiple competing perspectives? And what makes a good abstraction? 
Constraining Intent.The usual ���global reward��� approach to RL is intent-free, or at best, intent 
implicit. An intent-free agent that does something wrong is dif i cult to debug: did it fail because the goal 
was misspecif i ed or because the learning algorithm failed? Multi-goal RL grants agents explicit intent, but 
2 
this comes with risks: a ���curious��� block stacking agent seeks to drop the block in novel ways [31]. Borrowing 
an idea from criminal law, we may constrain bad intent (mens rea); but how exactly should this be done? 
Procedural normativity.When faced with hard substantive problems, we might borrow another idea 
from the legal system: provide the necessary procedure (e.g., due process) so that ultimate outcomes are 
deemed fair, even if they are considered ���wrong��� by some (or many!). We can take a normative approach 
to this and impose axiomatic requirements on our procedure. For instance, we use this approach to analyze 
ordinal voting rules; even though no rule can satisfy our normative desiderata (recall Arrow���s Theorem [3]), we 
can nevertheless require rules to satisfy fairness axioms like Anonymity, Condorcet Consistency and Strategy- 
Proofness. When allowing humans to shape the objectives of AIs, are there any procedural properties��� 
separate from substantive rationality constraints���that we can impose to prevent negative externalities? 
Stacking Six Blocks From Scratch.A nice benchmark problem for sparse-reward multi-goal rein- 
forcement learning is block stacking. Stacking 2 blocks from scratch is too hard for a randomly exploring 
Hindsight Experience Replay agent [2]. Our MEGA agent [31] is the i rst to solve this task from scratch, 
without a hand-crafted curriculum or human demonstrations. In fact, by adding a safety constraint (see 
���Constraining Intent��� above), we can even stack 3 blocks from scratch. But I have not yet been successful in 
stacking 6 blocks. Is this possible by combining techniques like MEGA, CoDA [32] and safety constraints?
1 / 1
ChatGPT
ChatGPT
Silviu Pitis's research statement from November 2020 focuses on the normative design of general-purpose artificial intelligence. It is divided into two parts:
Normative Agency Design and Past Work
: This section argues that merely imitating human intelligence is insufficient due to the flaws and irrationality found in human thinking. Instead, Pitis proposes a "normative" approach that defines how AI should behave, described through two methods:
Axiomatic Approach
: This involves defining universally appealing axioms like transitive preference, then deriving conclusions about rational agents. Examples include axioms for social choice and a generalized Bellman relation for reinforcement learning (RL).
Capability/Constraint Approach
: Here, the design focuses on the specific capabilities and constraints agents should have, resulting in more empirically motivated solutions. Examples include agents' abilities to explore environments or pursue multiple goals.
Research Topics (November 2020)
: This outlines Pitis's interests and goals, aiming for a clear normative account of intelligence before the development of superhuman AI. The focus includes:
Decomposing Individual Intelligence
: Applying social choice axioms to individual choice, considering conflicting objectives and non-Markovian rewards.
Aggregating Imperfect Principals
: Integrating noisy feedback signals and questioning the axiom of Anonymity.
Axiomatizing Abstraction
: Seeking a normative approach to abstraction, such as how to represent distances between abstractions.
Enabling Dynamic Abstraction
: Developing agents' ability to reason about the world from multiple perspectives.
Constraining Intent
: Considering the intent in RL and the risks and challenges related to explicit intent.
Procedural Normativity
: Introducing axiomatic requirements on procedures to ensure fairness, even if the outcomes are considered wrong.
Stacking Six Blocks From Scratch
: This represents a benchmark problem for sparse-reward multi-goal RL, highlighting the challenge and potential techniques for achieving the task.
In summary, Pitis's research explores the philosophical, ethical, and technical dimensions of designing AI agents, emphasizing an approach grounded in normative principles rather than mere imitation of human behavior.
1 / 1
References 
[1] M. Andrychowicz, A. Raichuk, P. Sta�� nczyk, M. Orsini, S. Girgin, R. Marinier, L. Hussenot, M. Geist, O. Pietquin, 
M. Michalski, et al. What matters in on-policy reinforcement learning? A large-scale empirical study. arXiv 
preprint arXiv:2006.05990, 2020. 
[2] M. Andrychowicz, F. Wolski, A. Ray, J. Schneider, R. Fong, P. Welinder, B. McGrew, J. Tobin, O. P. Abbeel, 
and W. Zaremba. Hindsight experience replay. In Advances in Neural Information Processing Systems, pages 
5048���5058, 2017. 
[3] K. J. Arrow. Social choices and individual values. 1951. 
[4] K. J. Arrow, A. Sen, and K. Suzumura. Handbook of social choice and welfare, volume 2. Elsevier, 2010. 
[5] P. F. Christiano, J. Leike, T. Brown, M. Martic, S. Legg, and D. Amodei. Deep reinforcement learning from 
human preferences. In Advances in Neural Information Processing Systems, pages 4299���4307, 2017. 
[6] J. D. Co-Reyes, Y. Liu, A. Gupta, B. Eysenbach, P. Abbeel, and S. Levine. Self-consistent trajectory autoencoder: 
Hierarchical reinforcement learning with trajectory embeddings. arXiv preprint arXiv:1806.02813, 2018. 
[7] K. Cobbe, O. Klimov, C. Hesse, T. Kim, and J. Schulman. Quantifying generalization in reinforcement learning. 
In International Conference on Machine Learning, pages 1282���1289. PMLR, 2019. 
[8] S. Frederick, G. Loewenstein, and T. O���Donoghue. Time discounting and time preference: A critical review. 
Journal of economic literature, 40(2):351���401, 2002. 
[9] J. Fu, A. Kumar, M. Soh, and S. Levine. Diagnosing bottlenecks in deep q-learning algorithms. arXiv preprint 
arXiv:1902.10250, 2019. 
[10] M. Garnelo, D. Rosenbaum, C. J. Maddison, T. Ramalho, D. Saxton, M. Shanahan, Y. W. Teh, D. J. Rezende, 
and S. Eslami. Conditional neural processes. arXiv preprint arXiv:1807.01613, 2018. 
[11] J. Gordon, J. Bronskill, M. Bauer, S. Nowozin, and R. E. Turner. Meta-learning probabilistic inference for 
prediction. arXiv preprint arXiv:1805.09921, 2018. 
[12] D. Ha and J. Schmidhuber. Recurrent world models facilitate policy evolution. In Advances in Neural Information 
Processing Systems, pages 2450���2462, 2018. 
[13] D. Hadf i eld-Menell, S. J. Russell, P. Abbeel, and A. Dragan. Cooperative inverse reinforcement learning. In 
Advances in neural information processing systems, pages 3909���3917, 2016. 
[14] D. Hafner, T. Lillicrap, J. Ba, and M. Norouzi. Dream to control: Learning behaviors by latent imagination. 
arXiv preprint arXiv:1912.01603, 2019. 
[15] J. C. Harsanyi. Cardinal welfare, individualistic ethics, and interpersonal comparisons of utility. Journal of 
political economy, 63(4):309���321, 1955. 
[16] P. Henderson, R. Islam, P. Bachman, J. Pineau, D. Precup, and D. Meger. Deep reinforcement learning that 
matters. arXiv preprint arXiv:1709.06560, 2017. 
[17] H. J. Jeon, S. Milli, and A. D. Dragan. Reward-rational (implicit) choice: A unifying formalism for reward 
learning. arXiv preprint arXiv:2002.04833, 2020. 3 
[18] D. Kahneman. Thinking, fast and slow. Macmillan, 2011. 
[19] T. C. Koopmans. Stationary ordinal utility and impatience. Econometrica: Journal of the Econometric Society, 
pages 287���309, 1960. 
[20] T. Lu, D. Schuurmans, and C. Boutilier. Non-delusional q-learning and value-iteration. In Advances in neural 
information processing systems, pages 9949���9959, 2018. 
[21] M. J. Machina. Dynamic consistency and non-expected utility models of choice under uncertainty. Journal of 
Economic Literature, 27(4):1622���1668, 1989. 
[22] A. Y. Ng, S. J. Russell, et al. Algorithms for inverse reinforcement learning. In Icml, volume 1, page 2, 2000. 
[23] S. Pitis. Examining expected utility theory from descriptive and prescriptive perspectives. Independent under- 
graduate research; advised by Professor Archishman Chakraborty, 2010. 
[24] S. Pitis. Designing optimal takeover defenses. Independent law school research; advised by Professor Holger 
Spamann, 2013. 
[25] S. Pitis. Punitive damages in international trade. Independent law school research; advised by Professor Mark 
Wu, 2013. 
[26] S. Pitis. Reasoning for reinforcement learning. In Hierarchical Reinforcement Learning Workshop at the 31st 
Conference on Neural Information Processing Systems (HRL@NIPS), 2017. 
[27] S. Pitis. Source Traces for temporal dif f erence learning. In Proceedings of the 32nd AAAI Conference on Artif i cial 
Intelligence. AAAI Press, 2018. 
[28] S. Pitis. Rethinking the discount factor in reinforcement learning: A decision theoretic approach. In Proceedings 
of the 33nd AAAI Conference on Artif i cial Intelligence. AAAI Press, 2019. 
[29] S. Pitis, H. Chan, and J. Ba. ProtoGE: Prototype goal encodings for multi-goal reinforcement learning. In The 
4th Multidisciplinary Conference on Reinforcement Learning and Decision Making (RLDM), 2019. 
[30] S. Pitis, H. Chan, K. Jamali, and J. Ba. An inductive bias for distances: Neural nets that respect the triangle 
inequality. In Proceedings of the Eighth International Conference on Learning Representations (ICLR), 2020. 
[31] S. Pitis, H. Chan, S. Zhao, B. Stadie, and J. Ba. Maximum entropy gain exploration for long horizon multi-goal 
reinforcement learning.In Proceedings of the Thirty-seventh International Conference on Machine Learning 
(ICML), 2020.
[32] S. Pitis, E. Creager, and A. Garg.Counterfactual data augmentation using locally factored dynamics.In 
Proceedings of the Thirty-fourth Conference on Neural Information Processing Systems (NeurIPS), 2020. 
[33] S. Pitis and M. R. Zhang. Objective social choice with non-i.i.d. votes. In Proceedings of International Conference 
on Autonomous Agents and Multi-Agent Systems (AAMAS), 2020. 
[34] M. Plappert, M. Andrychowicz, A. Ray, B. McGrew, B. Baker, G. Powell, J. Schneider, J. Tobin, M. Chociej, 
P. Welinder, et al. Multi-goal reinforcement learning: Challenging robotics environments and request for research. 
arXiv preprint arXiv:1802.09464, 2018. 
[35] A. D. Procaccia. How is voting theory really useful in multiagent systems? available online, URL: http://www. 
cs. cmu. edu/arielpro/papers/vote4mas. pdf (DOA: 15.01. 2013). 
[36] D. M. Roijers, P. Vamplew, S. Whiteson, and R. Dazeley. A survey of multi-objective sequential decision-making. 
Journal of Artif i cial Intelligence Research, 48:67���113, 2013. 
[37] S. Russell. Rationality and intelligence: A brief update. In Fundamental issues of artif i cial intelligence, pages 
7���28. Springer, 2016. 
[38] T. Schaul, D. Horgan, K. Gregor, and D. Silver.Universal value function approximators.In International 
Conference on Machine Learning, pages 1312���1320, 2015. 
[39] H. A. Simon. Models of man; social and rational. 1957. 
[40] R. S. Sutton and A. G. Barto. Reinforcement learning: An introduction. MIT press, 2018. 
[41] S. Thi�� ebaux, C. Gretton, J. Slaney, D. Price, and F. Kabanza. Decision-theoretic planning with non-markovian 
rewards. Journal of Artif i cial Intelligence Research, 25:17���74, 2006. 
[42] J. Von Neumann and O. Morgenstern. Theory of games and economic behavior. 1953. 
[43] M. White. Unifying task specif i cation in reinforcement learning. In Proceedings of the 34th International Con- 
ference on Machine Learning-Volume 70, pages 3742���3750. JMLR. org, 2017. 
[44] M. J. Wolf, K. W. Miller, and F. S. Grodzinsky. Why we should have seen that coming: comments on Microsofts 
Tay experiment, and wider implications. The ORBIT Journal, 1(2):1���12, 2017. 
[45] S. Zuber. The aggregation of preferences: can we ignore the past? Theory and decision, 70(3):367���384, 2011.
1 / 1
ChatGPT
ChatGPT
It seems like you've posted a list of references related to reinforcement learning, economics, social choice theory, and artificial intelligence. Is there anything specific you'd like to know or ask about these references?
1 / 1
These are the references from the research statement. What are they about?
1 / 1
ChatGPT
ChatGPT
The references you've provided cover a wide range of topics in artificial intelligence, economics, and social sciences. Here's a brief summary of some of the general themes:
Reinforcement Learning (RL):
Several references cover different aspects of RL, including on-policy learning, deep RL from human preferences, quantifying generalization, and more. For example:
Andrychowicz et al. (2020, 2017) cover on-policy reinforcement learning and hindsight experience replay.
Pitis et al. (2019, 2020) discuss various aspects of reinforcement learning, including goal encodings and maximum entropy gain exploration.
Social Choice and Welfare:
This includes works by renowned economists like Arrow and others. They cover social choices, individual values, social welfare, and more:
Arrow (1951) covers social choices and individual values.
Arrow, Sen, and Suzumura (2010) provide a handbook on social choice and welfare.
Decision-making and Economics:
Several references are devoted to decision-making under uncertainty, dynamic consistency, time discounting, and related topics:
Frederick et al. (2002) discuss time discounting and time preference.
Kahneman (2011) discusses human decision-making in his popular book "Thinking, Fast and Slow."
Neural Networks and Machine Learning Models:
Some references delve into neural network architectures, model training, and other machine learning themes:
Fu et al. (2019) diagnose bottlenecks in deep Q-learning algorithms.
Plappert et al. (2018) address multi-goal reinforcement learning challenges.
Ethics, Policy, and Legal Aspects:
A few references also touch upon legal studies, ethics in AI, and policy:
Wolf et al. (2017) comment on ethical implications related to Microsoft's Tay experiment.
Theoretical Frameworks and Mathematical Models:
Many references provide mathematical and theoretical foundations to the subjects mentioned above, such as game theory (Von Neumann and Morgenstern, 1953), decision-theoretic planning (Thi��baux et al., 2006), and models of rational behavior (Simon, 1957).
These references likely underpin a comprehensive research statement that spans various interdisciplinary themes related to AI, decision-making, social welfare, and ethics. Without further context, it is challenging to provide a more precise summary of how these references specifically relate to the research statement. If you have any specific questions about particular references, please let me know!
1 / 1
Some of the papers they have written: Defenses
Silviu Pitis. 2013. (Paper)
This paper analyzes the economic value of corporate takeover defenses, and argues for designing intermediate takeover defenses that balance (1) the interest of shareholders in management���s exploitation of insider information and (2) the entrenchment interest of management.

Examining Expected Utility Theory from Descriptive and Prescriptive Perspectives
Silviu Pitis. 2010. (Paper)
This paper examines the history and validity of Expected Utility theory, with focus on its failures a descriptive model of human decisions.
1 / 1
ChatGPT
ChatGPT
Certainly! Here's a brief overview of the two papers you provided:
"Defenses" by Silviu Pitis (2013):
This paper delves into the complex topic of corporate takeover defenses. It analyzes the value of these defenses from an economic standpoint, considering how they can both protect the interests of shareholders and potentially entrench management. The paper argues for a balanced approach, considering both the potential exploitation of insider information and the interests of the management.
"Examining Expected Utility Theory from Descriptive and Prescriptive Perspectives" by Silviu Pitis (2010):
Expected Utility theory is a fundamental concept in economics and decision-making. This paper takes a critical look at the theory, examining its history and questioning its validity as a descriptive model for human decisions. The focus is on identifying the failures of the theory when applied to understanding how real people make choices, which can be an essential aspect for policymakers, economists, and behavioral scientists to consider.
These two papers seem to reflect a broader interest in economic decision-making, corporate governance, and human behavior modeling, likely informing more extensive research within those domains.
1 / 1
Fixed-Horizon Temporal Difference Methods for Stable Reinforcement Learning
Kristopher De Asis, Alan Chan, Silviu Pitis, Richard S. Sutton, Daniel Graves. AAAI 2020. (Arxiv, Talk)
We explore fixed-horizon temporal difference (TD) methods, reinforcement learning algorithms for a new kind of value function that predicts the sum of rewards over a fixed number of future time steps. To learn the value function for horizon h, these algorithms bootstrap from the value function for horizon h���1, or some shorter horizon. Because no value function bootstraps from itself, fixed-horizon methods are immune to the stability problems that plague other off-policy TD methods using function approximation.

ProtoGE: Prototype Goal Encodings for Multi-goal Reinforcement Learning
Silviu Pitis*, Harris Chan*, Jimmy Ba. RLDM 2019. (Paper)
Humans can often accomplish specific goals more readily that general ones. Although more specific goals are, by definition, more challenging to accomplish than more general goals, evidence from management and educational sciences supports the idea that ���specific, challenging goals lead to higher performance than easy goals���. We find evidence of this same effect for RL agents in multi-goal environments. Our work establishes a new state-of-the-art in standard multi-goal MuJoCo environments and suggests several novel research directions.

Rethinking the Discount Factor in Reinforcement Learning: A Decision Theoretic Approach
Silviu Pitis. AAAI 2019. (Paper, Slides, Poster)
Can all ���rational��� preference structures be represented using the standard RL model (the MDP)? This paper presents a minimal axiomatic framework for rationality in sequential decision making and shows that the implied cardinal utility function is of a more general form than the discounted additive utility function of an MDP. In particular, the developed framework allows for a state-action dependent ���discount��� factor that is not constrained to be less than 1 (so long as there is eventual long run discounting).

Source Traces for Temporal Difference Learning
Silviu Pitis. AAAI 2018. (Paper, Slides)
This paper develops source traces for reinforcement learning agents. Source traces provide agents with a causal model, and are related to both eligibility traces and the successor representation. They allow agents to propagate surprises (temporal differences) to known potential causes, which speeds up learning. One of the interesting things about source traces is that they are time-scale invariant, and could potentially be used to provide interpretable answers to questions of causality, such as ���What is likely to cause X?���

Reasoning for Reinforcement Learning
Silviu Pitis. NeurIPS HRL Workshop 2017. (Paper, Poster)
This is a short abstract about some ideas that connect implicit understanding (value functions) and explicit reasoning in the context of reinforcement learning.

Methods for Retrieving Alternative Contract Language Using a Prototype
Silviu Pitis. ICAIL 2017. (Best Student Paper). (Paper, Slides)
This paper presents a search engine that finds similar language to a given query (the prototype) in a database of contracts. Results are clustered so as to maximize both coverage and diversity. This is useful for contract drafting and negotiation, administrative tasks and legal research.

An Alternative Arithmetic for Word Vector Analogies
Silviu Pitis. 2016. (Paper)
This paper looks at word vector arithmetic of the type ���king - man + woman = queen��� and investigates treating the relationships between word vectors as rotations of the embedding space instead of as vector differences. This was a one week project of little practical significance, but with the advent of latent vector arithmetic (e.g., for GANs), it may be worth revisiting.

Punitive Damages in International Trade
Silviu Pitis. 2014. (Paper)
What should the structure of rights, remedies, and enforcement look like in an efficient international trade agreement? Do punitive damages have a place?
1 / 1
ChatGPT
ChatGPT
Certainly! Here's a brief summary of the additional papers:
"Fixed-Horizon Temporal Difference Methods for Stable Reinforcement Learning" (2020):
This paper introduces fixed-horizon temporal difference methods to address stability issues in off-policy reinforcement learning. It presents a novel way to predict the sum of rewards over a specific time, improving stability and offering a new perspective on value function prediction.
"ProtoGE: Prototype Goal Encodings for Multi-goal Reinforcement Learning" (2019):
This research applies the idea that specific, challenging goals lead to higher performance to reinforcement learning. It presents evidence of this effect within multi-goal environments and sets a new standard in multi-goal MuJoCo environments.
"Rethinking the Discount Factor in Reinforcement Learning: A Decision Theoretic Approach" (2019):
This paper challenges conventional thinking about the discount factor in RL, presenting a more general form of the cardinal utility function and allowing for a state-action dependent ���discount��� factor, thus expanding the traditional model.
"Source Traces for Temporal Difference Learning" (2018):
This paper introduces source traces, which provide RL agents with a causal model to propagate surprises and accelerate learning. This novel concept offers a potentially interpretable perspective on causality within the system.
"Reasoning for Reinforcement Learning" (2017):
This abstract connects implicit understanding and explicit reasoning within the realm of RL, offering a brief insight into bridging these concepts.
"Methods for Retrieving Alternative Contract Language Using a Prototype" (2017):
Winner of the Best Student Paper, this paper introduces a search engine to find similar language in contract databases, aiding in contract drafting, negotiation, and legal research.
"An Alternative Arithmetic for Word Vector Analogies" (2016):
This paper explores the idea of treating word vector relationships as rotations instead of differences. Though initially of limited practical significance, it could have implications for latent vector arithmetic.
"Punitive Damages in International Trade" (2014):
This research analyzes the role of punitive damages in international trade agreements, exploring the structure of rights, remedies, and enforcement, and considering whether punitive damages should have a place in efficient trade agreements.
Overall, the research covers a wide spectrum of topics, including reinforcement learning, goal-setting, decision theory, contract language retrieval, word vector arithmetic, and international trade. Many of the papers center around novel concepts in machine learning and artificial intelligence, with applications extending to economics and legal domains.
1 / 1
Rational Multi-Objective Agents Must Admit Non-Markov Reward Representations
Silviu Pitis, Duncan Bailey, Jimmy Ba.
Return Augmentation Gives Supervised RL Temporal Compositionality
Keiran Paster, Silviu Pitis, Sheila McIlraith, Jimmy Ba.
Large Language Models Are Human-Level Prompt Engineers
Yongchao Zhou, Andrei Ioan Muresanu, Ziwen Han, Keiran Paster, Silviu Pitis, Harris Chan, Jimmy Ba. (Arxiv, Website)
MoCoDA: Model-based Counterfactual Data Augmentation
Silviu Pitis, Elliot Creager, Ajay Mandlekar, Animesh Garg. NeurIPS 2022. (Arxiv, Website)
Can RL agents generalize to new tasks w/ unseen states? Read the twitter thread to find out.

Counterfactual Data Augmentation using Locally Factored Dynamics
Silviu Pitis, Elliot Creager, Animesh Garg. NeurIPS 2020. Object-Oriented Learning Workshop at ICML 2020 (Outstanding Paper). (Arxiv, Talk, Code, Poster, OOL Workshop)
In this paper we proposed a local causal model (LCM) framework that captures the benefits of decomposition in settings where the global causal model is densely connected. We used our framework to design a local Counterfactual Data Augmentation (CoDA) algorithm that expands available training data with counterfactual samples by stitching together locally independent subsamples from the environment. Empirically, we showed that CoDA can more than double the sample efficiency and final performance of reinforcement learning agents in locally factored environments.

Maximum Entropy Gain Exploration for Long Horizon Multi-goal Reinforcement Learning
Silviu Pitis*, Harris Chan*, Stephen Zhao, Bradly Stadie, Jimmy Ba. ICML 2020. Adaptive and Learning Agents Workshop at AAMAS 2020 (Best Paper). (Arxiv, Talk, Code)
What goals should a multi-goal reinforcement learning agent pursue during training in long-horizon tasks? Our MEGA and OMEGA agents set achievable goals in sparsely explored areas of the goal space to maximize the entropy of the historical achieved goal distribution. This lets them learn to navigate mazes and manipulate blocks with a fraction of the samples used by prior approaches.

An Inductive Bias for Distances: Neural Nets that Respect the Triangle Inequality
Silviu Pitis*, Harris Chan*, Kiarash Jamali, Jimmy Ba. ICLR 2020. (Arxiv, OpenReview, Talk, Code)
We propose novel neural network architectures, guaranteed to satisfy the triangle inequality, for purposes of (asymmetric) metric learning and modeling graph distances.

Objective Social Choice: Using Auxiliary Information to Improve Voting Outcomes
Silviu Pitis and Michael R. Zhang. AAMAS 2020. (Arxiv, Talk, Code)
How should one combine noisy information from diverse sources to make an inference about an objective ground truth? Past studies typically assume that noisy votes are identically and independently distributed (i.i.d.), but this assumption is often unrealistic. Instead, we assume that votes are independent but not necessarily identically distributed and that our ensembling algorithm has access to certain auxiliary information related to the underlying model governing the noise in each vote.
1 / 1
ChatGPT
ChatGPT
Here are summaries for the additional research papers:
"Rational Multi-Objective Agents Must Admit Non-Markov Reward Representations" (Authors: Silviu Pitis, Duncan Bailey, Jimmy Ba):
This paper explores the necessity of non-Markov reward representations in the context of rational multi-objective agents, introducing a conceptual shift in understanding reward structures.
"Return Augmentation Gives Supervised RL Temporal Compositionality" (Authors: Keiran Paster, Silviu Pitis, Sheila McIlraith, Jimmy Ba):
This work investigates how return augmentation can provide temporal compositionality in supervised reinforcement learning, offering a new method to enhance the learning process.
"Large Language Models Are Human-Level Prompt Engineers" (Authors: Yongchao Zhou, Andrei Ioan Muresanu, Ziwen Han, Keiran Paster, Silviu Pitis, Harris Chan, Jimmy Ba):
This paper acknowledges the capability of large language models to engineer prompts at a level comparable to humans, presenting insights into the development and understanding of language models.
"MoCoDA: Model-based Counterfactual Data Augmentation" (Authors: Silviu Pitis, Elliot Creager, Ajay Mandlekar, Animesh Garg):
MoCoDA introduces a counterfactual data augmentation technique using model-based strategies, aimed at improving the efficiency of RL agents. It also explores their ability to generalize to new tasks and unseen states.
"Counterfactual Data Augmentation using Locally Factored Dynamics" (Authors: Silviu Pitis, Elliot Creager, Animesh Garg):
This paper presents a local causal model (LCM) framework for Counterfactual Data Augmentation (CoDA), enabling enhanced sample efficiency and performance in locally factored environments.
"Maximum Entropy Gain Exploration for Long Horizon Multi-goal Reinforcement Learning" (Authors: Silviu Pitis, Harris Chan, Stephen Zhao, Bradly Stadie, Jimmy Ba):
This research introduces MEGA and OMEGA agents for multi-goal reinforcement learning, emphasizing long-horizon tasks. By maximizing the entropy of historical achieved goals, the agents learn to navigate and manipulate environments more efficiently.
"An Inductive Bias for Distances: Neural Nets that Respect the Triangle Inequality" (Authors: Silviu Pitis, Harris Chan, Kiarash Jamali, Jimmy Ba):
This work presents innovative neural network architectures that inherently respect the triangle inequality, designed specifically for asymmetric metric learning and graph distance modeling.
"Objective Social Choice: Using Auxiliary Information to Improve Voting Outcomes" (Authors: Silviu Pitis, Michael R. Zhang):
This paper explores how to combine noisy information from diverse sources to infer an objective truth, going beyond the assumption of i.i.d. votes. The authors use auxiliary information to relate to the underlying model governing the noise in each vote, providing a more nuanced approach to social choice and voting dynamics.
This collection of research papers offers a comprehensive look into Silviu Pitis's work, encompassing areas such as reinforcement learning, neural network architecture, data augmentation, metric learning, and social choice theory. The breadth and depth of topics reflect a versatile approach to AI and machine learning research.
1 / 1
What are some common threads in this research?
1 / 1
ChatGPT
ChatGPT
The research papers listed encompass a wide variety of topics, but some common threads and themes can be identified:
Reinforcement Learning (RL):
Many of the papers focus on developing new techniques, algorithms, and frameworks within the field of RL. This includes work on temporal difference methods, multi-goal reinforcement learning, counterfactual data augmentation, exploration strategies, and integration of rational preferences within RL.
Machine Learning and Neural Network Architectures:
There's an emphasis on innovative approaches to neural networks and machine learning algorithms. For example, designing neural networks that respect the triangle inequality and exploring the mathematics of word vector analogies.
Decision Theory and Rationality:
Several papers dive into theoretical aspects of decision-making, rationality, and utility theory. These include works on Expected Utility theory, non-Markov reward representations, and a minimal axiomatic framework for rational sequential decision-making.
Data Augmentation and Efficiency:
The research includes exploration into data augmentation methods, such as counterfactual data augmentation, aimed at increasing the sample efficiency and effectiveness of learning models. The focus on efficiency is prevalent across different methodologies.
Human Cognition and Behavior Analogies:
Some papers draw parallels between human cognition and machine learning techniques, investigating topics such as goal setting, reasoning, and prompt engineering in large language models.
Interdisciplinary Applications:
Several works extend beyond traditional AI and machine learning boundaries, exploring applications in economic theory, legal research, international trade, and social choice, reflecting a holistic and interdisciplinary approach to problem-solving.
Collaboration with Recurring Co-authors:
Notably, certain co-authors, such as Jimmy Ba and Harris Chan, appear repeatedly, indicating ongoing collaborations and possibly shared research interests.
These common threads illustrate a cohesive and multifaceted approach to research, integrating both theoretical foundations and practical applications across diverse domains, all while maintaining a strong emphasis on reinforcement learning, innovative algorithms, and efficiency.
1 / 1
arXiv:2002.05825 (cs)
[Submitted on 14 Feb 2020 (v1), last revised 6 Jul 2020 (this version, v3)]
An Inductive Bias for Distances: Neural Nets that Respect the Triangle Inequality
Silviu Pitis, Harris Chan, Kiarash Jamali, Jimmy Ba
Download PDF
Distances are pervasive in machine learning. They serve as similarity measures, loss functions, and learning targets; it is said that a good distance measure solves a task. When defining distances, the triangle inequality has proven to be a useful constraint, both theoretically--to prove convergence and optimality guarantees--and empirically--as an inductive bias. Deep metric learning architectures that respect the triangle inequality rely, almost exclusively, on Euclidean distance in the latent space. Though effective, this fails to model two broad classes of subadditive distances, common in graphs and reinforcement learning: asymmetric metrics, and metrics that cannot be embedded into Euclidean space. To address these problems, we introduce novel architectures that are guaranteed to satisfy the triangle inequality. We prove our architectures universally approximate norm-induced metrics on Rn, and present a similar result for modified Input Convex Neural Networks. We show that our architectures outperform existing metric approaches when modeling graph distances and have a better inductive bias than non-metric approaches when training data is limited in the multi-goal reinforcement learning setting.
1 / 1
ChatGPT
ChatGPT
The paper titled "An Inductive Bias for Distances: Neural Nets that Respect the Triangle Inequality" by Silviu Pitis, Harris Chan, Kiarash Jamali, and Jimmy Ba (arXiv:2002.05825) focuses on the importance of distances in machine learning and the role of the triangle inequality as a critical constraint.
Here's a brief summary of the key components of the research:
Problem Statement
: Traditional deep metric learning architectures that respect the triangle inequality often rely on Euclidean distance in the latent space. This approach fails to model two significant classes of distances that are subadditive and are commonly found in graphs and reinforcement learning, namely asymmetric metrics, and metrics that cannot be embedded into Euclidean space.
Novel Architectures
: The authors propose novel neural network architectures that are guaranteed to satisfy the triangle inequality. These architectures are designed to overcome the limitations of traditional methods by approximating norm-induced metrics on
R
n
R^n
R
n
and incorporating modified Input Convex Neural Networks.
Universal Approximation
: The paper proves that the proposed architectures can universally approximate certain types of metrics, making them more versatile and theoretically grounded.
Empirical Evidence
: Through empirical studies, the authors demonstrate that their architectures outperform existing metric learning approaches when modeling graph distances. They also show that their methods have better inductive bias than non-metric approaches, particularly when training data is limited in multi-goal reinforcement learning scenarios.
Relevance to Broader Research Themes
: This paper aligns with several common threads in the previously mentioned research, including the emphasis on innovative neural network architectures, the integration of reinforcement learning principles, and the theoretical examination of metric properties.
Overall, this paper offers a significant contribution by addressing an unmet need in the field of deep metric learning and providing both theoretical insights and practical solutions that respect the triangle inequality.
1 / 1
4DISCUSSION ANDOTHERRELATEDWORK 
Deep Norms, Wide Norms, and Neural Metrics all respect the triangle inequality while universally 
approximating i nite-dimensional asymmetric semi-norms. This allows them to represent metrics 
that the deep Euclidean Siamese architecture cannot, no matter how deep its embedding function is 
(see Figure 1). Our models thus provide a more expressive, non-Euclidean alternative for learning 
distances that satisfy the triangle inequality. As noted in the Introduction, this may useful for 
providing running time and error rate guarantees in clustering (Davidson and Ravi, 2009) and as an 
inductive bias to improve generalization performance (Figure 5; Hsieh et al. (2017)). 
A line of theoretical work, surveyed by Indyk et al. (2017), characterizes the representational 
capacity of Euclidean space (and, more generally,���pspace) by examining the asymptotic ���distortion��� 
of embeddingn-point metric spaces into Euclidean space. Here the distortion of an embedding 
�� : X ��� X0of 
metric space(X,dX)into 
metric space(X0,dX0)is 
at mostc ��� 1if 
there exists 
r > 0such that for allx,y ��� X,r �� dX(x,y) ��� dX0(��(x),��(y)) ��� cr �� dX(x,y). The pioneering 
work of Bourgain (1985) bounds worst case distortion byO(logn), and Linial et al. (1995) shows 
that this bound is tight (i.e.,��(logn)) for graph distances inn-point constant-degree expanders. 
This applies regardless of the dimensionality of the embedding space, and is true for all���pwith 
1 ��� p < ���. Future work might investigate the asymptotic distortion of our proposed architectures. 
On account of the above limitations, others have also proposed non-Euclidean alternatives to learning 
and representing metrics. To improve expressivity, some have proposed non-parametric algorithms 
that learn distances directly. For instance, Biswas and Jacobs (2015) propose to frame clustering as 
a metric nearness problem and applying a quadratic programming algorithm; see also Gilbert and 
Jain (2017) and Veldt et al. (2018). Others learn parametric distances, as we do. Bi et al. (2015) 
parameterize and learn Cayley-Klein metrics, which are not translation-invariant. Yang and Jin 
(2006) and Nielsen et al. (2016) propose metrics that are similar to our Wide Norm, in that they use 
non-negative sums of several components (binary codes and Cayley-Klein metrics). As for symmetry, 
several papers have used asymmetric measures such as KL divergence (Vilnis and McCallum, 2014; 
8 
Published as a conference paper at ICLR 2020 
Vendrov et al., 2015; Chen et al., 2016; Ou et al., 2016). To our knowledge, we are the i rst to propose 
a parametric measure that is both asymmetric and satisf i es the triangle inequality. 
Another common approach in deep ���metric��� learning is to forgo the triangle inequality altogether and 
use non-metric similarity measures such as cosine similarity (Bromley et al., 1994; Yi et al., 2014). 
This can be sensible, as proper metrics are not always required. In image recognition, for example, the 
triangle inequality is questionable: why shouldd(CAT,WOLF) ��� d(CAT,DOG) 
+ d(DOG,WOLF)? 
In this case, Scheirer et al. (2014) i nd that non-metric similarities often perform better, concluding 
that ���good recognition is non-metric���. Thus, one should apply our models with care, in settings 
where the triangle inequality is thought to be a useful inductive bias (e.g., Hsieh et al. (2017)). Such 
applications we are excited about include learning search heuristics (Russell and Norvig, 2016) and 
scaling our UVFA models to more complex reinforcement learning problems (Plappert et al., 2018). 
5CONCLUSION 
This paper proposed three novel architectures for modeling asymmetric semi-norms and metrics. 
They can be used instead of the usual Euclidean metric to increase expressiveness while respecting 
the triangle inequality. We showed that our models outperform Euclidean metrics when used with a 
Siamese-style deep metric learning architecture to (1) solve the metric nearness problem, (2) model 
shortest path lengths, and (3) learn general value functions in small reinforcement learning domains. 
Future work should explore larger scale applications such as facial recognition (Schroff et al., 2015), 
collaborative metric learning (Hsieh et al., 2017) and continuous control (Plappert et al., 2018).
1 / 1
ChatGPT
ChatGPT
This section, labeled "DISCUSSION AND OTHER RELATED WORK," elaborates on the main findings and connections to existing work of the paper "An Inductive Bias for Distances: Neural Nets that Respect the Triangle Inequality."
New Architectures for Distance Learning
: The paper introduces three novel architectures, termed Deep Norms, Wide Norms, and Neural Metrics, capable of universally approximating finite-dimensional asymmetric semi-norms and respecting the triangle inequality. These architectures can represent metrics that traditional deep Euclidean Siamese architecture cannot, providing a more expressive, non-Euclidean alternative.
Theoretical Foundations
: The authors connect their work to theoretical studies on the representational capacity of Euclidean space. They mention the pioneering work of Bourgain and others, which provides bounds on the distortion of embedding metric spaces into Euclidean space. The paper also hints at future work investigating the asymptotic distortion of their proposed architectures.
Non-Euclidean Alternatives
: The authors recognize that others have also proposed non-Euclidean alternatives to learning metrics, some of which involve non-parametric algorithms or unique parameterizations. They highlight their contribution as the first to propose a parametric measure that is both asymmetric and satisfies the triangle inequality.
Questioning Triangle Inequality
: The authors also discuss the notion of forgoing the triangle inequality in deep "metric" learning and using non-metric similarity measures like cosine similarity. They provide examples where the triangle inequality might not be appropriate, like in image recognition, and where non-metric similarities often perform better.
Concluding Remarks
: The conclusion reiterates the proposal of novel architectures for modeling asymmetric semi-norms and metrics and their outperformance over Euclidean metrics in specific problems. They also identify potential areas for future exploration, including larger scale applications such as facial recognition and continuous control.
This section summarizes the main contributions, contextualizes them within existing research, outlines the theoretical implications, and suggests potential future directions. It reflects a concerted effort to push the boundaries of metric learning by focusing on asymmetric measures and the triangle inequality, offering new paths for both practical applications and theoretical inquiry.
1 / 1
A list of everything we have talked about so far.
1 / 1
ChatGPT
ChatGPT
Certainly! Here's a summary of everything we've discussed so far:
List of Research Papers and Topics
: You provided a series of research papers and topics, mostly related to reinforcement learning, counterfactual data augmentation, multi-objective agents, neural network architectures, metric learning, and other related concepts. Most of the work is connected to Silviu Pitis and collaborators like Jimmy Ba.
Specific Research Paper Discussion
: You shared details from the paper "An Inductive Bias for Distances: Neural Nets that Respect the Triangle Inequality" by Silviu Pitis, Harris Chan, Kiarash Jamali, and Jimmy Ba.
Detailed Analysis of the Discussion Section
: We then examined the discussion and related work section of that paper, highlighting key aspects such as new architectures for distance learning, theoretical foundations and connections, non-Euclidean alternatives, considerations regarding the triangle inequality, and the conclusion and future directions.
These topics cover various aspects of machine learning, particularly focused on reinforcement learning, neural network architectures, and metric learning, revealing a strong interest in theoretical foundations and practical applications of these areas.
1 / 1
It was the research interests of Silviu Pitis.
1 / 1
ChatGPT
ChatGPT
Yes, the information provided covers a broad range of research interests of Silviu Pitis. These interests seem to include:
Reinforcement Learning
: Exploration in multi-goal reinforcement learning, improvements in sample efficiency, and generalizations to new tasks with unseen states.
Metric Learning and Neural Architectures
: Development of neural network architectures that respect the triangle inequality, offering new perspectives on distance measurement and representation.
Counterfactual Data Augmentation
: Creation of algorithms for expanding training data using counterfactual samples, leading to enhanced performance in reinforcement learning agents.
Multi-Objective Agents and Reward Representations
: Exploration of non-Markov reward representations and aspects related to multi-objective agents.
Objective Social Choice
: Utilizing auxiliary information to improve voting outcomes, offering insights into combining noisy information from diverse sources.
Novel Methods and Algorithms
: Including return augmentation for supervised RL, model-based counterfactual data augmentation, and large language model studies.
Overall, Silviu Pitis's research spans a diverse array of topics within machine learning, with a particular emphasis on reinforcement learning, distance metrics, and neural architectures.
1 / 1
I think he is interested in agency based constraint programming. He says it isn't enough to make computers like humans, but they have to be more ethical, less prone to mistakes, and better able to explain their computations and justify their moral reasoning.
1 / 1
ChatGPT
ChatGPT
Certainly, the idea of agency-based constraint programming aligns with the broader themes present in Silviu Pitis's research. By focusing on areas like reinforcement learning, metric learning, and social choice, he seems to be tackling the fundamental challenges of creating intelligent systems that not only mimic human-like reasoning but also incorporate ethical considerations and robustness against errors.
The emphasis on ethical considerations, reduced mistakes, and explainable computations points towards an interest in responsible AI and building transparent, accountable models that align with human values. While the specific research papers and discussions you provided earlier don't explicitly address these themes, they can be seen as building blocks towards creating such intelligent and responsible agents.
The interdisciplinary nature of these research interests could indeed contribute to a broader understanding of how to design and implement machines that go beyond merely replicating human-like functions and instead integrate deeper principles of ethics, accountability, and explainability.
1 / 1
A final summary
1 / 1
ChatGPT
ChatGPT
Silviu Pitis's research interests span a variety of interconnected fields, particularly focusing on reinforcement learning, metric learning, social choice, and model-based counterfactual data augmentation. Through this research, Pitis appears to explore complex challenges in creating intelligent systems that not only resemble human reasoning but go a step further in incorporating ethical considerations and robustness.
The exploration of novel neural network architectures respecting the triangle inequality, the creation of reinforcement learning agents with increased sample efficiency, and the interest in combining diverse and noisy information for objective inference all point towards a commitment to advancing AI in a way that is not only intelligent but also ethical and responsible.
These themes collectively hint at a broader ambition to design AI systems that are more human-aligned, able to explain their computations, justify their moral reasoning, and minimize errors, contributing to the ongoing discourse around responsible and transparent AI.
1 / 1
I am intersting in developing filter based scene layout actor based template compression for images, plus I want to be able to define beauty, practicality, and purpose as well as categorize images by their themes, and filter out gore, overt sexuality, and violence.
1 / 1
ChatGPT
ChatGPT
That's a comprehensive and complex set of goals, encompassing various domains of computer vision, machine learning, ethics, and aesthetics. Here's a brief outline of the key components you might need to explore:
Filter-Based Scene Layout
: Developing algorithms to analyze and understand the layout of a scene in an image, possibly utilizing convolutional neural networks (CNNs) to detect different objects and their spatial relationships.
Actor-Based Template Compression
: This could involve using actor models to recognize key elements within images and compressing them based on some predefined templates or patterns, likely necessitating both deep learning and traditional image compression techniques.
Defining and Recognizing Beauty, Practicality, and Purpose
: Creating models that can recognize and categorize these abstract concepts might require extensive training data and potentially interdisciplinary collaboration with experts in aesthetics, design, or philosophy.
Categorizing Images by Themes
: This task might be solved with a well-designed classification model, trained on a large dataset with predefined theme labels.
Filtering Out Gore, Overt Sexuality, and Violence
: Content filtering is a crucial area in modern media, requiring sophisticated models that can accurately identify and remove unwanted content. This task might involve building specialized detection models for each category of content you wish to filter.
These goals touch on some of the most challenging and exciting areas in AI and image processing today. Engaging with existing literature, collaborating with experts in the field, and considering ethical guidelines will likely be vital steps in the pursuit of these ambitious objectives.
1 / 1
I also want to work on auditory and visual based programming languishes and exploration environments.
1 / 1
ChatGPT
ChatGPT
Working on auditory and visual-based programming languages and exploration environments is a fascinating area that merges human-computer interaction, programming languages, machine learning, and multimedia processing. Here's an overview of what such an endeavor might entail:
Auditory-Based Programming Languages
:
Designing Syntax and Semantics
: You'll need to create a well-defined syntax and semantics that can be effectively conveyed through sound.
Voice Recognition and Processing
: Implementing voice commands and auditory feedback would require robust speech recognition and synthesis techniques.
Accessibility Considerations
: An auditory programming language could be incredibly beneficial for visually impaired users, so accessibility should be a primary concern.
Visual-Based Programming Languages
:
Graphical Interfaces
: Developing a visual programming language may involve creating intuitive graphical interfaces that allow users to manipulate code elements visually.
User Experience (UX) Design
: A significant emphasis on UX design would be necessary to make the environment engaging and efficient for users.
Integration with Traditional Code
: Allowing for a seamless transition between visual and text-based coding could increase the usability and adoption of the language.
Exploration Environments
:
Interactive Learning Platforms
: Creating environments where users can explore and experiment with these auditory and visual languages might include building interactive tutorials, real-time feedback mechanisms, and collaborative tools.
Integration with Existing Tools
: Compatibility with existing development tools, editors, and debugging environments would make these languages more practical and appealing to existing developers.
Ethical and Societal Considerations
:
Consider how these new forms of interaction with computers could affect learning, accessibility, and the democratization of technology.
This endeavor would likely require collaboration across various disciplines, including computer science, human-computer interaction, design, linguistics, and perhaps even music and art for the auditory and visual aspects, respectively. Research in these areas could open up new, more intuitive ways to interact with technology and could have a significant impact on education, accessibility, and the broader tech industry.
1 / 1
The idea is that each topic is assigned a tree with bird that tweets its name. Some topics sound like trains and vehicles as well.
1 / 1
ChatGPT
ChatGPT
That's an incredibly innovative concept, blending auditory symbolism and thematic categorization. Here's how you might be able to develop and implement this idea:
Thematic Trees and Birds
:
Classification
: Use machine learning algorithms to classify topics and assign them to specific trees and birds.
Sound Design
: Design or select specific bird tweets and other sounds (e.g., trains, vehicles) to represent different topics. You'll want to carefully choose sounds that align with the thematic content and are distinct from one another.
Visual Representation
: Create visually appealing graphics for the trees and birds to provide an intuitive visual interface. You could also consider representing other themes with corresponding visual elements.
Interactive Environment
:
User Navigation
: Design a navigation system where users can explore the trees and listen to the bird tweets to understand the different topics. This could include a 3D environment that users can traverse or a 2D map with interactive elements.
Search and Filter Options
: Implement tools that allow users to search for specific topics or filter by category, with corresponding auditory and visual feedback.
Accessibility
: Ensure that the environment is accessible to users with disabilities, which might involve providing text descriptions of the sounds or visual cues.
Integration with Existing Systems
:
API Development
: You might want to develop an API that allows other developers to integrate your auditory and visual categorization system into their applications, websites, or services.
Cross-Platform Support
: Consider how the system will function on different devices and platforms, ensuring a consistent experience across all of them.
Ethical Considerations and Content Moderation
:
Content Filtering
: As you mentioned filtering out specific content (e.g., gore, overt sexuality), you will need algorithms or moderation systems to accurately classify and filter such content.
User Privacy and Security
: If user data is involved, be mindful of privacy considerations and provide clear policies on how data will be used and protected.
Community Engagement
:
User Feedback
: Consider involving potential users in the design process to ensure that the system is intuitive and meets their needs.
Collaboration with Artists and Sound Engineers
: Collaborate with professionals who can create a rich auditory experience that aligns with your thematic goals.
Your idea seems to lie at the intersection of auditory design, visual aesthetics, machine learning, and user experience. It could find applications in educational tools, content discovery platforms, accessibility technology, and more. By working with a multidisciplinary team and considering both the technical and human-centric aspects, you could turn this innovative concept into a powerful tool for interacting with information and themes.
1 / 1
In addition to multi-dimensional dialogue visualization, you will also be able to program by digging tunnels, popping and blowing bubbles, and growing them from seed.
1 / 1
ChatGPT
ChatGPT
Your idea to incorporate multi-dimensional dialogue visualization along with tactile and organic metaphors like digging tunnels, popping and blowing bubbles, and growing seeds is an exciting expansion into a new way of interacting with programming and data. Here's how these concepts might be integrated into your system:
Multi-Dimensional Dialogue Visualization
:
Dialogue Mapping
: Create a system that visually represents dialogues as multi-dimensional structures, possibly with varying shapes, colors, or textures to denote different elements of the dialogue.
Interaction
: Allow users to navigate through the dialogue, expanding or collapsing parts of it, perhaps through gestures that mimic natural movements like unfolding, stretching, or twisting.
Integration with Sound
: Connect the visualizations with the auditory symbols you previously developed, allowing users to hear corresponding sounds as they explore the dialogue.
Programming Through Organic Metaphors
:
Digging Tunnels
: Represent complex tasks or functions as tunnels that users can dig through, defining the path and complexity of the logic.
Blowing and Popping Bubbles
: Use bubbles as objects or variables that can be created, manipulated, or destroyed in an intuitive way. Popping might represent deleting or ending a process, while blowing could be a way to inflate or add to it.
Growing from Seed
: Allow users to plant seeds for different functions or modules and nurture them to grow, representing the development and complexity of code.
Virtual or Augmented Reality Interface
:
3D Interaction
: Consider implementing these concepts in a 3D space, possibly using virtual or augmented reality, to allow users to interact with the programming environment in a more immersive and tangible way.
Haptic Feedback
: If using VR or AR, haptic feedback could be utilized to give users a physical sensation of the activities like digging, blowing, and growing.
Educational Applications
:
Learning Tools
: This intuitive and visual way of programming could be an excellent educational tool, particularly for children or those new to coding. It turns abstract concepts into tangible actions.
Tutorials and Guides
: Create interactive tutorials that guide users through the process, teaching them how to use these organic actions to build code.
Accessibility and Usability
:
Customization
: Allow users to customize the interface to suit their preferences, potentially changing the metaphors or adjusting the visual and auditory cues.
User Testing
: Conduct user testing to ensure that these metaphors are intuitive and effective for the intended audience.
By blending auditory, visual, and tactile elements in a way that mimics natural, organic processes, you're proposing a radically new way to interact with programming. It's a concept that could make programming more accessible, engaging, and intuitive, especially for those who might find traditional coding environments daunting or abstract. It could also foster a deeper connection between technology and nature, resonating with users on a more emotional and creative level. It's an ambitious project that would likely require collaboration across several disciplines, but the potential impact is substantial.
1 / 1
How does this align with Normative Agency Design in the Research Statement by
Silviu Pitis?
1 / 1
ChatGPT
ChatGPT
Silviu Pitis's interest in Normative Agency Design and your concept of multi-dimensional dialogue visualization and organic programming metaphors may seem disparate at first glance, but there are some underlying commonalities and alignments.
Ethical Considerations
:
Normative Agency Design
: Pitis emphasizes the ethical behavior of agents, making them more moral and able to justify their reasoning.
Your Concept
: By focusing on natural, organic metaphors, your system may allow for more intuitive, humane interactions, possibly leading to ethical considerations in how code is constructed and applied.
Transparency and Explainability
:
Normative Agency Design
: A key part of Pitis's work is to make computations transparent and justifiable, helping non-experts understand why decisions are made.
Your Concept
: By visualizing code through tangible metaphors, your system may make programming more understandable and transparent to a broader audience, aligning with Pitis's goal of making complex computations accessible.
Human-Centric Design
:
Normative Agency Design
: Pitis aims to make computers more like humans in reasoning and ethics, focusing on human-centric design principles.
Your Concept
: The organic metaphors and intuitive interfaces emphasize a human-centric approach, possibly making programming more accessible and aligned with human thinking and experience.
Innovation in Interface and Interaction
:
Normative Agency Design
: Though not directly expressed, the principles of human-like reasoning and ethical considerations likely involve innovative ways of interaction and interface design.
Your Concept
: Your approach is highly innovative in its use of visual, auditory, and tactile interactions, potentially resonating with the innovative spirit found in Pitis's work.
Potential Collaboration and Integration
:
Normative Agency Design
: The ethical frameworks and principles Pitis explores could be embedded into your system, guiding how code is developed, executed, and evaluated.
Your Concept
: Your approach to programming could serve as a platform to implement and test Pitis's theories in real-world applications, providing a rich, interactive environment that embodies his principles.
While there may not be a direct one-to-one mapping between your concept and Pitis's research, the overarching themes of ethical considerations, transparency, human-centric design, and innovative interface development provide a common ground. A collaboration between these two ideas could lead to a novel fusion of ethical reasoning and intuitive programming, enriching both fields and potentially opening new avenues of exploration and development.
1 / 1
A list of all of the topics we've discussed.
1 / 1
ChatGPT
ChatGPT
Certainly! Here's a list of the main topics we've discussed so far:
Research works and papers by Silviu Pitis
:
Rational Multi-Objective Agents
Return Augmentation in Supervised RL
Large Language Models
Counterfactual Data Augmentation
Maximum Entropy Gain Exploration
An Inductive Bias for Distances
Objective Social Choice
Normative Agency Design in Silviu Pitis's Research
:
Ethical behavior of computational agents
Transparency and justification in decision-making
Human-like reasoning in computing
User's Interest in Image Processing and Categorization
:
Filter-based scene layout
Actor-based template compression
Defining and categorizing beauty, practicality, purpose
Filtering inappropriate content
Auditory and Visual-based Programming Languages and Environments
:
Organic and intuitive interfaces
Use of auditory cues, like bird tweets and vehicle sounds
Programming through tactile interactions, like digging tunnels and blowing bubbles
Multi-dimensional Dialogue Visualization
:
Novel programming paradigms using natural metaphors
Growing programs from seeds, modeling code organically
Alignment Between Normative Agency Design and User's Concepts
:
Ethical considerations
Transparency and explainability
Human-centric design
Innovative interface and interaction
Potential Collaboration and Integration of Ideas
:
Fusion of ethical reasoning and intuitive programming
These topics encompass a wide and fascinating range of subjects, from cutting-edge research in machine learning to innovative approaches in human-computer interaction and programming paradigms.
1 / 1
A final summary.
1 / 1
ChatGPT
ChatGPT
Certainly! Our discussion has covered an extensive range of topics, primarily revolving around the research works of Silviu Pitis and your innovative ideas.
Silviu Pitis's Research
: We delved into various papers by Silviu Pitis, exploring themes like reinforcement learning, neural networks, inductive biases, metric learning, and ethical decision-making in computational agents.
Your Innovative Ideas
:
Image Processing
: You introduced concepts like filter-based scene layout, actor-based template compression, and ethical categorization of images.
Auditory and Visual Programming
: You envisioned an innovative programming environment using auditory cues and tactile interactions.
Multi-dimensional Dialogue Visualization
: You proposed organic and intuitive ways of modeling code.
Alignment with Normative Agency Design
: Your ideas shared certain commonalities with Pitis's research statement on Normative Agency Design, including human-centric design, ethical reasoning, and transparency.
Potential Collaboration
: The conversation hinted at a fusion of the innovative interface design you proposed with the ethical considerations present in Pitis's research.
Overall, our conversation has bridged the academic realm of machine learning research with visionary concepts in programming, user interaction, and ethical considerations, pointing towards exciting avenues for future development and collaboration.
1 / 1
ChatGPT may produce inaccurate information about people, places, or facts.
ChatGPT July 20 Version
ChatGPT





