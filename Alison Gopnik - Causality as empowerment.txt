Great. Thank you very much. Thanks everyone for inviting me and thanks everyone for for coming.
What I'm going to do today is first talk about some very sort of theoretical new ideas about thinking about causation in the context of reinforcement learning. This is all very unbaked or half baked.
And then I'm going to give some examples of some recent research results that we have that are speaking to these general issues, although they're not directly testing these new theoretical ideas.
So I'm assuming that this is a workshop with all the people who I know are brilliant and wonderful and have thought deeply about causality. So I'm presenting some of these new ideas and hoping to get feedback both on the theoretical ideas and on these new empirical studies.
So this is all right on that page. So the big program that my my lab and I have had really for I'm horrified to say about 30 years now is trying to see if we could answer Alan Turner's questions so famously the first part of the famous imitation
paper in mind Turing presents this imitation game imitated an adult. And actually we know that AI is actually pretty good at solving that problem now.
But it's interesting that even at that very beginning Turing said, but wait a minute, maybe that's totally the wrong idea. Maybe what we should be doing is instead of trying to simulate the adult mind, what we should do is say child's mind.
And the reason he says this is because children aren't just doing a bunch of things confidently, they're learning how to do those things from their data and experience.
And Turing quite accurately said that's really what intelligence is about, and we should try to simulate children if we really want to understand about intelligence, and everybody totally ignored that for the next 30 years.
The next 50 years until very recently when the big advances in machine learning have made people conscious that learning is really an important key to intelligence, and have made people want to look at the way the children learn who are after all the best
So that's been the big theoretical project that we've been working on now for, for, as I say, almost 30 years.
But more recently, in the past five years we've been working as part of this data machine common sense project, and at least the folks at DARPA started recognizing that children were important.
And this is a project to try to see if we can use what we know about learning in children as a way of designing artificial systems that could have something more like human common sense.
And I've discovered from my experience with DARPA that the most important thing about dealing with DARPA is having good acronyms.
It's all about the acronyms.
So our Berkeley one appropriately for children is MESS model building exploratory social learning systems.
And the basic idea is that when we look at what children are doing they're extracting calls on models for statistical evidence something we've talked about already in this group.
And that's a paper summarizing again, you know 30 years worth of research, showing that this is true that even very young children are doing this.
What they're doing is they're doing active learning through exploratory play and speaks very nicely to the conversation with with rich. So not only are they very good at abstracting calls on models from statistics, but they're also good at getting
the data that they need to solve causal problems through their exploration and play and again, there's been beautiful work over the last 10 years, showing just how systematic and rational and competent children are doing this, even though when they're doing it.
Experimentation we call it getting into everything.
But I think a point that rich made is very relevant to this which is that, although we intuitively can see that experimentation science active learning exploration play and childhood is an important way that we solve causal problems.
We know much less about the formal structure of, or the formal and normative characteristics of that experimentation, then we do.
Ironically, you know, about the first part about abstracting puzzle structure from statistical evidence. So we've had a great deal of work with people in this room, showing how you can get normative models of causation for statistics.
When you try and figure out how do you do it by experimentation and active learning, it gets much hairier, and a lot of the sort of traditional mantras in philosophy of science about things like well you have to control everything except one variable and then you just change that one variable.
Don't actually seem to be very good descriptions of what's going on either in science one child.
Okay.
And then we've also done things on social learning that's the other part of the mess but I won't talk about here.
Now, what I, the first set of ideas I want to talk about are relating this project of trying to understand how children learn and particularly how they learn puzzle structure to what looks like a superficially very different project.
That's been very influential within recent machine learning. And this is the project of reinforcement learning.
And I have to say, as an old boomer cognitive scientists, the first time that people told me that they were doing all sorts of reinforcement learning stuff and a line my heart sank it was like, Oh my God, we're going to have to wear bell bottoms again.
It was, you know, we got rid of all that in 1960. Why are we returning to this evil behaviorist attempt.
Well, one reason why we're returning to it is because it works really well. So, it's like, I turn it.
And purposes probably say the same thing about bell bottoms, but I'm more skeptical.
So for things like AlphaGo, for example, reinforcement learning, particularly this new form of deep reinforcement learning has proved to be an extremely effective learning technique.
On the other hand, even though it is so effective in well defined narrow environments where rewards and goals are specified to begin with, it's notoriously has much more difficulty in high dimensional open ended environments where it isn't obvious what the reward is going to be.
And it isn't obvious about what the relationship is between the policies and where there's a much wider space of policies and goals and rewards to explore.
And exploration in general is a big problem in reinforcement learning and again from the very beginning people in reinforcement learning recognize this and have tried various kinds of techniques to allow not just maximizing reward but exploring the environment so that you can find out more about it so that in the long run you can
maximize reward.
On the other hand, what might look like a very different kind of learning, the kind of causal learning that comes out of a kind of causal phase net or graphical model or Bayesian hypothesis testing general model, probabilistic model approach had the advantage that instead of just increasing the likelihood of a policy
or reward, it actually lets you have structured hypotheses about the environment, and that lets you do much better generalization.
But it still has difficulty as I'm mentioning with this problem of exploration and active work. So just having say a causal graphical model doesn't tell you how you should go about doing experiments to decide which of the potential
causal graphical models is actually the best explanation for is the ground to is the best account of what's going on in the room.
And it faces intractable search and sampling problems in practice. So, it's very difficult as we all those of us who have tried again, including people in this room know it can be very difficult again to search through this very large space and potential
and try to evaluate and install the data.
Now, what, what's happened recently in reinforcement learning is this new idea about empowerment. And the more I've thought about this idea, the more it seems to me that it might be a really useful and interesting bridge between what goes on in reinforcement
and what goes on in causal learning. And, you know, people like Jim Woodward a long time ago have pointed out the philosopher Jim Woodward pointed out that reinforcement learning really is a kind of causal learning.
It's a kind of first person learning when you intervene on the world, you see what the outcomes are. If those outcomes are relevant and increase your utilities, then you continue with those with those policies, etc.
So you could think of it as being a very limited narrow kind of causal learning in contrast to something like simply correlations or associations or classical conditioning, where you don't have that element of actually having an intervention and seeing what the outcomes.
Of course, as I've said, in classic reinforcement learning, it's very, it does have these limitations that mean that it's very hard for it to explore and figure out the structure of the environment.
And that means that it's not good in open ended cases. So a way that people in reinforcement learning have tried to solve this problem is by using various kinds of meta reinforcement learning techniques.
So essentially the idea is instead of having the reward be the ultimate reward, the ultimate goal, how will you do in the task, how many stickers you get.
The reward is an intrinsic reward. So the reward is some kind of intrinsic measure of how you are doing as an agent, and what you're trying to do is to maximize that reward, rather than or in addition to maximizing external reward.
The last good example of this is you could try to maximize information gain, for example. So what you do is now you get rewarded, not just for getting a stickers or for winning the game, but you actually get rewarded for making moves that will give you higher amount of information about the environment.
So set up various kinds of curiosity, somewhat sometimes called curiosity based methods that use these kinds of intrinsic rewards, things like information gain or entropy.
The problem is that even using those kinds of measures, it's very easy for the open ended environment is still going to present you with a lot of problems. Here's a classical one.
This is the sitting in front of the television problem. So if you're maximizing information gain, for example, and you just have a screen that has random noise like a fuzzy television screen, then you're going to end up just sitting in front of the screen trying to get more and more information.
And if you use things like entropy, you have that problem as well. And yet that's not really what you want. That's not what you want your agent to do.
You want your agent to get information, but get information that's relevant to the problems that you're trying to solve. And the idea of empowerment is that what you're trying to maximize the intrinsic reward is the mutual information between policies and outcomes.
In other words, what you're trying to do in empowerment is get control over the environment. What you're trying to do is figure out which of the things that you can do are going to have the most effect in the environment, which of the many things that you could do systematically are going to
have the most chance of changing something out in the environment. And if you think about the technical term that classically people in say causal base nets abuse to try to explain causation, it's that you wiggle things, right?
The thing that makes something causal is that if you wiggle A, then B will wiggle too, right? That's really the kind of definition, fundamental intuitive definition of the quotation.
Empowerment is about wiggling. Empowerment is about going through your environment, wiggling as many things as you can, seeing what other things wiggle. And if when you wiggle, you get a wiggle from something else, then you should do more of that.
That's going to be the right path to figure out how the environment works.
And it turns out that using empowerment is a good way of allowing you to explore an environment, sort of for obvious reasons, and to explore the environment in a way that is going to actually be useful in the long run for the goals that you're trying to solve.
And what's striking if you're coming from the causal graphical models background the way I am or causal Bayesian model is how well this meshes with this basic intervention is to counter causation that underpins something like causal basis.
The basic philosophical idea of causation that someone like Jim Woodward or Jay Pearl has is that the way we should understand causation is it's the set of phenomena out there in the world, which allow us to wiggle, right?
They're the things that allow us to actually intervene on a variable in the world and reliably get an effect on another variable.
And that's what causal relationships are. And I think it's really interesting that some philosophers of science of Peter, Dr Smith has a very nice paper about this have argued that there actually isn't even a real natural kind out there in the world.
That is causal relationships.
I mean, when we say that something's causal is it's in this category of relationships out there in the world that we can intervene on.
So it might be there might be really, really, and I think there are really ontologically very, very different kinds of phenomena out there in the world.
The thing that makes them causal is the fact that agents can manipulate them in this particular way, or at least that's a philosophical argument.
And it's really interesting that on that picture of causation, in some sense causation is empowerment, right causation is the capacity to have high mutual information between between your policies and outcomes.
So in a sense, this is the idea that I think is very cool and exciting at least for me is that you could say that having an intrinsic reward for empowerment and doing causal learning are exactly the same thing.
That's what causal learning is about. And that's what empowerment is about. I imagine some of the people who have a more mechanistic account of causation might push back on that but I do think at least that's going to account for a number of cases of causation.
So that's the big theoretical idea that I think is really interesting.
And now let me present some of the empirical data. I wish I could say, and look, if we do empowerment as an intrinsic reward, we get something that looks just like what children do, but we're not quite there yet.
Although we have a little bit of it in one of the studies I'll talk about.
So first let me talk about this other problem about can children do causal learning through exploration. And as I said, we know that children do a tremendous amount of causal learning.
We know that they explore the world and that they do it in relatively rational ways if we give them a choice for example between this action to take or that action to take in order to solve a problem.
They'll choose the one that will be most informative that's worth that Laura Schultz and Liz Bonowitz, lots of other people have done.
But can they actually and will they actually learn this just spontaneously without our presentation. But what happens if we just let them.
So, and how does that compare with what artificial agents do. And the particular example I think people here will probably know this example so I won't go into all the details.
But the particular example that we looked at was whether children can learn not just causal structure at the basic level, but can they learn causal over hypothesis what's a causal over hypothesis.
It's a general principle about how a causal system works. So, for example, I have a machine like my look at detector. This is work with Jasmine Collins and Alonso.
I have a machine like my look at detector.
And I put things on it and if they're blankets they sorry blankets or look at this makes it go and non blankets.
And I want to figure out how it works. And one thing I want to figure out is which of these objects and which combinations of objects will actually make it go that's also what I want.
But this machine might also work on very different general principles. And in particular it might work in the obvious way it might work in a disjunctive principle, where there's something is a blanket or it isn't and if it's a blanket it makes it go and if it doesn't, it, if it isn't it doesn't.
But it might also work on this more unusual conjugal principle where you have to have a combination of blankets to make it go. So one blanket just doesn't have an effectiveness you have to have, you have to have a combination to actually be effective on the machine.
And that's not about any specific look at or any specific block that's a general principle about how machine words.
And a while ago we wanted to ask, could children infer not just specifics about plugins, which they can, but can they infer these general logical principles about how a causal system works.
And what we found was not only can they do if you give them the data so if you give the right pattern of data can they tell that this is a conjunctive or disjunctive machine.
And what we discovered was not only can they do it but they can do it better than grown up systematically do it better than grown up.
So what happens is that when you give adults this kind of task this is also probably relevant to what Rich was talking about, they just use their existing cultural models which are mostly this disjunctive model that's the obvious one.
They just stick with that even when they get a bunch of data that shows that it's wrong, whereas the children are willing to test the alternative hypothesis and therefore they're actually better at finding the solutions to this task.
And that's just one of a whole range of examples that a lot of this work children turned out to be better because they're not as constrained by their father puzzle knowledge.
But as I say what we did all these experiments was actually give the kids the data right. So we wanted to see what would happen if children just explored and how would that be related to artificial agents.
So what we did was design this online version of our blanket detector, and possibly the most interesting thing about this. This. So here's the virtual blanket detector you put things on it, and then it either lights up or it doesn't light up in various combinations.
Perhaps the most interesting thing about this experiment is we have to build the damn with the detectors back in the back in the odds because children didn't want to do anything with screens and now children are perfectly happy to treat this screen as if it's a causal system and they
swipe and they touch and they figure out how it works very very quickly and easily sort of terrifyingly quickly.
So here's a child this is a four year old who's playing with the online blanket detector. And what we did in this case with instead of actually giving them the data we just said, here's my machine play with it figure out how it works and see if you can make it go.
And then we just let the kids play. And what we discovered is kids were very happy and eager to do this. They did it usually sort of 15 to 20 minutes and then we told them to stop but they would often have done it for longer.
They produced somewhere between 20 and 30 different experimental trials different things that they did on the to the machine, and to a remarkable degree they figured out how the machine works so more than half of the children in the course of doing this fully disambiguated
And they did that in one condition we sort of told them here's how here's the different ways that work. Another condition we didn't tell them anything, and they still produce data that would disambiguate the two hypotheses.
And even when they didn't produce enough data to disambiguate the hypotheses, they came to the right conclusions. So they chose the buckets in the conjunctive case that were conjunctive look at send and vice versa in the district case they recognize they saw which the
kids were. And most notably when at the end of the experiment when we said now make it go. If it had a conjunctive structure, they put two buckets on to make it go and if it has disjuncture structure, they put just one on the ship to.
So they seem to have already, they seem to.
Part of the original motivation. Well, the motivation for this was to see if kids could explore this way, but we also wanted to compare this with various kinds of AI systems.
What we did was we actually this, we were actually giving the large language models a big advantage because instead of making them do the exploration we just as tax wrote in here's what you just saw.
And we took what the kids did and we just just text just typed it into the large language models.
As long as they're very bad at do.
They're not good at figuring this out. And we've had other studies now with lambda and other people have done studies with causal learning and causal learning really seems to be a weakness in my systems compared to something like grammar, for example, where they really do seem to be doing an incredibly
good job of learning and using this structure.
We also went to see whether various kinds of standard RL agents can do this.
And the answer is that they can but if you know anything about RL will know that as opposed to taking 20 trials to do with which is what the kids did. They did 720,000 trials.
Even, even an LSTM model that's one of the sort of state of the art models and reinforcement learning was still taking 140,000 iterations. And not only that but when we actually probed the models, not too surprisingly it turned out what they had done was get a look up to.
And the reason why it took them so long is because even with this extremely simple system. There's actually many, many different policies and combinations that you could have right many, many different things that you can do.
And what the this system had done was just figure out where for every conceivable combination of look at what would be the effect on the machine rather than figuring out what the job.
What we're trying to do now is to try to see if we could give RL agents more, whether we could give them some more information and train them in a different way in this intrinsically motivated way.
And would that make them more effective.
So in this experiment, what we did was we gave kids and agents a simplified form of Minecraft called crafter. And I must admit my major motivation for this was that I could go to my 12 year old grandson and actually tell him that I was doing a bunch of stuff about Minecraft, which was definitely
at least I'm a full quotient.
And again, and what we did was we just told them here's what each button does, but we didn't tell them anything about how the game and crafter and Minecraft in general is this very open ended game.
We are trying to find things that you can use to do accomplish various kinds of hands and there isn't a simple structure about how the game works. So it's really nice.
And what we discovered was, again, not too surprisingly, that both children and adults, we did this with adults as well, were much better at unlocking the information about the system, just from their own exploration than say a classic reinforcement
learning system.
Interesting. So there's a couple of interesting things one of the adults tend to be better than the kids, but there are some kids who are doing as well as the adults in terms of understanding how the, how the environment works there was more various.
If you took a kind of ground truth that gray one is a ground truth reward when you tell them exactly how the environment works and what the rewards are, and if you actually specify that in an oral system as well.
But if you compare that to say a random system that's just acting like a good old fashioned classic behaviors of reinforcement learning system that does terribly that can't figure out the structure of the environment.
But when you add some of these intrinsic rewards like information gain and novelty, where now the idea is you want to find out something that's new or you want to find out, get more information.
This is the kind of classic intrinsic reward of the system seem to be better than they do if you just have a random system, although they're still not doing as well as advocates and adults, but they're definitely doing better.
And then we actually went in and analyzed what the children and adults and agents were doing in terms of entropy this is the kind of novelty reward information gain and also empowerment.
So we analyzed what the kids were doing and said how much of an increase of empowerment are they get or do they have across across their, their play their exploration.
What we found was that there were significant correlations for the children and the adults and also for the agents between how much they were maximizing entropy information and especially empowerment, and how much they actually succeeded in exploring the space.
And in this case we didn't get the agents to use empowerment as an award that's what we're doing, literally as we speak right now, but we showed that the kinds of patterns of behavior that both the kids and the agents were producing.
When they seem to involve higher empowerment also allowed higher exploration is that 34.
Okay, so what that suggests is that the children might indeed be trying to do something like maximize empowerment in the course of their children and adults in the course of this of trying to deal with this very open-ended environment and also that that's working that that's helping them to get a higher level of exploration than they would have thought.
And as I say what we're doing now is trying to see if we can put empowerment as an intrinsic reward, is that going to lead to better reward and behavioral.
And let me just mention, since we're getting low on time here, another project that we're doing that's also trying to combine causal learning reinforcement learning, and this is a project looking at what's called curriculum.
So another thing that children do that's different from what Cossack RL systems do, is if they find something that's too hard for them to do, what they'll do is set themselves a task that's more feasible and that might give them skills that will enable them to be able to do part of tasks, that's what
And you can see the relationship of this to empowerment. So the idea is that you start out trying to find out what kinds of policies are going to enable you to do certain kinds of things, and then you can take that information and apply it to a more difficult task that you wouldn't have been able to do if you were just doing
random trial and error and policy and possibly a point in time.
So what we did, and of course, you know, I usually when I gesture like this, it's all these people right here I guess it is all these people right here but even more if you're sitting in a university classroom.
That's the whole idea of why we get the big box right is, is to enable this kind of curriculum.
And what we did was to take a different set of online environments, these are proctor and these are various kinds of video games. And what we did was start out giving kids and agents a level that they couldn't accomplish.
And then we tried to see what kind of what would they choose as their next level, would they just randomly choose another level now on the other level you got don't get any reward that you're told, you could try one of these other levels, because you can't get, and then go back.
And what we discovered was that the children were for quite systematically choosing the next level they were going to try, based on the progress that they had already made. So it looked as if they were designing their own curriculum to try and solve this task.
And when we get our L agents to do something similar I'm going to the details, their performance greatly improves they stabilize and they get to a solution.
Okay, so let me ask that.
One question you're kind of like, yes.
Yeah, that's an interesting question. I mean, I think, I think it's the, the main point is that's the one that you really want to look at.
Yeah, that's an interesting question. I mean, I think, I think it's the, the main point is that's the one that you really want to want to do, because the point about human intelligence is that we can slunk a human in any environment, and get them to figure
out how that environment works. So if you want to answer the question about how that's possible, then just saying, okay, here's what, you know, the adult mind as of 2023 and San Francisco looks like is not going to answer that more fundamental
question here. So I don't think it's an easier God knows it, it's, it, but it's the question that you will, it's more foundations, the foundation.
