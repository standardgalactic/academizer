Checking Science as Amateur Software Development (2023 edition
Professor Richard McElriss, an anthropologist and director of the Department of Human Behavior, Ecology and Culture at the Max Planck Institute for Evolutionary Anthropology, argued for the need to professionalize science, drawing analogies from software development and cooking.

He emphasized that science is a complex, integrative endeavor with empirical problems and the need for continuous integration and updating of scientific beliefs.

He criticized the chaotic nature of scientific progress and pointed out how other professions like software engineering have professionalized their processes to ensure quality control, testing, and collaboration.

Using the Stan Math Library as an example, McElriss highlighted that this project has a strong focus on continuous integration with teams of programmers making contributions and rigorous testing for quality assurance.

He contrasted this with the state of science, where there is a lack of professionalization in handling data, managing code, and ensuring reproducibility.

He emphasized the importance of adopting practices from professions like software engineering to improve scientific workflows, such as version control, testing, documentation, and collaboration.

McElriss also drew an analogy between programming and cooking, noting that both require planning, organization, attention to detail, and the ability to handle complex systems.

He encouraged scientists to learn from these professions and adopt professional practices to improve scientific research and ensure reproducibility.

Checking Statistical Rethinking 2023 - 01 - The Golem of Prague
This introduction to a statistics course for researchers emphasizes the importance of statistics as a tool for connecting data to scientific models and making scientific inferences.

The course focuses on causal inference, which is about understanding the relationship between causes and effects and making predictions or imputing missing observations based on interventions.

The course covers three main components: DAGs (directed acyclic graphs), Golems (computational models), and Owls (workflow for data analysis).

DAGs are used to represent causal relationships between variables, Golems are statistical models that process data, and Owls represent the workflow for designing, testing, and validating research projects.

The course has a Bayesian core and aims to provide a deeper understanding of the science behind the statistics, as well as focusing on the workflow of scientific research and testing the code developed throughout the course.

New topics in this edition include more examples of computing hypothetical interventions, sensitivity analysis, measurement error, missing data, and post-stratification.

The lectures are being actively revised for a third edition with drafts available to enrolled students.

Checking Statistical Rethinking 2023 - 02 - The Garden of Forking Data
In this lecture, we explored the concept of Bayesian estimation using a simple example of estimating the proportion of water on Earth based on tossing a globe.

The workflow included defining the generative model, specifying an S-demand (the proportion of water), designing an estimator using Bayesian inference, testing the estimator, and summarizing the results.

The generative model was defined as a causal relationship between variables - N (number of tosses), P (proportion of water), W (number of water observations), and L (number of land observations).

We learned that N influences W and L, while P influences W and L.

The garden of working data was introduced to visually understand these relationships and how they allow us to predict the consequences of interventions on variables.

Bayesian estimation involves counting all the ways each possible explanation (in this case, proportion of water) could produce the observed data under our generative model.

We learned that Bayesian analysis doesn't usually use raw counts but rather probabilities, which is exactly the same calculus used in Bayesian updating.

The estimator was developed using the posterior distribution, which is a function that links W and L to P and N, given our assumptions about the sampling process.

We learned that this function emerges from our scientific knowledge of the sampling process and serves as our estimator.

To summarize, Bayesian data analysis produces a Bayesian estimate by counting all the ways each possible explanation could produce the observed data under our generative model and then comparing those counts (or probabilities).

This approach is optimal within the small world of our model, which assumes our model is correct.

However, if our model is wrong, all bets are off.

Additionally, we briefly touched on misclassification problems, where true values are not observed correctly due to measurement errors or other factors.

We learned that Bayesian estimation can be adapted to handle such situations by modifying the generative model and developing an estimator accordingly.

Overall, this lecture provided a foundation for understanding how to develop Bayesian estimators using the AL workflow and introduced the concept of misclassification problems in Bayesian data analysis.

Checking Statistical Rethinking 2023 - 03 - Geocentric Models
In this lecture, we continued our exploration of Bayesian inference and statistical modeling using the example of planetary motion from ancient astronomy.

We discussed how the geocentric model was successful in making accurate predictions but failed to explain the true structure of the solar system.

The distinction between a statistical model and the causal model was emphasized, with linear regression being introduced as an example of a statistical model that can make excellent predictions but may not explain the underlying causal relationships.

We then delved into Gaussian distributions and their applications in statistics.

We learned about the origins of Gaussian distributions in natural processes, which arise from the summation of many small fluctuations, and their role as the error distribution in least squares estimation.

The lecture concluded with an introduction to linear regression as a Bayesian estimator, emphasizing its metaphorical geocentric nature and its inherent Gaussian assumptions.

We also touched upon the concept of causal models in the context of linear regression and the importance of understanding the distinction between the causal model and the statistical model.

Overall, this lecture provided a solid foundation for understanding the principles of Bayesian inference, statistical modeling, and their applications using linear regression as an example.

Checking Statistical Rethinking 2023 - 04 - Categories & Curves
In this lecture, the speaker introduced multiple estimators for a single generative model and their justifications based on the estimate in each case.

The speaker emphasized that linear models can handle both categorical variables and curves, even though neither of these things is linear.

Categories are discrete unordered types, such as shells or sex, which may be more familiar to us.

We will want to stratify by categories in our data because we will need to get at the proper estimates we'd like.

In the context of a linear model, this usually means fitting a separate regression line for each category.

The speaker also discussed the concept of indirect causes and how they don't show up in generative functions but can be addressed using counterfactual interventions or simulations from the estimated generative model.

The lecture concluded with an introduction to full luxury bases, where we express the whole generative model as a single statistical model that has multiple relationships and run simulations from it to get each estimate we require.

This approach requires fewer models to write but typically more simulations.

Overall, the lecture focused on the importance of understanding the relationship between categorical variables and continuous variables, the use of statistical models to make scientific estimates, and the concept of indirect causes in causal modeling.

The speaker emphasized the need for careful consideration when designing statistical models and highlighted the flexibility and power of linear models to handle complex relationships with appropriate design and post-processing.

Checking Statistical Rethinking 2023 - 05 - Elemental Confounds
In this lecture, the topic is threats to validity or confounding in statistical analysis.

The speaker uses the metaphor of a recipe to explain how estimations can go wrong due to various causes in the design or implementation of the recipe.

The focus is on the "four elemental confounds": fork, pipe, collider, and descendant.

1.

Fork: In this type of relationship, there are three variables (x, y, z).

Z is a common cause shared by x and y, resulting in an association between x and y when observed as a whole.

However, if you stratify the data by z, the association between x and y disappears within each level.

2.

Pipe: In this case, there are three variables (x, y, z).

X causes z, and z causes y.

If you stratify by z, there will be no association found between x and y because z acts as a mediator or filter for the relationship between x and y.

3.

Collider: In this situation, x and y both influence z, but they do not share a common cause.

When you stratify by z, you can observe a strong correlation within each level of z.

The important thing to note is that the absence of an association in the total sample does not necessarily mean there's no causal relationship between x and y.

4.

Descendant: A descendant variable is influenced by one or more variables, and when you condition on it, you are essentially conditioning on its parent(s).

This can lead to reduced or blocked associations between the original variables of interest.

The speaker emphasizes the importance of understanding these confounds to develop accurate statistical estimations and avoid misleading conclusions.

In the next lecture, more complex scenarios involving multiple confounds will be discussed.

Checking Statistical Rethinking 2023 - 06 - Good & Bad Controls
In this lecture, the focus is on using logic to derive implications of causal models.

The lecturer explains how to use the elemental confounds, such as fork, pipe, and collider, to make deductions about larger causal diagrams and generative models.

The lecturer emphasizes the importance of avoiding being clever in scientific research and instead focusing on reliable, transparent, and logical procedures that communicate assumptions, deduce their implications, and expose them to critique.

The lecturer also discusses the problem of confounds in observational studies, where there is a shared cause between the treatment (X) and the outcome (Y), which makes it difficult to estimate the causal influence of X on Y.

The classical solution to this problem is randomization, but when this is not feasible, statistical procedures can be used to mimic the effect of randomization.

The lecturer introduces the concept of "doing x" as a way to represent intervening on x, and explains how it relates to causal effects and adjustment sets.

The lecture includes an example of a conservation biology study involving cheetahs, baboons, and gazelles, and discusses how due calculus can be used to deduce the statistical implications of this system and justify graphical methods of analysis.

The lecturer also mentions the work of Uda Pearl and encourages listeners to read more about due calculus and causal inference in general.

Checking Statistical Rethinking 2023 - 07 - Fitting Over & Under
In this lecture, we discussed the challenges of estimation after having an estimator in place and focused on the struggles of causation and data.

We used the example of retrograde motion in astronomy to explain how Copernicus's heliocentric model required fewer circles than the geocentric model but still used epicycles.

The struggle against causation is about using causal assumptions to design estimators and contrasting alternative models, while the struggle against data is about making the estimators work.

We introduced different types of prediction tasks such as curve fitting, causal inference, intervention prediction, and typical prediction.

We also discussed lead one out cross validation as a method for assessing predictive accuracy and compared the performance of various functions like linear regression and polynomial regression using this method.

The importance of simplicity in modeling was discussed, but it was pointed out that in most realistic modeling contexts we are trading off simplicity against accuracy.

We also explored the concept of overfitting and regularization through priors, which help make models less flexible and improve their performance on new data.

Finally, we mentioned some useful metrics for estimating expected out-of-sample accuracy, such as Importance Sampling Approximation of Cross Validation (PSIS) and the Watanabe-Akaike information criterion (WAIC).

Checking Statistical Rethinking 2023 - 08 - Markov Chain Monte Carlo
In this lecture, the fine folks at the Bristol Science Center were used as an analogy to explain how randomness is exploited in science to do advanced calculations, specifically for computing posterior distributions in more complex scientific models.

The example given was of assessing student knowledge based on test scores and the hidden variables influencing the scoring, such as test difficulty or student effort.

Markov Chain Monte Carlo (MCMC) algorithms were introduced as a practical way to calculate these more complicated and involved posterior distributions.

The lecture began by discussing the historical background of randomness in science and how it has been used since the early 20th century.

The example of assessing student knowledge was then presented, highlighting the importance of addressing the uncertainty present in estimates and the influence of confounding variables.

The lecture explained that MCMC algorithms are a powerful tool for solving these types of problems statistically, but require some new machinery in the form of Markov chains.

The Metropolis Algorithm was then introduced as the oldest and simplest MCMC algorithm, using the analogy of King Markov visiting islands in proportion to their population sizes.

The importance of this algorithm in scientific computing was emphasized, with its use extending beyond just drawing samples from posterior distributions to taking integrals and doing calculus without having to do it explicitly.

The lecture concluded by discussing some important measurement problems in science that have the same basic structure as the student knowledge assessment example, and how MCMC algorithms can be used to infer the underlying latent variables in these cases.

The development of MCMC algorithms was also traced back to its origins in the 1950s and the contributions of Ariana Rosenbluth in making the mathematical theory work on a computer.

The importance of autodiff libraries, such as STAN, for automating differentiation and finding gradients was also mentioned.

Overall, this lecture introduced the concept of MCMC algorithms as a practical tool for computing posterior distributions in more complex scientific models and highlighted their historical significance and importance in modern scientific computing.

Checking Statistical Rethinking 2023 - 09 - Modeling Events
In this lecture, we started the second half of the course, focusing on statistical modeling and scientific prediction using nonlinear statistical models.

The professor introduced Generalized Linear Models (GLMs) as a powerful tool for modeling events with discrete outcomes, such as admissions to universities or disease diagnoses.

He also discussed the importance of understanding causal relationships in data analysis, especially when dealing with complex systems like social dynamics or biological processes.

The lecture focused on a historical dataset from UC Berkeley regarding college applications in the late 1960s and early 1970s.

The goal was to estimate discrimination in admissions based on gender.

The professor explained that this problem is an example of mediation analysis, where department mediates the relationship between gender and admission rates.

He emphasized that understanding the causal relationships in data analysis is crucial for addressing complex societal issues like equity and justice.

The lecture covered the basics of logistic regression, a type of GLM used to model binary outcomes.

The professor discussed how to estimate total and direct effects of covariates on the outcome using logistic regression and how to interpret the results in the context of mediation analysis.

He also introduced the concept of causal DAGs and explained their role in understanding complex relationships between variables.

Throughout the lecture, the professor encouraged students to keep flowing forward despite the challenges they might face in their scientific practice.

He emphasized the importance of embodied knowledge gained through practice and experience, rather than relying solely on theoretical understanding.

He also acknowledged that real research often presents more complex issues than what is covered in a course like this but encouraged students to use the skills they learn here as a foundation for further study.

Checking Statistical Rethinking 2023 - 10 - Counts & Hidden Confounds
In lecture 10 of Statistical Rethinking 2023, the focus is on confounding variables and sensitivity analysis in statistical modeling.

The lecturer reminds students that most scientific research does not yield desired results and emphasizes the importance of documenting procedures and justifying assumptions.

The topic of confounding variables arises from the UC Berkeley college admissions dataset, where there are unobserved common causes between choice of department and admission probability.

The lecturer simulates this relationship using a generative model and demonstrates how to estimate the total causal effect of gender, direct effect, and indirect effect using statistical models.

The lecturer also discusses sensitivity analysis as a method to understand the implications of unmeasured confounders and examines two papers on gender bias in scientific organizations that reached contrasting conclusions due to different causal assumptions.

The lecture concludes with an emphasis on making transparent assumptions when conducting research, especially in human sciences where data may be limited.

Checking Statistical Rethinking 2023 - 11 - Ordered Categories
In this lecture, the instructor discusses the importance of understanding ethical dilemmas and solving unknown problems in statistical modeling.

The instructor uses George Polya's problem-solving heuristics as an analogy for approaching complex statistical modeling issues.

The first step is to understand the problem, then come up with a plan, carry out the plan, and finally check the work.

The instructor then introduces the trolley problem, a famous ethical dilemma in philosophy, and discusses how experimental trolley problems are used in psychology to study intuitions about moral permissibility.

Three main principles that influence people's responses to these scenarios are action principle, intention, and contact principle.

When faced with complex statistical modeling issues, the instructor emphasizes the importance of being open-minded and adaptable, as some problems may not have straightforward solutions.

Lateral moves in research might include using different study designs or collecting new types of data.

The instructor also highlights the challenges of studying ethical decision making in real life since it often involves communities rather than individuals, but the psychology of individuals responding to ethical dilemmas is an interesting area of study in its own right.

Finally, the instructor discusses how to model variables like the ordered categorical variable from the trolley problem experiment, which has ordered categories but unknown distances between them and anchor points.

The solution involves converting the probability distribution into a cumulative distribution and building log odds parameters using cut points.

Checking Statistical Rethinking 2023 - 12 - Multilevel Models
In this lecture, the instructor introduces multi-level models and their advantages over traditional statistical models.

Multi-level models are also known as hierarchical models or mixed effects models, and they allow for modeling of data that is clustered within groups or units.

The key advantage of multi-level models is that they enable transfer of information from one group to another, resulting in faster learning and reduced overfitting.

The instructor uses the example of Starbucks coffee shops in Europe to explain how a robot designed to learn optimal waiting times at different cafes could benefit from memory.

By remembering what it has learned about previous cafes, the robot can make more informed estimates about new cafes it encounters.

This is achieved by modeling the population of cafes and their average waiting times, as well as the variation among cafes.

The instructor then explains how multi-level models are used to analyze data with unobserved variables that may vary between groups.

In such cases, fixed effects models cannot be used as they cannot include group level predictors, leading to confounding.

Multi-level models, on the other hand, allow for partial pooling and can deconfound the effect of interest while still allowing for estimation of group level tendencies.

The instructor also discusses some challenges in implementing multi-level models, such as dealing with large datasets and complex modeling techniques.

He encourages students to explore the literature on this topic for more detailed understanding.

Checking Statistical Rethinking 2023 - 13 - Multilevel Adventures
In this lecture, the instructor discussed the concept of varying effects in statistical modeling, which allows for estimating unmeasured confounds by utilizing repeat observations within clusters.

The instructor also introduced the distinction between clusters and features in multi-level models, highlighting the importance of understanding how to program these different things into models.

The instructor provided examples from previous lectures on education, trolley problems, and political science, emphasizing the need for varying effects both from a predictive and causal perspective.

The instructor also introduced the concept of fixed effects as an alternative approach but argued that varying effects are more practical in realistic research with finite sample sizes.

The lecture concluded by discussing the challenges involved in building complex models, such as dealing with group-level confounding, sampling efficiently, and understanding the underlying assumptions.

The instructor encouraged listeners to keep their focus on getting the story straight before worrying about specific estimators.

Checking Statistical Rethinking 2023 - 14 - Correlated Features
In this lecture, the focus was on building golems that can learn correlations among features and exploit them for partial pooling across features, especially when some features are under sampled relative to others.

The concept of multivariate priors was introduced, where all features for a cluster go into the same joint prior.

The advantages of this approach include faster learning and more accurate estimates, particularly in complex models with multiple correlated features.

The lecture also covered the importance of using reliable methods in research and avoiding divergent transitions in Markov chain Monte Carlo simulations.

The next lecture will cover social network modeling and the use of non-centered priors.

Checking Statistical Rethinking 2023 - 15 - Social Networks
In this lecture, we delve into the analysis of social networks and cooperation among the Native American community in Nicaragua named Arundhak.

The data collected consists of food transfers among 25 households over a year, amounting to 300 dyads and 2,871 observed exchanges.

The goal is to understand how much sharing can be explained by reciprocity and generalized giving.

The traditional approach of analyzing this data, which involves creating scatterplots to observe correlations between dyads, has proven ineffective due to the complex nature of social networks.

Instead, we'll employ multi-level modeling tools to analyze the data.

We will be particularly interested in inferring ties and how they influence transfers while dealing with confounding factors like household features.

The lecture begins by discussing the conceptual framework for analyzing social network data using a directed graph and identifying the influence of household features and social ties on gift-giving behavior.

The importance of understanding reciprocity and generalized giving is emphasized, as these concepts are fundamental to human societies and behavioral sciences.

We will then focus on simulating a social network by generating friendships and directed ties using probabilities.

After that, we discuss the statistical model for analyzing the sample data while considering confounding factors like household wealth and other features.

The lecture concludes with the importance of understanding social networks and their role in explaining human cooperation and mutual aid.

Checking Statistical Rethinking 2023 - 16 - Gaussian Processes
In this lecture, the focus is on studying processes at large scales using normal distributions and dealing with spatial confounding in the Oceanic Technology dataset from the rethinking package.

The model developed in the first part of the lecture ignores population for now and models the spatial covariation among islands as a function of their distance from one another.

This is achieved by using a multivariate normal prior for the intercepts, with a covariance matrix that lets us smuggle distance into the model.

The idea behind this is Gaussian processes, which is an infinite dimensional generalization of multivariate normal distributions.

It allows us to put predictors inside the covariance matrix and have fewer parameters while regularizing varying effects.

In the second half of the lecture, the topic shifts to phylogenetic regression, where we deal with unobserved confounds that influence multiple traits over time.

The goal is to infer patterns of co-variation at the tips, which can be used as a proxy for unobserved confounds.

The lecture concludes by discussing the importance and wide application of Gaussian processes in prediction and causal inference.

Checking Statistical Rethinking 2023 - 17 - Measurement & Misclassification
In this lecture, the topic of mismeasurement and misclassification in statistics was discussed.

Real-world data analyses often involve some degree of measurement error or misclassification.

The solution to these problems involves using the machinery of probability theory, which hasn't been extensively used until now in the course.

The lecture began with a discussion on the Shrove Tuesday tradition of burning palm fronds and pancakes, and the explanation for these rituals was emphasized as their function rather than their truth.

The concept of probability theory was then introduced through the problem of determining the probability that the other side of a pancake is burnt given that the upside is burnt.

This problem served to illustrate the importance of suppressing intuition and relying on the rules of probability theory to solve logical problems.

The lecture then transitioned to discussing measurement error, specifically in relation to estimating the proportion of children fathered by extra pair men among the Himba people.

The presence of measurement error necessitates accounting for misclassification errors, as there is a 5% false positive rate in assigning extra paternity.

The solution involved expressing the problem as a probabilistic graphical model and using Bayes' theorem to calculate the posterior distribution of the probability that a child has an extra pair of paternity given the observed data and the true values of the unobserved variables.

The use of logarithms was also discussed as a way to maintain numerical stability when dealing with small probabilities in calculations.

The lecture concluded with a brief mention of other applications of these concepts, such as in rating and assessment tasks, hurdle models for detecting substances or species, and occupancy models in ecology.

The importance of being aware of the limitations of measurement and accounting for errors was emphasized.

Checking Statistical Rethinking 2023 - 18 - Missing Data
In this lecture, the concept of missing data was introduced as a common occurrence in real research.

The speaker emphasized that observed data is a special case and missing data can be approached through probabilistic modeling using a generative model.

The speaker used the example of students not turning in their homework to illustrate different scenarios for dealing with missing data, depending on whether the cause of the missingness is random or influenced by other variables.

The speaker also discussed the potential biases that can arise when analyzing only observed samples and emphasized the importance of understanding the causal model and assuming correctly when dealing with missing data.

The lecture covered different methods for handling missing data, including simply dropping cases with missing values, imputation using Bayesian methods, and marginalization.

The speaker provided an example using the primate family tree dataset and demonstrated how to implement Bayesian imputation using Gaussian processes in R.

The lecture concluded by discussing the importance of considering the assumptions underlying any analysis and the role of generative models in handling missing data.

Checking Statistical Rethinking 2023 - 19 - Generalized Linear Madness
In this lecture, the scientific foundation of statistical modeling was explored with a focus on human height as an example.

The speaker discussed how linear models, including generalized linear models and generalized linear mixed models, have fundamental limitations because they are based on additive combinations of variables.

These models can give up some statistical power and opportunities to use scientific information to improve the analysis.

To address this issue, a more scientific approach was introduced using human height as an example.

The speaker modeled the relationship between body weight and stature by considering the physical properties of the human body.

He used the concept that the weight of a human is proportional to the volume of that human, which arises from the shape and structure of the human body.

The scientific model was then translated into a statistical model with a prior distribution for the observations, expressing the expected weight based on height, assigning priors for unknowns, and thinking about measurement scales.

The speaker emphasized the importance of understanding units and measurement scales and suggested getting rid of them by working with dimensionless quantities.

The lecture concluded with a discussion on state-based models, specifically in the context of population dynamics.

An example of a predator-prey relationship was given to illustrate reciprocal causation between different species.

The speaker emphasized that these models are useful in various fields and can help understand complex systems better.

Checking Statistical Rethinking 2023 - 20 - Horoscopes
In this lecture, the speaker discusses the importance of subjective responsibilities in scientific research and compares these responsibilities to horoscopes and other fortune-telling techniques.

The speaker argues that just as horoscopes have logic and produce seemingly correct advice due to their vagueness, statistics also rely on vague inputs to produce outputs.

However, unlike horoscopes, statistical analysis has the potential to be scientifically powerful when based on valid theories and models.

The lecture then focuses on three types of subjective responsibilities in scientific research: planning, doing the work, and reporting.

The speaker emphasizes the importance of having clear goals, making informed assumptions, considering ethical implications, documenting research processes, and sharing findings transparently.

The lecture also touches upon the potential for bias and distortion in the communication and dissemination of scientific research.

Throughout the lecture, the speaker encourages listeners to develop their subjective expertise while remaining aware of the limitations and responsibilities inherent in scientific work.

The lecture serves as a reminder that despite the objective nature of scientific data analysis, the subjective aspects of research play a crucial role in ensuring the validity and reliability of scientific findings.

Summary by Mistral

Statistical Rethinking 2023 - Richard McElreath
https://www.youtube.com/playlist?list=PLDcUM9US4XdPz-KxHM4XHt7uUVGWWVSus
