I think we'll get started now if everyone's ready.
Cool.
All right.
Thank you so much for coming to the inaugural bio and all seminar to kick off the spring
semester.
Really excited for the lineup we have this semester we're going to have people like Sam
Rodriguez.
We're going to have Tom Kahlo.
We're going to have Regie Doss in Stanford and a couple of other people in the works.
They had introduced our speaker, our first speaker of the semester is Sam Rodriguez,
who is CEO of Futurehouse, a non-academic research institute, developing semi-autonomous
AI and service of scientific research.
Futurehouse's recent work includes WikiPro, which is a retrieval augmented generation
agent for querying and summarizing scientific literature, and BioClaner, which is an automated
framework for evaluating scientific protocols.
Apart from his work at Futurehouse, Sam is the founder and PI of the applied biotech lab,
an academic research group at the Francis Crick Institute in London.
Some of Sam's notable inventions include a new nano fabrication method, and a technique
for neural activity sensing, we have bloodstream probes, and a method for protein sequencing,
and a technique for mapping brain connections.
So give her a round of applause for Sam Rodriguez.
Thank you.
Oh, yeah.
And then send this to your shirt if you want.
Oh, that's good.
I'm just going to leave that because I don't know how to bring that thing back after I put it away.
So guys, so first of all, thanks for having me.
Super excited to tell you guys about what we're doing at Futurehouse.
I thought I would editorialize for a little bit at the beginning on two points.
The first point is, I heard that this seminar is funded by or sponsored by a pillar.
Tony Calesa is a really good friend of mine who was, I don't know if he's a partner, if he's not a partner yet, he'll be a partner very soon, I think.
Anyway, the point is, he's at, he's an investor at Power, and I think the thing, he's also like a founding member of the board of directors at Futurehouse.
I just something to say on that is that like, for people who are thinking of starting their own companies.
When I was in your position, you know, the way I would think about it is like, oh, wow, like, if you have an idea for a company, you have to go out and you have to find some investors who really want to invest in you.
Well, it turns out that like, the best companies get to choose which investors invest in them, right, because they're like the top 1% of the companies that all the investors want to be powered on to, and then you get to pick and choose.
And when you're thinking of picking and choosing, you should be picking on the basis of, like, which of those investors are going to add value, because it turns out like everyone has money.
Money is like a really is like, you know, pretty abundant commodity.
But like some investors will let you take it to the next game, or take the game to like the next level.
And like Tony is a VC that you should take money from.
Like if I had to take money from like any one VC, it would be Tony.
Okay, like he's just like, and as you guys get to know him, I hope you all get to know him.
Like the value add that he provides for people who are starting new anything is just like insane.
Remember, future has is a nonprofit.
Right. And still, like Tony is just like available for a call, like whenever, whenever you want, like I guarantee you I could like pick up the phone right now and call him and like asking to help with something and he would like be there right and he makes no money.
So okay, so that's Tony. He's awesome. Strongly recommend.
The second thing is I'm going to tell you guys that future has future has to say a like film profit.
We funded like nonprofit research organization.
And I'm going to tell you about our mission and everything I wanted to say a little bit just like right at the very beginning about like this idea of professional research environments.
So like also when I was sitting, you know, in your position and I was like, going through my PhD.
I was inspired to tell the story by a conversation that was happening over here about, you know, people deciding whether to go grad school or not. Right. Like, I like many of you probably like really wanted to be doing research.
Or I should say like when I was in grad school, I like still like really want to just looking forward, you know, after I was done with grad school and like really want to be doing research and thought it was insane that like,
certainly in bioengineering, there was basically like nowhere where you could go where your job was going to be to do research, except in a university and if you're in a university you had to be a trainee.
So like, you know, you're like undergrads or obviously, you know, students graduate students are still are still basically students get treated like students and even postdocs they're like hard postdoctoral trainees right like, you know, the point is that like, you know,
the idea is that this is like an itinerant position that's not like your career is to do research you just get to sit there. And then one day you become a professor, which means finally your job is to do research.
And you do all day, you sit in your office and write grants. Right. So, I just thought that that was insane. And it like led me to really wish like hey shouldn't there be a place where you can go where like professionally you'll be able to do research.
And that led to this idea of these focus research organizations, which is this idea that out of the marble stone I came up with at the end of my PhD, which are basically like these philanthropically funded nonprofit startups for science for projects that like don't fit in
academia because they require a team that's larger than what you can do in academia, but also can't be done effectively for profit and like they actually provide a pathway for people to like kind of do research.
As a career professionally, and the way that they will become like this, so there are several FROs now, most of them are funded through convergent research, which is something that Adam Rawlsstone runs.
Future S is not part of convergent, but is also basically an FRO.
And I, you know, what needs to happen in order for them to become like a mainstream part of the scientific research ecosystem is for the government to start funding them, which it will do once we have like really good proofs of concept that the system works.
And I just really hope that that happens because I'm a very strong advocate for that happening, because I think that like in biology certainly there are very few, maybe not no places where you can go and like be an individual contributor and like do research professionally in machine
learning. There have been, and now those places still are like becoming are becoming more commercial because the commercial applications are being realized. And so I think this is an important part of the ecosystem that's missing.
The ironic part is that that was what I wanted, you know, to find a place where my job could be to do research and I created such a place and now what do I do. I sit in my office and I, you know, I don't write grants, but I like run the organization.
You know, but such as the life as a, as a founder.
So, good. So basically now to trash. So thank you guys for coming to listen. So,
let me see if I can make this great.
So, the past 10 years have seen like really incredible advances in the tooling that we have for biology research.
So maybe to put a little bit more context on this. I got I was a theoretical business originally I did my like, early research on like quantum information theory.
And I got into biology because it seemed like there were going to be more unsolved problems there. And I got to be around like 2013.
And it was like immediately apparent that biology was a very strongly data limited like we just did not have the tools that we need in order to like ask any of the questions that we wanted to ask.
But in the past 10 years, there's been this explosion in like the, in like the different kinds of tools that allow us to gather data and test hypotheses and biology.
So things like massively parallel, which allows you to basically like.
Well, as it says, synthesize 100,000 of all goes out of time where each all go can represent a different kind of experiment can represent different protein that you want to pass a different like enhancer or whatever.
Right.
Droplet based single cell sequencing so that you can like now sequence actually like come to the thousands of cells at a time where again each cell can be a different experiment because each cell can like encoded with protein.
Obviously next generation sequencing. And so the result of all these things is that like, we're, we actually are now like strongly bottlenecked in many areas by like human bandwidth.
In, in interpreting the data that we get. Right. So I think the best example of this is that the GTX consortium. This was one of my favorite ones.
So back in March, like last March, the GTX consortium, which is a big consortium road, release their new data set and publish it as 15 papers in science.
And the Allen, the Allen Institute of not just the Allen Institute, there's a consortium like brain cell Atlas consortium in December that when they release their data set they publish like 10 papers in nature.
And the point at which you'd like you release a data set and your initial data dump is like 10 papers in nature. You know that there's like, you're like limited now by literally your ability to just like go and get those insights and publish those papers like there's a lot more data in those data sets that
we're not a lot more insight to be gained from those assets that we're not extracted. And so our mission at future house is basically to build an AI scientists in order to scale up discovery research in biology and so the important thing is that
discovery research. So we're not like the goal is like not to make a truck or like maybe the long term goal is to make a drug, right, or to make drugs like make lots of drugs. But like, you know, the point is that my pieces at the moment is that we don't
know how the diseases work. And so it's like garbage in garbage out if you are doing, if you have like a new way to like make a better molecule, but you're going after the wrong target, your chance of success is going to be like exactly the same as everyone else's.
We want to understand actually how biology works we want to be creating mechanistic models what's up. Yes, that we're recording because they want to be the double check. Are we recording.
You're recording. Okay, good. There you go. All right, show goes on. Awesome. Great. So thank you.
Okay, and so our, and so the way that we're going about this is basically thinking about how do we automate like, how do we build automated systems that are going to be able to extract a now that are going to be able to extract insights from the data sets that will be able to figure
out where the gaps are come up with like new experiments they want to run. And ultimately, I had to interpret those experiments also and so these are some of the areas that we're thinking about like literature search kind of hypothesis generation analysis basically like as a way of getting a
foothold on the problem thinking about one of the various steps of the scientific process that we go through. And then how do we how do we figure out how to automate and paralyze them.
And so our, we have this like challenge book that kind of describe the set of problems that we would be that we would like to be able to solve once we once you know we've been a scientist and examples are right right like you know things like
the examples are generally problems that are kind of like too big for humans in some way, right so like generating a company has a network of all known or proposed like protein protein interactions for the human proteome, like, you know,
figuring out how to go and like create like a table that shows you all the DNA binding sites for just that you know, right.
The problems. These kinds of problems are generally of the form, like for all x, do why, where why is something that a human can do and when you add the like adding the qualifier for all x makes it both like way more impactful and like completely
like if you want to understand like the protein protein interactions for a single protein that's like pretty doable, you're going to look for the whole proteome and it becomes very hard.
And, and so why is this interesting, this interesting for two reasons. The first one is that actually like most of biology is too big for humans, right so they're 20,000 feet, you know, they're kind of thousands of different cell types, you know, I have no idea how many metabolites
there are.
Then they're like, you know, hundreds of millions or more organisms, I have like a kind of fringe hypothesis that like, because I was in I did my teaching at Boyden's lab and anyone who has met Ed like his whole thing is we have to solve the brain.
Right, like, I have a fringe hypothesis that the brain is actually already solved.
Like, we don't know it, because like no one has the time to go and like get like, like assemble the model from like what is already known in the literature. Right. Like, that's not, not exactly serious, but like the point is that if we, if like the information
in the literature to like solve the brain, however that means, we would not know, right, like no one has the time to go and and and like read all the literature process all that information come up with the model.
Okay, so that's reason one reason or two is that actually I think like science is like the perfect playground to develop a GI, whatever you want a GI to mean for you.
You know, it's maximally open ended. Right. It uses like, somehow it uses like most of the higher order cognitive functions that humans have like lots of like, and kind of in their most extreme form.
Right. Like, you know, motivational, like the kind of motivational reasoning or like how to figure out like how do humans go about creating the reward functions that we that we pursue.
Like, it's a really, it becomes a really interesting question in the absence of like any kind of objective reward. It's a really interesting question in the context of science and it's like you start your PhD, and you have like some hypothesis about like what will get you what what
conscious reward, maybe you're like, Oh, what I want is like a nature paper. And then you have some hypothesis about like how to go about writing that nature people you don't really know and like, you know, you have to you're integrating information like the super sparse information signals.
Super sparse information to kind of refine that board function gets like you, it ends up being this environment we think where you can.
Yeah, you got a lot of these cognitive functions in kind of most extreme form is pretty interesting. It reveals a lot of interesting problem that seem like they're on their way to a GI so interesting for those reasons.
The key to access is why the top got added in the three minutes before the party game by, you know, it's not about the conversation. We do research and we pay money, which is a different achievement.
So independent nonprofit.
So based in San Francisco in the dog patch, funded by Eric Schmidt, although that's no longer exclusively funded by our Schmidt we are, we now have other funders although we'll probably we'll make an announcement about that at some point but our records are like main.
We, that's sort of in August or we got there in September we announced in November.
And I think that one of the cool things is that we actually have a web lab also. So,
one of the inspiration behind our future has to find out a place where you have researchers like engineers and like what not biologists all in the same place that you get this really tight feedback loop.
And that was really essential. I'll tell you guys about wiki crowd that was really essential in wiki crowd and like we're focused on actually like inventing new techniques and like coming up with new like new fundamental breakthroughs, but we pay industry competitive
salaries. So, um, and we're currently eat people, but we'll be 11 on Monday.
This day Monday. No, yes, today is Monday a week from a week from today will be will be 11 so we're growing fast.
Okay, and this is the leadership team so I'm the director and CEO.
Actually, I am now everyone at future so future super flat organization. And so everyone is either a member of technical staff or the head of something.
And it was very awkward for a while that my title was director and CEO. So it just got revised I'm now head of future house.
And then Andrew White was a professor at the University of Rochester is now head of science.
And Andrew has built kind of like scalable and all systems at a bunch of different, a bunch of different companies now head of engineering.
And we may be expanding our leadership team as well.
So, these are our three focus areas and I'm going to tell you now about the literature search topic. This was like our first technical result that we came out with, which was wiki trial, which was in early December.
And the literature is basically like I was saying before I mean how we go about automating synthesis of human scientific knowledge. Right. I mean the scientific knowledge is like extremely dispersed.
And, and the literature is way too fast for anyone person to understand this problem became like, like very obviously much more tractable with like the dramatic improvements that we have seen in language models.
You know, now is obviously the right time to do it. And so, wiki pro in order to talk about wiki pro we're going to start with paper qa so paper qa is just is like, it's basically the algorithm that is that that underlies wiki pro.
And the basic idea behind paper qa is to be able to take a very technical question.
And, like this question on the left about brasses forms, and be able to provide a cited, you know, technical and accurate answer.
I'm just going to on that point about accurate, because I'm going to say this a few times but if there's any one thing that you guys should take away from this. It is that like 90% of the problem is being able to measure performance.
And we're like working on these problems like building as scientists involves like these like very nebulous like things like hypothesis generation, blah, blah, and like, you can't get any traction unless you can measure performance and so, you know, just spoiler alert.
One of the things where that is cool is that we actually get like a real world objective measure of the performance of the systems, which is the only like kind of real world objective measure of the performance of system like this that I have seen.
But so yes, so anyway the goal is to be able to return an accurate summary of the result the answer this question. And so this is the general flow.
This is a rag agent rag is retrieval augmented generation so you know, pretty big topic now basically just to be able to find in order to answer a question be able to go and find relevant resources and return them and so the and then answer a question that's informed by them.
And so the, the basic flow is that you have a question, like this question here about double cordon.
You go and you find relevant papers, you then find chunks, relevant chunks within the paper, and you basically ask you like chunk the paper and you ask for you chunk whether the chunk is relevant.
And then, once you have the relevant chunks you go and you generate a you go and you generate like a final context by combining the relevant chunks and then you provide an answer.
Now the thing that is cool about this is a agent which means that at every point you get to look and see whether it feels like it has enough evidence to answer the question and if it does not have enough evidence to answer the question then it can go and retry.
So it has like the agent has access to these three tools, which is that it can search for papers, it can gather evidence and then it can finally try to answer the question.
So gather evidence means look at the papers and look at the papers and look at the relevant.
Sometimes it does a search and it turns out that the papers that it comes up with some papers but it's only able to access the PDFs for some fraction of them, so that it might search again and then it can gather the evidence and can look and decide whether it has enough evidence and so on.
And so the first thing that we did after we built this was that we wanted to go and measure its performance on like the standard benchmarks.
Right. And so the first benchmark that everyone uses is the US medical licensing exam.
It's on par with, you know, PV4, I mean that is like not super, it's not super interesting, the US medical licensing exam, it's just like very much textbook knowledge.
The more interesting one is like PubMed QA, so PubMed QA is a benchmark that is basically built by answering questions that are contained in the abstracts of papers.
And the standard version of the benchmark has the context in the question, so you like provide it with the abstract or snippet of the abstract.
And so we did like a closed book version of PubMed QA, which is basically you just like don't provide it with the snippet. And then obviously, like now it would be very surprising if this were not the case.
And then the paper QA does much better because what it can do is it can go and find the relevant paper and then you can get the answer.
These like are slightly unsatisfying, these were like still like slightly unsatisfying as benchmarks.
This is just, I mean this is more funny than anything else. I think, before I tell you about the unsus factoriness of the benchmarks, one of the cool things about RAG, about RAG agents is that they like the RAG that really suppresses hallucination.
And that's why I'm asking a lot of trouble for reviewers, because like we over 260s or 237 like evaluations we did not like observe any hallucinations.
It's just like very well grounded and this took some prompt engineering also but it was like, very, very well grounded in the, like in the sources, which is great but then the problem is like no one believes it.
So, anyway, issues with the benchmarks. The first one is like there's definitely a huge communication issue of the benchmarks for old.
They are only from abstracts, abstracts are very diverse compared to full test papers.
They don't require any kind of synthesis, right.
And in more general, it's like not it's a synthetic task and so it doesn't like you know the, these benchmarks like they don't reflect kind of like any real world kind of thing that you want to do.
And so we'll talk about the first three problems and then we'll talk the synthetic task part we'll talk about, talk about later. But so the first thing that we had to do was we had to come up with a more challenging benchmark.
One that would be better able to distinguish between the performance of some of like a RAG agent and the performance of like a beast kind of model.
And so that meant that it had to, we need a task that could like provably like required retrieval.
Right, which means that we wanted to have questions from papers that were basically like after the cutoff of the models, where you were pretty sure that the information in the paper was not findable anywhere except for in that paper.
Or ideally in a couple, you know, in like maybe you had to synthesize it across a couple of papers. This is really hard. Like the papers are real, like these questions are really hard to generate because proving to yourself that there's a fact that is like actually novel for that paper that has not been reported before.
People don't tend to tell you when they publish something that has actually been reported before like they don't contact that. And so this becomes like really, really challenging.
But so these are just some examples of what these companies look like. You know, has anyone performed a base editing screen against play sites in CD33 before.
You know, you can find the people where people have, and you're like pretty sure and then you commit yourself that that is in fact a first time and you've done it.
And you can obviously come up with a lot of questions like that where you change CD33 to CD32, and no one has done that. And so you can generate questions that way.
But then you get to ones that are kind of like bizarrely specific, like this one about like RNAs, which like RNA is not showing increase in MMO macrophages among simulations of loss.
So you find a paper about like RNAs in macrophages and etc.
So this is the kind of benchmark. And so the attributes of this benchmark, there are 50 questions.
There are multiple choice. They require a full text search.
They are after QPT for cloud to cutoffs, which also means that this benchmark if you're going to use it needs constant upkeep sucks, but you know, don't have a better solution at the moment, right, because the cutoffs keep moving up.
And it's focused on biology.
And this is just like manually curated.
5050 is really small ball. This was kind of for, you know, I mean, this was mostly this gets to the point about like one in real world tasks.
It's like mostly for internal use. So we never, we haven't scaled it. We didn't scale it up previously. We are now scaling it up, but.
And so this is the performance.
We were pretty excited about this.
So the humans to do it and when the humans do it, you just like give them a set amount of time. Right. And you let them and we chose the amount of time arbitrarily I think it was like five minutes or something like that.
So random accuracy is 2025.7 the humans are 87.9 and paper Q is also 87.9. So that is, so that was pretty exciting. This is a human level recall for these questions.
I want to point out here.
Perplexity, the copilot person for people who don't use it, the copilot person for like he's very, very cool.
I actually like, like, there are several of these kind of commercial systems. None of them will these performance metrics, but like illicit site, perplexity is like clearly the best.
It's like become an obvious that perplexity is like pulling away at this kind of a task. So really strongly recommend it. It's great. We use perplexity. We use perplexity internal or your bench.
Yeah, so this is not that interesting. Okay, so I keep going on about this real world task. So this is what it was. So after we came up with the system, we're like, okay, what can we actually do with this?
And so this is wiki pro, you know, previously at the beginning, so there's 20,000 genes in the human genome.
At the beginning of December, there were Wikipedia pages for 3,639 of them. And so we said, okay, like, you know, we've done all this engineering work to create a system that we can like run at scale.
So let's go and just write the Wikipedia articles for the remaining 15,600. And then we were like, this will be like our real world evaluation task, because like this is a task where that like, you know, you're not going to be worried about like, you know, contamination you can compare it if you want against like the,
this is like the task that where the, this is like the performance that you care about. Right. Like, you know, it's not you can't like hack it.
Right. This is just like, if you can write the wiki articles, if they're good, if they're comprehensive, if they're accurate, then you win. Because like this is like an example of the real thing that you want these systems to be doing.
And there are still 3,639 articles on on Wikipedia, because not for the reason that you might think, which is to say that we did write the articles and Wikipedia has not adopted them. And the reason is not because they're written by AI, actually,
Wikipedia is super into AI generated content. Wikipedia is a fully volunteer run organization and we went to them and we were like, we have 15,000 articles to add to your purpose.
And they were like, we have two people who maintain our molecular biology articles. They're like, we cannot accept these articles. So I would love to find a creative solution to that. But, you know, that's just that's how it is at the moment.
And so basically, how Wikipedia, you know, what we did was that we took a Wikipedia article and thankful you like Wikipedia had a spec, like they have a spec on online for what you should.
And so you just take that and you kind of like turn it into a prompt.
I mean, you do some prompt engineering, obviously, but you basically just take it and you turn it into a prompt. And this is just an agent so you can just give it the prompt. Right. And then the agent will figure out that has to go and run itself several times.
It's pretty cool. And so what this brought down to was executing paper QA 100,000 times with in two days.
We drew from 871,000 papers.
Our corpus of Fortress papers is 80 million papers. So this is like, these articles are drawn from like 1% of the entire like scientific literature, which is pretty wild, actually.
And the really cool thing is that there are 670 articles that like very persistently failed. That is to say there are 670 articles that paper QA cannot find any information for the gene.
Okay, so like, there are still 670 genes that are just like they do not appear in the standard version they have. They usually have like, these kind of like procedural regenerative names, right, this is like, you know, see three or 15 or something.
It's like the 15th or from some three or something. Anyway, so they're, they're annotations, but they don't have any, any information on it. So examples, I guess I can just kind of do this, I think I can do this right. I can just do
this going to mess up the zoom. I hope not.
So these are just to show you guys what this actually looks like. You know, we can come and through and do literally any like these are just these are genes.
These are genes.
It's like a little bit out of the suit. Sorry, and you probably can't read it, but you know, something my EF to the world and look at mkks. I've never heard of these. These are like 100 randomly displayed ones, but like,
you can go in and you can find, you know, any gene that you want, you can go in and you can look, you know, and, and you just get this really nice article like well cited.
I don't know, it's a centrosomal shadowing protein. I'm not going to spend the time to read it right now, but you guys structure section and function section and it's all cited like this.
Okay, I'm just going to go back because I want to make sure that we have enough time to talk about the interesting things.
Maybe for people, I don't actually know like what the background is here, like what fraction is AI and what fraction is biology. So maybe just a quick section, quick side know about why you want this.
Um, if you do research in genomics, the, like your life consists of like running some assay and coming back with a list, the assay comes back with a list of like 50 or 100 genes.
And that are like significant according to that asset, right? Like maybe these are genes that like turn on in river biological process you're studying, you have never heard of any of them, right?
Because there are 20,000 and the lists are usually basically random. And so you take the first one you go to Wikipedia or you go to Google and you type it in and up comes nothing, right?
Because like the vast majority of genes just like 80% of the time. I mean like most genes really don't have like plain text annotations and so you have to go back to the primary literature.
It takes forever. It's impossible. And so this is like the actual use case here is that it just makes it way easier for biologists to get information out of it.
Like out about genes. Okay, so this is now the really interesting thing which I've been making this whole fuss about real world evaluation. So here it is.
So the way that we evaluated it was that we took 100 statements and we, from each of like, you know, natural from like human generated Wikipedia and from wiki grow, and we asked for every statement.
Is it cited? And is it, is it like accurate as cited, right? So like, which is to say like, is it accurate as per the citation? Does it agree with whatever the citation said because we're not going to like evaluate.
We're totally out of scope to evaluate an absolute term for the statement is true. We just evaluate based on what the citation says. And what you'll notice is number one, Wikipedia wiki pro is way better than Wikipedia at citing its sources.
So 16% of the sentence of the statements and Wikipedia that we found like are just like not cited. Right. I mean, this is consistent with your experience of reading with the media.
Um, uh, where, uh, wiki pro generates, uh, uh, cited basic in terms of like, for everything.
But wiki pro is about 10 acts more in like less accurate.
Like, uh, gets confused on big stuff up. Doesn't exactly make stuff up. I mean, this point about saying it doesn't hallucinate.
It is still true here. That doesn't really say what happens here is that it usually gets confused about what it's reading. So the most common thing is that, you know, it will be reading something about like the end terminus of the structure of the protein.
And the people will say, oh, you know, it has like a unique form of the C terminus. And then at the end terminus, there's this fall that it's like, you know,
broadly conserved among all members of this class. And then the wiki pro will write, you know, at the C terminus, there's a unique fall that is broadly conserved among all members of the class.
Right? Like, no, it cannot be both unique and like why we, you know, end very common.
It doesn't make any sense. And it's just like something, it has like discarded. The model has somehow just like ignored the like end terminus versus C terminus distinction.
We see that kind of thing a lot where it's just like, there are tokens that are important in biology that like the model probably hasn't seen enough in its purpose or something.
It will also do that with gene names, like, you know, rare genes to the keys or one keys or two, for example, it will take a sentence about keys or two and other things stick into the keys or one like, hey, this number on the end is definitely right now.
But so that that is like just really obvious, like super obvious ways to fix it that accounts for basically all of these errors.
I should copy up here, like engage you have a nervous.
This is 0.9% of statements in human generally with a year or in fact, out of 160 minutes, so this is become one.
So we don't know how, how many errors there are in the media.
But I mean, we're like a chat.
But I think it's certainly we know that what you throw right now is more errors, but I can guarantee you basically like three or six months.
Like, we will, we'll do a better measure to this, but three or six months is going to be on par with or better than the human generated.
And the point at which, like, you know, machine generated Wikipedia is like, or like wiki crow is both like more accurate than and also more comprehensive than human generated would be.
You kind of almost don't even need Wikipedia.
You just like generate, you can imagine just like generating like, you know, up to date accurate summaries of whatever you're looking for like on the fly.
I think we're like a few years away from really wanting to like replace Wikipedia, but like on the flip side, we're only a few years away from really wanting to replace Wikipedia, which is kind of insane.
Okay, so just a word about future directions.
So we are interested in a bunch of other things. I mean, I think that again, just like I said before, the only thing that matters is whether you can evaluate your performance.
And so what like what we're basically doing is we're trying to figure out like wiki probably like this evaluation that we're doing the way he grows like pretty cool.
It is manual, but it's manual, but it's like pretty scalable like it's very easy for things you need to do. And so, um, things are not valuable or definitely better.
But basically what we're doing right now is we're just trying to figure out like how do you evaluate these.
How do you evaluate all the various things that we have to do.
Everyone wants us to be coming up with like better like ideas like how do you generally, you know, how do you figure out like whether this idea is like good or not. Well, like, I mean, I, we don't have any way to evaluate that right now.
Like, all of the big advances in this area, maybe slight, slight exaggeration, but like not that much. Almost all the big advances in this area will come from like better ways to evaluate performance on the.
Like, because once you do that, I really want to do the best way, like, you know, the rest is kind of, I mean, I don't know, I don't want to curse myself by saying the rest is easy, but it's certainly at least then it's like tractable.
Right before you have a way to evaluate, it's just intractable. And so, yeah, um, I guess, happy to take any questions.
Yeah.
Thank you guys.
So, sorry, you're asking, did we.
Yes, we did not do that. Actually, we did not generate articles.
We did not generate articles for the.
Sorry, if I understand what you're saying, what you're saying is, did you generate articles for the articles that are human with media articles.
Yeah.
Um, so we didn't. And the reason is, um,
So, first of all, I'm exaggerating slightly when I say that these 15,000 do not have articles. Most of them have cells. I mean, like two centers, one sentence, two centers cells.
We count those the list of ones that I don't have articles.
Otherwise, there's a continuous range of human written with the article.
Right.
Right.
And so, um, yeah, we didn't do it and it was not fair how it...
It didn't seem like it would be different also.
It didn't seem like it would be different from these statistics and I'm pretty sure it would be the same.
Or maybe not because actually that's a good point.
It might be more favorable because the genes that have Wikipedia articles are the more common genes and are therefore easy.
So if you show the influence over human beings that way.
That would be a better way. Yeah, that would be a better way to show the improvement. I think what's for there.
Right. I mean, the point in which we can generate a k-brass article or an article for k-brass that sorry k-brass is like a really common like on-code protein.
It's involved in a huge number of cancers.
The point in which we can show a like article for k-brass that like most experts will look at and agree is like better than the human written to be article for k-brass.
And that would be that would be awesome. But like, you know, those articles are super high quality.
Great question.
Yeah, so
basically this is kind of similar. So this was paper QA. This is not what you've got. But like, you know,
but basically
we, we debated that question for a long time and we just came up with this taxonomy, which like what is generated when you get an answer from paper QA is a text that is like some facts or like some like statements.
And then it and then like some citation.
And the hallucination that we observed in the model in general, we're breaking into one of like three categories, one which we will call like a full hallucination, which basically just means this is all human.
Right. So it means that you read it and you're like, where does this come from? There's like no where this comes from. The statement is like not correct.
Right. This is just like, you know, when you don't have any other explanation, you call it a full hallucination.
Then the other two forms are basically like a citation, like the citation accuracy means that there's a statement.
And the statement is cited and that citation does not have any clear, like, that's like, it's like a state, it's a citation that is not accurate.
Like the citation doesn't exist. Right.
Often you can actually kind of come to a conclusion about what paper it was referring to. So you can check.
And you're like, there's a paper that's like close enough and you can kind of figure it out. But that's a, and then you can like check whether the fact is in the paper that's like citation accuracy.
The context of all this is that you get a correct citation to a paper that's just totally wrong.
Those are the three citations, the three that kinds of hallucinations that we observe.
So don't worry that, like, the language models on the road, like you provide accurate citations sometimes and sometimes actually provide like accurate citations where like the information you're looking for is in the citation.
It's just in the weeks. And that's cool. That's great. But like, if you guys haven't tried this, it's like, we're trying to kind of think that will happen is like when I try it on some of my papers, you get like the correct title.
And then you get different authors who are like people who have worked on highly believing topics. Right. And we would call that like, you know, citation and accuracy.
Yeah.
Oh, wait, in wiki.
Um, so that is literally, uh, I think it should say, in fact, it got corrected in the, it got corrected in the on the blog post, it should say your relevant or missing citation.
So that's that. So basically, we broke it down into like, we took it to the article, like, you know, randomly sample.
Uh, how, how we did this, I don't remember the details of how we did the sampling. I think we randomly sampled it with the year or two, and then we randomly sent it sample like a sentence to the year at all.
And then a human, but he found that sentence and looked at like the higher factual statements. I'm not a factual statement consistent more than one sentence.
Right. But looked at like a cohesive, a cohesive factual, factual statement.
Um, and asked, is there a say type citation to support that factual statement.
Um, and then this number is diffraction where either there is no citation.
Right. Sometimes label is like citation. You did it. Sometimes not. But there's no citation or there's a citation. Go and look at the citation and like the fact that some of them are just nothing.
Yeah, these are human written. So we're, and again, I mean, like, people do this, if you've written papers, like you may have done this where you're like, Oh, I'm pretty sure the fact is like in this paper, I'm going to say that paper.
But like something that you go back and looks and you know, the fact is not in that paper. Right.
Yeah, I mean, despite being educated, but how many times do you see that there's like, you know, for example, a last statement, like I said, it's a statement that does not fight all the information.
In.
If you were to imagine writing a review, which always has a part sometimes.
Yeah.
I know we don't have a good, we don't have a good way to isolate sentences that are true sentences.
Like, if you kind of see that it happens.
It definitely happens.
But we don't have a good way to explain that.
Yeah.
And it's one question I have.
Oh, no, I like, I think that.
Um, so in this case, like this number, we now have a really nice phone call.
It's humans.
Like, like I said, I mean, like, um, I mean, we definitely thought about whether you have a model, try to do it, but then you get into the weird circle regime.
It's just like, not with it. And so you can definitely, I do taste human.
If I give you a statement, I think that's a nice interface interface is like, the interface is really important.
You have a nice interface where you write them a statement and you provide them like a paper.
And just like the text is there, the statement there and just say, he's the statement supported by this task.
Right. And then they go through it.
If you can do that.
And so you're paying, you know, that ends up being like $2 or something.
Um, I mean, like, if you're paying $50 an hour, something like that, so say $60 hours or $1, right? So $2 or $3, for example.
Um, and so, like, the first thing I'm doing is bringing back of just like, you know, here are a bunch of statements.
Um, uh, and we're like, here are a bunch of, uh, uh, papers and like, can you.
Can you come up with like, you know, can you come up with like correct statements, but, but basically, yeah, we'll just, you can add to the contract and bring it easy, right?
Because it's like, you're $2 or $3 per statement.
Uh, yeah, you can do this, like this value that you scale through.
Yeah.
More than a question, not only relate to the work you've talked about so far, but especially hypothesis generation is all those tasks where, uh, like you said, valuating the card.
So would you consider working on proxy tasks that would result in transfer improvements?
Yeah.
But how are you going to measure transfer improvements if you don't have a way to measure something like that?
This is like my main, this is like my main objection, which is that like, um, it's a, it's fine.
It's like great for research to say, you know, here, like science is hard and therefore with your minecraft and it will just transfer.
But like, will it transfer?
Like if you don't know how to measure performance in science, you don't know it will transfer.
So, um, I mean, I personally, I suspect that it will transfer to what you said, right?
It would be weird if it did not transfer at all, but you don't know how much.
And so the problem remains, like there are independent problems.
You can build a great minecraft engine, but you still eventually need to be able to measure and have a low cost of stuff.
Um, yeah.
Yeah.
Yes.
I'm very interested in implementing that. So it doesn't do it.
But, um, uh, well, I think about the, like, you know, the kind of, um, uh, the dream is, is like for Ricky Crow to then like go and have it, like see some sentence, some statement, and then we go and like, try to, you know, in a paper and be like, Oh, that's even actually true.
And, you know, we're not quite there yet, but definitely kind of on the road now.
So, um, yeah.
Yeah.
Yeah, great question.
Um, what's it for right now?
Uh, so, um, ultimately.
Uh, how do you, what's the, what's the way that you evaluate hypotheses? I've made this point a few times, right? Like about like evaluating hypotheses. We don't know how to do it. I should have said, we don't know how to do it in silicon.
The way that you evaluate hypothesis is use of experiments.
Um, so, uh, basically the, like, what we're doing with the wet lab is we are building, like setting up very high throughput assays.
The wet lab takes a really long time, right? You can run 100,000, you can run paper to a 100,000 times in a few days. And in the same amount of time, you will wait for whatever IDT to sympathize with all guys.
Um, and so the what I was really slow, but you can do things in high throughput. So what we're doing is we're setting up like super high throughput assays that where you can get, you know, hundreds of thousands, millions and tens of millions of data points on a three day, four day turn around.
Something like that. So that you can start to generate hypotheses and you can evaluate whether you're doing a good job at generating hypotheses. Yeah. And more details on that are kind of like, to be determined.
But those are, but we're like starting to set up these assays and figure out what the right environment is. That was part one. I think.
Yeah.
Yeah, two things.
We don't really know. So they're like, definitely like something that's about what you throw that are just like optimized or like, they were great.
And we're like,
And we're like,
We also have like access to a waiting loads or full text purpose.
Which like my guess is that right now that is just like access to full text articles where like there are probably some questions that they are not helpful.
And we have access through like, you know, through our own descriptions and that we would not be able to provide those like you could not just like provide it's not like they can go sign up for description and then start offering it to every
kind of world. Like, you know, we're using it for like internal research.
So I do think like one point that I would like to make about these systems is that like, you know,
complexity is doing great because they're offering, they're like pursuing an asset market.
The market for these kinds of systems is like not large among scientists, there are like 100,000 maybe scientists in the world who will want a system like that.
And so there are companies that are like trying to make simpler systems and kind of sell them for like $20 a month.
And so their revenue is like half that $2 million a month that they have 100% like advancements.
And that's not enough to sustain that somebody.
And so I, you know, yeah, this is part of the reason why we're doing this on top of something.
I think that in like the long term future, what we really want to be doing is building kind of a comprehensive mechanistic models for biology.
And now what is a mechanistic model? I don't exactly know like, so I'm going to leave it there.
But something about, you know, if you had a mechanistic model, a comprehensive mechanistic model for the brain, like whatever that means, like what form it's in, what form it takes, I don't know.
Right, then it would not even, the question would not even necessarily be anymore like helping scientists come up with like better questions.
Right, like, because people like have the oracle.
And the oracle would give like, give you answers to your questions in, in like some understandable, like a mechanistic form.
It presumes the knowledge is only right. I mean, like, first of all, it's definitely not known. You'll have to conduct a lot of experiments to get that.
And there's another question about whether it's like no at all.
But, but certainly, I mean, that's kind of like, like I kind of said at the beginning, I think that part of the issue is that if I said to you that you have infinite money and infinite resources in order to generate even just like make it simple.
Like a full mechanistic model of the coa, I don't think that you wouldn't necessarily be able to come up with all the experiments that you need to come up with.
And like, to be able to like sequence them in the way that would actually get you the data that you need to generate that out.
That's like, ultimately, that's what I think it means to help us do.
Now, I'm like, really, I should also say, I'm like, I don't think that they will like replace human scientists.
Human scientists will replace human scientists again.
But, you know, ultimately, like what you said is probably also will also be true.
Humans will always have questions in the AI as I told us.
But yeah.
It's a mixture, actually, so the different like pre trained models are all good at different things.
So, certainly, it was actually funny because you can like see a different checkpoint.
If you have a good benchmark and different points available, you can like see the things change. And so in like September, it was a mixture of like two before and five.
And then it just became you need to work out for them and before.
And we've tried the open source open source models are not good enough at all.
People get better. I mean, we really, we really like it if they get better. I'm sure they will.
But yeah, at the moment, it's mostly yeah, but there's so much to it.
Yeah.
One thing I just thought about, like in terms of the
if we're imagining that this tool to be used either by biologists or anyone else who's trying to work towards the biological questions, one of the problems that I frequently encounter is like I work in the lab and drive lab.
And a lot of my dry lab colleagues will come to me with genetic questions and they will pull up a bunch of papers and citations for some gene of interest.
And about half of them are no fault. Yeah.
So, you know, I'm imagining what a powerful use of this. I don't know if it's on your radar would be to detect such instances of what low quality data falsified the goals for otherwise like serious observations that would put.
Absolutely.
Well, this is like what I was saying, ultimately, what you would like is you would like to find you would like to have quick control, like, and then go and be like, kind of stride to like see if it could look at the data, try to reproduce the analysis.
One day that access to a lab and it's sufficiently high value.
I mean, I think of the point, like the, the, I mean, this point about like the $20 a month thing. The flip side of that is that like, really high quality standard summaries, but really like what is going to work a lot.
Like, I mean, and so if this was like, hey, you're a month, and you need to go and like, answer this question, you really need like a really robust and a really robust answer.
And that means that you go, you see, you figure out what are the key experiments that you need to reproduce or if you can reproduce those experiments will have the answer, then you go run them.
And again, you come back and you say, hey, look, this is the answer and we're confident and say I'm going to do this experiment because like this data is published to be in high quality.
I mean, this experiment, that's not, and they reproduce and therefore, you know,
Sounds like that's what I'm talking about.
Especially right.
Yeah.
So you're not just responsible for, you know, showing the results you also need to make sure they're longer than square.
And it's also where the target, you know, it's also the target, like validation.
Yeah.
Yeah.
I would say
we don't have a bunch of very good ideas about in silco evaluations of hypotheses.
If you want to know more, we can take it offline and have a little bit more about what we're thinking.
But I think that ultimately, it will really boil down to like, what lab evaluations, which means that you need to have some really efficient, like,
right, you'll be learning a lot.
Every time you come with bad hypothesis, you'll be learning a lot about what that is about why the hypothesis is bad.
Right.
Like this is part of what I was saying about how this is like an interesting claim on for a GI because like, ultimately, for this to work.
Like, you're not, you can't do 100.
You can't like cast.
You can't like cast.
You can do these poor assets that I'm talking about.
We get to pass effectively 100,000.
You can't have 100,000.
Yeah.
Yeah.
No, of course.
Yes, you have a question.
We have not.
We have not.
And honestly, we haven't exactly seen it.
Um, relatively uncommon that two papers were directing each other.
I'm sure it's more common that you have like five papers where if you actually put the results all together, you'll get some like, you know, it'll be over specified.
You'll get some like, you know, it'll be three conferences, but we don't.
We haven't seen it.
Yeah.
Yes, actually, we have one person who's working on it.
I'm not up to date on the results, but if you, if you're interested in that, I can put you in touch.
I think it is.
Yeah, actually, I do know that there's one person who's playing around with it.
Ross.
No, we're out of the box.
Oh, and I should say, so we have like, uh, we're looking for, so we're hiring, so people are excited about this kind of stuff and want to come over to us like, let me know.
Um, we are also looking for interns and if you, uh, we've been like kind of putting our internship program together slowly.
And so if you're previously like, we had an opening, if you applied to it, or if you knew us before and you didn't give it back, it's like, you know, me again.
Um, because we, that's been like coming together kind of slowly, but we're getting more together now.
Um, and so, uh, and just to emphasize again, we both do research and also pay money.
Yeah. Okay. Thanks guys.
