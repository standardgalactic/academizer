Captain America
Captain America
Captain America
Captain America
Hello, good friends. Good friends of Artificial Intelligence, General Artificial Intelligence,
and Zbier and Johnny. Welcome to Artificial Intelligence journey.
And we start our discussion panel on Can Machine Sing.
We believe that for this year, the famous article of Alan Turing,
published in Mind Journal in October 1950,
which started actually the whole Artificial Intelligence journey.
So it's a good reason to discuss what is done on 70 years of Artificial Intelligence.
And today with us a few best in the world experts on Artificial Intelligence
and on Information Technologies.
First of all, you will hear from Gurdjil Pao,
Corporate Vice President of Microsoft Research,
and our long-time partner with whom we did not only one research project,
and we are happy that Gurdjil will join us for Artificial Intelligence journey.
Everybody of you actually who is a little bit older than 35 are familiar with Gurdjil,
because if you know Windows NT and if you know VPN,
then you know some of his products which he developed while he is quite a long-stay at Microsoft, almost 30 years.
So some of you younger than Gurdjil is working for Microsoft.
So Gurdjil is inventor of Windows NT, at least part of Windows NT.
Gurdjil is inventor of one of the first VPNs and stand in the beginning of the Internet Air,
Windows Air and now Artificial Intelligence Air.
So Gurdjil, please, what is allure of thinking machine? Please.
Hello, everyone. Firstly, I'd like to start by thanking Albert for inviting me and giving me the opportunity to talk to you
about one of my favorite topics, which is really in the Artificial Intelligence space.
I'm joining you here from the Turing room, which is in the Microsoft Research Building, Microsoft campus in Seattle area.
So, you know, the topic of thinking machines has been a fascination, you know, for a very long time.
I thought I would start with the intelligence in life itself.
Now, you know, the earth is about 4.6 billion years old.
About 500 million years ago, something very interesting happened.
At that time, all the life was living inside the oceans.
The oceans had very low oxygen levels.
But around 500 million years ago, there's what is something sometimes referred to as the Cambrian explosion or the Cambrian acceleration happened,
where life suddenly got more intelligent.
And scientists are still, you know, arguing about what is why that happened.
But the main thing which happened, the few things that happened at the time which we know is that number one, the oxygen level in the oceans went up.
It went from about 2 to 3% to about 10%.
Another thing which happened, scientists believe that the developmental genes developed through mutations, which allowed the regulation of how creatures were able to, you know, control their basic genetic processes.
But the most interesting explanation to me is that it was about 500 million years ago that the complex eye structure was developed in creatures.
And that was a very, very interesting development because once the creatures could see, they could find food easily, they could avoid predators and they could actually go after other creatures.
And that allowed much more complex organisms to be built.
I think it's good to remember this as we think about artificial intelligence moving forward.
Now, after the 500 million years ago, the first documented idea of artificial intelligence was provided by none other than Homer himself from around 8th century BC.
8th through 12th century BC is sort of the time when, you know, a lot of his writings are attributed.
And he talked about in the Iliad how the lame god Hivistus actually was talking about different kinds of robots around him to help him in his life.
So one were these tripods with wheels, then there were these robots to shape like the female form who were helping him with lots of things that Homer was, what Hivistus was trying to accomplish.
After Homer's writing, the next significant piece of thought in the area of artificial intelligence is attributed to Ramon Lull, who was a philosopher and a thinker, a Catalan region of Spain.
Ramon Lull actually came up with the idea, in fact, he had invented a kind of a seven disc rotating system where each seven discs had different kinds of concepts.
And he basically said that if you rotate the different combinations of these discs, you can pretty much contain all the ideas and concepts around, you know, that that humans exhibit.
And it was, it was a very, very interesting, you know, idea that inspired just a lot of thinking actually centuries later.
You know, it was, you know, William Gottfried Wilhelm Leibniz in the 17th century, who then took that idea to create, to create perhaps even a more developed system, in which he created sort of this alphabet of thought, and he wanted to sort of create this computation of ideas.
And he sort of furthered that cause.
You know, after that, of course, why we are gathered here today, you know, this Alan Turing sort of seminal paper on can machines think, you know, in that paper, Alan Turing did a couple of things.
I think number one, really created really formulated this, how to even think about this idea of artificial intelligence by having this so called Turing test.
But also, you know, I thought the work is his taking the different arguments and really sort of, you know, playing them out. I thought that was incredible work.
And it was also very telling towards the end of that paper when he talks about things like telepathy and so on, which sort of, you know, he kind of created the sort of very open ended aspect to artificial intelligence, which I think is something that we should all keep in mind as we move forward.
The last 70 years have been quite interesting in the artificial intelligence journey.
You know, of course, it started off with a lot of the rules based thinking, a lot of the symbolic systems that that people worked with expert systems.
And while that gave some pretty early sort of off the races, we found that that sort of thinking, you know, ran out of steam, and we ended up with this AI winter, until some of the data driven approaches started to show promise.
But it was only in the last decade that we saw the neural nets make comeback, thanks to, you know, a lot more computing power and a lot more digitized data being available.
And we're starting to see some really, really incredible progress in the last 10 years.
Now, if you take a step back from this, this journey in the last 70 years, you will find there is about three different schools of thought, which represent these cognitive metaphors.
You know, you have the connectionists who are really trying to model, you know, intelligence in the sort of this graph form, you know, with these, these units which are connected to other units and it's through this composite analysis you're able to find you're
able to sort of emulate intelligence. Then you have the symbolists who were, I would say, have tried to, you know, sort of latch onto the idea that humans think in certain concepts, and humans think in certain building blocks of knowledge.
And the best thing for us to do is to really emulate that inside a computing system, and you can build, you know, the different layers of intelligence just based on that particular idea.
And then there are the probably the lesser known dynamicists, you know, who believe that these dynamical systems can be best modeled with things like differential equations and so on.
And what we've seen in the last 70 years is, you know, these different cognitive metaphors sort of, you know, taking the lead, pushing forward until they hit some walls and then, you know, we take, we stall for a while and we go pick up the other idea and we run with it.
But generally, you know, you can classify all the work so far in these three sort of buckets.
Now, let's talk about the connectionists who are really having their day in the sun as it were with the resurgence of neural networks and with deep learning.
Now the core construct there is really the construct of the neuron, which as you see, you know, there is a biological neuron inside the human brain.
So he said, Well, you know, how about we create an artificial neuron, which actually is very simple when it comes to sort of the mathematical operation that it does.
And, you know, with the activation of that particular neuron, recognizing that it has already been proven that the biological neuron is much more superior than the artificial neuron that we envision today.
One simple example of that is that a single biological neuron has been shown to exhibit nonlinear capability.
For example, if you have to take the XR function and you have to do it with artificial neurons, you will need at least two layers, but it can actually be done in one biological neuron.
Now, taking this basic idea of of a artificial neuron, of course, we create neural networks in the same way that our biological networks in the human brain, we create artificial neuron neural networks.
And it is again through similar kind of a connectivity of these different neurons through which in biology is with synapse here it is connections between the layers of the neurons.
And, you know, in the last 10 years or so, some of this is little predates that we've seen just tremendous amount of deep learning architectures for specialized tasks starting to emerge.
And some of these are actually inspired by biology. In fact, if you look at a lot of the work in computer vision, not only, you know, it was inspired by biology.
It is now explaining human biology, when it comes to computer vision, and some of the processes that we see in the visual cortex.
Similarly, you know, some of the work, you know, with, with memory based architectures, whether it be, you know, LSTM, etc. are starting to really, you know, get very, very specialized, you know, really taking some inspiration from biology.
Thanks to the progress in deep learning, we have seen some amazing breakthroughs in the last five years.
Now, all the breakthroughs that I show to you on the slide here, all happened in Microsoft research, everything from the ability to have speech recognition at a, at a level that is better than humans,
the ability to detect objects, better than humans, machine reading and comprehension where the AI model reads a corpus of text, and is able to answer questions based on that captioning of video things.
All these have happened just actually in the last four to five years. And these milestones in Microsoft research, of course, tremendous amount of progress has been made globally in the industry, not just in Microsoft research,
but just a glimpse to show you how much progress we have made. Now, none of these tests actually would qualify as a Turing test by definition because the Turing test basically, you know, said you could ask a question, you could ask the entity something that, you know, could be outside of the domain that these models are
working for. But regardless, you know, they represent significant progress, but also the significant limitations of the progress that has been made.
So, before we talk about, you know, some of the, you know, the limitations, I want to talk about the GPT-3 model, which I'm sure that you have heard about.
The work that has been done by OpenAI, this year, you know, they announced GPT-3, which can be called the grandmaster of language models. It is a generative model using some of the transformer architectures that have been perfected.
It is incredible. The kind of results that we are able to see with GPT-3, you know, you could say they're language results, but actually many of them are fact-based results as well.
Because this thing was trained on about five billion tokens of data, and it has 175 billion parameters, which is, you know, absolutely incredible.
And you could say that, you know, sort of underlines the fact that the deep learning-based approaches really became possible because we had this incredible amount of compute that we could bring to bear and also incredible amounts of digitized data.
This GPT-3 has, in addition to, you know, of course, you know, being able to write different kinds of long-form text, it is being used for many, many, many applications largely in the language domain.
For example, you know, there are already applications for writing Python code. There are applications for automatically generating emails.
There are applications where, you know, there are plugins where you can automatically fill out Excel data, etc., which have all been written on top of the GPT-3 model, which has been very, very, very impressive.
I think if you have to look at the progress of AI today, you have to use this as where we are 70 years after Alan Turing's paper.
And so this marks probably the most significant progress. You know, we are very happy to be working with OpenAI at Microsoft.
We have a deep strategic partnership with them, and a lot of this work happened on the Azure platform itself.
So, okay, great progress, but we also have lots of limitations.
I'll talk about some of them which I've listed here. Well, number one, you know, the amount of data required to train these models is just, I mean, it's just really, really too much for us to get this level of performance.
And we know that, you know, humans are able to do very well without using that much data.
The models themselves are opaque. You know, all the learning itself itself is sort of encoded in these multi-dimensional vectors, you know, which no one can make sense of.
We have this problem which humans don't have, which is that these models are used for training offline, and then, you know, they're used for inference when they are being used, but there isn't this notion of continuous learning in most cases.
You know, there is no semantic understanding. I'm going to talk about some examples on that. The power required to train these models is incredible.
You know, estimations on GPT-3, which I have read on the web, you know, take us into megawatts of power for training these models while the human brain, you know, seems to do fine with about 20 watts of power that it sort of runs in.
So there are many, many different limitations, you know, all these models are trained for fairly narrow set of tasks, though with GPT-3, we're starting to see that change.
You know, these are multitask models can be built on top of the single representation that is learned, which is quite impressive.
So you can see, you know, we've made a lot of progress in the 70 years, but there are so many other hard things that we need to solve for, you know, including causal reasoning.
So I talked, you know, how powerful the GPT-3 model is, but it also has lots of limitations, which kind of highlights the point I was making about the kind of limitations that exist today, even with the best models.
If you ask GPT-3, which is heavier, a toaster or a pencil, it says a pencil is heavier than a toaster.
And you ask it like when counting what number comes before 10,000, it says 9,099 comes before 10,000.
And you ask, like, who is the president of the United States in 1700?
It says William Penn was the president of the United States in 1700.
Okay, let's examine these three questions.
Well, the first reason, the first answer that got wrong about pencil being heavier was because in all the 5 billion tokens of text that were there, nowhere it had actually had a direct reference to the weight of a pencil and a toaster.
So it got it wrong.
When it comes to counting, it actually does not have an underlying idea of mathematics, which is a very important point.
That's the semantic understanding part.
It, therefore, it basically tries to find a close answer and gives it, it turns out to be wrong.
In the third case, you know, this was a bit of a trick question because United States did not exist in 1700.
But it made up an answer, because it, you know, it thought, you know, I'm going to take some famous people from the 1700 Peter time in this area.
And they basically came up with an answer basically telling you that while we have made a lot of progress on some very fundamental things that you could probably see if you ask a maybe a nine year old kid and ask the answer to these questions,
they will probably get these answers right, which this amazing model could not get right.
Now, Marvin Minsky, who, you know, can be called one of the elders of artificial intelligence in 1970, you know, said that in three to eight years, we will have a machine with the general intelligence of an average human being.
You know, this was 50 years ago.
So, you know, predicting when we will have the average, the intelligence of average human being is, is a very, very difficult task if Marvin Minsky got it wrong.
You know, many of us will probably get it wrong if we try to predict it.
So it was very interesting for me to read another prediction by the Turing Award winner this year, Jeffrey Hinton, who's considered as one of the three, I would say the fathers of deep learning movement, you know, who made a statement,
deep learning will be able to do everything, which made me, you know, think, you know, like, could this be true.
And I came to the conclusion that it is, it is, he's probably right.
And for one very, very fundamental reason is that deep learning has given us a, an approach which allows us to do function approximation better than we have ever been able to do.
Now, the place where I feel, you know, his statement is, is ambiguous.
And maybe it was intentionally so is that I believe that some of the approaches that are going to be needed to get to artificial general intelligence are going to be a lot more coming from the, from the, the symbolists and the
dynamicists world as well, even though some of their ideas may be solved best with some of the deep learning approaches that are there.
So, you know, that's kind of where I see things going. I feel that if you look at, you know, where do we need to make tremendous amounts of progress.
If we are going to, you know, get to the sort of the human level of intelligence.
Number one, I think model based notions of space, time and physics. I mean, this can also be called common sense.
You know, a little child doesn't need to learn that, you know, if you throw object they've never seen before up in this, in the, you know, up in the air, what's going to happen to that object.
You know, they certainly have notions of time, they have notions of space. These are fundamental ideas that somehow needs to be encoded into all the approaches for AI that we have.
I think knowledge representation, we are starting to make really good progress, you know, certainly on the language side, but I think this needs to expand into numbers into graphs into, you know, pretty much how humans, you know, have this notion of knowledge that we sort of
require and we organize. I think reinforcement learning, I think it's a very, very important, maybe coming more from the dynamicists sort of side. I think it's a very, very important discipline, largely because if you look at how children humans
learn, and a lot of those approaches can be considered as reinforcement learning, though I would say it is also, you know, online reinforcement learning is probably better be to say it.
I think causal inference work is critical. You know, today artificial intelligence work is completely devoid of causal inference, at least as we know it today.
The work that Julia Perlman, Julia Perl has been driving has been, you know, I think really amazing, it'll be great to see a lot more people focused on that.
And then lastly, I put in sparse learning, because I think that, you know, this power hungry approach to building these really, really deep and high parameterized models is not sustainable from an energy perspective.
You know, so we, I think sparse learning is pretty much, you know, how human brains also actually operate. I think it's something that needs a lot more attention. We started to see some, some really interesting work here.
But lastly, I would say, I think if we are going to make progress and towards a truly thinking machine, it will require the connectionist symbol, the symbolists and the dynamicists to all come together.
And perhaps using some of the constructs that we've got from deep learning to solve the problems. You know, for example, if you look at the symbolists, you know, they, their, their entire thinking is based on the idea of symbols that are really used to construct, you know, all the different layers above it.
But symbols, you know, don't necessarily have to be symbols that humans also recognize. And I think this is a very important insight.
In fact, you could argue that some of the breakthroughs in GPT three, which came through the representation in the Latin latent space with with vectors, you could say they are symbols it's just that we just don't recognize them.
Similarly, you know, deep reinforcement learning, you know, neural ODE work. This is how, you know, we're seeing how deep learning is even helping the classical disciplines like, you know, how do you solve differential equations, or build system how do you build models based on differential equations, except using the neural approaches.
So I think this is where we are. I think these three disciplines work together. We will have a truly thinking machine, hopefully in our lifetimes. Thank you very much.
So we keep our discussion panel during 70 years of publishing mind article.
Right now, besides Gordy Powell, who is temporarily out of connection, I would like to introduce our other guests who are joining our panel.
Our first guest is Mr. Dr. Leonid Zhukov, who is the head of artificial intelligence laboratory in Sber.
And, well, our closest collaborator on many fields of artificial intelligence, which is running here and you can learn about activities of his laboratory, I think, everywhere at this conference.
Dr. Leonid Zhukov joins Berbank quite recently from high school of economics where he was one of the head of faculties for computer science and artificial intelligence.
So we are very happy to welcome Leonid in Sber and research and development block for Sber.
So today you already hear Michael Woodridge, professor from Oxford University, computer science department, head of computer science department.
And Michael is author of a wonderful book, best book of the year by Financial Times, and I'm very proud that I read it from cover to the end of the book.
And please buy this book at Amazon, or maybe it's Sbershop, because it's really worth reading and it's not a chance that it's the really best book of the year.
So, but before discussing with Gurdip, with Michael, with Leonid during 70, can machine sink.
I thought that I need to give a context of our discussion, because this is really important.
Probably I forgot to introduce myself for those who are not really familiar with me, but maybe some of you at least my students might be familiar with you.
My name is Albert Yefimov. I'm working in Sber more than three years, and now I'm heading R&D block for whole Sber with vice president title.
I wear many hats in my career, and I would say I'm just amateur scientist and R&D manager here in Sber, and trying to be very useful for everybody.
So, back to our topic. Computing, machinery and intelligence was published first time 70 years ago.
It's quite a long time. Many people might remember a poha where we have no computers.
At least my scientific advisor for my PhD was born long ago before Turing published his thesis.
But it's very important to look at the past because we recall the past. It serves the present.
Today, it gives us fresh insights on what we sometimes overlooked and cause our attention to ideas that might be missed for some reasons.
It might give us new perspectives, and that's why we are looking for new ideas.
70 years passed since Turing published this article in October 1950.
So, it really was discussing, but first of all, let me remind you who was Alan Turing.
He was scientific prodigy. Some of his friends and childhood called him Scientific Shelley.
At the age of 14, he rode by 120 km to his new school, and next year he actually published a small article dedicated to Einstein's theory of relativity.
At the picture here, the drawing here, you see drawing by Turing's mother, Sarah.
And it's Alan playing a game, but he actually gave up game and just looking at how daisies grow.
And this is all Alan Turing. He always did something contradictory to others.
Later on in his life, he was famous for very ugly trousers.
It was not the fashion at the time in England.
Trousers should be always very, very well-ironed, and Alan was always with wrong trousers, so everybody actually paying attention to it.
But he was still very much scientific prodigy, and very early he was elected as a fellow in King's College, Cambridge,
and published a paper on computable numbers with application to Einstein's problem, which is problem of computability,
which got attention of Alan the Church and his PhD visit to the United States to work for his PhD with Alan the Church.
He finished it, returned back to Cambridge, and actually came to Ludwig Wittgenstein to have some discussion on intelligence and the foundation of mass with him.
And as soon as World War II broke, he joined government court in the Cipher School as one of the leading court breakers there.
The foundation of peace, which we are enjoying right now, was actually late at the time.
It was 1940 when he traveled in France during the war to meet with Polish crypto team, and then he established so-called hot-aid team,
which helped actually not to win the war, but to save enormous amount of lives for the fight of England and North Atlantic.
In 1941, he breaks few court transmissions, and Bletchley Park, where Alan Turing was working, was reading all the messages asked instantly as Germans were typing them.
In 1942, he created first automated machine to read German messages, and it helped to defeat Germans in North Africa.
Also, at the time, he visited US again and meet Claude Shannon in Bell Labs.
In 1943, he works on the first in the world speech encryption system in Bell Labs, and then returns to United Kingdom, where he builds world's first electronic computer colossus on the premises of Bletchley Park.
I advise everybody of you to visit Bletchley Park, which is now a museum, and it's very, very worth visiting.
In 1945, he completes this speech encryption system, but there is no use for it, because he celebrates V-Day in 1945 with quite work with his friends.
And then he immediately travels to Germany to study the cryptology, as well as to deliver a lecture.
This picture shows you how it looks Bletchley Park at that time. It was actually the head of Alan Turing.
And after the war, it was mentioned that he was one of the founders of Racial Club, which was founded in 1948, and it was very much multidisciplinary, and it was very much dedicated to new ideas.
So it says, you see that no professors, only young people were allowed, and Alan Turing was a kind of celebrity there.
So a lot of new ideas were proposed during Racial Club his days, and he actually, many, many later ideas he developed there.
So in 1946, he started to develop first electronic calculators in NPL Lab, which is in London, and also started running marathons, 30 kilometers.
In 1947, he visited the U.S. again, met with some U.S. scientists, including von Neyman.
And his meeting with von Neyman was actually laying foundation for the future study of von Neyman himself, and for Neyman architecture.
Then, after his, I would say, not successful presentation to NPL National Physical Lab, he decided to move for Cambridge for sabbatical.
One of his bosses at NPL said on Turing Report on Artificial Intelligence, first Turing Report, that it's a schoolboy set and not suitable for publication.
That's a well-known fact in Turing historians.
Actually, it should be very much encouraging for all young scientists, please keep working and don't be upset if old guys like me and somebody else are criticizing your ideas, which you are presenting to them on internal workshops.
So please be ready for critics and don't afraid of it.
And in 1950, he writes a world-first programming manual and then complete publishing, computing, machinery, and intelligence in mind journal.
Mind journal is philosophical journal, and that's why Alan Turing's become father of rye.
That year, he also bought a house in Manchester to be happy there.
Now I come to Foundation of Life and Eternity. In 1951-52, he gives a few talks to BBC on artificial intelligence with some predictions, and then he switches to actually biology.
And I think he also published the first paper dedicated to bioinformatics. So he might be also as well as father of bioinformatics.
Many modern historians believe that due to accident, not suicide, but accident, he died at June 7, 1954, just a little bit not living to his 42 birthday.
So he left no death note, and many people believe that it was just tragic accident.
So I strongly recommend you to, if you are willing to know more about Alan Turing, read this book.
Written to one of my good friends, Humashan and Kevin Barwick, as well as of course Mike Wu-Ridge book and some other books which are available widely.
So I have some further ideas, but first let me tell you two major ideas which gave us Alan Turing.
First is universal Turing machine, and second is Turing test which laid out foundation for computer functionalism.
What is computer functionalism? Very simple idea.
Descartes believed that 500 years ago that humans are machines, might be machines, animals are machines.
But Turing and later philosophers who created computer functionalism believed that humans are machines with software.
Brains are hardware and software is running on them is what actually makes us humans.
And he created Turing test which probably many of you heard of.
But universal Turing machine as well as very important one, his achievement, because every machine sooner or later become Turing machine.
It means we have emulation of everything possible with that simple Turing machine which is actually going back and forth on the paper, print, erase and print again.
That's it. Very simple algorithm, very simple machine, but it can emulate everything.
So what we do, what Gurdeep is doing, what Leonid is doing right now is just make Turing machines a little bit faster.
And that's it. We are not doing anything more.
So going back to Turing test is enormously popular. It's millions of mentions on Google.
It's a foundation of imitation game where we can think, can machine think comes from this, but we discussed it might be in more detail.
And I don't want to spend much time on some of my slides and I go to questions for our discussion which are very important.
And I see that my friend Gary Buratsky also joined us.
So we welcomed Gary to our discussion.
Gary Buratsky is a, well, I started with from past.
So he is one of the leading members of the team which won DARPA Grand Challenge in 2005 and then sold this team to Google.
He later sold another startup to Google and Gary, we are happy with it.
Gary is famous for being founder of OpenCV.
And Gary has got enormous amount of fans here in Moscow, in Russia.
And actually every young man and girl who are working on computer vision start with OpenCV which was developed by Gary team already many years, 20 years ago I think.
And also Gary has got many interesting ideas on what are computers right now.
What is computer science and what is artificial intelligence.
So all together we have four great experts.
One of the world greatest experts, you see, I'm like, I don't understand what happened to me.
Why I'm so lucky and happy to have you here at my panel.
And I put in front of you actually six questions you might see right now and three of them, but two questions actually most important for me right now.
First question is what was wrong for the last 70 years in search of artificial intelligence and thinking machine.
Would be chewing, surprised if he come to date our conference and see what is going on and what his surprise the most.
And second important questions, what should we do to make our research, our policy right for the next 70 years of research.
Yurgin Schmidt Hooper today was laying out picture of billions of years.
Well, that's impressive.
But let's not go that far 70 years is just enough for any predictions.
What we should do right for next 70 years and what mistakes we should certainly avoid.
So, I will start with Leonid.
And I would like me to give you maybe five minutes to express briefly but very clearly your ideas on those two issues.
What was wrong for 70 years and what should we do next 70 years.
Thank you very much.
Well, I'm not sure if it was wrong for 70 years right.
We definitely got a lot of achievements in those 70 years and the breakthrough, for example, in of course in deep learning and the way we heard today in all our presentations it's actually proving it.
But at the same time, it could be that we interpreted, you know, the paper the ideas in slightly if we interpreted the ideas slightly differently the story the history could also be different.
Because for me, you know, I think we should start with this notion of intelligence right and what is intelligence and I don't want to be like philosophical here.
I want to be very, very practical.
Right. And so intelligence is, you know, when I think about intelligence is really ability to set up and solve problems and achieve certain goals in real world.
And that's it right and artificial intelligence it's a computer or program that can do it.
Nowhere in this definition have I ever said the word human.
And so, you know, somehow, you know, going back to the Turing test, we put machines in this unfavorable positions in order to pass Turing tests they need to know what humans are right they need to know how to simulate and mimic and behave like humans.
At the same time, intelligence really doesn't mean to be human intelligence means ability to solve to set up and solve problems.
And so, I don't think we went in in the wrong direction, but part of the effort could have been avoided in the sense of trying to make machines that exactly mimic humans.
Now, while doing so, we of course, you know, make huge interesting discoveries and learn a lot about humans and learn a lot about, for example, human brain structure, etc.
But in general setting up a goal of building a machine that mimics human might not be the right goal.
And going forward, I think we should focus on things like actually building machine that solve things that can set up problems and solve them and not necessarily the problems that human would be able to solve.
In fact, why do we need machines to solve problems that humans can solve. So we need to have machines that will solve problems that humans cannot solve.
And we need to go and look in that direction more than trying to replicate humans.
And though, you know, humans and our brains are probably a huge inspiration for computer scientists to build machines, they're insanely complex.
And I think trying to reconstruct them in and to reconstruct, for example, continuous way the brain operates in our sort of digital binary zero ones that computers operate this probably is not going to work anyhow.
So bottom line, I think we need to refocus a bit.
Understanding intelligence as ability again to solve to set up and solve real world problem and work into solving intelligence in this sense and not in replicating how human brains work and what us as humans can do.
Thank you.
I have a question. Can you give us example of that kind of problem that humans cannot really solve.
Without.
You know, for example, you know, genomics right now with a lot of things we cannot solve we cannot, for example, map precisely genes on the, you know, genomes on the phenotype genes on to precisely diseases, right.
It might be a reason for huge in the mirror, huge number of possibilities or maybe we're approaching it in the wrong way. And I think the great advantage that could come from AI is being able to solve the problems in a different way that we're thinking about that.
That's one thing. Another thing is, I think, and as a professor you perfectly know that, you know, solving problems when it is well posed well positioned well specified.
I mean, when you specify the problem really well. It's not that hard to solve it. It's actually hard to formulate the problem precisely that it is solvable right.
And so if machines learn how to actually formulate problems before solving them right. That would be a huge advantage. So, and that's what I cannot do like I cannot formulate tasks for AI to work on.
I cannot put any actually goals and that's a big problem for artificial intelligence right now because goal setting aim setting target for moving forward is the biggest problem for artificial intelligence right now I think I also wrote a paper on this too.
Okay. Do you think that deep mind alpha something with molecules might work for application AI in science. Do you see that example of what you're talking about?
Well, it's actually a very good example. You're talking about the recent, you know, this the folding example right when I fold in case of folding. This is actually amazing example and it's also, you know, I would say it's on the same level for me amusement for me as, you know, GPT three when you interact with it.
But the thing is, the truth is, you know, GPT three has no clue what it is talking about. Right.
And so it learned the statistics of the sequences right. And honestly, if Alpha go by learning the certain statistical patterns is, you know, can you manage to predict the right protein folding that allow us to build new new new, for example, new medicines.
That's that's that's terrific. Does it bring us closer to like AI in terms of understanding. No.
So, thank you very much.
So, actually, Gary, I would like you to join us our discussion and also to elaborate on those two things. What was wrong for the last 70 years in terms of our quest for artificial intelligence. And at least I know two things you saw two startups to Google.
That's a good one. But what was wrong. And what should we write to make it better. You sell another one startup to Google maybe.
And something else.
Okay, can you hear me.
Yes, excellent.
Okay, so what was wrong. Well, you know, early on, we, we didn't actually have a lot of compute for number one.
I mean, our brain has a fantastic amount of compute in it. And we had not, we still don't, we're still not at that level that's like probably available anywhere, but is, you know, we're getting there.
So that that was one there. There was some understandings of neurology. There was also what the idea is of intelligence what it really is. I mean, people got enamored with like logical proofs and other things.
And this isn't really, you know, a large part, a large part of what our intelligence is, is simply survival as, you know, a Simeon in an environment for, for whatever reason.
So, you know, there were the goal that we had set out to do was was largely wrong like logical proofs and and
we didn't have the compute to do it. We didn't have a lot of understanding.
There weren't kind of sort of interesting machines like robots that were even capable hardware or the compute. So, you know, what was wrong was like too early and naivete.
So, you know, what do we do.
Well, it's, it's beginning to get a little better and that was like, let's say, you know, I've been, I started my academic career and in neural networks and those of us working at it like an early friend of mine was
John Lacoon. I contacted him when I was in graduate school because I wanted a corner detector he was working on but but you know we were working on neural networks and we knew that was the kind of structure not this symbolic AI.
So, you know, but but what we have right now is not it's a fantastically useful tool, but I call it it's still basically a deep associator it's not it's not intelligence itself.
And intelligence itself is is a little bit difficult to define but but you know I think there's there's a right now you have to understand like what the mind is and what a mind can do.
And, and I think there's, you know, a lot of misconceptions there's, you know, it can a mind live forever it's pretty clearly no not not in any not in principle, like it can, because a mind is built up off of structures, and those structures suffer a kind of decay of when you are supposed to represent like distributions and and they become, you know, overloaded.
But but an example, like I'm going to use another talk is is when you're building a robot you have to build a mind and that mind explicitly or implicitly represents the world and the robots and like you're forced to do this.
Every time you do it every time you want to make something operate an environment and then the very primitive ones have a very primitive model but you know some of them that I've worked on it let's say a Stanley which is a simple example.
What it did is that it had the sensing in the various modes of sensing lasers, GPS, will odometry whatever in an environment and it and vision and it fused all these information into a world map.
And then it knew the direction of gravity and in that little world map so it had this model of itself. It had the world and the world was very simple it was bad things good things and things I don't know at a tilt.
And then it ran that in a physics simulator and like how it and and so another thing people don't understand is that we aren't our intelligence where our emotions, those are what drive you not.
You know your intelligence is a how what is always by your emotion so so if you love like computers, given a certain intellectual level you're going to become very good at computers if that if that's what you really love.
And, and so Stanley's emotion system was GPS waypoints that's what drove it it wanted to get from one to another safely. And, and so that was its entire mind, its mind form the world of bad, bad good and I don't know on a, you know, on a tilt and it knew where it wanted to go next and next and next and it simulated itself going there and then it would actually take that step in the world.
We're very much like that you see your simulation.
When you're asleep, which I just was.
And, you know, and I like to say when you wake up your dreams don't stop.
They, they just connect to a data source which is the world, but you look at Stanley's mind. And, and this is all mine so it's a very simple mind but what can it understand all it understands is kind of like bad good don't know.
You know, direction of world. Well, if you wanted to explain.
I, I, you know, maybe it's Dostoevsky, you know, crime and punishment, you know what this the person takes a very bad rash move. Well, a rash move to Stanley if you wanted to explain this Stanley the robot.
A rash move would be cutting across the I don't know land to take a shortcut to the to, you know, not go the long way around but jump to a further waypoint. Well, that's a, a rash move.
And so you could explain Dostoevsky, you know, crime and punishment to the robot, but not really its brain cannot understand this because it's not rich enough.
Well, to rounding error, that's us.
So we will never understand the world under this theory, but we can create the self operating agents who have a certain level and can do certain things in the same way if you have your cat, it will never understand, you know, you play it.
Dostoevsky, it will never understand it because it doesn't have, but if you could talk its language you could say well it's like a kitten that doesn't listen to its mother, and it's going to get in trouble.
And then the cat will say I understand you go no you don't.
We're the same like to rounding error we're the same will never understand the world.
All we get is a model of the world and that model is a causal model that like Stanley that allows us Stanley needed to drive across the desert we need to operate in a social simulation.
We will never understand the world and physics there'll be no final theories because we don't even see the what needs to have a theory of it.
None of that circuitry. And, you know, so we can create these machines when we we can create a machine that has this interior model that socially simulates and it will become conscious and aware.
And it might be more powerful us and we can say well what's the real structure or meaning of the universe and it will say well it's, it's like a kitten that's going in danger.
We won't be able to understand what it understands after a certain level.
I totally agree with the last point we will not be able to understand the machine conscious machine if you ever be conscious.
At that point, I would like probably to return to one of my slides which I prepared for our talk.
Please bring up this slide which I selected. I don't know if you see it.
The Turing Continuum.
Yes, Turing Continuum. So I actually thought about it for a while and I think that Turing Continuum consists of Turing himself was working on very noble things like solving mathematical tasks.
And by the way, our listeners, our viewers, please get caught and answer my question.
Going back to Turing. Turing Continuum is from virtual world to physical world and from non-verbal interaction to verbal interaction.
And Turing was concentrating on the left upper corner of this with this noble task of solving mathematical tasks, cryptography, talking, playing chess, learning languages, whatever gentlemen are doing.
But he gave no attention to the physical world because he considered it too complicated for realizing in robotics.
And I think that what I thought in goodie talk before and what I heard right now in Gary talk that physical world and interaction with physical world in a verbal and non-verbal way is might hold the key to creating thinking machine.
It's maybe archival position and of course you can argue with this. But Gary, I would like a little bit you to elaborate on very briefly, but very clearly, please excuse us that late night in the United States.
That's early morning.
What we should avoid and be very careful in exploration of disabilities which artificial intelligence is creating for us. What we should be avoiding for next 70 years? What is dangerous?
I think what's dangerous is not going fast enough with AI. I think humanity's survival depends on getting these techniques fast for design.
I'm not one of these that's worried about that AI is going to kill us. I'm worried on the other side that we're not doing it fast enough.
Also, I'm not worried that AI will kill us. We're a technology. The primary thing of our species is we create technologies.
If that's not stable and survivable, so what? Then we die. That was what nature intended because that's what we do.
And yet we can't shy back from this. We have to accelerate it in my view. And I'm not really worried. Also, if AI turns on us and decides to destroy us, well, then we succeeded, right?
We created another species that can live without us. That'll be our greatest day.
So I'm on the total opposite side of this. We should be applying much more efforts and funding and acceleration and just go all out.
We need the robots. The human labor is failing around the world. There's not a population problem. There's an underpopulation problem that's rapidly going to hit us.
Yeah, it's going to hit us like a freight train and we need the labor. We need the AI's ability to solve environmental problems, global warming problems, energy problems.
We just cannot go fast enough.
Thank you very much, Gary. Okay, I'm going. Please bring up my slide with chewing continuum. Is it possible?
For me to bring it up?
Yeah, yeah, yeah.
I have it.
No, no, no. Gary, it's all right. So thank you very much, Gary.
And chewing continuum also allows us to do one thing. I sought out all possible chewing tests around this chewing continuum and you can see that all chewing tests which created for the last 70 years are a group in the upper left corner
and almost nothing in physical world and nonverbal interaction, which is also very important because dark matter of culture is what is our, I would say, nonverbal interaction might be as important as verbal interaction, chatbots and some other things.
Also, please take some time to answer my question.
And I would like to go to Gurdeep Pao. Gurdeep, I listened to your presentation as careful as I could in terms of, I'm also one of organizers of my journey.
And I know that Microsoft is betting on GPT-3. It's good to know that. But that we also know for sure, and I think Gary mentioned it a little bit, that GPT-3 is at least not energy efficient, as well as all other frameworks in terms of, I would say, chatbots, computer vision.
So my mind right now is consuming only 25 watts of energy. And I'm not that smart as GPT-3, maybe, but I'm extremely energy efficient.
So for today I have only one cup of coffee and it's just enough for me to run all day. And robots need a battery replaced maybe eight hours, at least in hours.
Berbank mobile robots need a battery replaced quite often.
So Gurdeep, do you think that this bottleneck, energy bottleneck is a very serious bottleneck for the next 70 years of development artificial intelligence?
And those GPT-13 will be ever the energy efficient as human. And of course, you can also help us to liberate what was wrong for the last 70 years.
So Gurdeep, please.
Great. So I was thinking about your question about last 70 years, and I would say that humans are very resourceful creatures.
And we approached, after the Alan Turing famous paper, we approached artificial intelligence in a very resourceful manner as well.
Now, what I mean by that is that we basically immediately looked at the problem and said, we understand logic, we understand rules, so we took that bit and we ran with it.
And then we realized, well, we can only get this far with artificial intelligence with this sort of rules-based approach.
Then things sort of quietened down, and then we realized, well, we know math pretty well, so let's start using some of these math-based techniques, k-nearest neighbor, some of the clustering approaches,
and sort of this classic data-driven machine learning came about.
Sorry. So I was saying, we're pretty resourceful creatures, and now because there is so much digitized data and there is so much compute, of course, we are leaning heavily on deep learning methods.
And GPT-3, I completely agree with you, it reminds me of this expression, more thrust Scotty, which is basically saying that let's throw more and more at it in terms of compute and data.
And of course, the performance is going to get better.
So that sort of, I think, has been sort of the journey of the last 70 years, but obviously this is not a sustainable model as well.
And I think that if there is one thing I would look at sort of critically in the last 70 years is that we tend to get obsessed in the community with one method or the other.
And we go through these different waves, and right now we are on the deep learning wave, which I'm a huge fan of, and I think as Gary said, it's a deep learning and these neural network architectures are great function approximators.
But we have to take a step back and say, what are the big gaps that exist, and are we making sufficient progress in addressing those gaps versus just pushing forward with more energy and more data.
And I think towards one of my last slides, I talked about some of the areas we need to make progress on.
You know, specifically with the area of power consumption, you know, I think that, you know, since we are inspired by the brain, let's look at the human brain and see, you know, how it deals with, you know, so work so efficiently.
And I think part of it is that it is actually reasoning with a lot of these sort of what we could call model based approaches, where it doesn't need a lots of data to understand that if I throw an object up, it's going to come down, because it has fundamental notions of physics.
Similarly, you know, I think, you know, some of the sparse network based approaches are something that, you know, I think are pretty important that I think we need to pay attention to, you know, just because we have power doesn't mean we should, you know, go waste it.
You know, the human brain, for example, does amazing things, even on power with things like heavy and sort of learning and, you know, when it finds like two adjacent neurons firing it sort of short circuits them.
So, I think that, you know, certainly we should be impressed by GPT three, but I think it is just a very small stop on a very long journey.
And we need to always keep these things in perspective that, you know, what are we really trying to solve for what are we optimizing for as a, you know, as I said earlier in the talk.
I think if we define that clearly, I think the next 70 years we'll make tremendous progress.
So, I have a question for a good debate that might be go later.
Right now, there is a famous saying of Mark Anderson on software is the world.
So, probably everybody heard of it means every engineering complexity sooner or later defined by software and actually universal truing machine.
But for us is actually software is not only one part of equation because software is robot for breakfast, culture for lunch and knowledge for dinner.
So, software is actually it's everything, including culture, including knowledge and everybody knows that deep Kasparov lost to computer in 1997.
And it was kind of beginning of victories for computers and artificial intelligence in board games, even they were playing board games a little bit earlier.
But in 1997, it was not a computer was playing against Kasparov.
It was chess culture, which was accumulated in that IBM computer was playing against Kasparov.
And in 2018, it was go culture rules of go matches, which are alpha go zero playing played against each other, which were was kind of playing against Lysidol.
So, and I believe that we have time when our artificial intelligence systems can just accumulate our culture, make huge, huge storage of everything we say, everything we do, and then sort it out and then make a response, a response.
And also our questions based on the whole humanity history.
So, now I'm going to you, Michael.
And actually, I remember something similar in your book, which you wrote on limits of artificial intelligence right now, but not going to your book, but going to my two original questions.
I would like to say what was wrong for the last 70 years.
By the way, what is the distance between Oxford and Cambridge about 60 miles 60 miles.
Well, that's the physical the physical distance, the emotional distance is far, far greater than that.
Yeah, I understand where well this by the way about emotional distance.
I have the same situation in Scotland where I graduated and we said we usually say that funerals in Glasgow, a lot of Mary's and Edinburgh marriage.
Yes, emotional distance might be more, but I thought that Alan Turing was running distance something like 15 miles almost every week.
So it's maybe a quarter or something of distance between Cambridge and Oxford.
You are from Oxford and I want you across from Cambridge.
So this emotional distance between two schools actually quite quite important right now.
But going back to the question, what was wrong for the last 70 years, except the fact that I want you from from from Cambridge.
So I think there's a couple of things and I think my answer will resonate with what Gary said and and and others today.
So the first thing is, I think there's an obsession in a I historically was discovering one idea and thinking that one idea that one technique is going to be a magic ingredient which takes you all the way.
And so, as, as, as you said, Albert, as Gary said, I mean, for a long time, it was the idea that knowledge, knowledge representation, we just write, we need the right way to find human knowledge and to write that down.
And if we can just give that to machines, then that will take us all the way. So I mean, there was a famous experiment, the psych experiment, where they tried to code up the entirety of human consensus knowledge.
And the idea was, if you could give that to a machine, then, you know, AI would be solved.
So it's that idea that there's one kind of one single technique, you know, we have one advanced. Wow, this is it. This is the magic ingredient and that's going to take us all the way.
And it doesn't. Right. I mean, it just doesn't. And I guess I'm, I'm very, I'm hugely impressed as I'm as impressed as anybody is by the current advances in AI, the things that are going on in deep learning.
These are real and they cause for excitement. I mean, I think they are real advances, but they are narrow advances and they are ingredients, but they're not the whole ingredient.
They're not going to take us the entirety of the way to, to artificial intelligence. So I think that's the first thing. That's the first lesson, right?
Don't imagine that there's one single ingredient, which is going to take you all the way.
Second, I think this is quite interesting. The examples we've seen in the example on your slide, things like chess and go.
Those, why, why do AI researchers study problems like chess and go and proving mathematical theorems?
They do it because those are the things that they regard as requiring intelligence, as requiring genius, right? Those, you know, being a good mathematician is something that, you know, lots of computer scientists aspire to be.
So they look at those problems and they say, well, if we can solve, you know, if we can prove mathematical theorems with a computer, then we must have solved the intelligence problem.
But actually the truth is that's not where a lot of the hard problems are. So to go back to the driverless car problem that Gary was talking about earlier and I'd be interested to see whether Gary disagrees with me or not.
What is the hard problem in driverless cars? Is it knowing whether to speed up or slow down or to turn left or to turn your indicators on?
No, I don't think so. The problems with driverless cars are knowing where you are and what's around you, what's going on in your environment, right?
All of the problems that my guess anyway is that Gary and his team had to solve back in 2005 were to do with perceiving your environment.
If you have all that information, then knowing whether to speed up or slow down and so on is going to be, is going to be easy.
Actually, that part of it, I think, is a relatively straightforward and conventional bit of computer code.
So perception and actually that's where neural networks, the current wave of AI technology, that's where it's turned out to be very, very good at dealing with problems related to perception.
Problems in computer vision, in understanding speech and so on, these were fiercely difficult problems and that's where that technology has proved to be very, very successful.
And the third thing I think that the third historical mistake is not understanding that where the difficult problems are is the world, right?
We have evolved over billions of years to succeed in inhabiting the physical environment that we inhabit, the planet Earth and the narrow bit of planet Earth that we do actually inhabit.
Imagining that you can build a neural network and train it up in the lab over a couple of days and that that's going to be as good at dealing with the world as we are, I think is just a huge, huge, huge mistake.
So the world is really, really important. Let's go back to GPT-3. So what is GPT-3? GPT-3 is this program developed by OpenAI, I believe, and it was trained by giving this program huge numbers of texts, vast amounts of written texts.
So let's take an example, something that you might want to do, something that Gary might want to do for breakfast is make an omelet, right?
Completely routine task for a human being, making an omelet. So GPT-3 has read every omelet recipe that's ever been written, right?
There's no recipe for an omelet out there. It's written every, it's read every essay about omelets. It's probably read books about omelets.
So in terms of just knowledge, it must surely have all the knowledge about omelets that there exists in the world. But could it make an omelet? No, of course it couldn't.
It doesn't know anything about omelets because omelets are things that exist in the real world. For it, omelet is just a symbol, but it's seen time and time again in these huge numbers of repositories that have been thrown at it.
None of that knowledge that is coded into GPT-3, none of that stuff is actually grounded in any experience of the world that we all have.
For me, the word omelet represents every experience I've ever had with an omelet, every omelet I've ever tried to make, every egg that I've ever tried to break to go into an omelet, every successful and bad omelet.
It reminds me of an omelet I had in Paris in 1997 and so on, right? In that sense, the concept of omelets means something to me because it's grounded in my experiences with the world.
GPT-3 doesn't have any experience of the world to ground itself in. So in that respect, it's just a disembodied, I mean, it's a very, to be clear, it's an incredibly impressive feat of engineering and people will do really, really cool things with it.
But it doesn't understand, I think, just to reiterate a point. And I say it doesn't understand because it can't. It doesn't have any experience of the world.
So those are the three things, I think, where, you know, this obsessing on one idea and imagining that there's one single technique, which is going to take you just kind of the holy grail of AI.
That's a mistake. Focusing on problems which actually you might find impressive as requiring intelligence in people, but actually is not where the real hard problems are.
I think that historically has been a mistake. And finally, not realizing the importance of dealing with the world.
You know, experience, human experience, human knowledge, everything about the human condition is grounded in our experiences in the human world.
And I think if we ever succeed in building machines that are self aware and conscious and sentient and all of those things, machines that have understanding, then that understanding will have to be grounded in the world in the same way.
Thank you very much, Michael.
So going back to some ideas which I said before that you see Alan Turing, I don't know if you see this slide, but Alan Turing said that board games, talking is very important for artificial intelligence.
But he said that there is real human pleasures, food, sport and sex. And those are very important actually for understanding the world, exactly as you, Michael said.
So I think that without understanding those three, we will not have any intelligence at all.
And at that point, I'm going back to Gurdjie because I was asking questions on the role of embodied intelligence and robotics.
Is it important for the future research in artificial intelligence, Gurdjie?
Together, Sber and Microsoft Research did wonderful projects using reinforcement learning in robotics.
Then I would like to pose this question now openly to anybody who would like to answer this, Leonid, Gary, Michael.
But my question is how important embodied intelligence to study perception, cognition, machines and how important it is for research of general artificial intelligence
and artificial intelligence with specific applications. So can we create artificial intelligence with no embodiment?
Or we really need embodied machines to advance fully in that direction? So who would like to answer this question?
Leonid, first, Michael, please.
Yeah, I'll be short. I actually want to support Michael in this in the sense that in order to build truly artificial intelligence, machines should gain the experience that human has.
And what's the best way to gain an experience than to explore the world and to explore the world and to do it autonomously?
Yes, you need to have those type of systems, those embedded systems with intelligence.
And when you look at, for example, the robot dogs running around outside Boston, you realize that we're actually pretty close to the phase where the systems will be exploring the world on their own,
gaining the experience, learning what omelette is eventually, and hopefully will not only read about it, but try it.
Thank you. We will soon have some surprise with robot dogs around here, but we should be patient with this. Michael, please.
Yeah, so I'm going to bring in a slightly different sort of aspect to this problem. So there's a famous six-word exchange that was formulated by Stephen Pinker, the psychologist and linguist.
And it goes like this. So Bob says, I'm leaving you. And then says, who is she? So the six words, right? And everybody who hears those six words immediately has a rich mental picture of what's going on as people, we all understand that, right?
Maybe somebody listening to this actually just lived it last night. I'm very sorry if that's the case. So what's going on there? How do we understand that?
We understand that because we've got experience of the human world, right? We understand about human relationships, human beliefs, human desires and so on. And nobody trained us in that.
That's just our experience as human beings living as social animals that have relationships with people in the human world.
Now, this is a big problem for AI. If AI, you know, suppose you're talking on the telephone to an AI system which decides whether you get a loan or not.
And it says, no, you're not going to have a loan. It must surely be, if it's going to be any use, it has to be able to understand that this is going to make you upset or angry and so on.
I mean, an AI system that would be a doctor, right? An AI system that took the place of a human doctor would have to be able to understand the intricacies of human relationships because being a doctor is not just knowing about diseases and illnesses,
it's knowing about people and the lengthy training process that doctors go through is every bit as much about training them to deal with people and to understand the people that they're dealing with and what kind of treatment regimes are going to work for them and so on.
As individuals, understanding about their personal circumstances, their relationships, you know, all of that kind of stuff.
So we all have that because we've been trained both genetically through billions of years of evolution and since birth, right, where, you know, where our parents teach us right from wrong and so on and teaches about what's acceptable in relationships and so on.
We all learn about that because we're part of the human world. How are we going to give that stuff to machines? I think it's a big, I mean, technically it's called theory of mind, right?
How are we going to give machines a theory of mind so they can understand what that six word dialogue means? I'm leaving you. Who is she?
Sometimes I also don't know who she is.
You know, for, I'm unclear whether embodiment, again, there's many kinds of mind, right? And so, you know, I'm unclear to the extent of, you know, we can be disembodied but embodied in a virtual world, right?
So, you know, it can all be happening in a cloud but I do think embodiment does give you certain, like, things which are primary to us, like the fear of death and the need to survive and that's a lot of what we're based on.
And so we form this causal model that's necessary for our existence and then we press that causal model for everything else we think we understand.
And so if we want AIs that are intelligences that are going to be useful to us, they must share certain, like, embodiments and, you know, the same kinds of things that we do.
And it's unclear, you know, how much a giant body of associations, like GDP3, it's kind of shocking how far it can carry you and yet there's like no soul in the structure.
It's just a chain of associations, right, between, and so you can, it has all these brittle points and unexpected failures that are hard to predict versus, you know, with us we're kind of regularized in our environment by our physical, you know, need to understanding of how we would get damaged, how we would survive.
What's particular with humans is a large part of our mind is devoted to a social world, which is what we created. And so we're always modeling our place in the social world and how to interact and survive with other entities in it.
And that's a very key thing for us. In fact, it is a large amount of our whole brain and understanding. And again, you see this social world in your dreams, you fully simulate other people and your interaction and your hierarchical place with them in your dreams.
So a large part of our mind is devoted to that. And so to that extent, like, I think, like if we want Roba, I mean, we want intelligences, they have to sort of be embodied with us. And, you know, if we want them for the human factors, there are many things like protein folding and whatever that are a very specialized area.
And yeah, we want, we just want a machine to do that. Again, such a machine really needs to explicitly or implicitly embody a kind of internal causal model of folding if we really want it not to just be associational to sort of mostly like getting a lot of stuff right but not really fundamentally understanding the 3D chemical world that these things are in.
And it would just be a kind of weird associational manifold over chemistry without the sort of physics and the causal physics that it really needs to understand it. So, so in that sense, it should embody a causal model of the world upon which it grounds its meaning.
And that's what's GDP 3 lacks. It lacks any of this grounding of meaning, right?
Any model lacks actually.
Yes. But, but, but, you know, it's not that the problem isn't that models lack this is that you want a model that spans your space, because we're always going to lack like there is no super intelligence that that can know everything and every aspect it's always going to be
grounded to a causal situation. And when you try to go too far out of that, it's just going to break it won't you won't be able to expand this mine indefinitely. It will be based and built upon a causal structure, and that will be its limits forever.
You need a new mind to to go into some completely new area. But I could write books about this but
Yes, please do.
I might.
Okay, so you see now I actually before I kind of knew that our discussion will go this way. So this slide is summarized three ways to build artificial general intelligence.
First one we discussed a lot is representation of symbolic approach to the description of the world.
Second is connection is based on basically neural networks deep learning neural networks. And the third one is embedded intelligence and kind of combination of maybe two of these robots.
So I would like briefly to any of you ask, which, which approach you believe the most, or might be non and there is a fourth approach, or maybe only combination of all three will bring us forward.
So I'll start with Leonid with you and then Gary and then Michael, please.
What is the most important approach here to me right now than the most nearest future is is I believe lies in this newer symbolic computations right so which is in some sense connecting symbolic representation and connection ism and to me that's probably where the next advances will happen and that's the sort of I think the most fruitful direction for the
near future going forward. Yes, embodiment definitely will join to actually gain those experience and, you know, the meaning and understanding of the world around us physical world.
Yes, I think you are in line with what Alan Turing said, because he said robots are very important for future development of thinking machine, but they're not ready here to be built here because there is no spare parts for them.
And it's not very clear how to hit them.
Gary, please.
Well, I've, you know, for a GI. I first of all, I don't believe in a GI. It's a si a specific intelligence. I don't think we have a general intelligence because we use our internal structures to extend to like what is a quanta, you know, what is a particle it goes as a wave and a particle.
Down at the small scale, it does not. That's just where our metaphor that we have for our brain breaks down. And, and so I do think you need all of these things.
I think the symbolic representations derive from, from this kind of this necessity to build a world model for yourself. And, and it's those parts that give you the symbols.
Again, like this would extend to people think, well, math is universal. And I go, no, no, math only exists in here. It's not a property of the world. It's a human property.
And it exists in the human mind and it works for our models.
But, but it's not some magical thing that existed out in the universe that we tap into it. It's a structure that works in our causal models. And it breaks down like most of math is unknowable to us.
Like, most thing, the vast majority of the universe is unexpressible by, by even unknowable by our functions and symbols right, but causally at our scale at our time, we have a very flexible model that can extend.
So, you know, the quick answer is we need all these things and they'll always be relative to the job we want it to solve. So to achieve artificial specific intelligence, you know, that can be do very general tasks, but there's no such thing as this universal intelligence.
It's always in terms of learn grounded models, in my opinion.
Thank you very much, Gary.
Michael, please.
Yeah, so like Gary, I am a skeptic about AGI and I don't see anything on the table at the moment, which is, which is going to take us to to AGI.
I think all of those things are important.
Connectionism symbolic representations. I mean, I think it's pretty widely accepted now that symbolic approaches and connectionism, I think need to talk to each other.
I don't think the symbolic approaches that were popular 30 years ago are likely to be the way forward. So I don't think that's going to be it, but there will be some form of symbolic reasoning that's that's that's got to go on.
And I think what I would say is the reason I'm a skeptic about AGI is that, again, going back to a point I made earlier, you know, we are the product of billions of years of reinforcement learning with thousands of generations of our ancestors.
And Mother Nature has tuned us over that immense period of time to be able to be generally intelligent in the world that we're in.
The current techniques, you know, the headline techniques like AlphaGo and the chess playing programs that are so successful and and so on, these are all based on reinforcement learning where a program just experiments and gets feedback.
It tries to do something. It does badly and in a computer game, for example, right, a computer game like space invaders.
You can have a program which learns to play the game of space invaders, but actually it does that by just essentially starting by moving randomly seeing what works and what doesn't work.
And when something works when it gets a good score, it does that again. That's reinforcement learning. It's just over time playing and playing and playing good.
The problem is reinforcement learning doesn't work in the real world. Gary couldn't have used reinforcement learning to train his cars in 2005, right, because they would have got through a great many cars.
These cars wouldn't have got anywhere. Reinforcement learning doesn't work. The kind of reinforcement learning which is so successful in games, for example, virtual environments is very, very successful there, but doesn't work in the real world.
So I'm somewhat of a skeptic about AGI for that reason. I mean, we are the product of reinforcement learning, but it's billions of years of evolution which have given us a kind of reinforcement learning to generate human beings that can successfully occupy
the human world.
In Sberbank, we recently published a book, and probably half of this book are presented on this conference, and it says in Russian, strong artificial intelligence, and actually the book was written on artificial
general intelligence, AGI.
Well, not to go deep into the story, but here we are actually putting a lot of efforts in understanding how to advance science further on.
And I think our discussion, like we just had before, is a very important step on not only advancing artificial intelligence research in general, but also finding bridges between our communities here in Russia,
the United States, England, Great Britain, and I think that it's very important for us to listen to, and especially in the conditions that we cannot travel, we cannot go to our countries to visit us,
And believe me, Gary and Michael, you will not regret if you come to Russia and enjoy. Gary being here quite a few times with us, visit us, and we always like to welcome you here.
But Michael, when you have time, certainly you should visit us and you will enjoy staying in Russia, in Moscow, and visiting Sberbank laboratories, which quite a few of them.
And finally, Leonid will join this invitation and extend to all of you.
I will remember that invitation.
Yes, please. We love guests here.
I hope that the pandemic and COVID will finish quite soon and we will have opportunity to travel to each other and also to send our researchers to study with each other, because we welcome your researchers here to study with us and to do research.
And I'm sure that our researchers will be more than happy to visit Britain, the United States.
In robotics laboratory, we have five people visiting Microsoft Research, and for almost half a year studying there, artificial intelligence and reinforcement, you're learning and applications, these two robotics.
So Gurdeep is not with us, so he will not tell us about it, but I'm sure that my colleagues from robotics laboratory are explaining in sections for this conference.
I would like to summarize what we said today, and the general idea which we just discussed is that we need all three approaches for building artificial, general intelligence, strong artificial intelligence, symbolic approach,
connectionism approach, embodied approach, but the phases of this might be different.
So it might be all together or it might be first approach where we combine best parts of symbolic and connectionism together to build a very solid foundation as Leonid said before.
So the one thing which Mike also said very important to us is that we should not focus on only one thing.
Many years ago, Chinese leader Mao Zedong said 100 flowers should blossom.
So probably for artificial intelligence, we need more than 100 flowers, and we need more than 100 speakers, and we also need to exchange embodied approach and make conference dread again.
So thank you very much for spending time with us, and I would like to thank you, all our visitors, all our listeners, viewers, which I'm sorry for technical troubles, I'm sorry to all of you, you'll be patient, our guestware patient.
And I have wonderful team and I hope that next year we will have you and maybe some other excellent speakers for my panel where we discuss something else, I don't know what.
So thank you very much.
