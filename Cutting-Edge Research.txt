
Cutting-Edge ResearchOpen sidebarCutting-Edge ResearchModel: Default (GPT-3.5)Summarize: Nomadic Pict
Implementation and Semantics of Mobile Agent Infrastructure
The Nomadic Pict project studied programming language design and communication primitives for mobile computation. Communication primitives for interaction between mobile agents can be classified into two levels of abstraction. At a low level there are location dependent primitives that require an programmer to know the current site of a mobile agent in order to communicate with it. At a high level there are location independent primitives that allow communication with a mobile agent irrespective of its current site and of any migrations. Implementation of these requires delicate distributed infrastructure. We proposed a simple calculus of agents that allows implementations of such distributed infrastructure algorithms to be expressed as encodings, or compilations, of the whole calculus into the fragment with only location dependent communication. These encodings give executable descriptions of the algorithms, providing a clean implementation strategy for prototype languages. The calculus is equipped with a precise semantics, providing a solid basis for understanding the algorithms and for reasoning about their correctness and robustness.
We implemented a language, Nomadic Pict, based on the calculus to enable experiments with distributed algorithms and agent computing. Nomadic Pict is an extension of Pict with the notion of locations, agents, migration, distribution, and failures. At the same time we developed reasoning techniques for correctness and robustness proofs of these algorithms.

Those involved include

Ken Moody
Peter Sewell
Benjamin Pierce (University of Pennsylvania)
Asis Unypoth
Pawel T. Wojciechowski (Poznan)
An alpha release of the implementation is available from the language web page. It includes a tutorial.
Some relevant papers are below; others can be found on the individual pages above.

Nomadic Pict: Programming Languages, Communication Infrastructure Overlays, and Semantics for Mobile Computation. Sewell, Wojciechowski, and Unyapoth. In TOPLAS 2010 32(4).
Verifying Overlay Networks for Relocatable Computations (or: Nomadic Pict, relocated). Sewell and Wojciechowski. Position paper for the MSR/HP Labs research meeting on The Rise and Rise of the Declarative Datacentre, May 2008.
Nomadic Pi Calculi: Expressing and Verifying Infrastructure for Mobile Computation, Asis Unyapoth, Ph.D. thesis. Appeared as Technical Report 514, Computer Laboratory, University of Cambridge, June 2001. Also in ps and pdf. Here is the abstract .
Algorithms for Location-Independent Communication between Mobile Agents. Pawel Wojciechowski. Technical Report 2001/13, Communication Systems Department, EPFL, March 2001. Abstract. In pdf. Here is a shorter version which appeared in the AISB'01 / Symposium on Software mobility and adaptive behaviour. These are copied in Cambridge: ps.gz, pdf, and abstract .
Nomadic Pict: Correct Communication Infrastructure for Mobile Computation, Asis Unypoth and Peter Sewell. In POPL 2001. Also in pdf, dvi, and ps.
Nomadic Pict: Language and Infrastructure Design for Mobile Computation . Pawel Wojciechowski, Ph.D. thesis. Appeared as Technical Report 492, Computer Laboratory, University of Cambridge, June 2000. Also in pdf. Here is the abstract . These are copied in Cambridge: ps.gz, pdf, and abstract .
Nomadic Pict: Language and Infrastructure Design for Mobile Agents, Pawel Wojciechowski and Peter Sewell. This appeared in ASA/MA'99 (First International Symposium on Agent Systems and Applications/Third International Symposium on Mobile Agents), October 1999. An extended version appeared in IEEE Concurrency vol 8 no 2, 2000.
Location-Independent Communication for Mobile Agents: a Two-Level Architecture, Peter Sewell, Pawel Wojciechowski and Benjamin Pierce. Technical Report 462, Computer Laboratory, University of Cambridge. A version of this appeared in Internet Programming Languages, LNCS 1686.
Location Independence for Mobile Agents, Peter Sewell, Pawel Wojciechowski and Benjamin Pierce. This appeared in the Workshop on Internet Programming Languages, 1998. It is largely superseded by Technical Report 462 above.
Peter.Sewell@cl.cam.ac.uk
[Validate this page.]1 / 1ChatGPTChatGPTThe Nomadic Pict project was a research initiative focused on programming language design and communication methods for mobile computation. It developed the concept of location-dependent and location-independent primitives for interaction between mobile agents. The former requires the programmer to know the agent's current site for communication, while the latter allows communication irrespective of its location and migrations.The project resulted in the proposal of a calculus of agents, which provides a framework to express implementations of distributed infrastructure algorithms. This allowed for encodings or compilations of the whole calculus into the fragment with location-dependent communication only. It also provided a solid basis for1 / 1Action Calculi
Action calculi provide a framework for exploring many models of interactive behaviour, such as the pi calculus and its extensions, lambda calculi, calculi for security and object-oriented calculi.
Papers
Here are pointers to a few papers. The list is not comprehensive; for more details see the individual web pages listed below.

A general introduction to action calculi can be found in:
Calculi for Interaction, Robin Milner, Acta Informatica 33 1996;
Higher-order Action Calculi, Robin Milner, which allows processes as well as names to be passed along channels;
Reflexive Action Calculi, Robin Milner, which in the presence of higher-order features gives recursion.
Models of action calculi are given in:
Control Structures , Alex Mifsud, Robin Milner and John Power;
Elementary Control Structures, John Power.
Connections between action calculi, type theory and category theory can be found in:
Types and Models for Higher-order Action Calculi, Philippa Gardner and Masahito Hasegawa;
From Action Calculi to Linear Logic, Andrew Barber, Philippa Gardner, Masahito Hasegawa and Gordon Plotkin, which gives a propositions-as-types account of action calculi and shows the connection with linear logic.
Some preliminary work on deriving labelled transition relations and bisimulation congruences can be found here:
From Rewrite Rules to Bisimulation Congruences, Peter Sewell.
Bisimulation congruences based on Relative Pushouts are introduced here:
Deriving Bisimulation Congruences for Reactive Systems, James J. Leifer and Robin Milner.
People
A partial list of people who have worked with action calculi:
Andrew Barber
Adriana Compangnoni
Philippa Gardner
Masahito Hasegawa
Claudio Hermida
Kohei Honda
Ole Jensen
Jamey Leifer
Alex Mifsud
Robin Milner
Dusko Pavlovic
Gordon Plotkin
John Power
Peter Sewell
Project
The study of action calculi at Cambridge formed part of a larger project, funded by the EPSRC from 1998 to 2001 (Calculi for Interactive Systems: Theory and Experiment, GR/L62290), consisting of two interlinked strands:
the development of action calculi, in particular focussing on the dynamics;
the design and study of foundational calculi based upon the pi calculus, and accompanying prototype programming languages, in which to describe and analyse migratory distributed systems. Some of this work can be found on the Nomadic Pict page.
The final report for this grant can be found here. Research from 1/1995 to 5/1998 was supported by the EPSRC grant Action Structures and the Pi-Calculus (GR/K38403); here is a summary of the final report.
Any comments and additions are welcome.
Peter.Sewell@cl.cam.ac.uk
[Validate this page.]1 / 1ChatGPTChatGPTAction calculi is a theoretical framework that is used to explore various models of interactive behavior, including pi calculus and its extensions, lambda calculus, calculi for security, and object-oriented calculi. This concept has been used extensively in studies of interactive systems and has seen a wide range of applications.Several papers have been written on the subject, touching on topics such as higher-order action calculi, reflexive action calculi, and control structures. Some research has also been conducted on connections between action calculi, type theory, and category theory. Preliminary work has been done on deriving labelled transition relations and bisimulation congruences, and bisimulation congruences based on Relative Pushouts have also been introduced.The study of action calculi at the University of Cambridge was part of a larger project funded by the EPSRC from 1998 to 2001. This project was comprised of two interconnected strands: the development of action calculi, particularly focusing on dynamics; and the design and study of foundational calculi based on the pi calculus. This work was aimed at describing and analysing migratory distributed systems.Prominent researchers who have contributed to the field of action calculi include Robin Milner, Philippa Gardner, Peter Sewell, and many others. The project has resulted in numerous significant contributions to the understanding of interactive systems and the development of related programming languages.1 / 1mugshot
Peter Sewell
Professor of Computer Science, Computer Laboratory, University of Cambridge
Member of the Cambridge Programming, Logic, and Semantics Group
Fellow of Wolfson college
Here are my contact details, a photo, short bio, and CV

PhD students, RAs, and Co-authors   Meetings   Funding   Papers (by date)   Papers (by topic)   Software
Teaching
Part II and Part III/ACS Project suggestions for 2023-24
The 2022-23 Multicore Semantics and Programming (L304) ACS MPhil module and Part II Unit (with Tim Harris and a guest lecture by Christopher Pulte)
Multicore Semantics: Making Sense of Relaxed Memory (slides for the above)
...previous teaching
Research
My research aims to put the engineering of real-world computer systems on better foundations, developing techniques (both mathematically rigorous and pragmatically useful) to make systems that are better-understood, more robust, and more secure. This applied semantics needs tightly integrated theoretical and practical research, spanning a range of Computer Science: I work in architectural description, programming languages, networking, and security, developing and using techniques from semantics, type systems, automated reasoning, and concurrency theory.
REMS logoOur REMS: Rigorous Engineering for Mainstream Systems project, in collaboration with Systems, Security, Architecture, and Semantics researchers from Cambridge, Imperial, and Edinburgh, and with colleagues in Arm, IBM, and elsewhere, was originally funded by the eponymous EPSRC Programme Grant; it now continues with other funding, including the EU ERC Advanced Grant ELVER: Engineering with Logic and Verification: Mathematically Rigorous Engineering for Safe and Secure Computer Systems, and donations from Arm and Google.

The CHERI project, led by Robert Watson, Simon Moore, and myself at Cambridge, and Peter Neumann at SRI International, extends conventional hardware Instruction-Set Architectures (ISAs) with new architectural features to enable fine-grained memory protection and highly scalable software compartmentalization. It is a hardware/software/semantics co-design project, involving architecture design, hardware implementation, system software adaption, and formal semantics and mechanised proof of security properties. The UKRI Digital Security by Design Challenge, with Â£70m government and Â£117m industry funding from 2019--2024, is developing the industrial prototype Arm Morello CHERI-ARM 64-bit CPU, SoC, and board, to evaluate and demonstrate the mass-market industrial feasibility of CHERI. Here are our DSbD pages.
Recent Papers
Isla: Integrating full-scale ISA semantics and axiomatic concurrency models (extended version). Alasdair Armstrong, Brian Campbell, Ben Simner, Christopher Pulte, and Peter Sewell. Formal Methods in System Design, May 2023. [ bib | doi | pdf | http | abstract ]
The Arm Morello Evaluation Platform---Validating CHERI-Based Security in a High-Performance System. Richard Grisenthwaite, Graeme Barnes, Robert N. M. Watson, Simon W. Moore, Peter Sewell, and Jonathan Woodruff. IEEE Micro, 43(3):50--57, 2023. [ bib | doi | http | abstract ]
Improving Security with Hardware Support: CHERI and Arm's Morello. Robert N. M. Watson, Peter Sewell, and William Martin. The Next Wave (The National Security Agency's review of emerging technologies), 4(1):10--21, 2023. ISSN 2640-1789 (print), ISSN 2640-1797 (online). [ bib | project page | pdf | abstract ]
CN: Verifying systems C code with separation-logic refinement types. Christopher Pulte, Dhruv C. Makwana, Thomas Sewell, Kayvan Memarian, Peter Sewell, and Neel Krishnaswami. In POPL 2023. [ bib | doi | project page | pdf | abstract ]
Multicore Semantics: Making Sense of Relaxed Memory (MPhil slides), Peter Sewell, Christopher Pulte, Shaked Flur, Mark Batty, Luc Maranget, and Alasdair Armstrong, October 2022. [ bib | .pdf ]
Formal CHERI: rigorous engineering and design-time proof of full-scale architecture security properties, Peter Sewell, Thomas Bauereiss, Brian Campbell, and Robert N. M. Watson. Blog post, https://www.lightbluetouchpaper.org/2022/07/22/formal-cheri/, July 2022. [ bib | project page | http ]
Islaris: Verification of Machine Code Against Authoritative ISA Semantics. Michael Sammler, Angus Hammond, Rodolphe Lepigre, Brian Campbell, Jean Pichon-Pharabod, Derek Dreyer, Deepak Garg, and Peter Sewell. In PLDI 2022. [ bib | doi | project page | pdf | abstract ]
N3005: A Provenance-aware Memory Object Model for C. Working Draft Technical Specification ISO/IEC TS 6010:2023 (E), Jens Gustedt, Peter Sewell, Kayvan Memarian, Victor B. F. Gomes, and Martin Uecker. ISO/IEC JTC1/SC22/WG14 N3005 http://www.open-std.org/jtc1/sc22/wg14/www/docs/n3005.pdf, June 2022. [ bib | pdf | abstract ]
Verified Security for the Morello Capability-enhanced Prototype Arm Architecture. Thomas Bauereiss, Brian Campbell, Thomas Sewell, Alasdair Armstrong, Lawrence Esswood, Ian Stark, Graeme Barnes, Robert N. M. Watson, and Peter Sewell. In ESOP 2022. [ bib | doi | project page | pdf | http | abstract ]
Relaxed virtual memory in Armv8-A. Ben Simner, Alasdair Armstrong, Jean Pichon-Pharabod, Christopher Pulte, Richard Grisenthwaite, and Peter Sewell. In ESOP 2022. [ bib | doi | project page | pdf | http | abstract ]
Relaxed virtual memory in Armv8-A (extended version), Ben Simner, Alasdair Armstrong, Jean Pichon-Pharabod, Christopher Pulte, Richard Grisenthwaite, and Peter Sewell, March 2022. http://arxiv.org/abs/2203.00642. [ bib | project page | pdf | abstract ]
VIP: Verifying Real-World C Idioms with Integer-Pointer Casts. Rodolphe Lepigre, Michael Sammler, Kayvan Memarian, Robbert Krebbers, Derek Dreyer, and Peter Sewell. In POPL 2022, Proc. ACM Program. Lang. 6, POPL, Article 20. [ bib | doi | supplementary material | project page | pdf | abstract ]
Bad Reasons to Reject Good Papers, and vice versa, Peter Sewell. SIGPLAN PL Perspectives Blog, December 2021. Also published 2022-12-07 on the Communications of the ACM Blog https://cacm.acm.org/blogs/blog-cacm/267440-bad-reasons-to-reject-good-papers-and-vice-versa/fulltext. [ bib | http ]
Isla: Integrating full-scale ISA semantics and axiomatic concurrency models. Alasdair Armstrong, Brian Campbell, Ben Simner, Christopher Pulte, and Peter Sewell. In CAV 2021. [ bib | doi | pdf | http | abstract ]
N2676: A Provenance-aware Memory Object Model for C. Working Draft Technical Specification, Jens Gustedt, Peter Sewell, Kayvan Memarian, Victor B. F. Gomes, and Martin Uecker. ISO/IEC JTC1/SC22/WG14 N2676 http://www.open-std.org/jtc1/sc22/wg14/www/docs/n2676.pdf, March 2021. [ bib | pdf | abstract ]
Capability Hardware Enhanced RISC Instructions: CHERI Instruction-Set Architecture (Version 8). Robert N. M. Watson, Peter G. Neumann, Jonathan Woodruff, Michael Roe, Hesham Almatary, Jonathan Anderson, John Baldwin, Graeme Barnes, David Chisnall, Jessica Clarke, Brooks Davis, Lee Eisen, Nathaniel Wesley Filardo, Richard Grisenthwaite, Alexandre Joannou, Ben Laurie, A. Theodore Markettos, Simon W. Moore, Steven J. Murdoch, Kyndylan Nienhuis, Robert Norton, Alexander Richardson, Peter Rugg, Peter Sewell, Stacey Son, and Hongyan Xia. Technical Report UCAM-CL-TR-951, University of Cambridge, Computer Laboratory, October 2020. [ bib | doi | project page | pdf | abstract ]
N2577: A Provenance-aware Memory Object Model for C. Working Draft Technical Specification, Jens Gustedt, Peter Sewell, Kayvan Memarian, Victor B. F. Gomes, and Martin Uecker. ISO/IEC JTC1/SC22/WG14 N2577 http://www.open-std.org/jtc1/sc22/wg14/www/docs/n2577.pdf, September 2020. [ bib | pdf | abstract ]
CHERI C/C++ Programming Guide. Robert N. M. Watson, Alexander Richardson, Brooks Davis, John Baldwin, David Chisnall, Jessica Clarke, Nathaniel Filardo, Simon W. Moore, Edward Napierala, Peter Sewell, and Peter G. Neumann. Technical Report UCAM-CL-TR-947, University of Cambridge, Computer Laboratory, June 2020. [ bib | project page | pdf | abstract ]
Cornucopia: Temporal Safety for CHERI Heaps. Nathaniel Wesley Filardo, Brett F. Gutstein, Jonathan Woodruff, Sam Ainsworth, Lucian Paul-Trifu, Brooks Davis, Hongyan Xia, Edward Tomasz Napierala, Alexander Richardson, John Baldwin, David Chisnall, Jessica Clarke, Khilan Gudka, Alexandre Joannou, A. Theodore Markettos, Alfredo Mazzinghi, Robert M. Norton, Michael Roe, Peter Sewell, Stacey Son, Timothy M. Jones, Simon W. Moore, Peter G. Neumann, and Robert N. M. Watson. In Security and Privacy 2020. [ bib | doi | project page | pdf | abstract ]
Rigorous engineering for hardware security: Formal modelling and proof in the CHERI design and implementation process. Kyndylan Nienhuis, Alexandre Joannou, Thomas Bauereiss, Anthony Fox, Michael Roe, Brian Campbell, Matthew Naylor, Robert M. Norton, Simon W. Moore, Peter G. Neumann, Ian Stark, Robert N. M. Watson, and Peter Sewell. In Security and Privacy 2020. [ bib | doi | project page | pdf | abstract ]
ARMv8-A system semantics: instruction fetch in relaxed architectures. Ben Simner, Shaked Flur, Christopher Pulte, Alasdair Armstrong, Jean Pichon-Pharabod, Luc Maranget, and Peter Sewell. In ESOP 2020. [ bib | project page | pdf | abstract ]
An Introduction to CHERI. Robert N. M. Watson, Simon W. Moore, Peter Sewell, and Peter Neumann. Technical Report UCAM-CL-TR-941, University of Cambridge, Computer Laboratory, September 2019. [ bib | project page | pdf | abstract ]
Research Topics
CHERI: Capability enhanced systems (project page)
ISA semantics and Sail (project page)
Relaxed-memory concurrency
Relaxed-memory concurrency: Power, ARM, and RISC-V (rmem operational and isla axiomatic tools for hardware concurrency)
Relaxed-memory concurrency: C/C++11 (cppmem tool)
Relaxed-memory concurrency: x86
Relaxed-memory concurrency: the CompCert-TSO verified compiler (project page)
Relaxed-memory concurrency: general
Cerberus: C semantics (project page) (Cerberus tool)
Cerberus: ISO WG14 C standards committee papers (project page)
LinkSem: ELF linking semantics (project page)
SibylFS: POSIX filesystem semantics (project page)
nqsb-TLS (project page)
Tool support for semantics
Lem: tool support for semantics (project page)
Ott: tool support for semantics (project page)
The POPLmark challenge (project page)
Netsem: semantics for TCP, UDP, and Sockets (project page)
Type-safe distributed languages (Acute and HashCaml)
Dynamic update
Java modules (project page)
Language and communication infrastructure for mobile computation: Nomadic Pict (project page)
Security policy
Anonymity
Secure encapsulation
Iota: XML scripting
Observational concurrent language semantics
Process calculi
Models for name-passing processes
From rewrite rules to bisimulation congruences
Locality typing
Pi calculus tutorials
Finite Axiomatisability
Hardware model abstraction
Misc
Plasma Physics
I also maintain a page of Action Calculi links.
Other
Another photo

[Validate this page.]
1 / 1ChatGPTChatGPTIt seems like you've posted a brief overview of Professor Peter Sewell's webpage. He is a professor at the University of Cambridge, who works in the field of Computer Science. His research primarily focuses on the rigorous engineering of real-world computer systems, with the goal of making them more robust, secure, and easier to understand.He has contributed significantly to multiple projects, such as REMS (Rigorous Engineering for Mainstream Systems) and CHERI (Capability Hardware Enhanced RISC Instructions). Both these projects aim to improve system security and reliability using mathematical rigor and formal verification.Prof. Sewell's list of recent papers suggests his active participation in pushing the boundaries of his field. His work covers a wide range of topics from architectural descriptions, programming languages, to networking and security.1 / 1Symbolic Algorithms for Graphs 
and Markov Decision Processes 
with Fairness Objectives 
Krishnendu Chatterjee1(B), Monika Henzinger2, Veronika Loitzenbauer3, 
Simin Oraee4, and Viktor Toman1 
1 IST Austria, Klosterneuburg, Austria 
krish.chat@gmail.com 
2 University of Vienna, Vienna, Austria 
3 Johannes Kepler University Linz, Linz, Austria 
4 Max Planck Institute for Software Systems, Kaiserslautern, Germany 
Abstract. Given a model and a specif i cation, the fundamental model- 
checking problem asks for algorithmic verif i cation of whether the model 
satisf i es the specif i cation. We consider graphs and Markov decision pro- 
cesses (MDPs), which are fundamental models for reactive systems. One 
of the very basic specif i cations that arise in verif i cation of reactive sys- 
tems is the strong fairness (aka Streett) objective. Given dif f erent types 
of requests and corresponding grants, the objective requires that for each 
type, if the request event happens inf i nitely often, then the corresponding 
grant event must also happen inf i nitely often. All Ï-regular objectives 
can be expressed as Streett objectives and hence they are canonical in 
verif i cation. To handle the state-space explosion, symbolic algorithms are 
required that operate on a succinct implicit representation of the system 
rather than explicitly accessing the system. While explicit algorithms for 
graphs and MDPs with Streett objectives have been widely studied, there 
has been no improvement of the basic symbolic algorithms. The worst- 
case numbers of symbolic steps required for the basic symbolic algorithms 
are as follows: quadratic for graphs and cubic for MDPs. In this work 
we present the i rst sub-quadratic symbolic algorithm for graphs with 
Streett objectives, and our algorithm is sub-quadratic even for MDPs. 
Based on our algorithmic insights we present an implementation of the 
new symbolic approach and show that it improves the existing approach 
on several academic benchmark examples. 
1Introduction 
In this work we present faster symbolic algorithms for graphs and Markov deci- 
sion processes (MDPs) with strong fairness objectives. For the fundamental 
model-checking problem, the input consists of a model and a specif i cation, and 
the algorithmic verif i cation problem is to check whether the model satisf i es the 
specif i cation. We i rst describe the specif i c model-checking problem we consider 
and then our contributions. 
c? The Author(s) 2018 
H. Chockler and G. Weissenbacher (Eds.): CAV 2018, LNCS 10982, pp. 178â197, 2018. 
https://doi.org/10.1007/978-3-319-96142-2_13 
Symbolic Algorithms for Graphs and MDPs with Fairness Objectives179 
Models: Graphs and MDPs. Two standard models for reactive systems are graphs 
and Markov decision processes (MDPs). Vertices of a graph represent states 
of a reactive system, edges represent transitions of the system, and inf i nite 
paths of the graph represent non-terminating trajectories of the reactive sys- 
tem. MDPs extend graphs with probabilistic transitions that represent reactive 
systems with uncertainty. Thus graphs and MDPs are the de-facto model of reac- 
tive systems with nondeterminism, and nondeterminism with stochastic aspects, 
respectively [3,19].1 / 1ChatGPTChatGPTThe abstract you've shared is from a research paper that presents a new algorithmic approach to verifying models in reactive systems. Reactive systems are systems that maintain an ongoing interaction with their environment, often used in the context of computer systems that have real-time response requirements.The key models used for these systems are graphs and Markov decision processes (MDPs). Graphs represent the different states of a system and the transitions between them. MDPs extend this idea by adding a level of probability to the transitions, thereby allowing the representation of systems with uncertainty.The key objective of the study is to develop more efficient symbolic algorithms for these models with a strong fairness or Streett objective. A Streett objective requires that for every request event that occurs an infinite number of times, the corresponding grant event must also happen infinitely often.Symbolic algorithms are beneficial because they operate on an implicit representation of the system, which helps to manage state-space explosion - a situation where the number of states in a system grows exponentially.The paper claims to present the first sub-quadratic symbolic algorithm for both graphs and MDPs with Streett objectives. In computer science, a quadratic time complexity means the time taken by an algorithm grows quadratically with the size of the input, so a sub-quadratic time complexity represents a significant improvement.The new symbolic approach, as per the authors, has shown improvement over existing approaches in academic benchmark examples.1 / 1Symbolic Liveness Analysis 
of Real-World Software 
Daniel Schemmel1(B), Julian BÂ¨ uning1, Oscar Soria Dustmann1, Thomas Noll2, 
and Klaus Wehrle1 
1 Communication and Distributed Systems, 
RWTH Aachen University, Aachen, Germany 
{schemmel,buening,soriadustmann, 
wehrle}@comsys.rwth-aachen.de 
2 Software Modeling and Verif i cation, 
RWTH Aachen University, Aachen, Germany 
noll@cs.rwth-aachen.de 
Abstract. Liveness violation bugs are notoriously hard to detect, espe- 
cially due to the dif i culty inherent in applying formal methods to real- 
world programs. We present a generic and practically useful liveness 
property which def i nes a program as being live as long as it will eventu- 
ally either consume more input or terminate. We show that this property 
naturally maps to many dif f erent kinds of real-world programs. 
To demonstrate the usefulness of our liveness property, we also present 
an algorithm that can be ef i ciently implemented to dynamically i nd las- 
sos in the target programâs state space during Symbolic Execution. This 
extends Symbolic Execution, a well known dynamic testing technique, 
to i nd a new class of program defects, namely liveness violations, while 
only incurring a small runtime and memory overhead, as evidenced by 
our evaluation. The implementation of our method found a total of i ve 
previously undiscovered software defects in BusyBox and the GNU Core- 
utils. All i ve defects have been conf i rmed and i xed by the respective 
maintainers after shipping for years, most of them well over a decade. 
Keywords: Liveness analysisÂ·Symbolic ExecutionÂ·Software 
testing 
Non-termination bugs 
1Introduction 
Advances in formal testing and verif i cation methods, such as Symbolic Execution 
[10â12,22â24,42,49] and Model Checking [5,6,13,17,21,27,29,30,43,50], have 
enabled the practical analysis of real-world software. Many of these approaches 
are based on the formal specif i cation of temporal system properties using sets of 
inf i nite sequences of states [1], which can be classif i ed as either safety, liveness, or 
properties that are neither [31]. (However, every linear-time property can be rep- 
resented as the conjunction of a safety and a liveness property.) This distinction 
is motivated by the dif f erent techniques employed for proving or disproving such 
c? The Author(s) 2018 
H. Chockler and G. Weissenbacher (Eds.): CAV 2018, LNCS 10982, pp. 447â466, 2018. 
https://doi.org/10.1007/978-3-319-96142-2_27 
448D. Schemmel et al. 
properties. In practical applications, safety properties are prevalent. They con- 
strain the i nite behavior of a system, ensuring that ânothing badâ happens, and 
can therefore be checked by reachability analysis. Hence, ef i cient algorithms 
and tools have been devised for checking such properties that return a i nite 
counterexample in case of a violation [34]. 
Liveness properties, on the other hand, do not rule out any i nite behavior 
but constrain inf i nite behavior to eventually do âsomething goodâ [2]. Their 
checking generally requires more sophisticated algorithms since they must be 
able to generate (f i nite representations of) inf i nite counterexamples. Moreover, 
common i nite-state abstractions that are often employed for checking safety do 
generally not preserve liveness properties.1 / 1ChatGPTChatGPTThe abstract you've provided is from a research paper focused on liveness analysis of real-world software using Symbolic Execution. Liveness properties in computer science are those which ensure a system will eventually perform "something good" or a particular action. Liveness violation bugs, which break these properties, can be very difficult to detect due to the complexities of applying formal methods to real-world programs.The authors propose a generic and practically applicable liveness property. They define a program as being live as long as it will eventually either consume more input or terminate. This property is said to apply naturally to many different types of real-world software programs.They also present an algorithm that finds loops (referred to as lassos here) in the state space of the program during Symbolic Execution, a common dynamic testing technique. By doing so, they extend Symbolic Execution's capabilities to find liveness violations. The implementation of this method reportedly found previously undiscovered software defects in BusyBox and GNU Coreutils. All these defects were confirmed and fixed by the respective maintainers.The paper seems to focus on balancing the complexity of these formal methods with their effectiveness, as applying them to real-world software requires careful consideration of runtime and memory overhead. The authors claim that their method provides an efficient way to dynamically find liveness violation bugs in real-world software.1 / 1While it may be easy to create a domain-specif i c liveness property (e.g., âa 
GET/HTTP/1.1 must eventually be answered with an HTTP/1.1 {status}â), it 
is much harder to formulate general liveness properties. We tackle this challenge 
by proposing a liveness property based on the notion of programs as implemen- 
tations of algorithms that transform input into output: 
Def i nition 1. A program is live if and only if it always eventually consumes 
input or terminates. 
By relying on input instead of output as the measure of progress, we circumnavi- 
gate dif i culties caused by many common programming patterns such as printing 
status messages or logging the current state. 
Detection. We present an algorithm to detect violations of this liveness property 
based on a straightforward idea: Execute the program and check after each 
instruction if the whole program state has been encountered before (identical 
contents of all registers and addressable memory). If a repetition is found that 
does not consume input, it is deterministic and will keep recurring ad inf i nitum. 
To facilitate checking real-world programs, we perform the search for such lassos 
in the programâs state space while executing it symbolically. 
Examples. Some examples that show the generality of this liveness property 
are: 1. Programs that operate on input from i les and streams, such as cat, 
sha256sum or tail. This kind of program is intended to continue running as 
long as input is available. In some cases this input may be inf i nite (e.g., cat -). 
2. Reactive programs, such as calc.exe or nginx wait for events to occur. Once 
an event occurs, a burst of activity computes an answer, before the software 
goes back to waiting for the next event. Often, an event can be sent to signal a 
termination request. Such events are input just as much as the contents of a i le 
read by the program are input. 
In rare cases, a program can intuitively be considered live without satisfying 
our liveness property. Most prominent is the yes utility, which will loop forever, 
only printing output. According to our experience the set of useful programs 
that intentionally allow for an inf i nite trace consuming only i nite input is very 
small and the violation of our liveness property can, in such cases, easily be 
recognized as intentional. Our evaluation supports this claim (cf. Sect.6). 
Symbolic Liveness Analysis of Real-World Software449 
Bugs and Violations. The implementation of our algorithm detected a total of 
i ve unintended and previously unknown liveness violations in the GNU Coreutils 
and BusyBox, all of which have been in the respective codebases for at least 
7 to 19 years. All i ve bugs have been conf i rmed and i xed within days. The 
three implementations of yes we tested as part of our evaluation, were correctly 
detected to not be live. We also automatically generated liveness violating input 
programs for all sed interpreters.1 / 1ChatGPTChatGPTIn this excerpt, the authors continue to explain their approach to analyzing the liveness property of real-world software. They define a program as live if it always eventually consumes input or terminates, which is a general liveness property that can apply to a variety of software. This avoids issues that could arise when monitoring output, such as being confused by routine status messages or logs.The authors propose an algorithm for detecting violations of this liveness property. It involves executing the program and checking the state of the program after each instruction. If a repetition occurs that doesn't consume input, it is determined to be deterministic and will continue to recur indefinitely.The authors highlight some examples where this liveness property applies, including programs that operate on input from files and streams (like 'cat', 'sha256sum' or 'tail'), and reactive programs like 'calc.exe' or 'nginx' that wait for events to occur before performing a series of actions.The authors also mention a special case where a program, such as 'yes', might be considered live even though it doesn't satisfy their liveness property. However, they argue that this set of programs is very small and deviations from their liveness property in these cases can be interpreted as intentional.Finally, they mention their algorithm's successes in uncovering bugs. Their implementation identified five previously unknown liveness violations in GNU Coreutils and BusyBox, which had been in their codebases for many years. The violations were confirmed and fixed promptly. The algorithm also correctly identified 'yes' implementations as not being live and generated liveness violating input programs for all 'sed' interpreters.1 / 11.1Key Contributions 
This paper presents four key contributions: 
1. The def i nition of a generic liveness property for real-world software. 
2. An algorithm to detect its violations. 
3. An open-source implementation of the algorithm, available on GitHub1, 
implemented as an extension to the Symbolic Execution engine KLEE [10]. 
4. An evaluation of the above implementation on a total of 354 tools from the 
GNU Coreutils, BusyBox and toybox, which so far detects i ve previously 
unknown defects in widely deployed real-world software. 
1.2Structure 
We discuss related work (Sect.2), before formally def i ning our liveness property 
(Sect.3). Then, we describe the lasso detection algorithm (Sect.4), demonstrate 
the practical applicability by implementing the algorithm for the SymEx engine 
KLEE (Sect.5) and evaluate it on three real-world software suites (Sect.6). We 
i nally discuss the practical limitations (Sect.7) and conclude (Sect.8). 
2Related Work 
General liveness properties [2] can be verif i ed by proof-based methods [40], which 
generally require heavy user support. Contrarily, our work is based upon the 
state-exploration approach to verif i cation. Another prominent approach to verify 
the correctness of a system with respect to its specif i cation is automatic Model 
Checking using automata or tableau based methods [5]. 
In order to combat state-space explosion, many optimization techniques have 
been developed. Most of these, however, are only applicable to safety properties. 
For example, Bounded Model Checking (BMC) of software is a well-established 
method for detecting bugs and runtime errors [7,18,19] that is implemented by 
a number of tools [16,38]. These tools investigate i nite paths in programs by 
bounding the number of loop iterations and the depth of function calls, which is 
not necessarily suited to detect the sort of liveness violations we aim to discover. 
There is work trying to establish completeness thresholds of BMC for (safety 
and) liveness properties [33], but these are useful only for comparatively small 
1 https://github.com/COMSYS/SymbolicLivenessAnalysis. 
450D. Schemmel et al. 
systems. Moreover, most BMC techniques are based on boolean SAT, instead of 
SMT, as required for dealing with the intricacies of real-world software. 
Termination is closely related to liveness in our sense, and has been inten- 
sively studied. It boils down to showing the well-foundedness of the programâs 
transition relation by identifying an appropriate ranking function. In recent 
works, this is accomplished by i rst synthesizing conditional termination proofs 
for program fragments such as loops, and then combining sub-proofs using a 
transformation that isolates program states for which termination has not been 
proven yet [8]. A common assumption in this setting is that program variables 
are mathematical integers, which eases reasoning but is generally unsound. A 
notable exception is AProVE [28], an automated tool for termination and com- 
plexity analysis that takes (amongst others) LLVM intermediate code and builds 
a SymEx graph that combines SymEx and state-space abstraction, covering both 
byte-accurate pointer arithmetic and bit-precise modeling of integers. However, 
advanced liveness properties, l oating point values, complex data structures and 
recursive procedures are unsupported. While a termination proof is a witness for 
our liveness property, an inf i nite program execution constitutes neither witness 
nor violation. Therefore, non-termination proof generators, such as TNT [26], 
while still related, are not relevant to our liveness property.1 / 1ChatGPTChatGPTThe authors outline four key contributions of their paper:They define a general liveness property for real-world software.They propose an algorithm to detect violations of this liveness property.They provide an open-source implementation of their algorithm, which is available on GitHub and is an extension to the Symbolic Execution engine KLEE.They evaluated their implementation on 354 tools from the GNU Coreutils, BusyBox, and toybox. Their evaluation found five previously unknown defects in these widely-used software tools.The paper's structure is outlined: discussing related work, formally defining the liveness property, describing the liveness violation detection algorithm, implementing and evaluating the algorithm, discussing practical limitations, and concluding.The authors then delve into related work. Traditional methods for verifying liveness properties require heavy user support and are based on proof-based methods. Their work, however, is based on a state-exploration approach. They also mention Model Checking, which verifies the correctness of a system relative to its specification.They also discuss Bounded Model Checking (BMC), a method that investigates finite paths in programs, though it is not suited to detect the type of liveness violations they aim to discover. They then consider the relationship between termination and liveness, referencing tools like AProVE which provides termination and complexity analysis.However, they argue that advanced liveness properties and certain aspects of complex software are still unsupported. Notably, an infinite program execution does not constitute either a witness or violation of their liveness property, so tools focused on non-termination proofs, like TNT, are not relevant to their approach.1 / 1Quasi-Optimal Partial Order Reduction . . . . . . . . . . . . . . . . . . . . . . . . . . .354 
Huyen T. T. Nguyen, CÃ©sar RodrÃ­guez, Marcelo Sousa, Camille Coti, 
and Laure Petrucci 
On the Completeness of Verifying Message Passing Programs Under 
Bounded Asynchrony . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .372 
Ahmed Bouajjani, Constantin Enea, Kailiang Ji, and Shaz Qadeer 
Constrained Dynamic Partial Order Reduction . . . . . . . . . . . . . . . . . . . . . .392 
Elvira Albert, Miguel GÃ³mez-Zamalloa, Miguel Isabel, and Albert Rubio 
CPS, Hardware, Industrial Applications 
Formal Verification of a Vehicle-to-Vehicle (V2V) Messaging System . . . . .413 
Mark Tullsen, Lee Pike, Nathan Collins, and Aaron Tomb 
Continuous Formal Verification of Amazon s2n . . . . . . . . . . . . . . . . . . . . .430 
Andrey Chudnov, Nathan Collins, Byron Cook, Joey Dodds, 
Brian Huffman, Colm MacCÃ¡rthaigh, Stephen Magill, Eric Mertens, 
Eric Mullen, Serdar Tasiran, Aaron Tomb, and Eddy Westbrook 
Symbolic Liveness Analysis of Real-World Software. . . . . . . . . . . . . . . . . .447 
Daniel Schemmel, Julian BÃ¼ning, Oscar Soria Dustmann, Thomas Noll, 
and Klaus Wehrle 
Model Checking Boot Code from AWS Data Centers . . . . . . . . . . . . . . . . .467 
Byron Cook, Kareem Khazem, Daniel Kroening, Serdar Tasiran, 
Michael Tautschnig, and Mark R. Tuttle 
Android Stack Machine . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .487 
Taolue Chen, Jinlong He, Fu Song, Guozhen Wang, Zhilin Wu, 
and Jun Yan 
Formally Verified Montgomery Multiplication . . . . . . . . . . . . . . . . . . . . . .505 
Christoph Walther 
Inner and Outer Approximating Flowpipes for Delay Differential Equations . . .523 
Eric Goubault, Sylvie Putot, and Lorenz Sahlmann1 / 1ChatGPTChatGPTThis text appears to be a list of papers or presentations in a conference or symposium. Each entry includes the title of the presentation, the authors, and a page number. Here are a few examples:"Quasi-Optimal Partial Order Reduction" by Huyen T. T. Nguyen, CÃ©sar RodrÃ­guez, Marcelo Sousa, Camille Coti, and Laure Petrucci (page 354)"On the Completeness of Verifying Message Passing Programs Under Bounded Asynchrony" by Ahmed Bouajjani, Constantin Enea, Kailiang Ji, and Shaz Qadeer (page 372)"Formal Verification of a Vehicle-to-Vehicle (V2V) Messaging System" by Mark Tullsen, Lee Pike, Nathan Collins, and Aaron Tomb (page 413)"Symbolic Liveness Analysis of Real-World Software" by Daniel Schemmel, Julian BÃ¼ning, Oscar Soria Dustmann, Thomas Noll, and Klaus Wehrle (page 447)Each entry focuses on a specific topic within the field of computer science, specifically focusing on formal verification and analysis methods.1 / 1Symbolic Liveness Analysis of Real-World Software455 
5.1LLVM Intermediate Representation 
LLVM Intermediate Representation (IR) was designed as a typed, low-level lan- 
guage independent from both (high-level) source language and any specif i c tar- 
get architecture, to facilitate compiler optimizations. It operates on an unlim- 
ited number of typed registers of arbitrary size, as well as addressable memory. 
Instructions in IR operate in Static Single Assignment (SSA) form, i.e., registers 
are only ever assigned once and never modif i ed. The language also has functions, 
which have a return type and an arbitrary number of typed parameters. Apart 
from global scope, there is only function scope, but IR features no block scope. 
Addressable objects are either global variables, or explicitly allocated, e.g., 
using malloc (cleaned up with free) or alloca (cleaned up on return from 
function). 
Fig.2. Six kinds of fragments suf i ce to denote all possible variants. Symbolic values 
are written as serialized symbolic expressions consisting of all relevant constraints. All 
other i elds only ever contain concrete values, which are simply used verbatim. Fields 
of dynamic size are denoted by a ragged right edge. 
5.2Fragments 
When determining what is to become a fragment, i.e., an atomic portion of a 
i ngerprint, two major design goals should be taken into consideration: 
1. Collisions between hashed fragments should not occur, as they would expunge 
one another from the i ngerprint. This goal can be decomposed further: 
(a) The hashing algorithm should be chosen in a manner that makes collisions 
so unlikely, as to be non-existent in practice. 
(b) The fragments themselves need to be generated in a way that ensures that 
no two dif f erent fragments have the same representation, as that would 
of course cause their hashes to be equal as well. 
2. Fragment sizes should be as close as possible to what will be modif i ed by the 
program in one step. Longer fragments are more expensive to compute and 
hash, and shorter fragments become invalidated more frequently. 
Avoiding Collisions. In order to minimize the risk of accidental collisions, 
which would reduce the ef i cacy of our methodology, we chose the cryptographi- 
cally secure checksum algorithm BLAKE2b [4] to generate 256 bit hashes, provid- 
ing 128 bit collision resistance. To the best of our knowledge, there are currently1 / 1ChatGPTChatGPTThis excerpt from a paper discusses some technical aspects of LLVM Intermediate Representation (IR) and a technique for segmenting and hashing code into "fragments."LLVM IR: LLVM IR is a low-level language that is independent of both the source language and the specific target architecture. It operates on unlimited registers of any size and addressable memory. It operates in Static Single Assignment (SSA) form, meaning that registers are assigned once and never modified. LLVM IR also supports functions, with a return type and an arbitrary number of parameters.Fragments: The text also discusses a technique for dividing code into "fragments" which are used to create a fingerprint of the program. The size of the fragments is important. They should be large enough to minimize the computation and hashing overhead but not so large that they are frequently invalidated. Avoiding collisions in the hashes is also crucial. To minimize the risk of accidental collisions, the authors have chosen to use the cryptographic hash function BLAKE2b, which provides 128-bit collision resistance.The figure referred to in the text (Fig.2) seems to illustrate how different parts of code or program state are represented in these fragments, but it's not included in the text you've provided.1 / 1462D. Schemmel et al. 
6.5Summary 
All evaluated tool suites show a low average performance and memory penalty 
when comparing our approach to baseline KLEE. While the slowdown is signif- 
icant for some tools in each suite, it is consistent as long as time and memory 
limits are not chosen too tightly. In fact, for these kinds of programs, it is rea- 
sonable to accept a limited slowdown in exchange for opening up a whole new 
category of defects that can be detected. In direct comparison, performance 
varies in between suites, but remains reasonable in each case. 
7Limitations 
Our approach does not distinguish between interpreters and interpreted pro- 
grams. While this enables the automatic derivation of input programs for such 
interpreters as sed, it also makes it hard to recognize meaningful error cases. This 
causes the analysis of all three implementations of sed used in the evaluation 
(Sect.6) to return liveness violations. 
In its current form, our implementation struggles with runaway counters, as a 
64 bit counter cannot be practically enumerated on current hardware. Combining 
static analyses, such as those done by optimizing compilers may signif i cantly 
reduce the impact of this problem in the future. 
A dif f erent pattern that may confound our implementation is related to 
repeated allocations. If memory is requested again after releasing it, the newly 
acquired memory may not be at the same position, which causes any pointers 
to it to have dif f erent values. While this is fully correct, it may cause the imple- 
mentation to not recognize cycles in a reasonable time frame. This could be 
mitigated by analyzing whether the value of the pointer ever actually matters. 
For example, in the C programming language, it is fairly uncommon to inspect 
the numerical value of a pointer beyond comparing it to NULL or other pointers. 
A valid solution would however require strengthening KLEEâs memory model, 
which currently does not model pointer inspection very well. 
Another potential problem is how the PC is serialized when using symbolic 
expressions as the value of a fragment (c.f. Sect.5.2). We currently reuse KLEEâs 
serialization routines, which are not exactly tuned for performance. Also, each 
symbolic value that is generated by KLEE is assigned a unique name, that is 
then displayed by the serialization, which discounts potential equivalence. 
Finally, by building upon SymEx, we inherit not only its strengths, but also 
its weaknesses, such as a certain predilection for state explosion and a reliance 
on repeated SMT solving [12]. Also, actual SymEx implementations are limited 
further than that. For example, KLEE returns a concrete pointer from allocation 
routines instead of a symbolic value representing all possible addresses. 
8Conclusion and Outlook 
It is our strong belief that the testing and verif i cation of liveness properties 
needs to become more attractive to developers of real-world programs. Our work 
Symbolic Liveness Analysis of Real-World Software463 
provides a step in that direction with the formulation of a liveness property that 
is general and practically useful, thereby enabling even developers uncomfortable 
with interacting with formal testing and verif i cation methods to at least check 
their software for liveness violation bugs. 
We demonstrated the usefulness of our liveness property by implementing it 
as an extension to the Symbolic Execution engine KLEE, thereby enabling it to 
discover a class of software defects it could not previously detect, and analyzing 
several large and well-tested programs. Our implementation caused the discovery 
and eventual correction of a total of i ve previously unknown defects, three in the GNU Coreutils, arguably one of the most well-tested code bases in existence, 
and two in BusyBox. Each of these bugs had been in released software for over 7 
yearsâfour of them even for over 16 years, which goes to show that this class of 
bugs has so far proven elusive. Our implementation did not cause a single false 
positive: all reported violations are indeed accompanied by concrete test cases 
that reproduce a violation of our liveness property. 
The evaluation in Sect.6 also showed that the performance impact, in matters 
of throughput as well as in matters of memory consumption, remains signif i cantly 
below 2Ã on average, while allowing the analysis to detect a completely new 
range of software defects. We demonstrated that this overhead remains stable 
over a range of dif f erent analysis durations. 
In future work, we will explore the opportunities for same-state merging that 
our approach enables by implementing ef i cient equality testing of SymEx states 
via our i ngerprinting scheme. We expect that this will further improve the per- 
formance of our approach and maybe even exceed KLEEâs baseline performance 
by reducing the amount of duplicate work done.1 / 1ChatGPTChatGPTThis section concludes a study on symbolic liveness analysis in software. The authors identify the following points:Performance: The analysis shows low average performance and memory penalties when comparing their approach to baseline KLEE (Kernel-based Virtual Machine LLVM, a popular symbolic execution engine). Even though there are slowdowns for some tools, the authors argue these are acceptable given the ability to identify a new category of software defects.Limitations: The approach does not distinguish between interpreters and interpreted programs, which can lead to difficulty in identifying meaningful error cases. Other challenges include dealing with runaway counters and repeated allocations. The approach also inherits the limitations of symbolic execution, such as state explosion and repeated SMT (Satisfiability Modulo Theories) solving.Contributions and Future Work: The work helps to make the testing and verification of liveness properties more accessible to developers. The implementation has successfully identified previously unknown defects in large, well-tested software, demonstrating the practical usefulness of the technique. In the future, the authors plan to further explore opportunities for merging identical states, which could improve performance and reduce duplicate work.1 / 1A summary of the article1 / 1ChatGPTChatGPTThe paper titled "Symbolic Liveness Analysis of Real-World Software" contributes to the field of software testing and verification, with a particular focus on liveness property detection. The authors' work is based on symbolic execution, a method for exploring all possible paths in a program's execution.The main contributions of this paper include:Liveness Property Definition: The authors define a generic liveness property for real-world software, which serves as a formalized notion of a program eventually making progress in some form.Violation Detection Algorithm: They develop an algorithm to detect violations of this liveness property.Implementation: The authors provide an open-source implementation of the algorithm as an extension to the Symbolic Execution engine KLEE.Evaluation: The implementation is tested on 354 tools from GNU Coreutils, BusyBox, and toybox, revealing five previously unknown defects.However, the approach has certain limitations. For example, it doesn't distinguish between interpreters and interpreted programs and struggles with certain patterns like runaway counters and repeated allocations. Furthermore, it inherits limitations of symbolic execution, such as state explosion and reliance on repeated SMT solving.Despite these limitations, the authors demonstrate the effectiveness of their approach by identifying previously unknown defects in real-world software. The method's performance overhead remains significantly below 2x on average, which the authors argue is acceptable for the ability to detect a completely new range of software defects.In future work, the authors intend to explore opportunities for merging identical states to further enhance the performance of their methodology.1 / 1
Symbolic Liveness Analysis of Real-World Software
In Symbolic Liveness Analysis, we extended KLEE, a popular Symbolic Execution engine, for the detection of liveness violations.

To this end, we defined a generic and practically useful livenss violation for real-world software:

A program is live iff it always eventually consumes input or terminates. [1]

This liveness property allows us to detect infinite loop bugs, while excluding intentional infinite loops, such as event loops in GUI or server applications.

For detection, we constructed a fingerprinting scheme that allows us to efficiently compute and update fingerprints of symbolic execution states, which can then be compared against each other.

Additional information on our technique can be found in our paper [1], as well as an evaluation of its runtime and memory overhead.

Bugs Found
During our evaluation on GNU Coreutils, BusyBox, toybox and GNU sed, we found a total of five previously undiscovered software defects:

GNU Coreutils, tail #1: https://debbugs.gnu.org/24495
GNU Coreutils, tail #2: https://debbugs.gnu.org/24903
GNU Coreutils, ptx: https://debbugs.gnu.org/28417
BusyBox, hush #1: https://bugs.busybox.net/10421
BusyBox, hush #2: https://bugs.busybox.net/10686
In addition, we identified a bug in the long decommissioned GNU regex, which is still used in klee-uclibc by default:

GNU regex: http://lists.gnu.org/archive/html/bug-gnu-utils/2018-04/msg00006.html
Requirements
We recommend to compile our version of KLEE against LLVM 11.0. For information on how to compile KLEE in general, please refer to README-CMake.md.

In addition to the usual requirements of KLEE, we use CryptoPP (Version 8.2.0) to generate BLAKE2b hashes (for fingerprint creation).

If CryptoPP is located at a custom location on your system, you can use the following to tell CMake where to find it:

cmake [...] -DCRYPTOPP_INCLUDE_DIRS="/path/to/CryptoPP/include" -DCRYPTOPP_LIBRARIES="/path/to/CryptoPP/libcryptopp.so"
In addition, make sure that KLEE can find libcryptopp.so in the library path at runtime.

Usage
To use our analysis, we added the following options to KLEE:

-detect-infinite-loops
Enable detection of infinite loops (default=false)

-debug-infinite-loop-detection={state:stderr,trace:stderr}
Log all MemoryState / MemoryTrace information to stderr

-infinite-loop-detection-truncate-on-fork
Truncate memory trace (used for infinite loop detection) on every state fork (default=true)

-infinite-loop-detection-disable-two-predecessor-optimization
Disable infinite loop detection optimzation that only starts searching for loops on basic blocks with at least two predecessors (default=false)

CAV 2018 Artifact
Along with submission to CAV 2018, we prepared an artifact, which can be obtained from 10.5281/zenodo.5771192.

Publication
If you use any portion of Symbolic Liveness Analysis in your work, please cite the following paper:

[1] Daniel Schemmel, Julian BÃ¼ning, Oscar Soria Dustmann, Thomas Noll and Klaus Wehrle. Symbolic Liveness Analysis of Real-World Software. In Proceedings of the 30th International Conference on Computer Aided Verification (CAV'18). Lecture Notes in Computer Science, vol 10982. Springer, Cham1 / 1ChatGPTChatGPTThe "Symbolic Liveness Analysis of Real-World Software" project extends KLEE, a well-known Symbolic Execution engine, to detect liveness violations. A liveness violation, as defined here, occurs when a program does not eventually consume input or terminate, hence falling into an infinite loop. The methodology excludes intentional infinite loops often found in GUI or server applications.To achieve detection, a fingerprinting scheme is introduced that efficiently calculates and updates fingerprints of symbolic execution states for comparison.The implementation was evaluated on real-world software including GNU Coreutils, BusyBox, toybox, and GNU sed, and it successfully detected five previously unknown software defects. An additional bug was identified in the GNU regex.To use this approach, the authors recommend compiling their version of KLEE against LLVM 11.0, with CryptoPP (Version 8.2.0) being used to generate BLAKE2b hashes for fingerprint creation. The analysis can be enabled with specific options added to KLEE.They have prepared an artifact in conjunction with their submission to CAV 2018, and they request that users cite their work if it's utilized in any form. The citation provided is: Daniel Schemmel, Julian BÃ¼ning, Oscar Soria Dustmann, Thomas Noll and Klaus Wehrle. "Symbolic Liveness Analysis of Real-World Software." In Proceedings of the 30th International Conference on Computer Aided Verification (CAV'18). Lecture Notes in Computer Science, vol 10982. Springer, Cham.1 / 1KLEE Symbolic Execution Engine
KLEE is a dynamic symbolic execution engine built on top of the LLVM compiler infrastructure, and available under the UIUC open source license. For more information on what KLEE is and what it can do, see the OSDI 2008 paper.1 / 1ChatGPTChatGPTKLEE is a dynamic symbolic execution engine designed to automatically detect bugs in software. By employing symbolic execution, KLEE systematically explores various paths in a program, helping it uncover numerous types of bugs, such as assertion violations or out-of-bound accesses. It also generates test cases to reproduce detected errors.Built on the LLVM compiler infrastructure, KLEE can execute programs written in any language that can be compiled into LLVM bitcode (which includes C and C++, among others). This allows KLEE to work directly with a low-level, platform-independent representation of the source code.Here are some of the features and capabilities of KLEE:Automated test case generation: KLEE can generate test cases automatically, which hit specific parts of a program, or trigger specific bugs.Support for complex data types: KLEE handles complex data types such as arrays and structures, in addition to simple numeric data types.Extensive coverage: By simulating all possible paths of a program, KLEE can achieve higher code coverage compared to traditional testing methods.Performance: KLEE is designed to handle real-world software and can scale to large codebases.High bug-finding capability: According to the OSDI 2008 paper, KLEE was able to find bugs in real-world programs such as the UNIX coreutils, which were missed by other sophisticated analysis tools.KLEE is available under the University of Illinois/NCSA Open Source License. For more details on the specifics of what KLEE can do, you can refer to its presentation in the OSDI (Operating Systems Design and Implementation) 2008 paper.1 / 1KLEE: Unassisted and Automatic Generation of High-Coverage Tests for Complex Systems Programs
Cristian Cadar, Daniel Dunbar, Dawson Engler
Abstract:
We present a new symbolic execution tool, KLEE, capable of automatically generating tests that achieve high coverage on a diverse set of complex and environmentally-intensive programs. We used KLEE to thoroughly check all 89 stand-alone programs in the GNU COREUTILS utility suite, which form the core user-level environment installed on millions of Unix systems, and arguably are the single most heavily tested set of open-source programs in existence. KLEE-generated tests achieve high line coverage â on average over 90% per tool (median: over 94%) â and significantly beat the coverage of the developers' own hand-written test suites. When we did the same for 75 equivalent tools in the BUSYBOX embedded system suite, results were even better, including 100% coverage on 31 of them. We also used KLEE as a bug finding tool, applying it to 452 applications (over 430K total lines of code), where it found 56 serious bugs, including three in COREUTILS that had been missed for over 15 years. Finally, we used KLEE to cross-check purportedly identical BUSY-BOX and COREUTILS utilities, finding functional correctness errors and a myriad of inconsistencies.

dition) to produce a test case that will follow the same 
pathwhenrerunonanunmodif i edversionofthe checked 
program (e.g, compiled with gcc). 
KLEEis designed so that the paths followed by the 
unmodif i ed program will always follow the same path 
KLEEtook (i.e., there are no false positives). However, 
non-determinism in checked code and bugs inKLEEor 
its models have produced false positives in practice. The 
abilityto reruntests outsideofKLEE,in conjunctionwith 
standard tools such as gdb and gcov is invaluable for 
diagnosing such errors and for validating our results. 
We nextshow how to useKLEE, then givean overview 
of how it works. 
2.1Usage 
A user can start checkingmany real programswithKLEE 
in seconds:KLEEtypically requires no source modif i- 
cations or manual work. Users i rst compile their code 
to bytecode using the publicly-available LLVM com- 
piler [33] for GNU C. We compiled tr using: 
llvm-gcc --emit-llvm -c tr.c -o tr.bc 
Users then runKLEEon the generated bytecode, option- 
ally stating the number,size, and type of symbolic inputs 
to test the code on. For tr we used the command: 
klee --max-time 2 --sym-args 1 10 10 
--sym-files 2 2000 --max-fail 1 tr.bc 
2The program has one line of dead code, an unreachable return 
statement, which, reassuringly,KLEEcannot run. 
1 : void expand(char *arg, unsigned char *buffer) {8 
2 :int i, ac;9 
3 :while (*arg) {10* 
4 :if (*arg == â\\â) {11* 
5 :arg++; 
6 :i = ac = 0; 
7 :if (*arg >= â0â && *arg <= â7â) { 
8 :do { 
9 :ac = (ac << 3) + *arg++ â â0â; 
10:i++; 
11:} while (i<4 && *arg>=â0â && *arg<=â7â); 
12:*buffer++ = ac; 
13:} else if (*arg != â\0â) 
14:*buffer++ = *arg++; 
15:} else if (*arg == â[â) {12* 
16:arg++;13 
17:i = *arg++;14 
18:if (*arg++ != â-â) {15! 
19:*buffer++ = â[â; 
20:arg â= 2; 
21:continue; 
22:} 
23:ac = *arg++; 
24:while (i <= ac) *buffer++ = i++; 
25:arg++;/* Skip â]â */ 
26:} else 
27:*buffer++ = *arg++; 
28:} 
29: } 
30: ... 
31: int main(int argc, char* argv[]) {1 
32:int index = 1;2 
33:if (argc > 1 && argv[index][0] == â-â) {3* 
34:...4 
35:}5 
36:...6 
37:expand(argv[index++], index);7 
38:... 
39: } 
Figure 1: Code snippet from MINIXâs tr, representative 
of the programs checked in this paper: tricky, non-obvious, 
diff i cult to verify by inspection or testing. The order of the 
statements on the path to the error at line 18 are numbered on 
the right hand side. 
The i rst option, --max-time, tellsKLEEto check 
tr.bc for at most two minutes. The rest describe the 
symbolic inputs. The option --sym-args 1 10 10 
says to use zero to three command line arguments, the 
i rst one character long, the others 10 characters long. 
3 
The option --sym-files 2 2000 says to use stan- 
dard input and one i le, each holding 2000 bytes of sym- 
bolic data. The option --max-fail 1 says to fail at 
mostonesystemcallalongeachprogrampath(see Â§4.2). 
2.2Symbolic execution withKLEE 
WhenKLEEruns on tr, it i nds a buffer overf l ow error 
at line 18 in Figure 1 and then produces a concrete test 
3Since strings in C are zero terminated, this essentially generates 
arguments of up to that size. 
3 
case (tr [ "" "") that hits it. Assuming the options 
of the previous subsection,KLEEruns tr as follows: 
1KLEEconstructssymbolic commandline string argu- 
ments whose contents have no constraints other than 
zero-termination. It then constrainsthe numberof ar- 
guments to be between 1 and 3, and their sizes to be 
1, 10 and 10 respectively. It then calls main with 
these initial path constraints. 
2 WhenKLEEhits the branch argc > 1 at line 33, 
it uses its constraint solver STP [23] to see which di- 
rectionscan executegiventhe current path condition. 
For this branch, both directions are possible;KLEE 
forks execution and follows both paths, adding the 
constraintargc > 1onthefalsepathandargc â¤ 1 
on the true path. 
3 Given more than one active path,KLEEmust pick 
which one to execute i rst. We describe its algorithm 
in Section 3.4. For now assume it follows the path 
that reachesthe bug. As it doesso,KLEEadds further 
constraints to the contents of arg, and forks for a 
total of i ve times (lines denoted with a â*â): twice 
on line 33, and then on lines 3, 4, and 15 in expand. 
4 At each dangerous operation (e.g., pointer derefer- 
ence),KLEEchecks if any possible value allowed by 
the current path condition would cause an error. On 
theannotatedpath,KLEEdetectsnoerrorsbeforeline 
18. At that point, however, it determines that input 
values exist that allow the read of arg to go out of 
bounds: after taking the true branch at line 15, the 
code increments arg twice without checking if the 
string has ended. If it has, this increment skips the 
terminating â\0â and points to invalid memory. 
5KLEEgenerates concrete values for argc and argv 
(i.e., tr [ "" "") that when rerun on a raw ver- 
sion of tr will hit this bug. It then continues follow- 
ing the current path, adding the constraint that the 
error does not occur (in order to i nd other errors). 
3TheKLEEArchitecture 
KLEEis a complete redesign of our previous system 
EXE [16]. At a high level,KLEEfunctions as a hybrid 
between an operating system for symbolic processes and 
an interpreter. Each symbolic process has a register i le, 
stack, heap, program counter, and path condition. To 
avoid confusion with a Unix process, we refer toKLEEâs 
representationofa symbolicprocessas a state. Programs 
are compiled to the LLVM [33] assembly language, a 
RISC-like virtual instruction set.KLEEdirectly inter- 
prets this instruction set, and maps instructions to con- 
straints without approximation(i.e. bit-level accuracy). 
4 
4KLEE does not currently support:symbolic l oating point, 
longjmp, threads, and assembly code. Additionally, memory objects 
are required to have concrete sizes. 
3.1Basic Architecture 
At any one time,KLEEmay be executing a large number 
of states. The core ofKLEEis an interpreter loop which 
selects a state to run and then symbolically executes a 
single instruction in the context of that state. This loop 
continues until there are no states remaining, or a user- 
def i ned timeout is reached. 
Unlike a normal process, storage locations for a state 
â registers, stack and heap objects â refer to expres- 
sions (trees) instead of raw data values. The leaves of 
an expression are symbolic variables or constants, and 
the interior nodes come from LLVM assembly language 
operations (e.g., arithmetic operations, bitwise manipu- 
lation, comparisons, and memory accesses). Storage lo- 
cations which hold a constant expression are said to be 
concrete. 
Symbolic execution of the majority of instructions is 
straightforward. For example, to symbolically execute 
an LLVM add instruction: 
%dst = add i32 %src0, %src1 
KLEEretrieves the addends from the %src0 and %src1 
registers and writes a new expression Add(%src0, 
%src1) to the %dst register. For eff i ciency, the code 
that builds expressions checks if all given operands are 
concrete (i.e., constants) and, if so, performs the opera- 
tion natively, returning a constant expression. 
Conditional branches take a boolean expression 
(branch condition) and alter the instruction pointer of 
the state based on whether the condition is true or false. 
KLEEqueries the constraint solver to determine if the 
branch condition is either provablytrue or provablyfalse 
along the current path; if so, the instruction pointer is 
updated to the appropriate location.Otherwise, both 
branches are possible:KLEEclones the state so that it 
can explore both paths, updating the instruction pointer 
and path condition on each path appropriately. 
Potentially dangerous operations implicitly generate 
branches that check if any input value exists that could 
cause an error. For example, a division instruction gen- 
erates a branch that checks for a zero divisor.Such 
branches work identically to normal branches.Thus, 
even when the check succeeds (i.e., an error is detected), 
execution continues on the false path, which adds the 
negation of the check as a constraint (e.g., making the 
divisor not zero). If an error is detected,KLEEgenerates 
a test case to trigger the error and terminates the state. 
As with other dangerous operations, load and store in- 
structions generate checks: in this case to check that the 
address is in-boundsof a valid memoryobject. However, 
load and store operations present an additional compli- 
cation. The most straightforward representation of the 
memory used by checked code would be a l at byte ar- 
ray. In this case, loads and stores would simply map to1 / 1ChatGPTChatGPTThe provided text appears to be an excerpt from the original research paper "KLEE: Unassisted and Automatic Generation of High-Coverage Tests for Complex Systems Programs" authored by Cristian Cadar, Daniel Dunbar, and Dawson Engler. This research introduced the KLEE tool and demonstrated its capability in symbolic execution for generating high-coverage tests.In this paper, the authors presented KLEE as a symbolic execution tool capable of generating tests that achieve high coverage for complex, environment-intensive programs. They demonstrated its capability by testing it on the GNU COREUTILS suite, which are heavily used in Unix systems, achieving an average line coverage of over 90%. When tested on the BUSYBOX suite, the results were even better, achieving 100% coverage on 31 out of 75 tools.They also demonstrated KLEE's capability as a bug-finding tool. When applied to 452 applications (over 430,000 total lines of code), KLEE found 56 serious bugs, including three in COREUTILS that had been missed for over 15 years.The excerpt also includes a code example from MINIXâs tr program, demonstrating how KLEE detects a buffer overflow error at line 18 and subsequently generates a test case to hit this bug.Furthermore, the authors present details of KLEE's architecture, explaining its functioning as a hybrid between an operating system for symbolic processes and an interpreter. A symbolic process in KLEE, termed a state, has its own register file, stack, heap, program counter, and path condition.The excerpt goes into the details of how KLEE interprets and maps instructions to constraints, handles conditional branches, dangerous operations, and error detection.The authors mention the primary limitations of KLEE at the time, which include lack of support for symbolic floating point, longjmp, threads, and assembly code, in addition to the requirement that memory objects have concrete sizes. These details highlight the trade-offs made to allow for efficient symbolic execution and high-coverage test generation.1 / 1applications (over 430K total lines of code), where 
it found 56 serious bugs, including three in COREUTILS 
that had been missed for over 15 years.Finally, we 
usedKLEEto cross-check purportedly identical BUSY- 
BOXand COREUTILSutilities, i nding functional cor- 
rectness errors and a myriad of inconsistencies. 
1Introduction 
Many classes of errors, such as functional correctness 
bugs, are diff i cult to i nd without executing a piece of 
code. The importance of such testing â combined with 
the diff i culty and poor performance of random and man- 
ual approaches â has led to much recent work in us- 
ing symbolic execution to automatically generate test in- 
puts [11,14â16,20â22,24,26,27,36]. At a high-level, 
these tools use variations on the following idea: Instead 
of running code on manually- or randomly-constructed 
input, they run it on symbolic input initially allowed to 
be âanything.â They substitute programinputs with sym- 
âAuthor names are in alphabetical order. Daniel Dunbar is the main 
author of theKLEEsystem. 
bolicvaluesandreplacecorrespondingconcreteprogram 
operations with ones that manipulate symbolic values. 
When program execution branches based on a symbolic 
value, the system (conceptually) follows both branches, 
on each path maintaining a set of constraints called the 
path condition which must hold on execution of that 
path. When a path terminates or hits a bug, a test case 
can be generated by solving the current path condition 
for concrete values. Assuming deterministic code, feed- 
ing this concrete input to a raw, unmodif i ed version of 
the checked code will make it follow the same path and 
hit the same bug. 
Results are promising.However, while researchers 
have shown such tools can sometimes get good cover- 
age and i nd bugs on a small number of programs, it 
has been an open question whether the approach has any 
hope of consistently achieving high coverage on real ap- 
plications. Two common concerns are (1) the exponen- 
tial number of paths through code and (2) the challenges 
in handling code that interacts with its surrounding envi- 
ronment, such as the operating system, the network, or 
the user (colloquially: âthe environmentproblemâ). Nei- 
ther concern has been much helped by the fact that most 
past work, includingours, has usually reportedresults on 
a limited set of hand-picked benchmarks and typically 
has not included any coverage numbers. 
This paper makes two contributions. First, we present 
a new symbolic execution tool,KLEE, which we de- 
signed for robust, deep checking of a broad range of ap- 
plications, leveraging several years of lessons from our 
previoustool, EXE [16].KLEEemploys a variety of con- 
straint solving optimizations, represents program states 
compactly, and uses search heuristics to get high code 
coverage. Additionally, it uses a simple and straight- 
forward approach to dealing with the external environ- 
ment. These features improveKLEEâs performance by 
overan orderof magnitudeand let it check a broad range 
of system-intensive programs âout of the box.â 
Second,we show thatKLEEâs automatically-generated 
tests get high coverage on a diverse set of real, com- 
plicated, and environmentally-intensive programs. Our 
most in-depth evaluation appliesKLEEto all 89 pro- 
grams 1 in the latest stable version of GNU COREUTILS 
(version 6.10), which contains roughly 80,000 lines of 
library code and 61,000 lines in the actual utilities [2]. 
These programs interact extensively with their environ- 
ment to provide a variety of functions, including man- 
aging the i le system (e.g., ls, dd, chmod), display- 
ing and conf i guring system properties (e.g., logname, 
printenv, hostname), controlling command invo- 
cation (e.g., nohup, nice, env), processing text i les 
(e.g., sort, od, patch), and so on. They form the 
core user-level environment installed on many Unix sys- 
tems. They are used daily by millions of people, bug 
i xes are handled promptly, and new releases are pushed 
regularly. Moreover, their extensive interaction with the 
environmentstress-tests symbolic executionwhere it has 
historically been weakest. 
Further, i nding bugs in COREUTILSis hard. They are 
arguablythe single most well-tested suite of open-source 
applications available (e.g., is there a program the reader 
has used more under Unix than âlsâ?). In 1995, ran- 
dom testing of a subset of COREUTILSutilities found 
markedly fewer failures as compared to seven commer- 
cial Unix systems [35]. The last COREUTILSvulnerabil- 
ity reportedonthe SecurityFocusorUS National Vulner- 
ability databases was three years ago [5,7]. 
In addition, we checked two other UNIXutility suites: 
BUSYBOX, a widely-useddistributionforembeddedsys- 
tems [1],andthe latest releasefor MINIX[4]. Finally,we 
checked the HISTARoperating system kernel as a con- 
trast to application code [39]. 
Our experiments fall into three categories: (1) those 
wherewe dointensiverunsto bothf i ndbugsandgethigh 
coverage (COREUTILS, HISTAR, and 75 BUSYBOXutil- 
ities), (2) those where we quickly run over many appli- 
cations to i nd bugs (an additional 207 BUSYBOXutil- 
ities and 77 MINIXutilities), and (3) those where we 
crosscheck equivalent programs to i nd deeper correct- 
ness bugs (67 BUSYBOXutilities vs. the equivalent 67 in 
COREUTILS). 
Intotal, weranKLEEonmorethan452programs,con- 
taining over 430K total lines of code. To the best of our 
knowledge, this represents an order of magnitude more 
code and distinct programs than checked by prior sym- 
bolic test generation work. Our experiments show: 
1KLEEgets high coverage on a broad set of complex 
programs. Its automatically generated tests covered 
84.5% of the total lines in COREUTILSand 90.5% in 
BUSYBOX(ignoring library code). On average these 
1We ignored utilities that are simply wrapper calls to others, such 
as arch (âuname -mâ) and vdir (âls -l -bâ). 
tests hit over 90% of the lines in each tool (median: 
over 94%), achieving perfect 100% coverage in 16 
COREUTILStools and 31 BUSYBOXtools. 
2KLEEcan get signif i cantly more code coverage than 
a concentrated, sustained manual effort. The roughly 
89-hourrun used to generate COREUTILSline cover- 
age beat the developersâown test suite â built incre- 
mentally over i fteen years â by 16.8%! 
3 With one exception,KLEEachieved these high- 
coverage results on unaltered applications. The sole 
exception, sort in COREUTILS, required a single 
edit to shrink a large buffer that caused problems for 
the constraint solver. 
4KLEEf i nds importanterrorsin heavily-testedcode. It 
foundten fatal errors in COREUTILS(includingthree 
that had escaped detection for 15 years), which ac- 
count for more crashing bugs than were reported in 
2006, 2007 and 2008 combined. It further found 24 
bugs in BUSYBOX, 21 bugs in MINIX, and a security 
vulnerability in HISTARâ a total of 56 serious bugs. 
5 The fact thatKLEEtest cases can be run on the raw 
versionof the code (e.g., compiledwith gcc) greatly 
simplif i es debugging and error reporting. For exam- 
ple, all COREUTILSbugs were conf i rmed and i xed 
within two days and versions of the testsKLEEgen- 
erated were included in the standard regression suite. 
6KLEEis not limited to low-level programming er- 
rors: when used to crosscheck purportedly identical 
BUSYBOXand GNU COREUTILStools, it automat- 
ically found functional correctness errors and a myr- 
iad of inconsistencies. 
7KLEEcan also be applied to non-application code. 
When applied to the core of the HISTARkernel, it 
achieved an average line coverage of 76.4% (with 
disk) and 67.1% (without disk) and found a serious 
security bug. 
The next section gives an overview of our approach. 
Section 3 describesKLEE, focusing on its key optimiza- 
tions. Section4 discusseshowtomodeltheenvironment. 
The heart of the paper is Section 5, which presents our 
experimental results. Finally, Section 6 describes related 
work and Section 7 concludes.1 / 1ChatGPTChatGPTThe text describes a paper about KLEE, a symbolic execution tool designed to generate test inputs automatically for robust, deep checking of a broad range of applications. This tool is an evolution of a previous tool, EXE.The authors' main goal was to demonstrate that symbolic execution could consistently achieve high code coverage on real applications, addressing concerns about the exponential number of paths through code and the challenges in handling code that interacts with the environment. KLEE's method is to substitute program inputs with symbolic values and follows all branches of execution, creating a set of constraints (path condition) for each path. When a path terminates or hits a bug, a test case can be generated.In terms of results, KLEE showed promising outcomes. They applied KLEE to all 89 programs in the latest stable version of GNU COREUTILS (version 6.10), BUSYBOX, the latest release for MINIX, and the HISTAR operating system kernel.For example, on COREUTILS and BUSYBOX, KLEE's automatically generated tests covered 84.5% and 90.5% of the total lines, respectively. KLEE also found significant errors in heavily-tested code, identifying ten fatal errors in COREUTILS, including three that had been missed for 15 years, 24 bugs in BUSYBOX, 21 bugs in MINIX, and a security vulnerability in HISTAR.The authors also pointed out that KLEE is not limited to low-level programming errors and can be used to crosscheck identical tools, finding functional correctness errors and inconsistencies. Furthermore, KLEE can be applied to non-application code, demonstrated by their application to the HISTAR kernel, where they achieved an average line coverage of 76.4% and found a serious security bug.1 / 1KLEE: Unassisted and Automatic Generation of High-Coverage 
Tests for Complex Systems Programs 
Cristian Cadar, Daniel Dunbar, Dawson Engler 
â 
Stanford University 
Abstract 
We present a new symbolic execution tool,KLEE, ca- 
pable of automatically generating tests that achieve 
high coverage on a diverse set of complex and 
environmentally-intensive programs. We usedKLEEto 
thoroughly check all 89 stand-alone programs in the 
GNU COREUTILSutility suite, which form the core 
user-level environmentinstalled on millions of Unix sys- 
tems, and arguably are the single most heavily tested set 
of open-source programs in existence.KLEE-generated 
tests achieve high line coverage â on average over 90% 
per tool (median: over 94%) â and signif i cantly beat 
the coverage of the developersâ own hand-written test 
suites. When we did the same for 75 equivalent tools in 
the BUSYBOXembedded system suite, results were even 
better, including 100% coverage on 31 of them. 
We also usedKLEEas a bug i ndingtool, applyingit to 
452 applications (over 430K total lines of code), where 
it found 56 serious bugs, including three in COREUTILS 
that had been missed for over 15 years.Finally, we 
usedKLEEto cross-check purportedly identical BUSY- 
BOXand COREUTILSutilities, i nding functional cor- 
rectness errors and a myriad of inconsistencies. 
1Introduction 
Many classes of errors, such as functional correctness 
bugs, are diff i cult to i nd without executing a piece of 
code. The importance of such testing â combined with 
the diff i culty and poor performance of random and man- 
ual approaches â has led to much recent work in us- 
ing symbolic execution to automatically generate test in- 
puts [11,14â16,20â22,24,26,27,36]. At a high-level, 
these tools use variations on the following idea: Instead 
of running code on manually- or randomly-constructed 
input, they run it on symbolic input initially allowed to 
be âanything.â They substitute programinputs with sym- 
âAuthor names are in alphabetical order. Daniel Dunbar is the main 
author of theKLEEsystem. 
bolicvaluesandreplacecorrespondingconcreteprogram 
operations with ones that manipulate symbolic values. 
When program execution branches based on a symbolic 
value, the system (conceptually) follows both branches, 
on each path maintaining a set of constraints called the 
path condition which must hold on execution of that 
path. When a path terminates or hits a bug, a test case 
can be generated by solving the current path condition 
for concrete values. Assuming deterministic code, feed- 
ing this concrete input to a raw, unmodif i ed version of 
the checked code will make it follow the same path and 
hit the same bug. 
Results are promising.1 / 1ChatGPTChatGPTThe text is an abstract and introduction for a paper discussing a symbolic execution tool called KLEE. The tool has been developed to automatically generate high coverage tests for various complex and environmentally-intensive programs.KLEE was applied to thoroughly check all 89 stand-alone programs in the GNU COREUTILS utility suite. These utilities form the core user-level environment installed on millions of Unix systems and are arguably the most tested set of open-source programs. KLEE-generated tests achieved impressive line coverage, exceeding 90% per tool on average (over 94% median) and significantly surpassing the coverage of developer-created test suites. Applying KLEE to 75 equivalent tools in the BUSYBOX embedded system suite achieved even better results, including 100% coverage on 31 of them.KLEE was also used as a bug finding tool, applied to 452 applications (totalling over 430K lines of code). It found 56 serious bugs, including three in COREUTILS that had gone unnoticed for over 15 years. KLEE was used to cross-check identical BUSYBOX and COREUTILS utilities, discovering functional correctness errors and many inconsistencies.The authors explain the concept of symbolic execution: the process of running code not on manually or randomly constructed input, but on symbolic input that can initially be anything. The program inputs are substituted with symbolic values, and when the program execution branches based on a symbolic value, the system conceptually follows both branches. Each path maintains a set of constraints (the path condition), and when a path hits a bug or terminates, a test case can be generated by solving the path condition for concrete values. Assuming deterministic code, feeding this concrete input to the unchecked version of the code will cause it to follow the same path and hit the same bug. Initial results from this approach are promising.1 / 1# Lint as: python3
"""Pseudocode description of the MuZero algorithm."""
# pylint: disable=unused-argument
# pylint: disable=missing-docstring
# pylint: disable=g-explicit-length-test

import collections
import math
import typing
from typing import Dict, List, Optional

import numpy
import tensorflow as tf

##########################
####### Helpers ##########

MAXIMUM_FLOAT_VALUE = float('inf')

KnownBounds = collections.namedtuple('KnownBounds', ['min', 'max'])


class MinMaxStats(object):
  """A class that holds the min-max values of the tree."""

  def __init__(self, known_bounds: Optional[KnownBounds]):
    self.maximum = known_bounds.max if known_bounds else -MAXIMUM_FLOAT_VALUE
    self.minimum = known_bounds.min if known_bounds else MAXIMUM_FLOAT_VALUE

  def update(self, value: float):
    self.maximum = max(self.maximum, value)
    self.minimum = min(self.minimum, value)

  def normalize(self, value: float) -> float:
    if self.maximum > self.minimum:
      # We normalize only when we have set the maximum and minimum values.
      return (value - self.minimum) / (self.maximum - self.minimum)
    return value


class MuZeroConfig(object):

  def __init__(self,
               action_space_size: int,
               max_moves: int,
               discount: float,
               dirichlet_alpha: float,
               num_simulations: int,
               batch_size: int,
               td_steps: int,
               num_actors: int,
               lr_init: float,
               lr_decay_steps: float,
               visit_softmax_temperature_fn,
               known_bounds: Optional[KnownBounds] = None):
    ### Self-Play
    self.action_space_size = action_space_size
    self.num_actors = num_actors

    self.visit_softmax_temperature_fn = visit_softmax_temperature_fn
    self.max_moves = max_moves
    self.num_simulations = num_simulations
    self.discount = discount

    # Root prior exploration noise.
    self.root_dirichlet_alpha = dirichlet_alpha
    self.root_exploration_fraction = 0.25

    # UCB formula
    self.pb_c_base = 19652
    self.pb_c_init = 1.25

    # If we already have some information about which values occur in the
    # environment, we can use them to initialize the rescaling.
    # This is not strictly necessary, but establishes identical behaviour to
    # AlphaZero in board games.
    self.known_bounds = known_bounds

    ### Training
    self.training_steps = int(1000e3)
    self.checkpoint_interval = int(1e3)
    self.window_size = int(1e6)
    self.batch_size = batch_size
    self.num_unroll_steps = 5
    self.td_steps = td_steps

    self.weight_decay = 1e-4
    self.momentum = 0.9

    # Exponential learning rate schedule
    self.lr_init = lr_init
    self.lr_decay_rate = 0.1
    self.lr_decay_steps = lr_decay_steps

  def new_game(self):
    return Game(self.action_space_size, self.discount)


def make_board_game_config(action_space_size: int, max_moves: int,
                           dirichlet_alpha: float,
                           lr_init: float) -> MuZeroConfig:

  def visit_softmax_temperature(num_moves, training_steps):
    if num_moves < 30:
      return 1.0
    else:
      return 0.0  # Play according to the max.

  return MuZeroConfig(
      action_space_size=action_space_size,
      max_moves=max_moves,
      discount=1.0,
      dirichlet_alpha=dirichlet_alpha,
      num_simulations=800,
      batch_size=2048,
      td_steps=max_moves,  # Always use Monte Carlo return.
      num_actors=3000,
      lr_init=lr_init,
      lr_decay_steps=400e3,
      visit_softmax_temperature_fn=visit_softmax_temperature,
      known_bounds=KnownBounds(-1, 1))


def make_go_config() -> MuZeroConfig:
  return make_board_game_config(
      action_space_size=362, max_moves=722, dirichlet_alpha=0.03, lr_init=0.01)


def make_chess_config() -> MuZeroConfig:
  return make_board_game_config(
      action_space_size=4672, max_moves=512, dirichlet_alpha=0.3, lr_init=0.1)


def make_shogi_config() -> MuZeroConfig:
  return make_board_game_config(
      action_space_size=11259, max_moves=512, dirichlet_alpha=0.15, lr_init=0.1)


def make_atari_config() -> MuZeroConfig:

  def visit_softmax_temperature(num_moves, training_steps):
    if training_steps < 500e3:
      return 1.0
    elif training_steps < 750e3:
      return 0.5
    else:
      return 0.25

  return MuZeroConfig(
      action_space_size=18,
      max_moves=27000,  # Half an hour at action repeat 4.
      discount=0.997,
      dirichlet_alpha=0.25,
      num_simulations=50,
      batch_size=1024,
      td_steps=10,
      num_actors=350,
      lr_init=0.05,
      lr_decay_steps=350e3,
      visit_softmax_temperature_fn=visit_softmax_temperature)


class Action(object):

  def __init__(self, index: int):
    self.index = index

  def __hash__(self):
    return self.index

  def __eq__(self, other):
    return self.index == other.index

  def __gt__(self, other):
    return self.index > other.index


class Player(object):
  pass


class Node(object):

  def __init__(self, prior: float):
    self.visit_count = 0
    self.to_play = -1
    self.prior = prior
    self.value_sum = 0
    self.children = {}
    self.hidden_state = None
    self.reward = 0

  def expanded(self) -> bool:
    return len(self.children) > 0

  def value(self) -> float:
    if self.visit_count == 0:
      return 0
    return self.value_sum / self.visit_count


class ActionHistory(object):
  """Simple history container used inside the search.

  Only used to keep track of the actions executed.
  """

  def __init__(self, history: List[Action], action_space_size: int):
    self.history = list(history)
    self.action_space_size = action_space_size

  def clone(self):
    return ActionHistory(self.history, self.action_space_size)

  def add_action(self, action: Action):
    self.history.append(action)

  def last_action(self) -> Action:
    return self.history[-1]

  def action_space(self) -> List[Action]:
    return [Action(i) for i in range(self.action_space_size)]

  def to_play(self) -> Player:
    return Player()


class Environment(object):
  """The environment MuZero is interacting with."""

  def step(self, action):
    pass


class Game(object):
  """A single episode of interaction with the environment."""

  def __init__(self, action_space_size: int, discount: float):
    self.environment = Environment()  # Game specific environment.
    self.history = []
    self.rewards = []
    self.child_visits = []
    self.root_values = []
    self.action_space_size = action_space_size
    self.discount = discount

  def terminal(self) -> bool:
    # Game specific termination rules.
    pass

  def legal_actions(self) -> List[Action]:
    # Game specific calculation of legal actions.
    return []

  def apply(self, action: Action):
    reward = self.environment.step(action)
    self.rewards.append(reward)
    self.history.append(action)

  def store_search_statistics(self, root: Node):
    sum_visits = sum(child.visit_count for child in root.children.values())
    action_space = (Action(index) for index in range(self.action_space_size))
    self.child_visits.append([
        root.children[a].visit_count / sum_visits if a in root.children else 0
        for a in action_space
    ])
    self.root_values.append(root.value())

  def make_image(self, state_index: int):
    # Game specific feature planes.
    return []

  def make_target(self, state_index: int, num_unroll_steps: int, td_steps: int,
                  to_play: Player):
    # The value target is the discounted root value of the search tree N steps
    # into the future, plus the discounted sum of all rewards until then.
    targets = []
    for current_index in range(state_index, state_index + num_unroll_steps + 1):
      bootstrap_index = current_index + td_steps
      if bootstrap_index < len(self.root_values):
        value = self.root_values[bootstrap_index] * self.discount**td_steps
      else:
        value = 0

      for i, reward in enumerate(self.rewards[current_index:bootstrap_index]):
        value += reward * self.discount**i  # pytype: disable=unsupported-operands

      # For simplicity the network always predicts the most recently received
      # reward, even for the initial representation network where we already
      # know this reward.
      if current_index > 0 and current_index <= len(self.rewards):
        last_reward = self.rewards[current_index - 1]
      else:
        last_reward = 0

      if current_index < len(self.root_values):
        targets.append((value, last_reward, self.child_visits[current_index]))
      else:
        # States past the end of games are treated as absorbing states.
        targets.append((0, last_reward, []))
    return targets

  def to_play(self) -> Player:
    return Player()

  def action_history(self) -> ActionHistory:
    return ActionHistory(self.history, self.action_space_size)


class ReplayBuffer(object):

  def __init__(self, config: MuZeroConfig):
    self.window_size = config.window_size
    self.batch_size = config.batch_size
    self.buffer = []

  def save_game(self, game):
    if len(self.buffer) > self.window_size:
      self.buffer.pop(0)
    self.buffer.append(game)

  def sample_batch(self, num_unroll_steps: int, td_steps: int):
    games = [self.sample_game() for _ in range(self.batch_size)]
    game_pos = [(g, self.sample_position(g)) for g in games]
    return [(g.make_image(i), g.history[i:i + num_unroll_steps],
             g.make_target(i, num_unroll_steps, td_steps, g.to_play()))
            for (g, i) in game_pos]

  def sample_game(self) -> Game:
    # Sample game from buffer either uniformly or according to some priority.
    return self.buffer[0]

  def sample_position(self, game) -> int:
    # Sample position from game either uniformly or according to some priority.
    return -1


class NetworkOutput(typing.NamedTuple):
  value: float
  reward: float
  policy_logits: Dict[Action, float]
  hidden_state: List[float]


class Network(object):

  def initial_inference(self, image) -> NetworkOutput:
    # representation + prediction function
    return NetworkOutput(0, 0, {}, [])

  def recurrent_inference(self, hidden_state, action) -> NetworkOutput:
    # dynamics + prediction function
    return NetworkOutput(0, 0, {}, [])

  def get_weights(self):
    # Returns the weights of this network.
    return []

  def training_steps(self) -> int:
    # How many steps / batches the network has been trained for.
    return 0


class SharedStorage(object):

  def __init__(self):
    self._networks = {}

  def latest_network(self) -> Network:
    if self._networks:
      return self._networks[max(self._networks.keys())]
    else:
      # policy -> uniform, value -> 0, reward -> 0
      return make_uniform_network()

  def save_network(self, step: int, network: Network):
    self._networks[step] = network


##### End Helpers ########
##########################


# MuZero training is split into two independent parts: Network training and
# self-play data generation.
# These two parts only communicate by transferring the latest network checkpoint
# from the training to the self-play, and the finished games from the self-play
# to the training.
def muzero(config: MuZeroConfig):
  storage = SharedStorage()
  replay_buffer = ReplayBuffer(config)

  for _ in range(config.num_actors):
    launch_job(run_selfplay, config, storage, replay_buffer)

  train_network(config, storage, replay_buffer)

  return storage.latest_network()


##################################
####### Part 1: Self-Play ########


# Each self-play job is independent of all others; it takes the latest network
# snapshot, produces a game and makes it available to the training job by
# writing it to a shared replay buffer.
def run_selfplay(config: MuZeroConfig, storage: SharedStorage,
                 replay_buffer: ReplayBuffer):
  while True:
    network = storage.latest_network()
    game = play_game(config, network)
    replay_buffer.save_game(game)


# Each game is produced by starting at the initial board position, then
# repeatedly executing a Monte Carlo Tree Search to generate moves until the end
# of the game is reached.
def play_game(config: MuZeroConfig, network: Network) -> Game:
  game = config.new_game()

  while not game.terminal() and len(game.history) < config.max_moves:
    # At the root of the search tree we use the representation function to
    # obtain a hidden state given the current observation.
    root = Node(0)
    current_observation = game.make_image(-1)
    expand_node(root, game.to_play(), game.legal_actions(),
                network.initial_inference(current_observation))
    add_exploration_noise(config, root)

    # We then run a Monte Carlo Tree Search using only action sequences and the
    # model learned by the network.
    run_mcts(config, root, game.action_history(), network)
    action = select_action(config, len(game.history), root, network)
    game.apply(action)
    1 / 1ChatGPTChatGPTYour code is incomplete. After game.apply(action), you need to store the root for later analysis and continue the loop until the game ends. Below is the missing part of your code.pythonCopy code    game.store_search_statistics(root)
  return game
So the play_game function should be like:pythonCopy codedef play_game(config: MuZeroConfig, network: Network) -> Game:
  game = config.new_game()

  while not game.terminal() and len(game.history) < config.max_moves:
    # At the root of the search tree we use the representation function to
    # obtain a hidden state given the current observation.
    root = Node(0)
    current_observation = game.make_image(-1)
    expand_node(root, game.to_play(), game.legal_actions(),
                network.initial_inference(current_observation))
    add_exploration_noise(config, root)

    # We then run a Monte Carlo Tree Search using only action sequences and the
    # model learned by the network.
    run_mcts(config, root, game.action_history(), network)
    action = select_action(config, len(game.history), root, network)
    game.apply(action)
    game.store_search_statistics(root)
  return game
Once an action is selected and applied to the game, store_search_statistics(root) is called to store the MCTS statistics from the root for future training.Please keep in mind that this is just the self-play part of the MuZero algorithm, where the agent plays the game against itself to generate data for training. The training part is another substantial portion of the algorithm, which you will need to implement separately.1 / 1game.store_search_statistics(root)
  return game


# Core Monte Carlo Tree Search algorithm.
# To decide on an action, we run N simulations, always starting at the root of
# the search tree and traversing the tree according to the UCB formula until we
# reach a leaf node.
def run_mcts(config: MuZeroConfig, root: Node, action_history: ActionHistory,
             network: Network):
  min_max_stats = MinMaxStats(config.known_bounds)

  for _ in range(config.num_simulations):
    history = action_history.clone()
    node = root
    search_path = [node]

    while node.expanded():
      action, node = select_child(config, node, min_max_stats)
      history.add_action(action)
      search_path.append(node)

    # Inside the search tree we use the dynamics function to obtain the next
    # hidden state given an action and the previous hidden state.
    parent = search_path[-2]
    network_output = network.recurrent_inference(parent.hidden_state,
                                                 history.last_action())
    expand_node(node, history.to_play(), history.action_space(), network_output)

    backpropagate(search_path, network_output.value, history.to_play(),
                  config.discount, min_max_stats)


def select_action(config: MuZeroConfig, num_moves: int, node: Node,
                  network: Network):
  visit_counts = [
      (child.visit_count, action) for action, child in node.children.items()
  ]
  t = config.visit_softmax_temperature_fn(
      num_moves=num_moves, training_steps=network.training_steps())
  _, action = softmax_sample(visit_counts, t)
  return action


# Select the child with the highest UCB score.
def select_child(config: MuZeroConfig, node: Node,
                 min_max_stats: MinMaxStats):
  _, action, child = max(
      (ucb_score(config, node, child, min_max_stats), action,
       child) for action, child in node.children.items())
  return action, child


# The score for a node is based on its value, plus an exploration bonus based on
# the prior.
def ucb_score(config: MuZeroConfig, parent: Node, child: Node,
              min_max_stats: MinMaxStats) -> float:
  pb_c = math.log((parent.visit_count + config.pb_c_base + 1) /
                  config.pb_c_base) + config.pb_c_init
  pb_c *= math.sqrt(parent.visit_count) / (child.visit_count + 1)

  prior_score = pb_c * child.prior
  if child.visit_count > 0:
    value_score = child.reward + config.discount * min_max_stats.normalize(
        child.value())
  else:
    value_score = 0
  return prior_score + value_score


# We expand a node using the value, reward and policy prediction obtained from
# the neural network.
def expand_node(node: Node, to_play: Player, actions: List[Action],
                network_output: NetworkOutput):
  node.to_play = to_play
  node.hidden_state = network_output.hidden_state
  node.reward = network_output.reward
  policy = {a: math.exp(network_output.policy_logits[a]) for a in actions}
  policy_sum = sum(policy.values())
  for action, p in policy.items():
    node.children[action] = Node(p / policy_sum)


# At the end of a simulation, we propagate the evaluation all the way up the
# tree to the root.
def backpropagate(search_path: List[Node], value: float, to_play: Player,
                  discount: float, min_max_stats: MinMaxStats):
  for node in reversed(search_path):
    node.value_sum += value if node.to_play == to_play else -value
    node.visit_count += 1
    min_max_stats.update(node.value())

    value = node.reward + discount * value


# At the start of each search, we add dirichlet noise to the prior of the root
# to encourage the search to explore new actions.
def add_exploration_noise(config: MuZeroConfig, node: Node):
  actions = list(node.children.keys())
  noise = numpy.random.dirichlet([config.root_dirichlet_alpha] * len(actions))
  frac = config.root_exploration_fraction
  for a, n in zip(actions, noise):
    node.children[a].prior = node.children[a].prior * (1 - frac) + n * frac


######### End Self-Play ##########
##################################

##################################
####### Part 2: Training #########


def train_network(config: MuZeroConfig, storage: SharedStorage,
                  replay_buffer: ReplayBuffer):
  network = Network()
  learning_rate = config.lr_init * config.lr_decay_rate**(
      tf.train.get_global_step() / config.lr_decay_steps)
  optimizer = tf.train.MomentumOptimizer(learning_rate, config.momentum)

  for i in range(config.training_steps):
    if i % config.checkpoint_interval == 0:
      storage.save_network(i, network)
    batch = replay_buffer.sample_batch(config.num_unroll_steps, config.td_steps)
    update_weights(optimizer, network, batch, config.weight_decay)
  storage.save_network(config.training_steps, network)


def scale_gradient(tensor, scale):
  """Scales the gradient for the backward pass."""
  return tensor * scale + tf.stop_gradient(tensor) * (1 - scale)


def update_weights(optimizer: tf.train.Optimizer, network: Network, batch,
                   weight_decay: float):
  loss = 0
  for image, actions, targets in batch:
    # Initial step, from the real observation.
    value, reward, policy_logits, hidden_state = network.initial_inference(
        image)
    predictions = [(1.0, value, reward, policy_logits)]

    # Recurrent steps, from action and previous hidden state.
    for action in actions:
      value, reward, policy_logits, hidden_state = network.recurrent_inference(
          hidden_state, action)
      predictions.append((1.0 / len(actions), value, reward, policy_logits))

      hidden_state = scale_gradient(hidden_state, 0.5)

    for prediction, target in zip(predictions, targets):
      gradient_scale, value, reward, policy_logits = prediction
      target_value, target_reward, target_policy = target

      l = (
          scalar_loss(value, target_value) +
          scalar_loss(reward, target_reward) +
          tf.nn.softmax_cross_entropy_with_logits(
              logits=policy_logits, labels=target_policy))

      loss += scale_gradient(l, gradient_scale)

  for weights in network.get_weights():
    loss += weight_decay * tf.nn.l2_loss(weights)

  optimizer.minimize(loss)


def scalar_loss(prediction, target) -> float:
  # MSE in board games, cross entropy between categorical values in Atari.
  return -1

######### End Training ###########
##################################

################################################################################
############################# End of pseudocode ################################
################################################################################


# Stubs to make the typechecker happy.
def softmax_sample(distribution, temperature: float):
  return 0, 0


def launch_job(f, *args):
  f(*args)


def make_uniform_network():
  return Network()1 / 1ChatGPTChatGPTThis is a high-level pseudocode overview of the MuZero algorithm, which is a model-based reinforcement learning algorithm. It's organized into several sections:Self-play: It first begins with a self-play part, where the AI plays games against itself to generate data. This is done using a Monte Carlo Tree Search (MCTS) to make decisions, and as the AI explores the game tree, it stores the game states and actions it encounters.Training: This is where the neural network model is trained on the data generated by the self-play. The network is optimized using gradient descent to minimize the difference between the predicted and actual outcomes.Some specific functions and sections:run_mcts: This function runs the MCTS algorithm to guide the AI's decisions during self-play. It includes a loop over the number of desired simulations, and for each simulation, it descends the game tree by selecting the child node with the highest Upper Confidence Bound (UCB) score until it reaches a leaf node.select_child: This function is used to select the child node to visit next during the MCTS. It uses the UCB score to balance exploitation and exploration.ucb_score: This function calculates the UCB score for a node. The UCB score is a combination of the value of the node (exploitation) and an exploration bonus based on the prior probability of the node.expand_node: This function is used to expand a node in the game tree. It takes as input the node to expand and the output of the neural network, which includes the predicted value, reward, and policy for the node.backpropagate: After a simulation is complete, this function is used to update the value and visit count of each node in the path from the leaf node back to the root.train_network: This is the main function for training the neural network. It updates the weights of the network based on the data collected during self-play.update_weights: This function is called within train_network to perform a single update of the network weights.Please note that this is an overview of the MuZero algorithm and is missing many implementation details. In particular, the training function currently returns -1 for the scalar_loss function, which would not be correct in a real implementation. The actual loss function would depend on the specific problem and could be mean squared error for regression problems or cross-entropy loss for classification problems, for example.Please let me know if you have specific questions about any part of this pseudocode, and I'll be happy to help clarify!1 / 1Faster sorting algorithms discovered using 
deep reinforcement learning 
Daniel J. Mankowitz1,3ââ, Andrea Michi1,3, Anton Zhernov1,3, Marco Gelmi1,3, Marco Selvi1,3, 
Cosmin Paduraru1,3, Edouard Leurent1,3, Shariq Iqbal1, Jean-Baptiste Lespiau1, Alex Ahern1, 
Thomas KÃ¶ppe1, Kevin Millikin1, Stephen Gaffney1, Sophie Elster1, Jackson Broshear1, 
Chris Gamble1, Kieran Milan1, Robert Tung1, Minjae Hwang2, Taylan Cemgil1, 
Mohammadamin Barekatain1, Yujia Li1, Amol Mandhane1, Thomas Hubert1, 
Julian Schrittwieser1, Demis Hassabis1, Pushmeet Kohli1, Martin Riedmiller1, Oriol Vinyals1 & 
David Silver1 
Fundamental algorithms such as sorting or hashing are used trillions of times on any 
given day1. As demand for computation grows, it has become critical for these 
algorithms to be as performant as possible. Whereas remarkable progress has been 
achieved in the past2, making further improvements on the ef i ciency of these 
routines has proved challenging for both human scientists and computational 
approaches. Here we show how artif i cial intelligence can go beyond the current state 
of the art by discovering hitherto unknown routines. To realize this, we formulated the 
task of fi nding a better sorting routine as a single-player game. We then trained a new 
deep reinforcement learning agent, AlphaDev, to play this game. AlphaDev 
discovered small sorting algorithms from scratch that outperformed previously 
known human benchmarks. These algorithms have been integrated into the LLVM 
standard C++ sort library3. This change to this part of the sort library represents the 
replacement of a component with an algorithm that has been automatically 
discovered using reinforcement learning. We also present results in extra domains, 
showcasing the generality of the approach. 
Human intuition and know-how have been crucial in improving algo- 
rithms. However, many algorithms have reached a stage whereby 
human experts have not been able to optimize them further, leading 
to an ever-growing computational bottleneck. The work in classical 
program synthesis literature, spanning many decades, aims to gen- 
erate correct programs and/or optimize programs using proxies for 
latency. These include enumerative search techniques4â7 and stochastic 
search5,6,8â10 as well as the more recent trend of using deep learning in 
program synthesis for generating correct programs11â16. Using deep 
reinforcement learning (DRL), we can take this a step further by generat- 
ing correct and performant algorithms by optimizing for actual meas- 
ured latency at the CPU instruction level, by more efficiently searching 
and considering the space of correct and fast programs compared to 
previous work. 
One of the fundamental questions in computer science is how to 
sort a sequence17â20. This is taught in elementary computer science 
classes around the world21,22 and is used ubiquitously by a vast range of 
applications23â25. Decades of computer science research have focused 
on discovering and optimizing sorting algorithms26â28. A key component 
of practical solutions is a small sort over a short sequence of elements; 
this algorithm is called repeatedly when sorting large arrays that use 
divide-and-conquer approaches29. In this work, we focus on two types 
of small sort algorithm: (1) the fixed sort and (2) the variable sort. Fixed 
sort algorithms sort sequences of a fixed length (for example, sort 3 
can only sort sequences of length 3), whereas variable sort algorithms 
can sort a sequence of varying size (for example, variable sort 5 can sort 
sequences ranging from one to five elements). 
We formulate the problem of discovering new, efficient sorting algo- 
rithms as a single-player game that we refer to as AssemblyGame. In this 
game, the player selects a series of low-level CPU instructions, which 
we refer to as assembly instructions30, to combine to yield a new and 
efficient sorting algorithm. This is challenging as the player needs to 
consider the combinatorial space of assembly instructions to yield an 
algorithm that is both provably correct and fast. The hardness of the 
AssemblyGame arises not only from the size of the search space, which 
is similar to extremely challenging games such as chess (10120 games)31 
and Go (10700 games)32, but also from the nature of the reward function. 
A single incorrect instruction in the AssemblyGame can potentially 
invalidate the entire algorithm, making exploration in this space of 
games incredibly challenging. 
To play the game, we introduce AlphaDev, a learning agent that is 
trained to search for correct and efficient algorithms. This agent is 
comprised of two core components, namely (1) a learning algorithm 
and (2) a representation function. The AlphaDev learning algorithm 
can incorporate both DRL as well as stochastic search optimization 
algorithms to play AssemblyGame. The primary learning algorithm 
in AlphaDev is an extension of AlphaZero33, a well-known DRL algo- 
rithm, in which a neural network is trained to guide a search to solve 
https://doi.org/10.1038/s41586-023-06004-9 
Received: 25 July 2022 
Accepted: 23 March 2023 
Published online: 7 June 2023 
Open access 
Check for updates 
1Deepmind, London, UK. 2Google, Mountain View, CA, USA. 3These authors contributed equally: Daniel J. Mankowitz, Andrea Michi, Anton Zhernov, Marco Gelmi, Marco Selvi, Cosmin Paduraru, 
Edouard Leurent. âe-mail: dmankowitz@deepmind.com 
258 | Nature | Vol 618 | 8 June 2023 
Article 
AssemblyGame. The representation function is interchangeable and 
captures the underlying structure of assembly programs. The primary 
AlphaDev representation is based on Transformers34. 
Using AlphaDev, we have discovered fixed and variable sort algo- 
rithms from scratch that are both new and more efficient than the 
state-of-the-art human benchmarks. The fixed sort solutions for sort 3, 
sort 4 and sort 5 discovered by AlphaDev have been integrated into the 
standard sort function in the LLVM standard C++ library3. This library 
is used by several million users including universities and numerous 
international companies35. In addition, we analyse the new algorithm 
discoveries, compare AlphaDev to stochastic search optimization 
approaches and apply AlphaDev to further domains to showcase the 
generality of the approach. 
Representing algorithms as low-level CPU instructions 
When compiling algorithms to machine code from a high level language 
such as C++ (for example, the sorting function in Fig. 1a), the algorithm 
is first compiled into assembly (Fig. 1b). The assembler then converts 
the assembly program into executable machine code. In this work, we 
optimize algorithms at the assembly level30. In a typical assembly pro- 
gram, the values are copied from memory into registers, manipulated 
between registers and then written back to memory. The set of assembly 
instructions supported depends on the processor architecture. For the 
purposes of this work, we focus on a subset of assembly instructions 
supported by the x86 processor architecture using the AT&T syntax36. 
Each instruction is of the format Opcodeâ¨OperandA,âOperandBâ©. An 
example instruction is mov<A,B>, which is defined as move a value 
from source (A) to destination (B). Further instruction definitions such 
as compare (cmp<A,B>), conditional move (cmovX<A,B>) and jump 
(jX<A>) can be found in Extended Data Table 1. In the example in Fig. 1b, 
%eax, %ecx, %edx, %edi correspond to four different register locations 
and (%rsi), 4(%rsi) correspond to two different memory locations. The 
symbol $2 is a placeholder for a constant value, which corresponds to 
the length of the vector in this example. We use the terms assembly 
program and assembly algorithm interchangeably in this work. This 
is because AlphaDev builds an assembly program from scratch, from 
an initially unordered set of instructions, each time it plays Assemb- 
lyGame, defining a new and efficient algorithm. 
DRL for discovering faster algorithms 
In this section, we formulate optimizing algorithms at the CPU instruc- 
tion level as a reinforcement learning (RL) problem37, in which the 
environment is modelled as a single-player game that we refer to as 
AssemblyGame. Each state in this game is defined as a vector Stâ=ââ¨Pt,âZtâ© 
where Pt is a representation of the algorithm generated thus far in 
the game and Zt represents the state of memory and registers after 
executing the current algorithm on a set of predefined inputs. As seen 
in Fig. 2a, at timestep t, the player receives the current state St and 
executes an action at. This involves appending a legal assembly instruc- 
tion (for example, mov<A,B>) to the current algorithm generated thus 
far. A reward rtis received that comprises both a measure of algorithm 
correctness and latency. Algorithm correctness (Fig. 2b) involves input- 
ting a set of N test sequences into the current algorithm Pt to generate 
N outputs. These outputs are then compared to the expected outputs 
and a correctness reward rt is computed. Latency rewards can be gen- 
erated by either (1) penalizing the agent for increasing the length of 
the algorithm (when length and latency are highly correlated) that we 
refer to as the algorithm length reward, or (2) measuring the actual 
latency of the algorithm. The game is executed for a limited number

AlphaDev discovers new, state-of-the-art sorting algorithms from 
scratch that have been incorporated into the LLVM C++ library, used 
by millions of developers and applications around the world23â25. Both 
AlphaDev and stochastic search are powerful algorithms. An inter- 
esting direction for future research is to investigate combining these 
algorithms together to realize the complementary advantages of both 
approaches. 
It is important to note that AlphaDev can, in theory, generalize to 
functions that do not require exhaustive verification of test cases. 
For example, hashing functions48 as well as cryptographic hashing 
functions49 define function correctness by the number of hashing 
collisions. Therefore, in this case, AlphaDev can optimize for minimiz- 
ing collisions as well as latency. AlphaDev can also, in theory, optimize 
complicated logic components within the body of large, impressive 
functions. We hope that AlphaDev can provide interesting insights and 
inspire new approaches in both the artificial intelligence and program 
synthesis communities.1 / 1ChatGPTChatGPTThis article discusses the application of deep reinforcement learning (DRL) for discovering more efficient sorting algorithms, an essential function used in computer science and software development. The research was conducted by an international team, including DeepMind and Google researchers.The team developed a DRL agent named AlphaDev that is trained to find improved sorting routines. They structured the task of finding a better sorting routine as a single-player game, which they called AssemblyGame. In this game, the player selects a series of low-level CPU instructions, also known as assembly instructions, to create a new, efficient sorting algorithm.The task is challenging because it involves exploring a vast combinatorial space of assembly instructions to find an algorithm that is both correct and fast. AlphaDev incorporates both DRL and stochastic search optimization algorithms to solve AssemblyGame. The agent is equipped with a learning algorithm and a representation function based on Transformers, a popular machine learning model architecture.AlphaDev discovered new and efficient fixed and variable sort algorithms that outperform previous benchmarks established by humans. These algorithms have been integrated into the LLVM standard C++ sort library, a library widely used by millions of developers and applications worldwide.Furthermore, the researchers showed that AlphaDev could be generalized to other domains, highlighting the versatility of the approach. They hope that the use of AlphaDev can provide interesting insights and inspire new approaches in both artificial intelligence and program synthesis communities.1 / 1arXiv:2102.04897 (cs)
[Submitted on 9 Feb 2021 (v1), last revised 5 Nov 2021 (this version, v2)]
Learning State Representations from Random Deep Action-conditional Predictions
Zeyu Zheng, Vivek Veeriah, Risto Vuorio, Richard Lewis, Satinder Singh
Download PDF
Our main contribution in this work is an empirical finding that random General Value Functions (GVFs), i.e., deep action-conditional predictions -- random both in what feature of observations they predict as well as in the sequence of actions the predictions are conditioned upon -- form good auxiliary tasks for reinforcement learning (RL) problems. In particular, we show that random deep action-conditional predictions when used as auxiliary tasks yield state representations that produce control performance competitive with state-of-the-art hand-crafted auxiliary tasks like value prediction, pixel control, and CURL in both Atari and DeepMind Lab tasks. In another set of experiments we stop the gradients from the RL part of the network to the state representation learning part of the network and show, perhaps surprisingly, that the auxiliary tasks alone are sufficient to learn state representations good enough to outperform an end-to-end trained actor-critic baseline. We opensourced our code at this https URL.1 / 1ChatGPTChatGPTThis paper discusses the use of random General Value Functions (GVFs), which are deep action-conditional predictions, as auxiliary tasks for reinforcement learning (RL) problems. The authors found that these random deep action-conditional predictions yield state representations that offer competitive control performance with state-of-the-art hand-crafted auxiliary tasks such as value prediction, pixel control, and CURL in both Atari and DeepMind Lab tasks.Interestingly, the authors also discovered that when they stopped the gradients from the RL part of the network to the state representation learning part of the network, the auxiliary tasks alone were sufficient to learn state representations that outperformed an end-to-end trained actor-critic baseline. This surprising finding suggests that auxiliary tasks might play a more significant role in learning state representations than previously thought.This work can be a significant contribution to the field of reinforcement learning and how we approach the training of such systems. Further exploration of this work could potentially lead to the development of more efficient and effective RL systems. The authors have made their code publicly available, allowing other researchers to validate and build upon their findings.1 / 1Representations from Random Deep Action-conditional Predictions
This repository is the official implementation of Learning State Representations from Random Deep Action-conditional Predictions.

Requirements
Run the following command to create a new conda environment with all dependencies:

conda env create -f conda_cuda11_cp38_linux64.yml
Then activate the conda environment by

conda activate rgvfs
Or if you prefer using your own Python environment, run the following command to install the dependencies:

pip install -r requirements.txt
Atari
OpenAI Gym does not contain the ROMs for the Atari games. Please refer to atari-py for how to download and import the ROMs.

DeepMind Lab
Please refer to the instructions at https://github.com/deepmind/lab for installation.1 / 1ChatGPTChatGPTThis is a description of the official implementation for "Learning State Representations from Random Deep Action-conditional Predictions." The instructions outline how to set up the necessary environment and dependencies to run the code associated with this paper. Here's a simple breakdown of the instructions:Setting up the Conda environment: The first command provided creates a new Conda environment with all the necessary dependencies:bashCopy codeconda env create -f conda_cuda11_cp38_linux64.yml
After the environment is created, you can activate it with the following command:bashCopy codeconda activate rgvfs
Using your own Python environment: If you don't want to use Conda and would rather use your own Python environment, you can install the required dependencies with the following command:bashCopy codepip install -r requirements.txt
Setting up the Atari environment: The OpenAI Gym doesn't include the ROMs for Atari games. You'll have to refer to the instructions provided by atari-py to download and import the ROMs.Setting up DeepMind Lab: For installation of DeepMind Lab, the authors ask you to follow the instructions provided on the official DeepMind Lab GitHub page.Please note that setting up these environments might require additional steps based on your own system configuration. Always make sure to follow any installation instructions closely.1 / 1Zeyu Zheng

I am a fifth-year Ph.D. student at Computer Science and Engineering, University of Michigan, advised by Prof. Satinder Singh. My research interests lie in deep reinforcement learning.

I did my undergraduate at School of Electronics Engineering and Computer Science, Peking University.

Email  /  Google Scholar  /  GitHub


Publications
Adaptive Pairwise Weights for Temporal Credit Assignment
Zeyu Zheng*, Risto Vuorio*, Richard Lewis, Satinder Singh
AAAI Conference on Artificial Intelligence (AAAI), 2022
[arXiv]
Abridged version presented at NeurIPS Deep Reinforcement Learning Workshop, 2020
GrASP: Gradient-Based Affordance Selection for Planning
Vivek Veeriah, Zeyu Zheng, Richard Lewis, Satinder Singh
NeurIPS Deep Reinforcement Learning Workshop, 2021
Learning State Representations from Random Deep Action-conditional Predictions
Zeyu Zheng, Vivek Veeriah, Risto Vuorio, Richard Lewis, Satinder Singh
Neural Information Processing Systems (NeurIPS), 2021
[arXiv] [code]
Automated Multi-layer Optical Design via Deep Reinforcement Learning
Haozhu Wang, Zeyu Zheng, Chengang Ji, L. Jay Guo
Machine Learning: Science and Technology, 2020
[paper] [arXiv] [code]
What Can Learned Intrinsic Rewards Capture?
Zeyu Zheng*, Junhyuk Oh*, Matteo Hessel, Zhongwen Xu, Manuel Kroiss, Hado van Hasselt, David Silver, Satinder Singh
International Conference on Machine Learning (ICML), 2020
[paper] [arXiv]
On Learning Intrinsic Rewards for Policy Gradient Methods
Zeyu Zheng, Junhyuk Oh, Satinder Singh
Neural Information Processing Systems (NeurIPS), 2018
[paper] [arXiv] [code]
Poseidon: An Efficient Communication Architecture for Distributed Deep Learning on GPU Clusters
Hao Zhang, Zeyu Zheng, Shizhen Xu, Wei Dai, Qirong Ho, Xiaodan Liang, Zhiting Hu, Jinliang Wei, Pengtao Xie, Eric P. Xing
USENIX Annual Technical Conference (ATC), 2017
[paper] [arXiv] [release]
Parallelizing Sequential Graph Computations
Wenfei Fan, Jingbo Xu, Yinghui Wu, Wenyuan Yu, Jiaxin Jiang, Zeyu Zheng, Bohan Zhang, Yang Cao, Chao Tian
ACM SIGMOD Conference on Management of Data (SIGMOD), 2017, Best Paper Award
[paper]1 / 1ChatGPTChatGPTZeyu Zheng is a fifth-year Ph.D. student at the Computer Science and Engineering department of the University of Michigan, where he is advised by Prof. Satinder Singh. His research interests are primarily in deep reinforcement learning.Zeyu completed his undergraduate studies at the School of Electronics Engineering and Computer Science at Peking University.He has contributed to several notable publications in the field of AI and deep reinforcement learning, including:"Adaptive Pairwise Weights for Temporal Credit Assignment" (2022), presented at the AAAI Conference on Artificial Intelligence (AAAI). An abridged version of this work was presented at the NeurIPS Deep Reinforcement Learning Workshop in 2020."GrASP: Gradient-Based Affordance Selection for Planning" (2021), presented at the NeurIPS Deep Reinforcement Learning Workshop."Learning State Representations from Random Deep Action-conditional Predictions" (2021), presented at the Neural Information Processing Systems (NeurIPS). This work also has associated code available."Automated Multi-layer Optical Design via Deep Reinforcement Learning" (2020), published in Machine Learning: Science and Technology."What Can Learned Intrinsic Rewards Capture?" (2020), presented at the International Conference on Machine Learning (ICML)."On Learning Intrinsic Rewards for Policy Gradient Methods" (2018), presented at the Neural Information Processing Systems (NeurIPS). This work also has associated code available."Poseidon: An Efficient Communication Architecture for Distributed Deep Learning on GPU Clusters" (2017), presented at the USENIX Annual Technical Conference (ATC)."Parallelizing Sequential Graph Computations" (2017), which won the Best Paper Award at the ACM SIGMOD Conference on Management of Data (SIGMOD).1 / 1arXiv:2106.10316 (cs)
[Submitted on 18 Jun 2021 (v1), last revised 13 Dec 2021 (this version, v2)]
Proper Value Equivalence
Christopher Grimm, AndrÃ© Barreto, Gregory Farquhar, David Silver, Satinder Singh
Download PDF
One of the main challenges in model-based reinforcement learning (RL) is to decide which aspects of the environment should be modeled. The value-equivalence (VE) principle proposes a simple answer to this question: a model should capture the aspects of the environment that are relevant for value-based planning. Technically, VE distinguishes models based on a set of policies and a set of functions: a model is said to be VE to the environment if the Bellman operators it induces for the policies yield the correct result when applied to the functions. As the number of policies and functions increase, the set of VE models shrinks, eventually collapsing to a single point corresponding to a perfect model. A fundamental question underlying the VE principle is thus how to select the smallest sets of policies and functions that are sufficient for planning. In this paper we take an important step towards answering this question. We start by generalizing the concept of VE to order-k counterparts defined with respect to k applications of the Bellman operator. This leads to a family of VE classes that increase in size as kââ. In the limit, all functions become value functions, and we have a special instantiation of VE which we call proper VE or simply PVE. Unlike VE, the PVE class may contain multiple models even in the limit when all value functions are used. Crucially, all these models are sufficient for planning, meaning that they will yield an optimal policy despite the fact that they may ignore many aspects of the environment. We construct a loss function for learning PVE models and argue that popular algorithms such as MuZero can be understood as minimizing an upper bound for this loss. We leverage this connection to propose a modification to MuZero and show that it can lead to improved performance in practice1 / 1ChatGPTChatGPT"Proper Value Equivalence" is a paper authored by Christopher Grimm, AndrÃ© Barreto, Gregory Farquhar, David Silver, and Satinder Singh. The paper tackles a major challenge in model-based reinforcement learning (RL): deciding which aspects of the environment should be modeled. The authors propose the value-equivalence (VE) principle, which suggests that a model should capture the aspects of the environment relevant for value-based planning.Technically, VE distinguishes models based on a set of policies and functions. A model is considered VE to the environment if the Bellman operators it applies to the policies give the correct result when applied to the functions. As the number of policies and functions increase, the VE models reduce in number, with a perfect model being the end point.The authors also explore how to choose the smallest sets of policies and functions necessary for planning. They generalize the VE concept to order-k counterparts in relation to k applications of the Bellman operator. This results in VE classes that increase in size as k approaches infinity.In the limit, all functions become value functions, leading to a specific instantiation of VE called proper VE or PVE. Unlike VE, the PVE class may contain multiple models even when all value functions are used. Importantly, all these models are enough for planning, meaning they can yield an optimal policy even if they ignore many environment aspects.The authors develop a loss function for learning PVE models and argue that popular algorithms like MuZero can be viewed as minimizing an upper bound for this loss. They use this connection to suggest a modification to MuZero and demonstrate that it can lead to improved performance in practice.1 / 1arXiv:2209.13742  [pdf, other]  cs.RO
Optimization-Based Mechanical Perception for Peduncle Localization During Robotic Fruit Harvest

Authors: Miranda Cravetz, Cindy Grimm, Joseph R. Davidson

Abstract: Rising global food demand and harsh working conditions make fruit harvest an important domain to automate. Peduncle localization is an important step for any automated fruit harvesting system, since fruit separation techniques are highly sensitive to peduncle location. Most work on peduncle localization has focused on computer vision, but peduncles can be difficult to visually access due to the clâ¦ â½ More
Submitted 27 September, 2022; originally announced September 2022.

Comments: Submitted to ICRA20231 / 1ChatGPTChatGPT"Optimization-Based Mechanical Perception for Peduncle Localization During Robotic Fruit Harvest" is a paper authored by Miranda Cravetz, Cindy Grimm, and Joseph R. Davidson. It addresses the rising demand for food globally and the challenging working conditions that make automating fruit harvest an important field.An essential aspect of any automated fruit harvesting system is the localization of the peduncle, the stem that connects the fruit to the plant. The authors note that most research on peduncle localization has focused on computer vision. However, visual access to peduncles can be challenging due to the complex structure of the plant and the fruit's position.Unfortunately, the abstract you've provided doesn't include the specific methods or findings that the authors propose to overcome this challenge. The paper seems to suggest a new approach that relies on optimization-based mechanical perception for identifying the location of the peduncle. This could potentially provide more effective and accurate localization, leading to better fruit separation techniques during robotic harvest. The paper has been submitted to ICRA2023, a prominent conference in the field of robotics and automation.For a comprehensive understanding of the methods, experimental setup, and results, you would need to access the full paper.1 / 1arXiv:2206.07201  [pdf, other]  cs.RO
An autonomous robot for pruning modern, planar fruit trees

Authors: Alexander You, Nidhi Parayil, Josyula Gopala Krishna, Uddhav Bhattarai, Ranjan Sapkota, Dawood Ahmed, Matthew Whiting, Manoj Karkee, Cindy M. Grimm, Joseph R. Davidson

Abstract: Dormant pruning of fruit trees is an important task for maintaining tree health and ensuring high-quality fruit. Due to decreasing labor availability, pruning is a prime candidate for robotic automation. However, pruning also represents a uniquely difficult problem for robots, requiring robust systems for perception, pruning point determination, and manipulation that must operate under variable liâ¦ â½ More
Submitted 14 June, 2022; originally announced June 2022.1 / 1The previous model used in this conversation is unavailable. We've switched you to the latest default model.ChatGPTChatGPT"An autonomous robot for pruning modern, planar fruit trees" is a paper authored by Alexander You, Nidhi Parayil, Josyula Gopala Krishna, Uddhav Bhattarai, Ranjan Sapkota, Dawood Ahmed, Matthew Whiting, Manoj Karkee, Cindy M. Grimm, and Joseph R. Davidson. The paper focuses on the autonomous pruning of modern, planar fruit trees.Dormant pruning plays a crucial role in maintaining the health of fruit trees and ensuring high-quality fruit production. However, with a decrease in labor availability, automating the pruning process using robots has become a desirable solution. Pruning presents unique challenges for robots as it requires robust perception systems, precise determination of pruning points, and manipulation capabilities that can adapt to varying tree structures.The abstract does not provide specific details about the methods and techniques employed in the paper. However, it highlights the importance of addressing perception, pruning point determination, and manipulation challenges for successful autonomous pruning. The authors aim to develop an autonomous robot capable of efficiently and effectively pruning modern, planar fruit trees.To gain a comprehensive understanding of the research, including the specific approaches, experimental results, and contributions, it is necessary to access the full paper.1 / 1arXiv:2204.12658  [pdf, other]  cs.RO
Measuring a Robot Hand's Graspable Region using Power and Precision Grasps

Authors: John Morrow, Joshua Campbell, Nuha Nishat, Ravi Balasubramanian, Cindy Grimm

Abstract: The variety of robotic hand designs and actuation schemes makes it difficult to measure a hand's graspable volume. For end-users, this lack of standardized measurements makes it challenging to determine a priori if a robot hand is the right size for grasping an object. We propose a practical hand measurement standard, based on precision and power grasps, that is applicable to a wide variety of robâ¦ â½ More
Submitted 26 April, 2022; originally announced April 2022.

Comments: Significantly changed second version of arXiv:2106.10402. Paper originally submitted to Robotics Automation Letters1 / 1ChatGPTChatGPT"Measuring a Robot Hand's Graspable Region using Power and Precision Grasps" is a paper authored by John Morrow, Joshua Campbell, Nuha Nishat, Ravi Balasubramanian, and Cindy Grimm. The paper addresses the challenge of measuring the graspable volume of a robot hand.With the variety of robotic hand designs and actuation schemes available, there is no standardized measurement for a hand's graspable volume. This lack of standardized measurements makes it difficult for end-users to determine if a robot hand is suitable for grasping a specific object before deployment. To address this issue, the authors propose a practical hand measurement standard based on power and precision grasps.The abstract does not provide specific details about the measurement methodology or techniques used in the paper. However, the authors aim to develop a measurement standard that is applicable to a wide range of robotic hand designs and can provide valuable information to end-users about a hand's graspable region.To gain a more in-depth understanding of the research, including the measurement approach, experimental results, and contributions, it is recommended to access the full paper.1 / 1Grasping Benchmarks: Normalizing for Object Size \& Approximating Hand Workspaces

Authors: John Morrow, Nuha Nishat, Joshua Campbell, Ravi Balasubramanian, Cindy Grimm

Abstract: The varied landscape of robotic hand designs makes it difficult to set a standard for how to measure hand size and to communicate the size of objects it can grasp. Defining consistent workspace measurements would greatly assist scientific communication in robotic grasping research because it would allow researchers to 1) quantitatively communicate an object's relative size to a hand's and 2) approâ¦ â½ More
Submitted 18 June, 2021; originally announced June 2021.

Comments: Submitted to IROS 2021, waiting for response1 / 1ChatGPTChatGPT"Grasping Benchmarks: Normalizing for Object Size & Approximating Hand Workspaces" is a paper authored by John Morrow, Nuha Nishat, Joshua Campbell, Ravi Balasubramanian, and Cindy Grimm. The paper addresses the challenge of setting a standard for measuring hand size and communicating the size of objects that a robotic hand can grasp.With the diverse landscape of robotic hand designs, it is challenging to establish a consistent measurement standard for hand size and workspace. This lack of standardization hinders scientific communication in robotic grasping research. To address this issue, the authors propose defining consistent workspace measurements that can facilitate quantitative communication.The abstract indicates that the proposed approach aims to enable researchers to communicate the relative size of objects to a hand's size in a quantitative manner. Additionally, the authors aim to provide an approximation of hand workspaces, allowing researchers to estimate the range of object sizes that a hand can effectively grasp.To gain further insights into the research, including the methodology, benchmarks, and potential contributions, it is recommended to access the full paper.1 / 1arXiv:2103.02833  [pdf, other]  cs.RO
Semantics-guided Skeletonization of Sweet Cherry Trees for Robotic Pruning

Authors: Alexander You, Cindy Grimm, Abhisesh Silwal, Joseph R. Davidson

Abstract: Dormant pruning for fresh market fruit trees is a relatively unexplored application of agricultural robotics for which few end-to-end systems exist. One of the biggest challenges in creating an autonomous pruning system is the need to reconstruct a model of a tree which is accurate and informative enough to be useful for deciding where to cut. One useful structure for modeling a tree is a skeleton: a 1D, lightweight representation of the geometry and the topology of a tree. This skeletonization problem is an important one within the field of computer graphics, and a number of algorithms have been specifically developed for the task of modeling trees. These skeletonization algorithms have largely addressed the problem as a geometric one. In agricultural contexts, however, the parts of the tree have distinct labels, such as the trunk, supporting branches, etc. This labeled structure is important for understanding where to prune. We introduce an algorithm which produces such a labeled skeleton, using the topological and geometric priors associated with these labels to improve our skeletons. We test our skeletonization algorithm on point clouds from 29 upright fruiting offshoot (UFO) trees and demonstrate a median accuracy of 70% with respect to a human-evaluated gold standard. We also make point cloud scans of 82 UFO trees open-source to other researchers. Our work represents a significant first step towards a robust tree modeling framework which can be used in an autonomous pruning system. â³ Less

Submitted 3 March, 2021; originally announced March 2021.1 / 1ChatGPTChatGPT"Semantics-guided Skeletonization of Sweet Cherry Trees for Robotic Pruning" is a paper authored by Alexander You, Cindy Grimm, Abhisesh Silwal, and Joseph R. Davidson. The paper focuses on the application of agricultural robotics in the context of dormant pruning for fresh market fruit trees.The authors address the challenge of creating an autonomous pruning system by reconstructing an accurate and informative model of a tree that guides the pruning process. They propose using a skeleton, which is a lightweight, one-dimensional representation of the tree's geometry and topology. Skeletonization algorithms from the field of computer graphics have been traditionally used for this purpose. However, in agricultural contexts, the distinct labels associated with different parts of the tree (e.g., trunk, supporting branches) are crucial for pruning decisions.In this paper, the authors introduce an algorithm that produces a labeled skeleton, leveraging both topological and geometric priors associated with the labels. The algorithm is tested on point clouds obtained from 29 upright fruiting offshoot (UFO) trees, demonstrating a median accuracy of 70% compared to a human-evaluated gold standard. Furthermore, the authors provide open-source access to point cloud scans of 82 UFO trees, enabling other researchers to utilize the data.The work presented in the paper is considered a significant step towards developing a robust tree modeling framework for autonomous pruning systems. The proposed semantics-guided skeletonization approach incorporates labeled structure information, enhancing the accuracy and usefulness of the generated skeletons in guiding pruning operations.1 / 1arXiv:2011.03506  [pdf, other]  cs.LG cs.AI
The Value Equivalence Principle for Model-Based Reinforcement Learning

Authors: Christopher Grimm, AndrÃ© Barreto, Satinder Singh, David Silver

Abstract: Learning models of the environment from data is often viewed as an essential component to building intelligent reinforcement learning (RL) agents. The common practice is to separate the learning of the model from its use, by constructing a model of the environment's dynamics that correctly predicts the observed state transitions. In this paper we argue that the limited representational resources of model-based RL agents are better used to build models that are directly useful for value-based planning. As our main contribution, we introduce the principle of value equivalence: two models are value equivalent with respect to a set of functions and policies if they yield the same Bellman updates. We propose a formulation of the model learning problem based on the value equivalence principle and analyze how the set of feasible solutions is impacted by the choice of policies and functions. Specifically, we show that, as we augment the set of policies and functions considered, the class of value equivalent models shrinks, until eventually collapsing to a single point corresponding to a model that perfectly describes the environment. In many problems, directly modelling state-to-state transitions may be both difficult and unnecessary. By leveraging the value-equivalence principle one may find simpler models without compromising performance, saving computation and memory. We illustrate the benefits of value-equivalent model learning with experiments comparing it against more traditional counterparts like maximum likelihood estimation. More generally, we argue that the principle of value equivalence underlies a number of recent empirical successes in RL, such as Value Iteration Networks, the Predictron, Value Prediction Networks, TreeQN, and MuZero, and provides a first theoretical underpinning of those results. â³ Less

Submitted 6 November, 2020; originally announced November 2020.

Comments: NeurIPS-20201 / 1ChatGPTChatGPT"The Value Equivalence Principle for Model-Based Reinforcement Learning" is a paper authored by Christopher Grimm, AndrÃ© Barreto, Satinder Singh, and David Silver. The paper addresses the problem of learning models of the environment in reinforcement learning (RL) settings.Traditionally, RL agents learn a model of the environment's dynamics separate from their use in value-based planning. However, the authors argue that the limited representational resources of model-based RL agents can be better utilized by constructing models that are directly useful for value-based planning.The main contribution of the paper is the introduction of the principle of value equivalence. According to this principle, two models are considered value equivalent with respect to a set of functions and policies if they produce the same Bellman updates. The authors propose a formulation of the model learning problem based on the value equivalence principle and investigate how the choice of policies and functions affects the set of feasible solutions.The paper shows that as the set of policies and functions considered is expanded, the class of value equivalent models shrinks until it collapses to a single model that perfectly describes the environment. By leveraging the value equivalence principle, simpler models can be found without compromising performance, leading to computational and memory savings.The authors provide experiments comparing value-equivalent model learning with traditional approaches like maximum likelihood estimation. The benefits of value-equivalent model learning are illustrated, and the paper argues that the value equivalence principle underlies several empirical successes in RL, such as Value Iteration Networks, the Predictron, Value Prediction Networks, TreeQN, and MuZero. The principle provides a theoretical foundation for understanding these results.1 / 1Optimizing functions with computational graphs
A computational graph is simply a representation of an arithmetic expression using a graph (or in our program, specifically a tree). For example: x + 2 becomes

  +
 / \
x   2
3 (x + 2) becomes

  *
 / \
3   +
   / \
  x   2
Why represent expressions this way? By traversing from root to leaves, we can recursively handle each operation in isolation. For example, we can evaluate each node by recursing down its branches and combining the results according to some rule associated with the value or operation at the node -- let's say the operation at the node is +; then we add the values returned from recursion down the branches. In general, graphs are a convenient format for dealing with the structure of an expression.

In our program, we are not just interested in evaluating expressions, we are interested in optimizing them. This means taking certain designated values or parameters tweaking them so as to make the value of the expression as large or as small as possible.

How do we do this? The technique that this program uses -- and which is fairly ubiquitous in the world of optimization -- is gradient descent. That means that we calculate the gradient of the function with respect to each of its parameters and then add that gradient to the parameter. A gradient is simply a measure of how much the value of the function changes when the value of a parameter changes. For example,



means that y depends on x and increasing x by one roughly increases y by two. Most importantly, it means that increasing x also increases y. If the result of the expression were negative, it would mean that decreasing x would increase y. Adding gradients to parameters maximizes an expression. Subtracting gradients minimizes it.

Once you have the gradients of an expression, you have a very powerful tool for optimization. But getting those gradients can be tricky. My program does so in a particularly efficient way, using an algorithm that I call generalized backpropagation. This algorithm is certainly not original, since there are many programs like Torch and Tensorflow that use it. However, it's not exactly backpropagation because backpropagation applies specifically to the equations of a neural network. Generalized backpropagation takes the principles that make backpropagation so powerful for neural networks and generalizes them to any expression.

I've put together a little video here that explains this algorithm and demonstrates the improvements in speed that you get using the algorithm over the way you might have been taught to calculate gradients in school. It also has a nice demonstration of the power of using this algorithm with a GPU (in the demo, I use a GeForce GTX TITAN X).

Currently the program implements the following operations for scalars and 2d matrices:

negation
squaring
taking absolute value
taking the (elementwise) sign (i.e. +/-)
sigmoid
tanh
multiplication
addition
subtraction
The library is easily extensible for anyone familiar with Rust and CUDA. This library is obviously not as extensive as Tensorflow or Torch, but it implements the same core capabilities. The goal of this project was not to replace either of those libraries, but to get a better understanding of how they work by reimplementing them myself.

Usage
The master branch is configured to work on CPU. For GPU, switch to the gpu branch. To test the program on a function, you need to write that function in main.rs and run cargo run, which compiles and runs the program.

Here's an example program:

let args = HashMap::new();                          // this hashmap can be populated with constants at runtime
let x = Function::param("x", Constant::Scalar(1.)); // a tunable `parameter` Function initialized to 1.
let a = Function::constant(Constant::Scalar(3.));   // a constant scalar Function with value 3.
let f = sq(&(x + a));                               // the function to minimize: (x + a)^2
f.minimize(&args, 0.01, 1000);                      // minimize the function with learning rate of 0.01 and 1000 iterations.
An API is pending. For now, check function/constructors.rs and constant/constructors.rs for different ways to create functions and constants (scalars/matrices).

As for arithmetic operations, most can either take a Function type or an &Function type (a reference to a Function). In general, it is always safe to provide a reference in place of a Function since the borrow checker will sometimes complain otherwise.

Performance
In the next two graphs we compare performance on an LSTM optimizing a randomly generated dataset that takes a sequence of two matrices as input. The first graph compares a GPU with a CPU as the number of tunable parameters increases:1 / 1ChatGPTChatGPTThe provided text explains the concept of optimizing functions using computational graphs. A computational graph is a graphical representation of an arithmetic expression, where nodes represent operations and edges represent dependencies between the operations. This representation allows for efficient evaluation and optimization of the expression.In the context of the described program, the focus is on optimizing expressions using gradient descent, a popular optimization technique. Gradient descent involves calculating the gradient of the function with respect to each of its parameters and adjusting the parameters based on the gradient. The gradient measures the rate of change of the function with respect to the parameter, indicating how the function value will change when the parameter is modified. By adding or subtracting the gradients, the program aims to maximize or minimize the function, respectively.The program employs an algorithm called generalized backpropagation to efficiently calculate gradients. While backpropagation is commonly used in neural networks, generalized backpropagation extends the principles of backpropagation to any expression, making it a powerful tool for optimization. The program also demonstrates the benefits of utilizing GPUs for faster computation.The program provides various operations for scalars and matrices, such as negation, squaring, absolute value, sign, sigmoid, tanh, multiplication, addition, and subtraction. It is designed to be extensible, allowing users familiar with Rust and CUDA to add additional operations.To use the program, one can define functions and constants, construct the desired expression, and then apply the minimize function to optimize it. The program provides a HashMap for specifying constants at runtime. The API is still being developed, but the provided example showcases the usage and demonstrates the minimization of a function.The text mentions that the program's performance can be compared between CPU and GPU implementations, with the second graph illustrating the performance comparison for optimizing an LSTM model on a randomly generated dataset.Overall, the program aims to provide a better understanding of optimization techniques used in libraries like Tensorflow and Torch by reimplementing them in Rust.1 / 1A list of all of the topics we've discussed.1 / 1ChatGPTChatGPTCertainly! Here is a list of the topics we've discussed:Faster sorting algorithms discovered using deep reinforcement learningLearning State Representations from Random Deep Action-conditional PredictionsZeyu Zheng's research interests and publicationsOptimization-Based Mechanical Perception for Peduncle Localization During Robotic Fruit HarvestAn autonomous robot for pruning modern, planar fruit treesMeasuring a Robot Hand's Graspable Region using Power and Precision GraspsGrasping Benchmarks: Normalizing for Object Size & Approximating Hand WorkspacesSemantics-guided Skeletonization of Sweet Cherry Trees for Robotic PruningThe Value Equivalence Principle for Model-Based Reinforcement LearningOptimizing functions with computational graphsPlease let me know if there's anything specific you would like to revisit or if there's a new topic you'd like to discuss!1 / 1More detailed1 / 1ChatGPTChatGPTCertainly! Here is a more detailed list of the topics we've discussed:Faster sorting algorithms discovered using deep reinforcement learning: We discussed a research paper that introduced AlphaDev, a deep reinforcement learning agent trained to discover new, more efficient sorting algorithms. The agent discovered small sorting algorithms from scratch that outperformed previously known human benchmarks.Learning State Representations from Random Deep Action-conditional Predictions: We discussed another research paper that explored the use of random deep action-conditional predictions as auxiliary tasks for reinforcement learning problems. The paper showed that using these predictions as auxiliary tasks resulted in state representations that achieved competitive control performance in Atari and DeepMind Lab tasks.Zeyu Zheng's research interests and publications: We mentioned Zeyu Zheng, a Ph.D. student at the University of Michigan, whose research interests lie in deep reinforcement learning. We also listed some of Zeyu Zheng's publications, including papers on adaptive pairwise weights for temporal credit assignment and learning state representations.Optimization-Based Mechanical Perception for Peduncle Localization During Robotic Fruit Harvest: We briefly described a research paper that focused on developing an autonomous robotic system for accurately localizing the peduncle during fruit harvest. The paper highlighted the importance of peduncle localization for effective fruit separation techniques.An autonomous robot for pruning modern, planar fruit trees: We mentioned a research paper that discussed the development of an autonomous robot designed for pruning modern, planar fruit trees. The paper highlighted the challenges of pruning and the need for robust perception, pruning point determination, and manipulation systems in variable lighting and occlusion conditions.Measuring a Robot Hand's Graspable Region using Power and Precision Grasps: We mentioned a research paper that proposed a measurement standard for a robot hand's graspable region. The paper introduced the concept of power and precision grasps and demonstrated how to measure the graspable region of a hand using these grasping techniques.Grasping Benchmarks: Normalizing for Object Size & Approximating Hand Workspaces: We discussed a research paper that addressed the challenge of setting a standard for measuring hand size and communicating the size of objects that a robot hand can grasp. The paper proposed a method for normalizing object size and approximating hand workspaces, which would aid in scientific communication in robotic grasping research.Semantics-guided Skeletonization of Sweet Cherry Trees for Robotic Pruning: We discussed a research paper that focused on the development of an algorithm for generating labeled skeletons of sweet cherry trees to facilitate robotic pruning. The paper introduced an algorithm that utilized both topological and geometric priors associated with tree labels to improve the accuracy of the generated skeletons.The Value Equivalence Principle for Model-Based Reinforcement Learning: We discussed a research paper that introduced the value equivalence principle for model-based reinforcement learning. The principle suggests that models should be built to capture aspects of the environment relevant for value-based planning. The paper proposed a formulation of the model learning problem based on the value equivalence principle and discussed its implications.Optimizing functions with computational graphs: We discussed the concept of computational graphs for representing arithmetic expressions and how they facilitate the optimization of functions. We mentioned the use of gradient descent as a technique for optimizing functions by calculating gradients and updating parameters. We also mentioned the generalized backpropagation algorithm as an efficient way to compute gradients for optimization.I hope this provides a more detailed overview of the topics we've covered. Let me know if you have any further questions or if there's anything specific you would like to delve into!1 / 1What would be a could title for this conversation?1 / 1ChatGPTChatGPT"Exploring Cutting-Edge Research in Deep Reinforcement Learning, Robotic Perception, and Optimization Techniques"1 / 1I am just going to call it "Cutting-Edge Research"1 / 1ChatGPTChatGPTThat's a concise and general title that captures the essence of the topics discussed. "Cutting-Edge Research" effectively conveys the exploration of advanced concepts in various fields, providing a broad overview of the conversation.1 / 1Connections between the topics1 / 1ChatGPTChatGPTThroughout the conversation, several connections can be identified between the different topics discussed. Here are some of the connections:Reinforcement Learning and Robotics: Reinforcement learning algorithms are being applied in the field of robotics for tasks such as fruit harvesting, pruning, and object grasping. These applications highlight the intersection of AI and robotics, where learning-based techniques are used to optimize robot behavior and decision-making.Optimization and Computational Graphs: The concept of computational graphs is utilized in optimizing functions. By representing expressions as computational graphs, optimization techniques such as gradient descent can be applied to adjust parameters and maximize or minimize the value of the expressions.Model-Based Reinforcement Learning: The value equivalence principle is discussed as a guiding principle for model-based reinforcement learning. This principle suggests that models should be constructed to capture the aspects of the environment that are relevant for value-based planning. It connects the concepts of model learning, value functions, and planning in the context of RL.Deep Learning and Program Synthesis: Deep learning techniques, such as deep reinforcement learning and deep neural networks, are employed in program synthesis tasks. The use of deep learning algorithms extends beyond traditional problem domains and is explored for generating correct and efficient sorting algorithms, program optimization, and discovering algorithms through reinforcement learning.Agricultural Robotics and Computer Vision: Agricultural applications, such as fruit tree pruning and peduncle localization, involve the integration of robotics and computer vision techniques. Computer vision algorithms are used to analyze images and identify tree structures or fruit characteristics, which guide robotic systems in performing tasks like pruning or harvesting.Artificial Intelligence and Algorithm Development: The conversation touches upon the development and improvement of algorithms in various domains, including sorting algorithms, hashing functions, and robotic control algorithms. The integration of AI techniques, such as deep learning and reinforcement learning, plays a crucial role in advancing algorithmic solutions.These connections illustrate the interdisciplinary nature of the discussed topics, highlighting how advancements in one field, such as reinforcement learning or robotics, can impact and intersect with other areas, including optimization, deep learning, computer vision, and algorithm development.1 / 1It's too abstract.1 / 1ChatGPTChatGPTTitle: "Exploring Cutting-Edge Applications of AI and Robotics"Abstract:
This conversation delves into the fascinating world of cutting-edge research at the intersection of artificial intelligence (AI) and robotics. Through a series of topics, including reinforcement learning, optimization, computational graphs, model-based RL, deep learning, program synthesis, agricultural robotics, and computer vision, we explore the connections and advancements in these fields. From the use of deep reinforcement learning in discovering sorting algorithms to the application of AI in robotic fruit harvesting and pruning, these discussions shed light on the practical applications and emerging trends in AI and robotics. By examining the integration of AI techniques, such as deep learning and reinforcement learning, with algorithm development and robotics, we gain insights into the exciting possibilities and challenges in this rapidly evolving domain. Join us on this journey as we uncover the frontiers of AI and robotics research and their potential impact on various industries.1 / 1HOOD: Hierarchical Graphs for Generalized Modelling of Clothing Dynamics 
Artur Grigorev1,2Bernhard Thomaszewski1Michael J. Black2 
Otmar Hilliges1 
1 ETH Zurich, Department of Computer Science 
2 Max Planck Institute for Intelligent Systems, Tubingen 
https://dolorousrtur.github.io/HOOD/ 
Figure 1. We combine graph neural networks, a hierarchical graph representation and multi-level message passing with an unsupervised 
training scheme to enable real-time prediction of realistic clothing dynamics for arbitrary types of garments and body shapes. Our method 
models both tight-f i tting and free-f l owing clothes draped over arbitrary body shapes. The method generalizes to new, entirely unseen, 
garments (left), and allows for dynamic and unconstrained poses (right) and changes in material parameters and topology at test time. 
Abstract 
We propose a method that leverages graph neural 
networks, multi-level message passing, and unsupervised 
training to enable eff i cient prediction of realistic clothing 
dynamics. Whereas existing methods based on linear blend 
skinning must be trained for specif i c garments, our method, 
called HOOD, is agnostic to body shape and applies to 
tight-f i tting garments as well as loose, free-f l owing cloth- 
ing.Furthermore, HOOD handles changes in topology 
(e.g., garments with buttons or zippers) and material prop- 
erties at inference time. As one key contribution, we pro- 
pose a hierarchical message-passing scheme that eff i ciently 
propagates stiff stretching modes while preserving local de- 
tail. We empirically show that HOOD outperforms strong 
baselines quantitatively and that its results are perceived as 
more realistic than state-of-the-art methods. 
1. Introduction 
The ability to model realistic and compelling clothing 
behavior is crucial for telepresence, virtual try-on, video 
games, and many other applications that rely on high- 
arXiv:2212.07242v3 
[cs.CV] 
16 
Jun 
2023 
fi delity digital humans. A common approach to generating 
plausible dynamic motions is physics-based simulation [2]. 
While impressive results can be obtained, physical simula- 
tion is sensitive to initial conditions, requires animator ex- 
pertise, and is computationally expensive; state-of-the-art 
approaches [14,22,36] are not designed for the strict com- 
putation budgets imposed by real-time applications. 
Deep learning-based methods have started to show 
promising results both in terms of eff i ciency and quality. 
However, there are several limitations that have so far pre- 
vented such approaches from unlocking their full potential: 
First, existing methods rely on linear-blend skinning 
and compute clothing deformations primarily as a func- 
tion of body pose [24, 43]. While compelling results can 
be obtained for tight-f i tting garments such as shirts and 
sportswear, skinning-based methods struggle with dresses, 
skirts, and other types of loose-f i tting clothing that do not 
closely follow body motion. 
Crucially, many state-of-the-art learning-based methods 
are garment-specif i c [20,38,43,45,53] and can only predict 
deformations for the particular outf i t they were trained on. 
The need to retrain these methods for each garment limits 
applicability. 
In this paper, we propose a novel approach for predict- 
ing dynamic garment deformations using graph neural net- 
works (GNNs). Our method learns to predict physically- 
realistic fabric behavior by reasoning about the map be- 
tweenlocaldeformations, forces, andaccelerations. Thanks 
to its locality, our method is agnostic to both the global 
structure and shape of the garment and directly generalizes 
to arbitrary body shapes and motions. 
GNNs have shown promise in replacing physics-based 
simulation [40,41], but a straightforward application of this 
concept to clothing simulation yields unsatisfying results. 
GNNs apply local transformations (implemented as 
MLPs) to feature vectors of vertices and their one-ring 
neighborhood in a given mesh. Each transformation results 
in a set of messages that are then used to update feature vec- 
tors. This process is repeated, allowing signals to propagate 
through the mesh. However, a i xed number of message- 
passing steps limits signal propagation to a i nite radius. 
This is problematic for garment simulation, where elastic 
waves due to stretching travel rapidly through the material, 
leading to quasi-global and immediate long-range coupling 
between vertices. Using too few steps delays signal propa- 
gationandleadstodisturbingover-stretchingartifacts, mak- 
ing garments look unnatural and rubbery. NaÂ¨ Ä±vely increas- 
ing the number of iterations comes at the expense of rapidly 
growing computation times. This problem is amplif i ed by 
the fact that the maximum size and resolution of simulation 
meshes is not known a priori, which would allow setting a 
conservative, suff i ciently large number of iterations. 
To address this problem, we propose a message-passing 
scheme over a hierarchical graph that interleaves propaga- 
tion steps at different levels of resolution. In this way, fast- 
travelling waves due to stiff stretching modes can be eff i- 
ciently treated on coarse scales, while i ner levels provide 
the resolution needed to model local detail such as folds 
and wrinkles. We show through experiments that our graph 
representation improves predictions both qualitatively and 
quantitatively for equal computation budgets. 
To extend the generalization capabilities of our ap- 
proach, we combine the concepts of graph-based neural 
networks and differentiable simulation by using an incre- 
mental potential for implicit time stepping as a loss func- 
tion [33,43]. 
This formulation allows our network to be trained in a 
fully unsupervised way and to simultaneously learn multi- 
scale clothing dynamics, the inf l uence of material parame- 
ters, as well as collision reaction and frictional contact with 
the underlying body, without the need for any ground-truth 
(GT) annotations. Additionally, the graph formulation en- 
ables us to model garments of varied and changing topol- 
ogy; e.g. the unbuttoning of a shirt in motion. 
In summary, we propose a method, called HOOD, that 
leverages graph neural networks, multi-level message pass- 
ing, and unsupervised training to enable real-time predic- 
tion of realistic clothing dynamics for arbitrary types of gar- 
ments and body shapes.1 / 1ChatGPTChatGPTTitle: "HOOD: Hierarchical Graphs for Generalized Modelling of Clothing Dynamics"Abstract:
In this research paper, the authors present a novel method called HOOD (Hierarchical Graphs for Generalized Modelling of Clothing Dynamics) that combines graph neural networks, multi-level message passing, and unsupervised training to enable efficient prediction of realistic clothing dynamics. The method is designed to handle various types of garments, including tight-fitting and loose-flowing clothing, and can adapt to changes in topology and material properties at inference time.The authors propose a hierarchical message-passing scheme that efficiently propagates stiff stretching modes while preserving local detail. They demonstrate through empirical evaluation that HOOD outperforms strong baselines quantitatively and produces more realistic results compared to state-of-the-art methods.The paper addresses limitations of existing methods based on linear blend skinning and garment-specific approaches. HOOD is agnostic to body shape, directly generalizes to arbitrary body shapes and motions, and can handle loose-fitting garments that do not closely follow body motion. It eliminates the need for retraining for each garment, making it more applicable in practical settings.The proposed approach leverages graph neural networks and differentiable simulation to learn multi-scale clothing dynamics, material parameters, collision reactions, and frictional contact without requiring ground-truth annotations. The unsupervised training enables the model to learn in a fully data-driven manner.HOOD has significant potential for applications such as telepresence, virtual try-on, and video games, where realistic clothing behavior is crucial for creating high-fidelity digital humans. The combination of graph neural networks, hierarchical message passing, and unsupervised training in HOOD opens up new possibilities for real-time prediction of clothing dynamics for a wide range of garments and body shapes.1 / 1arXiv:2112.00322 (cs)
[Submitted on 1 Dec 2021 (v1), last revised 24 Mar 2022 (this version, v2)]
FCAF3D: Fully Convolutional Anchor-Free 3D Object Detection
Danila Rukhovich, Anna Vorontsova, Anton Konushin
Download PDF
Recently, promising applications in robotics and augmented reality have attracted considerable attention to 3D object detection from point clouds. In this paper, we present FCAF3D - a first-in-class fully convolutional anchor-free indoor 3D object detection method. It is a simple yet effective method that uses a voxel representation of a point cloud and processes voxels with sparse convolutions. FCAF3D can handle large-scale scenes with minimal runtime through a single fully convolutional feed-forward pass. Existing 3D object detection methods make prior assumptions on the geometry of objects, and we argue that it limits their generalization ability. To get rid of any prior assumptions, we propose a novel parametrization of oriented bounding boxes that allows obtaining better results in a purely data-driven way. The proposed method achieves state-of-the-art 3D object detection results in terms of mAP@0.5 on ScanNet V2 (+4.5), SUN RGB-D (+3.5), and S3DIS (+20.5) datasets. The code and models are available at this https URL1 / 1ChatGPTChatGPTTitle: "FCAF3D: Fully Convolutional Anchor-Free 3D Object Detection"Abstract:
This research paper presents FCAF3D, a fully convolutional anchor-free indoor 3D object detection method. With the increasing interest in applications such as robotics and augmented reality, accurate and efficient 3D object detection from point clouds has become crucial. FCAF3D utilizes a voxel representation of a point cloud and processes voxels using sparse convolutions. The method is designed to handle large-scale scenes with minimal runtime by employing a single fully convolutional feed-forward pass.Unlike existing 3D object detection methods that rely on prior assumptions about the geometry of objects, FCAF3D aims to eliminate such assumptions to enhance generalization ability. It introduces a novel parametrization of oriented bounding boxes that allows for better results based on a purely data-driven approach.Experimental evaluations demonstrate that FCAF3D achieves state-of-the-art performance in 3D object detection, as measured by mAP@0.5, on datasets such as ScanNet V2, SUN RGB-D, and S3DIS. The proposed method outperforms existing approaches with significant improvements in accuracy. The code and pre-trained models are made available for further research and implementation.The advancements presented in FCAF3D contribute to the field of 3D object detection, offering a simple yet effective approach that can be applied in various domains, including robotics and augmented reality applications.1 / 1arXiv:2106.01178 (cs)
[Submitted on 2 Jun 2021 (v1), last revised 15 Oct 2021 (this version, v3)]
ImVoxelNet: Image to Voxels Projection for Monocular and Multi-View General-Purpose 3D Object Detection
Danila Rukhovich, Anna Vorontsova, Anton Konushin
Download PDF
In this paper, we introduce the task of multi-view RGB-based 3D object detection as an end-to-end optimization problem. To address this problem, we propose ImVoxelNet, a novel fully convolutional method of 3D object detection based on monocular or multi-view RGB images. The number of monocular images in each multi-view input can variate during training and inference; actually, this number might be unique for each multi-view input. ImVoxelNet successfully handles both indoor and outdoor scenes, which makes it general-purpose. Specifically, it achieves state-of-the-art results in car detection on KITTI (monocular) and nuScenes (multi-view) benchmarks among all methods that accept RGB images. Moreover, it surpasses existing RGB-based 3D object detection methods on the SUN RGB-D dataset. On ScanNet, ImVoxelNet sets a new benchmark for multi-view 3D object detection. The source code and the trained models are available at this https URL.1 / 1ChatGPTChatGPTTitle: "ImVoxelNet: Image to Voxels Projection for Monocular and Multi-View General-Purpose 3D Object Detection"Abstract:
This research paper introduces ImVoxelNet, a novel fully convolutional method for 3D object detection based on monocular or multi-view RGB images. The objective is to address the task of multi-view RGB-based 3D object detection by formulating it as an end-to-end optimization problem. ImVoxelNet is designed to handle both indoor and outdoor scenes, making it a versatile and general-purpose solution.Key to ImVoxelNet's approach is the projection of RGB images onto voxel representations, enabling the detection of 3D objects in a fully convolutional manner. The method accommodates varying numbers of monocular images in each multi-view input during training and inference. This flexibility allows for effective handling of different scenarios, as the number of monocular images can be unique for each multi-view input.Experimental evaluations demonstrate the effectiveness of ImVoxelNet on various benchmark datasets. In car detection tasks, it achieves state-of-the-art performance on the KITTI dataset for monocular images and the nuScenes dataset for multi-view inputs. Additionally, ImVoxelNet outperforms existing RGB-based 3D object detection methods on the SUN RGB-D dataset. On the ScanNet dataset, it establishes a new benchmark for multi-view 3D object detection.The availability of the source code and trained models facilitates further research and implementation of ImVoxelNet. The contributions of this paper provide significant advancements in the field of 3D object detection, particularly in the context of monocular and multi-view RGB-based approaches.1 / 1
%pylab inline  
import GPy
import GPyOpt
import numpy as np
import  Functions
from Functions import *
Populating the interactive namespace from numpy and matplotlib
def f (x): 
    return - 0.7 * np.sin(0.5 * x) **2 * x**2

def f_without_noise(X):
    return f (X)
    
    #second function - without homo noise  
def f_with_hm_noise(X):
    return f(X) + np.random.normal(loc=0.0, scale=128, size=(X.shape[0], 1)).reshape(-1, )        

    #third function
def f_with_ht_noise(X):
         #target function  + heteroscadastic noise
    return f (X)  + \
        8 * np.random.multivariate_normal(np.zeros(X.reshape(-1, ).shape[0]), np.diag(abs (X.reshape (-1, ))**2), 1)[0]        

N = 2000
X = np.linspace(40,  62, N)
V1 = np.array (f_with_hm_noise (X) - f_without_noise (X))
V2 = np.array (f_with_ht_noise (X) - f_without_noise (X))
V1 = V1 **2
V2 = V2 **2

print 'variance_hm = ',  V1.mean()
print 'variance_ht =  ', V2.mean()





y = f_without_noise(X)
pyplot.figure(figsize = (10, 7))
pyplot.title("f_without_noise")
pyplot.plot(X, y, '.')

y = f_with_hm_noise(X)
pyplot.figure(figsize = (10, 7))
pyplot.title("f_with_hm_noise")
pyplot.plot(X, y, '.')
    

y = f_with_ht_noise(X)
pyplot.figure(figsize = (10, 7))
pyplot.title("f_with_ht_noise")
pyplot.plot(X, y, '.')
def f_with_noise(X):
    
    y  = np.array([])
    
    for i in range(X.shape[0]):
        if ( -15. < X[i] < -13.3):#13
            y = np.append(y,   np.sin (X[i]) * (1 + 0.04 * np.abs (X[i]))+ np.random.normal(loc=0.0, scale=1.))
        elif(-21.5 < X[i] < -19.5):
            y = np.append(y,   np.sin (X[i]) * (1 + 0.04 * np.abs (X[i]))+ np.random.normal(loc=0.0, scale=0.2))
            
        else:
            y = np.append(y,   np.sin (X[i]) * (1 + 0.04* np.abs (X[i]))) 
            
    return y 
variance_hm =  16440.15521902485
variance_ht =   173688.84728060683



bounds = [{'name': 'var_1', 'type': 'continuous', 'domain': (-22, -10)}]  # problem constraints 


#these constants are responsible for the number of iterations of the optimization algorithm
max_iter_for_func = 30
max_iter_for_noise_func = 30


max_time = 200000000000   # time budget 
eps      = 10e-6  # Minimum allows distance between the las two observations


iter = 30

# four arrays for storing optimal values
x_opt_without_noise = np.array([])
x_opt_with_noise = np.array([])

y_opt_without_noise = np.array([])
y_opt_with_noise = np.array([])

iter = 100

for i in range(iter):
    print i
    
    bayes_opt_without_noise = GPyOpt.methods.BayesianOptimization(f = f_with_noise,# function to optimize       
                                                domain = bounds, # box-constraints of the problem
                                                acquisition_type = 'EI',
                                                 exact_feval = True) # Selects the Expected improvement

    bayes_opt_without_noise.run_optimization(max_iter_for_func, max_time, eps)     
    
    x_opt_without_noise = np.append (x_opt_without_noise, bayes_opt_without_noise.x_opt)
    y_opt_without_noise = np.append (y_opt_without_noise, bayes_opt_without_noise.fx_opt)
    
    
    
    bayes_opt_with_noise = GPyOpt.methods.BayesianOptimization(f=f_with_noise,# function to optimize       
                                                domain=bounds, # box-constraints of the problem
                                                acquisition_type='EI',
                                                 exact_feval = True, model_type = 'GPHet')# Selects the Expected improvement

    bayes_opt_with_noise.run_optimization(max_iter_for_noise_func, max_time, eps) 
    
    x_opt_with_noise = np.append (x_opt_with_noise, bayes_opt_with_noise.x_opt)
    y_opt_with_noise = np.append (y_opt_with_noise, bayes_opt_with_noise.fx_opt)
    
    


    print 'x_opt_without_noise:\n', x_opt_without_noise, '\n'


    print 'x_opt_with_noise:\n', x_opt_with_noise, '\n'



print "average x without noise = ", x_opt_without_noise.mean()
print "average x with noise = ", x_opt_with_noise.mean()
print "diff = ", np.abs (x_opt_without_noise.mean() - x_opt_with_noise.mean()), "\n"


print "average y without noise = ", y_opt_without_noise.mean()
print "average y with noise = ", y_opt_with_noise.mean()
print "diff = ", np.abs (y_opt_without_noise.mean() - y_opt_with_noise.mean()), "\n"
    
print "median x without noise = ", np.median (x_opt_without_noise)
print "median x with noise = ", np.median (x_opt_with_noise)
print "diff = ", np.abs (np.median (x_opt_without_noise) - np.median (x_opt_with_noise)), "\n"


print "median y without noise = ", np.median (y_opt_without_noise)
print "median y with noise = ", np.median (y_opt_with_noise)
print "diff = ", np.abs (np.median (y_opt_without_noise) - np.median (y_opt_with_noise)), "\n"
    

https://github.com/col14m/Heteroscedastic_noise/tree/master1 / 1ChatGPTChatGPTThe provided code appears to be a Python script that performs Bayesian optimization using the GPyOpt library. Here's a breakdown of the code:Importing the necessary libraries: pylab, GPy, GPyOpt, numpy, and custom functions from the Functions module.Definition of three different functions: f, f_without_noise, and f_with_noise. These functions compute the values of certain mathematical expressions with and without noise.Generation of data points (X) and corresponding noisy values (V1 and V2) using the defined functions.Visualization of the generated data using matplotlib.Definition of optimization bounds for the Bayesian optimization problem.Configuration of optimization parameters such as maximum iterations, maximum time, and convergence threshold.Iterating over a specified number of iterations and performing Bayesian optimization for two different scenarios: with and without noise.Storing the optimal values obtained during each iteration in separate arrays.Printing the average and median values of the obtained optimal solutions for both scenarios.Providing a GitHub link for more details on the code.Overall, this code demonstrates how to use Bayesian optimization to find optimal solutions for a given objective function, with and without noise. The optimization is performed using the GPyOpt library, and the results are analyzed and compared. The GitHub link is provided for additional information.1 / 1What is heteroscedastic noise?1 / 1ChatGPTChatGPTHeteroscedastic noise refers to a type of noise or variability in data where the variance or standard deviation of the noise is not constant across the data points. In other words, the amount of noise or uncertainty in the data varies depending on the value of the independent variable.In contrast, homoscedastic noise, or constant variance, assumes that the noise or variability in the data is the same for all data points. This means that the spread or dispersion of the data points is consistent across the entire range of the independent variable.Heteroscedastic noise is often encountered in real-world data, where the level of noise may depend on certain factors or conditions. For example, in financial data, the volatility or variance of stock prices may vary depending on market conditions. Similarly, in sensor measurements, the level of measurement uncertainty may vary depending on the operating conditions or calibration of the sensor.In statistical modeling and analysis, it is important to account for heteroscedasticity, as failing to do so can lead to biased estimates or incorrect inferences. Various techniques, such as weighted regression or heteroscedasticity-consistent standard errors, can be used to address heteroscedasticity and obtain more accurate results.1 / 1Google


heteroscedastic noise

Videos
Images
Vs homoscedastic
News
Shopping
Books
Maps
Flights
Finance
All filters

When the variance of the random process is constant, we speak of homoscedastic noise. Its opposite is called heteroscedastic noise, i.e. the variance changes with the height of the signal (often it is simply proportional to it).
http://www.statistics4u.com âº cc_no...
Types of Noise
Feedback
About featured snippets

Wikipedia
https://en.wikipedia.org âº wiki âº Ho...
Homoscedasticity and heteroscedasticity
In statistics, a sequence (or a vector) of random variables is homoscedastic if all its random variables have the same finite variance; this is also known ...
Definition
Examples
Consequences
Correction
Testing
Generalisations
People also ask
What is heteroscedastic mean?
What is an example of heteroscedastic?
What is homoscedastic and heteroscedastic noise?
What is the homoscedasticity noise?
Feedback

Electronics Stack Exchange
https://electronics.stackexchange.com âº ...
What makes noise heteroscedastic?
Feb 1, 2020 Â· 4 answers
I work in the field of spectroscopy, for instance NMR and FT-IR. For both of these instruments, the advice is to process the data to compensate ...
3 votes
Carriers crossing junctions exhibit SHOT NOISE, proportional to â2qIB, where IB is the ...  More
1 vote
Added Thermal Noise sounds like the eddy currents of wind and water in nature but in electronics ...  More
1 vote
For both of these instruments, the advice is to process the data to compensate for heteroscedasticit ...  More
0 votes
OK, here is another shot at answering. In NMR, we have two distinct situations. If we have ...  More
View all posts

Stanford Computer Science
https://cs.stanford.edu âº LeS...PDF
Heteroscedastic Gaussian Process Regression

by QV Le Â· Cited by 216 â For in- stance, in Gaussian Process regression one treats the noise level as a hyperparameter and solves the arising nonconvex problem by iterative optimization ...
8 pagesÂ·748 KB

Proceedings of Machine Learning Research
https://proceedings.mlr.press âº ...
Inferring Cause and Effect in the Presence of Heteroscedastic Noise
by S Xu Â· 2022 Â· Cited by 5 â We study the problem of identifying cause and effect over two univariate continuous variables X and Y from a sample of their joint distribution.
https://proceedings.mlr.press âº ...
PDF
Inferring Cause and Effect in the Presence of Heteroscedastic Noise
by S Xu Â· 2022 Â· Cited by 5 â We study the problem of identifying cause and effect over two univariate continuous variables. X and Y from a sample of their joint distri- bution.

CISPA Helmholtz Center for Information Security
https://eda.rg.cispa.io âº pubsPDF
Causal Inference with Heteroscedastic Noise Models

by S Xu Â· 2022 Â· Cited by 2 â In thorough experiments we show, that our ability to model heteroscedastic noise translates into a superior performance on a wide range of synthetic and real- ...
6 pagesÂ·277 KB

arXiv
https://arxiv.org âº stat
Information Directed Sampling and Bandits with Heteroscedastic Noise
by J Kirschner Â· 2018 Â· Cited by 83 â In this work, we consider bandits with heteroscedastic noise, where we explicitly allow the noise distribution to depend on the evaluation ...

OpenReview
https://openreview.net âº pdfPDF
ON LEARNING HETEROSCEDASTIC NOISE ...
by A Kloss Â· 2018 Â· Cited by 1 â In this work, we propose a method to learn heteroscedastic noise models from data by optimizing the prediction likelihood end-to-end through differentiable ...
People also search for

University of Alberta
https://sites.ualberta.ca âº papersPDF
Active Learning in Heteroscedastic Noise $

by A Antosa Â· Cited by 70 â The performance of the algorithm is illustrated on a simple problem. Key words: active learning, heteroscedastic noise, regression, sequential.
27 pagesÂ·352 KB1 / 1ChatGPTChatGPTHeteroscedastic noise is a term used in statistics and signal processing to describe noise or variability in data where the variance or standard deviation is not constant across the data points. It is the opposite of homoscedastic noise, where the variance remains constant.Heteroscedastic noise means that the amount of noise or uncertainty in the data varies depending on the value of the independent variable. This can occur in various domains, such as finance, spectroscopy, or regression analysis.In the context of statistical modeling and analysis, it is important to account for heteroscedastic noise as it can affect the accuracy of the results. Techniques such as weighted regression or heteroscedasticity-consistent standard errors are often used to address heteroscedasticity and obtain more reliable estimates.Researchers and practitioners in various fields, such as computer science, machine learning, and information security, have studied and proposed methods to handle heteroscedastic noise in different applications. These include methods for regression analysis, causal inference, active learning, and bandit problems.Overall, heteroscedastic noise is an important concept to consider when analyzing data, as it can provide valuable insights into the variability and uncertainty associated with different data points.1 / 1Under review as a conference paper at ICLR 2019 
ONLEARNINGHETEROSCEDASTICNOISEMODELS 
WITHINDIFFERENTIABLEBAYESFILTERS 
Anonymous authors 
Paper under double-blind review ABSTRACT 
In many robotic applications, it is crucial to maintain a belief about the state of 
a system, like the location of a robot or the pose of an object. These state es- 
timates serve as input for planning and decision making and provide feedback 
during task execution. Recursive Bayesian Filtering algorithms address the state 
estimation problem, but they require a model of the process dynamics and the 
sensory observations as well as noise estimates that quantify the accuracy of these 
models. Recently, multiple works have demonstrated that the process and sensor 
models can be learned by end-to-end training through differentiable versions of 
Recursive Filtering methods. However, even if the predictive models are known, 
i nding suitable noise models remains challenging. Therefore, many practical ap- 
plications rely on very simplistic noise models. Our hypothesis is that end-to-end 
training through differentiable Bayesian Filters enables us to learn more complex 
heteroscedastic noise models for the system dynamics. We evaluate learning such 
models with different types of i ltering algorithms and on two different robotic 
tasks. Our experiments show that especially for sampling-based i lters like the 
Particle Filter, learning heteroscedastic noise models can drastically improve the 
tracking performance in comparison to using constant noise models. 
1INTRODUCTION 
For many real-world systems that we would like to control, we cannot directly observe the current 
state directly. However, in order to stabilize a system at a goal state or make it track a trajectory, 
we need to have access to state feedback. An observer provides an estimate of the current system 
state from sensor measurements. Recursive Bayesian Filtering is a probabilistic approach towards 
estimating a belief about the current state. The method relies on a process model that predicts how 
the system behaves over time and an observation model that generates the expected observations 
given the predicted state. While the approach itself is general and makes few assumptions, the 
challenge is to formulate the process and observation models and to estimate the noise in these 
models. Process and observation noise quantify how certain the i lter is about either the prediction 
or the observations. This information is used to determine how much the predicted state is updated 
based on the observation. 
Deep neural networks are well suited for tasks that require i nding patterns or extracting information 
from raw, high-dimensional input signals and compressing them into a more compact representa- 
tion. They have therefore become the method of choice especially in perception problems. For many 
robotics tasks like modeling dynamics, planning or tracking however, it has been shown that com- 
bining prior knowledge in the form of analytical models and/or algorithmic structure with trainable 
network components leads to better performance and generalizability than trying to learn the com- 
plete tasks from scratch (Kloss et al., 2017; Karkus et al., 2017; Jonschkowski et al., 2018; Tamar 
et al., 2016; Okada et al., 2017; Jonschkowski & Brock, 2016; Haarnoja et al., 2016; Karkus et al., 
2018). 
Specif i cally, (Jonschkowski & Brock, 2016; Haarnoja et al., 2016; Jonschkowski et al., 2018; Karkus 
et al., 2018) have presented differentiable Bayesian Filtering algorithms. The authors focus on 
learning the observation and dynamics models end-to-end through the i lters and demonstrate that 
the recursive i ltering structure improves prediction results over using recurrent neural networks that 
were trained for the same task. 1 
Under review as a conference paper at ICLR 2019 
In many robotic applications, it is possible to formulate the process and observation model based on 
i rst-order principles. However, i nding appropriate values for the process and observation noise is 
often diff i cult and despite of much research on identif i cation methods (e.g. (Bavdekar et al., 2011; 
Valappil & Georgakis, 2000)) they are often tuned manually. To reduce the tedious tuning effort, the 
noise models are typically assumed to be a Gaussian with zero mean and constant covariance. Many 
real systems can however be better modeled with heteroscedastic noise models, where the level of 
uncertainty depends on the state of the system and/or possible control inputs. Taking heterostochas- 
ticity into account has been demonstrated to improve i ltering performance in many robotic tasks 
(Bauza & Rodriguez, 2017; Kersting et al., 2007). 
In this work, we propose a method to learn heteroscedastic noise models from data by optimizing the 
prediction likelihood end-to-end through differentiable Bayesian Filters. In addition to differentiable 
Extended Kalman Filters and Particle Filters, which have been proposed in related work, we also 
propose two different versions of the Unscented Kalman Filter. 
In our experiments we focus on learning the noise models and therefore assume that observation and 
process models are known or at least pretrained. We evaluate the performance of the different i lters 
and noise models on two different real-world robotic problems: (i) Visual Odometry for an driving 
car (Haarnoja et al., 2016; Jonschkowski et al., 2018; Geiger et al., 2012) which has simple smooth 
dynamics and a low-dimensional state, and (ii) Visual tracking of an object that is pushed by a robot 
(Yu et al., 2016; Kloss et al., 2017). Planar pushing has challenging, discontinuous dynamics and 
was shown to have a heteroscedastic noise distribution (Bauza & Rodriguez, 2017). Furthermore, 
the dimensionality of the state is double of the Visual Odometry task. 
Our experiments show that using heteroscedastic process noise models drastically improves the 
tracking performance of the Particle Filter and Unscented Filter variants and facilitated learning 
as compared to learning a constant process noise model. While learning the noise models can be 
benef i cial for all i lters, the tracking performance of the EKF turned out to be least sensitive to the1 / 1ChatGPTChatGPTThe paper titled "ON LEARNING HETEROSCEDASTIC NOISE MODELS WITH DIFFERENTIABLE BAYES FILTERS" is currently under review as a conference paper at ICLR 2019. The authors of the paper are anonymous.The paper addresses the problem of maintaining belief about the state of a system, such as the location of a robot or the pose of an object, in robotic applications. Recursive Bayesian Filtering algorithms are commonly used for state estimation, but they require accurate models of process dynamics and sensor observations, as well as noise estimates to quantify the accuracy of these models. However, finding suitable noise models, particularly heteroscedastic noise models where the noise varies with the state of the system, remains challenging.The authors propose a method to learn heteroscedastic noise models from data by optimizing the prediction likelihood through differentiable Bayesian Filters. They focus on learning the noise models while assuming that the observation and process models are known or pretrained. The paper evaluates the performance of different filtering algorithms, including Extended Kalman Filters, Particle Filters, and Unscented Kalman Filters, with different noise models on two real-world robotic problems: visual odometry and visual tracking of an object pushed by a robot.The experiments show that using heteroscedastic noise models, particularly for the Particle Filter and Unscented Filter variants, significantly improves tracking performance compared to learning a constant noise model. The paper concludes that learning heteroscedastic noise models can be beneficial in various filtering algorithms and can improve the performance of robotic tasks.Please note that the details provided are based on the information given in the abstract of the paper, and the full content of the paper may contain additional details and findings.1 / 1A list of all of the topics discussed1 / 1ChatGPTChatGPTBased on the information provided, here is a list of topics discussed in the given code and paper:Heteroscedastic noise modelsRecursive Bayesian Filtering algorithmsState estimation in robotic applicationsEnd-to-end training through differentiable Bayesian FiltersLearning observation and dynamics models in Bayesian FiltersFormulating process and observation modelsEstimating noise in process and observation modelsCombining neural networks with analytical models and algorithmic structureDifferentiable Extended Kalman FiltersDifferentiable Particle FiltersUnscented Kalman Filter variantsVisual OdometryVisual tracking of objectsPerformance evaluation of filtering algorithmsTracking performance improvement with heteroscedastic noise modelsLearning heteroscedastic noise models from dataComparison of tracking performance with constant noise modelsReal-world robotic problems and applicationsPlease note that this list is based on the provided code and abstract of the paper, and there may be additional topics discussed in the full content of the paper.1 / 1In our conversation1 / 1ChatGPTChatGPTIn our conversation, we discussed the following topics:Computational graphs and their representation of arithmetic expressions.Optimization of functions using gradient descent and backpropagation.The concept of generalized backpropagation for calculating gradients in any expression.The use of computational graphs for evaluating and optimizing expressions.The implementation of a library for function optimization using computational graphs.The usage and examples of the library for optimizing functions.Performance comparison of CPU and GPU for function optimization.The connection between different topics discussed in the conversation.Please note that this list includes the main topics discussed in our conversation and may not cover every single detail.1 / 1A summary2 / 2ChatGPTChatGPTIn our conversation, we explored the concept of computational graphs, which are a representation of arithmetic expressions using a graph structure. We discussed how computational graphs are useful for evaluating and optimizing functions. We learned about the process of gradient descent and how it can be used to optimize functions by calculating gradients and updating parameters. We also touched upon the technique of generalized backpropagation, which extends the principles of backpropagation to any expression.Furthermore, we delved into the implementation of a library for function optimization using computational graphs. We saw examples of how to use the library to define functions, set parameters, and minimize functions using gradient descent. We also discussed the performance comparison between CPU and GPU implementations for function optimization.Throughout our conversation, we explored various mathematical operations and their implementations in the library, such as negation, squaring, absolute value, sign, sigmoid, tanh, multiplication, addition, and subtraction. We also examined the impact of noise on function optimization and the importance of considering heteroscedastic noise models.In summary, our conversation covered the fundamentals of computational graphs, function optimization using gradient descent, the implementation of a library for optimization, and the consideration of noise models in function optimization.1 / 1Heteroscedastic noise models are noise models where the variance of the noise varies across different parts of the data. This type of noise model can be useful in many applications, including state estimation in robotic applications. Recursive Bayesian filtering algorithms, such as the Extended Kalman Filter and the Particle Filter, are commonly used for state estimation in robotic systems.

End-to-end training through differentiable Bayesian Filters is a technique that combines neural networks with analytical models and algorithmic structure. By using differentiable Bayesian Filters, it is possible to learn observation and dynamics models directly from data, while also estimating the noise in the process and observation models. This approach can be particularly useful in applications such as visual odometry, where the dynamics and observation models may be complex and difficult to model analytically.

In formulating process and observation models, it is important to consider the specific problem at hand and choose models that accurately capture the underlying dynamics and observations. Estimating the noise in these models is also crucial, as it can significantly impact the performance of the filtering algorithm.

Differentiable Extended Kalman Filters, Differentiable Particle Filters, and Unscented Kalman Filter variants are all examples of filtering algorithms that can be trained using end-to-end techniques. These algorithms are designed to handle non-linear dynamics and non-Gaussian noise, which are common in many robotic applications.

Visual odometry and visual tracking of objects are examples of applications where tracking performance is critical. Performance evaluation of filtering algorithms is therefore an important aspect of these applications, and comparison of tracking performance with constant noise models and with heteroscedastic noise models can provide insights into the effectiveness of different noise models for different types of data.

Real-world robotic problems and applications vary widely, and may include tasks such as object recognition, path planning, and control. Filtering algorithms are often used in these applications for state estimation and tracking, and can play a crucial role in enabling robots to operate autonomously in complex and dynamic environments.1 / 1ChatGPTChatGPTHeteroscedastic noise models, which capture varying variances of noise across different parts of the data, are valuable in numerous applications. They find particular utility in state estimation within robotic systems. Recursive Bayesian filtering algorithms like the Extended Kalman Filter and Particle Filter are commonly employed for state estimation in robotics.End-to-end training via differentiable Bayesian Filters combines neural networks with analytical models and algorithmic structures. This approach enables the learning of observation and dynamics models directly from data, while also estimating the noise in process and observation models. It proves especially advantageous in complex scenarios like visual odometry, where analytical modeling of dynamics and observations can be challenging.When formulating process and observation models, it is essential to consider the specific problem and select models that accurately capture the underlying dynamics and observations. Accurately estimating the noise in these models is crucial, as it significantly affects the filtering algorithm's performance.Differentiable Extended Kalman Filters, Differentiable Particle Filters, and variants of the Unscented Kalman Filter are examples of filtering algorithms that can be trained using end-to-end techniques. These algorithms are designed to handle non-linear dynamics and non-Gaussian noise, commonly encountered in robotic applications.Applications such as visual odometry and visual object tracking require robust tracking performance. Evaluating the performance of filtering algorithms becomes crucial in such cases. Comparing tracking performance using constant noise models versus heteroscedastic noise models provides valuable insights into the effectiveness of different noise models for different types of data.Real-world robotic problems encompass a wide range of tasks, including object recognition, path planning, and control. Filtering algorithms play a vital role in these applications by enabling state estimation and tracking. They are instrumental in empowering robots to operate autonomously in complex and dynamic environments.1 / 1Faster sorting algorithms discovered using deep reinforcement learning is an interesting recent development in the field of machine learning. The idea is to use a deep reinforcement learning agent to learn how to sort a list of numbers by selecting the appropriate sorting operations at each step. The agent is trained to maximize a reward signal that reflects the efficiency of the sorting algorithm in terms of the number of comparisons and swaps required to sort the list.

Learning State Representations from Random Deep Action-conditional Predictions (SR3DP) is another interesting approach that uses deep learning techniques to learn state representations. The idea is to train a neural network to predict the next state of a system given a current state and an action. The network is then used to generate random action-conditional predictions, which are used to learn a state representation that captures the underlying structure of the system.

Both of these approaches demonstrate the power of deep learning techniques for discovering new algorithms and representations. They also highlight the potential for combining reinforcement learning and deep learning to solve complex problems in a wide range of domains.1 / 1ChatGPTChatGPTThe development of faster sorting algorithms using deep reinforcement learning is an intriguing advancement in machine learning. By employing a deep reinforcement learning agent, the goal is to train the agent to learn how to sort a list of numbers efficiently by selecting the most suitable sorting operations at each step. The training process involves maximizing a reward signal that reflects the algorithm's efficiency based on the number of comparisons and swaps required for sorting.Learning State Representations from Random Deep Action-conditional Predictions (SR3DP) is another noteworthy approach that leverages deep learning techniques to learn state representations. The concept revolves around training a neural network to predict the next state of a system using the current state and an action. By generating random action-conditional predictions through the network, it becomes possible to learn a state representation that captures the underlying structure of the system.Both of these approaches showcase the capabilities of deep learning techniques in discovering new algorithms and representations. They also highlight the potential for combining reinforcement learning and deep learning to address complex problems across various domains. These advancements open up exciting possibilities for developing more efficient algorithms and understanding the underlying structure of complex systems.1 / 1The research interests and publications of Zeyu Zheng include:

1. Optimization-Based Mechanical Perception for Peduncle Localization During Robotic Fruit Harvest
2. An autonomous robot for pruning modern, planar fruit trees
3. Measuring a Robot Hand's Graspable Region using Power and Precision Grasps
4. Grasping Benchmarks: Normalizing for Object Size & Approximating Hand Workspaces
5. Semantics-guided Skeletonization of Sweet Cherry Trees for Robotic Pruning
6. The Value Equivalence Principle for Model-Based Reinforcement Learning
7. Optimizing functions with computational graphs

These publications suggest that Zeyu Zheng's research interests are mainly in the areas of robotics, perception, and optimization. The papers focus on developing techniques for robotic fruit harvesting, pruning, and grasping, as well as developing algorithms for reinforcement learning and optimization-based perception.1 / 1ChatGPTChatGPTBased on the listed publications, Zeyu Zheng's research interests primarily lie in the fields of robotics, perception, and optimization. The publications cover a range of topics related to these areas, including robotic fruit harvesting, pruning of fruit trees, grasp analysis, and optimization techniques. The research work demonstrates a focus on developing algorithms and methodologies to enhance robotic perception, optimize robotic tasks, and explore model-based reinforcement learning. Overall, Zeyu Zheng's work contributes to advancing the capabilities of robots in various domains through innovative approaches in perception, manipulation, and optimization.1 / 15.1KITTIVISUALODOMETRY 
As a i rst application we chose the Kitti Visual Odometry task (Geiger et al., 2012) that was also 
evaluated in (Haarnoja et al., 2016) and (Jonschkowski et al., 2018). The aim is to estimate the 
position and orientation of a driving car given a sequence of rgb images from a front facing camera 
and the true initial state. 
The state is 5-dimensional and includes the position p and orientation Î¸ of the car as well as the cur- 
rent linear and angular velocity v and Ë 
Î¸. As the control inputs are unknown, the estimated velocities 
are predicted by sampling random accelerations a,Â¨Î¸, according to the process noise for v and Ë Î¸. The 
position and heading estimate are update by Euler integration (see Appendix 7.2.1). 
While the dynamics model is simple, the challenge comes from the fact that the drivers actions are 
not known and the absolute position and orientation are not observable. The i lters can therefore 
only rely on estimating the angular and linear velocity from pairs of input images to update the state, 
but the uncertainty about the position and heading will inevitably grow due to missing feedback. 
We pretrain a neural network to extract this information from the current input image and the differ- 
ence image between the current and previous one. The network architecture is the same as was used 
in (Haarnoja et al., 2016; Jonschkowski et al., 2018), we only replace the response normalization 
layers with tensorf l owâs standard batch normalization layers. Since both related work allowed for 
i netuning of the perception network trough the i lter, we do the same here for better comparability 
of results. As in (Jonschkowski et al., 2018), we test the Particle Filter using 1000 particles and also 
use 1000 sigma points for the MCUKF. 
The process and observation noise are initialized to the same values in every condition. For the 
observation noise, we look at the average error of the perception network at the end of the pretraining 
phase. To set the process noise, we use the ground truth standard deviation of the velocities to 
initialize the terms for linear and angular velocity. The terms for position and heading are initialized 
to identity. See the Appendix 7.2.1 for exact values. 
5.1.1DATA 
The Kitti Visual Odometry dataset consists 11 trajectories of varying length (from 270 to over 4500 
steps) with ground truth annotations for position and heading and image sequences from two dif- 
ferent cameras collected at 10Hz. We use the two shortest sequences for validation and perform 
a 9-fold cross-validation on the remaining sequences. We use both image sequences from each 
trajectory and further augment the data by adding the mirrored sequences as well. For training, 
we extract non-overlapping sequences of length 50 with a different random starting point for each 
image-sequence. The sequences for validation and testing consist of 100 timesteps. 
7 
Under review as a conference paper at ICLR 2019 
No learningRQhR + QR + Qh 
Translational error [m/m] 
EKF0.22 Â± 0.130.19 Â± 0.130.19 Â± 0.110.20 Â± 0.130.19 Â± 0.12 
UKF0.39 Â± 0.140.29 Â± 0.150.41 Â± 0.170.26 Â± 0.140.24 Â± 0.16 
MCUKF0.99 Â± 0.041.01 Â± 0.020.36 Â± 0.140.76 Â± 0.090.44 Â± 0.25 
PF0.81 Â± 0.950.73 Â± 0.400.40 Â± 0.250.26 Â± 0.170.3 Â± 0.22 
Rotational error [deg/m] 
EKF0.11 Â± 0.100.17 Â± 0.190.10 Â± 0.100.11 Â± 0.080.10 Â± 0.13 
UKF0.26 Â± 0.080.29 Â± 0.350.21 Â± 0.110.14 Â± 0.060.16 Â± 0.12 
MCUKF1.17 Â± 0.921.24 Â± 0.560.30 Â± 0.280.48 Â± 0.420.27 Â± 0.20 
PF0.95 Â± 0.890.93 Â± 0.850.29 Â± 0.260.86 Â± 1.310.20 Â± 0.20 
Table 1: Kitti Visual Odometry task. Evaluation of four non-linear i lters under i ve different noise 
learning conditions: No learning, learning constant observation noise R, learning heteroscedastic 
process noise Qh, learning constant observation and process noise R + Q, learning constant obser- 
vation noise and heteroscedastic process noise R + Qh. In each condition, the perception network 
was pretrained off l ine and i netuned through the i lters. We evaluate the models on different trajec- 
tories with 100 timesteps. As in (Jonschkowski et al., 2018; Haarnoja et al., 2016) we report mean 
and std of the end-point- error in position and orientation normalized by the distance between start 
and end point. 
1 / 1ChatGPTChatGPTThe passage describes the application of different filtering algorithms to the Kitti Visual Odometry task, which involves estimating the position and orientation of a driving car based on a sequence of RGB images from a front-facing camera. The state of the car includes its position, orientation, linear velocity, and angular velocity.The goal is to use filtering algorithms, such as the Extended Kalman Filter (EKF), Unscented Kalman Filter (UKF), Modified Cubature Kalman Filter (MCUKF), and Particle Filter (PF), to estimate the car's state given the image sequences. The challenge lies in the fact that the driver's actions are unknown, and the absolute position and orientation are not observable. The filters can only rely on estimating velocity from pairs of input images, which leads to increasing uncertainty about the position and heading over time.A neural network is pretrained to extract information from the current and difference images, and the network architecture is similar to previous works. The filters are tested using a certain number of particles or sigma points, and the process and observation noise are initialized based on pretraining and ground truth values.The Kitti Visual Odometry dataset, consisting of 11 trajectories with ground truth annotations and image sequences, is used for training, validation, and testing. The data is augmented by adding mirrored sequences, and non-overlapping sequences of length 50 are extracted for training. The evaluation is performed on different trajectories with 100 timesteps, and the performance is measured in terms of translational and rotational errors.The results in Table 1 show the evaluation of the filters under different noise learning conditions, including no learning, learning constant observation noise, learning heteroscedastic process noise, learning constant observation and process noise, and learning constant observation noise with heteroscedastic process noise.Overall, this section provides an overview of the Kitti Visual Odometry task, the use of filtering algorithms, and the evaluation of their performance in estimating the car's position and orientation based on image sequences.1 / 1While all i lters perform worse on the unrealistic noise setting, the Particle Filter is affected the 
most. This is presumably because without well-tuned noise models, it samples many particles far 
away from the true state and cannot discriminate well between likely and unlikely particles given 
the observations. 
Learning the noise modelsThe remaining columns of Table 2 show the results when learning 
the different combinations of noise models. The process and observation noise are initialized to the 
realistic values from the no learning setting for every condition. We can see that the performance 
of the Extended Kalman Filter again remains mostly constant over all conditions and also does not 
improve much over the model with well-tuned noise. 
Both the UKF and the MCUKF do not show much difference for tracking the position of the object. 
We see a slightly improved performance for tracking the orientation of the object when a constant 
process noise model is trained and a stronger improvement with the heteroscedastic Qh. This is 
consistent with the results in the previous experiment, as the orientation of the object can again not 
be observed directly and it is thus not desirable to vary it much when creating the sigma points. 
Overall, the traditional UKF with trained R and heteroscedastic Q performs best, but the MCUKF 
is similar and could potentially perform better if more sigma points were sampled. 
In this experiment, the Particle Filter prof i ts most from learning: In the two conditions with het- 
eroscedastic process noise, its tracking performance improves dramatically and even outperforms 
the other i lters on the position metric. The improvement over the untrained setting is much smaller 
when Q is constrained to be constant. Why is learning a heteroscedastic process noise model so 
important for the PF? We believe that learning a separate Q for each particle helps the i lter to steer 
the particle set towards more likely regions of the state space. It can for example get rid of particles 
that encode a state conf i guration that is not physically plausible and will therefore lead to a bad 
prediction from the analytical model by sampling higher noise and thus decreasing the likelihood of 
the particle. 
Training the observation noise R did not have a very big effect in this experiment, but inspecting the 
learned diagonal values showed that all i lters learned to predict higher uncertainty for the y coordi- 
nate of positions, which makes sense as the y axis of the world frame points towards the background 
of the image and perspective transform thus reduces the accuracy in this direction. In contrast to 
the results in (Haarnoja et al., 2016), we did not see any evidence that the heterostochasticity of the 
observation noise was helpful. This can probably be explained by the absence of complete occlu- 
sions of the object in our dataset. We could also not identify any other common feature of scenes 
for which our prediction model produced high prediction errors. It is therefore likely that a constant 
observation noise model would have been suff i cient in this setting. 
10 
Under review as a conference paper at ICLR 2019 
6CONCLUSIONS 
We proposed to optimize the process and observation noise for Bayesian Filters through end-to-end 
training and evaluated the method with different i ltering algorithms and on two robotic applications. 
Our experiments showed that learning the process noise is especially important for i lters that sample 
around the mean estimate of the state, like the Particle Filter but also the Unscented Kalman Filters. 
The Extended Kalman Filter in contrast proved to be most robust to suboptimal choices of the noise 
models. While this makes it a good choice for problems with simple and smooth dynamics, our 
experiments on the pushing task demonstrated that the (optimized) Unscented Filters can perform 
better on problems with more complex and even discontinuous dynamics. 
Training a state-dependent process noise model instead of a constant one improves the prediction 
accuracy for dynamic systems that are expected to have heteroscedastic noise. In our experiments, 
it also facilitated learning in general and lead to faster convergence of the models. 
We also used a heteroscedastic observation noise model in all our experiments. But different from 
the results in (Haarnoja et al., 2016), we could not see a large benef i t from it: Inspection on the 
pushing task showed that larger errors in the prediction of the preprocessing networks were not 
associated with higher observation noise. Identifying inputs that will lead to bad predictions is a 
diff i cult task if no obvious problems like occlusions are present to explain such outliers. Developing 
better methods for communicating uncertainty about the predictions of a neural network would thus 
be an impotent next step to further improve the performance of differentiable Bayesian Filters.1 / 1ChatGPTChatGPTThe passage discusses the results and conclusions of the experiments conducted on different filtering algorithms for the Kitti Visual Odometry task. It states that without well-tuned noise models, all filters perform worse, with the Particle Filter being the most affected due to its sampling of particles far away from the true state.When learning the different combinations of noise models, the Extended Kalman Filter's performance remains mostly constant and does not improve significantly. The UKF and MCUKF show slight improvements in tracking orientation when trained with a constant process noise model and stronger improvements with heteroscedastic noise. The traditional UKF with trained observation noise and heteroscedastic process noise performs the best, with the MCUKF showing similar performance potential with more sigma points.The Particle Filter benefits the most from learning, with its tracking performance dramatically improving in conditions with heteroscedastic process noise, even outperforming other filters in terms of position tracking. Learning a separate Q for each particle helps steer the particle set towards more likely regions of the state space, eliminating particles that encode physically implausible states.Training the observation noise R did not have a significant effect, but it was observed that all filters learned to predict higher uncertainty for the y-coordinate of positions, which is expected due to the perspective transform reducing accuracy in that direction. The absence of complete occlusions in the dataset suggests that a constant observation noise model would have been sufficient.In conclusion, the experiments showed that learning the process noise is crucial for filters that sample around the mean estimate of the state, such as the Particle Filter. The Unscented Kalman Filters performed better in problems with complex and discontinuous dynamics. Training a state-dependent process noise model improved prediction accuracy and facilitated learning. However, the benefits of a heteroscedastic observation noise model were not prominent in the experiments, highlighting the need for better methods to communicate uncertainty in neural network predictions.1 / 1Connections with the previous topics.1 / 1ChatGPTChatGPTThe previous topics discussed heteroscedastic noise models, deep reinforcement learning for sorting algorithms, and learning state representations. While there may not be direct connections between these topics and the specific research on Kitti Visual Odometry and differentiable Bayesian Filters, there are some broader connections and overlaps in terms of the underlying principles and techniques used.Heteroscedastic Noise Models: The Kitti Visual Odometry task involves estimating the position and orientation of a car given image sequences and the true initial state. The experiments conducted in the research paper explore the impact of different noise models, including heteroscedastic process noise. This connects to the concept of heteroscedastic noise discussed earlier, where the variance of the noise varies across different parts of the data. By learning a state-dependent process noise model, the research aims to improve prediction accuracy in the presence of heteroscedastic noise.Deep Reinforcement Learning: While the research paper focuses on differentiable Bayesian Filters and their application to robotic perception tasks, the broader concept of deep reinforcement learning can be connected to the topic of learning state representations. Deep reinforcement learning involves training agents to make sequential decisions by interacting with an environment and receiving feedback in the form of rewards. In the context of the research paper, the neural network used for perception tasks can be seen as part of the deep reinforcement learning framework, as it is trained to extract relevant information from the input images to make accurate predictions about the state of the system.Optimization and Learning: The research paper explores the optimization of process and observation noise for Bayesian Filters through end-to-end training. This connects to the topic of optimization-based learning discussed earlier, where algorithms are trained to optimize specific objectives or performance metrics. In the research paper, the optimization process involves training the neural network to extract information from images and learn noise models that maximize the efficiency and accuracy of the filtering algorithms.Overall, while the specific research on Kitti Visual Odometry and differentiable Bayesian Filters may not directly relate to the previous topics, there are broader connections in terms of the underlying principles and techniques used in the research, such as dealing with heteroscedastic noise, applying deep learning and reinforcement learning methods, and using optimization-based approaches to improve performance.1 / 1A final summary1 / 1ChatGPTChatGPTIn summary, the research paper explores the use of differentiable Bayesian Filters and deep learning techniques in the context of robotic perception tasks. Specifically, it focuses on the Kitti Visual Odometry task, where the goal is to estimate the position and orientation of a driving car given image sequences and the true initial state.The paper investigates the impact of different noise models, including heteroscedastic process noise, on the performance of various filtering algorithms such as the Extended Kalman Filter, Unscented Kalman Filter, and Particle Filter. It demonstrates that learning a state-dependent process noise model can significantly improve the prediction accuracy, particularly for filtering algorithms that sample particles around the mean estimate of the state.The research also highlights the importance of optimizing the process and observation noise through end-to-end training. By training a neural network to extract relevant information from the input images, the perception accuracy is improved, leading to more accurate state estimation.Overall, the research showcases the potential of combining deep learning, reinforcement learning, and optimization techniques in the field of robotic perception. It provides insights into the benefits of learning noise models and demonstrates the effectiveness of differentiable Bayesian Filters in improving tracking performance in complex and dynamic environments.1 / 1ChatGPT may produce inaccurate information about people, places, or facts. ChatGPT May 24 VersionChatGPT
------MultipartBoundary--d1L5cNE49EQ4Yph33YtSG31B6q1OA5hLrmgkI1GWnw----
Content-Type: text/css
Content-Transfer-Encoding: binary
Content-Location: cid:css-1e7c881b-9f2c-4fd5-a60d-57b54f3f77ea@mhtml.blink

@charset "utf-8";

.react-scroll-to-bottom--css-shywg-79elbk { position: relative; }

.react-scroll-to-bottom--css-shywg-1n7m0yu { height: 100%; overflow-y: auto; width: 100%; }

.react-scroll-to-bottom--css-shywg-1tj0vk3 { background-color: rgba(0, 0, 0, 0.2); border-radius: 10px; border-width: 0px; bottom: 5px; cursor: pointer; height: 20px; outline: 0px; position: absolute; right: 20px; width: 20px; }

.react-scroll-to-bottom--css-shywg-1tj0vk3:hover { background-color: rgba(0, 0, 0, 0.4); }

.react-scroll-to-bottom--css-shywg-1tj0vk3:active { background-color: rgba(0, 0, 0, 0.6); }
------MultipartBoundary--d1L5cNE49EQ4Yph33YtSG31B6q1OA5hLrmgkI1GWnw----
Content-Type: text/css
Content-Transfer-Encoding: binary
Content-Location: cid:css-de61771e-4b22-4fe6-9965-b509504174f0@mhtml.blink

@charset "utf-8";

.r34K7X1zGgAi6DllVF3T { box-sizing: border-box; border: 0px; margin: 0px; padding: 0px; overflow: hidden; display: none; z-index: 2147483647; pointer-events: none; visibility: hidden; opacity: 0; transition: opacity 300ms linear 0s; height: 0px; width: 0px; }

.r34K7X1zGgAi6DllVF3T.active { display: block; visibility: visible; }

.r34K7X1zGgAi6DllVF3T.active.show { opacity: 1; pointer-events: inherit; position: inherit; }

.r34K7X1zGgAi6DllVF3T.active.show.in-situ { width: inherit; height: inherit; }

.r34K7X1zGgAi6DllVF3T.active.show.lightbox { position: fixed; inset: 0px; width: 100% !important; height: 100% !important; }
------MultipartBoundary--d1L5cNE49EQ4Yph33YtSG31B6q1OA5hLrmgkI1GWnw----
Content-Type: text/css
Content-Transfer-Encoding: binary
Content-Location: cid:css-6ad05fa8-e158-4bb0-b3d3-77bf12282b52@mhtml.blink

@charset "utf-8";

@keyframes intercom-lightweight-app-launcher { 
  0% { opacity: 0; transform: scale(0.5); }
  100% { opacity: 1; transform: scale(1); }
}

@keyframes intercom-lightweight-app-gradient { 
  0% { opacity: 0; }
  100% { opacity: 1; }
}

@keyframes intercom-lightweight-app-messenger { 
  0% { opacity: 0; transform: scale(0); }
  40% { opacity: 1; }
  100% { transform: scale(1); }
}

.intercom-lightweight-app { position: fixed; z-index: 2147483001; width: 0px; height: 0px; font-family: intercom-font, "Helvetica Neue", "Apple Color Emoji", Helvetica, Arial, sans-serif; }

.intercom-lightweight-app-gradient { position: fixed; z-index: 2147483002; width: 500px; height: 500px; bottom: 0px; right: 0px; pointer-events: none; background: radial-gradient(at right bottom, rgba(29, 39, 54, 0.16) 0%, rgba(29, 39, 54, 0) 72%); animation: 200ms ease-out 0s 1 normal none running intercom-lightweight-app-gradient; }

.intercom-lightweight-app-launcher { position: fixed; z-index: 2147483003; border: none; bottom: 20px; right: 20px; max-width: 48px; width: 48px; max-height: 48px; height: 48px; border-radius: 50%; background: rgb(32, 33, 35); cursor: pointer; box-shadow: rgba(0, 0, 0, 0.06) 0px 1px 6px 0px, rgba(0, 0, 0, 0.16) 0px 2px 32px 0px; transition: transform 167ms cubic-bezier(0.33, 0, 0, 1) 0s; box-sizing: content-box; padding: 0px !important; margin: 0px !important; }

.intercom-lightweight-app-launcher:hover { transition: transform 250ms cubic-bezier(0.33, 0, 0, 1) 0s; transform: scale(1.1); }

.intercom-lightweight-app-launcher:active { transform: scale(0.85); transition: transform 134ms cubic-bezier(0.45, 0, 0.2, 1) 0s; }

.intercom-lightweight-app-launcher:focus { outline: none; }

.intercom-lightweight-app-launcher-icon { display: flex; align-items: center; justify-content: center; position: absolute; top: 0px; left: 0px; width: 48px; height: 48px; transition: transform 100ms linear 0s, opacity 80ms linear 0s; }

.intercom-lightweight-app-launcher-icon-open { opacity: 1; transform: rotate(0deg) scale(1); }

.intercom-lightweight-app-launcher-icon-open svg { width: 24px; height: 24px; }

.intercom-lightweight-app-launcher-icon-open svg path { fill: rgb(255, 255, 255); }

.intercom-lightweight-app-launcher-icon-self-serve { opacity: 1; transform: rotate(0deg) scale(1); }

.intercom-lightweight-app-launcher-icon-self-serve svg { height: 44px; }

.intercom-lightweight-app-launcher-icon-self-serve svg path { fill: rgb(255, 255, 255); }

.intercom-lightweight-app-launcher-custom-icon-open { max-height: 24px; max-width: 24px; opacity: 1; transform: rotate(0deg) scale(1); }

.intercom-lightweight-app-launcher-icon-minimize { opacity: 0; transform: rotate(-60deg) scale(0); }

.intercom-lightweight-app-launcher-icon-minimize svg path { fill: rgb(255, 255, 255); }

.intercom-lightweight-app-messenger { position: fixed; z-index: 2147483003; overflow: hidden; background-color: white; animation: 250ms cubic-bezier(0, 1, 1, 1) 0s 1 normal none running intercom-lightweight-app-messenger; transform-origin: right bottom; inset: 0px; border-radius: 16px; }

.intercom-lightweight-app-messenger-header { height: 64px; border-bottom: none; background: rgb(32, 33, 35); }

.intercom-lightweight-app-messenger-footer { position: absolute; bottom: 0px; width: 100%; height: 80px; background: rgb(255, 255, 255); font-size: 14px; line-height: 21px; border-top: 1px solid rgba(0, 0, 0, 0.05); box-shadow: rgba(0, 0, 0, 0.05) 0px 0px 25px; }

@media print {
  .intercom-lightweight-app { display: none; }
}
------MultipartBoundary--d1L5cNE49EQ4Yph33YtSG31B6q1OA5hLrmgkI1GWnw----
Content-Type: text/css
Content-Transfer-Encoding: binary
Content-Location: https://chat.openai.com/_next/static/css/76b9661cdb7aed92.css

@charset "utf-8";

@font-face { font-display: swap; font-family: Signifier; font-style: normal; font-weight: 400; src: url("https://cdn.openai.com/common/fonts/signifier/signifier-light.woff2") format("woff2"); }

@font-face { font-display: swap; font-family: Signifier; font-style: italic; font-weight: 400; src: url("https://cdn.openai.com/common/fonts/signifier/signifier-light-italic.woff2") format("woff2"); }

@font-face { font-display: swap; font-family: Signifier; font-style: normal; font-weight: 700; src: url("https://cdn.openai.com/common/fonts/signifier/signifier-bold.woff2") format("woff2"); }

@font-face { font-display: swap; font-family: Signifier; font-style: italic; font-weight: 700; src: url("https://cdn.openai.com/common/fonts/signifier/signifier-bold-italic.woff2") format("woff2"); }

@font-face { font-display: swap; font-family: SÃ¶hne; font-style: normal; font-weight: 400; src: url("https://cdn.openai.com/common/fonts/soehne/soehne-buch.woff2") format("woff2"); }

@font-face { font-display: swap; font-family: SÃ¶hne; font-style: italic; font-weight: 400; src: url("https://cdn.openai.com/common/fonts/soehne/soehne-buch-kursiv.woff2") format("woff2"); }

@font-face { font-display: swap; font-family: SÃ¶hne; font-style: normal; font-weight: 500; src: url("https://cdn.openai.com/common/fonts/soehne/soehne-kraftig.woff2") format("woff2"); }

@font-face { font-display: swap; font-family: SÃ¶hne; font-style: italic; font-weight: 500; src: url("https://cdn.openai.com/common/fonts/soehne/soehne-kraftig-kursiv.woff2") format("woff2"); }

@font-face { font-display: swap; font-family: SÃ¶hne; font-style: normal; font-weight: 600; src: url("https://cdn.openai.com/common/fonts/soehne/soehne-halbfett.woff2") format("woff2"); }

@font-face { font-display: swap; font-family: SÃ¶hne; font-style: italic; font-weight: 600; src: url("https://cdn.openai.com/common/fonts/soehne/soehne-halbfett-kursiv.woff2") format("woff2"); }

@font-face { font-display: swap; font-family: "SÃ¶hne Mono"; font-style: normal; font-weight: 400; src: url("https://cdn.openai.com/common/fonts/soehne/soehne-mono-buch.woff2") format("woff2"); }

@font-face { font-display: swap; font-family: "SÃ¶hne Mono"; font-style: normal; font-weight: 700; src: url("https://cdn.openai.com/common/fonts/soehne/soehne-mono-halbfett.woff2") format("woff2"); }

@font-face { font-display: swap; font-family: "SÃ¶hne Mono"; font-style: italic; font-weight: 400; src: url("https://cdn.openai.com/common/fonts/soehne/soehne-mono-buch-kursiv.woff2") format("woff2"); }

*, ::after, ::before { border: 0px solid rgb(217, 217, 227); box-sizing: border-box; }

::after, ::before { --tw-content:""; }

html { text-size-adjust: 100%; font-feature-settings: normal; font-family: SÃ¶hne, ui-sans-serif, system-ui, -apple-system, "Segoe UI", Roboto, Ubuntu, Cantarell, "Noto Sans", sans-serif, "Helvetica Neue", Arial, "Apple Color Emoji", "Segoe UI Emoji", "Segoe UI Symbol", "Noto Color Emoji"; font-variation-settings: normal; line-height: 1.5; tab-size: 4; }

body { line-height: inherit; margin: 0px; }

hr { border-top-width: 1px; color: inherit; height: 0px; }

abbr:where([title]) { text-decoration: underline dotted currentcolor; }

h1, h2, h3, h4, h5, h6 { font-size: inherit; font-weight: inherit; }

a { color: inherit; text-decoration: inherit; }

b, strong { font-weight: bolder; }

code, kbd, pre, samp { font-family: "SÃ¶hne Mono", Monaco, "Andale Mono", "Ubuntu Mono", monospace; font-size: 1em; }

small { font-size: 80%; }

sub, sup { font-size: 75%; line-height: 0; position: relative; vertical-align: baseline; }

sub { bottom: -0.25em; }

sup { top: -0.5em; }

table { border-collapse: collapse; border-color: inherit; text-indent: 0px; }

button, input, optgroup, select, textarea { color: inherit; font-family: inherit; font-size: 100%; font-weight: inherit; line-height: inherit; margin: 0px; padding: 0px; }

button, select { text-transform: none; }

[type="button"], [type="reset"], [type="submit"], button { appearance: button; background-color: transparent; background-image: none; }

progress { vertical-align: baseline; }

::-webkit-inner-spin-button, ::-webkit-outer-spin-button { height: auto; }

[type="search"] { appearance: textfield; outline-offset: -2px; }

::-webkit-search-decoration { appearance: none; }

::-webkit-file-upload-button { appearance: button; font: inherit; }

summary { display: list-item; }

blockquote, dd, dl, fieldset, figure, h1, h2, h3, h4, h5, h6, hr, p, pre { margin: 0px; }

fieldset, legend { padding: 0px; }

menu, ol, ul { list-style: none; margin: 0px; padding: 0px; }

textarea { resize: vertical; }

input::-webkit-input-placeholder, textarea::-webkit-input-placeholder { color: rgb(172, 172, 190); }

input::placeholder, textarea::placeholder { color: rgb(172, 172, 190); }

[role="button"], button { cursor: pointer; }

:disabled { cursor: default; }

audio, canvas, embed, iframe, img, object, svg, video { display: block; vertical-align: middle; }

img, video { height: auto; max-width: 100%; }

[hidden] { display: none; }

[multiple], [type="date"], [type="datetime-local"], [type="email"], [type="month"], [type="number"], [type="password"], [type="search"], [type="tel"], [type="text"], [type="time"], [type="url"], [type="week"], select, textarea { --tw-shadow:0 0 transparent; appearance: none; background-color: rgb(255, 255, 255); border-color: rgb(142, 142, 160); border-radius: 0px; border-width: 1px; font-size: 1rem; line-height: 1.5rem; padding: 0.5rem 0.75rem; }

[multiple]:focus, [type="date"]:focus, [type="datetime-local"]:focus, [type="email"]:focus, [type="month"]:focus, [type="number"]:focus, [type="password"]:focus, [type="search"]:focus, [type="tel"]:focus, [type="text"]:focus, [type="time"]:focus, [type="url"]:focus, [type="week"]:focus, select:focus, textarea:focus { --tw-ring-inset:var(--tw-empty, ); --tw-ring-offset-width:0px; --tw-ring-offset-color:#fff; --tw-ring-color:#2563eb; --tw-ring-offset-shadow:var(--tw-ring-inset) 0 0 0 var(--tw-ring-offset-width) var(--tw-ring-offset-color); --tw-ring-shadow:var(--tw-ring-inset) 0 0 0 calc(1px + var(--tw-ring-offset-width)) var(--tw-ring-color); border-color: rgb(37, 99, 235); box-shadow: var(--tw-ring-offset-shadow),var(--tw-ring-shadow),var(--tw-shadow); outline: transparent solid 2px; outline-offset: 2px; }

input::-webkit-input-placeholder, textarea::-webkit-input-placeholder { color: rgb(142, 142, 160); opacity: 1; }

input::placeholder, textarea::placeholder { color: rgb(142, 142, 160); opacity: 1; }

::-webkit-datetime-edit-fields-wrapper { padding: 0px; }

::-webkit-date-and-time-value { min-height: 1.5em; }

::-webkit-datetime-edit, ::-webkit-datetime-edit-day-field, ::-webkit-datetime-edit-hour-field, ::-webkit-datetime-edit-meridiem-field, ::-webkit-datetime-edit-millisecond-field, ::-webkit-datetime-edit-minute-field, ::-webkit-datetime-edit-month-field, ::-webkit-datetime-edit-second-field, ::-webkit-datetime-edit-year-field { padding-bottom: 0px; padding-top: 0px; }

select { background-image: url("data:image/svg+xml;charset=utf-8,%3Csvg xmlns='http://www.w3.org/2000/svg' fill='none' viewBox='0 0 20 20'%3E%3Cpath stroke='%238e8ea0' stroke-linecap='round' stroke-linejoin='round' stroke-width='1.5' d='m6 8 4 4 4-4'/%3E%3C/svg%3E"); background-position: right 0.5rem center; background-repeat: no-repeat; background-size: 1.5em 1.5em; padding-right: 2.5rem; -webkit-print-color-adjust: exact; }

[multiple] { background-image: none; background-position: 0px 0px; background-repeat: repeat; background-size: initial; padding-right: 0.75rem; -webkit-print-color-adjust: inherit; }

[type="checkbox"], [type="radio"] { --tw-shadow:0 0 transparent; appearance: none; background-color: rgb(255, 255, 255); background-origin: border-box; border-color: rgb(142, 142, 160); border-width: 1px; color: rgb(37, 99, 235); display: inline-block; flex-shrink: 0; height: 1rem; padding: 0px; -webkit-print-color-adjust: exact; user-select: none; vertical-align: middle; width: 1rem; }

[type="checkbox"] { border-radius: 0px; }

[type="radio"] { border-radius: 100%; }

[type="checkbox"]:focus, [type="radio"]:focus { --tw-ring-inset:var(--tw-empty, ); --tw-ring-offset-width:2px; --tw-ring-offset-color:#fff; --tw-ring-color:#2563eb; --tw-ring-offset-shadow:var(--tw-ring-inset) 0 0 0 var(--tw-ring-offset-width) var(--tw-ring-offset-color); --tw-ring-shadow:var(--tw-ring-inset) 0 0 0 calc(2px + var(--tw-ring-offset-width)) var(--tw-ring-color); box-shadow: var(--tw-ring-offset-shadow),var(--tw-ring-shadow),var(--tw-shadow); outline: transparent solid 2px; outline-offset: 2px; }

[type="checkbox"]:checked, [type="radio"]:checked { background-color: currentcolor; background-position: 50% center; background-repeat: no-repeat; background-size: 100% 100%; border-color: transparent; }

[type="checkbox"]:checked { background-image: url("data:image/svg+xml;charset=utf-8,%3Csvg viewBox='0 0 16 16' fill='%23fff' xmlns='http://www.w3.org/2000/svg'%3E%3Cpath d='M12.207 4.793a1 1 0 0 1 0 1.414l-5 5a1 1 0 0 1-1.414 0l-2-2a1 1 0 0 1 1.414-1.414L6.5 9.086l4.293-4.293a1 1 0 0 1 1.414 0z'/%3E%3C/svg%3E"); }

[type="radio"]:checked { background-image: url("data:image/svg+xml;charset=utf-8,%3Csvg viewBox='0 0 16 16' fill='%23fff' xmlns='http://www.w3.org/2000/svg'%3E%3Ccircle cx='8' cy='8' r='3'/%3E%3C/svg%3E"); }

[type="checkbox"]:checked:focus, [type="checkbox"]:checked:hover, [type="radio"]:checked:focus, [type="radio"]:checked:hover { background-color: currentcolor; border-color: transparent; }

[type="checkbox"]:indeterminate { background-color: currentcolor; background-image: url("data:image/svg+xml;charset=utf-8,%3Csvg xmlns='http://www.w3.org/2000/svg' fill='none' viewBox='0 0 16 16'%3E%3Cpath stroke='%23fff' stroke-linecap='round' stroke-linejoin='round' stroke-width='2' d='M4 8h8'/%3E%3C/svg%3E"); background-position: 50% center; background-repeat: no-repeat; background-size: 100% 100%; border-color: transparent; }

[type="checkbox"]:indeterminate:focus, [type="checkbox"]:indeterminate:hover { background-color: currentcolor; border-color: transparent; }

[type="file"] { background: initial; border-color: inherit; border-radius: 0px; border-width: 0px; font-size: inherit; line-height: inherit; padding: 0px; }

[type="file"]:focus { outline: -webkit-focus-ring-color auto 1px; }

h1 { font-size: 1.875rem; line-height: 2.25rem; }

h1, h2, h3 { font-weight: 600; }

h2, h3 { font-size: 1.5rem; line-height: 2rem; }

*, ::after, ::before { --tw-border-spacing-x:0; --tw-border-spacing-y:0; --tw-translate-x:0; --tw-translate-y:0; --tw-rotate:0; --tw-skew-x:0; --tw-skew-y:0; --tw-scale-x:1; --tw-scale-y:1; --tw-pan-x: ; --tw-pan-y: ; --tw-pinch-zoom: ; --tw-scroll-snap-strictness:proximity; --tw-gradient-from-position: ; --tw-gradient-via-position: ; --tw-gradient-to-position: ; --tw-ordinal: ; --tw-slashed-zero: ; --tw-numeric-figure: ; --tw-numeric-spacing: ; --tw-numeric-fraction: ; --tw-ring-inset: ; --tw-ring-offset-width:0px; --tw-ring-offset-color:#fff; --tw-ring-color:rgba(69,89,164,0.5); --tw-ring-offset-shadow:0 0 transparent; --tw-ring-shadow:0 0 transparent; --tw-shadow:0 0 transparent; --tw-shadow-colored:0 0 transparent; --tw-blur: ; --tw-brightness: ; --tw-contrast: ; --tw-grayscale: ; --tw-hue-rotate: ; --tw-invert: ; --tw-saturate: ; --tw-sepia: ; --tw-drop-shadow: ; --tw-backdrop-blur: ; --tw-backdrop-brightness: ; --tw-backdrop-contrast: ; --tw-backdrop-grayscale: ; --tw-backdrop-hue-rotate: ; --tw-backdrop-invert: ; --tw-backdrop-opacity: ; --tw-backdrop-saturate: ; --tw-backdrop-sepia: ; }

::-webkit-backdrop { --tw-border-spacing-x:0; --tw-border-spacing-y:0; --tw-translate-x:0; --tw-translate-y:0; --tw-rotate:0; --tw-skew-x:0; --tw-skew-y:0; --tw-scale-x:1; --tw-scale-y:1; --tw-pan-x: ; --tw-pan-y: ; --tw-pinch-zoom: ; --tw-scroll-snap-strictness:proximity; --tw-gradient-from-position: ; --tw-gradient-via-position: ; --tw-gradient-to-position: ; --tw-ordinal: ; --tw-slashed-zero: ; --tw-numeric-figure: ; --tw-numeric-spacing: ; --tw-numeric-fraction: ; --tw-ring-inset: ; --tw-ring-offset-width:0px; --tw-ring-offset-color:#fff; --tw-ring-color:rgba(69,89,164,0.5); --tw-ring-offset-shadow:0 0 transparent; --tw-ring-shadow:0 0 transparent; --tw-shadow:0 0 transparent; --tw-shadow-colored:0 0 transparent; --tw-blur: ; --tw-brightness: ; --tw-contrast: ; --tw-grayscale: ; --tw-hue-rotate: ; --tw-invert: ; --tw-saturate: ; --tw-sepia: ; --tw-drop-shadow: ; --tw-backdrop-blur: ; --tw-backdrop-brightness: ; --tw-backdrop-contrast: ; --tw-backdrop-grayscale: ; --tw-backdrop-hue-rotate: ; --tw-backdrop-invert: ; --tw-backdrop-opacity: ; --tw-backdrop-saturate: ; --tw-backdrop-sepia: ; }

::backdrop { --tw-border-spacing-x:0; --tw-border-spacing-y:0; --tw-translate-x:0; --tw-translate-y:0; --tw-rotate:0; --tw-skew-x:0; --tw-skew-y:0; --tw-scale-x:1; --tw-scale-y:1; --tw-pan-x: ; --tw-pan-y: ; --tw-pinch-zoom: ; --tw-scroll-snap-strictness:proximity; --tw-gradient-from-position: ; --tw-gradient-via-position: ; --tw-gradient-to-position: ; --tw-ordinal: ; --tw-slashed-zero: ; --tw-numeric-figure: ; --tw-numeric-spacing: ; --tw-numeric-fraction: ; --tw-ring-inset: ; --tw-ring-offset-width:0px; --tw-ring-offset-color:#fff; --tw-ring-color:rgba(69,89,164,0.5); --tw-ring-offset-shadow:0 0 transparent; --tw-ring-shadow:0 0 transparent; --tw-shadow:0 0 transparent; --tw-shadow-colored:0 0 transparent; --tw-blur: ; --tw-brightness: ; --tw-contrast: ; --tw-grayscale: ; --tw-hue-rotate: ; --tw-invert: ; --tw-saturate: ; --tw-sepia: ; --tw-drop-shadow: ; --tw-backdrop-blur: ; --tw-backdrop-brightness: ; --tw-backdrop-contrast: ; --tw-backdrop-grayscale: ; --tw-backdrop-hue-rotate: ; --tw-backdrop-invert: ; --tw-backdrop-opacity: ; --tw-backdrop-saturate: ; --tw-backdrop-sepia: ; }

:root { --color-primary:#19c37d; --color-secondary:#715fde; --color-error:#ef4146; --gradient-primary:linear-gradient(90deg,#a29bd4,#989fdd); --text-primary:#202123; --text-default:#353740; --text-secondary:#6e6e80; --text-disabled:#acacbe; --text-error:var(--color-error); }

textarea:focus { border-color: inherit; box-shadow: none; outline: none; }

::-webkit-scrollbar { height: 1rem; width: 0.5rem; }

::-webkit-scrollbar:horizontal { height: 0.5rem; width: 1rem; }

::-webkit-scrollbar-track { background-color: transparent; border-radius: 9999px; }

::-webkit-scrollbar-thumb { --tw-border-opacity:1; background-color: rgba(217, 217, 227, 0.8); border-color: rgba(255,255,255,var(--tw-border-opacity)); border-radius: 9999px; border-width: 1px; }

::-webkit-scrollbar-thumb:hover { --tw-bg-opacity:1; background-color: rgba(236,236,241,var(--tw-bg-opacity)); }

.dark::-webkit-scrollbar-thumb { --tw-bg-opacity:1; background-color: rgba(86,88,105,var(--tw-bg-opacity)); }

.dark::-webkit-scrollbar-thumb:hover { --tw-bg-opacity:1; background-color: rgba(172,172,190,var(--tw-bg-opacity)); }

@media (min-width: 768px) {
  .scrollbar-trigger ::-webkit-scrollbar-thumb { visibility: hidden; }
  .scrollbar-trigger:hover ::-webkit-scrollbar-thumb { visibility: visible; }
}

.container { width: 100%; }

@media (min-width: 640px) {
  .container { max-width: 640px; }
}

@media (min-width: 768px) {
  .container { max-width: 768px; }
}

@media (min-width: 1024px) {
  .container { max-width: 1024px; }
}

@media (min-width: 1280px) {
  .container { max-width: 1280px; }
}

@media (min-width: 1536px) {
  .container { max-width: 1536px; }
}

.prose { color: var(--tw-prose-body); max-width: 65ch; }

.prose :where([class~="lead"]):not(:where([class~="not-prose"] *)) { color: var(--tw-prose-lead); font-size: 1.25em; line-height: 1.6; margin-bottom: 1.2em; margin-top: 1.2em; }

.prose :where(a):not(:where([class~="not-prose"] *)) { color: var(--tw-prose-links); font-weight: 500; text-decoration: underline; }

.prose :where(strong):not(:where([class~="not-prose"] *)) { color: var(--tw-prose-bold); font-weight: 600; }

.prose :where(a strong):not(:where([class~="not-prose"] *)) { color: inherit; }

.prose :where(blockquote strong):not(:where([class~="not-prose"] *)) { color: inherit; }

.prose :where(thead th strong):not(:where([class~="not-prose"] *)) { color: inherit; }

.prose :where(ol):not(:where([class~="not-prose"] *)) { list-style-type: decimal; margin-bottom: 1.25em; margin-top: 1.25em; padding-left: 1.625em; }

.prose :where(ol[type="A"]):not(:where([class~="not-prose"] *)) { list-style-type: upper-alpha; }

.prose :where(ol[type="a"]):not(:where([class~="not-prose"] *)) { list-style-type: lower-alpha; }

.prose :where():not(:where([class~="not-prose"] *)) { list-style-type: upper-alpha; }

.prose :where():not(:where([class~="not-prose"] *)) { list-style-type: lower-alpha; }

.prose :where(ol[type="I"]):not(:where([class~="not-prose"] *)) { list-style-type: upper-roman; }

.prose :where(ol[type="i"]):not(:where([class~="not-prose"] *)) { list-style-type: lower-roman; }

.prose :where():not(:where([class~="not-prose"] *)) { list-style-type: upper-roman; }

.prose :where():not(:where([class~="not-prose"] *)) { list-style-type: lower-roman; }

.prose :where(ol[type="1"]):not(:where([class~="not-prose"] *)) { list-style-type: decimal; }

.prose :where(ul):not(:where([class~="not-prose"] *)) { list-style-type: disc; margin-bottom: 1.25em; margin-top: 1.25em; padding-left: 1.625em; }

.prose :where(ol > li):not(:where([class~="not-prose"] *))::marker { color: var(--tw-prose-counters); font-weight: 400; }

.prose :where(ul > li):not(:where([class~="not-prose"] *))::marker { color: var(--tw-prose-bullets); }

.prose :where(hr):not(:where([class~="not-prose"] *)) { border-color: var(--tw-prose-hr); border-top-width: 1px; margin-bottom: 3em; margin-top: 3em; }

.prose :where(blockquote):not(:where([class~="not-prose"] *)) { border-left-color: var(--tw-prose-quote-borders); border-left-width: 0.25rem; color: var(--tw-prose-quotes); font-style: normal; font-weight: 500; margin-bottom: 1.6em; margin-top: 1.6em; padding-left: 1em; quotes: "â" "â" "â" "â"; }

.prose :where(blockquote p:first-of-type):not(:where([class~="not-prose"] *))::before { content: open-quote; }

.prose :where(blockquote p:last-of-type):not(:where([class~="not-prose"] *))::after { content: close-quote; }

.prose :where(h1):not(:where([class~="not-prose"] *)) { color: var(--tw-prose-headings); font-size: 2.25em; font-weight: 800; line-height: 1.11111; margin-bottom: 0.888889em; margin-top: 0px; }

.prose :where(h1 strong):not(:where([class~="not-prose"] *)) { color: inherit; font-weight: 900; }

.prose :where(h2):not(:where([class~="not-prose"] *)) { color: var(--tw-prose-headings); font-size: 1.5em; font-weight: 700; line-height: 1.33333; margin-bottom: 1em; margin-top: 2em; }

.prose :where(h2 strong):not(:where([class~="not-prose"] *)) { color: inherit; font-weight: 800; }

.prose :where(h3):not(:where([class~="not-prose"] *)) { color: var(--tw-prose-headings); font-size: 1.25em; font-weight: 600; line-height: 1.6; margin-bottom: 0.6em; margin-top: 1.6em; }

.prose :where(h3 strong):not(:where([class~="not-prose"] *)) { color: inherit; font-weight: 700; }

.prose :where(h4):not(:where([class~="not-prose"] *)) { color: var(--tw-prose-headings); font-weight: 600; line-height: 1.5; margin-bottom: 0.5em; margin-top: 1.5em; }

.prose :where(h4 strong):not(:where([class~="not-prose"] *)) { color: inherit; font-weight: 700; }

.prose :where(img):not(:where([class~="not-prose"] *)) { margin-bottom: 2em; margin-top: 2em; }

.prose :where(figure > *):not(:where([class~="not-prose"] *)) { margin-bottom: 0px; margin-top: 0px; }

.prose :where(figcaption):not(:where([class~="not-prose"] *)) { color: var(--tw-prose-captions); font-size: 0.875em; line-height: 1.42857; margin-top: 0.857143em; }

.prose :where(code):not(:where([class~="not-prose"] *)) { color: var(--tw-prose-code); font-size: 0.875em; font-weight: 600; }

.prose :where(code):not(:where([class~="not-prose"] *))::before { content: "`"; }

.prose :where(code):not(:where([class~="not-prose"] *))::after { content: "`"; }

.prose :where(a code):not(:where([class~="not-prose"] *)) { color: inherit; }

.prose :where(h1 code):not(:where([class~="not-prose"] *)) { color: inherit; }

.prose :where(h2 code):not(:where([class~="not-prose"] *)) { color: inherit; font-size: 0.875em; }

.prose :where(h3 code):not(:where([class~="not-prose"] *)) { color: inherit; font-size: 0.9em; }

.prose :where(h4 code):not(:where([class~="not-prose"] *)) { color: inherit; }

.prose :where(blockquote code):not(:where([class~="not-prose"] *)) { color: inherit; }

.prose :where(thead th code):not(:where([class~="not-prose"] *)) { color: inherit; }

.prose :where(pre):not(:where([class~="not-prose"] *)) { background-color: transparent; border-radius: 0.375rem; color: currentcolor; font-size: 0.875em; font-weight: 400; line-height: 1.71429; margin: 0px; overflow-x: auto; padding: 0px; }

.prose :where(pre code):not(:where([class~="not-prose"] *)) { background-color: transparent; border-radius: 0px; border-width: 0px; color: inherit; font-family: inherit; font-size: inherit; font-weight: inherit; line-height: inherit; padding: 0px; }

.prose :where(pre code):not(:where([class~="not-prose"] *))::before { content: none; }

.prose :where(pre code):not(:where([class~="not-prose"] *))::after { content: none; }

.prose :where(table):not(:where([class~="not-prose"] *)) { font-size: 0.875em; line-height: 1.71429; margin-bottom: 2em; margin-top: 2em; table-layout: auto; text-align: left; width: 100%; }

.prose :where(thead):not(:where([class~="not-prose"] *)) { border-bottom-color: var(--tw-prose-th-borders); border-bottom-width: 1px; }

.prose :where(thead th):not(:where([class~="not-prose"] *)) { color: var(--tw-prose-headings); font-weight: 600; padding-bottom: 0.571429em; padding-left: 0.571429em; padding-right: 0.571429em; vertical-align: bottom; }

.prose :where(tbody tr):not(:where([class~="not-prose"] *)) { border-bottom-color: var(--tw-prose-td-borders); border-bottom-width: 1px; }

.prose :where(tbody tr:last-child):not(:where([class~="not-prose"] *)) { border-bottom-width: 0px; }

.prose :where(tbody td):not(:where([class~="not-prose"] *)) { vertical-align: baseline; }

.prose :where(tfoot):not(:where([class~="not-prose"] *)) { border-top-color: var(--tw-prose-th-borders); border-top-width: 1px; }

.prose :where(tfoot td):not(:where([class~="not-prose"] *)) { vertical-align: top; }

.prose { --tw-prose-body:#374151; --tw-prose-headings:#111827; --tw-prose-lead:#4b5563; --tw-prose-links:#111827; --tw-prose-bold:#111827; --tw-prose-counters:#6b7280; --tw-prose-bullets:#d1d5db; --tw-prose-hr:#e5e7eb; --tw-prose-quotes:#111827; --tw-prose-quote-borders:#e5e7eb; --tw-prose-captions:#6b7280; --tw-prose-code:#111827; --tw-prose-pre-code:#e5e7eb; --tw-prose-pre-bg:#1f2937; --tw-prose-th-borders:#d1d5db; --tw-prose-td-borders:#e5e7eb; --tw-prose-invert-body:#d1d5db; --tw-prose-invert-headings:#fff; --tw-prose-invert-lead:#9ca3af; --tw-prose-invert-links:#fff; --tw-prose-invert-bold:#fff; --tw-prose-invert-counters:#9ca3af; --tw-prose-invert-bullets:#4b5563; --tw-prose-invert-hr:#374151; --tw-prose-invert-quotes:#f3f4f6; --tw-prose-invert-quote-borders:#374151; --tw-prose-invert-captions:#9ca3af; --tw-prose-invert-code:#fff; --tw-prose-invert-pre-code:#d1d5db; --tw-prose-invert-pre-bg:rgba(0,0,0,0.5); --tw-prose-invert-th-borders:#4b5563; --tw-prose-invert-td-borders:#374151; font-size: 1rem; line-height: 1.75; }

.prose :where(p):not(:where([class~="not-prose"] *)) { margin-bottom: 1.25em; margin-top: 1.25em; }

.prose :where(video):not(:where([class~="not-prose"] *)) { margin-bottom: 2em; margin-top: 2em; }

.prose :where(figure):not(:where([class~="not-prose"] *)) { margin-bottom: 2em; margin-top: 2em; }

.prose :where(li):not(:where([class~="not-prose"] *)) { margin-bottom: 0.5em; margin-top: 0.5em; }

.prose :where(ol > li):not(:where([class~="not-prose"] *)) { padding-left: 0.375em; }

.prose :where(ul > li):not(:where([class~="not-prose"] *)) { padding-left: 0.375em; }

.prose :where(.prose > ul > li p):not(:where([class~="not-prose"] *)) { margin-bottom: 0.75em; margin-top: 0.75em; }

.prose :where(.prose > ul > li > :first-child):not(:where([class~="not-prose"] *)) { margin-top: 1.25em; }

.prose :where(.prose > ul > li > :last-child):not(:where([class~="not-prose"] *)) { margin-bottom: 1.25em; }

.prose :where(.prose > ol > li > :first-child):not(:where([class~="not-prose"] *)) { margin-top: 1.25em; }

.prose :where(.prose > ol > li > :last-child):not(:where([class~="not-prose"] *)) { margin-bottom: 1.25em; }

.prose :where(ul ul, ul ol, ol ul, ol ol):not(:where([class~="not-prose"] *)) { margin-bottom: 0.75em; margin-top: 0.75em; }

.prose :where(hr + *):not(:where([class~="not-prose"] *)) { margin-top: 0px; }

.prose :where(h2 + *):not(:where([class~="not-prose"] *)) { margin-top: 0px; }

.prose :where(h3 + *):not(:where([class~="not-prose"] *)) { margin-top: 0px; }

.prose :where(h4 + *):not(:where([class~="not-prose"] *)) { margin-top: 0px; }

.prose :where(thead th:first-child):not(:where([class~="not-prose"] *)) { padding-left: 0px; }

.prose :where(thead th:last-child):not(:where([class~="not-prose"] *)) { padding-right: 0px; }

.prose :where(tbody td, tfoot td):not(:where([class~="not-prose"] *)) { padding: 0.571429em; }

.prose :where(tbody td:first-child, tfoot td:first-child):not(:where([class~="not-prose"] *)) { padding-left: 0px; }

.prose :where(tbody td:last-child, tfoot td:last-child):not(:where([class~="not-prose"] *)) { padding-right: 0px; }

.prose :where(.prose > :first-child):not(:where([class~="not-prose"] *)) { margin-top: 0px; }

.prose :where(.prose > :last-child):not(:where([class~="not-prose"] *)) { margin-bottom: 0px; }

.prose-sm :where(.prose > ul > li p):not(:where([class~="not-prose"] *)) { margin-bottom: 0.571429em; margin-top: 0.571429em; }

.prose-sm :where(.prose > ul > li > :first-child):not(:where([class~="not-prose"] *)) { margin-top: 1.14286em; }

.prose-sm :where(.prose > ul > li > :last-child):not(:where([class~="not-prose"] *)) { margin-bottom: 1.14286em; }

.prose-sm :where(.prose > ol > li > :first-child):not(:where([class~="not-prose"] *)) { margin-top: 1.14286em; }

.prose-sm :where(.prose > ol > li > :last-child):not(:where([class~="not-prose"] *)) { margin-bottom: 1.14286em; }

.prose-sm :where(.prose > :first-child):not(:where([class~="not-prose"] *)) { margin-top: 0px; }

.prose-sm :where(.prose > :last-child):not(:where([class~="not-prose"] *)) { margin-bottom: 0px; }

.prose-base :where(.prose > ul > li p):not(:where([class~="not-prose"] *)) { margin-bottom: 0.75em; margin-top: 0.75em; }

.prose-base :where(.prose > ul > li > :first-child):not(:where([class~="not-prose"] *)) { margin-top: 1.25em; }

.prose-base :where(.prose > ul > li > :last-child):not(:where([class~="not-prose"] *)) { margin-bottom: 1.25em; }

.prose-base :where(.prose > ol > li > :first-child):not(:where([class~="not-prose"] *)) { margin-top: 1.25em; }

.prose-base :where(.prose > ol > li > :last-child):not(:where([class~="not-prose"] *)) { margin-bottom: 1.25em; }

.prose-base :where(.prose > :first-child):not(:where([class~="not-prose"] *)) { margin-top: 0px; }

.prose-base :where(.prose > :last-child):not(:where([class~="not-prose"] *)) { margin-bottom: 0px; }

.prose-lg :where(.prose > ul > li p):not(:where([class~="not-prose"] *)) { margin-bottom: 0.888889em; margin-top: 0.888889em; }

.prose-lg :where(.prose > ul > li > :first-child):not(:where([class~="not-prose"] *)) { margin-top: 1.33333em; }

.prose-lg :where(.prose > ul > li > :last-child):not(:where([class~="not-prose"] *)) { margin-bottom: 1.33333em; }

.prose-lg :where(.prose > ol > li > :first-child):not(:where([class~="not-prose"] *)) { margin-top: 1.33333em; }

.prose-lg :where(.prose > ol > li > :last-child):not(:where([class~="not-prose"] *)) { margin-bottom: 1.33333em; }

.prose-lg :where(.prose > :first-child):not(:where([class~="not-prose"] *)) { margin-top: 0px; }

.prose-lg :where(.prose > :last-child):not(:where([class~="not-prose"] *)) { margin-bottom: 0px; }

.prose-xl :where(.prose > ul > li p):not(:where([class~="not-prose"] *)) { margin-bottom: 0.8em; margin-top: 0.8em; }

.prose-xl :where(.prose > ul > li > :first-child):not(:where([class~="not-prose"] *)) { margin-top: 1.2em; }

.prose-xl :where(.prose > ul > li > :last-child):not(:where([class~="not-prose"] *)) { margin-bottom: 1.2em; }

.prose-xl :where(.prose > ol > li > :first-child):not(:where([class~="not-prose"] *)) { margin-top: 1.2em; }

.prose-xl :where(.prose > ol > li > :last-child):not(:where([class~="not-prose"] *)) { margin-bottom: 1.2em; }

.prose-xl :where(.prose > :first-child):not(:where([class~="not-prose"] *)) { margin-top: 0px; }

.prose-xl :where(.prose > :last-child):not(:where([class~="not-prose"] *)) { margin-bottom: 0px; }

.prose-2xl :where(.prose > ul > li p):not(:where([class~="not-prose"] *)) { margin-bottom: 0.833333em; margin-top: 0.833333em; }

.prose-2xl :where(.prose > ul > li > :first-child):not(:where([class~="not-prose"] *)) { margin-top: 1.33333em; }

.prose-2xl :where(.prose > ul > li > :last-child):not(:where([class~="not-prose"] *)) { margin-bottom: 1.33333em; }

.prose-2xl :where(.prose > ol > li > :first-child):not(:where([class~="not-prose"] *)) { margin-top: 1.33333em; }

.prose-2xl :where(.prose > ol > li > :last-child):not(:where([class~="not-prose"] *)) { margin-bottom: 1.33333em; }

.prose-2xl :where(.prose > :first-child):not(:where([class~="not-prose"] *)) { margin-top: 0px; }

.prose-2xl :where(.prose > :last-child):not(:where([class~="not-prose"] *)) { margin-bottom: 0px; }

.prose-invert { --tw-prose-body:var(--tw-prose-invert-body); --tw-prose-headings:var(--tw-prose-invert-headings); --tw-prose-lead:var(--tw-prose-invert-lead); --tw-prose-links:var(--tw-prose-invert-links); --tw-prose-bold:var(--tw-prose-invert-bold); --tw-prose-counters:var(--tw-prose-invert-counters); --tw-prose-bullets:var(--tw-prose-invert-bullets); --tw-prose-hr:var(--tw-prose-invert-hr); --tw-prose-quotes:var(--tw-prose-invert-quotes); --tw-prose-quote-borders:var(--tw-prose-invert-quote-borders); --tw-prose-captions:var(--tw-prose-invert-captions); --tw-prose-code:var(--tw-prose-invert-code); --tw-prose-pre-code:var(--tw-prose-invert-pre-code); --tw-prose-pre-bg:var(--tw-prose-invert-pre-bg); --tw-prose-th-borders:var(--tw-prose-invert-th-borders); --tw-prose-td-borders:var(--tw-prose-invert-td-borders); }

.form-input, .form-multiselect, .form-select, .form-textarea { --tw-shadow:0 0 transparent; appearance: none; background-color: rgb(255, 255, 255); border-color: rgb(142, 142, 160); border-radius: 0px; border-width: 1px; font-size: 1rem; line-height: 1.5rem; padding: 0.5rem 0.75rem; }

.form-input:focus, .form-multiselect:focus, .form-select:focus, .form-textarea:focus { --tw-ring-inset:var(--tw-empty, ); --tw-ring-offset-width:0px; --tw-ring-offset-color:#fff; --tw-ring-color:#2563eb; --tw-ring-offset-shadow:var(--tw-ring-inset) 0 0 0 var(--tw-ring-offset-width) var(--tw-ring-offset-color); --tw-ring-shadow:var(--tw-ring-inset) 0 0 0 calc(1px + var(--tw-ring-offset-width)) var(--tw-ring-color); border-color: rgb(37, 99, 235); box-shadow: var(--tw-ring-offset-shadow),var(--tw-ring-shadow),var(--tw-shadow); outline: transparent solid 2px; outline-offset: 2px; }

.form-input::-webkit-input-placeholder, .form-textarea::-webkit-input-placeholder { color: rgb(142, 142, 160); opacity: 1; }

.form-input::placeholder, .form-textarea::placeholder { color: rgb(142, 142, 160); opacity: 1; }

.form-input::-webkit-datetime-edit-fields-wrapper { padding: 0px; }

.form-input::-webkit-date-and-time-value { min-height: 1.5em; }

.form-input::-webkit-datetime-edit, .form-input::-webkit-datetime-edit-day-field, .form-input::-webkit-datetime-edit-hour-field, .form-input::-webkit-datetime-edit-meridiem-field, .form-input::-webkit-datetime-edit-millisecond-field, .form-input::-webkit-datetime-edit-minute-field, .form-input::-webkit-datetime-edit-month-field, .form-input::-webkit-datetime-edit-second-field, .form-input::-webkit-datetime-edit-year-field { padding-bottom: 0px; padding-top: 0px; }

.btn { align-items: center; border-color: transparent; border-radius: 0.25rem; border-width: 1px; display: inline-flex; font-size: 0.875rem; line-height: 1.25rem; padding: 0.5rem 0.75rem; pointer-events: auto; }

.btn:focus { outline: transparent solid 2px; outline-offset: 2px; }

.btn:disabled { cursor: not-allowed; opacity: 0.5; }

.btn-primary { --tw-bg-opacity:1; --tw-text-opacity:1; background-color: rgba(16,163,127,var(--tw-bg-opacity)); color: rgba(255,255,255,var(--tw-text-opacity)); }

.btn-primary:hover { --tw-bg-opacity:1; background-color: rgba(26,127,100,var(--tw-bg-opacity)); }

.btn-primary:focus { --tw-ring-offset-shadow:var(--tw-ring-inset) 0 0 0 var(--tw-ring-offset-width) var(--tw-ring-offset-color); --tw-ring-shadow:var(--tw-ring-inset) 0 0 0 calc(2px + var(--tw-ring-offset-width)) var(--tw-ring-color); --tw-ring-offset-width:2px; box-shadow: var(--tw-ring-offset-shadow),var(--tw-ring-shadow),var(--tw-shadow,0 0 transparent); }

.btn-primary.focus-visible { --tw-ring-opacity:1; --tw-ring-color:rgba(25,195,125,var(--tw-ring-opacity)); }

.btn-primary:focus-visible { --tw-ring-opacity:1; --tw-ring-color:rgba(25,195,125,var(--tw-ring-opacity)); }

.btn-danger { --tw-bg-opacity:1; --tw-text-opacity:1; background-color: rgba(185,28,28,var(--tw-bg-opacity)); color: rgba(255,255,255,var(--tw-text-opacity)); }

.btn-danger:hover { --tw-bg-opacity:1; background-color: rgba(153,27,27,var(--tw-bg-opacity)); }

.btn-danger:focus { --tw-ring-offset-shadow:var(--tw-ring-inset) 0 0 0 var(--tw-ring-offset-width) var(--tw-ring-offset-color); --tw-ring-shadow:var(--tw-ring-inset) 0 0 0 calc(2px + var(--tw-ring-offset-width)) var(--tw-ring-color); --tw-ring-offset-width:2px; box-shadow: var(--tw-ring-offset-shadow),var(--tw-ring-shadow),var(--tw-shadow,0 0 transparent); }

.btn-danger.focus-visible { --tw-ring-opacity:1; --tw-ring-color:rgba(220,38,38,var(--tw-ring-opacity)); }

.btn-danger:focus-visible { --tw-ring-opacity:1; --tw-ring-color:rgba(220,38,38,var(--tw-ring-opacity)); }

.btn-danger:disabled:hover { --tw-bg-opacity:1; background-color: rgba(185,28,28,var(--tw-bg-opacity)); }

.btn-neutral { --tw-bg-opacity:1; --tw-text-opacity:1; background-color: rgba(255,255,255,var(--tw-bg-opacity)); border-color: rgba(0, 0, 0, 0.1); border-width: 1px; color: rgba(64,65,79,var(--tw-text-opacity)); font-size: 0.875rem; line-height: 1.25rem; }

.btn-neutral:hover { --tw-bg-opacity:1; background-color: rgba(236,236,241,var(--tw-bg-opacity)); }

.btn-neutral:focus { --tw-ring-offset-shadow:var(--tw-ring-inset) 0 0 0 var(--tw-ring-offset-width) var(--tw-ring-offset-color); --tw-ring-shadow:var(--tw-ring-inset) 0 0 0 calc(2px + var(--tw-ring-offset-width)) var(--tw-ring-color); --tw-ring-offset-width:2px; box-shadow: var(--tw-ring-offset-shadow),var(--tw-ring-shadow),var(--tw-shadow,0 0 transparent); }

.btn-neutral.focus-visible { --tw-ring-opacity:1; --tw-ring-color:rgba(99,102,241,var(--tw-ring-opacity)); }

.btn-neutral:focus-visible { --tw-ring-opacity:1; --tw-ring-color:rgba(99,102,241,var(--tw-ring-opacity)); }

.dark .btn-neutral { --tw-border-opacity:1; --tw-bg-opacity:1; --tw-text-opacity:1; background-color: rgba(52,53,65,var(--tw-bg-opacity)); border-color: rgba(86,88,105,var(--tw-border-opacity)); color: rgba(217,217,227,var(--tw-text-opacity)); }

.dark .btn-neutral:hover { --tw-bg-opacity:1; background-color: rgba(64,65,79,var(--tw-bg-opacity)); }

.btn-dark { --tw-border-opacity:1; --tw-bg-opacity:1; --tw-text-opacity:1; background-color: rgba(52,53,65,var(--tw-bg-opacity)); border-color: rgba(86,88,105,var(--tw-border-opacity)); border-width: 1px; color: rgba(255,255,255,var(--tw-text-opacity)); }

.btn-dark:hover { --tw-bg-opacity:1; background-color: rgba(64,65,79,var(--tw-bg-opacity)); }

.btn-light { --tw-bg-opacity:1; --tw-text-opacity:1; background-color: rgba(236,236,241,var(--tw-bg-opacity)); color: rgba(0,0,0,var(--tw-text-opacity)); }

.btn-light:hover { --tw-bg-opacity:1; background-color: rgba(255,255,255,var(--tw-bg-opacity)); }

.btn-disabled { --tw-bg-opacity:1; --tw-text-opacity:1; color: rgba(142,142,160,var(--tw-text-opacity)); cursor: not-allowed; }

.btn-disabled, .btn-disabled:hover { background-color: rgba(236,236,241,var(--tw-bg-opacity)); }

.btn-disabled:hover { --tw-bg-opacity:1; }

.btn-small { padding: 0.25rem 0.5rem; }

.hide-scrollbar { }

.hide-scrollbar ::-webkit-scrollbar { display: none; }

.sr-only { clip: rect(0px, 0px, 0px, 0px); border-width: 0px; height: 1px; margin: -1px; overflow: hidden; padding: 0px; position: absolute; white-space: nowrap; width: 1px; }

.pointer-events-none { pointer-events: none; }

.pointer-events-auto { pointer-events: auto; }

.visible { visibility: visible; }

.\!invisible { visibility: hidden !important; }

.invisible { visibility: hidden; }

.static { position: static; }

.fixed { position: fixed; }

.absolute { position: absolute; }

.relative { position: relative; }

.\!sticky { position: sticky !important; }

.sticky { position: sticky; }

.inset-0 { left: 0px; right: 0px; }

.inset-0, .inset-y-0 { bottom: 0px; top: 0px; }

.-left-2 { left: -0.5rem; }

.-top-0 { top: 0px; }

.-top-0\.5 { top: -0.125rem; }

.-top-4 { top: -1rem; }

.bottom-0 { bottom: 0px; }

.bottom-1 { bottom: 0.25rem; }

.bottom-1\.5 { bottom: 0.375rem; }

.bottom-2 { bottom: 0.5rem; }

.bottom-4 { bottom: 1rem; }

.bottom-\[10px\] { bottom: 10px; }

.bottom-\[124px\] { bottom: 124px; }

.bottom-full { bottom: 100%; }

.left-0 { left: 0px; }

.left-1 { left: 0.25rem; }

.left-1\.5 { left: 0.375rem; }

.left-1\/2 { left: 50%; }

.left-2 { left: 0.5rem; }

.left-4 { left: 1rem; }

.right-0 { right: 0px; }

.right-1 { right: 0.25rem; }

.right-12 { right: 3rem; }

.right-2 { right: 0.5rem; }

.right-3 { right: 0.75rem; }

.right-4 { right: 1rem; }

.right-6 { right: 1.5rem; }

.top-0 { top: 0px; }

.top-1 { top: 0.25rem; }

.top-1\/2 { top: 50%; }

.top-14 { top: 3.5rem; }

.top-2 { top: 0.5rem; }

.top-\[-3px\] { top: -3px; }

.top-\[-6px\] { top: -6px; }

.top-\[20px\] { top: 20px; }

.top-full { top: 100%; }

.-z-0, .z-0 { z-index: 0; }

.z-10 { z-index: 10; }

.z-20 { z-index: 20; }

.z-50 { z-index: 50; }

.z-\[14\] { z-index: 14; }

.z-\[15\] { z-index: 15; }

.z-\[16\] { z-index: 16; }

.z-\[17\] { z-index: 17; }

.z-\[2\] { z-index: 2; }

.z-\[60\] { z-index: 60; }

.order-1 { order: 1; }

.order-2 { order: 2; }

.col-auto { grid-column: auto / auto; }

.col-span-1 { grid-column: span 1 / span 1; }

.col-start-2 { grid-column-start: 2; }

.row-auto { grid-row: auto / auto; }

.row-start-2 { grid-row-start: 2; }

.float-left { float: left; }

.m-0 { margin: 0px; }

.m-1 { margin: 0.25rem; }

.m-2 { margin: 0.5rem; }

.m-4 { margin: 1rem; }

.m-6 { margin: 1.5rem; }

.m-auto { margin: auto; }

.-my-0 { margin-bottom: 0px; margin-top: 0px; }

.-my-0\.5 { margin-bottom: -0.125rem; margin-top: -0.125rem; }

.-my-1 { margin-bottom: -0.25rem; margin-top: -0.25rem; }

.mx-1 { margin-left: 0.25rem; margin-right: 0.25rem; }

.mx-2 { margin-left: 0.5rem; margin-right: 0.5rem; }

.mx-3 { margin-left: 0.75rem; margin-right: 0.75rem; }

.mx-6 { margin-left: 1.5rem; margin-right: 1.5rem; }

.mx-8 { margin-left: 2rem; margin-right: 2rem; }

.mx-\[10px\] { margin-left: 10px; margin-right: 10px; }

.mx-auto { margin-left: auto; margin-right: auto; }

.my-0 { margin-bottom: 0px; margin-top: 0px; }

.my-1 { margin-bottom: 0.25rem; margin-top: 0.25rem; }

.my-1\.5 { margin-bottom: 0.375rem; margin-top: 0.375rem; }

.my-2 { margin-bottom: 0.5rem; margin-top: 0.5rem; }

.my-3 { margin-bottom: 0.75rem; margin-top: 0.75rem; }

.my-4 { margin-bottom: 1rem; margin-top: 1rem; }

.-ml-0 { margin-left: 0px; }

.-ml-0\.5 { margin-left: -0.125rem; }

.-ml-1 { margin-left: -0.25rem; }

.-ml-4 { margin-left: -1rem; }

.-ml-\[1px\] { margin-left: -1px; }

.-ml-\[8px\] { margin-left: -8px; }

.-ml-px { margin-left: -1px; }

.-mr-1 { margin-right: -0.25rem; }

.-mr-12 { margin-right: -3rem; }

.-mr-2 { margin-right: -0.5rem; }

.-mt-0 { margin-top: 0px; }

.-mt-0\.5 { margin-top: -0.125rem; }

.-mt-\[3px\] { margin-top: -3px; }

.mb-0 { margin-bottom: 0px; }

.mb-0\.5 { margin-bottom: 0.125rem; }

.mb-1 { margin-bottom: 0.25rem; }

.mb-10 { margin-bottom: 2.5rem; }

.mb-12 { margin-bottom: 3rem; }

.mb-2 { margin-bottom: 0.5rem; }

.mb-3 { margin-bottom: 0.75rem; }

.mb-4 { margin-bottom: 1rem; }

.mb-5 { margin-bottom: 1.25rem; }

.mb-6 { margin-bottom: 1.5rem; }

.mb-7 { margin-bottom: 1.75rem; }

.mb-8 { margin-bottom: 2rem; }

.mb-9 { margin-bottom: 2.25rem; }

.mb-\[5px\] { margin-bottom: 5px; }

.mb-auto { margin-bottom: auto; }

.ml-0 { margin-left: 0px; }

.ml-0\.5 { margin-left: 0.125rem; }

.ml-1 { margin-left: 0.25rem; }

.ml-12 { margin-left: 3rem; }

.ml-5 { margin-left: 1.25rem; }

.ml-auto { margin-left: auto; }

.mr-0 { margin-right: 0px; }

.mr-1 { margin-right: 0.25rem; }

.mr-2 { margin-right: 0.5rem; }

.mr-3 { margin-right: 0.75rem; }

.mr-4 { margin-right: 1rem; }

.mr-8 { margin-right: 2rem; }

.mr-auto { margin-right: auto; }

.mt-1 { margin-top: 0.25rem; }

.mt-1\.5 { margin-top: 0.375rem; }

.mt-2 { margin-top: 0.5rem; }

.mt-3 { margin-top: 0.75rem; }

.mt-4 { margin-top: 1rem; }

.mt-5 { margin-top: 1.25rem; }

.mt-6 { margin-top: 1.5rem; }

.mt-8 { margin-top: 2rem; }

.mt-auto { margin-top: auto; }

.box-border { box-sizing: border-box; }

.line-clamp-1 { -webkit-line-clamp: 1; }

.line-clamp-1, .line-clamp-2 { -webkit-box-orient: vertical; display: -webkit-box; overflow: hidden; }

.line-clamp-2 { -webkit-line-clamp: 2; }

.line-clamp-3 { -webkit-box-orient: vertical; -webkit-line-clamp: 3; display: -webkit-box; overflow: hidden; }

.block { display: block; }

.inline-block { display: inline-block; }

.inline { display: inline; }

.flex { display: flex; }

.inline-flex { display: inline-flex; }

.table { display: table; }

.grid { display: grid; }

.contents { display: contents; }

.hidden { display: none; }

.aspect-\[4\/7\] { aspect-ratio: 4 / 7; }

.aspect-\[7\/4\] { aspect-ratio: 7 / 4; }

.aspect-square { aspect-ratio: 1 / 1; }

.\!h-5 { height: 1.25rem !important; }

.h-1 { height: 0.25rem; }

.h-10 { height: 2.5rem; }

.h-11 { height: 2.75rem; }

.h-12 { height: 3rem; }

.h-14 { height: 3.5rem; }

.h-16 { height: 4rem; }

.h-2 { height: 0.5rem; }

.h-24 { height: 6rem; }

.h-3 { height: 0.75rem; }

.h-32 { height: 8rem; }

.h-4 { height: 1rem; }

.h-5 { height: 1.25rem; }

.h-52 { height: 13rem; }

.h-6 { height: 1.5rem; }

.h-64 { height: 16rem; }

.h-7 { height: 1.75rem; }

.h-8 { height: 2rem; }

.h-9 { height: 2.25rem; }

.h-\[1200px\] { height: 1200px; }

.h-\[12px\] { height: 12px; }

.h-\[14px\] { height: 14px; }

.h-\[18px\] { height: 18px; }

.h-\[19px\] { height: 19px; }

.h-\[21px\] { height: 21px; }

.h-\[25px\] { height: 25px; }

.h-\[30px\] { height: 30px; }

.h-\[36px\] { height: 36px; }

.h-\[404px\] { height: 404px; }

.h-\[42px\] { height: 42px; }

.h-\[50px\] { height: 50px; }

.h-\[60px\] { height: 60px; }

.h-\[70px\] { height: 70px; }

.h-auto { height: auto; }

.h-full { height: 100%; }

.h-px { height: 1px; }

.h-screen { height: 100vh; }

.max-h-0 { max-height: 0px; }

.max-h-5 { max-height: 1.25rem; }

.max-h-60 { max-height: 15rem; }

.max-h-64 { max-height: 16rem; }

.max-h-72 { max-height: 18rem; }

.max-h-96 { max-height: 24rem; }

.max-h-\[28rem\] { max-height: 28rem; }

.max-h-\[80vh\] { max-height: 80vh; }

.min-h-0 { min-height: 0px; }

.min-h-\[1\.5rem\] { min-height: 1.5rem; }

.min-h-\[20px\] { min-height: 20px; }

.min-h-\[24px\] { min-height: 24px; }

.min-h-\[71px\] { min-height: 71px; }

.min-h-full { min-height: 100%; }

.min-h-screen { min-height: 100vh; }

.\!w-5 { width: 1.25rem !important; }

.w-10 { width: 2.5rem; }

.w-11 { width: 2.75rem; }

.w-12 { width: 3rem; }

.w-14 { width: 3.5rem; }

.w-16 { width: 4rem; }

.w-2 { width: 0.5rem; }

.w-28 { width: 7rem; }

.w-3 { width: 0.75rem; }

.w-4 { width: 1rem; }

.w-5 { width: 1.25rem; }

.w-6 { width: 1.5rem; }

.w-64 { width: 16rem; }

.w-7 { width: 1.75rem; }

.w-8 { width: 2rem; }

.w-8\/12 { width: 66.6667%; }

.w-96 { width: 24rem; }

.w-\[103px\] { width: 103px; }

.w-\[184px\] { width: 184px; }

.w-\[18px\] { width: 18px; }

.w-\[2\.5px\] { width: 2.5px; }

.w-\[209px\] { width: 209px; }

.w-\[218px\] { width: 218px; }

.w-\[21px\] { width: 21px; }

.w-\[260px\] { width: 260px; }

.w-\[30px\] { width: 30px; }

.w-\[390px\] { width: 390px; }

.w-\[42px\] { width: 42px; }

.w-\[70px\] { width: 70px; }

.w-\[860px\] { width: 860px; }

.w-\[88px\] { width: 88px; }

.w-\[960px\] { width: 960px; }

.w-\[calc\(100\%-50px\)\] { width: calc(100% - 50px); }

.w-auto { width: auto; }

.w-full { width: 100%; }

.w-screen { width: 100vw; }

.min-w-0 { min-width: 0px; }

.min-w-\[100vw\] { min-width: 100vw; }

.min-w-\[180px\] { min-width: 180px; }

.min-w-\[220px\] { min-width: 220px; }

.min-w-\[300px\] { min-width: 300px; }

.min-w-\[8rem\] { min-width: 8rem; }

.max-w-5xl { max-width: 64rem; }

.max-w-7xl { max-width: 80rem; }

.max-w-\[100vw\] { max-width: 100vw; }

.max-w-\[220px\] { max-width: 220px; }

.max-w-\[400px\] { max-width: 400px; }

.max-w-\[44rem\] { max-width: 44rem; }

.max-w-\[510px\] { max-width: 510px; }

.max-w-\[550px\] { max-width: 550px; }

.max-w-\[560px\] { max-width: 560px; }

.max-w-full { max-width: 100%; }

.max-w-lg { max-width: 32rem; }

.max-w-md { max-width: 28rem; }

.max-w-sm { max-width: 24rem; }

.max-w-xl { max-width: 36rem; }

.max-w-xs { max-width: 20rem; }

.flex-1 { flex: 1 1 0%; }

.flex-auto { flex: 1 1 auto; }

.flex-initial { flex: 0 1 auto; }

.flex-none { flex: 0 0 auto; }

.flex-shrink { flex-shrink: 1; }

.flex-shrink-0 { flex-shrink: 0; }

.shrink { flex-shrink: 1; }

.shrink-0 { flex-shrink: 0; }

.flex-grow { flex-grow: 1; }

.flex-grow-0 { flex-grow: 0; }

.grow { flex-grow: 1; }

.table-fixed { table-layout: fixed; }

.border-separate { border-collapse: separate; }

.border-spacing-0 { --tw-border-spacing-x:0px; --tw-border-spacing-y:0px; border-spacing: var(--tw-border-spacing-x) var(--tw-border-spacing-y); }

.origin-\[50\%_50\%\] { transform-origin: 50% 50%; }

.-translate-x-1\/2 { --tw-translate-x:-50%; }

.-translate-x-1\/2, .-translate-x-\[calc\(100\%\+4px\)\] { transform: translate(var(--tw-translate-x),var(--tw-translate-y)) rotate(var(--tw-rotate)) skewX(var(--tw-skew-x)) skewY(var(--tw-skew-y)) scaleX(var(--tw-scale-x)) scaleY(var(--tw-scale-y)); }

.-translate-x-\[calc\(100\%\+4px\)\] { --tw-translate-x:calc(-100% + -4px); }

.-translate-x-full { --tw-translate-x:-100%; }

.-translate-x-full, .-translate-y-1\/2 { transform: translate(var(--tw-translate-x),var(--tw-translate-y)) rotate(var(--tw-rotate)) skewX(var(--tw-skew-x)) skewY(var(--tw-skew-y)) scaleX(var(--tw-scale-x)) scaleY(var(--tw-scale-y)); }

.-translate-y-1\/2 { --tw-translate-y:-50%; }

.translate-x-0 { --tw-translate-x:0px; }

.translate-x-0, .translate-x-0\.5 { transform: translate(var(--tw-translate-x),var(--tw-translate-y)) rotate(var(--tw-rotate)) skewX(var(--tw-skew-x)) skewY(var(--tw-skew-y)) scaleX(var(--tw-scale-x)) scaleY(var(--tw-scale-y)); }

.translate-x-0\.5 { --tw-translate-x:0.125rem; }

.translate-x-1 { --tw-translate-x:0.25rem; }

.translate-x-1, .translate-x-1\/2 { transform: translate(var(--tw-translate-x),var(--tw-translate-y)) rotate(var(--tw-rotate)) skewX(var(--tw-skew-x)) skewY(var(--tw-skew-y)) scaleX(var(--tw-scale-x)) scaleY(var(--tw-scale-y)); }

.translate-x-1\/2 { --tw-translate-x:50%; }

.translate-x-full { --tw-translate-x:100%; }

.translate-x-full, .translate-y-0 { transform: translate(var(--tw-translate-x),var(--tw-translate-y)) rotate(var(--tw-rotate)) skewX(var(--tw-skew-x)) skewY(var(--tw-skew-y)) scaleX(var(--tw-scale-x)) scaleY(var(--tw-scale-y)); }

.translate-y-0 { --tw-translate-y:0px; }

.translate-y-1 { --tw-translate-y:0.25rem; }

.translate-y-1, .translate-y-3 { transform: translate(var(--tw-translate-x),var(--tw-translate-y)) rotate(var(--tw-rotate)) skewX(var(--tw-skew-x)) skewY(var(--tw-skew-y)) scaleX(var(--tw-scale-x)) scaleY(var(--tw-scale-y)); }

.translate-y-3 { --tw-translate-y:0.75rem; }

.translate-y-\[calc\(100\%-71px\)\] { --tw-translate-y:calc(100% - 71px); }

.-rotate-180, .translate-y-\[calc\(100\%-71px\)\] { transform: translate(var(--tw-translate-x),var(--tw-translate-y)) rotate(var(--tw-rotate)) skewX(var(--tw-skew-x)) skewY(var(--tw-skew-y)) scaleX(var(--tw-scale-x)) scaleY(var(--tw-scale-y)); }

.-rotate-180 { --tw-rotate:-180deg; }

.-rotate-45 { --tw-rotate:-45deg; }

.-rotate-45, .-rotate-90 { transform: translate(var(--tw-translate-x),var(--tw-translate-y)) rotate(var(--tw-rotate)) skewX(var(--tw-skew-x)) skewY(var(--tw-skew-y)) scaleX(var(--tw-scale-x)) scaleY(var(--tw-scale-y)); }

.-rotate-90 { --tw-rotate:-90deg; }

.rotate-180 { --tw-rotate:180deg; }

.rotate-180, .rotate-45 { transform: translate(var(--tw-translate-x),var(--tw-translate-y)) rotate(var(--tw-rotate)) skewX(var(--tw-skew-x)) skewY(var(--tw-skew-y)) scaleX(var(--tw-scale-x)) scaleY(var(--tw-scale-y)); }

.rotate-45 { --tw-rotate:45deg; }

.transform { transform: translate(var(--tw-translate-x),var(--tw-translate-y)) rotate(var(--tw-rotate)) skewX(var(--tw-skew-x)) skewY(var(--tw-skew-y)) scaleX(var(--tw-scale-x)) scaleY(var(--tw-scale-y)); }

@-webkit-keyframes slideDownAndFade { 
  0% { opacity: 0; transform: translateY(-2px); }
  100% { opacity: 1; transform: translateY(0px); }
}

.animate-slideDownAndFade { animation: 0.4s cubic-bezier(0.16, 1, 0.3, 1) 0s 1 normal none running slideDownAndFade; }

@-webkit-keyframes slideLeftAndFade { 
  0% { opacity: 0; transform: translateX(2px); }
  100% { opacity: 1; transform: translateX(0px); }
}

.animate-slideLeftAndFade { animation: 0.4s cubic-bezier(0.16, 1, 0.3, 1) 0s 1 normal none running slideLeftAndFade; }

@-webkit-keyframes slideUpAndFade { 
  0% { opacity: 0; transform: translateY(2px); }
  100% { opacity: 1; transform: translateY(0px); }
}

.animate-slideUpAndFade { animation: 0.4s cubic-bezier(0.16, 1, 0.3, 1) 0s 1 normal none running slideUpAndFade; }

@-webkit-keyframes spin { 
  100% { transform: rotate(1turn); }
}

@keyframes spin { 
  100% { transform: rotate(1turn); }
}

.animate-spin { animation: 1s linear 0s infinite normal none running spin; }

.cursor-auto { cursor: auto; }

.cursor-default { cursor: default; }

.cursor-not-allowed { cursor: not-allowed; }

.cursor-pointer { cursor: pointer; }

.select-none { user-select: none; }

.resize-none { resize: none; }

.resize { resize: both; }

.list-disc { list-style-type: disc; }

.list-none { list-style-type: none; }

.appearance-none { appearance: none; }

.grid-cols-1 { grid-template-columns: repeat(1, minmax(0px, 1fr)); }

.grid-cols-2 { grid-template-columns: repeat(2, minmax(0px, 1fr)); }

.grid-cols-\[10px_1fr_10px\] { grid-template-columns: 10px 1fr 10px; }

.grid-cols-\[auto_max-content\] { grid-template-columns: auto max-content; }

.grid-rows-\[minmax\(10px\,_1fr\)_auto_minmax\(10px\,_1fr\)\] { grid-template-rows: minmax(10px, 1fr) auto minmax(10px, 1fr); }

.flex-row { flex-direction: row; }

.flex-col { flex-direction: column; }

.flex-col-reverse { flex-direction: column-reverse; }

.flex-wrap { flex-wrap: wrap; }

.place-items-center { place-items: center; }

.items-start { align-items: flex-start; }

.items-end { align-items: flex-end; }

.items-center { align-items: center; }

.items-stretch { align-items: stretch; }

.justify-start { justify-content: flex-start; }

.justify-end { justify-content: flex-end; }

.justify-center { justify-content: center; }

.justify-between { justify-content: space-between; }

.justify-around { justify-content: space-around; }

.justify-stretch { justify-content: stretch; }

.gap-0 { gap: 0px; }

.gap-1 { gap: 0.25rem; }

.gap-1\.5 { gap: 0.375rem; }

.gap-2 { gap: 0.5rem; }

.gap-2\.5 { gap: 0.625rem; }

.gap-3 { gap: 0.75rem; }

.gap-3\.5 { gap: 0.875rem; }

.gap-4 { gap: 1rem; }

.gap-5 { gap: 1.25rem; }

.gap-6 { gap: 1.5rem; }

.gap-\[10px\] { gap: 10px; }

.gap-x-\[15px\] { column-gap: 15px; }

.space-x-1 > :not([hidden]) ~ :not([hidden]) { --tw-space-x-reverse:0; margin-left: calc(0.25rem*(1 - var(--tw-space-x-reverse))); margin-right: calc(0.25rem*var(--tw-space-x-reverse)); }

.space-x-1\.5 > :not([hidden]) ~ :not([hidden]) { --tw-space-x-reverse:0; margin-left: calc(0.375rem*(1 - var(--tw-space-x-reverse))); margin-right: calc(0.375rem*var(--tw-space-x-reverse)); }

.space-x-2 > :not([hidden]) ~ :not([hidden]) { --tw-space-x-reverse:0; margin-left: calc(0.5rem*(1 - var(--tw-space-x-reverse))); margin-right: calc(0.5rem*var(--tw-space-x-reverse)); }

.space-y-2 > :not([hidden]) ~ :not([hidden]) { --tw-space-y-reverse:0; margin-bottom: calc(0.5rem*var(--tw-space-y-reverse)); margin-top: calc(0.5rem*(1 - var(--tw-space-y-reverse))); }

.self-end { align-self: flex-end; }

.self-center { align-self: center; }

.self-stretch { align-self: stretch; }

.overflow-auto { overflow: auto; }

.overflow-hidden { overflow: hidden; }

.overflow-x-auto { overflow-x: auto; }

.overflow-y-auto { overflow-y: auto; }

.overflow-x-hidden { overflow-x: hidden; }

.overflow-y-hidden { overflow-y: hidden; }

.truncate { overflow: hidden; white-space: nowrap; }

.overflow-ellipsis, .text-ellipsis, .truncate { text-overflow: ellipsis; }

.whitespace-normal { white-space: normal; }

.whitespace-nowrap { white-space: nowrap; }

.\!whitespace-pre { white-space: pre !important; }

.whitespace-pre-line { white-space: pre-line; }

.\!whitespace-pre-wrap { white-space: pre-wrap !important; }

.whitespace-pre-wrap { white-space: pre-wrap; }

.break-words { overflow-wrap: break-word; }

.break-all { word-break: break-all; }

.rounded { border-radius: 0.25rem; }

.rounded-2xl { border-radius: 1rem; }

.rounded-\[4px\] { border-radius: 4px; }

.rounded-\[5px\] { border-radius: 5px; }

.rounded-full { border-radius: 9999px; }

.rounded-lg { border-radius: 0.5rem; }

.rounded-md { border-radius: 0.375rem; }

.rounded-none { border-radius: 0px; }

.rounded-sm { border-radius: 0.125rem; }

.rounded-xl { border-radius: 0.75rem; }

.rounded-b-lg { border-bottom-left-radius: 0.5rem; border-bottom-right-radius: 0.5rem; }

.rounded-l-md { border-bottom-left-radius: 0.375rem; border-top-left-radius: 0.375rem; }

.rounded-r-md { border-bottom-right-radius: 0.375rem; }

.rounded-r-md, .rounded-t-md { border-top-right-radius: 0.375rem; }

.rounded-t-md { border-top-left-radius: 0.375rem; }

.rounded-t-xl { border-top-left-radius: 0.75rem; border-top-right-radius: 0.75rem; }

.rounded-br-sm { border-bottom-right-radius: 0.125rem; }

.border { border-width: 1px; }

.border-0 { border-width: 0px; }

.border-\[0\.5px\] { border-width: 0.5px; }

.border-b { border-bottom-width: 1px; }

.border-l { border-left-width: 1px; }

.border-l-4 { border-left-width: 4px; }

.border-r { border-right-width: 1px; }

.border-r-0 { border-right-width: 0px; }

.border-t { border-top-width: 1px; }

.border-none { border-style: none; }

.border-black\/10 { border-color: rgba(0, 0, 0, 0.1); }

.border-black\/20 { border-color: rgba(0, 0, 0, 0.2); }

.border-black\/30 { border-color: rgba(0, 0, 0, 0.3); }

.border-black\/5 { border-color: rgba(0, 0, 0, 0.05); }

.border-brand-green { --tw-border-opacity:1; border-color: rgba(25,195,125,var(--tw-border-opacity)); }

.border-gray-100 { --tw-border-opacity:1; border-color: rgba(236,236,241,var(--tw-border-opacity)); }

.border-gray-200 { --tw-border-opacity:1; border-color: rgba(217,217,227,var(--tw-border-opacity)); }

.border-gray-300 { --tw-border-opacity:1; border-color: rgba(197,197,210,var(--tw-border-opacity)); }

.border-gray-400 { --tw-border-opacity:1; border-color: rgba(172,172,190,var(--tw-border-opacity)); }

.border-gray-500 { --tw-border-opacity:1; border-color: rgba(142,142,160,var(--tw-border-opacity)); }

.border-gray-700 { --tw-border-opacity:1; border-color: rgba(64,65,79,var(--tw-border-opacity)); }

.border-gray-800 { --tw-border-opacity:1; border-color: rgba(52,53,65,var(--tw-border-opacity)); }

.border-green-500 { --tw-border-opacity:1; border-color: rgba(25,195,125,var(--tw-border-opacity)); }

.border-indigo-400 { --tw-border-opacity:1; border-color: rgba(129,140,248,var(--tw-border-opacity)); }

.border-orange-400 { --tw-border-opacity:1; border-color: rgba(255,128,82,var(--tw-border-opacity)); }

.border-orange-500 { --tw-border-opacity:1; border-color: rgba(224,108,43,var(--tw-border-opacity)); }

.border-red-500 { --tw-border-opacity:1; border-color: rgba(239,68,68,var(--tw-border-opacity)); }

.border-transparent { border-color: transparent; }

.border-white { --tw-border-opacity:1; border-color: rgba(255,255,255,var(--tw-border-opacity)); }

.border-white\/20 { border-color: rgba(255, 255, 255, 0.2); }

.\!bg-gray-700 { --tw-bg-opacity:1 !important; background-color: rgba(64,65,79,var(--tw-bg-opacity)) !important; }

.\!bg-transparent { background-color: transparent !important; }

.\!bg-white { --tw-bg-opacity:1 !important; background-color: rgba(255,255,255,var(--tw-bg-opacity)) !important; }

.bg-\[\#0077FF\] { --tw-bg-opacity:1; background-color: rgba(0,119,255,var(--tw-bg-opacity)); }

.bg-black { --tw-bg-opacity:1; background-color: rgba(0,0,0,var(--tw-bg-opacity)); }

.bg-black\/\[\.04\] { background-color: rgba(0, 0, 0, 0.04); }

.bg-black\/\[\.08\] { background-color: rgba(0, 0, 0, 0.08); }

.bg-black\/\[\.12\] { background-color: rgba(0, 0, 0, 0.12); }

.bg-blue-100 { --tw-bg-opacity:1; background-color: rgba(219,234,254,var(--tw-bg-opacity)); }

.bg-blue-200 { --tw-bg-opacity:1; background-color: rgba(191,219,254,var(--tw-bg-opacity)); }

.bg-blue-300 { --tw-bg-opacity:1; background-color: rgba(121,137,255,var(--tw-bg-opacity)); }

.bg-gray-100 { --tw-bg-opacity:1; background-color: rgba(236,236,241,var(--tw-bg-opacity)); }

.bg-gray-200 { --tw-bg-opacity:1; background-color: rgba(217,217,227,var(--tw-bg-opacity)); }

.bg-gray-300 { --tw-bg-opacity:1; background-color: rgba(197,197,210,var(--tw-bg-opacity)); }

.bg-gray-50 { --tw-bg-opacity:1; background-color: rgba(247,247,248,var(--tw-bg-opacity)); }

.bg-gray-500 { --tw-bg-opacity:1; background-color: rgba(142,142,160,var(--tw-bg-opacity)); }

.bg-gray-500\/90 { background-color: rgba(141, 141, 160, 0.9); }

.bg-gray-600 { --tw-bg-opacity:1; background-color: rgba(86,88,105,var(--tw-bg-opacity)); }

.bg-gray-700 { --tw-bg-opacity:1; background-color: rgba(64,65,79,var(--tw-bg-opacity)); }

.bg-gray-800 { --tw-bg-opacity:1; background-color: rgba(52,53,65,var(--tw-bg-opacity)); }

.bg-gray-900 { --tw-bg-opacity:1; background-color: rgba(32,33,35,var(--tw-bg-opacity)); }

.bg-gray-950 { --tw-bg-opacity:1; background-color: rgba(5,5,9,var(--tw-bg-opacity)); }

.bg-green-100 { --tw-bg-opacity:1; background-color: rgba(210,244,211,var(--tw-bg-opacity)); }

.bg-green-500 { --tw-bg-opacity:1; background-color: rgba(25,195,125,var(--tw-bg-opacity)); }

.bg-green-500\/10 { background-color: rgba(25, 195, 125, 0.1); }

.bg-green-600 { --tw-bg-opacity:1; background-color: rgba(16,163,127,var(--tw-bg-opacity)); }

.bg-orange-500 { --tw-bg-opacity:1; background-color: rgba(224,108,43,var(--tw-bg-opacity)); }

.bg-orange-500\/10 { background-color: rgba(224, 108, 43, 0.1); }

.bg-red-100 { --tw-bg-opacity:1; background-color: rgba(254,226,226,var(--tw-bg-opacity)); }

.bg-red-200 { --tw-bg-opacity:1; background-color: rgba(254,202,202,var(--tw-bg-opacity)); }

.bg-red-500 { --tw-bg-opacity:1; background-color: rgba(239,68,68,var(--tw-bg-opacity)); }

.bg-red-500\/10 { background-color: rgba(239, 68, 68, 0.1); }

.bg-red-600 { --tw-bg-opacity:1; background-color: rgba(220,38,38,var(--tw-bg-opacity)); }

.bg-transparent { background-color: transparent; }

.bg-white { --tw-bg-opacity:1; background-color: rgba(255,255,255,var(--tw-bg-opacity)); }

.bg-white\/20 { background-color: rgba(255, 255, 255, 0.2); }

.bg-white\/25 { background-color: rgba(255, 255, 255, 0.25); }

.bg-yellow-100 { --tw-bg-opacity:1; background-color: rgba(254,249,195,var(--tw-bg-opacity)); }

.bg-yellow-200 { --tw-bg-opacity:1; background-color: rgba(250,230,158,var(--tw-bg-opacity)); }

.bg-yellow-300 { --tw-bg-opacity:1; background-color: rgba(255,243,201,var(--tw-bg-opacity)); }

.bg-opacity-75 { --tw-bg-opacity:0.75; }

.bg-gradient-to-l { background-image: linear-gradient(to left,var(--tw-gradient-stops)); }

.bg-gradient-to-t { background-image: linear-gradient(to top,var(--tw-gradient-stops)); }

.from-gray-100 { --tw-gradient-from:#ececf1 var(--tw-gradient-from-position); --tw-gradient-to:rgba(236,236,241,0) var(--tw-gradient-to-position); --tw-gradient-stops:var(--tw-gradient-from),var(--tw-gradient-to); }

.from-gray-800 { --tw-gradient-from:#343541 var(--tw-gradient-from-position); --tw-gradient-to:rgba(52,53,65,0) var(--tw-gradient-to-position); --tw-gradient-stops:var(--tw-gradient-from),var(--tw-gradient-to); }

.from-gray-900 { --tw-gradient-from:#202123 var(--tw-gradient-from-position); --tw-gradient-to:rgba(32,33,35,0) var(--tw-gradient-to-position); --tw-gradient-stops:var(--tw-gradient-from),var(--tw-gradient-to); }

.from-gray-900\/0 { --tw-gradient-from:rgba(32,33,35,0) var(--tw-gradient-from-position); --tw-gradient-to:rgba(32,33,35,0) var(--tw-gradient-to-position); --tw-gradient-stops:var(--tw-gradient-from),var(--tw-gradient-to); }

.from-\[60\%\] { --tw-gradient-from-position:60%; }

.to-gray-900 { --tw-gradient-to:#202123 var(--tw-gradient-to-position); }

.bg-contain { background-size: contain; }

.bg-cover { background-size: cover; }

.bg-center { background-position: 50% center; }

.bg-no-repeat { background-repeat: no-repeat; }

.fill-gray-800 { fill: rgb(52, 53, 65); }

.stroke-gray-200 { stroke: rgb(217, 217, 227); }

.object-cover { object-fit: cover; }

.p-0 { padding: 0px; }

.p-0\.5 { padding: 0.125rem; }

.p-1 { padding: 0.25rem; }

.p-2 { padding: 0.5rem; }

.p-3 { padding: 0.75rem; }

.p-4 { padding: 1rem; }

.p-6 { padding: 1.5rem; }

.p-7 { padding: 1.75rem; }

.p-\[15px\] { padding: 15px; }

.p-\[5px\] { padding: 5px; }

.p-\[var\(--viewport-padding\)\] { padding: var(--viewport-padding); }

.px-0 { padding-left: 0px; padding-right: 0px; }

.px-0\.5 { padding-left: 0.125rem; padding-right: 0.125rem; }

.px-1 { padding-left: 0.25rem; padding-right: 0.25rem; }

.px-1\.5 { padding-left: 0.375rem; padding-right: 0.375rem; }

.px-2 { padding-left: 0.5rem; padding-right: 0.5rem; }

.px-3 { padding-left: 0.75rem; padding-right: 0.75rem; }

.px-4 { padding-left: 1rem; padding-right: 1rem; }

.px-5 { padding-left: 1.25rem; padding-right: 1.25rem; }

.px-6 { padding-left: 1.5rem; padding-right: 1.5rem; }

.px-8 { padding-left: 2rem; padding-right: 2rem; }

.py-0 { padding-bottom: 0px; padding-top: 0px; }

.py-0\.5 { padding-bottom: 0.125rem; padding-top: 0.125rem; }

.py-1 { padding-bottom: 0.25rem; padding-top: 0.25rem; }

.py-1\.5 { padding-bottom: 0.375rem; padding-top: 0.375rem; }

.py-2 { padding-bottom: 0.5rem; padding-top: 0.5rem; }

.py-3 { padding-bottom: 0.75rem; padding-top: 0.75rem; }

.py-4 { padding-bottom: 1rem; padding-top: 1rem; }

.py-6 { padding-bottom: 1.5rem; padding-top: 1.5rem; }

.py-7 { padding-bottom: 1.75rem; padding-top: 1.75rem; }

.py-8 { padding-bottom: 2rem; padding-top: 2rem; }

.py-\[10px\] { padding-bottom: 10px; padding-top: 10px; }

.\!pt-0 { padding-top: 0px !important; }

.pb-0 { padding-bottom: 0px; }

.pb-0\.5 { padding-bottom: 0.125rem; }

.pb-1 { padding-bottom: 0.25rem; }

.pb-1\.5 { padding-bottom: 0.375rem; }

.pb-2 { padding-bottom: 0.5rem; }

.pb-3 { padding-bottom: 0.75rem; }

.pb-4 { padding-bottom: 1rem; }

.pb-5 { padding-bottom: 1.25rem; }

.pb-8 { padding-bottom: 2rem; }

.pl-0 { padding-left: 0px; }

.pl-1 { padding-left: 0.25rem; }

.pl-10 { padding-left: 2.5rem; }

.pl-12 { padding-left: 3rem; }

.pl-2 { padding-left: 0.5rem; }

.pl-3 { padding-left: 0.75rem; }

.pl-4 { padding-left: 1rem; }

.pl-5 { padding-left: 1.25rem; }

.pl-6 { padding-left: 1.5rem; }

.pr-1 { padding-right: 0.25rem; }

.pr-1\.5 { padding-right: 0.375rem; }

.pr-10 { padding-right: 2.5rem; }

.pr-11 { padding-right: 2.75rem; }

.pr-12 { padding-right: 3rem; }

.pr-14 { padding-right: 3.5rem; }

.pr-2 { padding-right: 0.5rem; }

.pr-3 { padding-right: 0.75rem; }

.pr-4 { padding-right: 1rem; }

.pr-5 { padding-right: 1.25rem; }

.pr-6 { padding-right: 1.5rem; }

.pr-7 { padding-right: 1.75rem; }

.pr-9 { padding-right: 2.25rem; }

.pr-\[4\.5rem\] { padding-right: 4.5rem; }

.pt-0 { padding-top: 0px; }

.pt-0\.5 { padding-top: 0.125rem; }

.pt-1 { padding-top: 0.25rem; }

.pt-2 { padding-top: 0.5rem; }

.pt-20 { padding-top: 5rem; }

.pt-3 { padding-top: 0.75rem; }

.pt-4 { padding-top: 1rem; }

.pt-5 { padding-top: 1.25rem; }

.pt-\[67\%\] { padding-top: 67%; }

.text-left { text-align: left; }

.text-center { text-align: center; }

.text-right { text-align: right; }

.align-top { vertical-align: top; }

.font-mono { font-family: "SÃ¶hne Mono", Monaco, "Andale Mono", "Ubuntu Mono", monospace; }

.font-sans { font-family: SÃ¶hne, ui-sans-serif, system-ui, -apple-system, "Segoe UI", Roboto, Ubuntu, Cantarell, "Noto Sans", sans-serif, "Helvetica Neue", Arial, "Apple Color Emoji", "Segoe UI Emoji", "Segoe UI Symbol", "Noto Color Emoji"; }

.text-2xl { font-size: 1.5rem; line-height: 2rem; }

.text-3xl { font-size: 1.875rem; line-height: 2.25rem; }

.text-4xl { font-size: 2.25rem; line-height: 2.5rem; }

.text-\[10px\] { font-size: 10px; }

.text-\[13px\] { font-size: 13px; }

.text-\[15px\] { font-size: 15px; }

.text-\[28px\] { font-size: 28px; }

.text-base { font-size: 1rem; line-height: 1.5rem; }

.text-lg { font-size: 1.125rem; line-height: 1.75rem; }

.text-sm { font-size: 0.875rem; line-height: 1.25rem; }

.text-xl { font-size: 1.25rem; line-height: 1.75rem; }

.text-xs { font-size: 0.75rem; line-height: 1rem; }

.font-bold { font-weight: 700; }

.font-medium { font-weight: 500; }

.font-normal { font-weight: 400; }

.font-semibold { font-weight: 600; }

.uppercase { text-transform: uppercase; }

.capitalize { text-transform: capitalize; }

.normal-case { text-transform: none; }

.italic { font-style: italic; }

.leading-3 { line-height: 0.75rem; }

.leading-5 { line-height: 1.25rem; }

.leading-6 { line-height: 1.5rem; }

.leading-\[1\.3\] { line-height: 1.3; }

.leading-none { line-height: 1; }

.leading-normal { line-height: 1.5; }

.leading-tight { line-height: 1.25; }

.tracking-wide { letter-spacing: 0.025em; }

.tracking-widest { letter-spacing: 0.1em; }

.text-\[\#756B5C\] { --tw-text-opacity:1; color: rgba(117,107,92,var(--tw-text-opacity)); }

.text-\[\#800080\] { --tw-text-opacity:1; color: rgba(128,0,128,var(--tw-text-opacity)); }

.text-black { --tw-text-opacity:1; color: rgba(0,0,0,var(--tw-text-opacity)); }

.text-black\/50 { color: rgba(0, 0, 0, 0.5); }

.text-black\/60 { color: rgba(0, 0, 0, 0.6); }

.text-black\/70 { color: rgba(0, 0, 0, 0.7); }

.text-blue-400 { --tw-text-opacity:1; color: rgba(58,136,255,var(--tw-text-opacity)); }

.text-blue-500 { --tw-text-opacity:1; color: rgba(69,89,164,var(--tw-text-opacity)); }

.text-blue-600 { --tw-text-opacity:1; color: rgba(37,99,235,var(--tw-text-opacity)); }

.text-blue-700 { --tw-text-opacity:1; color: rgba(29,78,216,var(--tw-text-opacity)); }

.text-brand-green { --tw-text-opacity:1; color: rgba(25,195,125,var(--tw-text-opacity)); }

.text-brand-purple { --tw-text-opacity:1; color: rgba(171,104,255,var(--tw-text-opacity)); }

.text-gray-100 { --tw-text-opacity:1; color: rgba(236,236,241,var(--tw-text-opacity)); }

.text-gray-200 { --tw-text-opacity:1; color: rgba(217,217,227,var(--tw-text-opacity)); }

.text-gray-300 { --tw-text-opacity:1; color: rgba(197,197,210,var(--tw-text-opacity)); }

.text-gray-400 { --tw-text-opacity:1; color: rgba(172,172,190,var(--tw-text-opacity)); }

.text-gray-500 { --tw-text-opacity:1; color: rgba(142,142,160,var(--tw-text-opacity)); }

.text-gray-600 { --tw-text-opacity:1; color: rgba(86,88,105,var(--tw-text-opacity)); }

.text-gray-700 { --tw-text-opacity:1; color: rgba(64,65,79,var(--tw-text-opacity)); }

.text-gray-800 { --tw-text-opacity:1; color: rgba(52,53,65,var(--tw-text-opacity)); }

.text-gray-900 { --tw-text-opacity:1; color: rgba(32,33,35,var(--tw-text-opacity)); }

.text-green-500 { --tw-text-opacity:1; color: rgba(25,195,125,var(--tw-text-opacity)); }

.text-green-600 { --tw-text-opacity:1; color: rgba(16,163,127,var(--tw-text-opacity)); }

.text-green-700 { --tw-text-opacity:1; color: rgba(26,127,100,var(--tw-text-opacity)); }

.text-indigo-500 { --tw-text-opacity:1; color: rgba(99,102,241,var(--tw-text-opacity)); }

.text-orange-400 { --tw-text-opacity:1; color: rgba(255,128,82,var(--tw-text-opacity)); }

.text-orange-500 { --tw-text-opacity:1; color: rgba(224,108,43,var(--tw-text-opacity)); }

.text-red-300 { --tw-text-opacity:1; color: rgba(252,165,165,var(--tw-text-opacity)); }

.text-red-500 { --tw-text-opacity:1; color: rgba(239,68,68,var(--tw-text-opacity)); }

.text-red-600 { --tw-text-opacity:1; color: rgba(220,38,38,var(--tw-text-opacity)); }

.text-red-700 { --tw-text-opacity:1; color: rgba(185,28,28,var(--tw-text-opacity)); }

.text-red-800 { --tw-text-opacity:1; color: rgba(153,27,27,var(--tw-text-opacity)); }

.text-white { --tw-text-opacity:1; color: rgba(255,255,255,var(--tw-text-opacity)); }

.text-white\/25 { color: rgba(255, 255, 255, 0.25); }

.text-white\/50 { color: rgba(255, 255, 255, 0.5); }

.text-white\/80 { color: rgba(255, 255, 255, 0.8); }

.text-yellow-400 { --tw-text-opacity:1; color: rgba(255,198,87,var(--tw-text-opacity)); }

.text-yellow-500 { --tw-text-opacity:1; color: rgba(234,179,8,var(--tw-text-opacity)); }

.text-yellow-700 { --tw-text-opacity:1; color: rgba(161,98,7,var(--tw-text-opacity)); }

.text-yellow-900 { --tw-text-opacity:1; color: rgba(146,114,1,var(--tw-text-opacity)); }

.underline { text-decoration-line: underline; }

.\!no-underline { text-decoration-line: none !important; }

.antialiased { -webkit-font-smoothing: antialiased; }

.placeholder-gray-500::-webkit-input-placeholder { --tw-placeholder-opacity:1; color: rgba(142,142,160,var(--tw-placeholder-opacity)); }

.placeholder-gray-500::placeholder { --tw-placeholder-opacity:1; color: rgba(142,142,160,var(--tw-placeholder-opacity)); }

.opacity-0 { opacity: 0; }

.opacity-100 { opacity: 1; }

.opacity-20 { opacity: 0.2; }

.opacity-30 { opacity: 0.3; }

.opacity-50 { opacity: 0.5; }

.opacity-70 { opacity: 0.7; }

.opacity-80 { opacity: 0.8; }

.\!shadow-none { --tw-shadow:0 0 transparent !important; --tw-shadow-colored:0 0 transparent !important; box-shadow: var(--tw-ring-offset-shadow,0 0 transparent),var(--tw-ring-shadow,0 0 transparent),var(--tw-shadow) !important; }

.shadow { --tw-shadow:0 1px 3px 0 rgba(0,0,0,0.1),0 1px 2px -1px rgba(0,0,0,0.1); --tw-shadow-colored:0 1px 3px 0 var(--tw-shadow-color),0 1px 2px -1px var(--tw-shadow-color); }

.shadow, .shadow-\[0_0\.5px_2px_rgba\(0\,0\,0\,0\.15\)\] { box-shadow: var(--tw-ring-offset-shadow,0 0 transparent),var(--tw-ring-shadow,0 0 transparent),var(--tw-shadow); }

.shadow-\[0_0\.5px_2px_rgba\(0\,0\,0\,0\.15\)\] { --tw-shadow:0 0.5px 2px rgba(0,0,0,0.15); --tw-shadow-colored:0 0.5px 2px var(--tw-shadow-color); }

.shadow-\[0_1px_2px_rgba\(0\,0\,0\,0\.45\)\] { --tw-shadow:0 1px 2px rgba(0,0,0,0.45); --tw-shadow-colored:0 1px 2px var(--tw-shadow-color); }

.shadow-\[0_1px_2px_rgba\(0\,0\,0\,0\.45\)\], .shadow-\[0_1px_7px_0_rgba\(0\,0\,0\,0\.03\)\] { box-shadow: var(--tw-ring-offset-shadow,0 0 transparent),var(--tw-ring-shadow,0 0 transparent),var(--tw-shadow); }

.shadow-\[0_1px_7px_0_rgba\(0\,0\,0\,0\.03\)\] { --tw-shadow:0 1px 7px 0 rgba(0,0,0,0.03); --tw-shadow-colored:0 1px 7px 0 var(--tw-shadow-color); }

.shadow-\[0_1px_7px_0px_rgba\(0\,0\,0\,0\.06\)\] { --tw-shadow:0 1px 7px 0px rgba(0,0,0,0.06); --tw-shadow-colored:0 1px 7px 0px var(--tw-shadow-color); }

.shadow-\[0_1px_7px_0px_rgba\(0\,0\,0\,0\.06\)\], .shadow-\[0_2px_12px_0px_rgba\(0\,0\,0\,0\.08\)\] { box-shadow: var(--tw-ring-offset-shadow,0 0 transparent),var(--tw-ring-shadow,0 0 transparent),var(--tw-shadow); }

.shadow-\[0_2px_12px_0px_rgba\(0\,0\,0\,0\.08\)\] { --tw-shadow:0 2px 12px 0px rgba(0,0,0,0.08); --tw-shadow-colored:0 2px 12px 0px var(--tw-shadow-color); }

.shadow-\[0_2px_24px_rgba\(0\,0\,0\,0\.05\)\] { --tw-shadow:0 2px 24px rgba(0,0,0,0.05); --tw-shadow-colored:0 2px 24px var(--tw-shadow-color); }

.shadow-\[0_2px_24px_rgba\(0\,0\,0\,0\.05\)\], .shadow-\[0px_4px_14px_rgba\(0\,0\,0\,0\.06\)\] { box-shadow: var(--tw-ring-offset-shadow,0 0 transparent),var(--tw-ring-shadow,0 0 transparent),var(--tw-shadow); }

.shadow-\[0px_4px_14px_rgba\(0\,0\,0\,0\.06\)\] { --tw-shadow:0px 4px 14px rgba(0,0,0,0.06); --tw-shadow-colored:0px 4px 14px var(--tw-shadow-color); }

.shadow-lg { --tw-shadow:0 10px 15px -3px rgba(0,0,0,0.1),0 4px 6px -4px rgba(0,0,0,0.1); --tw-shadow-colored:0 10px 15px -3px var(--tw-shadow-color),0 4px 6px -4px var(--tw-shadow-color); }

.shadow-lg, .shadow-md { box-shadow: var(--tw-ring-offset-shadow,0 0 transparent),var(--tw-ring-shadow,0 0 transparent),var(--tw-shadow); }

.shadow-md { --tw-shadow:0 4px 6px -1px rgba(0,0,0,0.1),0 2px 4px -2px rgba(0,0,0,0.1); --tw-shadow-colored:0 4px 6px -1px var(--tw-shadow-color),0 2px 4px -2px var(--tw-shadow-color); }

.shadow-sm { --tw-shadow:0 1px 2px 0 rgba(0,0,0,0.05); --tw-shadow-colored:0 1px 2px 0 var(--tw-shadow-color); }

.shadow-sm, .shadow-xl { box-shadow: var(--tw-ring-offset-shadow,0 0 transparent),var(--tw-ring-shadow,0 0 transparent),var(--tw-shadow); }

.shadow-xl { --tw-shadow:0 20px 25px -5px rgba(0,0,0,0.1),0 8px 10px -6px rgba(0,0,0,0.1); --tw-shadow-colored:0 20px 25px -5px var(--tw-shadow-color),0 8px 10px -6px var(--tw-shadow-color); }

.shadow-xs { --tw-shadow:0 0 15px rgba(0,0,0,0.1); --tw-shadow-colored:0 0 15px var(--tw-shadow-color); }

.shadow-xs, .shadow-xxs { box-shadow: var(--tw-ring-offset-shadow,0 0 transparent),var(--tw-ring-shadow,0 0 transparent),var(--tw-shadow); }

.shadow-xxs { --tw-shadow:0 1px 7px 0 rgba(0,0,0,0.03); --tw-shadow-colored:0 1px 7px 0 var(--tw-shadow-color); }

.shadow-indigo-400 { --tw-shadow-color:#818cf8; --tw-shadow:var(--tw-shadow-colored); }

.outline-none { outline: transparent solid 2px; outline-offset: 2px; }

.ring-1 { --tw-ring-offset-shadow:var(--tw-ring-inset) 0 0 0 var(--tw-ring-offset-width) var(--tw-ring-offset-color); --tw-ring-shadow:var(--tw-ring-inset) 0 0 0 calc(1px + var(--tw-ring-offset-width)) var(--tw-ring-color); box-shadow: var(--tw-ring-offset-shadow),var(--tw-ring-shadow),var(--tw-shadow,0 0 transparent); }

.ring-inset { --tw-ring-inset:inset; }

.ring-black\/10 { --tw-ring-color:rgba(0,0,0,0.1); }

.blur-xl { --tw-blur:blur(24px); }

.blur-xl, .filter { filter: var(--tw-blur) var(--tw-brightness) var(--tw-contrast) var(--tw-grayscale) var(--tw-hue-rotate) var(--tw-invert) var(--tw-saturate) var(--tw-sepia) var(--tw-drop-shadow); }

.filter-none { filter: none; }

.transition { transition-duration: 0.15s; transition-property: color, background-color, border-color, text-decoration-color, fill, stroke, opacity, box-shadow, transform, filter, backdrop-filter, -webkit-transform, -webkit-filter, -webkit-backdrop-filter; transition-timing-function: cubic-bezier(0.4, 0, 0.2, 1); }

.transition-\[stroke-dashoffset\] { transition-duration: 0.15s; transition-property: stroke-dashoffset; transition-timing-function: cubic-bezier(0.4, 0, 0.2, 1); }

.transition-all { transition-duration: 0.15s; transition-property: all; transition-timing-function: cubic-bezier(0.4, 0, 0.2, 1); }

.transition-colors { transition-duration: 0.15s; transition-property: color, background-color, border-color, text-decoration-color, fill, stroke; transition-timing-function: cubic-bezier(0.4, 0, 0.2, 1); }

.transition-opacity { transition-duration: 0.15s; transition-property: opacity; transition-timing-function: cubic-bezier(0.4, 0, 0.2, 1); }

.transition-transform { transition-duration: 0.15s; transition-property: transform, -webkit-transform; transition-timing-function: cubic-bezier(0.4, 0, 0.2, 1); }

.transition-width { transition-duration: 0.15s; transition-property: width; transition-timing-function: cubic-bezier(0.4, 0, 0.2, 1); }

.duration-100 { transition-duration: 0.1s; }

.duration-1000 { transition-duration: 1s; }

.duration-150 { transition-duration: 0.15s; }

.duration-200 { transition-duration: 0.2s; }

.duration-300 { transition-duration: 0.3s; }

.duration-500 { transition-duration: 0.5s; }

.ease-in { transition-timing-function: cubic-bezier(0.4, 0, 1, 1); }

.ease-in-out { transition-timing-function: cubic-bezier(0.4, 0, 0.2, 1); }

.ease-linear { transition-timing-function: linear; }

.ease-out { transition-timing-function: cubic-bezier(0, 0, 0.2, 1); }

.will-change-\[opacity\,transform\] { will-change: opacity, transform; }

.will-change-transform { will-change: transform; }

.\[--viewport-padding\:_25px\] { --viewport-padding:25px; }

.\[grid-area\:_description\] { grid-area: description / description / description / description; }

.\[grid-area\:_title\] { grid-area: title / title / title / title; }

.\[grid-template-areas\:_\'title_action\'_\'description_action\'\] { grid-template-areas: "title action" "description action"; }

body, html { height: 100%; }

.dark body, .dark html { --tw-bg-opacity:1; background-color: rgba(52,53,65,var(--tw-bg-opacity)); }

#__next, #root { height: 100%; }

.markdown { max-width: none; }

.markdown h1, .markdown h2 { font-weight: 600; }

.markdown h2 { margin-bottom: 1rem; margin-top: 2rem; }

.markdown h3 { font-weight: 600; }

.markdown h3, .markdown h4 { margin-bottom: 0.5rem; margin-top: 1rem; }

.markdown h4 { font-weight: 400; }

.markdown h5 { font-weight: 600; }

.markdown blockquote { --tw-border-opacity:1; border-color: rgba(142,142,160,var(--tw-border-opacity)); border-left-width: 2px; line-height: 1.5rem; margin: 0px; padding-bottom: 0.5rem; padding-left: 1rem; padding-top: 0.5rem; }

.markdown blockquote > p { margin: 0px; }

.markdown blockquote > p::after, .markdown blockquote > p::before { display: none; }

.markdown ol { counter-reset: list-number 0; display: flex; flex-direction: column; list-style-type: none; padding-left: 0px; }

.markdown ol > li { counter-increment: list-number 1; display: block; margin-bottom: 0px; margin-top: 0px; min-height: 28px; }

.markdown ol > li::before { --tw-translate-x:-100%; --tw-text-opacity:1; color: rgba(142,142,160,var(--tw-text-opacity)); content: counters(list-number, ".") "."; padding-right: 0.5rem; position: absolute; transform: translate(var(--tw-translate-x),var(--tw-translate-y)) rotate(var(--tw-rotate)) skewX(var(--tw-skew-x)) skewY(var(--tw-skew-y)) scaleX(var(--tw-scale-x)) scaleY(var(--tw-scale-y)); }

:is(.dark .markdown ol > li)::before { --tw-text-opacity:1; color: rgba(172,172,190,var(--tw-text-opacity)); }

.markdown ul { display: flex; flex-direction: column; padding-left: 0px; }

.markdown ul > li { display: block; margin: 0px; min-height: 28px; }

.markdown ul > li::before { --tw-translate-x:-100%; content: "â¢"; font-size: 0.875rem; line-height: 1.25rem; padding-right: 0.5rem; position: absolute; transform: translate(var(--tw-translate-x),var(--tw-translate-y)) rotate(var(--tw-rotate)) skewX(var(--tw-skew-x)) skewY(var(--tw-skew-y)) scaleX(var(--tw-scale-x)) scaleY(var(--tw-scale-y)); }

.markdown ol li > p, .markdown ol li > pre, .markdown ul li > p, .markdown ul li > pre { margin: 0px; }

.markdown ol ol, .markdown ol ul, .markdown ul ol, .markdown ul ul { margin-bottom: 0px; margin-left: 1rem; margin-top: 0px; }

.markdown table { --tw-border-spacing-x:0px; --tw-border-spacing-y:0px; border-collapse: separate; border-spacing: var(--tw-border-spacing-x) var(--tw-border-spacing-y); width: 100%; }

.markdown th { background-color: rgba(236, 236, 241, 0.2); border-bottom-width: 1px; border-left-width: 1px; border-top-width: 1px; padding: 0.25rem 0.75rem; }

.markdown th:first-child { border-top-left-radius: 0.375rem; }

.markdown th:last-child { border-right-width: 1px; border-top-right-radius: 0.375rem; }

.markdown td { border-bottom-width: 1px; border-left-width: 1px; padding: 0.25rem 0.75rem; }

.markdown td:last-child { border-right-width: 1px; }

.markdown tbody tr:last-child td:first-child { border-bottom-left-radius: 0.375rem; }

.markdown tbody tr:last-child td:last-child { border-bottom-right-radius: 0.375rem; }

.markdown a { text-decoration-line: underline; text-underline-offset: 2px; }

.conversation-item-time::before { content: attr(data-time); }

button.scroll-convo { display: none; }

@-webkit-keyframes blink { 
  100% { visibility: hidden; }
}

@keyframes blink { 
  100% { visibility: hidden; }
}

.animate-flash { animation: 2s steps(60, start) 0s 1 normal none running flash; }

@-webkit-keyframes flash { 
  0% { background-color: rgba(255, 255, 255, 0.4); }
}

@keyframes flash { 
  0% { background-color: rgba(255, 255, 255, 0.4); }
}

.result-streaming > :not(ol):not(ul):not(pre):last-child::after, .result-streaming > ol:last-child li:last-child::after, .result-streaming > pre:last-child code::after, .result-streaming > ul:last-child li:last-child::after { animation: 1s steps(5, start) 0s infinite normal none running blink; content: "â"; margin-left: 0.25rem; vertical-align: baseline; }

@-webkit-keyframes toast-open { 
  0% { opacity: 0; transform: translateY(-100%); }
  100% { transform: translateY(0px); }
}

@keyframes toast-open { 
  0% { opacity: 0; transform: translateY(-100%); }
  100% { transform: translateY(0px); }
}

@-webkit-keyframes toast-close { 
  0% { opacity: 1; }
  100% { opacity: 0; }
}

@keyframes toast-close { 
  0% { opacity: 1; }
  100% { opacity: 0; }
}

button[data-state="closed"] .toggle-item-button-closed, button[data-state="open"] .toggle-item-button-open { display: none; }

.toast-root { align-items: center; display: flex; flex-direction: column; height: 0px; transition: all 0.24s cubic-bezier(0, 0, 0.2, 1) 0s; }

.toast-root[data-state="entered"], .toast-root[data-state="entering"] { animation: 0.24s cubic-bezier(0.175, 0.885, 0.32, 1.175) 0s 1 normal both running toast-open; }

.toast-root[data-state="exiting"] { animation: 0.12s cubic-bezier(0.4, 0, 1, 1) 0s 1 normal both running toast-close; }

.toast-root .alert-root { box-shadow: rgba(67, 90, 111, 0.3) 0px 0px 1px, rgba(67, 90, 111, 0.3) 0px 5px 8px -4px; flex-shrink: 0; pointer-events: all; }

.interact-bounce { transition-duration: 0.15s; transition-property: transform, -webkit-transform; transition-timing-function: cubic-bezier(0.4, 0, 0.2, 1); }

.interact-bounce:hover { --tw-scale-x:1.05; --tw-scale-y:1.05; }

.interact-bounce:active, .interact-bounce:hover { transform: translate(var(--tw-translate-x),var(--tw-translate-y)) rotate(var(--tw-rotate)) skewX(var(--tw-skew-x)) skewY(var(--tw-skew-y)) scaleX(var(--tw-scale-x)) scaleY(var(--tw-scale-y)); }

.interact-bounce:active { --tw-scale-x:0.9; --tw-scale-y:0.9; }

.dark .dark\:prose-invert { --tw-prose-body:var(--tw-prose-invert-body); --tw-prose-headings:var(--tw-prose-invert-headings); --tw-prose-lead:var(--tw-prose-invert-lead); --tw-prose-links:var(--tw-prose-invert-links); --tw-prose-bold:var(--tw-prose-invert-bold); --tw-prose-counters:var(--tw-prose-invert-counters); --tw-prose-bullets:var(--tw-prose-invert-bullets); --tw-prose-hr:var(--tw-prose-invert-hr); --tw-prose-quotes:var(--tw-prose-invert-quotes); --tw-prose-quote-borders:var(--tw-prose-invert-quote-borders); --tw-prose-captions:var(--tw-prose-invert-captions); --tw-prose-code:var(--tw-prose-invert-code); --tw-prose-pre-code:var(--tw-prose-invert-pre-code); --tw-prose-pre-bg:var(--tw-prose-invert-pre-bg); --tw-prose-th-borders:var(--tw-prose-invert-th-borders); --tw-prose-td-borders:var(--tw-prose-invert-td-borders); }

.placeholder\:text-gray-300::-webkit-input-placeholder { --tw-text-opacity:1; color: rgba(197,197,210,var(--tw-text-opacity)); }

.placeholder\:text-gray-300::placeholder { --tw-text-opacity:1; color: rgba(197,197,210,var(--tw-text-opacity)); }

.after\:content-\[\'\"\'\]::after, .before\:content-\[\'\"\'\]::before { --tw-content:"\""; content: var(--tw-content); }

.last\:mb-0:last-child { margin-bottom: 0px; }

.last\:mb-2:last-child { margin-bottom: 0.5rem; }

.last\:border-0:last-child { border-width: 0px; }

.last\:pr-0:last-child { padding-right: 0px; }

.last-of-type\:border-b-0:last-of-type { border-bottom-width: 0px; }

.checked\:border-blue-600:checked { --tw-border-opacity:1; border-color: rgba(37,99,235,var(--tw-border-opacity)); }

.checked\:bg-blue-600:checked { --tw-bg-opacity:1; background-color: rgba(37,99,235,var(--tw-bg-opacity)); }

.empty\:mt-0:empty { margin-top: 0px; }

.empty\:hidden:empty { display: none; }

.focus-within\:z-10[focus-within] { z-index: 10; }

.focus-within\:z-10:focus-within { z-index: 10; }

.focus-within\:border-indigo-600[focus-within] { --tw-border-opacity:1; border-color: rgba(79,70,229,var(--tw-border-opacity)); }

.focus-within\:border-indigo-600:focus-within { --tw-border-opacity:1; border-color: rgba(79,70,229,var(--tw-border-opacity)); }

.focus-within\:ring-1[focus-within] { --tw-ring-offset-shadow:var(--tw-ring-inset) 0 0 0 var(--tw-ring-offset-width) var(--tw-ring-offset-color); --tw-ring-shadow:var(--tw-ring-inset) 0 0 0 calc(1px + var(--tw-ring-offset-width)) var(--tw-ring-color); box-shadow: var(--tw-ring-offset-shadow),var(--tw-ring-shadow),var(--tw-shadow,0 0 transparent); }

.focus-within\:ring-1:focus-within { --tw-ring-offset-shadow:var(--tw-ring-inset) 0 0 0 var(--tw-ring-offset-width) var(--tw-ring-offset-color); --tw-ring-shadow:var(--tw-ring-inset) 0 0 0 calc(1px + var(--tw-ring-offset-width)) var(--tw-ring-color); box-shadow: var(--tw-ring-offset-shadow),var(--tw-ring-shadow),var(--tw-shadow,0 0 transparent); }

.focus-within\:ring-indigo-600[focus-within] { --tw-ring-opacity:1; --tw-ring-color:rgba(79,70,229,var(--tw-ring-opacity)); }

.focus-within\:ring-indigo-600:focus-within { --tw-ring-opacity:1; --tw-ring-color:rgba(79,70,229,var(--tw-ring-opacity)); }

.hover\:border-black\/10:hover { border-color: rgba(0, 0, 0, 0.1); }

.hover\:border-gray-300:hover { --tw-border-opacity:1; border-color: rgba(197,197,210,var(--tw-border-opacity)); }

.hover\:border-gray-700:hover { --tw-border-opacity:1; border-color: rgba(64,65,79,var(--tw-border-opacity)); }

.hover\:\!bg-gray-50:hover { --tw-bg-opacity:1 !important; background-color: rgba(247,247,248,var(--tw-bg-opacity)) !important; }

.hover\:bg-\[\#2A2B32\]:hover { --tw-bg-opacity:1; background-color: rgba(42,43,50,var(--tw-bg-opacity)); }

.hover\:bg-black:hover { --tw-bg-opacity:1; background-color: rgba(0,0,0,var(--tw-bg-opacity)); }

.hover\:bg-black\/10:hover { background-color: rgba(0, 0, 0, 0.1); }

.hover\:bg-gray-100:hover { --tw-bg-opacity:1; background-color: rgba(236,236,241,var(--tw-bg-opacity)); }

.hover\:bg-gray-200:hover { --tw-bg-opacity:1; background-color: rgba(217,217,227,var(--tw-bg-opacity)); }

.hover\:bg-gray-300:hover { --tw-bg-opacity:1; background-color: rgba(197,197,210,var(--tw-bg-opacity)); }

.hover\:bg-gray-50:hover { --tw-bg-opacity:1; background-color: rgba(247,247,248,var(--tw-bg-opacity)); }

.hover\:bg-gray-50\/50:hover { background-color: rgba(247, 247, 248, 0.5); }

.hover\:bg-gray-500\/10:hover { background-color: rgba(141, 141, 160, 0.1); }

.hover\:bg-gray-700:hover { --tw-bg-opacity:1; background-color: rgba(64,65,79,var(--tw-bg-opacity)); }

.hover\:bg-gray-800:hover { --tw-bg-opacity:1; background-color: rgba(52,53,65,var(--tw-bg-opacity)); }

.hover\:bg-green-100:hover { --tw-bg-opacity:1; background-color: rgba(210,244,211,var(--tw-bg-opacity)); }

.hover\:bg-white\/40:hover { background-color: rgba(255, 255, 255, 0.4); }

.hover\:pr-14:hover { padding-right: 3.5rem; }

.hover\:pr-4:hover { padding-right: 1rem; }

.hover\:text-black\/50:hover { color: rgba(0, 0, 0, 0.5); }

.hover\:text-black\/70:hover { color: rgba(0, 0, 0, 0.7); }

.hover\:text-blue-600:hover { --tw-text-opacity:1; color: rgba(37,99,235,var(--tw-text-opacity)); }

.hover\:text-gray-500:hover { --tw-text-opacity:1; color: rgba(142,142,160,var(--tw-text-opacity)); }

.hover\:text-gray-600:hover { --tw-text-opacity:1; color: rgba(86,88,105,var(--tw-text-opacity)); }

.hover\:text-gray-700:hover { --tw-text-opacity:1; color: rgba(64,65,79,var(--tw-text-opacity)); }

.hover\:text-gray-800:hover { --tw-text-opacity:1; color: rgba(52,53,65,var(--tw-text-opacity)); }

.hover\:text-gray-900:hover { --tw-text-opacity:1; color: rgba(32,33,35,var(--tw-text-opacity)); }

.hover\:text-white:hover { --tw-text-opacity:1; color: rgba(255,255,255,var(--tw-text-opacity)); }

.hover\:text-white\/40:hover { color: rgba(255, 255, 255, 0.4); }

.hover\:\!opacity-100:hover { opacity: 1 !important; }

.hover\:opacity-100:hover { opacity: 1; }

.hover\:opacity-70:hover { opacity: 0.7; }

.hover\:opacity-75:hover { opacity: 0.75; }

.hover\:opacity-80:hover { opacity: 0.8; }

.focus\:z-10:focus { z-index: 10; }

.focus\:border-brand-green:focus, .focus\:border-green-500:focus { --tw-border-opacity:1; border-color: rgba(25,195,125,var(--tw-border-opacity)); }

.focus\:border-green-600:focus { --tw-border-opacity:1; border-color: rgba(16,163,127,var(--tw-border-opacity)); }

.focus\:border-indigo-500:focus { --tw-border-opacity:1; border-color: rgba(99,102,241,var(--tw-border-opacity)); }

.focus\:border-orange-400:focus { --tw-border-opacity:1; border-color: rgba(255,128,82,var(--tw-border-opacity)); }

.focus\:border-red-500:focus { --tw-border-opacity:1; border-color: rgba(239,68,68,var(--tw-border-opacity)); }

.focus\:bg-gray-50:focus { --tw-bg-opacity:1; background-color: rgba(247,247,248,var(--tw-bg-opacity)); }

.focus\:outline-none:focus { outline: transparent solid 2px; outline-offset: 2px; }

.focus\:ring-0:focus { --tw-ring-offset-shadow:var(--tw-ring-inset) 0 0 0 var(--tw-ring-offset-width) var(--tw-ring-offset-color); --tw-ring-shadow:var(--tw-ring-inset) 0 0 0 calc(var(--tw-ring-offset-width)) var(--tw-ring-color); }

.focus\:ring-0:focus, .focus\:ring-1:focus { box-shadow: var(--tw-ring-offset-shadow),var(--tw-ring-shadow),var(--tw-shadow,0 0 transparent); }

.focus\:ring-1:focus { --tw-ring-offset-shadow:var(--tw-ring-inset) 0 0 0 var(--tw-ring-offset-width) var(--tw-ring-offset-color); --tw-ring-shadow:var(--tw-ring-inset) 0 0 0 calc(1px + var(--tw-ring-offset-width)) var(--tw-ring-color); }

.focus\:ring-2:focus { --tw-ring-offset-shadow:var(--tw-ring-inset) 0 0 0 var(--tw-ring-offset-width) var(--tw-ring-offset-color); --tw-ring-shadow:var(--tw-ring-inset) 0 0 0 calc(2px + var(--tw-ring-offset-width)) var(--tw-ring-color); box-shadow: var(--tw-ring-offset-shadow),var(--tw-ring-shadow),var(--tw-shadow,0 0 transparent); }

.focus\:ring-inset:focus { --tw-ring-inset:inset; }

.focus\:ring-gray-200:focus { --tw-ring-opacity:1; --tw-ring-color:rgba(217,217,227,var(--tw-ring-opacity)); }

.focus\:ring-gray-500:focus { --tw-ring-opacity:1; --tw-ring-color:rgba(142,142,160,var(--tw-ring-opacity)); }

.focus\:ring-green-500:focus { --tw-ring-opacity:1; --tw-ring-color:rgba(25,195,125,var(--tw-ring-opacity)); }

.focus\:ring-green-600:focus { --tw-ring-opacity:1; --tw-ring-color:rgba(16,163,127,var(--tw-ring-opacity)); }

.focus\:ring-indigo-500:focus { --tw-ring-opacity:1; --tw-ring-color:rgba(99,102,241,var(--tw-ring-opacity)); }

.focus\:ring-white:focus { --tw-ring-opacity:1; --tw-ring-color:rgba(255,255,255,var(--tw-ring-opacity)); }

.focus\:ring-offset-2:focus { --tw-ring-offset-width:2px; }

.focus-visible\:border-blue-500.focus-visible { --tw-border-opacity:1; border-color: rgba(69,89,164,var(--tw-border-opacity)); }

.focus-visible\:border-blue-500:focus-visible { --tw-border-opacity:1; border-color: rgba(69,89,164,var(--tw-border-opacity)); }

.focus-visible\:ring-0.focus-visible { --tw-ring-offset-shadow:var(--tw-ring-inset) 0 0 0 var(--tw-ring-offset-width) var(--tw-ring-offset-color); --tw-ring-shadow:var(--tw-ring-inset) 0 0 0 calc(var(--tw-ring-offset-width)) var(--tw-ring-color); box-shadow: var(--tw-ring-offset-shadow),var(--tw-ring-shadow),var(--tw-shadow,0 0 transparent); }

.focus-visible\:ring-0:focus-visible { --tw-ring-offset-shadow:var(--tw-ring-inset) 0 0 0 var(--tw-ring-offset-width) var(--tw-ring-offset-color); --tw-ring-shadow:var(--tw-ring-inset) 0 0 0 calc(var(--tw-ring-offset-width)) var(--tw-ring-color); box-shadow: var(--tw-ring-offset-shadow),var(--tw-ring-shadow),var(--tw-shadow,0 0 transparent); }

.enabled\:bg-brand-purple:enabled { --tw-bg-opacity:1; background-color: rgba(171,104,255,var(--tw-bg-opacity)); }

.enabled\:hover\:text-gray-700:hover:enabled { --tw-text-opacity:1; color: rgba(64,65,79,var(--tw-text-opacity)); }

.disabled\:bottom-0:disabled { bottom: 0px; }

.disabled\:bottom-0\.5:disabled { bottom: 0.125rem; }

.disabled\:cursor-auto:disabled { cursor: auto; }

.disabled\:text-gray-300:disabled { --tw-text-opacity:1; color: rgba(197,197,210,var(--tw-text-opacity)); }

.disabled\:text-gray-400:disabled { --tw-text-opacity:1; color: rgba(172,172,190,var(--tw-text-opacity)); }

.disabled\:opacity-20:disabled { opacity: 0.2; }

.disabled\:opacity-40:disabled { opacity: 0.4; }

.disabled\:hover\:bg-gray-900:hover:disabled { --tw-bg-opacity:1; background-color: rgba(32,33,35,var(--tw-bg-opacity)); }

.group:hover .group-hover\:visible { visibility: visible; }

.group:hover .group-hover\:block { display: block; }

.group:hover .group-hover\:hidden { display: none; }

.group\/options:hover .group-hover\/options\:bg-transparent { background-color: transparent; }

.group\/toggle:hover .group-hover\/toggle\:bg-red-500 { --tw-bg-opacity:1; background-color: rgba(239,68,68,var(--tw-bg-opacity)); }

.group:hover .group-hover\:bg-black { --tw-bg-opacity:1; background-color: rgba(0,0,0,var(--tw-bg-opacity)); }

.group:hover .group-hover\:bg-gray-100 { --tw-bg-opacity:1; background-color: rgba(236,236,241,var(--tw-bg-opacity)); }

.group:hover .group-hover\:from-\[\#2A2B32\] { --tw-gradient-from:#2a2b32 var(--tw-gradient-from-position); --tw-gradient-to:rgba(42,43,50,0) var(--tw-gradient-to-position); --tw-gradient-stops:var(--tw-gradient-from),var(--tw-gradient-to); }

.group\/button:hover .group-hover\/button\:text-brand-green { --tw-text-opacity:1; color: rgba(25,195,125,var(--tw-text-opacity)); }

.group\/button:hover .group-hover\/button\:text-brand-purple { --tw-text-opacity:1; color: rgba(171,104,255,var(--tw-text-opacity)); }

.group\/button:hover .group-hover\/button\:text-orange-500 { --tw-text-opacity:1; color: rgba(224,108,43,var(--tw-text-opacity)); }

.group\/button:hover .group-hover\/button\:text-red-500 { --tw-text-opacity:1; color: rgba(239,68,68,var(--tw-text-opacity)); }

.group\/option:hover .group-hover\/option\:\!text-brand-green { --tw-text-opacity:1 !important; color: rgba(25,195,125,var(--tw-text-opacity)) !important; }

.group\/option:hover .group-hover\/option\:\!text-brand-purple { --tw-text-opacity:1 !important; color: rgba(171,104,255,var(--tw-text-opacity)) !important; }

.group\/option:hover .group-hover\/option\:text-gray-100 { --tw-text-opacity:1; color: rgba(236,236,241,var(--tw-text-opacity)); }

.group\/option:hover .group-hover\/option\:text-gray-800 { --tw-text-opacity:1; color: rgba(52,53,65,var(--tw-text-opacity)); }

.group:hover .group-hover\:text-gray-500, .group\/options:hover .group-hover\/options\:text-gray-500 { --tw-text-opacity:1; color: rgba(142,142,160,var(--tw-text-opacity)); }

.group\/options:hover .group-hover\/options\:opacity-0 { opacity: 0; }

.group:hover .group-hover\:opacity-100 { opacity: 1; }

.group:hover .group-hover\:opacity-50 { opacity: 0.5; }

.group-ui-open\:bg-gray-800.group[data-headlessui-state~="open"] { --tw-bg-opacity:1; background-color: rgba(52,53,65,var(--tw-bg-opacity)); }

.group:where([data-headlessui-state~="open"]) .group-ui-open\:bg-gray-800 { --tw-bg-opacity:1; background-color: rgba(52,53,65,var(--tw-bg-opacity)); }

.radix-disabled\:pointer-events-none[data-disabled] { pointer-events: none; }

.radix-disabled\:cursor-auto[data-disabled] { cursor: auto; }

.radix-disabled\:bg-transparent[data-disabled] { background-color: transparent; }

.radix-disabled\:text-gray-300[data-disabled] { --tw-text-opacity:1; color: rgba(197,197,210,var(--tw-text-opacity)); }

.radix-disabled\:text-gray-500[data-disabled] { --tw-text-opacity:1; color: rgba(142,142,160,var(--tw-text-opacity)); }

.radix-highlighted\:bg-gray-50[data-highlighted] { --tw-bg-opacity:1; background-color: rgba(247,247,248,var(--tw-bg-opacity)); }

.radix-highlighted\:outline-none[data-highlighted] { outline: transparent solid 2px; outline-offset: 2px; }

.radix-state-active\:bg-gray-800[data-state="active"] { --tw-bg-opacity:1; background-color: rgba(52,53,65,var(--tw-bg-opacity)); }

.radix-state-active\:text-gray-600[data-state="active"] { --tw-text-opacity:1; color: rgba(86,88,105,var(--tw-text-opacity)); }

.radix-state-active\:text-white[data-state="active"] { --tw-text-opacity:1; color: rgba(255,255,255,var(--tw-text-opacity)); }

.group[data-state="active"] .group-radix-state-active\:fill-white { fill: rgb(255, 255, 255); }

.radix-state-checked\:translate-x-\[19px\][data-state="checked"] { --tw-translate-x:19px; transform: translate(var(--tw-translate-x),var(--tw-translate-y)) rotate(var(--tw-rotate)) skewX(var(--tw-skew-x)) skewY(var(--tw-skew-y)) scaleX(var(--tw-scale-x)) scaleY(var(--tw-scale-y)); }

.radix-state-checked\:bg-green-600[data-state="checked"] { --tw-bg-opacity:1; background-color: rgba(16,163,127,var(--tw-bg-opacity)); }

@-webkit-keyframes hide { 
  0% { opacity: 1; }
  100% { opacity: 0; }
}

@keyframes hide { 
  0% { opacity: 1; }
  100% { opacity: 0; }
}

.radix-state-closed\:animate-hide[data-state="closed"] { animation: 0.1s ease-in 0s 1 normal none running hide; }

@-webkit-keyframes slideIn { 
  0% { transform: translateX(calc(100% + var(--viewport-padding))); }
  100% { transform: translateX(0px); }
}

@keyframes slideIn { 
  0% { transform: translateX(calc(100% + var(--viewport-padding))); }
  100% { transform: translateX(0px); }
}

.radix-state-open\:animate-slideIn[data-state="open"] { animation: 0.15s cubic-bezier(0.16, 1, 0.3, 1) 0s 1 normal none running slideIn; }

.radix-state-open\:text-gray-600[data-state="open"] { --tw-text-opacity:1; color: rgba(86,88,105,var(--tw-text-opacity)); }

.radix-side-bottom\:animate-slideDownAndFade[data-side="bottom"] { animation: 0.4s cubic-bezier(0.16, 1, 0.3, 1) 0s 1 normal none running slideDownAndFade; }

.radix-side-bottom\:animate-slideUpAndFade[data-side="bottom"] { animation: 0.4s cubic-bezier(0.16, 1, 0.3, 1) 0s 1 normal none running slideUpAndFade; }

.radix-side-left\:animate-slideLeftAndFade[data-side="left"] { animation: 0.4s cubic-bezier(0.16, 1, 0.3, 1) 0s 1 normal none running slideLeftAndFade; }

@-webkit-keyframes slideRightAndFade { 
  0% { opacity: 0; transform: translateX(-2px); }
  100% { opacity: 1; transform: translateX(0px); }
}

.radix-side-left\:animate-slideRightAndFade[data-side="left"] { animation: 0.4s cubic-bezier(0.16, 1, 0.3, 1) 0s 1 normal none running slideRightAndFade; }

@keyframes slideLeftAndFade { 
  0% { opacity: 0; transform: translateX(2px); }
  100% { opacity: 1; transform: translateX(0px); }
}

.radix-side-right\:animate-slideLeftAndFade[data-side="right"] { animation: 0.4s cubic-bezier(0.16, 1, 0.3, 1) 0s 1 normal none running slideLeftAndFade; }

@keyframes slideRightAndFade { 
  0% { opacity: 0; transform: translateX(-2px); }
  100% { opacity: 1; transform: translateX(0px); }
}

.radix-side-right\:animate-slideRightAndFade[data-side="right"] { animation: 0.4s cubic-bezier(0.16, 1, 0.3, 1) 0s 1 normal none running slideRightAndFade; }

@keyframes slideDownAndFade { 
  0% { opacity: 0; transform: translateY(-2px); }
  100% { opacity: 1; transform: translateY(0px); }
}

.radix-side-top\:animate-slideDownAndFade[data-side="top"] { animation: 0.4s cubic-bezier(0.16, 1, 0.3, 1) 0s 1 normal none running slideDownAndFade; }

@keyframes slideUpAndFade { 
  0% { opacity: 0; transform: translateY(2px); }
  100% { opacity: 1; transform: translateY(0px); }
}

.radix-side-top\:animate-slideUpAndFade[data-side="top"] { animation: 0.4s cubic-bezier(0.16, 1, 0.3, 1) 0s 1 normal none running slideUpAndFade; }

.dark .dark\:border-\[\#4E4F60\] { --tw-border-opacity:1; border-color: rgba(78,79,96,var(--tw-border-opacity)); }

.dark .dark\:border-black\/20 { border-color: rgba(0, 0, 0, 0.2); }

.dark .dark\:border-gray-300 { --tw-border-opacity:1; border-color: rgba(197,197,210,var(--tw-border-opacity)); }

.dark .dark\:border-gray-600 { --tw-border-opacity:1; border-color: rgba(86,88,105,var(--tw-border-opacity)); }

.dark .dark\:border-gray-700 { --tw-border-opacity:1; border-color: rgba(64,65,79,var(--tw-border-opacity)); }

.dark .dark\:border-gray-800 { --tw-border-opacity:1; border-color: rgba(52,53,65,var(--tw-border-opacity)); }

.dark .dark\:border-gray-900\/50 { border-color: rgba(32, 33, 35, 0.5); }

.dark .dark\:border-white\/10 { border-color: rgba(255, 255, 255, 0.1); }

.dark .dark\:border-white\/20 { border-color: rgba(255, 255, 255, 0.2); }

.dark .dark\:border-white\/30 { border-color: rgba(255, 255, 255, 0.298); }

.dark .dark\:bg-\[\#272832\] { --tw-bg-opacity:1; background-color: rgba(39,40,50,var(--tw-bg-opacity)); }

.dark .dark\:bg-\[\#444654\] { --tw-bg-opacity:1; background-color: rgba(68,70,84,var(--tw-bg-opacity)); }

.dark .dark\:bg-gray-50 { --tw-bg-opacity:1; background-color: rgba(247,247,248,var(--tw-bg-opacity)); }

.dark .dark\:bg-gray-500 { --tw-bg-opacity:1; background-color: rgba(142,142,160,var(--tw-bg-opacity)); }

.dark .dark\:bg-gray-700 { --tw-bg-opacity:1; background-color: rgba(64,65,79,var(--tw-bg-opacity)); }

.dark .dark\:bg-gray-800 { --tw-bg-opacity:1; background-color: rgba(52,53,65,var(--tw-bg-opacity)); }

.dark .dark\:bg-gray-800\/30 { background-color: rgba(52, 53, 65, 0.3); }

.dark .dark\:bg-gray-800\/90 { background-color: rgba(52, 53, 65, 0.9); }

.dark .dark\:bg-gray-900 { --tw-bg-opacity:1; background-color: rgba(32,33,35,var(--tw-bg-opacity)); }

.dark .dark\:bg-gray-950 { --tw-bg-opacity:1; background-color: rgba(5,5,9,var(--tw-bg-opacity)); }

.dark .dark\:bg-red-200 { --tw-bg-opacity:1; background-color: rgba(254,202,202,var(--tw-bg-opacity)); }

.dark .dark\:bg-transparent { background-color: transparent; }

.dark .dark\:bg-white { --tw-bg-opacity:1; background-color: rgba(255,255,255,var(--tw-bg-opacity)); }

.dark .dark\:bg-white\/10 { background-color: rgba(255, 255, 255, 0.1); }

.dark .dark\:bg-white\/5 { background-color: rgba(255, 255, 255, 0.05); }

.dark .dark\:from-gray-700 { --tw-gradient-from:#40414f var(--tw-gradient-from-position); --tw-gradient-to:rgba(64,65,79,0) var(--tw-gradient-to-position); --tw-gradient-stops:var(--tw-gradient-from),var(--tw-gradient-to); }

.dark .dark\:fill-gray-500 { fill: rgb(142, 142, 160); }

.dark .dark\:text-blue-400 { --tw-text-opacity:1; color: rgba(58,136,255,var(--tw-text-opacity)); }

.dark .dark\:text-blue-600 { --tw-text-opacity:1; color: rgba(37,99,235,var(--tw-text-opacity)); }

.dark .dark\:text-gray-100 { --tw-text-opacity:1; color: rgba(236,236,241,var(--tw-text-opacity)); }

.dark .dark\:text-gray-200 { --tw-text-opacity:1; color: rgba(217,217,227,var(--tw-text-opacity)); }

.dark .dark\:text-gray-300 { --tw-text-opacity:1; color: rgba(197,197,210,var(--tw-text-opacity)); }

.dark .dark\:text-gray-400 { --tw-text-opacity:1; color: rgba(172,172,190,var(--tw-text-opacity)); }

.dark .dark\:text-gray-500 { --tw-text-opacity:1; color: rgba(142,142,160,var(--tw-text-opacity)); }

.dark .dark\:text-gray-600 { --tw-text-opacity:1; color: rgba(86,88,105,var(--tw-text-opacity)); }

.dark .dark\:text-gray-700 { --tw-text-opacity:1; color: rgba(64,65,79,var(--tw-text-opacity)); }

.dark .dark\:text-gray-900 { --tw-text-opacity:1; color: rgba(32,33,35,var(--tw-text-opacity)); }

.dark .dark\:text-red-500 { --tw-text-opacity:1; color: rgba(239,68,68,var(--tw-text-opacity)); }

.dark .dark\:text-red-800 { --tw-text-opacity:1; color: rgba(153,27,27,var(--tw-text-opacity)); }

.dark .dark\:text-white { --tw-text-opacity:1; color: rgba(255,255,255,var(--tw-text-opacity)); }

.dark .dark\:text-white\/70 { color: rgba(255, 255, 255, 0.698); }

.dark .dark\:opacity-100 { opacity: 1; }

.dark .dark\:shadow-xs { --tw-shadow:0 0 15px rgba(0,0,0,0.1); --tw-shadow-colored:0 0 15px var(--tw-shadow-color); box-shadow: var(--tw-ring-offset-shadow,0 0 transparent),var(--tw-ring-shadow,0 0 transparent),var(--tw-shadow); }

.dark .dark\:ring-white\/20 { --tw-ring-color:hsla(0,0%,100%,0.2); }

.dark .dark\:last\:border-0:last-child { border-width: 0px; }

.dark .dark\:hover\:border-gray-600:hover { --tw-border-opacity:1; border-color: rgba(86,88,105,var(--tw-border-opacity)); }

.dark .dark\:hover\:border-white\/10:hover { border-color: rgba(255, 255, 255, 0.1); }

.dark .dark\:hover\:\!bg-gray-700:hover { --tw-bg-opacity:1 !important; background-color: rgba(64,65,79,var(--tw-bg-opacity)) !important; }

.dark .dark\:hover\:bg-\[\#2E2F33\]:hover { --tw-bg-opacity:1; background-color: rgba(46,47,51,var(--tw-bg-opacity)); }

.dark .dark\:hover\:bg-\[\#494A54\]:hover { --tw-bg-opacity:1; background-color: rgba(73,74,84,var(--tw-bg-opacity)); }

.dark .dark\:hover\:bg-gray-200:hover { --tw-bg-opacity:1; background-color: rgba(217,217,227,var(--tw-bg-opacity)); }

.dark .dark\:hover\:bg-gray-500\/10:hover { background-color: rgba(141, 141, 160, 0.1); }

.dark .dark\:hover\:bg-gray-600:hover { --tw-bg-opacity:1; background-color: rgba(86,88,105,var(--tw-bg-opacity)); }

.dark .dark\:hover\:bg-gray-700:hover { --tw-bg-opacity:1; background-color: rgba(64,65,79,var(--tw-bg-opacity)); }

.dark .dark\:hover\:bg-gray-700\/50:hover { background-color: rgba(64, 65, 79, 0.5); }

.dark .dark\:hover\:bg-gray-800:hover { --tw-bg-opacity:1; background-color: rgba(52,53,65,var(--tw-bg-opacity)); }

.dark .dark\:hover\:bg-gray-900:hover { --tw-bg-opacity:1; background-color: rgba(32,33,35,var(--tw-bg-opacity)); }

:is(.dark .hover\:dark\:bg-white\/10):hover { background-color: rgba(255, 255, 255, 0.1); }

.dark .dark\:hover\:text-blue-600:hover { --tw-text-opacity:1; color: rgba(37,99,235,var(--tw-text-opacity)); }

.dark .dark\:hover\:text-gray-100:hover { --tw-text-opacity:1; color: rgba(236,236,241,var(--tw-text-opacity)); }

.dark .dark\:hover\:text-gray-200:hover { --tw-text-opacity:1; color: rgba(217,217,227,var(--tw-text-opacity)); }

.dark .dark\:hover\:text-gray-400:hover { --tw-text-opacity:1; color: rgba(172,172,190,var(--tw-text-opacity)); }

.dark .dark\:hover\:text-white:hover { --tw-text-opacity:1; color: rgba(255,255,255,var(--tw-text-opacity)); }

.dark .dark\:hover\:text-white\/50:hover { color: rgba(255, 255, 255, 0.5); }

:is(.dark .hover\:dark\:text-gray-100):hover { --tw-text-opacity:1; color: rgba(236,236,241,var(--tw-text-opacity)); }

.dark .dark\:focus\:border-white:focus { --tw-border-opacity:1; border-color: rgba(255,255,255,var(--tw-border-opacity)); }

.dark .dark\:focus\:bg-gray-800:focus { --tw-bg-opacity:1; background-color: rgba(52,53,65,var(--tw-bg-opacity)); }

:is(.dark .focus\:dark\:bg-gray-700):focus { --tw-bg-opacity:1; background-color: rgba(64,65,79,var(--tw-bg-opacity)); }

.dark .dark\:focus\:ring-gray-600:focus { --tw-ring-opacity:1; --tw-ring-color:rgba(86,88,105,var(--tw-ring-opacity)); }

.dark .dark\:focus\:ring-white:focus { --tw-ring-opacity:1; --tw-ring-color:rgba(255,255,255,var(--tw-ring-opacity)); }

.dark .dark\:focus-visible\:border-blue-500.focus-visible { --tw-border-opacity:1; border-color: rgba(69,89,164,var(--tw-border-opacity)); }

.dark .dark\:focus-visible\:border-blue-500:focus-visible { --tw-border-opacity:1; border-color: rgba(69,89,164,var(--tw-border-opacity)); }

:is(.dark .enabled\:dark\:hover\:text-gray-400:hover):enabled { --tw-text-opacity:1; color: rgba(172,172,190,var(--tw-text-opacity)); }

.dark .dark\:disabled\:text-gray-400:disabled { --tw-text-opacity:1; color: rgba(172,172,190,var(--tw-text-opacity)); }

.dark .dark\:disabled\:hover\:bg-transparent:hover:disabled { background-color: transparent; }

.dark .dark\:disabled\:hover\:bg-white:hover:disabled { --tw-bg-opacity:1; background-color: rgba(255,255,255,var(--tw-bg-opacity)); }

:is(.dark .disabled\:dark\:hover\:text-gray-400:hover):disabled { --tw-text-opacity:1; color: rgba(172,172,190,var(--tw-text-opacity)); }

.dark .group:hover .dark\:group-hover\:bg-gray-700 { --tw-bg-opacity:1; background-color: rgba(64,65,79,var(--tw-bg-opacity)); }

.dark .group\/option:hover .dark\:group-hover\/option\:text-gray-100 { --tw-text-opacity:1; color: rgba(236,236,241,var(--tw-text-opacity)); }

.dark .dark\:radix-disabled\:bg-transparent[data-disabled] { background-color: transparent; }

.dark .dark\:radix-disabled\:text-gray-500[data-disabled] { --tw-text-opacity:1; color: rgba(142,142,160,var(--tw-text-opacity)); }

.dark .dark\:radix-disabled\:text-gray-600[data-disabled] { --tw-text-opacity:1; color: rgba(86,88,105,var(--tw-text-opacity)); }

.dark .dark\:radix-highlighted\:bg-\[\#2E2F33\][data-highlighted] { --tw-bg-opacity:1; background-color: rgba(46,47,51,var(--tw-bg-opacity)); }

.dark .dark\:radix-highlighted\:bg-gray-700[data-highlighted] { --tw-bg-opacity:1; background-color: rgba(64,65,79,var(--tw-bg-opacity)); }

.dark .dark\:radix-highlighted\:hover\:bg-\[\#2E2F33\][data-highlighted]:hover { --tw-bg-opacity:1; background-color: rgba(46,47,51,var(--tw-bg-opacity)); }

.dark .dark\:radix-state-active\:text-white[data-state="active"] { --tw-text-opacity:1; color: rgba(255,255,255,var(--tw-text-opacity)); }

.dark .dark\:radix-state-open\:text-gray-400[data-state="open"] { --tw-text-opacity:1; color: rgba(172,172,190,var(--tw-text-opacity)); }

@media not all and (min-width: 1024px) {
  .max-lg\:justify-center { justify-content: center; }
}

@media (max-width: 370px) {
  .max-\[370px\]\:hidden { display: none; }
}

@media (min-width: 640px) {
  .sm\:order-1 { order: 1; }
  .sm\:order-2 { order: 2; }
  .sm\:mx-1 { margin-left: 0.25rem; margin-right: 0.25rem; }
  .sm\:mb-16 { margin-bottom: 4rem; }
  .sm\:mb-2 { margin-bottom: 0.5rem; }
  .sm\:mt-0 { margin-top: 0px; }
  .sm\:mt-16 { margin-top: 4rem; }
  .sm\:mt-4 { margin-top: 1rem; }
  .sm\:mt-6 { margin-top: 1.5rem; }
  .sm\:mt-\[20vh\] { margin-top: 20vh; }
  .sm\:block { display: block; }
  .sm\:h-10 { height: 2.5rem; }
  .sm\:w-10 { width: 2.5rem; }
  .sm\:w-\[312px\] { width: 312px; }
  .sm\:w-\[calc\(\(100\%-12px\)\/2\)\] { width: calc(50% - 6px); }
  .sm\:w-auto { width: auto; }
  .sm\:min-w-\[148px\] { min-width: 148px; }
  .sm\:min-w-\[312px\] { min-width: 312px; }
  .sm\:min-w-\[420px\] { min-width: 420px; }
  .sm\:max-w-md { max-width: 28rem; }
  .sm\:grid-cols-2 { grid-template-columns: repeat(2, minmax(0px, 1fr)); }
  .sm\:grid-rows-2 { grid-template-rows: repeat(2, minmax(0px, 1fr)); }
  .sm\:flex-row { flex-direction: row; }
  .sm\:flex-row-reverse { flex-direction: row-reverse; }
  .sm\:items-center { align-items: center; }
  .sm\:gap-3 { gap: 0.75rem; }
  .sm\:rounded-2xl { border-radius: 1rem; }
  .sm\:border-r { border-right-width: 1px; }
  .sm\:border-t-0 { border-top-width: 0px; }
  .sm\:p-4 { padding: 1rem; }
  .sm\:p-6 { padding: 1.5rem; }
  .sm\:px-0 { padding-left: 0px; padding-right: 0px; }
  .sm\:px-14 { padding-left: 3.5rem; padding-right: 3.5rem; }
  .sm\:px-4 { padding-left: 1rem; padding-right: 1rem; }
  .sm\:px-6 { padding-left: 1.5rem; padding-right: 1.5rem; }
  .sm\:pb-1 { padding-bottom: 0.25rem; }
  .sm\:pb-2 { padding-bottom: 0.5rem; }
  .sm\:pb-6 { padding-bottom: 1.5rem; }
  .sm\:pl-3 { padding-left: 0.75rem; }
  .sm\:pt-4 { padding-top: 1rem; }
  .sm\:pt-8 { padding-top: 2rem; }
  .sm\:text-left { text-align: left; }
  .sm\:text-4xl { font-size: 2.25rem; line-height: 2.5rem; }
  .sm\:text-base { font-size: 1rem; line-height: 1.5rem; }
  .sm\:text-sm { font-size: 0.875rem; line-height: 1.25rem; }
  .sm\:duration-300 { transition-duration: 0.3s; }
  .sm\:duration-500 { transition-duration: 0.5s; }
}

@media (min-width: 768px) {
  .md\:invisible { visibility: hidden; }
  .md\:static { position: static; }
  .md\:bottom-3 { bottom: 0.75rem; }
  .md\:bottom-3\.5 { bottom: 0.875rem; }
  .md\:bottom-\[120px\] { bottom: 120px; }
  .md\:left-4 { left: 1rem; }
  .md\:right-3 { right: 0.75rem; }
  .md\:m-0 { margin: 0px; }
  .md\:m-auto { margin: auto; }
  .md\:mx-0 { margin-left: 0px; margin-right: 0px; }
  .md\:mx-4 { margin-left: 1rem; margin-right: 1rem; }
  .md\:mb-0 { margin-bottom: 0px; }
  .md\:mb-2 { margin-bottom: 0.5rem; }
  .md\:mb-auto { margin-bottom: auto; }
  .md\:inline-block { display: inline-block; }
  .md\:flex { display: flex; }
  .md\:hidden { display: none; }
  .md\:h-48 { height: 12rem; }
  .md\:h-full { height: 100%; }
  .md\:min-h-\[300px\] { min-height: 300px; }
  .md\:w-1\/2 { width: 50%; }
  .md\:w-\[100\%\] { width: 100%; }
  .md\:w-\[250px\] { width: 250px; }
  .md\:w-\[672px\] { width: 672px; }
  .md\:w-\[calc\(100\%-\.5rem\)\] { width: calc(100% - 0.5rem); }
  .md\:w-auto { width: auto; }
  .md\:w-full { width: 100%; }
  .md\:min-w-\[672px\] { min-width: 672px; }
  .md\:max-w-2xl { max-width: 42rem; }
  .md\:max-w-3xl { max-width: 48rem; }
  .md\:max-w-\[672px\] { max-width: 672px; }
  .md\:max-w-\[680px\] { max-width: 680px; }
  .md\:max-w-none { max-width: none; }
  .md\:grid-rows-\[minmax\(20px\,_1fr\)_auto_minmax\(20px\,_1fr\)\] { grid-template-rows: minmax(20px, 1fr) auto minmax(20px, 1fr); }
  .md\:flex-row { flex-direction: row; }
  .md\:flex-col { flex-direction: column; }
  .md\:justify-between { justify-content: space-between; }
  .md\:gap-2 { gap: 0.5rem; }
  .md\:gap-3 { gap: 0.75rem; }
  .md\:gap-6 { gap: 1.5rem; }
  .md\:overflow-visible { overflow: visible; }
  .md\:whitespace-normal { white-space: normal; }
  .md\:border { border-width: 1px; }
  .md\:border-l { border-left-width: 1px; }
  .md\:border-t-0 { border-top-width: 0px; }
  .md\:border-gray-100 { --tw-border-opacity:1; border-color: rgba(236,236,241,var(--tw-border-opacity)); }
  .md\:border-transparent { border-color: transparent; }
  .md\:\!bg-transparent { background-color: transparent !important; }
  .md\:bg-vert-light-gradient { background-image: linear-gradient(rgba(255, 255, 255, 0) 13.94%, rgb(255, 255, 255) 54.73%); }
  .md\:p-2 { padding: 0.5rem; }
  .md\:px-0 { padding-left: 0px; padding-right: 0px; }
  .md\:px-4 { padding-left: 1rem; padding-right: 1rem; }
  .md\:py-2 { padding-bottom: 0.5rem; padding-top: 0.5rem; }
  .md\:py-2\.5 { padding-bottom: 0.625rem; padding-top: 0.625rem; }
  .md\:py-4 { padding-bottom: 1rem; padding-top: 1rem; }
  .md\:py-6 { padding-bottom: 1.5rem; padding-top: 1.5rem; }
  .md\:\!pt-0 { padding-top: 0px !important; }
  .md\:pb-5 { padding-bottom: 1.25rem; }
  .md\:pb-6 { padding-bottom: 1.5rem; }
  .md\:pl-0 { padding-left: 0px; }
  .md\:pl-2 { padding-left: 0.5rem; }
  .md\:pl-4 { padding-left: 1rem; }
  .md\:pl-\[30px\] { padding-left: 30px; }
  .md\:pr-1 { padding-right: 0.25rem; }
  .md\:pr-1\.5 { padding-right: 0.375rem; }
  .md\:pr-12 { padding-right: 3rem; }
  .md\:pt-3 { padding-top: 0.75rem; }
  .md\:text-justify { text-align: justify; }
  .md\:text-sm { font-size: 0.875rem; line-height: 1.25rem; }
  .md\:opacity-0 { opacity: 0; }
  .md\:last\:mb-6:last-child { margin-bottom: 1.5rem; }
  .md\:disabled\:bottom-0:disabled { bottom: 0px; }
  .group:hover .md\:group-hover\:visible { visibility: visible; }
  .dark .md\:dark\:border-gray-700 { --tw-border-opacity:1; border-color: rgba(64,65,79,var(--tw-border-opacity)); }
  .dark .md\:dark\:border-transparent { border-color: transparent; }
  .dark .dark\:md\:bg-vert-dark-gradient { background-image: linear-gradient(rgba(53, 55, 64, 0), rgb(53, 55, 64) 58.85%); }
}

@media (min-width: 1024px) {
  .lg\:absolute { position: absolute; }
  .lg\:relative { position: relative; }
  .lg\:-left-3 { left: -0.75rem; }
  .lg\:-right-3 { right: -0.75rem; }
  .lg\:left-auto { left: auto; }
  .lg\:right-0 { right: 0px; }
  .lg\:top-0 { top: 0px; }
  .lg\:top-1\/2 { top: 50%; }
  .lg\:mx-auto { margin-left: auto; margin-right: auto; }
  .lg\:mb-0 { margin-bottom: 0px; }
  .lg\:mt-0 { margin-top: 0px; }
  .lg\:block { display: block; }
  .lg\:hidden { display: none; }
  .lg\:w-1\/3 { width: 33.3333%; }
  .lg\:w-3\/5 { width: 60%; }
  .lg\:w-\[300px\] { width: 300px; }
  .lg\:w-\[896px\] { width: 896px; }
  .lg\:w-\[calc\(\(100\%-24px\)\/3\)\] { width: calc(33.3333% - 8px); }
  .lg\:w-\[calc\(100\%-115px\)\] { width: calc(100% - 115px); }
  .lg\:w-auto { width: auto; }
  .lg\:min-w-\[896px\] { min-width: 896px; }
  .lg\:max-w-2xl { max-width: 42rem; }
  .lg\:max-w-3xl { max-width: 48rem; }
  .lg\:max-w-\[38rem\] { max-width: 38rem; }
  .lg\:max-w-\[896px\] { max-width: 896px; }
  .lg\:max-w-none { max-width: none; }
  .lg\:max-w-sm { max-width: 24rem; }
  .lg\:max-w-xl { max-width: 36rem; }
  .lg\:-translate-x-full { --tw-translate-x:-100%; }
  .lg\:-translate-x-full, .lg\:-translate-y-1\/2 { transform: translate(var(--tw-translate-x),var(--tw-translate-y)) rotate(var(--tw-rotate)) skewX(var(--tw-skew-x)) skewY(var(--tw-skew-y)) scaleX(var(--tw-scale-x)) scaleY(var(--tw-scale-y)); }
  .lg\:-translate-y-1\/2 { --tw-translate-y:-50%; }
  .lg\:translate-x-full { --tw-translate-x:100%; }
  .lg\:translate-x-full, .lg\:translate-y-0 { transform: translate(var(--tw-translate-x),var(--tw-translate-y)) rotate(var(--tw-rotate)) skewX(var(--tw-skew-x)) skewY(var(--tw-skew-y)) scaleX(var(--tw-scale-x)) scaleY(var(--tw-scale-y)); }
  .lg\:translate-y-0 { --tw-translate-y:0px; }
  .lg\:grid-cols-3 { grid-template-columns: repeat(3, minmax(0px, 1fr)); }
  .lg\:gap-1 { gap: 0.25rem; }
  .lg\:self-center { align-self: center; }
  .lg\:rounded-lg { border-radius: 0.5rem; }
  .lg\:rounded-none { border-radius: 0px; }
  .lg\:border { border-width: 1px; }
  .lg\:border-0 { border-width: 0px; }
  .lg\:p-0 { padding: 0px; }
  .lg\:p-6 { padding: 1.5rem; }
  .lg\:px-0 { padding-left: 0px; padding-right: 0px; }
  .lg\:px-14 { padding-left: 3.5rem; padding-right: 3.5rem; }
  .lg\:px-8 { padding-left: 2rem; padding-right: 2rem; }
  .lg\:pl-2 { padding-left: 0.5rem; }
  .lg\:pt-32 { padding-top: 8rem; }
  .lg\:text-left { text-align: left; }
}

@media (min-width: 1280px) {
  .xl\:w-1\/4 { width: 25%; }
  .xl\:w-\[1024px\] { width: 1024px; }
  .xl\:w-\[350px\] { width: 350px; }
  .xl\:min-w-\[1024px\] { min-width: 1024px; }
  .xl\:max-w-3xl { max-width: 48rem; }
  .xl\:max-w-6xl { max-width: 72rem; }
  .xl\:max-w-xl { max-width: 36rem; }
  .xl\:max-w-xs { max-width: 20rem; }
  .xl\:grid-cols-4 { grid-template-columns: repeat(4, minmax(0px, 1fr)); }
}

@media (min-width: 1536px) {
  .\32 xl\:w-\[400px\] { width: 400px; }
}

tr:last-child .\[tr\:last-child_\&\]\:border-b-0 { border-bottom-width: 0px; }

tr[data-disabled="true"] .\[tr\[data-disabled\=true\]_\&\]\:opacity-50 { opacity: 0.5; }

@font-face { font-family: KaTeX_AMS; font-style: normal; font-weight: 400; src: url("https://cdn.openai.com/common/fonts/katex/KaTeX_AMS-Regular.woff2") format("woff2"); }

@font-face { font-family: KaTeX_Caligraphic; font-style: normal; font-weight: 700; src: url("https://cdn.openai.com/common/fonts/katex/KaTeX_Caligraphic-Bold.woff2") format("woff2"); }

@font-face { font-family: KaTeX_Caligraphic; font-style: normal; font-weight: 400; src: url("https://cdn.openai.com/common/fonts/katex/KaTeX_Caligraphic-Regular.woff2") format("woff2"); }

@font-face { font-family: KaTeX_Fraktur; font-style: normal; font-weight: 700; src: url("https://cdn.openai.com/common/fonts/katex/KaTeX_Fraktur-Bold.woff2") format("woff2"); }

@font-face { font-family: KaTeX_Fraktur; font-style: normal; font-weight: 400; src: url("https://cdn.openai.com/common/fonts/katex/KaTeX_Fraktur-Regular.woff2") format("woff2"); }

@font-face { font-family: KaTeX_Main; font-style: normal; font-weight: 700; src: url("https://cdn.openai.com/common/fonts/katex/KaTeX_Main-Bold.woff2") format("woff2"); }

@font-face { font-family: KaTeX_Main; font-style: italic; font-weight: 700; src: url("https://cdn.openai.com/common/fonts/katex/KaTeX_Main-BoldItalic.woff2") format("woff2"); }

@font-face { font-family: KaTeX_Main; font-style: italic; font-weight: 400; src: url("https://cdn.openai.com/common/fonts/katex/KaTeX_Main-Italic.woff2") format("woff2"); }

@font-face { font-family: KaTeX_Main; font-style: normal; font-weight: 400; src: url("https://cdn.openai.com/common/fonts/katex/KaTeX_Main-Regular.woff2") format("woff2"); }

@font-face { font-family: KaTeX_Math; font-style: italic; font-weight: 700; src: url("https://cdn.openai.com/common/fonts/katex/KaTeX_Math-BoldItalic.woff2") format("woff2"); }

@font-face { font-family: KaTeX_Math; font-style: italic; font-weight: 400; src: url("https://cdn.openai.com/common/fonts/katex/KaTeX_Math-Italic.woff2") format("woff2"); }

@font-face { font-family: KaTeX_SansSerif; font-style: normal; font-weight: 700; src: url("https://cdn.openai.com/common/fonts/katex/KaTeX_SansSerif-Bold.woff2") format("woff2"); }

@font-face { font-family: KaTeX_SansSerif; font-style: italic; font-weight: 400; src: url("https://cdn.openai.com/common/fonts/katex/KaTeX_SansSerif-Italic.woff2") format("woff2"); }

@font-face { font-family: KaTeX_SansSerif; font-style: normal; font-weight: 400; src: url("https://cdn.openai.com/common/fonts/katex/KaTeX_SansSerif-Regular.woff2") format("woff2"); }

@font-face { font-family: KaTeX_Script; font-style: normal; font-weight: 400; src: url("https://cdn.openai.com/common/fonts/katex/KaTeX_Script-Regular.woff2") format("woff2"); }

@font-face { font-family: KaTeX_Size1; font-style: normal; font-weight: 400; src: url("https://cdn.openai.com/common/fonts/katex/KaTeX_Size1-Regular.woff2") format("woff2"); }

@font-face { font-family: KaTeX_Size2; font-style: normal; font-weight: 400; src: url("https://cdn.openai.com/common/fonts/katex/KaTeX_Size2-Regular.woff2") format("woff2"); }

@font-face { font-family: KaTeX_Size3; font-style: normal; font-weight: 400; src: url("https://cdn.openai.com/common/fonts/katex/KaTeX_Size3-Regular.woff2") format("woff2"); }

@font-face { font-family: KaTeX_Size4; font-style: normal; font-weight: 400; src: url("https://cdn.openai.com/common/fonts/katex/KaTeX_Size4-Regular.woff2") format("woff2"); }

@font-face { font-family: KaTeX_Typewriter; font-style: normal; font-weight: 400; src: url("https://cdn.openai.com/common/fonts/katex/KaTeX_Typewriter-Regular.woff2") format("woff2"); }

.katex { font: 1.21em / 1.2 KaTeX_Main, "Times New Roman", serif; text-indent: 0px; text-rendering: auto; }

.katex * { border-color: currentcolor; forced-color-adjust: none !important; }

.katex .katex-version::after { content: "0.16.0"; }

.katex .katex-mathml { clip: rect(1px, 1px, 1px, 1px); border: 0px; height: 1px; overflow: hidden; padding: 0px; position: absolute; width: 1px; }

.katex .katex-html > .newline { display: block; }

.katex .base { position: relative; white-space: nowrap; width: min-content; }

.katex .base, .katex .strut { display: inline-block; }

.katex .textbf { font-weight: 700; }

.katex .textit { font-style: italic; }

.katex .textrm { font-family: KaTeX_Main; }

.katex .textsf { font-family: KaTeX_SansSerif; }

.katex .texttt { font-family: KaTeX_Typewriter; }

.katex .mathnormal { font-family: KaTeX_Math; font-style: italic; }

.katex .mathit { font-family: KaTeX_Main; font-style: italic; }

.katex .mathrm { font-style: normal; }

.katex .mathbf { font-family: KaTeX_Main; font-weight: 700; }

.katex .boldsymbol { font-family: KaTeX_Math; font-style: italic; font-weight: 700; }

.katex .amsrm, .katex .mathbb, .katex .textbb { font-family: KaTeX_AMS; }

.katex .mathcal { font-family: KaTeX_Caligraphic; }

.katex .mathfrak, .katex .textfrak { font-family: KaTeX_Fraktur; }

.katex .mathtt { font-family: KaTeX_Typewriter; }

.katex .mathscr, .katex .textscr { font-family: KaTeX_Script; }

.katex .mathsf, .katex .textsf { font-family: KaTeX_SansSerif; }

.katex .mathboldsf, .katex .textboldsf { font-family: KaTeX_SansSerif; font-weight: 700; }

.katex .mathitsf, .katex .textitsf { font-family: KaTeX_SansSerif; font-style: italic; }

.katex .mainrm { font-family: KaTeX_Main; font-style: normal; }

.katex .vlist-t { border-collapse: collapse; display: inline-table; table-layout: fixed; }

.katex .vlist-r { display: table-row; }

.katex .vlist { display: table-cell; position: relative; vertical-align: bottom; }

.katex .vlist > span { display: block; height: 0px; position: relative; }

.katex .vlist > span > span { display: inline-block; }

.katex .vlist > span > .pstrut { overflow: hidden; width: 0px; }

.katex .vlist-t2 { margin-right: -2px; }

.katex .vlist-s { display: table-cell; font-size: 1px; min-width: 2px; vertical-align: bottom; width: 2px; }

.katex .vbox { align-items: baseline; display: inline-flex; flex-direction: column; }

.katex .hbox { width: 100%; }

.katex .hbox, .katex .thinbox { display: inline-flex; flex-direction: row; }

.katex .thinbox { max-width: 0px; width: 0px; }

.katex .msupsub { text-align: left; }

.katex .mfrac > span > span { text-align: center; }

.katex .mfrac .frac-line { border-bottom-style: solid; display: inline-block; width: 100%; }

.katex .hdashline, .katex .hline, .katex .mfrac .frac-line, .katex .overline .overline-line, .katex .rule, .katex .underline .underline-line { min-height: 1px; }

.katex .mspace { display: inline-block; }

.katex .clap, .katex .llap, .katex .rlap { position: relative; width: 0px; }

.katex .clap > .inner, .katex .llap > .inner, .katex .rlap > .inner { position: absolute; }

.katex .clap > .fix, .katex .llap > .fix, .katex .rlap > .fix { display: inline-block; }

.katex .llap > .inner { right: 0px; }

.katex .clap > .inner, .katex .rlap > .inner { left: 0px; }

.katex .clap > .inner > span { margin-left: -50%; margin-right: 50%; }

.katex .rule { border: 0px solid; display: inline-block; position: relative; }

.katex .hline, .katex .overline .overline-line, .katex .underline .underline-line { border-bottom-style: solid; display: inline-block; width: 100%; }

.katex .hdashline { border-bottom-style: dashed; display: inline-block; width: 100%; }

.katex .sqrt > .root { margin-left: 0.277778em; margin-right: -0.555556em; }

.katex .fontsize-ensurer.reset-size1.size1, .katex .sizing.reset-size1.size1 { font-size: 1em; }

.katex .fontsize-ensurer.reset-size1.size2, .katex .sizing.reset-size1.size2 { font-size: 1.2em; }

.katex .fontsize-ensurer.reset-size1.size3, .katex .sizing.reset-size1.size3 { font-size: 1.4em; }

.katex .fontsize-ensurer.reset-size1.size4, .katex .sizing.reset-size1.size4 { font-size: 1.6em; }

.katex .fontsize-ensurer.reset-size1.size5, .katex .sizing.reset-size1.size5 { font-size: 1.8em; }

.katex .fontsize-ensurer.reset-size1.size6, .katex .sizing.reset-size1.size6 { font-size: 2em; }

.katex .fontsize-ensurer.reset-size1.size7, .katex .sizing.reset-size1.size7 { font-size: 2.4em; }

.katex .fontsize-ensurer.reset-size1.size8, .katex .sizing.reset-size1.size8 { font-size: 2.88em; }

.katex .fontsize-ensurer.reset-size1.size9, .katex .sizing.reset-size1.size9 { font-size: 3.456em; }

.katex .fontsize-ensurer.reset-size1.size10, .katex .sizing.reset-size1.size10 { font-size: 4.148em; }

.katex .fontsize-ensurer.reset-size1.size11, .katex .sizing.reset-size1.size11 { font-size: 4.976em; }

.katex .fontsize-ensurer.reset-size2.size1, .katex .sizing.reset-size2.size1 { font-size: 0.833333em; }

.katex .fontsize-ensurer.reset-size2.size2, .katex .sizing.reset-size2.size2 { font-size: 1em; }

.katex .fontsize-ensurer.reset-size2.size3, .katex .sizing.reset-size2.size3 { font-size: 1.16667em; }

.katex .fontsize-ensurer.reset-size2.size4, .katex .sizing.reset-size2.size4 { font-size: 1.33333em; }

.katex .fontsize-ensurer.reset-size2.size5, .katex .sizing.reset-size2.size5 { font-size: 1.5em; }

.katex .fontsize-ensurer.reset-size2.size6, .katex .sizing.reset-size2.size6 { font-size: 1.66667em; }

.katex .fontsize-ensurer.reset-size2.size7, .katex .sizing.reset-size2.size7 { font-size: 2em; }

.katex .fontsize-ensurer.reset-size2.size8, .katex .sizing.reset-size2.size8 { font-size: 2.4em; }

.katex .fontsize-ensurer.reset-size2.size9, .katex .sizing.reset-size2.size9 { font-size: 2.88em; }

.katex .fontsize-ensurer.reset-size2.size10, .katex .sizing.reset-size2.size10 { font-size: 3.45667em; }

.katex .fontsize-ensurer.reset-size2.size11, .katex .sizing.reset-size2.size11 { font-size: 4.14667em; }

.katex .fontsize-ensurer.reset-size3.size1, .katex .sizing.reset-size3.size1 { font-size: 0.714286em; }

.katex .fontsize-ensurer.reset-size3.size2, .katex .sizing.reset-size3.size2 { font-size: 0.857143em; }

.katex .fontsize-ensurer.reset-size3.size3, .katex .sizing.reset-size3.size3 { font-size: 1em; }

.katex .fontsize-ensurer.reset-size3.size4, .katex .sizing.reset-size3.size4 { font-size: 1.14286em; }

.katex .fontsize-ensurer.reset-size3.size5, .katex .sizing.reset-size3.size5 { font-size: 1.28571em; }

.katex .fontsize-ensurer.reset-size3.size6, .katex .sizing.reset-size3.size6 { font-size: 1.42857em; }

.katex .fontsize-ensurer.reset-size3.size7, .katex .sizing.reset-size3.size7 { font-size: 1.71429em; }

.katex .fontsize-ensurer.reset-size3.size8, .katex .sizing.reset-size3.size8 { font-size: 2.05714em; }

.katex .fontsize-ensurer.reset-size3.size9, .katex .sizing.reset-size3.size9 { font-size: 2.46857em; }

.katex .fontsize-ensurer.reset-size3.size10, .katex .sizing.reset-size3.size10 { font-size: 2.96286em; }

.katex .fontsize-ensurer.reset-size3.size11, .katex .sizing.reset-size3.size11 { font-size: 3.55429em; }

.katex .fontsize-ensurer.reset-size4.size1, .katex .sizing.reset-size4.size1 { font-size: 0.625em; }

.katex .fontsize-ensurer.reset-size4.size2, .katex .sizing.reset-size4.size2 { font-size: 0.75em; }

.katex .fontsize-ensurer.reset-size4.size3, .katex .sizing.reset-size4.size3 { font-size: 0.875em; }

.katex .fontsize-ensurer.reset-size4.size4, .katex .sizing.reset-size4.size4 { font-size: 1em; }

.katex .fontsize-ensurer.reset-size4.size5, .katex .sizing.reset-size4.size5 { font-size: 1.125em; }

.katex .fontsize-ensurer.reset-size4.size6, .katex .sizing.reset-size4.size6 { font-size: 1.25em; }

.katex .fontsize-ensurer.reset-size4.size7, .katex .sizing.reset-size4.size7 { font-size: 1.5em; }

.katex .fontsize-ensurer.reset-size4.size8, .katex .sizing.reset-size4.size8 { font-size: 1.8em; }

.katex .fontsize-ensurer.reset-size4.size9, .katex .sizing.reset-size4.size9 { font-size: 2.16em; }

.katex .fontsize-ensurer.reset-size4.size10, .katex .sizing.reset-size4.size10 { font-size: 2.5925em; }

.katex .fontsize-ensurer.reset-size4.size11, .katex .sizing.reset-size4.size11 { font-size: 3.11em; }

.katex .fontsize-ensurer.reset-size5.size1, .katex .sizing.reset-size5.size1 { font-size: 0.555556em; }

.katex .fontsize-ensurer.reset-size5.size2, .katex .sizing.reset-size5.size2 { font-size: 0.666667em; }

.katex .fontsize-ensurer.reset-size5.size3, .katex .sizing.reset-size5.size3 { font-size: 0.777778em; }

.katex .fontsize-ensurer.reset-size5.size4, .katex .sizing.reset-size5.size4 { font-size: 0.888889em; }

.katex .fontsize-ensurer.reset-size5.size5, .katex .sizing.reset-size5.size5 { font-size: 1em; }

.katex .fontsize-ensurer.reset-size5.size6, .katex .sizing.reset-size5.size6 { font-size: 1.11111em; }

.katex .fontsize-ensurer.reset-size5.size7, .katex .sizing.reset-size5.size7 { font-size: 1.33333em; }

.katex .fontsize-ensurer.reset-size5.size8, .katex .sizing.reset-size5.size8 { font-size: 1.6em; }

.katex .fontsize-ensurer.reset-size5.size9, .katex .sizing.reset-size5.size9 { font-size: 1.92em; }

.katex .fontsize-ensurer.reset-size5.size10, .katex .sizing.reset-size5.size10 { font-size: 2.30444em; }

.katex .fontsize-ensurer.reset-size5.size11, .katex .sizing.reset-size5.size11 { font-size: 2.76444em; }

.katex .fontsize-ensurer.reset-size6.size1, .katex .sizing.reset-size6.size1 { font-size: 0.5em; }

.katex .fontsize-ensurer.reset-size6.size2, .katex .sizing.reset-size6.size2 { font-size: 0.6em; }

.katex .fontsize-ensurer.reset-size6.size3, .katex .sizing.reset-size6.size3 { font-size: 0.7em; }

.katex .fontsize-ensurer.reset-size6.size4, .katex .sizing.reset-size6.size4 { font-size: 0.8em; }

.katex .fontsize-ensurer.reset-size6.size5, .katex .sizing.reset-size6.size5 { font-size: 0.9em; }

.katex .fontsize-ensurer.reset-size6.size6, .katex .sizing.reset-size6.size6 { font-size: 1em; }

.katex .fontsize-ensurer.reset-size6.size7, .katex .sizing.reset-size6.size7 { font-size: 1.2em; }

.katex .fontsize-ensurer.reset-size6.size8, .katex .sizing.reset-size6.size8 { font-size: 1.44em; }

.katex .fontsize-ensurer.reset-size6.size9, .katex .sizing.reset-size6.size9 { font-size: 1.728em; }

.katex .fontsize-ensurer.reset-size6.size10, .katex .sizing.reset-size6.size10 { font-size: 2.074em; }

.katex .fontsize-ensurer.reset-size6.size11, .katex .sizing.reset-size6.size11 { font-size: 2.488em; }

.katex .fontsize-ensurer.reset-size7.size1, .katex .sizing.reset-size7.size1 { font-size: 0.416667em; }

.katex .fontsize-ensurer.reset-size7.size2, .katex .sizing.reset-size7.size2 { font-size: 0.5em; }

.katex .fontsize-ensurer.reset-size7.size3, .katex .sizing.reset-size7.size3 { font-size: 0.583333em; }

.katex .fontsize-ensurer.reset-size7.size4, .katex .sizing.reset-size7.size4 { font-size: 0.666667em; }

.katex .fontsize-ensurer.reset-size7.size5, .katex .sizing.reset-size7.size5 { font-size: 0.75em; }

.katex .fontsize-ensurer.reset-size7.size6, .katex .sizing.reset-size7.size6 { font-size: 0.833333em; }

.katex .fontsize-ensurer.reset-size7.size7, .katex .sizing.reset-size7.size7 { font-size: 1em; }

.katex .fontsize-ensurer.reset-size7.size8, .katex .sizing.reset-size7.size8 { font-size: 1.2em; }

.katex .fontsize-ensurer.reset-size7.size9, .katex .sizing.reset-size7.size9 { font-size: 1.44em; }

.katex .fontsize-ensurer.reset-size7.size10, .katex .sizing.reset-size7.size10 { font-size: 1.72833em; }

.katex .fontsize-ensurer.reset-size7.size11, .katex .sizing.reset-size7.size11 { font-size: 2.07333em; }

.katex .fontsize-ensurer.reset-size8.size1, .katex .sizing.reset-size8.size1 { font-size: 0.347222em; }

.katex .fontsize-ensurer.reset-size8.size2, .katex .sizing.reset-size8.size2 { font-size: 0.416667em; }

.katex .fontsize-ensurer.reset-size8.size3, .katex .sizing.reset-size8.size3 { font-size: 0.486111em; }

.katex .fontsize-ensurer.reset-size8.size4, .katex .sizing.reset-size8.size4 { font-size: 0.555556em; }

.katex .fontsize-ensurer.reset-size8.size5, .katex .sizing.reset-size8.size5 { font-size: 0.625em; }

.katex .fontsize-ensurer.reset-size8.size6, .katex .sizing.reset-size8.size6 { font-size: 0.694444em; }

.katex .fontsize-ensurer.reset-size8.size7, .katex .sizing.reset-size8.size7 { font-size: 0.833333em; }

.katex .fontsize-ensurer.reset-size8.size8, .katex .sizing.reset-size8.size8 { font-size: 1em; }

.katex .fontsize-ensurer.reset-size8.size9, .katex .sizing.reset-size8.size9 { font-size: 1.2em; }

.katex .fontsize-ensurer.reset-size8.size10, .katex .sizing.reset-size8.size10 { font-size: 1.44028em; }

.katex .fontsize-ensurer.reset-size8.size11, .katex .sizing.reset-size8.size11 { font-size: 1.72778em; }

.katex .fontsize-ensurer.reset-size9.size1, .katex .sizing.reset-size9.size1 { font-size: 0.289352em; }

.katex .fontsize-ensurer.reset-size9.size2, .katex .sizing.reset-size9.size2 { font-size: 0.347222em; }

.katex .fontsize-ensurer.reset-size9.size3, .katex .sizing.reset-size9.size3 { font-size: 0.405093em; }

.katex .fontsize-ensurer.reset-size9.size4, .katex .sizing.reset-size9.size4 { font-size: 0.462963em; }

.katex .fontsize-ensurer.reset-size9.size5, .katex .sizing.reset-size9.size5 { font-size: 0.520833em; }

.katex .fontsize-ensurer.reset-size9.size6, .katex .sizing.reset-size9.size6 { font-size: 0.578704em; }

.katex .fontsize-ensurer.reset-size9.size7, .katex .sizing.reset-size9.size7 { font-size: 0.694444em; }

.katex .fontsize-ensurer.reset-size9.size8, .katex .sizing.reset-size9.size8 { font-size: 0.833333em; }

.katex .fontsize-ensurer.reset-size9.size9, .katex .sizing.reset-size9.size9 { font-size: 1em; }

.katex .fontsize-ensurer.reset-size9.size10, .katex .sizing.reset-size9.size10 { font-size: 1.20023em; }

.katex .fontsize-ensurer.reset-size9.size11, .katex .sizing.reset-size9.size11 { font-size: 1.43981em; }

.katex .fontsize-ensurer.reset-size10.size1, .katex .sizing.reset-size10.size1 { font-size: 0.24108em; }

.katex .fontsize-ensurer.reset-size10.size2, .katex .sizing.reset-size10.size2 { font-size: 0.289296em; }

.katex .fontsize-ensurer.reset-size10.size3, .katex .sizing.reset-size10.size3 { font-size: 0.337512em; }

.katex .fontsize-ensurer.reset-size10.size4, .katex .sizing.reset-size10.size4 { font-size: 0.385728em; }

.katex .fontsize-ensurer.reset-size10.size5, .katex .sizing.reset-size10.size5 { font-size: 0.433944em; }

.katex .fontsize-ensurer.reset-size10.size6, .katex .sizing.reset-size10.size6 { font-size: 0.48216em; }

.katex .fontsize-ensurer.reset-size10.size7, .katex .sizing.reset-size10.size7 { font-size: 0.578592em; }

.katex .fontsize-ensurer.reset-size10.size8, .katex .sizing.reset-size10.size8 { font-size: 0.694311em; }

.katex .fontsize-ensurer.reset-size10.size9, .katex .sizing.reset-size10.size9 { font-size: 0.833173em; }

.katex .fontsize-ensurer.reset-size10.size10, .katex .sizing.reset-size10.size10 { font-size: 1em; }

.katex .fontsize-ensurer.reset-size10.size11, .katex .sizing.reset-size10.size11 { font-size: 1.19961em; }

.katex .fontsize-ensurer.reset-size11.size1, .katex .sizing.reset-size11.size1 { font-size: 0.200965em; }

.katex .fontsize-ensurer.reset-size11.size2, .katex .sizing.reset-size11.size2 { font-size: 0.241158em; }

.katex .fontsize-ensurer.reset-size11.size3, .katex .sizing.reset-size11.size3 { font-size: 0.28135em; }

.katex .fontsize-ensurer.reset-size11.size4, .katex .sizing.reset-size11.size4 { font-size: 0.321543em; }

.katex .fontsize-ensurer.reset-size11.size5, .katex .sizing.reset-size11.size5 { font-size: 0.361736em; }

.katex .fontsize-ensurer.reset-size11.size6, .katex .sizing.reset-size11.size6 { font-size: 0.401929em; }

.katex .fontsize-ensurer.reset-size11.size7, .katex .sizing.reset-size11.size7 { font-size: 0.482315em; }

.katex .fontsize-ensurer.reset-size11.size8, .katex .sizing.reset-size11.size8 { font-size: 0.578778em; }

.katex .fontsize-ensurer.reset-size11.size9, .katex .sizing.reset-size11.size9 { font-size: 0.694534em; }

.katex .fontsize-ensurer.reset-size11.size10, .katex .sizing.reset-size11.size10 { font-size: 0.833601em; }

.katex .fontsize-ensurer.reset-size11.size11, .katex .sizing.reset-size11.size11 { font-size: 1em; }

.katex .delimsizing.size1 { font-family: KaTeX_Size1; }

.katex .delimsizing.size2 { font-family: KaTeX_Size2; }

.katex .delimsizing.size3 { font-family: KaTeX_Size3; }

.katex .delimsizing.size4 { font-family: KaTeX_Size4; }

.katex .delimsizing.mult .delim-size1 > span { font-family: KaTeX_Size1; }

.katex .delimsizing.mult .delim-size4 > span { font-family: KaTeX_Size4; }

.katex .nulldelimiter { display: inline-block; width: 0.12em; }

.katex .delimcenter, .katex .op-symbol { position: relative; }

.katex .op-symbol.small-op { font-family: KaTeX_Size1; }

.katex .op-symbol.large-op { font-family: KaTeX_Size2; }

.katex .accent > .vlist-t, .katex .op-limits > .vlist-t { text-align: center; }

.katex .accent .accent-body { position: relative; }

.katex .accent .accent-body:not(.accent-full) { width: 0px; }

.katex .overlay { display: block; }

.katex .mtable .vertical-separator { display: inline-block; min-width: 1px; }

.katex .mtable .arraycolsep { display: inline-block; }

.katex .mtable .col-align-c > .vlist-t { text-align: center; }

.katex .mtable .col-align-l > .vlist-t { text-align: left; }

.katex .mtable .col-align-r > .vlist-t { text-align: right; }

.katex .svg-align { text-align: left; }

.katex svg { fill: currentcolor; stroke: currentcolor; fill-rule: nonzero; fill-opacity: 1; stroke-width: 1; stroke-linecap: butt; stroke-linejoin: miter; stroke-miterlimit: 4; stroke-dasharray: none; stroke-dashoffset: 0; stroke-opacity: 1; display: block; height: inherit; position: absolute; width: 100%; }

.katex svg path { stroke: none; }

.katex img { border-style: none; max-height: none; max-width: none; min-height: 0px; min-width: 0px; }

.katex .stretchy { display: block; overflow: hidden; position: relative; width: 100%; }

.katex .stretchy::after, .katex .stretchy::before { content: ""; }

.katex .hide-tail { overflow: hidden; position: relative; width: 100%; }

.katex .halfarrow-left { left: 0px; overflow: hidden; position: absolute; width: 50.2%; }

.katex .halfarrow-right { overflow: hidden; position: absolute; right: 0px; width: 50.2%; }

.katex .brace-left { left: 0px; overflow: hidden; position: absolute; width: 25.1%; }

.katex .brace-center { left: 25%; overflow: hidden; position: absolute; width: 50%; }

.katex .brace-right { overflow: hidden; position: absolute; right: 0px; width: 25.1%; }

.katex .x-arrow-pad { padding: 0px 0.5em; }

.katex .cd-arrow-pad { padding: 0px 0.55556em 0px 0.27778em; }

.katex .mover, .katex .munder, .katex .x-arrow { text-align: center; }

.katex .boxpad { padding: 0px 0.3em; }

.katex .fbox, .katex .fcolorbox { border: 0.04em solid; box-sizing: border-box; }

.katex .cancel-pad { padding: 0px 0.2em; }

.katex .cancel-lap { margin-left: -0.2em; margin-right: -0.2em; }

.katex .sout { border-bottom-style: solid; border-bottom-width: 0.08em; }

.katex .angl { border-right: 0.049em solid; border-top: 0.049em solid; box-sizing: border-box; margin-right: 0.03889em; }

.katex .anglpad { padding: 0px 0.03889em; }

.katex .eqn-num::before { content: "(" counter(katexEqnNo) ")"; counter-increment: katexEqnNo 1; }

.katex .mml-eqn-num::before { content: "(" counter(mmlEqnNo) ")"; counter-increment: mmlEqnNo 1; }

.katex .mtr-glue { width: 50%; }

.katex .cd-vert-arrow { display: inline-block; position: relative; }

.katex .cd-label-left { display: inline-block; position: absolute; right: calc(50% + 0.3em); text-align: left; }

.katex .cd-label-right { display: inline-block; left: calc(50% + 0.3em); position: absolute; text-align: right; }

.katex-display { display: block; margin: 1em 0px; text-align: center; }

.katex-display > .katex { display: block; text-align: center; white-space: nowrap; }

.katex-display > .katex > .katex-html { display: block; position: relative; }

.katex-display > .katex > .katex-html > .tag { position: absolute; right: 0px; }

.katex-display.leqno > .katex > .katex-html > .tag { left: 0px; right: auto; }

.katex-display.fleqn > .katex { padding-left: 2em; text-align: left; }

body { counter-reset: katexEqnNo 0 mmlEqnNo 0; }

code, pre { font-family: "SÃ¶hne Mono", Monaco, "Andale Mono", "Ubuntu Mono", monospace !important; }

code[class="language-plaintext"] { white-space: pre-line; }

code.hljs, code[class*="language-"], pre[class*="language-"] { overflow-wrap: normal; background: none; color: rgb(255, 255, 255); hyphens: none; line-height: 1.5; tab-size: 4; text-align: left; white-space: pre; word-break: normal; word-spacing: normal; }

pre[class*="language-"] { border-radius: 0.3em; overflow: auto; }

:not(pre) > code.hljs, :not(pre) > code[class*="language-"] { border-radius: 0.3em; padding: 0.1em; white-space: normal; }

.hljs-comment { color: rgba(255, 255, 255, 0.5); }

.hljs-meta { color: rgba(255, 255, 255, 0.6); }

.hljs-built_in, .hljs-class .hljs-title { color: rgb(233, 149, 12); }

.hljs-doctag, .hljs-formula, .hljs-keyword, .hljs-literal { color: rgb(46, 149, 211); }

.hljs-addition, .hljs-attribute, .hljs-meta-string, .hljs-regexp, .hljs-string { color: rgb(0, 166, 125); }

.hljs-attr, .hljs-number, .hljs-selector-attr, .hljs-selector-class, .hljs-selector-pseudo, .hljs-template-variable, .hljs-type, .hljs-variable { color: rgb(223, 48, 121); }

.hljs-bullet, .hljs-link, .hljs-selector-id, .hljs-symbol, .hljs-title { color: rgb(242, 44, 61); }

.token.cdata, .token.comment, .token.doctype, .token.prolog { color: rgb(169, 174, 193); }

.token.punctuation { color: rgb(254, 254, 254); }

.token.constant, .token.deleted, .token.property, .token.symbol, .token.tag { color: rgb(255, 160, 122); }

.token.boolean, .token.number { color: rgb(0, 224, 224); }

.token.attr-name, .token.builtin, .token.char, .token.inserted, .token.selector, .token.string { color: rgb(171, 227, 56); }

.language-css .token.string, .style .token.string, .token.entity, .token.operator, .token.url, .token.variable { color: rgb(0, 224, 224); }

.token.atrule, .token.attr-value, .token.function { color: gold; }

.token.keyword { color: rgb(0, 224, 224); }

.token.important, .token.regex { color: gold; }

.token.bold, .token.important { font-weight: 700; }

.token.italic { font-style: italic; }

.token.entity { cursor: help; }

@media screen and (-ms-high-contrast: active) {
  code[class*="language-"], pre[class*="language-"] { background: window; color: windowtext; }
  :not(pre) > code[class*="language-"], pre[class*="language-"] { background: window; }
  .token.important { background: highlight; color: window; font-weight: 400; }
  .token.atrule, .token.attr-value, .token.function, .token.keyword, .token.operator, .token.selector { font-weight: 700; }
  .token.attr-value, .token.comment, .token.doctype, .token.function, .token.keyword, .token.operator, .token.property, .token.string { color: highlight; }
  .token.attr-value, .token.url { font-weight: 400; }
}

.PopoverContent { z-index: 1000; }

.react-select-container input:focus { --tw-ring-offset-shadow:var(--tw-ring-inset) 0 0 0 var(--tw-ring-offset-width) var(--tw-ring-offset-color); --tw-ring-shadow:var(--tw-ring-inset) 0 0 0 calc(var(--tw-ring-offset-width)) var(--tw-ring-color); box-shadow: var(--tw-ring-offset-shadow),var(--tw-ring-shadow),var(--tw-shadow,0 0 transparent); }

.react-select-container .react-select__control { --tw-border-opacity:1; align-items: flex-start; border-color: rgba(236,236,241,var(--tw-border-opacity)); border-radius: 0.5rem; font-size: 0.875rem; height: 192px; line-height: 1.25rem; min-height: 192px; overflow: auto; padding: 0.5rem; }

.react-select-container .react-select__control:hover { --tw-border-opacity:1; border-color: rgba(217,217,227,var(--tw-border-opacity)); }

.dark .react-select-container .react-select__control { --tw-bg-opacity:1; background-color: rgba(32,33,35,var(--tw-bg-opacity)); border-color: rgba(255, 255, 255, 0.1); }

:is(.dark .react-select-container .react-select__control):hover { border-color: rgba(255, 255, 255, 0.2); }

.dark .react-select-container.react-select--invalid .react-select__control, .react-select-container.react-select--invalid .react-select__control { --tw-border-opacity:1; border-color: rgba(239,68,68,var(--tw-border-opacity)); }

.react-select-container .react-select__input-container, .react-select-container .react-select__placeholder { padding-left: 0.5rem; padding-right: 0.5rem; }

.dark .react-select-container .react-select__input-container { --tw-text-opacity:1; color: rgba(197,197,210,var(--tw-text-opacity)); }

.react-select-container .react-select__input { height: 2rem; }

.react-select-container .react-select__control--is-focused { --tw-ring-offset-shadow:var(--tw-ring-inset) 0 0 0 var(--tw-ring-offset-width) var(--tw-ring-offset-color); --tw-ring-shadow:var(--tw-ring-inset) 0 0 0 calc(var(--tw-ring-offset-width)) var(--tw-ring-color); box-shadow: var(--tw-ring-offset-shadow),var(--tw-ring-shadow),var(--tw-shadow,0 0 transparent); --tw-border-opacity:1 !important; border-color: rgba(217,217,227,var(--tw-border-opacity)) !important; }

.dark .react-select-container .react-select__control--is-focused { border-color: rgba(255, 255, 255, 0.2) !important; }

.react-select-container .react-select__value-container { padding: 0px; }

.react-select-container .react-select__indicators:empty { display: none; }

.react-select-container .react-select__multi-value { background-color: transparent; margin: 0px; }

.react-select-container .react-select__multi-value__label { padding: 0px; }

#intercom-container { display: none; }
------MultipartBoundary--d1L5cNE49EQ4Yph33YtSG31B6q1OA5hLrmgkI1GWnw----
Content-Type: image/webp
Content-Transfer-Encoding: binary
Content-Location: https://chat.openai.com/_next/image?url=https%3A%2F%2Flh3.googleusercontent.com%2Fa%2FAAcHTtdF1IO9_ztXht2qmdVDc5wzKiB04cnEtzSNv8RWjhw-Tw%3Ds96-c&w=96&q=75

RIFF¨  WEBPVP8    *` ` >8G¥#¢!2´È 	@»ñÒ_p¨Ï¦?;ÁiÛEoè4õÿ d4ñ?qF|»µè)ô=ÂùýD¹äMÜKÏåçµª0¸vB³6}ª¹½ðöÎ(êLþ)6þ×òäKþfÀðxõ%×?«upXú	ÁYìIZ@ñÜöÌßp SjGNQTÈRÂ½#Eû&9ìI¸]òX[Ú×n¡dÙ÷6{=t­ÑÿXK¯Öí» ÇÍÝ¥\5a?)áÒ^§¿dÓ^§¼ô"xOÅ»?$êh þðêà&qÕîY¿`ù>F*VÓøUª«YèxüoYªeÇõvÕfCÂù:¬¬Ø} y¤ß<¡4*¤¼#	·"I½À@W1ìò@}TYÈn 5ÜjûiÝ=ÂµCUe¾²Äb&3"qþÍLtRjÝtüÛõnõ3¹Ñ]óËÎÓ ·LÏ6I!Bu$)ÚòægÉcowæÕÄ®y`Sã©2TÞ±aW¢=ÝíµN8¼X*ÌÚúÇiaBë¿ªÿ µ.Æ÷{øÍ&[Ó¢4ß]*J¥íËèß¨øâ~.ÜBw!ßÓ« 6Öm£{Æqý'Y¾L8@áqYæÔI¶{y]²Ìeµ»ºa:fi°¸O«ÂI6µ3zß¦Å©(õóüf
G;V¸ë7h6¢¹ÝRã{þÌý¯>½ZQ¹¬ë£/{?AË(8Äæ5¢÷î©]ûæ\®2(Zª°}Z<þf­µ¶hc¶Á@§9ÐNrn´«ÆýÊGgðuY4½ÌÙ^%ð>µ
kõÿnÌÞ1YÓfêªÐ×¼36kF«g1eé(9E]4;¥¬â<«nºrÏ69I\»c9Uá¨Jø:ù ô9ÝDLF¬uâYX(ù|±dÂ`²×ÙA½K)³tií/Å«ÉÐþ6HqøÞN´¶p)é·<Õ[{øpn·&CæÞ¨jWÉô> ¤&Ï"~Ò  .£¢GÂÜªêÒ;µ¿PúôÎÔ1Î½,Þ·°GM/Ò`ÄøXê*`ÜñPÊI;88.°å[ÒT E=ëä"=lDû7v&léÙôK>Ó¶½=F!ÔÿlÙ>BÀÒàÊÐüe
YoÓªUrpÆnY Û&¹Oÿ[h$k<ÿ¢[ðBeª¯¢6E]9âýR÷.ç+ÉÕÓX±hÛj®ùJqø_]nhîÅÜ3°IªCþÿâÅ%àñNhpÑlOëÞIr Ï¨ÃäUE¶Æ¡ò³KgttYÞ8ÀÏîhxWÝîkæUÀÊ,ò÷D|©Pêe;?ãTýl7×ß×¿Æy¤Í¸qäÄÔª55ò6%8¦ÊèÑ?_ÊÉkÃ÷Æa=Çnñ äíÓBÁîÜø2èbÀ:îåÈ#EDÒç;F	¯êé¾(¿,â]ïDxm¨õÅPI*bè®gö£È¦º3dpñé·hV2À*¸múwöM$q½DLD¶yµq»üRèÙÀ?·uÄ¶;$¶Ó>àcå ]¼928µùXÐônF^¸XYëÉ!D[ZôÄ`æ0|1úÒW¡³ßèAÞ`CI§õz,ÄÅÅX(tÉä-·u(@çæeX¡â%2ÑâÜÎÃÜ¦tD}1füØ°ìªì
¬¢ö©>LÆRzÓ ¶ÍB[ºè {KÁN8uüÊ¿3KZ¹ùþúw_ 2N2 òDÜÝ­é½û¨
X=ÆGÊ,\ÉÉ_}¶¦O}uYõÔÛ Yø  
------MultipartBoundary--d1L5cNE49EQ4Yph33YtSG31B6q1OA5hLrmgkI1GWnw----
Content-Type: text/html
Content-ID: 
Content-Transfer-Encoding: binary


------MultipartBoundary--d1L5cNE49EQ4Yph33YtSG31B6q1OA5hLrmgkI1GWnw----
Content-Type: text/html
Content-ID: 
Content-Transfer-Encoding: binary





------MultipartBoundary--d1L5cNE49EQ4Yph33YtSG31B6q1OA5hLrmgkI1GWnw----
Content-Type: text/html
Content-ID: 
Content-Transfer-Encoding: binary
Content-Location: https://tcr9i.chat.openai.com/v2/1.5.2/enforcement.64b3a4e29686f93d52816249ecbf9857.html#35536E1E-65B4-4D96-9D97-6ADB7EFF8147


------MultipartBoundary--d1L5cNE49EQ4Yph33YtSG31B6q1OA5hLrmgkI1GWnw----
Content-Type: text/css
Content-Transfer-Encoding: binary
Content-Location: cid:css-795879c0-a7e6-4a10-b09f-f369fa2109a0@mhtml.blink

@charset "utf-8";

html, body { margin: 0px; padding: 0px; height: 100%; }

* { box-sizing: border-box; }

#app { height: 100%; overflow: hidden; }
------MultipartBoundary--d1L5cNE49EQ4Yph33YtSG31B6q1OA5hLrmgkI1GWnw----
Content-Type: text/css
Content-Transfer-Encoding: binary
Content-Location: cid:css-0869a31c-770e-45ea-971b-3ed2a05dd87e@mhtml.blink

@charset "utf-8";

@keyframes spin { 
  0% { transform: rotate(0deg) translateZ(0px); }
  100% { transform: rotate(360deg) translateZ(0px); }
}

@keyframes fadeIn { 
  0% { opacity: 0; }
  100% { opacity: 1; }
}

.kXaXkDzIfSzLHRklX2r3 { position: fixed; top: 20px; right: 20px; width: 20px; height: 20px; z-index: 200; border: none; background-color: transparent; background-image: url("data:image/svg+xml;base64,PD94bWwgdmVyc2lvbj0iMS4wIiBlbmNvZGluZz0iVVRGLTgiPz4KPHN2ZyB3aWR0aD0iMzhweCIgaGVpZ2h0PSIzOHB4IiB2aWV3Qm94PSIwIDAgMzggMzgiIHZlcnNpb249IjEuMSIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIiB4bWxuczp4bGluaz0iaHR0cDovL3d3dy53My5vcmcvMTk5OS94bGluayI+CiAgICA8IS0tIEdlbmVyYXRvcjogU2tldGNoIDQ0LjEgKDQxNDU1KSAtIGh0dHA6Ly93d3cuYm9oZW1pYW5jb2RpbmcuY29tL3NrZXRjaCAtLT4KICAgIDx0aXRsZT5Hcm91cDwvdGl0bGU+CiAgICA8ZGVzYz5DcmVhdGVkIHdpdGggU2tldGNoLjwvZGVzYz4KICAgIDxkZWZzPjwvZGVmcz4KICAgIDxnIGlkPSJQYWdlLTEiIHN0cm9rZT0ibm9uZSIgc3Ryb2tlLXdpZHRoPSIxIiBmaWxsPSJub25lIiBmaWxsLXJ1bGU9ImV2ZW5vZGQiPgogICAgICAgIDxnIGlkPSJHcm91cCIgdHJhbnNmb3JtPSJ0cmFuc2xhdGUoMTkuMDAwMDAwLCAxOS4wMDAwMDApIHJvdGF0ZSg0NS4wMDAwMDApIHRyYW5zbGF0ZSgtMTkuMDAwMDAwLCAtMTkuMDAwMDAwKSB0cmFuc2xhdGUoLTYuMDAwMDAwLCAtNi4wMDAwMDApIiBmaWxsPSIjOTQ5NDk0Ij4KICAgICAgICAgICAgPHJlY3QgaWQ9IlJlY3RhbmdsZSIgeD0iMjQiIHk9IjAiIHdpZHRoPSIyIiBoZWlnaHQ9IjUwIj48L3JlY3Q+CiAgICAgICAgICAgIDxyZWN0IGlkPSJSZWN0YW5nbGUiIHRyYW5zZm9ybT0idHJhbnNsYXRlKDI1LjAwMDAwMCwgMjUuMDAwMDAwKSByb3RhdGUoOTAuMDAwMDAwKSB0cmFuc2xhdGUoLTI1LjAwMDAwMCwgLTI1LjAwMDAwMCkgIiB4PSIyNCIgeT0iMCIgd2lkdGg9IjIiIGhlaWdodD0iNTAiPjwvcmVjdD4KICAgICAgICA8L2c+CiAgICA8L2c+Cjwvc3ZnPg=="); background-repeat: no-repeat; background-position: center center; background-size: contain; cursor: pointer; pointer-events: none; transform: scale(1); transition: transform 100ms ease-in-out 0s; visibility: hidden; opacity: 0; }

.kXaXkDzIfSzLHRklX2r3:hover { transform: scale(1.3); }

.kXaXkDzIfSzLHRklX2r3.active { pointer-events: inherit; visibility: visible; opacity: 1; transition: opacity 400ms ease-in-out 0s; }

.nMUBxApsE7lSELvJiiAA { position: fixed; inset: 0px; background-color: rgba(255, 255, 255, 0.8); z-index: -1; opacity: 0; transition: opacity 300ms linear 0s; pointer-events: none; }

.nMUBxApsE7lSELvJiiAA.active { opacity: 1; pointer-events: inherit; }

.lTLYGVw1ASsTZWg0vUbC { position: absolute; top: 50%; left: 50%; margin-left: -15px; margin-top: -15px; z-index: 1010; border-radius: 50%; width: 30px; height: 30px; border-width: 3px; border-style: solid; border-color: transparent transparent transparent rgba(0, 0, 0, 0.2); transform: translateZ(0px); box-sizing: border-box; }

.lTLYGVw1ASsTZWg0vUbC { animation: 500ms linear 0s infinite normal none running spin; }

.slrEYyWESVLe_Cx3DM1k { transition: opacity 500ms ease 0s, transform 500ms ease 0s; opacity: 0; transform: scale(0.8); text-align: center; height: 100%; }

.slrEYyWESVLe_Cx3DM1k.active { opacity: 1; transform: scale(1); }

.slrEYyWESVLe_Cx3DM1k.challenge-enter { opacity: 0; transform: scale(0.8); }

.slrEYyWESVLe_Cx3DM1k.challenge-enter-active, .slrEYyWESVLe_Cx3DM1k.challenge-enter-done { transform: scale(1); opacity: 1; }

.slrEYyWESVLe_Cx3DM1k.challenge-exit { transform: scale(1); opacity: 1; }

.slrEYyWESVLe_Cx3DM1k.challenge-exit-active, .slrEYyWESVLe_Cx3DM1k.challenge-exit-done { transform: scale(0.8); opacity: 0; }

.slrEYyWESVLe_Cx3DM1k.dTcazKGGob_VbK_J853h { flex-direction: column; }

.dTcazKGGob_VbK_J853h { display: flex; align-items: center; justify-content: center; }
------MultipartBoundary--d1L5cNE49EQ4Yph33YtSG31B6q1OA5hLrmgkI1GWnw------
