Deontological Objectivism
Open sidebar
Deontological Objectivism
GPT-4
Summarize: 
into target system subject system in

their external environment because of

this tendency to dissect nature into

smaller bits and pieces mainstream

physics can be characterized as doing

physics in a box as Lee Smolin likes to

call it

it separates natural systems from the

rest of the universe including observers

in contrast process physics may be

called doing physics without a box in

that it models nature as an ecological

whole without any a priori separation

between subject and target side and

without excluding any environmental

influences when proposing laws of nature

and their initial conditions the problem

is that although these laws ultimately
1 / 1
ChatGPT
Mainstream physics often breaks nature down into smaller parts and operates in a separated context, as characterized by the phrase "doing physics in a box," as Lee Smolin puts it. In contrast, process physics takes a more holistic approach, modeling nature as an ecological whole without pre-defined separation between subject and target or excluding any environmental influences. This approach represents "doing physics without a box," which challenges the traditional methods of proposing laws of nature and their initial conditions.
1 / 1
like our conventional physics does but

it basically simulates the process you

ala T of nature by modeling it's be able

x' that is by modeling nature's events

and their relational crosstalk so to say

and all of that by means of a relational

moana dalla ji next in line is that

mainstream physics typically tries to

reduce all of nature to inert bits of

matter in motion whereas process physics

treats nature as an integrated web of

organismic relations finally because of

its belief in reductionism mainstream

physics treats conscious experience as

an epiphenomenon or even as entirely

illusory in process physics however

subjectivity is at Rom ��rio inherent

aspect of nature that is the emergent

activity patterns in the process monad

ology seem to come where they

collectively grow preference of how to

connect among each other so this can be

thought of as proto subjective and

biomimetic because it seems to exhibit

the same selectional connectivity as for

instance in neural networks and slime

all foraging patterns
1 / 1
ChatGPT
Conventional physics tends to reduce nature to inert bits of matter in motion, while process physics views nature as an integrated web of organismic relations. Mainstream physics, due to its belief in reductionism, considers conscious experience as a secondary phenomenon or even an illusion. In contrast, process physics sees subjectivity as an inherent aspect of nature. This approach models nature's events and their interrelations, simulating the processes of nature in a way that can be considered proto-subjective and biomimetic. This modeling seems to mirror natural selectional connectivity, akin to patterns found in neural networks and certain biological foraging behaviors.
1 / 1
so mainstream physics separates the

subject side from the target side of

observation something which are in

Schrodinger expressed like this without

being aware of it and without being

rigorously systematic about it we

exclude the subject of cognizance from

the domain of nature that we endeavor to

understand we step with their own person

back into the part of an onlooker who

does not belong to the world the world

which by this very procedure becomes an

objective world now this method has

worked very well for small subsystems of

nature like Galileo's favorite object of

study brass balls but it breaks down

when trying to extrapolate it to nature

as a whole because it cannot place

ourselves in our measurement equipment

outside of the universe

instead of being outside our target of

observation we are in fact inside the

world to be observed or actually

seamlessly embedded parts of it active

participants in a world just as active

as we ourselves coworkers in the process

of nature in stark contrast our familiar

method of external observation basically

treats the world as a collection of

external objects this however does not

reflect how nature hangs together as one

integrated process and therefore it

comes with a fair amount of problems

the first problem already appears quite

soon in the measurement stage mainstream

physics starts out by singling out some

process of interest P separate from the

measuring instrument and the observer

but ultimately it's impossible to

determine where the actual separation

between the target and subject side is

to be drawn it can be drawn between

process and measuring instrument between

instrument and observer between the eyes

and the brain between the brain and a

perception of process P or even between

a perception of process P and its

physical equation which is arguably

located in the abstract world of

mathematical ideas now despite the

serious ambiguity of the target subject

split it still stood the test of time

and from Galileo onward has given us

many empirically adequate physical

equations it worked so well in fact that

is often forgotten that our measurement

data and sensory data are actually not

fully complete carbon copies of the

target of observation itself but because

of the success of expressing our

observations in terms of numerical data

we started to think of observation in an

info computational manner as the mirror

registration of mathematically tractable

computational information accordingly when we look at this linear measurement

sequence from the left hand side it can

be likened with classical information

theory going from information source to

recipient and when we look at the right

hand side of the sequence from

measurement instrument to physical

equation we can recognize the format of

algorithmic information theory which is

all about compressing empirical data

into suitable algorithms
1 / 1
ChatGPT
Mainstream physics traditionally separates the subject from the target of observation, creating an objective world view, a method expressed by Erwin Schr��dinger. This approach has proven useful for studying isolated systems like Galileo's brass balls but fails when applied to nature as a whole. It does not reflect the reality that observers and their equipment are part of the universe, making them active participants in nature, rather than detached onlookers.
The separation between target and subject is ambiguous and challenging to define. It could be drawn at various points between the process of interest and its abstract mathematical representation. Despite this ambiguity, this method has stood the test of time, providing many accurate physical equations.
The success in expressing observations as numerical data has led to thinking of observation in an info-computational manner, mirroring registration of mathematically tractable information. This approach relates to classical information theory, focusing on the linear measurement sequence from information source to recipient, and algorithmic information theory, compressing empirical data into suitable algorithms. Yet, it may not reflect how nature operates as an integrated whole, potentially leading to problems in the understanding of the universe.
1 / 1
process physics doesn't give a reproduction formula for empirical data like our conventional physics does but it basically simulates the process you ala T of nature by modeling it's be able x' that is by modeling nature's events and their relational crosstalk so to say and all of that by means of a relational moana dalla ji next in line is that mainstream physics typically tries to reduce all of nature to inert bits of matter in motion whereas process physics treats nature as an integrated web of organismic relations finally because of its belief in reductionism mainstream physics treats conscious experience as an epiphenomenon or even as entirely illusory in process physics however subjectivity is at rom����rio inherent aspect of nature that is the emergent activity patterns in the process monad ology seem to come where they collectively grow preference of how to connect among each other so this can be thought of as proto subjective and biomimetic because it seems to exhibit the same selectional connectivity as for instance in neural networks and slime all foraging patterns so mainstream physics separates the subject side from the target side of observation something which are in Schrodinger expressed like this without being aware of it and without being rigorously systematic about it we exclude the subject of cognizance from the domain of nature that we endeavor to understand we step with their own person back into the part of an onlooker who does not belong to the world the world which by this very procedure becomes an objective world now this method has worked very well for small subsystems of nature like Galileo's favorite object of study brass balls but it breaks down when trying to extrapolate it to nature as a whole because it cannot place ourselves in our measurement equipment outside of the universe instead of being outside our target of observation we are in fact inside the world to be observed or actually seamlessly embedded parts of it active participants in a world just as active as we ourselves coworkers in the process of nature in stark contrast our familiar method of external observation basically treats the world as a collection of external objects this however does not reflect how nature hangs together as one integrated process and therefore it comes with a fair amount of problems the first problem already appears quite soon in the measurement stage mainstream physics starts out by singling out some process of interest P separate from the measuring instrument and the observer but ultimately it's impossible to determine where the actual separation between the target and subject side is to be drawn it can be drawn between process and measuring instrument between instrument and observer between the eyes and the brain between the brain and a perception of process P or even between a perception of process P and its physical equation which is arguably located in the abstract world of mathematical ideas now despite the serious ambiguity of the target subject split it still stood the test of time and from Galileo onward has given us many empirically adequate physical equations it worked so well in fact that is often forgotten that our measurement data and sensory data are actually not fully complete carbon copies of the target of observation itself but because of the success of expressing our observations in terms of numerical data we started to think of observation in an info computational manner as the mirror registration of mathematically tractable computational information accordingly when we look at this linear measurement sequence from the left hand side it can be likened with classical information theory going from information source to recipient and when we look at the right hand side of the sequence from measurement instrument to physical equation we can recognize the format of algorithmic information theory which is all about compressing empirical data into suitable algorithms a hidden side effect of this linear info computational methodology is that our numerical data mathematical equations concepts categories symbols and names are typically Sonata mised with their target process P or with whatever it is that we decided to call P but this Sonata me between the two sides of the target subject split can only be imagined to exist after the split has been drawn on top of that the empirical agreement between measurement data and physical equation must always be forced by us on the basis of subjective decision-making within the measurement practice that is we subjectively choose which data points deviate too much and should be left out which observable quantities should be included in our physical equations which statistical formats should be employed etc etc so despite what this linear setup might imply our physical equations are not exact objective representations of some target process p werner heisenberg already mentioned this long ago when he said that what we observe is not nature in itself but nature exposed to our method of questioning always with an element of subjectivity this effectively means that our targets of observation do not objectively exist separately and independently from our means of observation this doesn't of course make all electrons and other elementary particles illusory or pure fabrications of the mind but it does mean that our concept of an electron is better thought of as a context-dependent figure of speech or a fad for all practical purposes as John Stuart Bell used to put it so at the end of the day it is a figure of speech for how the process you ality of nature plays out through the lens of empirical experience so the linear sequence of info computational representation is actually quite misleading after all no external representation will ever be able to give an exact one and once anatomy with the target side therefore it would be better to look for a method that does not portray the target side from the subject side of the split semiotics which by the way includes biosemiotics seems to be much better equipped to give a view from within that is it enables us to look at the empirical experience not as a linear sequence but as a cyclic process well embedded within the greater process of nature itself hence we do not get to know nature by taking in empirical and sensory data from the outside of it but instead we make sense of nature from the inside by living through it therefore the process of going through a semiotic cycle is a far better way to depict scientific observation in a preparation part the target process is singled out or soaked loose from its embedding environment after which it can be submitted to observation further on down the line where it will have its impact on the sign interpreting observer furthermore the formalization part of the semiotic cycle deals with the ever more precise formulation of physical equations going into more detail the formalization process runs approximately like this there's a table of raw measurement outcomes it is typically identified with system states 1 2 3 and so on thus basically making them synonymous with the natural system that we are trying to capture mathematically it is supposed that the system evolves from one state to the next we can then try to model this with an algorithm accordingly possibly relevant member parameters have to be chosen the raw data have to be encoded into a carefully selected algorithm that fits these data after which this algorithm must be decoded again to check for empirical agreement with any future data when we put the subs subsequent system States on top of each other we get a sequence of system States or data entries accompanied by the outcomes produced by algorithm this sequence can then be interpreted as a time series and whenever a mathematical relation is found that agrees well enough with the past present and future empirical data we can safely speak of a well matured physical equation like for instance Newton's second law or Galileo's equation here oh and all a lot of subjective choices involved regarding which parameters to include into the algorithm choosing acceptable margins of error choosing which criteria should be the leading ones when it comes to algorithm choice algorithm selection and which statistical format to use etcetera etc however once a physical equation has reached full maturity and social acceptance the entire semiotics cycle and especially the role of subjective choice is banished to the background so what is left is basically the idea that nature can be equated or synonymous with the data and algorithms that come from empirical observation and both the semiotics cycle and any post algorithmic interpretation are typically pushed to the background in favor of no interpretation at all or in other words the so called shut up and calculate approach also known as instrumental ism like this mainstream physics simply becomes the act of turning all of nature into mere mathematics and it basically reasons away all acts of interpretation that ultimately make this mathematize ation possible at all likewise it also renders irrelevant the feedback loop that runs from post algorithmic interpretation to inspire the context of views with new possibilities all in all this denial of pre and post algorithmic interpretation allows mainstream physics to declare itself as fundamental by portraying nature as a real-world equivalent of its mathematics but in reality it is always preceded by pre algorithmic interpretation and subjective choice from this alternative perspective this pre algorithmic interpretation or metaphysics which is parts partly based on intuition and creative decision-making should then automatically be considered more fundamental than the physical equations that it helps put together in other words although mainstream physics typically supposes that our physical equations should ultimately be able to spell out nature in its full entirety it overlooks the fact that it's mathematical formulations can never be fundamental because there is always pre algorithmic interpretation that necessarily precedes it this Den tells us that the mathematical map should not be confused with the territory to be mapped just as observables cannot ever reach the status of be able so what does process physics offer as an alternative to avoid all these problems in order to illustrate this we can look at a possible scenario for biological evolution - coming into actuality of life from a prebiotic soup such a primordial soup can initially be a rather low grade background with nothing much happening but according to Stuart Kauffman and others an autocatalytic network again spontaneously sprang into being under the right circumstances for instance when there's enough chemical diversity within the superfluid Kaufmann holds that a collectively autocatalytic network can spring up from the lower great background after which further complexification can occur now in the process physics model something similar happens from an initially under free undifferentiated ground level of irregular noisy activity patterns a network of higher-order activity can lift itself into actuality through collective low-grade background process you allottee now mainstream physics claims more or less that our natural universe is called into being by laws of nature that are just given with no deeper origins of their own process physics on the other hand suggests that Nature has come into actuality from a largely undifferentiated background from a void like pre space that initially has no explicit connectivity or pattern formation to it this can be interpreted as a primordial vacuum like state like in this picture closer up however this vacuum like state may be seen as a fiercely fluctuating ocean of potential which contains all of existence in latent form this is actually very similar to what is believed in quantum field theory so we now know that proces physics does not start out with pre-existing laws of nature but with routine of nature it does so by setting up a modded ology matrix in which an initially panelist network of relations gradually starts to exhibit more higher-order pattern formation here we can see that this routine is written as an iterative update routine that indexes connection strength in the relational matrix that makes up the Monodology so this relational matrix forms an indexical grid of emergent connectivity with each iteration the update routine not only gives rise to the kind of system-wide cross connectivity but it can also be thought of as letting its connectivity strength all over the system fluctuate ever so slightly in the noisy manner this reflects the partial uncertainty about what is going on elsewhere within the system so like this the relational matrix starts evolving from an initially patternless pre geometric vacuum-like stage and as the matrix runs through its update cycles again and again each time it basically adds a layer of noise over all individual connection strengths within the matrix the preceding connection strengths of all member nodes in the relation matrix are represented by bij old which is the president's term looking like this from up close following two terms are the binding term or cross linkage term which can be thought of as a realization of machs principle and the novelty infusing noise term these two terms mostly cancel each other out but in the long run there will be enough reactive low-grade activity patterns to enable the emergence of a complex layout were branching network of higher order process structures as is illustrated on the extreme left hand side well at least the humble beginnings of it as the system goes through its iterations the precedents turn the cross linkage term and the noise term together will give rise to a constantly fluctuating landscape of connection strengths some entries become large because of the effect of the iterative update routine these large connection strengths tend to hook up together to form islands of elevated connectivity like this and when considering only the islands with large valued connections we can submit them to statistical analysis and see what global pattern there is to tower behavior the statistical analysis basically amounts to counting the total number of member notes in those islands then pick one reference node and see how many neighbors are nearest neighbors how many second nearest neighbors how many are three connections away and so on now the distance to strengths ratio for connectivity notes tells us that the overwhelming majority is formed by weak short distance connections and only a minut fraction is made up by strong long-distance connections in the islands of elevated connectivity this translates into short distance
1 / 1
ChatGPT
It seems like your message is an extensive description of process physics and how it contrasts with conventional physics. However, the text got cut off before it could conclude.
Process physics, as described in your text, presents an alternative perspective to conventional physics. While conventional physics often reduces nature to inert matter and tends to treat consciousness as an epiphenomenon, process physics sees nature as an integrated web of relations, recognizing subjectivity as an inherent aspect of nature.
The model you described appears to be more holistic and considers the universe as an evolving entity, where initially patternless networks gradually evolve to exhibit higher-order patterns. The complexity of the model embraces uncertainty, noise, and connectivity, moving away from the linear info computational methodology seen in mainstream physics.
If there's a specific question or part of this topic you'd like to know more about, please let me know!
1 / 1
Here is the conclusion: local connections being the most probable ones which then automatically leads to tree graph like branching structures as displayed above the table since this will be the most probable configuration that will occur the connectivity nodes within these branching structures will most likely organize themselves into a near three-dimensional distribution relative to one another that is the branching structures will end up getting the same distance distribution among their nodes as uniformly arranged points in a 3-dimensional space this can be found because the number of neighbors for our chosen reference node turns out to increase in proportion to the square of the number of steps away and this phenomenon is elsewhere only seen in three-dimensional spaces so what we get here is emergent three dimensionality from initial known connectivity driven by iterative stochastic routine the different branching structures basically grow to become embeddable in 3d as cell like sub networks looking somewhat like this although this artistic impression should be a bit more realistically looking what is also quite remarkable and should not be forgotten is that growth and decay of network connections occurs as the continuous addition of noisiness strengthens and weakens earlier growing islands of connectivity and their branching structures as this goes on for long enough each locality within the network will become so much correlated with the rest of the system that it acquires a local sense of how to contribute to global system preserving criticality is then saves the system from falling into chaos or at the other extreme end getting stuck in static water like this we may say that there is a local sense of global systemic self-preservation which can be seen as a very rudimentary sense of subjective decision-making through selectional connectivity as such the network as a whole can be seen to have directionality that is it exhibits a tendency to develop towards increasing complexity process ecologist Robert Ilana likes to call this ascendancy a feature which is also characteristic of autocatalytic networks and ecosystems you although this is a short time lapse of the surface of the Sun it illustrates quite well how to sell like branching structures may act together as an integrated dynamic network with individual cells coming in and out of actuality mind you this is just an analogy to help visualize things next to this complexity seeking behavior the network exhibits emergent relativistic inertial and gravitational effects emergent near classical behavior creative novelty inherent time like process well 'ti with open-ended evolution and many other features that we can also find in nature itself so to recap the process physics model evolves firstly from an initially patent l'esprit space where there is no manifest patterning towards a three-dimensional early universe with a relatively uniform distribution of matter in the long enough run then this will eventually take on the shape of a complex neural network like cosmic web when starting with the Cosmic Microwave Background beyond which we cannot make any observations it evolves like this running from initial near uniformity to the current situation of a neural network shaped cosmic web in animated motion the neural network like organization of the universe at a super galactic scale looks as follows this is a short piece taken from the Millennium simulation which is a large scale supercomputer simulation of the universe you from all this we can draw several conclusions namely process physics is an utterly ecological way of doing physics and that it does not split up its universe of discourse into target system subject system and their external environment furthermore process physics is organismic rather than mechanistic mutually informative rather than based on mere numerical data it aims to set up a more be able like modeling of nature instead of portraying its observables also via what may be called routine of nature it is habit establishing instead of being governed by pre available laws of nature and on top of all that it is also noise driven so that creative novelty becomes possible as an alternative for strict determinism all this however does not mean that we should get rid of our conventional nature dissecting physics as soon as possible instead I'd like to make the case that we need a binocular physics in which our conventional way of doing physics in a box and process physics become each other's frame of reference mainstream physics when it tries to mathematize are designated natural systems should be checked against what process physics has to say just as process physics has a modeling of nature as a whole should check its results against our conventional physics finally then process physics is based on what may be called foundations without foundation that is the process physics monadology models nature as if it were an emergent autocatalytic network that is lifted into actuality by the collective mutually upward nudging fluctuations of little level stochastic background activity so instead of being a limited to modeling nature at the level of observables as our contemporary physics does prosess physics is basically a large step closer to a be able like modeling of nature and this may very well free us from a lot of persistent troubles in the foundations of physics thank you very much for your attention if you have any questions just drop me an email using the address below have a nice day bye bye
1 / 2
ChatGPT
It seems you've provided a comprehensive summary or conclusion about a complex subject related to process physics, emergent three-dimensionality, network connections, and various phenomena that might be observed in a dynamic system. Your description touches on concepts like the organization of the universe, ecological physics, and systemic self-preservation.
Is there a specific question or topic you'd like to explore further or clarify within this content? I'm here to help!
1 / 1
It was a lecture:

Process Physics: An organismic
neo-Whiteheadian physics
(International Whitehead Conference
2017)

https://youtu.be/DB-jktr-BrI?si=tmS7aEfTXu5GQ5ue

12
Likes
503
Views
2020
Jul 19
Universidad de las Azores

Process Physics is a new way of doing physics
that has been developed by Australian professor
of physics Reg Cahill and his co-workers. It very
much agrees with Lee Smolin's line of reasoning
(2019) that our modeling of nature should be a
relational monadology, just as envisioned by
Newton's main opponent Gottfried Leibniz.
Smolin holds that a lot of our problems in
contemporary physics come from our Newtonian
way of 'doing physics in a box'. This is a way of
doing physics which isolates our target of
observation from the rest of the universe
(including the observer) and then problematically
extrapolates its findings to nature-as-a-whole.
In contrast, Process Physics can be
characterized as a neo-Whiteheadian,
habit-centered, biocentric way of doing physics
without a box. It starts out with an initially
undifferentiated homogeneity of noisy,
self-organizing background processuality which
gradually turns out to give rise to an ever-more
complex network of dynamically evolving
relationships. It does so by setting up a
stochastic, self-reference-based modeling of
nature in which all self-referential and initially
noisy activity patterns are 'mutually in-formative'
in the sense that they are actively making a
meaningful difference to each other (i.e
in-forming or 'actively giving shape to each
other). In this way, the system evolves from its
initial featurelessness to then 'branch out' to
higher and higher levels of complexity - all this
according to roughly the same basic principles as
naturally developing neural networks or slime
mold foraging patterns (Burchett et al. 2020).
Because of this self-organizing and noise-driven
branching behaviour, the thus emerging relational
network can be thought of as habit-bound with a
potential for creative novelty and open-ended
evolution. Furthermore, three-dimensionality,
(quasi-)classical behaviour, and gravitational,
relativistic and inertial effects are spontaneously
emergent features within this evolving web of
interrelations. Also, the network's constantly
renewing activity patterns bring along an inherent
present moment effect, thereby reintroducing
time as the system's becomingness'
As a final point, subjectivity - in the form of
mutual informativeness' (which is also used in
Gerald Edelman's and Giulio Tononi's extended
theory of neuronal group selection to explain how
higher-order consciousness can emerge) - is a
naturally evolving, innate feature, not a
coincidental, later-arriving side-effect or epiphenomenon.
1 / 1
ChatGPT
Thank you for providing additional context about the lecture. It's clear that the subject matter pertains to a new approach in physics known as Process Physics, developed by Australian professor Reg Cahill and his colleagues. This approach takes a neo-Whiteheadian, biocentric perspective that focuses on relational monadology, contrasting the conventional, Newtonian method of isolating targets of observation.
The lecture seems to cover a broad range of topics, from the initial undifferentiated homogeneity of processes to the emergence of complexity, relational networks, three-dimensionality, and even the concept of subjectivity within this framework.
Is there something specific about Process Physics or related topics that you'd like to know more about or discuss further?
1 / 1
I asked Bing to give me an analysis of the references you can tell me how it ties into the themes of the talk.
1 / 1
ChatGPT
Certainly! However, since I don't have access to Bing's search results, I won't be able to directly analyze the references from their search. But based on the description of the lecture and the provided information, I can discuss how some concepts and themes tie into established scientific ideas and philosophies.
Neo-Whiteheadian Physics
: Alfred North Whitehead's philosophy is known for its emphasis on process and relationality. The Process Physics approach seems to build on this idea, moving away from isolated, static entities to focus on the dynamic relationships and processes that give rise to complexity.
Gottfried Leibniz and Monadology
: Leibniz's monadology is about simple substances that interact without physically affecting each other. In Process Physics, the idea of relational monadology could represent entities that interact through information or relationships, rather than physical causation.
Lee Smolin's Line of Reasoning
: Lee Smolin has worked on relational approaches to physics and critiqued conventional methods. This seems to resonate with the Process Physics model's emphasis on relationships and breaking down the isolated "doing physics in a box" method.
Biocentrism
: The biocentric approach views life and consciousness as central to understanding the universe. This is reflected in Process Physics through the emphasis on subjectivity and mutual informativeness, suggesting an interconnected universe where consciousness isn't a mere side effect.
Gerald Edelman's and Giulio Tononi's Theory of Neuronal Group Selection
: This theory attempts to explain how higher-order consciousness emerges. The Process Physics model seems to incorporate similar ideas by suggesting that mutual informativeness could lead to higher-order complexity, possibly even subjectivity.
Slime Mold Foraging Patterns and Neural Networks
: The mention of naturally developing neural networks and slime mold foraging patterns connects Process Physics to biological systems. This connection might imply that the same underlying principles can be applied to both physical and biological systems.
Noise-Driven Branching Behavior
: The emphasis on stochastic, noise-driven behavior may resonate with chaos theory and complex systems theory, where small perturbations can lead to significant changes in a system's behavior.
Without specific references from Bing, these connections are based on the information you provided and well-known scientific and philosophical ideas that seem to relate to the themes of the lecture. Let me know if you'd like more information on a specific aspect or theme!
1 / 1
It's still writing the answer. It'll be just a second.
1 / 1
ChatGPT
Certainly! Take your time, and please let me know if there's anything specific you'd like to know or ask once you have the information. I'll be here to help!
1 / 1
Sure, I can give you a brief summary of each of these articles. Here they are:

- Joseph N. Burchett et al. "Revealing the dark threads of the cosmic web." The Astrophysical Journal Letters, Vol. 891, March 10, 2020, p. L35. doi:10.3847/2041-8213/ab700c.

This article reports on a new method to reconstruct the cosmic web, the network of filamentary structures that connect galaxies and span the universe. The method is inspired by the growth patterns of a slime mold, and uses galaxy surveys and absorption-line spectroscopy to infer the density field of the cosmic web. The authors show that their method can reveal the distribution and properties of the intergalactic medium (IGM), the diffuse gas that fills the space between galaxies. They find that the IGM is mostly located in the outskirts of cosmic web filaments, and that it is suppressed in the densest regions due to shock-heating and ionization. ��

- Lee Smolin, Time Reborn: From the Crisis of Physics to the Future of the Universe. London: Allen Lane, 2013.

This book is a philosophical and scientific exploration of the nature and role of time in physics and cosmology. Smolin argues that time is real and fundamental, contrary to the prevailing view that time is an illusion or an emergent phenomenon. He criticizes the existing theories of quantum mechanics and relativity for being incomplete and incompatible, and proposes alternative approaches that restore time to its central position. He also discusses the implications of his views for human agency, free will, and social and environmental issues. ��

- Einstein's Unfinished Revolution: The Search for What Lies Beyond the Quantum. New York: Penguin Press, 2019.

This book is a continuation of Smolin's quest for a deeper understanding of quantum mechanics and its relation to reality. Smolin challenges the orthodox interpretation of quantum mechanics, which he considers to be paradoxical, mysterious, and unsatisfactory. He advocates for a realist approach that seeks to discover the underlying causes and mechanisms of quantum phenomena, rather than accepting them as probabilistic and indeterminate. He reviews various existing proposals for such a realist theory, such as hidden variables, pilot waves, spontaneous collapse, and quantum gravity, and offers his own suggestions for future directions. ��

- Reginald T. Cahill and Christopher M. Klinger. Pregeometric Modelling of the Spacetime Phenomenology." Physics Letters A., 223.5 (1996): 313-319

This article presents a novel framework for modelling spacetime as a self-referential system that generates intrinsic randomness. The authors draw on ideas from G��del and Chaitin in mathematics to argue that self-referential systems are inherently unpredictable and incomputable. They propose a pregeometric model that does not assume any classical or geometric structures, but rather derives them from an order-disorder process driven by self-referential noise. They show how this model can reproduce some features of general relativity and quantum mechanics, such as gravity, inertia, light propagation, and wave-particle duality. ���

- "Self-Referential Noise and the Synthesis of Three-Dimensional Space." General Relativity and Gravitation 32.3 (2000): 529-540.

This article is a follow-up to the previous one, in which the authors further develop their pregeometric model and apply it to explain the emergence of three-dimensional space from a non-geometric system. They use a cellular automaton as an example of a self-referential system that exhibits order-disorder transitions and generates self-referential noise. They show how this noise can induce correlations between cells that result in an effective metric tensor that describes a three-dimensional space with curvature and torsion. They also discuss how this model can account for some cosmological observations, such as the expansion of the universe and the cosmic microwave background radiation. ���

- "Self-Referential Noise as a Fundamental Aspect of Reality" Proceedings of
the 2nd International Conference on Unsolved Problems of Noise and
Fluctuations. New York: American Institute of
Physics, 2000

This article is a summary of the main ideas and results of the pregeometric model proposed by Cahill and Klinger in their previous papers. It emphasizes the role of self-referential noise as a source of intrinsic randomness and complexity in nature, and as a possible explanation for various physical phenomena that are usually attributed to quantum mechanics or relativity. It also outlines some open questions and challenges for further research on this model, such as testing its predictions experimentally, extending it to include matter fields, and exploring its philosophical implications for realism and determinism. ���

- "Bootstrap Universe from Self-Referential Noise." Progress in Physics 2 (2005): 108-112.

This article is a brief update on the pregeometric model, in which the authors report some new developments and applications of their approach. They introduce a new concept of a bootstrap universe, in which the self-referential noise that generates space and time also acts as a source of energy and matter. They show how this idea can address some of the outstanding problems in cosmology, such as the origin of the big bang, the nature of dark energy and dark matter, and the fine-tuning of physical constants. They also suggest some possible experimental tests for their model, such as measuring the variation of the speed of light and the gravitational constant. ���

- Reginald T. Cahill, Christopher M. Klinger, and Kirsty Kitto. "Process Physics: Modelling Reality as Self-Organising Information" The Physicist 37.6 (2000): 191-195.

This article is an overview of a more general framework called process physics, which encompasses the pregeometric model as a special case. Process physics is based on the idea that reality is not composed of static entities or structures, but rather of dynamic processes that are constantly evolving and interacting. Information is the fundamental concept in this framework, and it is defined as a measure of order or pattern in a system. Process physics aims to model reality as a self-organising information system that generates its own laws and structures from self-referential noise. The authors describe some of the main features and applications of this framework, such as modelling space, time, matter, gravity, quantum phenomena, and consciousness. ���

- Jlanowicz, Robert. The Ascendent Perspective New York: Columbia UP 1997.

This book is a philosophical and ecological critique of the dominant scientific worldview that is based on reductionism, mechanism, and materialism. Jlanowicz argues that this worldview is inadequate and harmful for understanding and addressing the complex and interrelated problems that face humanity and the planet. He proposes an alternative perspective that he calls the ascendent perspective, which is based on holism, emergence, and information. He claims that this perspective can better account for the diversity, creativity, and evolution of natural systems, as well as their ethical and aesthetic value. He applies this perspective to various fields and issues, such as biology, thermodynamics, economics, politics, and spirituality. ���

- The Third Window: Natural Life Beyond Newton and Darwin. West Conshohocken, PA: Templeton Foundation Press, 2009

This book is a sequel to The Ascendent Perspective, in which Jlanowicz further develops his critique of the conventional scientific paradigm and his proposal for a new one. He focuses on the concept of natural life, which he defines as the capacity of natural systems to create order out of disorder through self-organisation and information processing. He argues that natural life is not adequately explained by the laws of physics or by the theory of evolution by natural selection, which he considers to be incomplete and limited. He suggests that natural life requires a third window of understanding, beyond Newton's mechanics and Darwin's biology, that can reveal its intrinsic purpose, meaning, and value. He explores some possible candidates for this third window, such as complexity theory, information theory, thermodynamics, quantum mechanics, and process philosophy. [^10^]

Source: Conversation with Bing, 2023-08-24
(1) [2003.04393] Revealing the Dark Threads of the Cosmic Web - arXiv.org. https://arxiv.org/abs/2003.04393.
(2) Astronomers use slime mold model to reveal dark threads of the cosmic web. https://phys.org/news/2020-03-astronomers-slime-mold-reveal-dark.pdf.
(3) How slime mold helped scientists map out the cosmic web - Science News. https://www.sciencenews.org/article/how-slime-mold-helped-scientists-map-cosmic-web-galaxies.
(4) Sci-Hub | Revealing the Dark Threads of the Cosmic Web. The .... https://sci-hub.se/10.3847/2041-8213/ab700c.
(5) Time Reborn - Wikipedia. https://en.wikipedia.org/wiki/Time_Reborn.
(6) Time reborn : from the crisis of physics to the future of the universe .... https://archive.org/details/timerebornfromcr0000smol_p6f9.
(7) Time Reborn: From the Crisis in Physics to the Future of the Universe .... https://books.google.com/books/about/Time_Reborn.html?id=O3z1nXjcDu4C.
(8) Einstein's Unfinished Revolution - Wikipedia. https://en.wikipedia.org/wiki/Einstein's_Unfinished_Revolution.
(9) Einstein's Unfinished Revolution: The Search for What Lies Beyond the .... https://www.amazon.com/Einsteins-Unfinished-Revolution-Search-Quantum/dp/1594206198.
(10) Einstein's Unfinished Revolution: The Search for What Lies Beyond the .... https://books.google.com/books/about/Einstein_s_Unfinished_Revolution.html?id=YnxqDwAAQBAJ.
(11) Einstein's Unfinished Revolution: The Search for What Lies Beyond the .... https://www.goodreads.com/book/show/40922833-einstein-s-unfinished-revolution.
(12) undefined. https://doi.org/10.48550/arXiv.2003.04393.
(13) undefined. https://doi.org/10.3847/2041-8213/ab700c.
(14) undefined. https://en.wikipedia.org/wiki/Einstein.
(15) Pregeometric modelling of the spacetime phenomenology. https://arxiv.org/abs/gr-qc/9605018.
(16) Pregeometric modelling of the spacetime phenomenology. https://archive.org/details/arxiv-gr-qc9605018.
(17) Title: Pregeometric modelling of the spacetime phenomenology. http://export.arxiv.org/abs/gr-qc/9605018v1.
(18) Self-Referential Noise and the Synthesis of Three-Dimensional Space. https://arxiv.org/abs/gr-qc/9812083.
(19) Self-Referential Noise and the Synthesis of Three-Dimensional Space .... https://link.springer.com/article/10.1023/A:1001984518976.
(20) Self-Referential Noise and the Synthesis of Three-Dimensional Space. https://arxiv.org/pdf/gr-qc/9812083v1.pdf.
(21) undefined. https://doi.org/10.48550/arXiv.gr-qc/9605018.
(22) undefined. https://doi.org/10.1016/S0375-9601%2896%2900775-X.
(23) undefined. https://doi.org/10.48550/arXiv.gr-qc/9812083.
(24) undefined. https://doi.org/10.1023/A%3A1001984518976.
(25) undefined. https://doi.org/10.1023/A:1001984518976.
1 / 1
ChatGPT
Thank you for providing the summaries! These references cover a broad range of topics, from the cosmic web and quantum mechanics to philosophical interpretations of physics.
Here's how these references might relate to the themes of the talk, based on the context provided:
Exploration of the Universe
: The papers by Burchett et al. and the work by Cahill and Klinger deal with the fundamental structures of the universe, exploring novel ways to understand cosmic phenomena such as the cosmic web, spacetime, and even the origin of the universe itself.
Understanding Time and Quantum Mechanics
: Smolin's works are focused on the nature of time and the shortcomings of current quantum mechanics interpretations. These themes resonate with the philosophical exploration of physics, challenging conventional ideas and proposing new paradigms.
Self-Referential Systems and Non-Geometric Models
: The pregeometric model papers and the concept of self-referential noise introduce innovative approaches to understanding spacetime, randomness, and complexity, offering insights that might bridge gaps between general relativity and quantum mechanics.
Holistic and Ecological Perspectives
: Jlanowicz's works critique reductionism and advocate for a more holistic and emergent understanding of natural systems. This approach aligns with themes of interconnectivity and ethical considerations in scientific research.
Unification of Diverse Ideas
: Overall, the collection of references seems to present a cohesive attempt to understand reality from different perspectives. This includes integrating ideas from biology, thermodynamics, economics, politics, and spirituality, representing a multidisciplinary approach to probing the deepest questions about the nature of existence.
These themes collectively point to a talk that could be exploring new frontiers in physics, challenging conventional wisdom, and seeking to unify various strands of thought in a cohesive, holistic understanding of reality. If you could provide more specific details about the talk's content or focus, I could further refine this analysis.
1 / 1
A list of all of the topics we have discussed so far.
1 / 1
ChatGPT
Certainly! Here's a list of the topics we've discussed so far:
Revealing the Cosmic Web
Method to Reconstruct the Cosmic Web
Intergalactic Medium (IGM) and Cosmic Web Filaments
Philosophical and Scientific Exploration of Time
Critique of Quantum Mechanics and Relativity
Realist Approach to Quantum Mechanics
Modelling Spacetime as a Self-Referential System
Pregeometric Models of Spacetime Phenomenology
Self-Referential Noise and the Emergence of Space
Bootstrap Universe Concept
Process Physics and Self-Organising Information
Philosophical and Ecological Critique of Scientific Worldview
Ascendent Perspective and Holistic Understanding
Exploration of Natural Life Beyond Newton and Darwin
Themes Related to the Talk, Including Exploration of the Universe, Understanding Time and Quantum Mechanics, Self-Referential Systems, Holistic Perspectives, and Unification of Diverse Ideas
Please let me know if you'd like any further information or clarification on any of these topics!
1 / 1
Ok that is good, just before that, the talk we discussed covered the following topics: 

Process Physics: An alternative to conventional physics that simulates nature's events and their relational crosstalk.
Nature as an Integrated Web: Process physics treats nature as an interconnected web rather than inert bits of matter.
Consciousness in Physics: While conventional physics often ignores or downplays consciousness, process physics considers it an inherent part of nature.
Reductionism vs. Organismic Relations: The difference in treating nature either as separate parts or as an integrated whole.
Measurement and Observation in Physics: The challenges and subjectivity involved in drawing the line between observer and observed.
Algorithmic Information Theory: The use and potential misapplication of mathematics and algorithms in understanding nature.
Semiotics in Science: The cyclic process of understanding through semiotics as an alternative to linear representation.
Subjective Choices in Mathematical Modeling: How subjective decisions shape the formulation of physical equations.
Instrumentalism in Physics: The "shut up and calculate" approach and its implications.
Pre-algorithmic Interpretation: The idea that mathematical equations are not fundamental, as they are preceded by interpretation.
Biological Evolution Analogy: Comparing the emergence of complex systems in process physics to the emergence of life.
Routine of Nature vs. Laws of Nature: How process physics starts with routines, not predefined laws.
Emergence of Complex Patterns: The evolution of initially patternless networks into higher-order structures through iterations.
These points together outline a broad and complex perspective on physics that challenges conventional understanding, emphasizing a more integrated, subjective, and process-oriented approach to understanding the universe.
1 / 1
ChatGPT
Absolutely, your summary excellently captures the key topics and themes from the talk we discussed on Process Physics and its contrasts with conventional physics. This holistic approach considers interconnectedness, consciousness, subjectivity, and the emergence of complex patterns as fundamental to understanding the universe. It offers a fascinating perspective that indeed challenges traditional ideas and methods in physics. If you have any more questions or need further elaboration on any of these points, please let me know!
1 / 1
Right now the title of the conversation is Holistic vs Dissected Physics
1 / 1
ChatGPT
That title accurately encapsulates the essence of our discussion, contrasting the holistic view of nature in process physics with the more dissected or reductionist approach in conventional physics. It's a fitting way to describe the exploration of these two paradigms and how they approach understanding the universe. If you'd like to explore more details or specific areas within this subject, just let me know!
1 / 1
Connections with this: 

(Neuroscience & Psychology)

The Self-Assembling Brain: How Neural Networks Grow Smarter

Peter Robin Hiesinger

What neurobiology and artificial intelligence tell us about how the brain builds itself


Look Inside
Hardcover
Published (US):
May 4, 2021
Published (UK):
Jun 8, 2021
Copyright:
2021
Pages:
384

How does a neural network become a brain? While neurobiologists investigate how nature accomplishes this feat, computer scientists interested in artificial intelligence strive to achieve this through technology. The Self-Assembling Brain tells the stories of both fields, exploring the historical and modern approaches taken by the scientists pursuing answers to the quandary: What information is necessary to make an intelligent neural network?

As Peter Robin Hiesinger argues, ���the information problem��� underlies both fields, motivating the questions driving forward the frontiers of research. How does genetic information unfold during the years-long process of human brain development���and is there a quicker path to creating human-level artificial intelligence? Is the biological brain just messy hardware, which scientists can improve upon by running learning algorithms on computers? Can AI bypass the evolutionary programming of ���grown��� networks? Through a series of fictional discussions between researchers across disciplines, complemented by in-depth seminars, Hiesinger explores these tightly linked questions, highlighting the challenges facing scientists, their different disciplinary perspectives and approaches, as well as the common ground shared by those interested in the development of biological brains and AI systems. In the end, Hiesinger contends that the information content of biological and artificial neural networks must unfold in an algorithmic process requiring time and energy. There is no genome and no blueprint that depicts the final product. The self-assembling brain knows no shortcuts.

Written for readers interested in advances in neuroscience and artificial intelligence, The Self-Assembling Brain looks at how neural networks grow smarter.

https://press.princeton.edu/books/hardcover/9780691181226/the-self-assembling-brain
1 / 1
ChatGPT
The book "The Self-Assembling Brain" by Peter Robin Hiesinger explores themes and questions that are indeed connected with our earlier discussion on holistic vs. dissected physics.
Emergence of Complex Patterns
: Both in process physics and the self-assembling brain, we see complex patterns emerging from initially simpler components. In process physics, the complex systems emerge through iterations, while in neurobiology, the brain builds itself through a dynamic process of self-organization.
Algorithmic Processes
: Our discussion touched on algorithmic information theory, which relates to the algorithmic process Hiesinger refers to in the development of both biological brains and AI systems.
Reductionism vs. Integration
: The exploration of whether the brain is merely "messy hardware" that can be replicated through AI relates to the holistic view of nature as an interconnected web, compared to the dissected or reductionist perspective that views the parts separately.
Consciousness and Intelligence
: The ambition to understand how neural networks grow smarter aligns with our conversation about including consciousness as an inherent part of nature, rather than something to be ignored or downplayed.
Subjective Choices and Interpretations
: The book's exploration of different disciplinary perspectives and approaches echoes our discussion about subjective choices in mathematical modeling, showing how different viewpoints can shape our understanding of complex phenomena.
Overall, the book adds a fascinating layer to our discussion, emphasizing that the questions of emergence, complexity, and intelligence are not limited to physics but extend into the realms of neuroscience, psychology, and artificial intelligence. It could provide a rich basis for further exploration of these interconnected topics.
1 / 1
One part I found interesting: We will explore how information is stored
in biological and artificial networks. How does the neural network save a
four-digit PIN? The amount of bits needed to store this information is clearly
defined in computational terms. Yet, neural networks save and retrieve this
information flexibly and robustly, even if random neurons in the network
drop out. In addition, the biological network does not have a separate
training and function period. Learning is inseparable from using the
network; storing is inseparable from retrieving information. And again, we
meet an evolutionary principle and the power of sequences in time. Many
bits of information in the biological network���memories���can only be
accessed by going through a sequence in time. Try saying your phone
number in reverse order. How is this information content programmed,
stored and retrieved?
1 / 1
ChatGPT
The passage you've highlighted is indeed fascinating, and it touches on several compelling concepts that relate to our previous discussions:
Information Storage in Neural Networks
: This challenges our conventional understanding of how information is stored and retrieved, especially when comparing biological to artificial networks. The idea that a neural network can save information like a four-digit PIN, and retrieve it robustly even with random neuron dropouts, opens up questions about the nature of memory and cognition.
Inseparability of Learning and Function
: The statement that learning is inseparable from using the network in biological systems is an intriguing departure from how we often conceptualize learning in artificial networks. This continuous integration of learning and function seems to point towards a more organic and process-oriented understanding, akin to the process physics perspective.
The Temporal Dimension
: The notion that many bits of information in the biological network can only be accessed through a sequence in time adds a rich layer to our understanding of memory. This resonates with the discussion about the emergence of complex patterns over time in process physics and the evolutionary principles we talked about.
The Complexity of Biological Systems
: This also highlights the intricate nature of biological systems compared to their artificial counterparts. While computational systems operate with clearly defined rules and structures, biological systems exhibit a level of flexibility, robustness, and complexity that isn't easily replicated in artificial networks.
Evolutionary Principles and Sequences
: The connection to an evolutionary principle and the power of sequences in time also links back to our analogy of biological evolution in understanding the emergence of complex systems.
The questions raised in this passage about how information content is programmed, stored, and retrieved in both biological and artificial systems are not only intriguing but could also lead to profound insights into the nature of intelligence, consciousness, and the fundamental processes that underlie both living organisms and technological constructs. It builds on and extends the themes of holistic understanding, interconnectedness, and the intricate dance between order and chaos that we have been exploring.
1 / 1
If self-assembly is any guide, then information has to enter by changing
the sequential, auto-associative network, which means it changes algorithmic
information. Maybe memories should not be understood as stored entities at
all, but rather as algorithmic rules sufficient to recreate them with a certain
robustness, flexibility and variability. This bring us back to the cellular
automaton that does not store the memory of the pattern at iteration 1,000,
but instead the information to recreate this state. We will explore to what
extent this process resembles algorithmic growth, and how it transitions
from development to function.
In Seminar 9: From Algorithmic Growth to Artificial Intelligence we
focus on artificial neural networks and their relationship to self-organization
and algorithmic growth. We will finally also discuss definitions of selfassembly and intelligence. Most ANNs are based on the idea of an
engineered design, flipping the on switch and training the network. By
contrast, in biological networks the information encoding goes hand in hand
with the development of the brain. The brains of a newborn, a toddler or a
10-year-old are clearly recognizable for their developmental stages
morphologically, functionally and by their behavioral output. The question is
whether a tedious, years-long process of self-assembly is a desirable step to
create an artificially intelligent system. More specifically, is there ever a
need to grow a neural network, or is training a predesigned network like in
deep learning sufficient, maybe equivalent, or even superior?
1 / 1
ChatGPT
This excerpt further deepens the exploration of the differences and similarities between biological and artificial neural networks, touching on these key ideas:
Memories as Algorithmic Rules
: The idea that memories might not be stored entities but algorithmic rules capable of recreating them echoes the themes from process physics and the interconnected web of nature. This view resonates with the concept of emergence, where complexity arises from simple rules rather than from static, stored information.
Algorithmic Growth and Self-Organization
: The notion of algorithmic growth in neural networks and the transition from development to function introduces a dynamic perspective. This parallels the process-oriented approach that recognizes the complexity of nature as an evolving system, rather than something that can be dissected into isolated parts.
Artificial vs. Biological Networks
: The comparison between artificial neural networks (ANNs) and biological networks highlights a divergence in methodology. ANNs typically focus on an engineered design that's trained, while biological networks intertwine information encoding with brain development. This dichotomy raises profound questions about the nature of intelligence and the paths to replicate or even surpass human-like cognition.
Growing vs. Training Neural Networks
: The question posed at the end about whether to grow a neural network or merely train a pre-designed one like in deep learning is thought-provoking. It reflects the broader debate between a more organic, developmental approach (resembling the holistic perspective) and a more mechanistic, reductionist one (analogous to dissected physics).
Desirability of Self-Assembly in AI
: By questioning if the self-assembly process is a desirable or necessary step to create AI, the excerpt prompts reflection on the fundamental principles that should guide the development of artificial intelligence. This connects to the broader philosophical and methodological considerations in both physics and neuroscience.
Overall, this passage illustrates a convergence of ideas across disciplines, reflecting on the intricate dance between design, growth, learning, and function. It contributes to our overarching theme of "Holistic vs. Dissected Physics" by introducing nuanced questions about the underlying principles of both natural and artificial systems, inviting contemplation of whether the complexity of the biological world can be captured or even improved upon through engineered designs.
1 / 1
There was a part that reminded me of something I said in a previous conversation: Ok this might be more contentious although there is some empirical support. Episodic memory and imagination are types of confabulation-like reconstructions or reenactments of scenes, created from a semantic template with current knowledge, and the scenario plays out differently if it is primed definitely. This helps explain how some individuals can lose their mental imagery capacity, sometimes without noticing, because they are still remembering the semantic information just not generating an accompanying visual scenario.
1 / 1
ChatGPT
Yes, the idea you've described about episodic memory and imagination being confabulation-like reconstructions or reenactments seems to resonate with the passage's notion that memories might not be stored entities, but rather algorithmic rules that recreate them. Both views challenge the traditional understanding of memory as a fixed and static storage system.
In the context of the previous excerpt, this perspective aligns with the idea that biological neural networks function through a series of flexible, robust, and variable rules that allow for the recreation of memories rather than their storage in a specific location. This dynamic approach to memory might help to explain phenomena such as losing mental imagery capacity, as you mentioned. Without the need to generate a concrete visual scenario, an individual can still access the semantic information and rules that underlie the memory.
It indeed adds a layer of complexity to our understanding of how memory functions, further connecting the ideas of neural networks, biological growth, and artificial intelligence to the broader theme of "Holistic vs. Dissected Physics." It emphasizes the importance of viewing these systems as fluid, evolving processes rather than fixed structures, in line with the more holistic perspective we've been discussing.
1 / 1
You said this:

Your point is indeed intriguing and delves into the complex relationship between episodic memory, imagination, and semantic understanding. Here's an expansion on this idea:

Episodic Memory and Imagination as Confabulation: Episodic memory, the recollection of specific personal experiences, is not a perfect replay of past events. Research suggests that these memories might be more akin to reconstructions or reenactments, pieced together using semantic templates and current knowledge. This view challenges the traditional understanding of episodic memory as a simple retrieval of a past snapshot.

Similarly, imagination, or the ability to visualize scenarios that haven't occurred, can be seen as a creative reconstruction using available semantic information. It's like a form of mental simulation guided by rules and principles rather than a specific visual ability.

Impact on Mental Imagery Capacity: Some individuals experience a loss of mental imagery capacity, a phenomenon known as aphantasia. What's fascinating is that this doesn't necessarily impair their ability to recall past events or imagine new ones. They still retain the semantic information ��� the facts, rules, and abstract concepts ��� even if they can't visualize them.

The Role of Priming: Priming, or the influence of prior exposure to stimuli on response to later stimuli, might play a significant role in how these reconstructions unfold. If a memory or imagined scenario is primed with specific information, it can steer the reenactment in a particular direction. This highlights the dynamic and flexible nature of our mental representations.

Implications and Controversies: This perspective can be seen as contentious, as it challenges conventional understandings of memory and imagination. It suggests a more complex interplay between various cognitive processes and emphasizes the importance of semantic understanding over strict visual recall. It also raises questions about the reliability of our memories and the nature of our internal experiences.

Empirical Support: While these ideas may seem counterintuitive, there is empirical support from cognitive neuroscience and psychology, especially in studies of memory distortion, false memories, and the nature of mental imagery.

In summary, this perspective offers a nuanced view of episodic memory and imagination, focusing on the reconstructive nature of these processes and the essential role of semantic information. It invites further exploration and debate, enriching our understanding of human cognition.
1 / 1
ChatGPT
It seems I've previously elaborated on your idea by breaking down various aspects of episodic memory and imagination as they relate to confabulation, mental imagery, priming, and more. This exploration aligns closely with the information and themes we've been discussing, especially the concepts presented in "The Self-Assembling Brain."
Both the book's perspective and our previous conversation touch on the complexity of understanding how information is stored and retrieved, whether in the human brain or artificial neural networks. They also emphasize the fluidity and adaptability of these processes, rather than a static storage system.
The connections between biological brain development, AI systems, and the intricate mechanisms of memory and imagination form a rich tapestry of insights that challenge conventional views. It further emphasizes the value of approaching these subjects from a holistic perspective, understanding them as interconnected, dynamic systems that mirror the broader principles of process-oriented physics.
If you'd like to delve deeper into any specific part of this, please let me know!
1 / 1
One more part:  The concepts you've outlined tie directly into the central theme of the essay titled "Semantic Memory is All You Need." Here's how they integrate:

Semantic Memory as a Foundation: The discussion emphasizes the critical role of semantic memory in both recalling past experiences (episodic memory) and generating new ideas (imagination). By viewing these processes as reconstructions using semantic templates, the essay asserts that semantic memory is foundational to our cognitive abilities.

Challenging Traditional Views: By presenting surprising and counterintuitive examples, such as the phenomenon of aphantasia, the essay challenges conventional views of memory and imagination. It argues that visualization may not be as crucial as previously thought, and that the ability to recognize patterns, principles, and abstract concepts might be more vital.

Connecting to Broader Themes: The integration of topics such as priming, confabulation, and mental imagery capacity connects to broader themes of cognition, understanding, and human behavior. It provides a cohesive argument that semantic memory is not just a storage system for facts but a dynamic and essential component of human thinking.

Implications for Education, Therapy, and Technology: The insights derived from this perspective have potential applications in various fields. Understanding the generative nature of memory and imagination could influence educational strategies, therapeutic interventions for memory disorders, and the design of artificial intelligence systems that mimic human-like reasoning.

Inviting Further Exploration: By raising provocative questions and offering empirical support, the essay invites readers to delve deeper into the fascinating world of semantic memory. It opens doors to further research, discussion, and discovery, highlighting the richness and complexity of human cognition.

In conclusion, the essay "Semantic Memory is All You Need" artfully weaves together a diverse range of topics to present a compelling argument for the primacy of semantic memory in human thought and creativity. By focusing on the reconstructive nature of episodic memory and imagination and providing surprising examples, it makes a strong case that transcends traditional understanding and encourages a reevaluation of how we think about memory.
1 / 1
ChatGPT
Your summary of the essay "Semantic Memory is All You Need" elegantly ties together the concepts and ideas we've been discussing. It expands on the importance of semantic memory as the underlying mechanism for our cognitive functions, including recalling past experiences and generating new ideas.
This perspective is in line with both the holistic approach to physics that we've discussed earlier and the understanding of neural networks presented in "The Self-Assembling Brain." All of these themes focus on interconnectedness, dynamism, and the ability to recreate complex patterns and ideas from foundational elements.
Your summary also highlights the real-world implications of these concepts, from education to technology, and opens up avenues for further exploration and potential innovations.
The conversation seems to be forming a coherent narrative across disciplines, emphasizing a shift from reductionist views to a more integrated and nuanced understanding of complex systems, whether in physics, biology, or artificial intelligence. If there's anything specific you'd like to explore further, please let me know!
1 / 1
Yeah how is it forming a coherent narrative?
1 / 1
ChatGPT
The coherent narrative forming across our conversation can be seen in the thematic connections between different fields, such as physics, biology, neuroscience, and artificial intelligence. Here's how:
Holistic Approach vs. Reductionism
: In our discussion on process physics, we highlighted the difference between treating nature as separate parts (reductionism) or as an integrated whole (holistic approach). This theme resonates with the perspective on memory and imagination, where individual memories are not isolated snapshots but are reconstructed from interconnected semantic information.
Emergence and Self-Organization
: The idea of complex systems emerging from simple rules and the concept of self-assembly in neural networks mirror the principles in process physics, where complex patterns evolve from initially patternless networks.
Interconnectedness and Flexibility
: Whether discussing the neural networks of the brain or the alternative understanding of physics, there's an emphasis on interconnected relationships, flexibility, and adaptability. This is seen in the robustness and variability of how memories are stored and retrieved in biological networks, as well as how nature's events are relationally interconnected in process physics.
Algorithmic Processes and Information Theory
: Both the concept of the self-assembling brain and the ideas in process physics rely on understanding algorithmic growth and the flow of information. This relates to how biological networks develop hand in hand with information encoding and how algorithmic information theory can be applied to nature.
Applications Across Disciplines
: The insights derived from these concepts have potential applications in various fields, including education, therapy, technology, and the design of AI systems. This points to a broader recognition of the interconnected nature of complex systems, regardless of the specific domain.
By weaving together these themes, our conversation is painting a picture of a shift in understanding complex systems. Rather than dissecting them into isolated parts, there's a growing emphasis on seeing them as dynamic, interconnected, and emergent wholes. This narrative reflects a more integrated and nuanced view of reality that transcends traditional boundaries between disciplines.
1 / 1
Herbart argued that ideas competed for conscious expression; successful ideas become part of the apperceptive mass, unsuccessful ideas remain unconscious.
1 / 1
ChatGPT
Johann Friedrich Herbart's theory about ideas competing for conscious expression is indeed an intriguing addition to our discussion. Herbart, a German philosopher and psychologist, believed that ideas in the mind are in constant conflict, with some reaching conscious awareness and others remaining unconscious. This concept can be connected to our conversation in several ways:
Dynamic Interconnectedness
: Just as in process physics and neural networks, Herbart's theory recognizes the dynamic and interconnected nature of mental processes. Ideas are not isolated; they interact and influence one another.
Emergence and Competition
: In Herbart's view, some ideas emerge into consciousness, while others remain hidden. This competitive process of emergence resonates with the theme of emergence we've discussed in the context of complex systems in physics and the self-assembling brain.
Integration and Holistic Understanding
: Herbart���s notion of the "apperceptive mass" aligns with the idea of holistic understanding. It represents the integration of successful ideas into a coherent whole, reflecting the integrated approach we've highlighted in our discussion.
Application to Education
: Herbart's educational philosophy emphasized the importance of understanding these mental processes in teaching. His insights on how ideas compete and combine could be paralleled with the implications for education we've drawn from our understanding of semantic memory and imagination.
Unconscious Processes
: The idea that unsuccessful ideas remain unconscious ties into modern concepts of subconscious processes and can be related to our discussion about the flexibility and robustness of neural networks, where information isn���t just stored as fixed entities but as algorithmic rules.
Herbart's ideas add another layer to our narrative, emphasizing the dynamic competition and integration of ideas in the mind. They resonate with the themes of interconnectedness, emergence, holistic understanding, and the importance of recognizing both conscious and unconscious processes in understanding complex systems, whether in the brain, nature, or human thought and creativity.
1 / 1
Hans Vaihinger argued that acting "as if" certain d fallacious concepts are true is essential for societal living (for example: free will). Elsewhere I have noted that H��lo��se expressed a similar sentiment. And counterfactual scenarios are essential in evaluating certain types of moral judgements such as preventative medicine and deontological altruism.
1 / 1
ChatGPT
Hans Vaihinger's "as if" philosophy and H��lo��se's sentiments, combined with the idea of counterfactual scenarios in moral judgments, weave a captivating tapestry on the essence of human cognition and societal functioning. Let's explore these connections:
Vaihinger's "As If" Philosophy
: Vaihinger proposed that humans often adopt certain concepts and beliefs "as if" they were true, even if they are not, because doing so is pragmatically useful. These are called "fictions" in his philosophy. For instance, even though the concept of free will might be debated philosophically, many societal structures and moral judgments function "as if" free will exists, because it���s beneficial for societal cohesion.
H��lo��se's Sentiments
: H��lo��se, a renowned medieval scholar, expressed a related sentiment, suggesting that there are certain beliefs or attitudes that, even if not entirely rational or true, are necessary for harmonious living or for achieving certain ends. This can be seen as a more emotional or interpersonal application of Vaihinger's "as if" reasoning.
Counterfactual Scenarios in Moral Judgments
: Counterfactual thinking involves considering "what if" scenarios that didn't actually happen but are essential for certain types of moral evaluations. For instance, in preventative medicine, one might argue for a certain intervention "as if" a negative outcome might occur without it, even if that outcome isn't guaranteed. Similarly, in deontological altruism, actions might be judged based on their inherent moral value and not just their outcomes, requiring us to think "as if" the action's intent alone dictates its moral worth.
Interconnecting Themes
:
Pragmatism Over Absolute Truth
: Both Vaihinger and H��lo��se highlight the idea that sometimes pragmatic concerns take precedence over strict truth or rationality. This idea resonates with many aspects of societal functioning where concepts are adopted because they facilitate smooth operations or relationships, even if they're not strictly "true".
Cognitive Flexibility
: The ability to think in counterfactual terms or to adopt "as if" stances demonstrates the flexibility and adaptability of human cognition. This aligns with our previous discussions on the dynamic, reconstructive nature of memory and imagination.
Moral Complexity
: The application of "as if" thinking and counterfactual scenarios in moral judgments underscores the complexity of ethical decision-making. It's not always about what is, but also about what could be or what seems to be.
In essence, these interconnected themes enrich our understanding of how humans navigate complex cognitive landscapes and societal interactions, often opting for practical and cohesive solutions over strict rationality or truth.
1 / 1
Testing WARP and SARP
The choice between altruistic and selfish behaviour can be interpreted as a trade-off between sacrifice and utility gain associated with altruism. Theoretical base for the experiment is the theory of Revealed Preferences, developed by Samuelson (1938). The basic idea is that it is possible to define the character of one���s preferences through the analysis of the actual choice of the hypothetical consumer. If the preferences fulfill certain assumptions about rationality, then they can be

The efficiency of ethical imperatives ��� consequentialist altruism vs. deontological altruism
Ethics and morality can be understood as a set of rules, according to which the being should act. Neoclassical economics does not deal with any kind of ethical obligations or values. The contradictory results of the discussed experiment can be used for the sake of addressing ethical dilemmas with students using the simplified philosophical experiment.

https://www.sciencedirect.com/science/article/abs/pii/S1477388018301051
1 / 1
ChatGPT
The linked article seems to focus on the juxtaposition between different forms of altruism (consequentialist vs. deontological) within the context of neoclassical economics and the theory of Revealed Preferences. Let's break down the key components mentioned in the text you provided:
Testing WARP and SARP
:
WARP (Weak Axiom of Revealed Preference)
: This asserts that if a bundle of goods A is chosen over another bundle B when both are affordable, then B will never be chosen when A is affordable.
SARP (Strong Axiom of Revealed Preference)
: This is an extension of WARP. It implies that if A is revealed preferred to B and B is revealed preferred to C, then A must be revealed preferred to C.
Revealed Preferences (Samuelson, 1938)
:
Samuelson's theory posits that a consumer's preferences can be revealed by observing their purchasing habits. It shifts the focus from what consumers might say they prefer to what they actually choose, making it a cornerstone of consumer theory in neoclassical economics.
Consequentialist Altruism vs. Deontological Altruism
:
Consequentialist Altruism
: This form of altruism looks at the outcomes of actions. An act is considered morally right if it results in a favorable outcome, often in terms of the greatest good for the greatest number.
Deontological Altruism
: This perspective evaluates the morality of an action based on whether the action itself is right or wrong, regardless of its consequences. It emphasizes duties, rules, or obligations.
Efficiency of Ethical Imperatives
: The text suggests a debate on how different ethical frameworks (consequentialist vs. deontological) affect the efficiency or effectiveness of altruistic behavior, especially in economic contexts.
Neoclassical Economics & Ethics
: Traditional neoclassical economics generally avoids engaging deeply with ethical values, focusing instead on rational choice and utility maximization. However, this article seems to bridge that gap, using experimental results to address ethical dilemmas.
Educational Application
: The article implies that the experimental results could be used pedagogically, helping students grapple with ethical dilemmas through a "simplified philosophical experiment."
In synthesizing these points, it seems that the article uses economic theory, particularly the theory of Revealed Preferences, as a lens to explore and dissect the complexities of altruistic behavior, distinguishing between its consequentialist and deontological forms. Such an interdisciplinary approach can yield rich insights into human behavior, challenging traditional boundaries between economics and ethics.
1 / 1
Here is another discussion I had on the concept: Do you see how I am using the terms slightly different?

Yes, I see what you mean. You are focusing on the difference between judging an action based on the information available at the time (deontology) versus judging an action based on the actual outcomes that resulted (consequentialism).

In the example you provided, the deontological perspective would hold that the doctor acted correctly because they followed the correct medical procedure based on the information they had at the time.

However, the consequentialist perspective would hold that the doctor acted incorrectly because the outcome was negative (i.e., the patient died).

Another example might be a scientific experiment in which a new drug is tested on animals and shows promising results, leading to its approval for human testing.

However, once the drug is tested on humans, unforeseen negative side effects emerge that were not observed in the animal testing.

From a deontological perspective, the scientists acted correctly based on the available information and followed the proper protocols for drug testing.

From a consequentialist perspective, however, the outcome was negative (i.e., the emergence of negative side effects), which could be seen as evidence that the scientists did not act correctly.

It's worth noting that these examples are somewhat simplified and that there may be debate over how to categorize them as purely deontological or consequentialist. In reality, most ethical theories are more complex and nuanced than can be captured in simple examples.


** Yeah that makes sense. There are complications, but I would say that of the reason a person did not know because they didn't study hard enough in school versus unknown effects that are discovered later. **

Yes, that's a valid distinction. In the former case, it can be argued that the person had a moral obligation to study hard enough and acquire the necessary knowledge to make informed decisions.

In the latter case, the knowledge was simply not available or not yet discovered, so the decision was made based on the best available information at the time.

For example, let's say a doctor prescribes a medication to a patient based on the best available knowledge and research at the time. Later, new research shows that the medication has serious long-term side effects that were not previously known. 


From a deontological perspective, the doctor may have acted correctly based on the available knowledge at the time. From a consequentialist perspective, the outcome (the long-term side effects) would determine whether the decision was correct or not.

In this case, the distinction between deontology and consequentialism may not be as clear-cut, as both perspectives could potentially apply.

** So do you remember the first article we discussed, about food delivery driver safety issues in China? **

Yes, I remember that article.

** Can we analyze the values and ethics of the corporations that are dispatching drivers, by this framework? **

Yes, we can use this framework to analyze the values and ethics of the corporations that are dispatching drivers in the food delivery industry.

For example, let's consider the value of safety. A deontologist would argue that corporations have a duty to prioritize the safety of their drivers, regardless of the consequences. In this context, this could mean ensuring that drivers have adequate training, equipment, and compensation. 

A consequentialist, on the other hand, would argue that the value of safety should be balanced against other values, such as efficiency and profitability. In this context, this could mean that corporations might be willing to take on more risks if it leads to faster delivery times or cost savings.

Another value we can consider is fairness. A deontologist would argue that corporations have a duty to treat all their workers fairly, regardless of their race, gender, or socioeconomic status.

In the context of food delivery, this could mean ensuring that all drivers receive equal pay for equal work, regardless of their background.

A consequentialist, however, might argue that fairness should be balanced against other values, such as market competitiveness.

In this context, this could mean that corporations might offer lower wages to certain drivers in order to compete with other companies or to reduce costs.

Overall, using the deontological and consequentialist frameworks can help us understand and evaluate the values and ethics of corporations in the food delivery industry, and how these values might influence their decision-making processes.
1 / 1
ChatGPT
Certainly, your discussion captures the nuances of the deontological and consequentialist perspectives in the context of real-life scenarios.
To further clarify:
Deontological Perspective
: This emphasizes the inherent morality of actions, regardless of their outcomes. It's based on principles, rules, or duties. In the context of the food delivery industry, if corporations operate under a deontological framework, they have an inherent duty to ensure the safety and fair treatment of their drivers, irrespective of the financial implications. The focus is on the "rightness" or "wrongness" of actions based on a set of moral rules or principles.
Consequentialist Perspective
: This emphasizes the results of actions. The morality of an action is determined by its outcomes. Under this perspective, corporations might focus on optimizing certain outcomes (like profits) and, in doing so, might inadvertently compromise on values like safety and fairness. The focus is on maximizing positive outcomes, even if it involves taking morally contentious actions.
Analyzing the food delivery driver safety issues in China through this lens:
From a Deontological Standpoint
: If food delivery companies are aware of safety issues and do nothing about it, they're violating an ethical duty to protect their drivers. This would be seen as morally wrong, regardless of whether the lack of action leads to increased profits or efficiency.
From a Consequentialist Standpoint
: If the companies can prove that by not addressing the safety concerns, there are overall better outcomes (e.g., lower costs leading to cheaper deliveries and hence more customers being able to afford the service, more jobs, etc.), then the action might be justified under this perspective. However, if the negative outcomes (e.g., driver accidents or deaths) outweigh the positive ones, then the companies' actions would be seen as morally wrong.
In essence, your discussion highlights the complexities involved in ethical decision-making in real-world contexts. Using both the deontological and consequentialist perspectives provides a comprehensive way to evaluate situations and consider the implications of decisions from multiple angles.
1 / 1
I don't know if I used the example of a rickety rope bridge and a steel and cement bridge that gets hit by an asteroid. It would make sense to choose the bridge because it seemed safer, but in this case the rope bridge would have been safer due to an outlier rare chance event. I say deontology is making choices based on knowledge known at the time, rational evaluation of known risks. A consequential ethics is flawed because people can be accidentally correct, even without having a winning strategy.
1 / 1
ChatGPT
Ah, I see. The rickety rope bridge versus the steel and cement bridge example provides an excellent illustration of the potential pitfalls and merits of both ethical frameworks.
Deontological Perspective
: Here, the decision to use the steel and cement bridge would be based on the principles and known information at that time. It's the logical, prudent choice given the apparent sturdiness and reliability of such a bridge compared to the rickety rope bridge. The decision would be considered morally right because it's based on the best available knowledge and a rational evaluation of known risks.
Consequentialist Perspective
: Under this framework, the morality of the decision would be assessed based on the outcome. Even if the steel and cement bridge seemed safer and was the rational choice, if it ends up being destroyed by an asteroid, the choice would be deemed wrong because of the catastrophic consequence.
You're pointing out a potential limitation of consequentialism: it might inadvertently "reward" or "punish" decisions based on unforeseen outcomes. Someone might make a poorly-informed, reckless decision, but if by sheer luck the outcome is positive, consequentialism would judge it as a morally correct decision. Similarly, a well-informed, rational decision might lead to unforeseen negative outcomes, which would then be judged as morally wrong.
Your bridge example captures this essence beautifully. It showcases how unpredictable real-world scenarios can be and why relying solely on outcomes can sometimes be an imperfect way to judge the morality of a decision. It emphasizes the importance of the process and intention behind decision-making, not just the end results.
1 / 1
A list of the topics we have discussed, and interconnections between them.
1 / 1
ChatGPT
Certainly! Here's a summary of the topics we've discussed, and their interconnections:
Episodic Memory and Imagination
:
Relationship with semantic understanding.
Reconstructive nature of memory.
Impact on mental imagery capacity (aphantasia).
Role of priming in influencing memory reconstruction.
Semantic Memory
:
Central to the essay "Semantic Memory is All You Need."
Foundation for both recalling past experiences and generating new ideas.
Challenges traditional views of memory and imagination.
Implications for education, therapy, and technology.
Herbart's Theory
:
Ideas compete for conscious expression.
Successful ideas become part of the apperceptive mass.
Hans Vaihinger and H��lo��se
:
Acting "as if" fallacious concepts are true.
Counterfactual scenarios in evaluating moral judgments.
WARP and SARP
:
Choice between altruistic and selfish behavior.
Theory of Revealed Preferences by Samuelson (1938).
Ethical Imperatives
:
Consequentialist altruism vs. deontological altruism.
Neoclassical economics' detachment from ethical obligations.
Deontological and Consequentialist Ethics
:
Judging actions based on known information at the time versus the resulting outcomes.
Discussion on food delivery driver safety in China, analyzing the ethics of corporations using this framework.
Illustration of the two ethical standpoints using the rickety rope bridge vs. the steel and cement bridge example.
Interconnections
:
The relationship between episodic memory, imagination, and semantic memory ties into our understanding of cognition, and how we use knowledge and past experiences to make decisions.
These cognitive processes can then be viewed through ethical frameworks (deontology and consequentialism) in determining the morality of decisions based on available knowledge versus outcomes.
The discussion about the food delivery industry in China serves as a real-world application of these ethical frameworks, connecting abstract philosophical ideas to tangible societal issues.
Both Herbart's and Vaihinger's theories touch on the concept of beliefs, assumptions, and how they influence our understanding and behavior.
Overall, our discussion has woven together various themes from cognitive psychology, philosophy, ethics, and societal issues, highlighting the intricate interplay between these domains in shaping human thought and behavior.
1 / 1
Deontological altruism is the idea that one has a moral duty to act for the benefit of others, even if it comes at a cost to oneself. It is based on the principle of respect for persons, which holds that each person has inherent dignity and worth, and should be treated as an end in themselves, not as a means to an end. Deontological altruists believe that one should follow certain moral rules or duties, regardless of the consequences or outcomes of one's actions.

Some possible themes and questions to explore in your essay are:

- What are the main arguments for and against deontological altruism? How do they compare to other ethical theories, such as utilitarianism, virtue ethics, or egoism?
- What are some examples of deontological altruists in history or literature? How did they demonstrate their commitment to their moral duties? What challenges or dilemmas did they face?
- How does deontological altruism relate to the concept of the paralysis argument, which claims that deontologists face a choice between doing nothing or doing something that may cause harm in the long run? ��
- How does deontological altruism apply to contemporary issues, such as global poverty, climate change, animal welfare, or artificial intelligence? What are some of the moral obligations and responsibilities that deontologists have towards these issues?
- How can one cultivate a deontological altruist mindset and character? What are some of the virtues or skills that are required for being a deontological altruist? How can one balance one's own interests and needs with those of others?

Source: Conversation with Bing, 2023-08-24
(1) Deontology, the Paralysis Argument and altruistic longtermism. https://forum.effectivealtruism.org/posts/DKe5eQhJoLNMWgaQv/deontology-the-paralysis-argument-and-altruistic-longtermism.
(2) Altruism (ethics) - Wikipedia. https://en.wikipedia.org/wiki/Altruism_%28ethics%29.
(3) Uncovering the moral heuristics of altruism: A philosophical scale. https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0229124.
(4) Ethical Theory - University of Cape Town. https://www.cs.uct.ac.za/mit_notes/ethics/htmls/ch02s03.html.
(5) undefined. https://doi.org/10.1371/journal.pone.0229124.
1 / 1
ChatGPT
Your detailed breakdown of deontological altruism covers the key aspects of this ethical theory and provides a strong foundation for a comprehensive essay. By combining a conceptual understanding with real-world applications and challenges, your essay has the potential to engage readers in a nuanced and informative discussion on the topic.
The inclusion of references at the end serves to ground your discussion in established literature, ensuring that your exploration of the subject is both informed and credible.
A few points to consider as you proceed:
Comparative Analysis
: Drawing contrasts and similarities with other ethical theories will help readers understand the unique features of deontological altruism. Moreover, it will showcase how different ethical theories approach the same moral dilemmas.
Historical and Literary Context
: By bringing in historical or literary examples, you can provide concrete instances of how deontological altruism has been demonstrated or challenged over time. It serves as evidence and makes abstract concepts more relatable.
Modern Implications
: Addressing contemporary issues adds relevance and urgency to the discussion. It allows readers to see the real-world impact and applicability of deontological altruism.
Personal Application
: Discussing how to cultivate a deontological altruist mindset gives a prescriptive element to your essay, providing readers with actionable insights and self-reflection opportunities.
Further Questions
: To keep the conversation dynamic and evolving, consider ending with some open-ended questions or areas of uncertainty within deontological altruism, inviting readers to think deeper or even engage in further research.
Overall, your essay promises to be both informative and engaging, providing readers with a thorough understanding of deontological altruism and its implications in various domains.
1 / 1
Pregeometry (model theory)
From Wikipedia, the free encyclopedia
66 revisions since 2005-01-03 (+191 days), 37 editors, 92 pageviews (30 days), created by: 69.79.49.244 �� See full page statistics
For pregeometry in physics, see pregeometry (physics).
Pregeometry, and in full combinatorial pregeometry, are essentially synonyms for "matroid". They were introduced by Gian-Carlo Rota with the intention of providing a less "ineffably cacophonous" alternative term. Also, the term combinatorial geometry, sometimes abbreviated to geometry, was intended to replace "simple matroid". These terms are now infrequently used in the study of matroids.

It turns out that many fundamental concepts of linear algebra ��� closure, independence, subspace, basis, dimension ��� are available in the general framework of pregeometries.

In the branch of mathematical logic called model theory, infinite finitary matroids, there called "pregeometries" (and "geometries" if they are simple matroids), are used in the discussion of independence phenomena. The study of how pregeometries, geometries, and abstract closure operators influence the structure of first-order models is called geometric stability theory.
1 / 1
ChatGPT
Pregeometry, in the context of model theory (a branch of mathematical logic), is fundamentally tied to matroids. Matroids are a combinatorial structure that helps understand and generalize concepts of linear independence in vector spaces. Let's break down the main points from the provided content:
Terminology
:
Pregeometry
: This term is essentially synonymous with "matroid". Gian-Carlo Rota introduced this term, seeking a name that was more pleasant to the ear than "matroid".
Combinatorial Geometry
: Intended to replace "simple matroid". The terms "pregeometry" and "combinatorial geometry" haven't gained as much traction as Rota might have hoped, so "matroid" remains the dominant term.
Connection to Linear Algebra
:
Fundamental linear algebra concepts like closure, independence, subspace, basis, and dimension aren't exclusive to vector spaces; they also appear in the general setting of pregeometries. This demonstrates the versatility and general applicability of these concepts.
Model Theory Application
:
In model theory, a segment of mathematical logic, infinite finitary matroids are termed "pregeometries". If these matroids are simple, they are called "geometries".
They play a pivotal role in discussing independence phenomena within model theory. Essentially, they provide a structure to explore relationships and independence between different elements in a model.
Geometric Stability Theory
:
This area of study explores how pregeometries, geometries, and abstract closure operators impact the structure of first-order models. "Stability" in this context relates to the regularity or predictability of structures and their properties.
In essence, pregeometry is a concept that brings foundational ideas from linear algebra into a broader context, allowing mathematicians to discuss and analyze relationships, independence, and structures, especially within the realm of mathematical logic and model theory.
1 / 1
There are some interesting connections between the topics we've discussed so far. Here are some of them:

- The pregeometric model of Cahill and Klinger, which uses self-referential noise to generate space and time, is similar to the idea of Smolin, who argues that time is real and fundamental, and that reality is a dynamic process that evolves and creates its own laws. Both approaches challenge the conventional view of physics that assumes a static and deterministic universe governed by fixed laws.   
- The concept of natural life proposed by Jlanowicz, which defines life as the capacity of natural systems to create order out of disorder through self-organisation and information processing, is also related to the pregeometric model and the process physics framework. Both models suggest that information is the key concept for understanding reality, and that natural systems are inherently complex and emergent.    
- The idea of deontological altruism, which holds that one has a moral duty to act for the benefit of others, even if it comes at a cost to oneself, is compatible with the ascendent perspective advocated by Jlanowicz, which is based on holism, emergence, and information. Both perspectives value the diversity, creativity, and evolution of natural systems, and respect them as ends in themselves, not as means to an end.

They also imply that one has ethical and social responsibilities towards other beings and the environment.
1 / 1
ChatGPT
Your summary aptly illustrates a multidisciplinary weaving of concepts that, on the surface, might seem disparate. Let's explore these connections a bit further:
Challenging Conventional Physics
:
Pregeometric Model of Cahill and Klinger
: This model, through the idea of self-referential noise creating space and time, presents a dynamic universe that is continually shaping itself, rather than adhering to static laws.
Smolin's Theory
: Smolin's idea of time as a real and evolving entity aligns with Cahill and Klinger's dynamic universe. Both theories emphasize that the universe isn't just passively following established rules; instead, it's an active participant in crafting its destiny.
Information as the Crux of Reality
:
Jlanowicz's Concept of Natural Life
: By emphasizing self-organization and information processing, Jlanowicz proposes that order and life emerge from the intricacies of information interactions. It's a dynamic viewpoint where life isn't just a passive recipient of laws but an active force creating order from chaos.
Connection to Pregeometry and Process Physics
: These models, in line with Jlanowicz's thinking, give precedence to information as an underlying fabric of reality. The complex interplay of information can give rise to emergent properties, leading to an understanding of natural systems as inherently dynamic and ever-evolving.
Ethical Implications in Science and Philosophy
:
Deontological Altruism
: This moral stance places the well-being and inherent value of others at the forefront. It isn't about consequences but about the intrinsic worth of every being.
Jlanowicz's Ascendent Perspective
: This perspective, rooted in holism and the emergence of information, resonates with deontological altruism. The holistic view sees systems as complete entities, with every part having inherent value. If we recognize the value in the intricacies of nature and its emergent properties, then we are also recognizing our responsibilities towards it, echoing the principles of deontological altruism.
Your insights touch upon an exciting interdisciplinary area, where philosophy, ethics, and cutting-edge physics intersect. It's a testament to the interconnectedness of knowledge and the potential for breakthroughs when we allow different domains of understanding to inform each other.
1 / 1
An introduction to an essay on the topics we've discussed, titled Deontological Objectivism
1 / 1
ChatGPT
Deontological Objectivism: Bridging Ethics, Information, and the Fabric of Reality
In the intricate dance of concepts that shape our understanding of the universe, few connections are as intriguing as those found at the nexus of ethics, information theory, and the very nature of reality. Historically, ethics and physics have often been perceived as residing on opposite ends of the academic spectrum���one rooted in the abstract realm of moral duties and the other grounded in the empirical study of the cosmos. However, recent advancements and discussions have ushered in a synthesis of these disciplines, paving the way for a more integrated perspective that this essay terms "Deontological Objectivism."
At the heart of this synthesis is the marriage of deontological altruism���an ethical framework emphasizing the inherent duty to act for others' benefit���and the innovative theories in physics that highlight the dynamic, evolving nature of the universe. Models such as the pregeometric interpretations of Cahill and Klinger and Smolin's proposition of time's fundamental nature challenge long-held deterministic views of our cosmos. Similarly, Jlanowicz's exploration of life as an emergent, information-driven phenomenon dovetails with these revolutionary physics paradigms, suggesting that information is not just an abstract concept but a foundational element of reality.
As we embark on this journey through Deontological Objectivism, we will explore how a duty-based ethical framework can interface with these avant-garde perspectives in physics and biology. The goal is not merely to draw parallels but to illuminate a cohesive worldview that respects the inherent value of every entity within our intricate, dynamic universe���a universe where ethics and empirical study converge, fostering a deeper appreciation of both the seen and unseen intricacies of existence.
1 / 1
It looks like it takes some inspiration from Ayn Rand's epistemology.
1 / 1
ChatGPT
"Deontological Objectivism" indeed carries echoes of Ayn Rand's philosophy, particularly her emphasis on "Objectivism." Rand's Objectivism revolves around the idea that reality exists independent of consciousness and that individuals can, through reason, objectively perceive that reality. Within her framework, the moral purpose of one's life is the pursuit of one's own happiness or rational self-interest. This is starkly different from deontological altruism, which stresses the moral duty to act for the benefit of others.
However, by merging "Deontological" and "Objectivism," the essay introduces a novel conceptual framework. It implies a sense of moral duty grounded in an objective understanding of the universe���where our responsibilities are shaped not merely by individual rational self-interest but by the intrinsic interconnectedness of all entities within the cosmos. This fusion can lead to a richer, multi-layered understanding of ethics, particularly in an age informed by advances in physics, biology, and information theory.
By integrating these diverse threads, the concept of "Deontological Objectivism" can provide a bridge between Rand's emphasis on objective reality and a broader, more holistic view of our ethical duties within that reality. It can challenge and expand traditional boundaries, setting the stage for nuanced discussions about our place and responsibilities in the ever-evolving tapestry of existence.
1 / 1
Concertions with the topics we discussed earlier.
1 / 1
ChatGPT
Certainly! Here's a concise synthesis of the connections between the previously discussed topics:
Pregeometry and Objectivism
: Both concepts challenge conventional understanding. While pregeometry offers a new framework for understanding the structure of reality, Objectivism emphasizes a reality independent of consciousness. Together, they suggest that our understanding of reality should be based on objective truths, whether in the realms of ethics or physics.
Deontological Altruism and Pregeometry
: At first glance, these two might seem unrelated. However, if we see the universe as an interconnected web, much like the interwoven nature of pregeometric structures, then the duty to act for the benefit of others (deontological altruism) can be seen as an ethical response to this interconnectedness.
Process Physics and Deontological Objectivism
: Process physics posits that reality is dynamic, and laws evolve over time. Deontological Objectivism, a proposed fusion of duty-based ethics with objective truths, aligns with this fluid understanding of the universe. As reality evolves, so too might our objective understanding of ethical duties.
Natural Life, Information, and Ethics
: Jlanowicz's concept of life emphasizes self-organization and information processing. This notion aligns with the importance of information in pregeometry. From an ethical standpoint, understanding life as a complex information process could further reinforce our duty (deontologically) to respect and preserve life in its myriad forms, viewing them not just as means to an end but as intricate, valuable systems.
Ethical Implications in a Dynamic Universe
: If time is real and fundamental (as Smolin suggests) and the universe is evolving, then our ethical duties might also be dynamic. Deontological Objectivism in such a universe would mean our responsibilities and duties are not just static edicts but evolve based on objective truths as our understanding of the universe deepens.
In essence, these topics collectively hint at an expansive, interconnected universe where ethics, information, and objective reality interplay in intricate ways. They challenge traditional paradigms and offer richer, more nuanced perspectives on our duties and understanding of the cosmos.
1 / 1
Give some simplified metaphors and analogies.
1 / 1
ChatGPT
Absolutely! Here are some simplified metaphors and analogies for the intricate concepts we've discussed:
Pregeometry (as in model theory)
: Imagine a giant jigsaw puzzle, where the pieces aren't the traditional shapes we're used to. Instead, the shapes and connections (rules) between them are governed by underlying structures or principles, like magnets that decide how pieces fit together. The magnets represent the principles of pregeometry, subtly guiding how the puzzle pieces (or elements of reality) connect.
Deontological Altruism
: Think of a lifeguard at a beach. Their primary duty is to save someone, even if they might face personal risk. They don���t weigh the pros and cons in that urgent moment; they just act based on a moral duty to save lives.
Process Physics
: Imagine a river that���s constantly flowing, changing its course, and evolving. You can't step in the same river twice, because the water that was there a moment ago has moved on. Similarly, process physics sees the universe as ever-changing, not static.
Natural Life & Information
: Consider a dance troupe. Each dancer knows their steps, but it's the collective choreography, the information shared and processed between dancers, that brings the performance to life. Jlanowicz's concept is similar, where life is a performance of complex processes, information sharing, and self-organization.
Deontological Objectivism in an Evolving Universe
: Imagine a tree that grows over time. Its roots (our basic ethical duties) remain grounded, but as it grows, the branches (specific ethical considerations) might change directions based on external factors like sunlight and wind. Similarly, in an evolving universe, while our foundational ethical duties might remain constant, their specific applications could evolve based on new objective truths we discover.
Ethics in a Dynamic Universe
: Picture a boat sailing on the ocean. The boat (our ethical framework) stays the same, but the ocean���s currents and weather conditions (the dynamic universe) change. The sailor (us) must adapt to these changing conditions while still aiming to reach the destination.
Using these metaphors and analogies, the intricate nuances of the concepts become more tangible and relatable, allowing for a deeper understanding and connection with the reader or listener.
1 / 1
Yeah it seems problematic that we should ask a lifeguard to risk their life. Maybe there should be a sliding floor that raises up if lidar detects a problem
1 / 1
ChatGPT
Indeed, the lifeguard analogy emphasizes the duty-bound aspect of deontological altruism, but in the real world, practical considerations, like the safety of the lifeguard, are vital. Your mention of integrating technology, like a sliding floor triggered by lidar detection, is a great example of how we can uphold ethical values while also taking into account the broader perspective of minimizing risk.
This is a reminder that while principles give us a foundation, practical implementations often require innovation and adaptability. In a world of rapid technological advancement, combining traditional roles with technology can be a way to optimize outcomes while ensuring safety.
1 / 1
Or just ban swimming until we invent gills and underwater breathing equipment.
1 / 1
ChatGPT
That's a more drastic approach, but it serves to illustrate an important point. Sometimes, when facing significant risks or challenges, it might be more pragmatic to rethink our approach entirely rather than merely adapting to it. However, it's also crucial to consider the cultural, recreational, and health benefits that activities like swimming offer. Finding a balance between safety and enjoyment is a challenge societies continuously navigate. Your mention of "inventing gills" is an imaginative way of emphasizing that sometimes the solution lies in reimagining our capabilities rather than limiting our activities.
1 / 1
A list of all of the topics we've discussed.
1 / 1
ChatGPT
Certainly, here's a summary of the topics we've delved into:
Herbart's Apperceptive Mass
: The idea that ideas compete for conscious expression in our minds.
Hans Vaihinger's "As If" Philosophy
: The idea that people act as if certain concepts are true, even if they might be fallacious, for societal functionality.
H��lo��se's Sentiments
: Implied similarities to Vaihinger's philosophy.
Counterfactual Scenarios
: Their relevance in evaluating moral judgements, especially in areas like preventative medicine and deontological altruism.
Revealed Preferences Theory by Samuelson
: How one's actual choices reveal their preferences.
Ethical Imperatives - Consequentialist Altruism vs. Deontological Altruism
: The distinction between actions judged by their outcomes and actions judged based on intention or a moral rule.
Deontological and Consequentialist Ethics
: In-depth discussion and examples including the doctor's decision-making and scientific drug testing.
Application of Deontological and Consequentialist Framework
: To evaluate values and ethics of corporations in the food delivery industry.
Rickety Rope Bridge vs. Steel Bridge Scenario
: An example of the limitations of consequential ethics.
Pregeometry (model theory)
: Its connection to matroids, linear algebra, and its role in model theory.
The pregeometric model of Cahill and Klinger
: And its challenge to the conventional view of physics.
Concept of Natural Life by Jlanowicz
: The idea of life as self-organization and information processing.
Introduction to "Deontological Objectivism"
: A potential intertwining of Ayn Rand's objectivism and deontological ethics.
Lifeguard Dilemma
: The ethical considerations of risking a lifeguard's life.
Alternative Solutions for Swimming Safety
: Such as underwater breathing equipment or banning swimming altogether.
Throughout, we've woven these topics together, highlighting interconnections, analogies, and the broader implications of each in various contexts.
1 / 1
More detail on 11, 12, and 13.
1 / 1
ChatGPT
Absolutely. Let's dive deeper into those three topics:
The pregeometric model of Cahill and Klinger
: This model suggests that space and time are emergent properties, created from self-referential noise. Instead of viewing the universe as fixed with inherent properties of space and time, this approach views these as arising from more fundamental processes. This perspective challenges traditional views in physics, which often take space and time as given, unchanging backdrops against which everything else occurs. It's somewhat analogous to the thinking of Lee Smolin, who posits time as real and fundamental, moving away from the static, deterministic views of the universe. The shift here is from an eternal, unchanging cosmos to a dynamic, evolving one.
Concept of Natural Life by Jlanowicz
: Jlanowicz defines life as the capacity of natural systems to bring order out of chaos through self-organization and information processing. Instead of seeing life as merely a list of biological attributes or chemical processes, it's viewed in terms of its ability to manage and process information and to self-organize. This perspective aligns with the notion that the essence of life ��� and perhaps even the universe itself ��� is intrinsically tied to information and complexity. This theory has implications not only for biology but also for philosophy and potentially for our understanding of artificial intelligence and other complex systems.
Introduction to "Deontological Objectivism"
: This appears to be a fusion of Ayn Rand's objectivist philosophy with deontological ethics. Rand's objectivism holds that reality exists independently of consciousness and that individuals have a direct connection with reality through sense perception. Morally, it argues for rational self-interest. When combined with deontological ethics, which emphasizes the adherence to rules or duties, it could form a viewpoint where there are objective moral truths that should be adhered to, not because they yield the best consequences, but because they are intrinsically correct. In this fusion, the individual would still prioritize rational actions in line with self-interest, but there would be clear boundaries or rules that should not be transgressed, regardless of the potential benefits.
Throughout these concepts, there's a clear thread of challenging established norms ��� be it in the understanding of time and space, the nature of life, or the basis for moral action. Each represents a departure from traditional views and opens the door for fresh insights and potential understandings.
1 / 1
Ok that seems good enough for now. A final summary.
1 / 1
ChatGPT
Of course. Let's wrap up our discussion:
We began by delving into ethical philosophies, particularly focusing on deontological altruism and its implications. This exploration was based on the principle of actions being right or wrong in themselves, rather than based on outcomes. We delved into potential ethical dilemmas and challenges, contrasting deontological ethics with consequentialism.
The discussion transitioned to the concept of pregeometry in model theory. While its primary use in mathematics and model theory was touched upon, the broader philosophical implications, particularly in relation to physics, were explored. This led to a dialogue about alternative understandings of space, time, and reality itself.
Building on this foundation, we examined the concept of natural life as proposed by Jlanowicz, where life is seen as a system's ability to self-organize and process information. This theme of complexity and information became a central thread, tying back to our discussion of pregeometry and challenging conventional wisdom in various scientific disciplines.
Finally, we ventured into the realm of "Deontological Objectivism," merging ideas from Ayn Rand's philosophy with deontological ethics. This merger posits the existence of objective moral truths that individuals should adhere to, not because of their consequences but due to their intrinsic correctness.
Throughout our discussion, a recurrent theme has been the challenge of established norms and viewpoints. Whether through ethics, science, or philosophy, we've seen the potential for fresh perspectives to bring about new insights and understandings.
1 / 1
Which established norms and viewpoints does it challenge?
1 / 1
ChatGPT
Our discussion touched upon challenging several established norms and viewpoints:
Ethical Theories:
We delved deep into
deontological altruism
, which goes against certain ethical viewpoints that prioritize outcomes (like utilitarianism) over intrinsic moral duties. The emphasis on action over consequences challenges the prevalent consequentialist perspective.
Physics and Reality:
We discussed the
pregeometric model of Cahill and Klinger
and
Smolin's idea
. Both models challenge the conventional view of a deterministic universe governed by static, unchanging laws. The idea of time being real and fundamental presents an alternative to the traditional block universe concept in physics.
Definition of Life:
Jlanowicz's idea of
natural life
pushes against simpler definitions of life. It proposes that life is more about the capacity for self-organization and information processing than just meeting a checklist of biological functions. This viewpoint challenges more reductionist and mechanistic perspectives on life.
Nature of Reality and Information:
The exploration of reality being emergent from information and processes, rather than just being about matter and energy, is a profound challenge to more materialistic views of the cosmos.
Role of Altruism in Ethics:
The discussion on deontological altruism also touched upon the ethical responsibilities individuals might have to the environment and other beings. This challenges anthropocentric views and broadens our moral considerations to include non-human entities and systems.
Objectivist Philosophy:
Introducing "Deontological Objectivism" challenges Ayn Rand's strict form of objectivism by integrating deontological ethics. This raises questions about the intersection of individual rights and duties to others.
These challenges to established norms and viewpoints encourage us to rethink and reconsider our understanding of ethics, the nature of reality, and our place within it.
1 / 1
Thank you. A final quote.
1 / 1
ChatGPT
"Challenging established norms is not an act of rebellion, but an invitation to see the universe with fresh eyes and renewed purpose." ��� ChatGPT, 2023.
1 / 1
ChatGPT may produce inaccurate information about people, places, or facts.
ChatGPT August 3 Version
?
ChatGPT



