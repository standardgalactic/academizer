I published this book just last month with a colleague who owns an AI company in Germany.
He's a mathematician amongst other things and he has been striving for many years to
make AI work in a number of different areas and he did succeed but in the process he learned
a lot of lessons about where AI will not work and he's now going around the world talking
to companies who are being offered all kinds of opportunities that AI is supposed to bring
telling them that AI will not bring many of those opportunities for mathematical reasons.
And I'm going to apply one simple lesson from the book to the case of digital twins.
So the summary of the book is that AI succeeds in certain narrow areas but wherever we're
dealing with complex systems, for instance human conversation, emulation or stock market
prediction, then the AI will fail and so and the reason is model induced escape.
So let's suppose you have an AI algorithm which will help you get rich buying and selling
stock then if you're successful other market participants will exploit the information that
you generate in order to make you fail.
That's model induced escape.
The model itself creates the reasons for the model failing.
Something similar happens with spam filters.
If you have a really good spam filter filter then the authors of spam will use that spam
filter in order to create new and wonderful kinds of spam and thereby render your spam
filter no longer operative.
So this has a consequence for digital twins, particularly where digital twins are designed
to reflect the patterns of behavior on the part of people.
This would apply also in other areas too.
So the idea is that the very existence of a digital twin operating in relation to people's
behavior patterns will lead to changes in those behavior patterns.
It will lead to a change in the system.
There will now be a digital twin inside that system and so the digital twin will no longer
be accurate because the digital twin does not accommodate this new feature of the system
which it's supposed to be a model of namely that the digital twin itself is part of the
system.
Now this is a very beautiful example of model induced escape.
Every digital twin which reflects people's behavior at least must always be no longer
a twin as soon as it's installed.
There are more substantial ways in which the digital twin will change behavior patterns.
So there will be some who react to the knowledge that they are being monitored by a digital
twin by protesting that their behavior is being monitored and thereby changing their
behavior perhaps in such a way as to deliberately undermine the digital twin and these kinds
of attitudes are nicely summarized in this book already from 1999.
Now there is a nice paper about all of these problems which I recommend people who are
interested in digital twins should read that he's particularly interested in those cases
where digital twins will allow prediction of the future and implement optimal control
of for instance people's behavior and he agrees with me that in those cases digital twins
will likely fail for the reason that they're using AI and the AI will likely fail for the
reasons which I've explained.
But digital twins will work where we're dealing with infrastructures and these are his words
now and geographies which change very little over time and for instance in production plants
or aircraft engines and so forth and the final chapter of the book describes those cases
where AI will work and they are pretty much the same sorts of cases as the cases where
digital twins will work.
So we believe that there is still a huge opportunity for AI and indeed for digital twin technology
but it will not do any of the marvellous wondrous things that some people imagine it will.
All right now the many people think that these problems can be solved if we can just collect
enough data and the main thesis of the book the central chapters which are quite mathematical
in nature we think they demonstrate that however much data you have you still could not create
algorithms with the sorts of predictive powers which would be needed to engineer an AI which
would work in any complex system domain that means weather, climate, geosizmic as well as
human behavior, animal behavior, bird behavior so we will never have an AI system which can
emulate the behavior of a parrot for instance or a rat perhaps even a bacterium.
And therefore there will be problems with digital rivers and other climate and environment based
digital twin models that people are developing so these are the examples of complex systems
where digital twins won't work and the simple systems have all of these mathematical properties
so they have a fixed the solar system is the best example they have a fixed set of element types
a fixed set of interaction types gravity in this case there is one relevant force again gravity
all processes are time reversal that means we have ergodic face spaces and that is the crucial
part of the book AI will not work if you have non ergodic face spaces because you will never
collect representative sampling data to train your neural network if you do not have an ergodic
face space because no subset of the data is representative of the whole space that's what
ergodicity means. I don't need to say anymore I think so simple systems have these properties
complex systems don't have those properties they have new element types for instance adults are
born dictators arise people start to hate digital twins all of these are changes in the
complex system which will affect the predictive power of the complex system so complex systems
are subject to continuous evolution this is true of the stock market it's true of traffic patterns
in large cities and it's true in the the weather so these are some examples and these are the seven
properties and these are the the cases where these properties are satisfied and not satisfied
and I'll leave you with that
