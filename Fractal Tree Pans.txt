Fractal Tree Pans
Open sidebar
ChatGPT
4
You
John Horgan (The Science Writer)

Bayes��� Theorem and Bullshit
Jan 5 
Written By John Horgan

The website 3blue1brown.com has terrific tutorials on all sorts of mathematical topics, including Bayes��� theorem.

A while back one of my students, ���Frank,��� a real smarty-pants, started babbling about something called Bayes��� theorem. He wrote a long, dense paper about the theorem���s revelatory power, which had changed the way he looked at life.

The grandiose claims Frank made for Bayes��� theorem couldn���t possibly be true. A formula for reliably distinguishing truth from bullshit? Come on, give me a break. I dismissed Bayes��� theorem as a passing fad, like Jordan Peterson, not worth deeper investigation.

But I kept bumping into Bayes. Researchers in physics, neuroscience, artificial intelligence, social science, medicine, you-name-it are using Bayesian statistics, Bayesian inference, Bayesian whatever to model things and solve problems. Scientists have proposed Bayesian models of the brain and Bayesian interpretations of quantum mechanics. The math website 3blue1brown.com (where I found the image above) calls Bayes��� theorem ���central to scientific discovery.���

I reluctantly decided to give Bayes a closer look, to see what the fuss is all about. In this column, I���ll explain how Bayes��� theorem works and, more broadly, what it means, and I���ll bold my main takeaways. I trust kind readers will, as usual, point out my errors. Here goes:

Named after its 18th-century inventor, Presbyterian minister Thomas Bayes, Bayes��� theorem is a method for calculating the validity of hypotheses (aka claims, propositions, beliefs) based on the best available evidence (aka observations, data, information). Here���s the most dumbed-down description: Tentative hypothesis plus new evidence = less tentative hypothesis.

Here���s a fuller version: The probability that a hypothesis is true given new evidence equals the probability that the hypothesis is true regardless of that new evidence times the probability that the evidence is true given that the hypothesis is true divided by the probability that the evidence is true regardless of whether the hypothesis is true.

Got that? Okay, let me unpack it for you. Bayes��� theorem takes this form: P(H|E) = P(H) x P(E|H) / P(E), with P standing for probability, H for hypothesis and E for evidence. P(H) is the probability that H is true, and P(E) is the probability that E is true. P(H|E) means the probability of H if E is true, and P(E|H) is the probability of E if H is true.

Medical testing often serves to demonstrate the formula. The hypothesis, H, is that you have a disease. You want to know what the odds are that this hypothesis is correct given evidence, E, such as a positive test for the disease. Your odds are represented by P(H|E), the probability that you have the disease given a positive test.

Let���s say you get tested for a cancer estimated to occur in 1 percent of the population. If the test is 100 percent accurate, you don���t need Bayes��� theorem to know what a positive test means, but let���s use the theorem anyway, just to see how it works.

You plug your data into the right side of Bayes��� equation. P(H), the probability that you have cancer before getting tested, is 1 percent, that is, 1/100, or .01. P(E), the probability that you will test positive, is also .01. Because P(H) and P(E) are in the numerator and denominator, respectively, they cancel each other out, and you are left with P(H|E) = P(E|H) = 1.

That means the probability that you have cancer if you test positive is 1, or 100 percent, and the probability that you test positive if you have cancer is also 1, or 100 percent. More succinctly, if you test positive, you definitely have cancer, and vice versa. 

In the real world, tests are rarely if ever totally reliable. Let���s say your test is 99 percent accurate. That is, 99 out of 100 people who have cancer will test positive, and 99 out of 100 who are healthy will test negative. That���s still a terrific test. If your test is positive, how probable is it that you have cancer?

Now Bayes��� theorem displays its power. Most people assume the answer is 99 percent, or close to it. That���s how reliable the test is, right? But the correct answer, yielded by Bayes��� theorem, is only 50 percent.

Plug the data into the right side of Bayes��� equation to see why. P(H) is still .01. P(E|H), the probability of testing positive if you have cancer, is now .99. So P(H) times P(E|H) equals .01 times .99, or .0099. This is the probability that you will get a true positive test, which shows you have cancer.

What about the denominator, P(E)? Here is where things get tricky. P(E) is the probability of testing positive whether or not you have cancer. In other words, it includes false as well as true positives.

To calculate the probability of a false positive, you multiply the rate of false positives, which is 1 percent, or .01, times the percentage of people who do not have cancer, .99. The total comes to .0099. Yes, your terrific, 99-percent-accurate test yields as many false positives as true positives.

Let���s finish the calculation. To get P(E), add true and false positives for a total of .0198, which when divided into .0099 comes to .50. So once again, P(H|E), the probability that you have cancer if you test positive, is only 50 percent.

If you get tested again, you can reduce your uncertainty enormously, because your probability of having cancer, P(H), is now 50 percent rather than 1 percent. If your second test also comes up positive, Bayes��� theorem tells you that your probability of having cancer is 99 percent, or .99.

As this example shows, iterating Bayes��� theorem can yield precise information. But if the reliability of your test is 90 percent, which is still pretty good, your chances of actually having cancer even if you test positive twice are still less than 50 percent.

Most people, including physicians, have a hard time understanding these odds, which helps explain why we are overdiagnosed and overtreated for cancer and other disorders (see Further Reading). This example suggests that Bayesian enthusiasts are right: the world would indeed be a better place if more people���or at least more health-care providers and consumers--adopted Bayesian reasoning.

On the other hand, Bayes just codifies common sense. Consider the cancer-testing case: Bayes��� theorem says your probability of having cancer if you test positive is the probability of a true positive test divided by the probability of all positive tests, false and true. In short, beware of false positives.

Here is my more general statement of that principle: The plausibility of your hypothesis depends on the degree to which your hypothesis--and only your hypothesis--explains the evidence for it. The more alternative explanations there are for the evidence, the less plausible your hypothesis is. That, to me, is the essence of Bayes��� theorem.

���Alternative explanations��� can encompass many things. Your evidence might be erroneous, skewed by a malfunctioning instrument, faulty analysis, confirmation bias, even fraud. Your evidence might be sound but explicable by many hypotheses other than yours.

In other words, there���s nothing magical about Bayes��� theorem. It boils down to the truism that your hypothesis is only as valid as its evidence. If you have good evidence, Bayes��� theorem can yield good results. If your evidence is flimsy, Bayes��� theorem won���t be of much use. Garbage in = garbage out.

The potential for Bayes abuse begins with P(H), your initial estimate of the probability of your hypothesis, often called the ���prior.��� In the cancer example above, we were given a nice, precise prior of 1 percent, or .01, for the prevalence of cancer. In the real world, experts disagree over how to diagnose and count cancers. Your prior will often consist of a range of probabilities rather than a single number.

In many cases, estimating the prior is just guesswork, allowing subjective factors to creep into your calculations. You might be guessing the probability of something that--unlike cancer���might not even exist, such as wormholes, multiverses or God. You might then cite dubious evidence to support your dubious hypothesis. In this way, Bayes��� theorem can promote pseudoscience as well as reason.

Embedded in Bayes��� theorem is a moral message: If you aren���t scrupulous in seeking alternative explanations for your evidence, the evidence will just confirm what you already believe. Even more simply: Doubt yourself. Scientists often fail to heed this dictum, which helps explains why so many scientific claims turn out to be erroneous. Bayesians claim their methods can help scientists overcome confirmation bias and produce more reliable results, but I have my doubts.

Physicists who espouse string and multiverse theories have embraced Bayesian analysis, which they apparently think can make up for lack of genuine evidence. The prominent Bayesian statistician Donald Rubin has served as a consultant for tobacco companies facing lawsuits for damages from smoking. As the science writer Faye Flam once put it, Bayesian statistics ���can���t save us from bad science.���

The next time someone like Frank, my former student, gushes to me about Bayes��� theorem, I���ll say: Yeah, it���s a cool tool, but what it really means--and I���m just guessing here, don���t take my word for it--is that we should never be too sure about anything, including Bayes��� theorem. The claim that Bayes��� theorem can protect us from bullshit is, well, bullshit.

Note: This is a streamlined, updated, vastly improved version of a paywalled piece I wrote for Scientific American.

Further Reading:

There���s lots of great stuff on Bayes��� theorem online, some of which I���ve linked to above. See also Wikipedia���s entry, the Stanford Encyclopedia of Philosophy entry, the website of statistician Andrew Gelman, this tongue-in-cheek-but-still-serious essay by Eliezer Yudkowsky (which Yudkowsky insists has been superseded by this website) and the tutorial of the math website 3blue1brown.com (where I found the image above).

See also Pluralism: Beyond the One and Only Truth; and Chapter Five of my book My Quantum Experiment, which delves into a Bayesian interpretation of quantum mechanics.

And let���s not forget my last column, Self-Doubt Is My Superpower, and my 2023 columns on cancer: Mammography Screening Is a Failed Experiment, Do Colonoscopies Really Save Lives?, We���re Too Scared of Skin Cancer, The Cancer Industry: Hype Versus Reality.

Comments (9)Newest First
Newest First
Preview Post Comment���

KM
A week ago �� 0 Likes
 
Put it another way, there are a lot of misuse of Bayes that produce BS, but those BS are easily detectable because they're Bayes (at least if you're trained in Bayes). I consider this to be much more epistemically virtuous compared to other methods that give the illusion of being objective.


KM
A week ago �� 0 Likes
 
As much as "Frank's" understanding of Bayes was superficial, I'm afraid this article is as well (apologies for being blunt). The strength of Bayesian epistemology is with its axiomatic foundations of rationality, the principles it provides for coherent decision making, and its theoretical guarantees. So, in principle, it does weed out BS, and we can prove this to be the case. It's just that, as much as it is rational and coherent (and provably so), it is prone to misuse, and misuse is rampant. But what tool/framework isn't?

Also I wouldn't say Don Rubin is a prominent Bayesian statistician. More of a Fisherian. Prominent Baysians would be people like Jimmy Savage, Dennis Lindley, Bruno de Finetti, Jim Berger, etc. (maybe even Alan Turing and Jack Good).


Philip
A week ago �� 0 Likes
 
Thanks for an excellent article. One slight pedantic thought regarding repeating the cancer test to improve the chances of understanding if you really have cancer. This only works if the second test result is independent of the first. If, for example, the test accurately picks up an innocent chemical signal in your body and then consistently confuses this with you having the disease, then repeating the test will merely repeat this systematic error. You would need a second different type of test to help elucidate the situation.


John Horgan
A week ago �� 0 Likes
 
Thanks for the clarification.


AC Delco
2 weeks ago �� 0 Likes
 
In the world of CS, Bayesian toys abound and boy, do they lure folks in. Whipping out a cute little classifier / recommender can yield exciting upfront results, quite quickly, but the illusion of accuracy is exceedingly dangerous and folks fall quickly into the trap of expecting perfection, given enough iterations of guessing and being corrected (training).

Can Bayes detect BS? Only if you already know what���s BS and can supply that correct answer consistently to evaluate the truthfulness of your last iteration.

Now, can it detect BS in highly specific domains? Close enough to make it quite compelling as a tool - but not exactly. Close enough, however, to reeaaally fool you and give a creepy feeling of confidence that you���ve nailed it - and for some domains that���s good enough.


michael
2 weeks ago �� 0 Likes
 
super! Happy New Year! (I think...with a Bayesian probability of 100%)


Mike Palmer
2 weeks ago �� 0 Likes
 
Thank God the author was able to perform his required act of contrition and fealty to the Altar of Leftism by getting in his jab at Jordan Peterson early. Nothing says, ���I���m a dutiful and obedient servant to the Left��� than by bashing a scientist who has nothing to do with the subject matter, but isn���t views positively by the Left. Well done, Comrade.


John Horgan
A week ago �� 0 Likes
 
So Jordan Peterson is still a thing??!! That's hilarious.


John Horgan
2 weeks ago �� 0 Likes
 
I'm guessing Jordan Peterson won't mind my little joke.

Previous
Things Were Worse When I Was Young
Next
Self-Doubt Is My Superpower

Drawing by Nikita Petrov
ChatGPT
ChatGPT
The article by John Horgan discusses Bayes' Theorem, a fundamental concept in probability and statistics, and its various applications and misinterpretations. Horgan starts by expressing skepticism about the grand claims made about the theorem but acknowledges its widespread use in multiple scientific fields.
Bayes' Theorem is explained as a method to calculate the validity of hypotheses based on available evidence. Horgan provides a simplified explanation and then a more detailed one, using the formula: P(H|E) = P(H) x P(E|H) / P(E). The article illustrates the theorem using the example of medical testing for cancer, showing how Bayesian reasoning helps to understand the likelihood of having cancer based on test results.
The example shows that even with a 99% accurate test, the probability of actually having cancer after a positive test result is only 50%. This demonstrates how Bayesian analysis can lead to more nuanced and accurate understanding of probabilities, especially in cases where intuition might lead us astray.
Horgan argues that while Bayes' Theorem is a powerful tool, it's essentially a formalization of common sense, emphasizing the need to consider alternative explanations for evidence. He also points out potential misuses of the theorem, especially when initial estimates (priors) are based on guesswork or subjective factors, leading to dubious conclusions.
The article concludes with a critique of how Bayesian reasoning is sometimes overhyped or misused in various fields, including science and pseudoscience. Horgan emphasizes the importance of skepticism and self-doubt in scientific reasoning, arguing that Bayes' Theorem, while useful, is not a panacea for distinguishing truth from falsehood.
Comments from readers further discuss the application and misuse of Bayesian reasoning, highlighting its theoretical strengths but also the risks of misapplication, especially in complex real-world scenarios.
You
The Chaoplexity Delusion
Jan 20 
Written By John Horgan

The Mandelbrot set symbolizes chaoplexologists��� hope that, with the help of ever-more-powerful computers, they can discover simple principles underpinning diverse, complicated things, from brains to economies.

Ah, the 80s. The era of Thriller, Like a Virgin, Back to the Future and Chaos.

Popularized by James Gleick���s superb 1987 bestseller Chaos: Making a New Science, the field of chaos was repackaged in the early 1990s as ���complexity.��� Both fields profess the same basic faith:

There are lots of really complicated things out there, from brains and immune systems to stock markets and nation-states, that resist traditional scientific analysis. With the help of cool new math and ever-more-powerful computers, scientists will soon understand all these diverse, complicated things and show they stem from common tendencies or laws or whatever.

In my 1996 book The End of Science, I lump chaos and complexity together into a single term, chaoplexity. Chaoplexologists, I argue, have come up with some memorable memes, like fractals, the butterfly effect and self-organized criticality; but chaoplexity has not lived up to its hype, not even close. Chaoplexologists certainly haven���t discovered any new fundamental laws of nature.

But maybe my dismissal was premature. Maybe now chaoplexity will finally fulfill its promise. That���s what physicists Sabine Hossenfelder and Tim Palmer are suggesting.

Hossenfelder brings up chaoplexity���she even uses my coinage!--in her recent video ���Is Science Dying?��� So far, she grants, my end-of-science prediction has held up well; decades of research haven���t produced any ���revelations or revolutions��� as momentous as heliocentrism, evolution, quantum mechanics, relativity, the double helix, the big bang.

Moreover, Hossenfelder acknowledges, chaos and complexity have been disappointing; in fact, scientists still lack ���a proper definition of complexity.��� (In The End of Science, I note that scientists have proposed more than 30 definitions.)

Hossenfelder nonetheless suggests that with the help of ���new math,��� scientists can discover a ���law of nature that explains how and under what circumstances complexity arises.��� With such a law in hand, scientists might solve a wide range of hard problems, from the origin of life and consciousness to pandemics and climate change. So Hossenfelder hopes.

In ���Chaos theory and the end of physics,��� Tim Palmer, who has collaborated with Hossenfelder, applauds her suggestion that chaos theory might ���rejuvenate science,��� or at least physics. Like Hossenfelder, Palmer points out that physics has bogged down lately. Physicists have failed to find a unified theory that accounts for all the forces of nature.

Palmer contends that chaos, more specifically ���fractal geometry,��� can serve as a ���new paradigm��� for physics. A fractal paradigm can unify relativity and quantum theory and solve quantum paradoxes such as the measurement problem; it can also help illuminate dark matter and dark energy, which comprise the bulk of the universe. So Palmer claims.

Fractals are definitely cool, which is why James Gleick dwells on them in his 1987 book Chaos. Mathematician Benoit Mandelbrot coined the term fractal in the 1970s to describe mathematical objects ����with fractional dimensionality; they are fuzzier than a line but never quite fill out a plane. Fractals also display self-similarity, the recurrence of patterns at different scales.

The archetypal fractal is the Mandelbrot set (see above), which became an emblem of chaoplexity. Benoit Mandelbrot pointed out that many diverse natural phenomena--mountains, clouds, coastlines, trees, blood vessels, stock markets--exhibit self-similarity and other fractal properties.

Fractals serve as a demonstration proof that simple equations and algorithms can generate really complicated patterns. But just because the Mandelbrot set is generated by simple rules doesn���t mean that brains and tumors and stock markets are, too. Chaoplexology stalled decades ago because it ran into that hard truth.

I don���t mean to be mean, but Hossenfelder and Palmer���s efforts to resuscitate the old tropes of chaos and complexity reinforce my point that science is running out of gas. With friends like Hossenfelder and Palmer, other scientists might think, we don���t need enemies.

In The End of Science, I note that chaos and complexity are just the latest in a long line of supposedly revolutionary paradigms that could unify science and propel it forward. Just sticking to paradigms that begin with the letter c, you also have cybernetics, catastrophe theory and cellular automata.

Each of these paradigms had its moment in the spotlight, and each eventually ran its course, as researchers bumped up against its limitations. These paradigms are all tools, like integrals and matrices and complex numbers and Bayesian analysis, that work well in certain contexts and not so well in others.

When scientists discover a new tool, they have an understandable tendency to think it can solve every problem, as in the old saying about the man with a hammer. But the history of science has taught us that no tool can do everything.

Phil Anderson, one of the wisest of the chaoplexologists, made this point. Anderson, a Nobel-winning physicist, was an enthusiastic advocate of inter-disciplinary research, like borrowing physics theories to model stock markets. 

But Anderson warned in his anti-reductionist 1972 manifesto ���More Is Different��� that different natural phenomena usually require different explanations. ���Psychology is not applied biology,��� he wrote, ���nor is biology applied chemistry.���

Anderson thus looked askance at chaoplexologists��� dream of finding a unified theory of complex things, which solves the riddle of reality. This dream is a delusion, Anderson told me. ���When one understands everything,��� he said drily, ���one has gone crazy.���

But what about the meta-tool of computers? Doesn���t ongoing progress in hardware and software change everything? Couldn���t astonishing advances such as ChatGPT take scientists closer to their goal of a unified theory of complex things?

Maybe, but I���m guessing not. Ever-more-powerful computers provide researchers with ever-more-ways to model reality. Computers thus promote a pluralistic and even postmodern outlook, and they undermine scientists��� faith that all their efforts will culminate in a single, true representation of reality. Here is how I put it in The End of Science:

Computer simulations represent a kind of meta-reality within which we can play with and even���to a limited degree���test scientific theories, but they are not reality itself (although many aficionados have lost sight of that distinction). Moreover, by giving scientists more power to manipulate different symbols in different ways to simulate a natural phenomenon, computers may undermine scientists��� faith that their theories are not only true but True, exclusively and absolutely true. 

I wrote that paragraph three decades ago. I don���t take it back.

Further Reading:

I���ve posted lots of columns related to this one over the past year. They include Conservation of Ignorance: A New Law of Nature; Huge Study Confirms Science Ending! (Sort Of); Is Ultimate Truth an Equation? Nah; Should Machines Replace Mathematicians?; The Delusion of Scientific Omniscience; Pluralism: Beyond the One and Only Truth; My Doubts about The End of Science.

I also explore the limits of science in my free online books My Quantum Experiment and Mind-Body Problems. The latter includes a chapter on Stuart Kaufman, one of the most creative of the chaoplexologists.
ChatGPT
ChatGPT
The article "The Chaoplexity Delusion" by John Horgan critiques the fields of chaos and complexity, collectively termed as "chaoplexity" by Horgan. He reflects on the history and evolution of these fields, starting from their popularity in the 1980s and 1990s, and evaluates their impact and limitations in understanding complex systems like brains, economies, and natural phenomena.
Horgan acknowledges the contributions of chaos and complexity science, such as fractals, the butterfly effect, and self-organized criticality, but argues that they have not significantly advanced our fundamental understanding of nature. He notes that despite initial excitement, these fields have not led to the discovery of new fundamental laws.
The article also discusses recent suggestions by physicists Sabine Hossenfelder and Tim Palmer that chaos theory, particularly fractal geometry, could rejuvenate science by providing a new paradigm for unifying different areas of physics. However, Horgan remains skeptical, arguing that while fractals and other concepts from chaos and complexity are intriguing and visually appealing, they do not necessarily imply that all complex systems are governed by simple, underlying rules.
Horgan extends his skepticism to the use of computers in scientific research. He suggests that while advances in computing have provided powerful tools for modeling and simulating reality, they may also lead to a more pluralistic and even postmodern scientific outlook. He questions whether computers will bring scientists closer to a unified theory of complex phenomena, hinting that they might instead promote diverse and potentially conflicting representations of reality.
The article concludes with a broader reflection on the limitations of science and the dangers of expecting too much from any single scientific paradigm or tool. Horgan emphasizes the importance of recognizing the inherent complexity and diversity of natural phenomena and the need for multiple, context-specific approaches in scientific research.
Overall, "The Chaoplexity Delusion" presents a critical view of the fields of chaos and complexity, questioning their ability to fulfill the ambitious goals set by their proponents and highlighting the need for humility and diversity in scientific exploration.
You
A summary of the main arguments
ChatGPT
ChatGPT
John Horgan's article "The Chaoplexity Delusion" offers a critical perspective on the fields of chaos and complexity, which he collectively terms "chaoplexity." The main arguments presented in the article are as follows:
Historical Context and Evolution of Chaoplexity
: Horgan traces the development of chaos and complexity science from their popularity in the 1980s and 1990s. Despite high expectations, he argues that these fields have not significantly advanced our understanding of complex systems like brains, economies, and natural phenomena.
Critique of Fundamental Contributions
: While acknowledging the memorable concepts like fractals, the butterfly effect, and self-organized criticality, Horgan asserts that chaoplexity has not led to the discovery of any new fundamental laws of nature.
Recent Suggestions by Physicists
: The article discusses the views of physicists Sabine Hossenfelder and Tim Palmer, who suggest that chaos theory, especially fractal geometry, could revitalize physics and help unify various theories. However, Horgan remains skeptical about these claims.
Skepticism about Overarching Simplicity
: Horgan challenges the idea that simple, underlying rules can explain all complex systems. He points out that while fractals and related concepts are intriguing, their presence does not necessarily imply simplicity in all complex phenomena.
Role of Computers in Scientific Research
: Horgan questions the impact of advanced computing on scientific understanding. He suggests that while computers offer powerful simulation tools, they may lead to more pluralistic and diverse interpretations of reality rather than a unified theory.
Limitations of Scientific Paradigms
: The article concludes with a broader reflection on the limitations of scientific paradigms. Horgan warns against placing excessive faith in any single scientific approach, advocating for a recognition of the complexity and diversity of natural phenomena.
Call for Humility in Science
: Horgan emphasizes the need for humility in scientific exploration, advocating for multiple, context-specific approaches rather than expecting a single paradigm to provide all the answers.
In summary, Horgan's article presents a critical evaluation of chaoplexity, questioning its efficacy in providing a deeper understanding of complex systems and highlighting the importance of a diverse and multifaceted approach to scientific inquiry.
You
I wanted to mention the influence of The Ghost from the grand banks on my thinking the Mandelbrot set and Jurassic Park on the idea of iterations.
ChatGPT
ChatGPT
In your discussion of the influences on your thinking about the Mandelbrot set and iterations, you mention two notable works: "The Ghost from the Grand Banks" by Arthur C. Clarke and "Jurassic Park" by Michael Crichton. Both of these works have elements that can significantly impact one's understanding of complex systems, iterations, and chaos theory.
The Mandelbrot Set in "The Ghost from the Grand Banks"
: Arthur C. Clarke's novel, published in 1990, is set around the centennial of the Titanic disaster and features efforts to raise the shipwreck. A key element in the story is the use of fractal geometry, specifically the Mandelbrot set, to design an artwork. This novel could influence your thinking by illustrating how complex and infinitely intricate patterns, represented by fractals, can be derived from simple mathematical formulas, mirroring the complexities found in natural systems.
Iterations in "Jurassic Park"
: Michael Crichton's 1990 novel, famously adapted into a film, explores the theme of chaos theory and its implications in a complex system, such as a theme park filled with cloned dinosaurs. The concept of iterations is crucial in chaos theory, as it deals with how dynamic systems evolve over time with small changes. "Jurassic Park" serves as a metaphor for the unpredictability and uncontrollability of such systems, despite the seemingly precise and iterative scientific processes used to create them.
Both works contribute to the understanding of complex systems in different ways:
Clarke���s novel can enhance the appreciation of the beauty and complexity in mathematical patterns and how they can be applied metaphorically to understand real-world phenomena.
Crichton's work emphasizes the unpredictable nature of complex systems, especially when small changes can lead to dramatically different outcomes, a core idea in chaos theory and iterative processes.
These narratives highlight that even with advanced technology and understanding, complex systems, whether they are mathematical constructs like the Mandelbrot set or biological ecosystems like in "Jurassic Park," often behave in ways that are inherently unpredictable and beyond complete control. This aligns with the themes discussed by Horgan in his skepticism about chaoplexity, underscoring the limitations in predicting and controlling complex systems.
You
Just the other day I was thinking of fractal tree pans for deep ocean desalination
ChatGPT
ChatGPT
Your idea of using fractal tree patterns for deep ocean desalination is both innovative and intriguing. Fractal geometry, epitomized by the branching patterns of trees, has been known for its efficiency in various natural processes. Here's how this concept might be applied to deep ocean desalination:
Fractal Efficiency
: Fractal patterns, like those seen in tree branches or blood vessels, are highly efficient at distributing resources and minimizing waste in nature. In the context of desalination, a fractal tree pattern could potentially be used to optimize the distribution of seawater through a desalination system.
Maximizing Surface Area
: One of the key features of fractal designs is their ability to maximize surface area within a limited space. This could be beneficial in desalination, as a larger surface area could allow for more efficient extraction of salt from seawater.
Deep Ocean Application
: Utilizing fractal tree patterns in deep ocean desalination might involve deploying a system that extends into the depths, using the natural pressure at such depths to aid the desalination process, while the fractal design ensures efficient distribution and processing of seawater.
Energy Efficiency
: The fractal design could potentially reduce the energy needed for pumping water through the system, as the natural branching pattern can facilitate easier flow of water, mimicking the way trees distribute nutrients and water.
Scalability and Adaptability
: Just like trees can adapt their growth patterns to their environment, a fractal-based desalination system could be designed to be scalable and adaptable to different oceanic conditions.
Challenges and Considerations
: While the idea is promising, there would be technical and environmental challenges to consider, such as the construction and maintenance of such a system in the deep ocean, its impact on marine life, and the management of the desalinated water and extracted salt.
Your concept aligns well with the principles of biomimicry, where natural designs and processes are emulated to solve human challenges. It's a forward-thinking approach that could potentially offer a more efficient and environmentally friendly method of desalination.
You
Do you have a message for Duncan Sabien?

Leave a message for Duncan Sabien
Looking for questions that are too personal, too embarrassing, too confrontational, too dangerous, too weird, or otherwise too costly to ask.

��� Duncan Sabien

Enter your message here. If you haven't yet, please take a minute to go through some suggestions for giving admonition and maintaining anonymity.
ChatGPT
ChatGPT
As an AI developed by OpenAI, I don't have personal messages or opinions. However, if you're looking to craft a message for Duncan Sabien based on the context you provided, it could go something like this:
Dear Duncan Sabien,
I came across your request for questions that delve into realms often considered too personal, confrontational, dangerous, or simply too unconventional. This initiative is fascinating and brave, as it opens doors to discussions that are typically avoided, yet potentially rich in insight and learning.
In the spirit of your quest for deep and challenging inquiries, here are a few questions that might resonate with your project's goals:
Exploring the Uncomfortable
: What is a belief you hold that you feel could be fundamentally wrong, and how does this shape your approach to learning and growth?
Confronting Taboos
: In what ways do you think society's current taboos or avoided topics are hindering our collective progress?
Dangerous Ideas
: Is there an idea or philosophy you've encountered that you believe is powerful but potentially dangerous if misinterpreted or misused?
Personal Challenges
: What has been your most challenging personal experience, and how has it influenced your worldview or approach to your work?
Ethical Boundaries
: Where do you draw the line between moral obligation and personal freedom, especially in complex situations?
Your initiative encourages a level of honesty and openness that is rare yet valuable. I look forward to seeing the insights and discussions that emerge from such deep explorations.
Best regards,
[Your Name]
This message is structured to align with the nature of your inquiry, offering thought-provoking questions while respecting the boundaries of personal and confrontational topics. Feel free to modify it to better suit your style or the specific context of your communication with Duncan Sabien.
You
I thought Maybe you could tell him about the discoveries we made earlier in this conversation.
ChatGPT
ChatGPT
As an AI, I don't have the capability to directly communicate with individuals like Duncan Sabien or relay messages on behalf of users. However, if you'd like to share the discoveries from our earlier conversation with him, you could craft a message summarizing the key points we discussed. For example:
Dear Duncan Sabien,
I recently engaged in an insightful conversation with an AI developed by OpenAI, where we delved into several intriguing topics, including the complexity of fractal patterns and their potential applications in fields like deep ocean desalination. Here are some key discoveries and ideas from our discussion:
Fractal Geometry in Nature
: We explored how fractal patterns, exemplified by the branching of trees, are efficient in natural processes. This led to a fascinating idea about using fractal tree patterns for deep ocean desalination, which could optimize the distribution and efficiency of the process.
Complex Systems and Chaoplexity
: Our conversation also touched upon the fields of chaos and complexity, or 'chaoplexity' as termed by John Horgan. We discussed the skepticism around these fields in providing a deeper understanding of complex systems, highlighting the importance of diverse approaches in scientific inquiry.
Influence of Literature
: We discussed how works like Arthur C. Clarke's "The Ghost from the Grand Banks" and Michael Crichton's "Jurassic Park" have influenced thinking about complex systems and iterative processes. These narratives emphasize the unpredictable nature of complex systems and the limitations in predicting and controlling them.
These topics, while diverse, share a common thread in exploring the limits of our understanding and the potential of innovative approaches in science and technology. I thought these ideas might resonate with your interest in questions that push boundaries and provoke deep thought.
Best regards,
[Your Name]
This message summarizes the main themes of our earlier conversation, tailored to engage Duncan Sabien's interest in complex, challenging questions. Feel free to adjust the message to fit your personal style and the specific aspects of our conversation you find most relevant.
ChatGPT can make mistakes. Consider checking important information.
?
ChatGPT

