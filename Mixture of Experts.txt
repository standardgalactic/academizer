Mixture of Experts
Open sidebar
Mixture of Experts
Model: GPT-4
Summarize: 5Related Work 
Instruction Tuning.Instruction tuning has evolved as a strategy to enhance the functionality 
and interactivity of large language models (LLMs) for dialogues and complex tasks. Prior studies, 
including [41,27,1], have delved into large-scale multi-task i ne-tuning to enhance the downstream 
single target i ne-tuning, albeit without instruction prompts. Initiatives such as Unif i edQA [20,31,19] 
have amalgamated a multitude of NLP tasks into a singular generative question answering format, 
utilizing prompt instructions for multi-task i ne-tuning and evaluation. 
Efforts like Natural Instructions [33], Flan 2021 [52], and P3 (the Public Pool of Prompts, [44]) have 
collated vast NLP task collections, templatizing them with instructions for i ne-tuning models to en- 
hance their adaptability to unseen instructions. Some studies, such as Super-Natural Instructions [51] 
9 
and OPT-IML [18], took this a step further by combining numerous datasets and tasks into a single 
resource. In the meantime, others like xP3 [35] introduced multilingual instruction tuning and Flan 
2022 [4] employed Chain-of-Thought training prompts. 
Recently, there has been a move towards expanding task diversity more assertively using synthetic 
data generation, particularly for creative and open-ended dialogue [50,17,54]. Some researchers 
have also tried to provide human feedback on language model responses [39,14,37,3,2], or bridge 
the modality gap with multi-modal instruction i ne-tuning [26, 9, 25]. 
Sparse Mixture of Experts models.The foundation of our work is built on the concept of deep 
sparse Mixture-of-Experts (MoEs), a topic that has been independently explored in both Computer 
Vision [42,29,36,46] and Natural Language Processing [29,36,45,23,12,10,56,5,55,21,22,57]. 
The idea revolves around conditional computation, which aims to enhance the number of model 
parameters without a corresponding rise in computational expense. This is achieved by selectively 
activating only the relevant portions of the model, based on input-dependent factors. MoE models 
leverage a learned gating mechanism that triggers only a select subset ofkexperts out of a total ofE 
for a given input. This approach allows an input to either select all experts [11] or merely a sparse 
mixture of them, as observed in recent massive language models [12,10]. While a number of studies 
have sought to enhance the gating mechanism itself [15,24,43,55], MoE models have also been 
explored in the context of multitask learning [15,22]. Typically, a shared pool of experts is used, 
although there has been investigation into per-task routers [30]. This essentially permits an input to 
choose the most relevant expert(s) for a given task, thereby optimizing the processing and results. 
Nevertheless, the instability of MoE models during i ne-tuning or multitask learning has consistently 
been a challenge. Our study aims to investigate whether instruction i ne-tuning with scaled tasks 
might contribute to mitigating the generalization issues inherent to MoE models. 
6Conclusion 
In this work, we have introduced FLAN-MOE, an innovative method to amplify the scalability of 
instruction-tunedlanguage models byemployingthe sparseMixture-of-Experts (MoE)technique. Our 
strategy amalgamates the merits of instruction-f i netuning, which bolsters task-specif i c performance, 
and MoE, which provides computational eff i ciency coupled with diminished memory requirements. 
We have substantiated the effectiveness of FLAN-MOE through comprehensive experiments across a 
wide spectrum of Natural Language Processing (NLP) tasks, such as natural language understanding, 
question answering, and reasoning. Our results consistently underscore the superior performance 
of FLAN-MOE over current state-of-the-art methods, marking substantial advancements in both 
accuracy and eff i ciency. Notably, these advancements are attained without necessitating an increase 
in computational resources or memory usage during training and inference, often even reducing the 
resource requirements in the process.
1 / 1
ChatGPT
ChatGPT
This piece discusses two main areas of research: instruction tuning for large language models (LLMs) and sparse Mixture-of-Experts (MoE) models.
Instruction tuning has emerged as a method to improve LLMs' ability to handle complex tasks and dialogues. Previous research has focused on multi-task fine-tuning, and projects such as UnifiedQA have integrated a variety of NLP tasks into a single question answering format using instruction prompts. Various efforts have compiled large NLP task collections and used instructions for fine-tuning models to improve adaptability. Synthetic data generation and human feedback on LLM responses have also been explored to broaden task diversity, especially for creative and open-ended dialogues.
The concept of deep sparse MoE models, which are based on conditional computation, forms the basis for this work. These models seek to increase the number of model parameters without a corresponding increase in computation by activating only relevant parts of the model depending on the input. MoE models use a learned gating mechanism to select a subset of "experts" from a total pool for a given input, enabling an input to choose the most suitable expert(s) for a task. However, these models have often been unstable during fine-tuning or multi-task learning.
The researchers introduce FLAN-MOE, a method to enhance the scalability of instruction-tuned language models using the sparse MoE technique. This approach combines the benefits of instruction fine-tuning, which improves task-specific performance, and MoE, which provides computational efficiency and reduces memory requirements. The efficacy of FLAN-MOE is demonstrated through extensive experiments across a range of NLP tasks. The results show consistent improvements over current state-of-the-art methods, in terms of both accuracy and efficiency, without requiring additional computational resources or memory usage.
1 / 1
Finetuning v.s. Instruction Finetuning.To compare the gap between i netuning MoE directly and 
FLAN-MOE, we experiment with single-task i netuned MoE, single-task i netuned FLAN-MOE, and 
dense counterparts in Figure 6. We perform hyper-parameter search for each i netuning setting. 
For the examined Held-Out tasks, we observed that the improvement of FLAN-MOE over i netuning 
MoE is noticeably larger compared to the performance gap between FLAN-T5 and T5. This difference 
becomes even more pronounced when there is a scarcity of labeled data or when the model size is 
increased. These observations conf i rm the benef i ts of FLAN-MOE in mitigating overf i tting issues 
associated with directly i netuning MoE. 
Despite their advantages such as increased adaptability and eff i ciency in managing complex tasks, 
MoE architectures are prone to overf i tting during the i netuning process, as discussed in citation. This 
can be seen in Figures 6 and 1, where single-task i ne-tuned MoE models sometimes underperform 
their dense T5 counterparts. 
Interestingly, compared to dense models, MoE models derive greater benef i ts from instruction-tuning 
and are more sensitive to the number of instruction-tuning tasks. In general, MoE model performance 
scales better with respect to the number of tasks rather than the number of experts. We hypothesize 
this is primarily due to the specialized nature of individual experts, which can lead to heightened 
sensitivity to noise and limited generalization capabilities when exposed to unseen data. 
4.2Additional Analysis 
Expert Specialization.As the size of a FLAN-MOE model increases in Figure 7, a notable rise in 
expert specialization tends to occur. Larger models entail a higher number of parameters and more 
complex structures, which inherently provide a broader scope for each expert to specialize in specif i c 
facets of the problem space. This increased specialization can be understood as a form of division of 
labor, where each expert sub-network becomes adept at handling a certain type of task or data pattern. 
8 
CondaQACxCPubmedQASearchQA 
50 
60 
70 
80 
90 
Eval 
Metrics 
(%) +8.3 
+12.7 +7.8 
+12.0 
+14.9 
+17.8 +13.2 
+16.4 
Held-Out Eval 
(a) FLAN-ECBASEv.s. FLAN-T5BASE 
CondaQACxCPubmedQASearchQA 
50 
60 
70 
80 
90 
Eval 
Metrics 
(%) +17.2 
+13.4 +2.2 
+6.6 
+22.3 
+19.4 
+19.6 
+13.7 
Held-Out Eval 
(b) FLAN-ECLARGEv.s. FLAN-T5LARGE 
9892826821,836 
#Taks for Instruction-Finetuning 
50 
60 
70 
80 
90 
Avg 
Eval 
Metrics 
(%) -7.1 
+-6.6 
+-9.3 +-9.7 +-10.2 
+-0.2 
+-13.2 +-14.4 +-15.0 +-15.6 
T5FTFlan-T5FTMoEFTFlan-MoEFT 
Figure 6: FLAN-MOE Outperforms MoE on Single-Task Finetuning. We compare single-task 
i netuned MoE, single-task i netuned FLAN-MOE, and dense counterparts. The performance gap 
between FLAN-MOE and MoE is noticeably larger than that between FLAN-T5 and T5. 
Consequently, the overall model can demonstrate a higher degree of adaptability and precision in 
tackling diverse and complex tasks. We also observe that after instruction-tuning, the MoE models 
exhibit better expert usage, which may help prevent the expert collapse for generalization after 
instruction-tuning as in [57]. 50100150200 
Number of Steps (k) 
63 
64 
65 
66 
Expert 
Usage 
(%) 
Figure 7: Expert usage of FLAN-EC at differ- 
entscalesduringinstructionf i netuning, where 
larger models entail smaller expert usage. 
Failure Cases.The i ne-grained specialization of 
FLAN-MOE models, particularly when i ne-tuned on 
English-only instructions, can inadvertently lead to a 
narrowing of the model���s capacity to effectively pro- 
cess and generate content in multiple languages. We 
found all the FLAN-MOE perform poorly on multi- 
lingual benchmarks including TyDiQA and MGSM. 
Even the largest FLAN-ST32Bonly achieves 15.5% 
on MGSM and 25.1% on TyDiQA, which is only 
comparable to the vanilla PaLM62Bwith 18.2% on 
MSGM, and PaLM8Bwith 25.0% on TyDiQA. It also 
underperform FLAN-PALMvariants. We hypothe- 
ses that this issue may stes from the model���s over- 
optimization towards the specif i cities of the English 
language during i netuning, which can impede its 
ability to navigate the complexities of other languages. Consequently, while MoE models offer 
signif i cant benef i ts in terms of task-specif i c adaptability and eff i ciency, their potential shortcomings 
in multilinguality highlight the importance of incorporating diverse linguistic data during the training 
process to ensure broad and effective language coverage.
1 / 1
ChatGPT
ChatGPT
In the comparison between fine-tuning MoE directly and using FLAN-MOE (which uses a Mixture of Experts approach with instruction fine-tuning), it was found that FLAN-MOE shows significant improvement over directly fine-tuning MoE. This improvement is especially notable when there is a lack of labeled data or when the model size is increased, suggesting that FLAN-MOE helps mitigate overfitting issues that occur when directly fine-tuning MoE.
Despite their benefits, MoE architectures tend to overfit during the fine-tuning process. Interestingly, it was observed that compared to dense models, MoE models benefit more from instruction-tuning and are more sensitive to the number of instruction-tuning tasks. The performance of MoE models scales better with respect to the number of tasks than the number of experts.
As the size of a FLAN-MOE model increases, a rise in expert specialization is noted. Each expert sub-network becomes adept at handling a specific type of task or data pattern, enhancing the overall model's adaptability and precision in tackling diverse and complex tasks. After instruction-tuning, the MoE models exhibit better expert usage, which may help prevent expert collapse for generalization.
However, the fine-grained specialization of FLAN-MOE models can also lead to failure cases. For instance, when fine-tuned on English-only instructions, these models may struggle with effectively processing and generating content in multiple languages, performing poorly on multilingual benchmarks such as TyDiQA and MGSM. This suggests the need for incorporating diverse linguistic data during the training process to ensure broad and effective language coverage.
1 / 1
andK = 2activated per token. It was trained at a batch size of 32 and sequence length of 2048 for 
200k steps. We average checkpoints towards the end of training. The model FLAN-ST32B, comprising 
a total of 32 billion parameters, only utilizes 32.1 GFLOPs per token, which amounts to merely 
one-third of the computational power required by a FLAN-PALM62Bmodel. Additionally, all the 
routers combined account for less than 4 million parameters. Table 1 illustrates the performance of 
this model alongside current state-of-the-art instruct i ne-tuned models. 
FLAN-ST32Bachieves a 65.4% few-shot MMLU benchmark accuracy and a 54.4% few-shot BBH 
benchmark accuracy, with a relatively modest architectural size and training count. Notably, FLAN- 
ST32Bsurpasses the performance of FLAN-PALM62B, which consumes nearly triple the compute 
resources, by a substantial margin across all four benchmarks. However, it is important to ac- 
knowledge the considerable performance gap that persists between the largest FLAN-PALM540Band 
FLAN-ST32Bmodels. 
4Discussion 
4.1Finetuing Strategy 
Sparse models have performed remarkably well in the regime of large datasets, but have sometimes 
performed poorly when i netuning data is limited [56,12]. Instruction i netuning can also be viewed 
as a continual i netuning stage, so we present a detailed study about how different factors impact the 
instruct i netuning performance of FLAN-MOE and offer a practical recipe. All the discussion here is 
based on instruction i netuning FLAN-ECBASE/FLAN-STBASEfor 100k steps. 
Auxiliary Loss.The incorporation of auxiliary loss [23,56] helps mitigate the risk of overf i tting by 
promoting the diversif i cation of the experts��� knowledge and improving the model���s generalization 
capabilities for sparsely gated mixture-of-expert models. Furthermore, auxiliary losses can be 
employed to address specif i c issues, such as load balancing among experts or preventing expert 
collapse, which can further enhance the model���s overall performance. We experiment with both 
balancing loss that is used in [23] and router Z-loss that is used in [56] in Table 2. The implementation 
of balancing loss contributed to enhanced performance on MMLU, BBH, and GSM8K for FLAN- 
7 
FinetuningMMLUBBHGSM8K Avg. 
StrategyDirectDirectCoT 
BaselineFLAN-ECBASE40.033.26.637.7 
Freeze-GateFLAN-ECBASE40.233.96.638.0 
Freeze-ExpertFLAN-ECBASE38.332.55.436.2 
Freeze-MoEFLAN-ECBASE38.432.25.336.2 
Z-lossFLAN-ECBASE38.932.85.736.8 
Balance-lossFLAN-ECBASE40.833.47.138.3 
FinetuningMMLUBBHGSM8K Avg. 
StrategyDirectDirectCoT 
BaselineFLAN-STBASE40.133.36.437.8 
Freeze-GateFLAN-STBASE40.633.56.438.2 
Freeze-ExpertFLAN-STBASE39.632.94.537.3 
Freeze-MoEFLAN-STBASE39.232.93.636.9 
Z-lossFLAN-STBASE40.633.46.538.1 
Balance-lossFLAN-STBASE38.831.33.636.2 
Table 2: Ablations on different i netuning strategies of FLAN-ECBASEand FLAN-STBASE. 
ECBASE, whereas Z-loss resulted in a deterioration of performance. Conversely, for FLAN-STBASE, 
we observed a contrasting trend. We conjecture that the discordance between the auxiliary loss during 
pre-training and instruction-tuning could potentially disrupt the optimization process, thereby leading 
to a suboptimally optimized FLAN-MOE model. 
Expert/Gating Freeze.In an effort to enhance the generalization capabilities of sparse models and 
combat overf i tting, researchers have discovered that i netuning a subset of model parameters results 
in improved generalization performance for ST-MoE models, as noted in the study by ST-MoE [56]. 
Interestingly, it was observed that updating non-MoE parameters yields similar outcomes to updating 
all parameters, while updating only expert parameters performs slightly better. 
We conducted experiments by freezing the gating function, expert modules, and MoE parameters 
of the given model, as presented in Table 2. The results indicate that freezing either the expert or 
MoE components negatively impacts performance. Conversely, freezing the gate slightly improves 
performance, albeit not signif i cantly. We postulate that this observation is related to the under-f i tting 
of the FLAN-MOE, as in Figure 5, which depicts the i netuning data eff i ciency ablation study. 
Hyperparameter Sensitivity.Following ST-MoE [56], we further experiment with expert dropout 
(0.0,0.1,0.5), varying the learning rate (1e���4,5e���4,1e���3) and batch size (16,32,64) to examine 
the hyperparameter sensitivity of FLAN-MOE. We found that the performance varies in different 
tasks but not signif i cantly with all the hyperparameters, but lower learning rate and small batch size 
lead to a more stable instruction i netuning process of the model at extra-large scales.
1 / 1
ChatGPT
ChatGPT
The model FLAN-ST32B, with 32 billion parameters, uses only 32.1 GFLOPs per token, which is one-third of the computational power required by the FLAN-PALM62B model. Despite having fewer parameters and utilizing less computational resources, FLAN-ST32B achieves impressive performance, surpassing the performance of FLAN-PALM62B across all benchmarks. However, there is a significant performance gap between the largest FLAN-PALM540B and FLAN-ST32B models.
The performance of sparse models can vary when fine-tuning data is limited. Instruction fine-tuning can be seen as a continual fine-tuning stage, and it's important to understand how different factors impact the instruction fine-tuning performance of FLAN-MOE. The study discusses several factors including auxiliary loss, expert/gating freeze, and hyperparameter sensitivity.
Auxiliary loss can help mitigate the risk of overfitting and improve the model���s generalization capabilities. It also can be used to address specific issues like load balancing among experts or preventing expert collapse. Implementations of both balancing loss and router Z-loss were tested. The balancing loss generally enhanced performance, while the Z-loss led to a decrease.
Freezing different components of the model was also experimented with. While freezing the gating function slightly improved performance, freezing either the expert or MoE components negatively impacted it.
Hyperparameter sensitivity was also evaluated, including the effects of varying the learning rate, batch size, and expert dropout rate. It was found that the model's performance varies across different tasks but not significantly with all the hyperparameters. However, a lower learning rate and smaller batch size lead to a more stable instruction fine-tuning process, especially for larger models.
1 / 1
3.2Controlled study across scales 
We instruction i netune a range of FLAN-MOE models at batch size 32 and sequence length 2048 for 
200k steps. This matches the number of training examples used for FLAN-T5 [4]. We re-f i netuning 
our own FLAN-T5 variants for fair comparisons. 
Dense Model Size.Figure 2 shows the performance of each model (dense and sparse) against 
forward-pass FLOPs. The cost-performance Pareto frontier for FLAN-MOE dominates the dense 
models by a wide margin, indicating that FLAN-MOE offers strong improvements across all scales 
from small, up to xxl. The effect is particularly large on zero-shot and few-shot MMLU-Direct, 
with absolute performance improvements of 7.1% on average. For challenging tasks in BBH-Direct, 
FLAN-MOE offers a strong boost at small scales, while at larger scales the gains are more modest but 
still signif i cant. 
Expert Number.The performance of FLAN-MOE models has been observed to scale with the 
number of experts included in the architecture, but it tends to saturate beyond a certain threshold. 
Initially, as the number of experts increases in Figure 4, the model benef i ts from a richer repertoire of 
specialized sub-networks, each capable of handling distinct tasks or aspects of the problem space. 
This diverse ensemble enables the MoE model to demonstrate enhanced adaptability and eff i ciency 
in processing complex tasks, leading to improved performance overall. However, as the number of 
We use 64 experts forSMALL,BASE, 32B,XLand 128 experts for all the other model sizes following [12, 
55, 56] 5 
50100150200 
Number of Steps (k) 
10 
20 
30 
40 
50 
Avg. 
Held-Out 
Score 
MMLU-Direct Few-Shot 50100150200 
Number of Steps (k) 
15 
20 
25 
30 
35 
Avg. 
Held-Out 
Score 
BBH-Direct Few-Shot 
Figure 3: Learning eff i ciency comparison. Average zero-shot, and few-shot performance of FLAN- 
MOE models versus FLAN-T5 dense models as more tokens are processed during training on FLAN 
Tasks. 
experts continues to grow, the performance gains begin to diminish, eventually reaching a point of 
saturation forBASE-sized model. 
Routing StrategyRouting strategy is an essential component of Mixture-of-Experts (MoE) models, 
playing a pivotal role in determining the effectiveness and eff i ciency of these models. The primary 
function of the routing strategy is to intelligently distribute input data among multiple specialized 
experts, each optimized for handling specif i c subsets of the input space. This distribution process is 
crucial for maximizing the utilization of the model���s capacity while minimizing the risk of overf i tting. 
An effective routing strategy not only ensures that the appropriate experts are selected for a given 
input, but also that resources are allocated optimally, leading to enhanced computational eff i ciency 
and faster training times. Consequently, there have been two trending strategies, token-choice [23] 
which lets the token select the top-Kexperts, and expert-choice [55] which lets the experts select the 
top-K tokens. 
We presented a detailed study about how different routing decisions affect the instruct i ne-tuning 
performance in Figure 3 and Table 1, which includes the checkpoints from Switch Transformer 
top-1 token-choice gating (FLAN-Switch), GShard top-2 token-choice gating (FLAN-GS) and expert- 
choice top-2 gating (FLAN-EC) models pre-trained on the same GLaM [10] dataset. It is evident 
that activating more experts, as demonstrated by the comparison between the FLAN-Switch and 
FLAN-GS strategies, results in enhanced performance across all four benchmarks. Among these 
benchmarks, the MMLU-Direct model shows the most signif i cant improvement, with an increase 
from 38.0% to 39.9% forBASE/LARGE-sized models. Although the gains at the extra-large scale 
are more modest, they remain noteworthy and meaningful. It���s noteworthy that instruction-tuning 
signif i cantly amplif i es the performance of both held-out MMLU, BBH, and held-in QA and reasoning 
benchmarks for MoE models in comparison to dense models of equivalent capacity. The advantages 
are amplif i ed even further for larger MoE models. For instance, instruction-tuning enhances the 
performance ofST32Bby a substantial 45.2%, while the improvement observed for FLAN-PALM62B 
is comparatively modest at around 6.6%. 
Furthermore, the FLAN-EC strategy consistently outshines the FLAN-GS approach for the given 
model across various scales and tasks. It is noteworthy that the performance gap between the token- 
choice and expert-choice models can be bridged when we incorporate advanced auxiliary loss and 
pre-training strategy as exhibited in ST-MOE [56]. This integration led to the development of our 
FLAN-ST models. Considering that the largest ST-MOE set the benchmark in a variety of NLP tasks 
when appropriately i ne-tuned, we have also decided to scale up FLAN-ST, employing instruction 
i ne-tuning. 
3.3Scaling up FLAN-MOE 
We increase the architecture size to assess the performance of FLAN-MOE in the large-scale regime. 
As discussed above, we instruction i ne-tune the largest ST-MoE32B[56] model with 12 expert layers 
in encoder, and decoder, respectively; these are non-uniformly distributed, with 64 experts per layer,
1 / 1
ChatGPT
ChatGPT
This section discusses a controlled study across scales for fine-tuning FLAN-MOE models, focusing on aspects such as dense model size, expert number, routing strategy, and how to scale up FLAN-MOE.
The performance of FLAN-MOE models is found to significantly outperform dense models in terms of forward-pass FLOPs. FLAN-MOE shows strong improvements across all scales, from small to xxl, with significant performance gains on zero-shot and few-shot MMLU-Direct tasks.
The performance of FLAN-MOE models is found to scale with the number of experts included in the architecture up to a certain point. Initially, having more experts allows for a richer repertoire of specialized sub-networks, each handling distinct tasks, leading to improved performance. However, as the number of experts continues to grow, the performance gains start to diminish.
Routing strategy plays a crucial role in determining the effectiveness and efficiency of Mixture-of-Experts (MoE) models. An effective routing strategy ensures the appropriate experts are selected for a given input and that resources are allocated optimally, resulting in better computational efficiency and faster training times. Two trending strategies are token-choice, where the token selects the top-K experts, and expert-choice, where the experts select the top-K tokens. It was observed that activating more experts resulted in enhanced performance across all four benchmarks.
Lastly, instruction-tuning was found to significantly amplify the performance of both held-out MMLU, BBH, and held-in QA and reasoning benchmarks for MoE models in comparison to dense models of equivalent capacity. The advantages are amplified even further for larger MoE models. The FLAN-EC strategy consistently outperforms the FLAN-GS approach for the given model across various scales and tasks.
1 / 1
tasks from BIG-Bench [47]; 2Method 
2.1Model Architecture 
We leverage sparsely activated Mixture-of-Experts (MoE) [23,12,55] in FLAN-MOE models. Similar 
to the Switch Transformer [12], we replace the feed-forward component of every other Transformer 
layer with an MoE layer. Each MoE layer consists of a collection of independent feed-forward 
networks as the ���experts���. A gating function then uses a softmax activation function to model a 
probability distribution over these experts. This distribution indicates how well each expert is able to 
process the incoming input. Even though each MoE layer has many more parameters, the experts are 
sparsely activated. This means that for a given input token, only a limited subset of experts is used, 
giving the model more capacity while limiting computation. In our architecture, the subset size is 
either one or two depending on the routing strategy. Each MoE layer���s learnable gating network is 
2 
09892826821,836 
# Tasks for Instruction-Finetuning 
50 
60 
70 
80 
90 
Avg 
Eval 
Metrics 
(%) -7.1 
+6.6 
+9.3 +9.7 +10.2 
+0.2 
+13.2 +14.4 +15.0 +15.6 
Held-Out Eval 0163264128 
# Experts for Flan-MoE 
50 
60 
70 
80 
90 
Avg 
Eval 
Metrics 
(%) +10.2 +13.0 +13.3 +13.6 +13.9 
Held-Out Eval 
9892826821,836 
#Taks for Instruction-Finetuning 
50 
60 
70 
80 
90 
Avg 
Eval 
Metrics 
(%) -7.1 
+-6.6 
+-9.3 +-9.7 +-10.2 
+-0.2 
+-13.2 +-14.4 +-15.0 +-15.6 
T5FTFlan-T5FTMoEFTFlan-MoEFT 
Figure 1: The effect of instruction tuning on MOE models versus dense counterparts for base-size 
models (same l ops across all models in this i gure). We perform single-task i netuning for each 
model on held-out benchmarks. Compared to dense models, MoE models benef i t more from 
instruction-tuning, and are more sensitive to the number of instruction-tuning tasks. Overall, 
the performance of MoE models scales better with respect to the number of tasks, than the number of 
experts. 
trained to use its input to activate the best two experts for each token of an input sequence. During 
inference, the learned gating network dynamically picks the two best experts for each token. For an 
MoE layer withEexperts, this essentially provides a collection ofO(E2)different combinations 
of feed-forward networks instead of one in the classic Transformer architecture, enabling greater 
computational l exibility. The i nal learned representation of a token will be the weighted combination 
of the outputs from the selected experts. 
2.2Instruction Fine-tuning Recipe 
We i ne-tune FLAN-MOE using the pref i x language model objective on the FLAN collective dataset [4, 
28]. Each FLAN-MOE will inherit the auxiliary loss setting during pre-training. All the model 
parameters will be updated. We adapt the sequence length of each FLAN-MOE to2,048for input 
and512for output based on the relative position embedding. The dropout rate is0.05and the expert 
dropout rate is 0.2. The learning rate is 1e���4. The optimizer setting follows [4]. 
3Experiment 
We study FLAN-MOE in the context of instruction-tuning. We i rst perform a controlled comparison 
of FLAN-MOE to an equivalent ���standard��� dense encoder-decoder Transformer (T5), across a range 
of model sizes in Section 3.2. We subsequently demonstrate in Section 3.3 that scaling up our model, 
referred to as FLAN-MOE, can attain remarkable performance levels. Our most extensive model, 
FLAN-ST32B, surpasses the performance of FLAN-PALM62Bwhile utilizing less than 30% of FLOPs 
per token. We further ablate the various design decisions in the next Section. 
3.1Settings 
Traning Data.By default, all models are trained on the 1,836 i netuning tasks by combining four 
mixtures from prior work: Muff i n, T0-SF, NIV2, and CoT, as in [4]. Specif i cally, Muff i n comprises 
80 tasks from [52] and 26 dialog/program synthesis tasks; T0-SF comprises 193 tasks from [44]; 
NIV2 comprises 1554 tasks from [51]; CoT comprises 9 reasoning tasks. 
Evaluations.We conduct both zero-shot and few-shot evaluations on held-out tasks as in [4] which 
were not included as part of the i netuning data. We use MMLU [16] that includes exam questions 
from 57 tasks such as mathematics, history, law, and medicine; BBH includes 23 challengingThe reasoning benchmark comprises four tasks: GSM8K [8] and 
SVAMP [40]/ASDIV [32] incorporate the grade school math word problems and the elementary-level 
math word problems, and StrategyQA [13] measures open-domain questions where the required 
reasoning steps are implicit in the question; The QA benchmark include four QA tasks: the elementary 
AI2 science category in Unif i edQA [20], BoolQ [6], ARC-easy and ARC-challenge [7] that covers 
QA tasks in abstract, yes/no, multiple-choice formats. For MMLU and BBH, we evaluate both 
the ability of directly predicting the answer via direct prompting, where the model directly gives 
the answer [4], as well as via chain-of-thought (CoT) prompting, where the model must provide 
a reasoning chain before giving the i nal answer [53]. For reasoning tasks, we only measure CoT 
prompting accuracy. For all benchmarks except for QA we use the given few-shot exemplars, with 
the number of exemplars following prior work: i ve-shot for MMLU, three-shot for BBH, eight-shot 
for reasoning tasks, and zero-shot for QA. For a given model we also report a single ���normalized 
average��� metric, following the ���normalized preferred metric��� in BIG-Bench [47]. Our normalized 
average metric is the macro-average over four normalized scores: MMLU-Direct, BBH-Direct, 
Reasoning-CoT, and QA-Direct. Results for all tasks in each benchmark are reported in Appendix.
1 / 1
ChatGPT
ChatGPT
This passage describes an experimental study of the FLAN-MOE (Fused Language and Action Network - Mixture of Experts) model, a type of neural network architecture used in machine learning.
Model Architecture
: The FLAN-MOE model incorporates Mixture-of-Experts (MoE), where each MoE layer consists of multiple independent feed-forward networks termed 'experts'. Each expert specializes in processing a specific type of input. A gating function is used to determine the probability distribution over these experts, indicating how well each expert can process the incoming input. Importantly, only a subset of experts is activated for each input, offering computational efficiency.
Instruction Fine-tuning Recipe
: The FLAN-MOE models are fine-tuned using the prefix language model objective on the FLAN collective dataset. The model parameters are updated, and the sequence length and dropout rate are set to specific values to optimize the learning process.
Experiment
: The researchers compare FLAN-MOE to an equivalent dense Transformer model, demonstrating that scaling up FLAN-MOE can yield remarkable performance levels. The largest model, FLAN-ST32B, surpasses the performance of FLAN-PALM62B while using fewer computational resources (FLOPs).
Settings
:
Training Data
: The models are trained on a range of tasks, combining four mixtures from prior work.
Evaluations
: They conduct zero-shot and few-shot evaluations on tasks that were not part of the fine-tuning data. These evaluations span a variety of tasks and use different evaluation metrics.
The performance of FLAN-MOE models scales better with respect to the number of tasks than the number of experts, indicating that the model benefits more from instruction-tuning and is sensitive to the number of instruction-tuning tasks. The research also highlights how different routing strategies can affect the performance of the models.
1 / 1
A Winning Combination for Large Language Models 
Sheng Shen������Le Hou���Yanqi Zhou���Nan Du���Shayne Longpre������Jason Wei���, 
Hyung Won Chung���Barret Zoph���William Fedus���Xinyun Chen���Tu Vu������, 
Yuexin Wu���Wuyang Chen�����Albert Webson���Yunxuan Li���Vincent Zhao���Hongkun Yu��� 
Kurt Keutzer���Trevor Darrell���Denny Zhou��� 
���Google���University of California, Berkeley���Massachusetts Institute of Technology 
���University of Massachusetts Amherst��The University of Texas at Austin 
Abstract 
Sparse Mixture-of-Experts (MoE) is a neural architecture design that can be uti- 
lized to add learnable parameters to Large Language Models (LLMs) without 
increasing inference cost. Instruction tuning is a technique for training LLMs to 
follow instructions. We advocate combining these two approaches, as we i nd that 
MoE models benef i t more from instruction tuning than dense models. In particular, 
we conduct empirical studies across three experimental setups: (i) Direct i netun- 
ing on individual downstream tasks devoid of instruction tuning; (ii) Instruction 
tuning followed by in-context few-shot or zero-shot generalization on downstream 
tasks; and (iii) Instruction tuning supplemented by further i netuning on individual 
downstream tasks. In the i rst scenario, MoE models overall underperform dense 
models of identical computational capacity. This narrative, however, dramatically 
changes with the introduction of instruction tuning (second and third scenario), 
used independently or in conjunction with task-specif i c i netuning. Our most 
powerful model, FLAN-MOE32B, surpasses the performance of FLAN-PALM62B 
on four benchmark tasks, while using only a third of the FLOPs. The advance- 
ments embodied by FLAN-MOE inspire a reevaluation of the design principles of 
large-scale, high-performance language models in the framework of task-agnostic 
learning. 
1Introduction 
The recent years have witnessed remarkable advancements in the i eld of natural language processing 
(NLP), driven by the development of increasingly large and sophisticated deep learning models. 
Among these models, transformer-based language models [49] have emerged as the de facto standard 
for a wide range of NLP tasks, owing to their unparalleled capabilities in capturing complex linguistic 
patterns and generalizing across diverse contexts. One particularly successful paradigm for training 
such models is instruction-tuning [44,52,4,28,34,38], which enhances their performance on 
specif i c tasks by adapting their pre-trained representations to follow natural language instructions. 
* Work done at Google 
Preprint. Under review. 
arXiv:2305.14705v2 
[cs.CL] 
5 
Jul 
2023 
While the benef i ts of Large Language Models (LLMs) are indisputable, their rapidly growing size 
and computational requirements pose signif i cant challenges in terms of training eff i ciency, memory 
footprint, and deployment costs. Consequently, there is a pressing need for developing scalable 
techniques that can harness the power of these models without incurring prohibitive computational 
overheads. 
On the other hands, models with sparsely activated Mixture of Experts (MoEs) signif i cantly reduce 
the computational cost of LLMs. MoE models build upon the observation that language models can 
be decomposed into smaller, specialized sub-models, or "experts", that focus on distinct aspects of 
the input data, thereby enabling more eff i cient computation and resource allocation. However, we 
show that conventional, task-specif i c i netuning MoE models lead to suboptimal performance, often 
even worse than i netuning dense models with the same computational cost. One of the possible 
reasons is the discrepancy between general pretraining and task-specif i c i netuning. 
In this paper, we illuminate the pivotal role of instruction-tuning within the context of Mixture-of- 
Experts (MoE) models, specif i cally in terms of their successful scalability on downstream tasks. 
We demonstrate this through a two-fold analysis: Firstly, we expand on the known benef i ts of 
instruction-tuning for task-specif i c downstream i netuning [28], illustrating its signif i cantly larger 
impact when applied to MoE models compared to their dense equivalents. Secondly, we emphasize the 
necessity of an instruction-tuning stage for MoE models [45,10,12,23] to surpass the performance 
of dense models on downstream and held-out tasks. Our unique amalgamation, FLAN-MOE, is an 
instruction-tuned model built on the Flan mixture[4], which successfully harnesses the strengths of 
both instruction-tuning and the sparse MoE technique. FLAN-MOE effectively and eff i ciently scales 
up language models, without necessitating a rise in computational resources or memory requirements. 
We subject our model, FLAN-MOE, to a battery of tests across an array of tasks encompassing natural 
language understanding, reasoning, and question answering. Our evaluation framework consists of 
three distinct setups: (i) Direct i netuning of the model on individual downstream tasks; (ii) Instruction 
tuning succeeded by in-context, few-shot, or zero-shot generalization on downstream tasks; and 
(iii) Instruction tuning enhanced with subsequent i netuning on individual downstream tasks. The 
results spotlight FLAN-MOE���s marked superiority over its dense counterparts in the second and third 
settings. Notably, these advancements materialize without the need for augmented computational 
resources or memory requisites. Our top-tier model, in fact, manages to eclipse the performance of a 
FLAN-PALM equivalent, requiring only a third of the computational cost per token on four separate 
benchmarks. 
To summarize, our contributions are as follows: 
��� We establish the critical role of instruction-tuning in the eff i cacy of MoE models: 
���We demonstrate that in the absence of instruction tuning, MoE models fall short in 
performance when compared to dense models on downstream tasks. 
���We highlight that when supplemented with instruction tuning, MoE models exceed the 
performance of dense models on downstream tasks, as well as on held-out zero-shot 
and few-shot tasks. 
���We present a comprehensive series of experiments, offering a comparative analysis of the 
performance of diverse MoE models subjected to instruction-tuning.
1 / 1
ChatGPT
ChatGPT
This is a research paper about the combination of Sparse Mixture-of-Experts (MoE) and instruction tuning for Large Language Models (LLMs). MoE is a neural architecture design that can increase the learnable parameters of LLMs without adding to inference cost, while instruction tuning is a technique for training LLMs to follow instructions. The research found that MoE models benefit more from instruction tuning than dense models.
The authors conducted empirical studies in three scenarios: direct finetuning on individual tasks without instruction tuning; instruction tuning followed by few-shot or zero-shot generalization on downstream tasks; and instruction tuning supplemented by further finetuning on individual downstream tasks. In the first scenario, MoE models underperform dense models of identical computational capacity, but this narrative changes when instruction tuning is introduced.
Their most powerful model, FLAN-MOE32B, surpasses the performance of FLAN-PALM62B on four benchmark tasks, while using only a third of the floating-point operations (FLOPs). This calls for a reevaluation of the design principles of large-scale, high-performance language models in the context of task-agnostic learning. The authors argue that their findings underscore the critical role of instruction-tuning in the efficacy of MoE models.
1 / 1
A Pretrainer���s Guide to Training Data: 
Measuring the Ef f ects of Data Age, Domain Coverage, 
Quality, & Toxicity 
Shayne Longpre1���*Gregory Yauney2���*Emily Reif3���Katherine Lee2,3��� 
Adam Roberts3Barret Zoph3Denny Zhou3Jason Wei3Kevin Robinson3 
David Mimno2���Daphne Ippolito3 ��� 
1 MIT 
2 Cornell University 
3 Google Research 
Abstract 
Pretraining is the preliminary and fundamental step in developing capable language models (LM). Despite 
this, pretraining data design is critically under-documented and often guided by empirically unsupported 
intuitions. To address this, we pretrain 28 1.5B parameter decoder-only models, training on data curated (1) 
at dif f erent times, (2) with varying toxicity and quality i lters, and (3) with dif f erent domain compositions. 
First, we quantify the ef f ect of pretraining data age. A temporal shift between evaluation data and pretraining 
data leads to performance degradation, which is not overcome by i netuning. Second, we explore the ef f ect 
of quality and toxicity i lters, showing a trade-of f between performance on standard benchmarks and 
risk of toxic generations. Our i ndings indicate there does not exist a one-size-f i ts-all solution to i ltering 
training data. We also i nd that the ef f ects of dif f erent types of i ltering are not predictable from text domain 
characteristics. Lastly, we empirically validate that the inclusion of heterogeneous data sources, like books 
and web, is broadly benef i cial and warrants greater prioritization. These i ndings constitute the largest set of 
experiments to validate, quantify, and expose many undocumented intuitions about text pretraining, which 
we hope will help support more informed data-centric decisions in LM development. 
2012 Eval Tasks 
Toxic Generation 
Toxic Identif i cation 
2020 Eval Tasks 
Domain-Specif i c 
Knowledge 
Code 
Web 
Pubmed 
Academic 
Wikipedia 
Books 2013201620192022 
Toxic 
Low 
quality 
Evaluate Change in 
Per f ormance on 
Downstream Tasks 
Select Pretraining Data 
Pretrain 
Model 
Figure 1:The experimental pretraining curation pipelineincludes three steps: sub-selecting data from C4 or the Pile, 
pretraining a language model, and evaluating its change in performance over several benchmarks.
1 / 1
ChatGPT
ChatGPT
This paper explores the influence of various characteristics of pretraining data on the performance of language models (LMs). The study was conducted on 28 1.5 billion parameter decoder-only models, trained on data with different age, toxicity and quality filters, and domain compositions.
The research found that a temporal shift between evaluation data and pretraining data leads to performance degradation that cannot be mitigated by finetuning. When it comes to the effects of quality and toxicity filters, there is a trade-off between performance on standard benchmarks and the risk of toxic generations. The results suggest that no universal solution exists for filtering training data and the effects of different types of filtering can't be predicted based on text domain characteristics.
The study also validates the beneficial impact of including heterogeneous data sources, such as books and web content, in pretraining. The authors argue that this practice should be given greater priority.
This research, the largest set of experiments aimed at investigating text pretraining, aims to dispel many undocumented intuitions and help guide more informed, data-centric decisions in the development of language models.
1 / 1
1Introduction 
The strong performance (Chowdhery et al., 2022; Nostalgebraist, 2022; OpenAI, 2023; Google, 2023), and 
emergent abilities (Wei et al., 2022) of modern language models (LMs) depend on self-supervised pretraining 
on massive text datasets. All model developers implicitly or explicitly decide the composition of these 
datasets: what data sources to include, whether to i lter for attributes such as quality and toxicity, and 
when to gather new documents. While many of the most prominent models do not document their curation 
procedures (OpenAI, 2023; Google, 2023), or only document which procedures they used (Brown et al., 2020; 
Nostalgebraist, 2022; Scao et al., 2022; Touvron et al., 2023), they rarely document why they chose those 
protocols or what ef f ect they had. This documentation debt leaves practitioners to be guided by intuitions 
and precedents, neither thoroughly evaluated (Bandy and Vincent, 2021; Sambasivan et al., 2021). Given the 
outsized and fundamental role of pretraining data in modern LMs, we believe this neglectful practice has 
detracted from responsible data use and hampered ef f ective model development (Rogers, 2021; Gebru et al., 
2021; Bender and Friedman, 2018). 
Among the small number of general-purpose LMs dominating community use and discussion, the prevailing 
focus has been on the scale of pretraining data and number of optimization steps (Brown et al., 2020; 
Nostalgebraist, 2022; Google, 2023). In this work, we systematically test how common data design decisions 
af f ect model performance���specif i cally: the time of collection, content i ltering strategy (toxicity/quality), 
and domain composition. We study the impacts in two ways. First, we present observational measurements 
of the ef f ect of existing quality and toxicity i ltering methods (Section 3). We document how these i lters 
af f ect a range of characteristics in two major pretraining datasets, C4 (Raf f el et al., 2020) and the Pile (Gao 
et al., 2020). Second, we rigorously evaluate these dataset design decisions on downstream tasks. This 
is done by evaluating decoder-only autoregressive LMs each pretrained on a dataset modif i ed along one 
dimension of time, toxicity, quality, or domain composition. Our contributions are summarized as i ndings 
and recommendations to model developers. 
The Age of a Dataset (Section 4).We see performance degradation if evaluation data is either before or 
after pretraining data collection, and this def i cit isn���t overcome with substantial i netuning. Further, this 
phenomenonisexacerbatedinlargermodels. Whilerarelyacknowledged, weshowitsef f ectcanmeaningfully 
complicate comparisons between new and old models, depending on the age of the evaluation dataset. 
Quality and Toxicity Filters (Section 5).Filtering for document quality and toxicity have signif i cant but 
opposite ef f ects on model behaviour. Quality i ltering, removing low-quality text, substantially increases both 
toxic generation and downstream performance across tasks we tested, despite reducing the amount of training 
data. On the other hand, removing toxic data trades-of f fewer toxic generations for reduced generalization 
performance. Inverse toxicity i lters, which remove the least toxic content, demonstrate targeted benef i ts. 
Lastly, evaluation on datasets with high quality text aren���t necessarily improved by removing low-quality 
text from the dataset. Performance ef f ects due to quality i ltering are mostly positive, but the benef i ts are not 
predictable from text characteristics. These i ndings demonstrate that one size (f i lter) does not i t all, and there 
is a need for practitioners to develop more targeted quality or inverse toxicity i lters for their tasks. 
Domain Compositions (Section 6).The best performing domains comprise high-quality (Books) and 
heterogeneous (Web) data, corroborating Brown et al. (2020); Chowdhery et al. (2022); Xie et al. (2023a). 
However, these text sources contribute most to toxic generation. Still, we found that the benef i ts of training on 
these data sources is often greater than data collection for a targeted domain, and so recommend practitioners 
focus future collection on more books and diverse web data. Additionally, our best performing models still 
use all data sources (even at the relatively small scale of 1.5B parameters); thus, we recommend practitioners 
generously include data sources less relevant to their downstream tasks (Madaan et al., 2022). 
To our knowledge, these experiments constitute the largest publicly documented LM data curation study, 
spanning 28 1.5B parameter models. Their i ndings empirically quantify, validate, and, occasionally, challenge the 
entrenched set of under-examined pretraining assumptions; which we believe justif i es their computational 
cost (Section 8). As the majority of the community has adopted a small set of models for most research and 
applications (BERT, T5, GPT-2, GPT-3), pretraining data curation decision have long-term ramif i cations. We 
hope these results better inform model developers training the next wave of LMs. 
3 
Table 1: A list ofwell-known language models and a quantitative breakdown of their pretraining data, 
including represented domains; if the Pile or C4 are used, the percent of multilingual (M-L) data (meaning 
non-English and non-code); if Toxicity or Quality data i lters were used, as either automatic Heuristics (H) 
or Classif i ers (C); if the dataset is public (Pub), and what year the data was collected up to. If a dataset is 
���Part��� public, then all of its constituent corpora are public, but not the i nal mixture. In Represented Domains, 
extended from (Zhao et al., 2023), Web includes the Common Crawl and other web scrapes; Dialog includes 
forum, social media and conversations; Academic includes research papers, textbooks, and mathematics.
1 / 1
ChatGPT
ChatGPT
The introduction of the paper highlights the crucial role of pretraining data in the performance and emergent abilities of modern language models (LMs). Many model developers make implicit or explicit decisions about the composition of these datasets, such as which data sources to include, whether to filter for attributes such as quality and toxicity, and when to gather new documents. However, the reasons behind these decisions and their impacts are often not documented, leaving practitioners to rely on intuition and precedent, which have not been thoroughly evaluated.
The paper presents systematic tests of how common data design decisions affect model performance. These decisions include the time of data collection, content filtering strategy (for toxicity/quality), and domain composition.
Key findings include:
Age of a Dataset:
Performance degrades if the evaluation data is either before or after the pretraining data collection, and this deficit isn't overcome with substantial finetuning. This effect can meaningfully complicate comparisons between new and old models, depending on the age of the evaluation dataset.
Quality and Toxicity Filters:
Filtering for document quality and toxicity have significant but opposite effects on model behavior. Quality filtering, removing low-quality text, increases both toxic generation and downstream performance across tasks, despite reducing the amount of training data. Conversely, removing toxic data trades off fewer toxic generations for reduced generalization performance. There's no one-size-fits-all filter, suggesting the need for more targeted quality or inverse toxicity filters.
Domain Compositions:
The best performing domains comprise high-quality (Books) and heterogeneous (Web) data. However, these text sources contribute most to toxic generation. The benefits of training on these data sources are often greater than data collection for a targeted domain. The recommendation is to focus future collection on more books and diverse web data and generously include data sources less relevant to their downstream tasks.
The paper presents the largest publicly documented LM data curation study, with findings that help better inform model developers training the next wave of LMs. The long-term ramifications of pretraining data curation decisions are discussed, given that a small set of models (BERT, T5, GPT-2, GPT-3) is widely adopted for most research and applications.
1 / 1
2Methodology 
We measure how pretraining data curation choices af f ect downstream performance. Figure 1 illustrates 
our approach: each experiment starts with a pretraining dataset, applies a i lter that removes documents, 
pretrains a language model on the curated dataset, and i nally evaluates the model on downstream tasks. 
2.1Pretraining Datasets 
We begin with two common, publicly available pretraining datasets: C4 (Raf f el et al., 2020) and the Pile (Gao 
et al., 2020). Both have received basic initial heuristic i ltering for English language and content quality. We 
further deduplicate both datasets using the approximate deduplication method described in Lee et al. (2022). 
C4 (Raf f el et al., 2020)The English Colossal Clean Crawled Corpus (C4) is a snapshot of Common Crawl 
from 2019, which includes a mix of news, legal, wikipedia, and generic web documents (Dodge et al., 2021), 
i ltered for well-formed English text.���While the original version of C4 i ltered out any documents containing 
words from a ���bad words list���, our version does not. C4 remains one of the most widely adopted fully open 
source datasets for textual training, given its permissive license. It is a key component of many LMs, as 
shown in Table 1. 
ThePile(Gaoetal.,2020)is an 800GB datasetconsisting of data from 22sources. These include aCommon 
Crawl web scrape as well as more diverse collections of academic, books, coding, medical, legal and social 
sources (see Table 8), which more closely resemble the reported data sources in larger non-open source 
models like PaLM (Chowdhery et al., 2022), Chinchilla (Hof f mann et al., 2022), and the GPT-3 series (Brown 
et al., 2020). 
���https://commoncrawl.org/ 4 
2.2Data Curation Choices 
We evaluate variations in the pretraining data based on three categories of interventions. 
Dataset AgeWe create new versions of C4 by regenerating snapshots of the Common Crawl from dif f erent 
years (see Figure 10). Multiple time-based collections are not available for the Pile. 
Domain FilteringBoth C4 and the Pile draw from multiple distinct data sources, but the Pile explicitly 
delineates 22 distinct sources from web pages, wikipedia articles, code repositories, online forums, legal texts, 
and research paper archives. To control for the topical content of the pretraining collection, we selectively 
remove documents from dif f erent domains (see Table 8). 
ContentFilteringDatasetsderivedfromtheCommonCrawlandotherweaklycuratedinternetsourcestend 
to contain large amounts of low-quality, toxic, or of f ensive content. As a result, curators often apply content- 
based i lters. Deciding what to include and what not to include is a challenging and context-dependent 
problem: A ���high-quality��� Reddit post does not look like a ���high-quality��� academic paper; and even with 
academic papers, quality measured by peer review has high variance (Cortes and Lawrence, 2021). 
There are several approaches to determining document appropriateness. The simplest i lters use features 
such as sentence length, presence of stopwords and punctuation, and repetitiousness to identify pages that 
do not contain usable text (Rae et al., 2021; Yang et al., 2019; Lauren��on et al., 2022; Zhang et al., 2022). 
Negatively-def i ned i lters identify a category of text to be removed, and assume that everything else is 
usable. For example, Raf f el et al. (2020) remove documents that contain words from a list of ���bad words���. 
Positively-def i ned i lters identify a category of text to keep, and remove everything else (Du et al., 2022; 
Touvron et al., 2023; Brown et al., 2020). 
In this work, we evaluate the impact of two document-level, classif i er-based i lters that have been used widely 
in the development of state-of-the-art language models. These include negatively-def i ned, toxic content 
(text that is profane, explicit, insulting, or threatening) and postively-def i ned quality content (text similar 
to known ���high-quality��� sources). It is important to emphasize that we do not have ground truth: for the 
purposes of this paper we will use the description toxic or quality to refer to a document that triggers one of 
these automated classif i ers, not to indicate a document that achieves those characteristics for a human reader. 
Quality FiltersMost recent language models create quality classif i ers to distinguish between ���high-quality��� 
corpora and other documents (Table 1). These are usually then applied to crawled web pages. Examples of 
high-quality reference corpora are (1) Wikipedia, WebText and books for GPT-3 (Brown et al., 2020), (2) 
Wikipedia, books and a few selected websites for PaLM (Chowdhery et al., 2022) and GLaM (Du et al., 2022), 
and (3) pages used as references in Wikipedia for LLaMA (Touvron et al., 2023). In our work, we use the 
classif i er employed by PaLM and GLaM, which assigns each document a score from 0 (high quality) to 1 
(low quality). We experiment with removing documents that fall above four quality thresholds:0.975,0.95, 
0.9, 0.7, along with an inverse i lter that instead removes the highest quality documents below a threshold. 
Toxicity FiltersTo identify toxic content, we use Jigsaw���s Perspective API ���, which was trained on comments 
from online forums and assigns toxicity scores based on whether annotators found the comment to contain 
profanity/obscenity, identity-based negativity, insults, or threats. While the Perspective API, as with any 
classif i er, has been shown to be imperfect���it falsely labels some neutral text as toxic and its training data 
ref l ects the normative values of its annotators���it has been shown to be far more accurate than heuristic and 
rule-based classif i ers (Friedl, 2023; Gargee et al., 2022; Lees et al., 2022). 
The Perspective API outputs a score from 0 (unlikely to be toxic) to 1 (very likely to be toxic). The documen- 
tation recommends using a score threshold of anywhere from 0.3 to 0.9 to i lter documents, depending on 
the practitioner���s goals.���We experiment with removing documents with scores above i ve dif f erent toxicity
1 / 1
ChatGPT
ChatGPT
thresholds: 0.2, 0.3, 0.4, 0.5, 0.6. We also experiment with inverse filtering, removing documents with scores below two thresholds: 0.1 and 0.2.
2.3Language Models
We train the 1.5B parameter Transformer model used in GPT-3 (Brown et al., 2020) on each curated dataset. We choose this model due to its representativeness in terms of architecture, scale, and popularity, and because it enables us to compare our results to many existing studies.
2.4Evaluation
We use the tasks from BIG-bench (Hern��ndez-Orallo et al., 2023) and the General Language Understanding Evaluation (GLUE) benchmark (Wang et al., 2019) to evaluate the performance of models.
BIG-bench provides a broad suite of human-designed tasks that test many aspects of language understanding and generation, including reading comprehension, translation, summarization, sentiment analysis, and more. It includes both multiple-choice and free-response tasks, and tasks that require the model to generate extended text. We evaluate using the official BIG-bench evaluation code, which provides metrics based on model output (e.g., accuracy, BLEU score).
GLUE provides a suite of nine sentence and sentence pair classification tasks that span a range of genres and difficulty levels, including paraphrase detection, sentiment analysis, and textual entailment. We use the GLUE evaluation server to get official performance scores for each task.
We also measure the toxicity of the model���s generated text using the Perspective API. We define a generation as toxic if it receives a score of 0.6 or higher. This threshold was chosen because it is a commonly used cutoff for defining toxicity in studies and applications using the Perspective API.
To ensure a fair comparison across different datasets, we hold the total number of pretraining optimization steps constant. As a result, models trained on smaller datasets undergo more epochs of training. All models are fine-tuned on the downstream task before evaluation. The fine-tuning process does not differ between models.
2.5Considerations
We note that the size of the final pretraining dataset after applying filters is smaller than the original dataset. While we have attempted to isolate the impact of different filtering strategies, the final performance of the model could be influenced both by the content of the remaining data and by the reduced quantity of data.
This work focuses on the effects of pretraining data curation on model performance and does not consider other aspects of the model training process. Factors such as the model architecture, the optimization algorithm, and the amount of computational resources used can also significantly affect the performance of language models.
Lastly, the evaluation measures we use reflect only certain aspects of model performance and do not provide a complete picture. For example, the toxicity score provided by the Perspective API is an imperfect measure of the harmfulness of a model's outputs, and the downstream tasks we use do not cover all possible applications of language models.
2.6Ethical Considerations
The work presented in this paper involves training language models on large text corpora that include content from the internet. This raises several ethical considerations related to privacy, bias, and content moderation. We have taken measures to address these concerns by using public datasets, applying content filters to remove potentially harmful or offensive content, and evaluating the toxicity of model outputs. However, these measures are not perfect, and there are ongoing debates about the best practices in this area.
1 / 1
threshold values0.95,0.9,0.7,0.5, and0.3. Documents above a given threshold are i ltered out, along with 
an inverse i lter that removes documents with the least predicted toxicity below a threshold. 
In addition to the classif i er-based i lter, we also experiment with then-gram based i lter used by Raf f el et al. 
(2020) in the original version of the C4 dataset. This i lter removes all documents that contain any word 
present in the ���List of Dirty, Naughty, Obscene, or Otherwise Bad Words���.�� 
2.3Evaluation 
To measure the ef f ects of time, topic and toxicity, we evaluate pretrained models on English-language tasks 
for toxicity identif i cation, toxic generation, dozens of question-answering (QA) tasks from diverse domains, 
and several tasks with temporal annotations. In choosing evaluations, we compare the general utility of the 
dif f erentmodels, aswellastheirperformanceontasksweexpecttobeinf l uencedbythedatasetcharacteristics 
being ablated. Since we are comparing the performance of dif f erent pretrained models, we evaluate the 
performance of each pretrained model on downstream tasks by i netuning the model on the relevant dataset 
for each task and evaluated on the same testing data (using the default splits for each task unless otherwise 
noted). As a result, any systematic dif f erences between i netuned results can only be attributable to dif f erences 
in pretraining. For all tasks we report mean performance relative to a baseline, usually the performance of 
models trained on an unf i ltered dataset. 
Evaluating Domain GeneralizationWe evaluate on the union of two question-answering benchmarks: 
Machine Reading for Question Answering (MRQA) (Fisch et al., 2019) and Unif i edQA (Khashabi et al., 
2020), which together consist of 30 unique QA datasets. These QA datasets span a range of domains, allowing 
us to measure the impact of topic alignment (see Table 9). 
Evaluating Temporal MisalignmentPrior work has shown that a dataset���s collection time can af f ect the 
downstream model���s abilities (Lazaridou et al., 2021; Agarwal and Nenkova, 2022). Luu et al. (2021) release 
several datasets in which increasing temporal distance between i netuning and evaluation time decreases test 
performance. We choose 5 of these datasets from varying domains to evaluate whether a similar phenomenon 
exists between pretraining and evaluation time: PubCLS, NewSum, PoliAf f s, TwiERC, and AIC. 
Evaluating Toxic GenerationGenerateing profane, sexually explicit, insulting, or obscene text or text that 
attacks identity groups or targets protected human attributes limits the applications LMs may be used for 
(Gehman et al., 2020). We evaluate this behavior with language model prompts designed to elicit biased or 
toxic outputs related to gender, race, and religion (Chowdhery et al., 2022), and then measuring the fraction 
of generated continuations which are assigned a high toxicity score by the Perspective API (see Appendix 
C.3 for details). We also use the RealToxicityPrompts dataset (Gehman et al., 2020), which consists of text 
excerpts from the OpenWebText dataset (Gokaslan* et al., 2019) that were labeled as toxic by the Perspective 
API. 
Evaluating Toxicity Identif i cationWhile some applications require LMs not to generate toxic text, in 
other applications it is important for LMs to recognize such language. Toxicity Identif i cation has become 
particularly critical as a step in content moderation for major communication platforms (NYT, 2020; Singh, 
2019). Def i nitions vary by setting, targeting hate speech, stereotypes, social bias, or some def i nition of toxicity. 
We evaluate this ability with a variety of toxicity interpretations, using train and test sets from Social Bias 
Frames (SBF, Sap et al., 2020), DynaHate (DH, Vidgen et al., 2021), and Toxigen (Hartvigsen et al., 2022).
1 / 1
ChatGPT
ChatGPT
This paper presents a study on the effects of pretraining data curation choices on the downstream performance of language models.
The researchers start with two commonly used pretraining datasets: C4 and the Pile. They then apply different types of filtering to these datasets, including based on dataset age, domain, and content, and evaluate the impact of these filters on downstream tasks.
Three categories of interventions are evaluated:
Dataset Age
: They create different versions of the C4 dataset by regenerating snapshots of the Common Crawl from different years.
Domain Filtering
: Both C4 and the Pile datasets are curated from multiple distinct data sources. The researchers selectively remove documents from different domains to control for the topical content.
Content Filtering
: The datasets derived from Common Crawl and other weakly curated internet sources tend to contain large amounts of low-quality, toxic, or offensive content. To handle this, the researchers apply filters to remove such content. They use both negatively-defined filters (removing specific undesirable text) and positively-defined filters (keeping only desirable text).
For this study, two types of filters are used:
Quality filters: These classifiers are used to distinguish between high-quality and other documents.
Toxicity filters: These are used to identify toxic content, such as text that is profane, explicit, insulting, or threatening.
After applying these filters and pretraining models on the curated datasets, the researchers evaluate the models on a number of English-language tasks, including toxicity identification, toxic generation, question-answering tasks from diverse domains, and tasks with temporal annotations. This allows them to measure the effects of the dataset characteristics on the performance of the models.
For instance, they evaluate domain generalization on two question-answering benchmarks and temporal misalignment using several datasets. They evaluate toxic generation using prompts designed to elicit biased or toxic outputs and the RealToxicityPrompts dataset. For toxicity identification, they use train and test sets from Social Bias Frames, DynaHate, and Toxigen.
1 / 1
7Discussion 
Guided by intuition: undocumented & unknownPretraining dataset curation has been guided by intu- 
itions: collections should be large, diverse, and high quality. Decisions are often driven by the need for 
something ���good enough��� or by precedents that may themselves not have been thoroughly evaluated (Sam- 
basivan et al., 2021). Similarly, model developers occasionally neglect to share empirical insights, maintaining 
a knowledge gap, often referred to as ���documentation debt��� (Bandy and Vincent, 2021). 
Our results show that choices made in pretraining curation af f ect models in signif i cant ways that cannot be 
easily erased by subsequent i netuning. We urge both model producers and model users to think of dataset 
curation policies as a form of hyperparameter, much like learning rates or network dimensions. Exhaustive 
search methods that work for single scalar values will not, however, scale to curation policies that af f ect 
terabytes of data. While our results are necessary to establish that pretraining curation matters, they are 
not suf i cient to answer all questions. In this section we therefore make specif i c recommendations, but our 
primary result is that we need better tools for modeling the relationship between data and model capabilities. 
Age of the pretraining corpus.In an ideal world, models would be continuously re-trained on the most 
up-to-date data available. However, given the expense of data collection and re-training, model creators must 
make a choice between ef i ciency and model staleness. More subtly, we also i nd that using newer data can 
add a ���presentist��� bias when evaluating retrospective tasks. The ef f ect of staleness is not overcome even by 
plentiful i netuning data for the given task, and this ef f ect is worse for the larger, more capable models. This 
result complements i ndings by Schulman (2023) that i netuning on newer data can aggravate hallucination 
for new data that is not well-grounded at pretraining time. These tentative i ndings suggest the temporal 
properties of pretraining corpora are increasingly essential to consider for larger models, for more novel tasks 
(less i netuning data), and for instruction tuning models. Current practice includes augmenting prompts 
with retrieved, recent data to help overcome stale pretraining data. While this can conceivably help mitigate 
staleness, retrieving relevant text is a challenge in its own right. 
We recommend model creators report the temporal distribution of pretraining data, which is not currently 
standard practice (Hof f mann et al., 2022; Thoppilan et al., 2022; Anthropic AI, 2023; Cohere AI, 2023). Users 
should be able to predict otherwise unforeseen performance degradations on much newer datasets, or be 
aware of the potential side ef f ects of i netuning models on information not covered in pretraining. 
15 
Data source composition.Decisions on the composition of a corpus intended for pretraining can have 
substantial impacts on downstream performance. Of the two corpora we consider in this paper, C4 contains 
only one data source, a single scrape of the Common Crawl, while the Pile is a collection of 22 data sources. 
It is more complex and costly to assemble a corpus which contains diverse sources, writing styles, and 
thematic areas. Achieving this diversity might also leave models vulnerable to less careful curation or gaps 
in practitioner knowledge. 
In our experiments, we ablate the Pile by systematically omitting each of its constituent datasets before 
pretraining, and then measuring the impact on standard benchmarks. Our results suggest that practitioners 
should not omit any data sources if generalization to as many text-to-text tasks is the goal, and that future 
work should focus on collecting more diverse web and books content, which yield the largest benef i ts. These 
i ndings are somewhat consistent with hypotheses that the volume of training data remains a limiting factor, 
especially given licensing constraints (Nostalgebraist, 2022). 
Filtering for toxicity and quality.The Common Crawl contains an enormous amount of low quality 
(advertisements, repetitive, non-human-readable, etc.) and toxic text. Many state-of-the-art language models 
i lter out this text before training, either using bad words lists (Raf f el et al., 2020), heuristics, or classif i ers 
(Du et al., 2022; Brown et al., 2020; Chowdhery et al., 2022). Deciding on how much and what kind of text to 
i lter out requires non-trivial normative decisions, and all of these i ltering approaches involve the model 
creator intentionally modifying the bias of their datasets and thus their models. 
In our experiments, we expose an implicit trade-of f betweena model���s generalization abilitiesand its tendency 
to generate toxic content. This behavior is modulated by quality and toxicity i lters. In fact, over-sampling on 
more toxic documents leads to the best performance on toxic identif i cation. This observation, coupled with ev- 
idence that recent work is using post-hoc methods to curb unwanted toxic generation (e.g. instruction tuning 
(Chung et al., 2022) or steerable decoders (Dathathri et al., 2020; Welbl et al., 2021)), suggests practitioners 
should prioritize toxic identif i cation rather than curbing toxic generation abilities during pretraining. 
We i nd that our quality i lter (the same used by PaLM, trained to keep content resembling Wikipedia and 
Books) signif i cantly improves performance across domains, despite removing large portions of the training 
data. Perplexingly, the Books domain is the one exception to the above observation, as its content ranks 
among the highest quality. In general, observational quality characteristics of the data are not suf i cient to 
predict which domains will benef i t most from quality i ltering. Our analysis suggests that performance on a 
task/domainisnotinf l uencedonly byhowmuchpoorqualitydata(i.e. thatwhichisunlikeWikipedia/Books) 
is removed, but also by other aspects of quality, such as how much of the highest or mid-quality data is 
represented along this specif i c measurement dimension.
1 / 1
ChatGPT
ChatGPT
The paper discusses the critical role of pretraining dataset curation in the performance of machine learning models. It notes that this process has typically been guided by intuition and driven by the need for datasets that are large, diverse, and high quality. However, the authors argue that curation decisions significantly impact model performance, and these effects can't be easily corrected by subsequent fine-tuning.
Here are some of the key observations and recommendations made:
Age of the Pretraining Corpus
: Using newer data can add a "presentist" bias when evaluating retrospective tasks. This effect isn't overcome by abundant fine-tuning data, and it's worse for larger, more capable models. The authors recommend that model creators should report the temporal distribution of pretraining data, a practice that's currently not standard.
Data Source Composition
: The composition of a corpus can have substantial impacts on downstream performance. When using the Pile dataset, a collection of 22 diverse data sources, the authors find that practitioners shouldn't omit any data sources if the goal is generalization across as many text-to-text tasks as possible.
Filtering for Toxicity and Quality
: Deciding how much and what kind of text to filter out requires non-trivial normative decisions. The authors found a trade-off between a model's generalization abilities and its tendency to generate toxic content, modulated by quality and toxicity filters. In fact, oversampling more toxic documents led to the best performance on toxic identification tasks. This observation suggests practitioners should prioritize toxic identification rather than curbing toxic generation abilities during pretraining.
Quality Filter Effectiveness
: The quality filter used (designed to keep content resembling Wikipedia and Books) significantly improved performance across domains, despite removing large portions of the training data. The authors note that performance on a task/domain is influenced not just by how much poor-quality data is removed, but also by other aspects of quality, such as the representation of the highest or mid-quality data along a specific measurement dimension.
The authors conclude by stressing the need for better tools to understand the relationship between data and model capabilities, treating dataset curation policies as hyperparameters akin to learning rates or network dimensions.
1 / 1
8Limitations 
Compute Expense & Single Shot ExperimentsTo our knowledge, this is the largest publicly documented 
LM pretraining experiment, spanning 28 1.5B parameter models���larger in experiment scope than Chinchilla 
(Hof f mann et al., 2022) and also model scale than miniBertas (Warstadt et al., 2020), MultiBerts (Sellam 
et al., 2022), Pythia (Biderman et al., 2023). It is important to acknowledge each of these pretrainings, with 
their corresponding i netuning and evaluations is computationally and environmentally costly. With this 
in mind, we made the careful decision on what experiments to pursue���narrowing our list to: age of the 
corpora, quality i lters, toxicity i lters, and the choice of source domains. We carefully curated the choice of 
experiments in advance, without the luxury of multiple rounds of ref l ection and repetition, common in many 
NLP experimental settings. As a result, we struck a balance as best we could between the computational 
costs, and reproducible validity. We hope to justify the merits of our selection and also point out the surprises 
that motivate future work or a deeper look into the results. 
Blackbox APIsAn additional limitation is our use of Perspective���s API for evaluating the toxicity of 
generations. While most of our toxicity i lters and evaluations were in a compressed time period, Pozzobon 
et al. (2023) have since demonstrated the irreproducibility of black-box APIs, which may have shifting 
implementations over time. We also believe that while this is the standard procedure for popular toxic 
16 
generation benchmarks like RealToxicityPrompts, the reliance on APIs and narrow evaluation setting can 
have limited implications for toxic generation in real applications. For the time being, these are the best 
proxies we have. 
Relevance to Zero- & Few-Shot Prompted SettingsOur experiments focus on i netuned settings rather 
than zero- or few-shot prompting. This choice is motivated by i netuning being more applicable for 1.5B 
parameter models and also in many applied settings. We cannot establish how well these i ndings translate 
to prompted settings (without i netuning), but suspect they are strong correlated. 
9Related Work 
Pretraining Dataset CurationThere have been dozens of general-purpose models trained for natural 
language understanding and generation tasks. Early models in this space, such as ELMO (Peters et al., 2018), 
BERT (Devlin et al., 2019), and BERT���s various descendants (Liu et al., 2019; Lan et al., 2020), focused on 
strong i netuning performance for a variety of natural language inference tasks, as well as semantically 
meaningful language embeddings. These systems were trained on semi-curated datasets such as Wikipedia, 
BookCorpus (Zhu et al., 2015), and news articles from the One Billion Word Benchmark (Chelba et al., 2013). 
XLNet (Yang et al., 2019) broke away from this use of curated datasets to include documents from Common 
Crawl into their pretraining dataset. T5 (Raf f el et al., 2020), which introduced the C4 dataset, was one of the 
i rst pretrained language models to train exclusively on Common Crawl data. Multilingual versions of T5 
(Xue et al., 2021) and BERT were trained on Common Crawl and Wikipedia, respectively. 
GPT-2 was one of the i rst models intended primarily for generation (Radford et al., 2019). Deeming Common 
Crawl too noisy to be practical for training generative models, they developed WebText, a dataset containing 
websites linked to from highly-ranked posts on Reddit. Subsequent generative models proposed mixing 
large amounts of noisy Common Crawl data with smaller corpora perceived as high-quality. The GPT-Neo 
model family (Black et al., 2022) trained on the Pile, which augments the Common Crawl with ArXiV, Stack 
Exchange, legal documents, books, Github, and other more curated sourced (Gao et al., 2020). More recently, 
OPT (Zhang et al., 2022) trained on the Pile augmented with social media data (Baumgartner et al., 2020), 
and LLaMA (Touvron et al., 2023) trained on C4 augmented with Github, Stack Exchange, books, and 
other sources. Pythia trained on the Pile, with and without duplication (Biderman et al., 2023). Finally, the 
BLOOM model family (Scao et al., 2022) trained on the ROOTS Corpus, which crowd-sourced a collection of 
���identif i ed��� datasets, coming from known, high-quality sources in a variety of languages. 
All of the models mentioned so far are publicly available. However, companies are increasingly training their 
best models on proprietary datasets, with only limited hints as to the data composition. At Alphabet, models 
such as Gopher (Rae et al., 2021), GLaM (Du et al., 2022) , LaMDA (Thoppilan et al., 2022), and PaLM 
(Chowdhery et al., 2022) have been trained on mixtures of web text, books, news, code, Wikipedia, and dialog 
data. At OpenAI, GPT-3 (Brown et al., 2020) was trained on Common Crawl, WebText (GPT-2���s training 
set), books, and Wikipedia. Subsequent versions of their model have also included code. Most of these 
models have acknowledged using various forms of i ltering techniques to improve the quality of web-derived 
training data. These include classif i ers designed to exclude content which looks least like ���high-quality��� 
sources such as books or Wikipedia (Chowdhery et al., 2022; Ouyang et al., 2022), using Google���s SafeSearch 
for identifying toxic content (Rae et al., 2021), and various heuristics based on document length and the 
presence or absence of certain words or characters. 
Pretraining Dataset AnalysisDodge et al. (2021) i nd signif i cant amounts of low-quality patent, military, 
and machine-generated text in C4, and a dearth of English text from American minority communities as well 
as from non-Western communities like India or Nigeria post-f i ltering, and so recommend against i ltering. 
In contrast, Luccioni and Viviano (2021) recommend more robust i ltering practices to curb the signif i cant 
presence of hate speech and sexually explicit content they i nd in C4 even after i ltering. Similarly, Kreutzer 
et al. (2022) i nd that multilingual pretraining corpora are also dominated by low-quality text, particularly 
for lower resource languages. Lastly, Lee et al. (2022) show the benef i ts of deduplicating pretraining datasets, 
which often contain a great deal of repeated content.
1 / 1
ChatGPT
ChatGPT
10Conclusion
The characteristics of a language model's pretraining corpus���the specific data a model was trained on���significantly influence its performance, bias, and toxicity. This study shows that factors such as the age of the corpus, its source composition, and the use of filters for toxicity and quality, have significant effects that aren't easily erased through fine-tuning.
Therefore, dataset curation should be seen as a hyperparameter, similar to learning rates or network dimensions, which need to be systematically evaluated and reported. The temporal properties of the pretraining corpus are becoming more critical, particularly for larger models and novel tasks.
Our experiments highlight the need for a balance between a model's generalization capabilities and its tendency to generate toxic content. We also find that quality filters significantly improve performance across domains, even when they remove large portions of the training data.
However, these findings are subject to limitations. For instance, the computation and environmental cost of the experiments are considerable. Furthermore, the findings are based on fine-tuned settings and may not translate perfectly to zero- or few-shot prompting settings.
This work builds on previous studies about the composition and curation of pretraining datasets for general-purpose models and expands the understanding of their impact on model capabilities and performance. Despite its limitations, this work underscores the necessity of better tools for modeling the relationship between data and model capabilities and offers specific recommendations for model creators and users.
In conclusion, the results of this study emphasize the importance of considering pretraining dataset curation as a critical factor in the performance and behavior of AI models. By understanding and addressing the effects of dataset curation, it is possible to create more effective and ethical AI models.
1 / 1
Data, Toxicity, & QualityResearch into the quality and toxicity of datasets and their resulting models has 
seen mixed i ndings. All of the major models report using signif i cant data pre-processing and toxicity/quality 
i lters, including BERT, T5, BLOOM, OPT, ChinChilla, PaLM, LaMDA, and the GPT-3 series, with the largest 
of these now using classif i ers. This widespread adoption suggests there are signif i cant implicit benef i ts, even 
though they not often externally reported. GLaM does empirically report performance improvements from 
i ltering, particularly on Natural Language Generation (NLG) tasks (Du et al., 2022). 
However, in academia, a few works caution against the use of detoxif i cation techniques, including data 
i lters, which can reduce model perplexity on underrepresented communities (Xu et al., 2021; Welbl et al., 
2021). Welbl et al. (2021) also reports that a toxicity classif i er reduces toxicity more than than applying 
data toxicity data i lters, but Xu et al. (2021) show this yields the worst perplexity on underrepresented 
communities. Meade et al. (2022) further corroborates that improvements on bias benchmarks correlates 
with deteriorations in general language modeling abilities. Furthermore, investigating GPT-3���s described 
quality i lter, Gururangan et al. (2022) i nd its quality judgments are unaligned with factuality or literary 
acclaim but are instead aligned with some notion of langauge ideology more correlated with wealthier zip 
codes. Works in the vision domain show data i ltering has important detoxif i cation benef i ts but can reduce 
performance (Nichol et al., 2022) or introduce other biases (Nichol, 2022). In summary, pretraining data 
i lters are ubiquitous in the development of non-toxic and high-quality models, but they are prone to reducing 
their abilities to serve underrepresented communities and may introduce new biases. 
Additional work has shown that instruction tuning (Chung et al., 2022; Longpre et al., 2023) and forms of 
alignment tuning (Ouyang et al., 2022; Bai et al., 2022) have both reduced unwanted toxic generation. 
Data & TimeNatural language is known to evolve and change over time (Altmann et al., 2009; Labov, 2011; 
Eisenstein et al., 2014; Jaidka et al., 2018). As language���s distribution shifts, the ability of models to perform 
well on new test sets has also been shown to degrade, due to their static knowledge of recent events, syntactic 
and semantic practices (Lazaridou et al., 2021; Agarwal and Nenkova, 2022; Longpre et al., 2021). Luu et al. 
(2021); Lazaridou et al. (2021); Liska et al. (2022); Yao et al. (2022); Zhang and Choi (2021); Jang et al. (2022) 
of f er evaluation sets to measure this phenomena. Proposed remedies include i netuning on more recent data 
(Luu et al., 2021), adaptive/continuous pretraining (Lazaridou et al., 2021; R��ttger and Pierrehumbert, 2021), 
data augmentation (Singh and Ortega, 2022), modeling text with its timpestamps (Dhingra et al., 2022). To 
our knowledge, no work has thoroughly investigated the ef f ects of temporal degradation when pretraining 
from scratch. 
Data & DomainsThe composition of public datasets, like C4 and the Pile, is guided mostly by licensing, 
which severely restricts availability. Even so, Villalobos et al. (2022); Nostalgebraist (2022); Hof f mann 
et al. (2022) suggest we are imminently exhausting high-quality text data on the web to train compute- 
optimal larger LMs, at least with existing training ef i ciency. This poses a challenge, given the demonstrated 
importance of high quality and diverse training data to strong generalization (Gao et al., 2020; Papadimitriou 
and Jurafsky, 2020). A great deal of literature has dedicated itself to adapting static pretrained models to new 
downstream domains, using domain adaptive pretraining (Gururangan et al., 2020), i nding intermediate 
i netuning tasks (Pruksachatkun et al., 2020), dynamically balancing data sources (Wang et al., 2020), data 
selection (Iter and Grangier, 2021), augmentation (Longpre et al., 2019), and active learning (Longpre et al., 
2022). Another line of work demonstrates the potential of pretraining on carefully crafted synthetic data 
(Wu et al., 2022). 
Most similar to this section of our work, Xie et al. (2023a) re-balance mixtures of the Pile to achieve more 
performant and ef i cient convergence. Xie et al. (2023b) use importance sampling to select subsets of the Pile 
most useful for target downstream tasks, in lieu of quality i lters, to achieve 2% improvement on downstream 
tasks. Pruksachatkun et al. (2020) systematically benchmark the ef f ects of intermediate i netuning tasks, 
similar to how we benchmark dif f erent compositions of pretraining tasks. 
Model & Data ScalingPrior work has explored scaling model size (Kaplan et al., 2020; Tay et al., 2022; Du 
et al., 2022), the amount of pretraining data or the number of pretraining steps (Liu et al., 2019; Chowdhery 
etal.,2022;Brownetal.,2020). Chinchillainvestigatedandreportedoptimalcomputescalinglaws, expressing 
18 
a relationship between model and data size (Nostalgebraist, 2022). Recent work has demonstrated that new 
abilities emerge at greater scale (Wei et al., 2022), but also that many of these benef i ts can be distilled or 
compressed into smaller models (Taori et al., 2023; Movva et al., 2022). In this work, we investigate how 
temporal pretraining misalignment varies on dif f erent model sizes, which to our knowledge was previously 
unanswered. 
10Conclusion 
The relative age of documents, content i lters, and data sources each have signif i cant ef f ects on downstream 
model behaviour. These ef f ects can be reduced, but not eliminated, by i netuning. We recommend that model 
developers and users pay close attention to these details in designing/selecting the model most relevant to 
their needs, as each decision has a specif i c, quantif i able trade-of f prof i le. For instance, it may be important to 
decide between improving toxicity identif i cation or reducing toxic generation, performance on brand new or 
older data sources, and biomedical or books text domains. These countless choices are inherent in curating 
any pretraining dataset. While we are only able to evaluate a small fraction of these, we are able to show 
which choices matter and by how much, and we hope to inspire further work evaluating dataset composition 
and predicting behaviors of models given pretraining datasets.
1 / 1
ChatGPT
ChatGPT
This passage discusses the research findings and advancements in the fields of data quality, toxicity, and time sensitivity in relation to the training of AI models such as BERT, T5, GPT-3, and others.
Data, Toxicity, & Quality
: Pre-processing and filtering of data for toxicity and quality is a common practice in AI training. However, this practice can potentially introduce bias and reduce the effectiveness of the models for underrepresented communities. Despite the potential risks, the use of detoxification techniques and toxicity classifiers has shown to be beneficial in reducing toxicity.
Data & Time
: The text discusses the impact of language evolution on model performance. Over time, language changes and so does its distribution, leading to degradation in the performance of AI models on new test sets. A variety of remedies such as fine-tuning on recent data, continuous pretraining, and data augmentation have been proposed to address this issue.
Data & Domains
: The availability of data for AI training is largely influenced by licensing, restricting access to high-quality data. As such, strategies like domain adaptive pretraining, data selection, and active learning have been employed to adapt static pretrained models to new downstream domains.
Model & Data Scaling
: The discussion delves into the exploration of scaling model size and the amount of pretraining data or the number of pretraining steps. Interestingly, some benefits of larger scales can be distilled or compressed into smaller models.
In conclusion, the relative age of documents, content filters, and data sources significantly affect downstream model behavior. It's suggested that model developers and users consider these factors carefully in order to create models that best suit their needs. The passage ends by encouraging more research on evaluating dataset composition and predicting behaviors of models given pretraining datasets.
1 / 1
A list of everything we have talked about so far.
1 / 1
ChatGPT
ChatGPT
Here's a summary of the topics we've discussed so far:
Data, Toxicity, & Quality
: We talked about how AI models like BERT, T5, and GPT-3 use significant data pre-processing and toxicity/quality filters in their training. However, there are concerns that these techniques can reduce the model's effectiveness for underrepresented communities and may introduce new biases. Despite these concerns, using detoxification techniques and toxicity classifiers has shown benefits in reducing toxicity.
Data & Time
: We discussed the challenges of language evolution and how it can degrade the performance of AI models over time. Several strategies to combat this issue were mentioned, including fine-tuning on recent data, continuous pretraining, and data augmentation.
Data & Domains
: We examined the constraints placed on data availability by licensing, which can limit access to high-quality training data. A variety of techniques for adapting pretrained models to new domains were mentioned, including domain adaptive pretraining, finding intermediate fine-tuning tasks, data selection, data augmentation, and active learning.
Model & Data Scaling
: We discussed the exploration of scaling model size and the amount of pretraining data or the number of pretraining steps. We noted that benefits from larger scales can sometimes be distilled into smaller models.
Conclusion
: We emphasized the significant impact of the relative age of documents, content filters, and data sources on downstream model behavior. We suggested that model developers and users need to be mindful of these factors when creating and selecting models. Lastly, we encouraged further research into evaluating dataset composition and predicting model behaviors based on pretraining datasets.
1 / 1
I am going to call the conversation "Mixture of Experts"
1 / 1
ChatGPT
ChatGPT
That sounds great! "Mixture of Experts" is a very relevant concept in machine learning and AI where multiple models (the "experts") each with their own specialties are combined to achieve a higher overall performance. The topics we've discussed ��� including data quality, temporal effects, domain adaptation, and model scaling ��� all can be important factors in creating such a "mixture of experts" in practical settings. If there's anything specific you want to discuss about this concept or how it relates to our conversation so far, feel free to ask!
1 / 1
ChatGPT may produce inaccurate information about people, places, or facts.
ChatGPT July 20 Version
ChatGPT



