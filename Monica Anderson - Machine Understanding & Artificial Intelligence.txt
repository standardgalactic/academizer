G'day everybody, we've got Monica Anderson here and she's an expert in artificial intelligence
sustain working on AI for quite some time, especially on artificial general intelligence.
So without further ado, I'll put a link to the bio in the description.
So let's get right to it.
So Monica, welcome, great to have you here.
Thank you.
So look, let's ask firstly, how did you first get into artificial intelligence?
Well, it started in college.
I was getting a few classes of AI and I was in fact teaching assistant on
the AI group for the freshman on AI classes for the freshman.
And we ran a few lab sessions taking apart various things that counted as AI at the time.
And since then, I've been working in industrial grade AI, if you want to call it that, for many,
many years and until 2001, and when I kind of split my attention between doing AI for others
the way they usually did it and doing some research on my own into what I perceived was
going to be the next way of a way to do AI, which was deep neural networks.
And so it's a pretty unusual career path, if you will, switching that early because
most people only got into AI around 2012 when deep learning got to be very popular.
And I basically had like 12 years of head start on getting into the deep neural networks.
Now remember, I was doing my own kind of networks.
So they were very different from deep learning, not very different,
just to write them on different from deep learning.
And so that's what I've been doing.
That's basically my main focus on my research is my own kind of deep neural networks,
which are called organic learning. And we can talk a lot more about those.
Right. Yeah. So that sounds really interesting.
So historically, at first, I think people really wanted to solve the big problems in AI,
and then found it was too difficult, I guess, because of computing power and all that sort of
thing. So how do you see the history of AI and how it shaped AI development and
for better or for worse? Yeah.
Oh, sure. So AI officially started in 1955 with the Dartmouth Conference.
And there was a handful of people who started the conference and Martin Minsky and John McCarthy
were the biggest names among them. Solomon was there. But the one forgotten name or rarely
talked about name is Oliver Selfridge. And Oliver Selfridge was the only person on the
conference that basically insisted or wanted to work on understanding of things, how to
get from low level representation to high level representations. And in the end, he was overruled
by Minsky in the final paper where Minsky basically said that we'd like to work on understanding,
but we don't know how to do it. So we'll just focus on behaviors. And that's set the course for AI
for the next decades, where people basically copied behavior in computers by directly programming
the behavior into the computer. And that's not really AI because all of the understanding of
what's going on is in the programmer's head. And all we're doing is we're basically making the
computer behave the way that the programmer wants it to behave. And Selfridge went on to
publish a number of papers about things like what he called the pandemonium system where he had
the he had the system of demons and the demons were shouting to each other. So the lowest level
demons were basically looking at the world, if you will. And they were shouting upward what they
saw in some descriptive terms. And the demons at the next level would listen to those. And they would
shout upwards, what they heard from below. And after multiple layers of demons, you basically
get the top level, you got the highest level understanding of what was going on at the bottom.
And this is like extremely much like deep neural networks work, like including deep learning. So
this was this was kind of proposed 1958-1959 after the conference in Selfridge's papers. And of course,
he was ignored because everybody else was doing this. That's programs on learning into this,
less programs on behaviors into the system. And that was basically what AI was all the way up to
2012. There's many, many different branches of AI at the point, if you will, at the all the way up
to that point. And they also have different sub problems of what they perceived as the main problem,
but nobody really looked at the most fundamental issues of AI. And there were many, many reasons
for that. In some sense, what Minsky did was right. I mean, it's the machines at the time,
they could not handle anything except simple models of the world which were programmed and
generated. That's the only thing we could feed in our machines. So that was the correct thing to do
in some sense. We couldn't do any better. But around 2006, the machines had gotten a lot better.
What happened was that a professor at the University of Toronto by the name of Jeff Hinton and his team
of students over many, many generations of students were basically working on neural networks from
all the way from something like 1984 or something like that. And Hinton had already written several
chapters in the book, which is known as the connectionist Bible, which is PDP. It's called PDP.
It's the book about neural net that defined neural networks and connectionism. It was published
in 1982, and it was written by Roman Hart and McClelland with Hinton was the major unnamed
author at the beginning. So all of that basically, what happened was that Hinton's work finally
started working, if you will, at 2006. And the reason for that was many fold. He had four algorithm
improvements to whatever counted as neural networks back in the 80s and 90s. And he had
large amounts of training data with the internet. And he also had very large computers. And he had
started to use matrix algebra and to do the computation of the neural network. And that was
very efficient for the kind of problems that he was working on. He was working on recognizing
things like handwritten digits on checkbooks and other things. And that worked out. Like I said,
that started working in 2006. And they continued working on those on those strategies and those
algorithms. And by 2012, they won several competitions. And now everybody understood that
this was the way forward for this very critical understanding part of the basically how you go
from sensory input such as pixels to an understanding that this is the digit four or
the understanding that's a cat and that's my favorite chair. So all of the how do you go from
low level to high level? Suddenly that problem had a solution in the vision space. And that was
the first and it was not just tiny improvement over what had gone before. They basically broke
all the records with tens of percent of margin, which was unheard of. I mean, in singular processing
was one of six disciplines where the new methods really worked well. And they were looking at 20%
improvement overnight in the field where you had said 1% improvement in a decade. And similarly
for vision understanding and certain aspects of language understanding etc. So this was a very
big deal in 2012. And there was a stampede of researchers to switch to this new deep neural
network to deep learning strategies. But many of the old-school people who had worked with the
model building over the years, they didn't want to switch. They thought that it was antithetical
to everything they had been working on. And some resist to this day and the notion of using neural
networks as a path towards artificial intelligence, especially general intelligence. So that's a
split in the community that remains to this day. Most of the people who are working on deep learning
today, they are basically fresh out of school in the past decade, since 2012, I should say, six
years. And they are very good at what they're doing and they're really using these tools as well.
And so I think of current AI mostly being deep learning.
That's interesting. So we're going to be focusing on whether deep learning has
understanding. But first, before this interview for those who didn't read the title, it'll be
focusing on artificial understanding. So before we get too deeply into the mechanics of it,
let's talk about definitions then. And it's important to understand what you mean by understanding.
Yes. It may differ from some people's versions or definitions from understanding.
Correct. Yes. In my writings, I use understanding with an uppercase U at the beginning and reasoning
with uppercase R to basically say these are my definitions of the terms.
But the key insight goes back many, many decades, all the way to Darwin, if you will,
who discussed it early on. But many people over the years have basically discussed this
split in attention or this is split in how the brain works. And Daniel Kahneman's book,
Thinking Fast and Slow, is the most obvious or most explicit about these things. And he basically
splits the human mind into subconscious understanding, which is intuition based and
conscious reasoning, which is logic based. And so those are very different. And there are very many
things, if you split the process of intelligence, if you split intelligence into these two parts,
if you will, and they do interact and mingle in interesting ways. But for paragogical reasons,
if you look at how they're different, there are many, many things that follow the same split
into the reasoning and understanding part. For instance, reasoning is reductionist,
which means that it uses models of the world that you are manipulating mathematically like
formulas, etc. You use high level understanding of things, high level enough to be called models,
and you use those models to reason about things stepwise, one after the other. Whereas understanding
is this big just basically looking around the room, understanding what's going on or reading a
sentence or hearing a sentence and understanding what it means. This is the low level processing
that happens before we become conscious of what's going on. You don't know how and why you understand
this sentence. It's completely hidden from you, it's completely in your subconscious. Kahneman has
an interesting example in his book, which is basically he says that one of the things that
differs between these two modes is that understanding is basically reasoning can be stopped and
continued. You can plan a European vacation and it might take you several days to figure out all
the train stops and hotels to stay at, but you can basically sleep in between each of these segments
of reasoning about your vacation. Whereas understanding, once you, for instance, learn to
understand your native language, it cannot be turned off. So you see there's a very big difference
in how they work and I have lists of differences between the reasoning and understanding part
between the etc. that affect basically everything. Starting from including the fact that the whole
understanding part is unscientific, it is non-scientific, it does not use science, it doesn't require
complete information, it doesn't follow logic. All it is is basically a best effort guess as
what's going on and it does that guess really well based on a lifetime of experience. So understanding
is the ability to jump to conclusions on scant evidence and that's 99.99 plus percent of what
the brain does. Our reasoning is a painting layer on top of the understanding of the world that we
have and that's the only part we're conscious of. So that's a radically different view of
intelligence and what most people have. To most people intelligence stops at the reasoning level
because that's the only thing they can see using introspection and that's what I call the introspection
trap just because you can't see the subconscious which I said is 100,000 times larger than the
conscious part of the brain if you count activation cycles. Understanding is much,
much more important than reasoning for intelligence. A lot of people
see understanding as having a clear conscious knowledge of how a model works but you've got a
different sort of take on that. It's not really different, I mean you have an understanding of
a model work stake like f equals ma. In order to use that to for instance measure the horsepower,
estimate the horsepower of your car, you basically you weigh the car and then you floor it to see
how fast it accelerates and you measure that and you figure out the acceleration. Now you can guess
how many horsepower you have. The reason you can do that is that you understand what f, m,
and a are, what they mean, what it means in this specific case. It means the mass of your car. It
doesn't mean the serial number or anything like that, the bin number. It's basically you are
you yourself understand enough about physics and about the equation to be able to use it in real
life by applying the right pieces from the world into the equation and that understanding is something
that's much harder to express than just a simple f equals ma formulate. It's a lot of the information
that goes into that. Interesting. So you've heard of Mary's room experiment. It may not directly
apply at that time. Let's just say, do you know this one, the the knowledge argument in philosophy?
No, I don't. Okay. Well, just imagine you're a late lady called Mary who
spent her whole life in a grayscale room and become a like an expert
neuroscientist on the knowledge of like a how color works. So you know where is to know about
color. And then you walk outside the door suddenly opens and then you suddenly experience color.
Do you think she understands color before she walks outside?
I have no clue. That's that's a deeply philosophical question. And it's
it's one of those things. I mean, there are very many interesting questions that you can waste your
entire life on. And I have basically a little box or red herrings that I call them, which includes
things like this very puzzle. And things like the search chain in Chinese room and the touring test
and all kinds of garbage that I don't put quality quality is the first thing in the box that I
basically don't feel like dealing with. It's it's a distraction I can basically I have
by my studying epistemology, I have basically a very simple minded view of what understanding is,
how you can implement it in a computer. And I'm going ahead and doing that. And I don't
particularly need to understand what qualia is because I can get the system from
knowing nothing to understanding something useful and that something useful may be monetized,
etc. Why should I bother whether there's any qualia involved or not, etc, etc. So the the
I don't know if Mary, the black and white slash color person has any understanding of a color
beforehand. It is very close to the argument about reductionism versus holism. So it's not
irrelevant in this context. It's just that I don't like the puzzle as such. I'd like to
find more obvious things that we can discuss that aren't puzzles, such as learning to ride a bike,
right? Yes, that's a good idea. And at first, I don't understand it. I know that I'm meant to
like circle my feet around the pedals and make sure like that I'm like moving my arms to see
the front wheel in the direction I want to go. But it's not until I, I guess, learn the muscle
memory and internalize the process, that is, I understand it, like once I've internalized,
and then become more like a bodily function, that we say, well, it's too much to say bodily
function, even I mean, it's everything we learn is like that. Some of the stuff we learn, we learn
from listening to others, some of the stuff we learn, but physical skills are more obviously
subconscious, if you will. And my example of that is, for instance, there is, it's known that if you,
what you see, before it gets understood by the brain and becomes part of your conscious, there
is a delay of half a second, 500 milliseconds roughly. That's called a libete delay, which means
you are actually living half a second in the past. Okay. Now, if you're living half a second in the
past, you couldn't play tennis. And if you're a beginner at tennis, you can't play tennis because
you go down to the tennis court and you see the ball coming against you, you're trying to compute
where it's going to be. So you can hit it and you're going to miss because it was there half a
second earlier. And you have to basically play enough tennis and constantly correct small mistakes
that you make so that your subconscious, I mean, if this is your subconscious and there's a layer
of consciousness on top here, if this is your subconscious initially, you start at the bottom
and you go all the way to the top and down again, and that's too slow. But as time goes on, you
learn more about tennis, you basically make little loops here that says, okay, I can see a ball on
my right, and it's there. And I tell my arm to hit it right there. And your conscious self is not
involved. I mean, you have to play tennis subconsciously in their tennis, I think it was
called 20 years ago, 30 years ago, you have to play tennis subconscious, you have to ski and
ride a bike, etc. These things are basically almost completely handled by your subconscious
through these shorter paths that don't take half a second to traverse. And those only appear after
a lot of practice. So, yeah. So let's talk about like the difference then between reduction and
holism, which is something you write a lot about in your essays. Is that somewhat a logical next
step at this point? We can do that, yes. Reductionism and holism are basically two terms that go back
at least to the 1920s. They come from epistemology. But already the Greeks were debating this thing,
they just didn't use those words. So I have, if you will, I looked at the definitions for
reductionism. And there's a number of them in Stanford encyclopedia of philosophy. And if you
look at all of those definitions, they all have one thing in common, which is that they deal with
models. And so I boldly went ahead and said, whenever I say there were reductionism, I mean
the use of models. And models are things like scientific models, naive models that we make
ourselves, but don't tell anybody or publish, et cetera. They are mathematical hypothesis,
they're hypothesis, they're formulas, equations, and computer programs. They're basically things that
we use to describe reality in simple terms. And in describing reality in simple terms,
we have basically discarded a lot of complexity in the process that's also called reduction. I mean
reductionism is the use of models and reduction is the process that allows you to use models.
You start with an unreduced reality, which is everything you know, everything you see,
everything you hear. And from that, you basically extract a little equation that you're going to
either invent it or you're going to use it, if equals that may as an example of a model of that
kind. So reductionism, we use some models. And once you have said that, you can switch around and
say, what is holism? Holism is the avoidance of models. So if you look at the whole, both of them
have basically problem solving strategies. Reduction is problem solving strategies is to take the
program, to take the problem apart into pieces and you solve, take each piece apart into smaller
pieces until you have pieces that are so small that you can solve them. And if you can solve all
the small pieces, you can solve the big problem. That's the theory of reductionism. And this works
a lot of the time, but it requires an understanding. It requires somebody who understands the big
picture and can do this thing. You have to have a brain, basically. You have to have an intelligence
of some kind that does the reduction from the, from the big rich world to the simple model
that you can speak about. Humans do reduction all the time. We, we see something, it basically hits
our eye as, and we can talk about pixels in the eye, if we're not particularly, we don't want to
talk about rods and cones and stuff. The pixels are then processed in the, in the vision chain,
and the vision chain then at the end says that we have, this is for instance, a cat, or this is
my favorite chair in the living room, or this is a car, or whatever you have. And the reduction from
pixels to the cat is basically the key problem in intelligence. And, and most people have ignored
that problem. Like I said, AI up till 2012 was mostly ignoring that problem. So the holistic
problem solving view is that you don't go from the problem to a model space, which is mathematically
pure, where you can do computations on, on, on the model and solve the model, and then you can
apply the results back to reality. That's not how we do it. If you do the holistic problem
solving and the holistic problem solving, we solve the problem in the domain where it appears
without ever going to models. Let me give you an example of that. If I'm giving a talk and I look
at the wall and it says maximum occupancy at 200 people. And I look at the audience and I say,
okay, there's every, there's no empty chairs in the room. So we have 200 people here, roughly.
How many of them are women? Well, in the US, roughly half the population is women. So we can
say there's 100 people in 100 women in the room. That's using reductions. That's using models for
everything. I was one model that says how many people can be in this room. And we look at the
seats that we say they all occupy them. We say there's 50% women. Those are all models.
And so the answer 100 women is, is an approximation. It is how we would do it if you were
scientific about it. We would use models for everything. Yeah, we didn't have time to count
everybody here. What do you say? That we didn't have time to count everybody. Like counting millions
of people counting them, counting the women is the holistic solution. You don't use models,
you just count them. That's the holistic way of doing things. And most of the time,
the holistic solution tends to be more expensive than the model use. And that's why we use models.
That's why we use science and math because it gives us shortcuts to many, many problems.
So long as we can understand the problem and know what models to apply, it gives us a lot of
leverage and solving problems cheaply. But most problems can also be solved holistically. Now,
there's a little, the strangest thing about this is that the majority of problems that any human
being solves on any given day are holistic. We are not using science for anything. You can make
breakfast without reasoning or using science. You can drive your car without using reasoning or
using science. When you come up to a stoplight, you don't compute a differential equation to
determine how hard to push the brake. You just do it. You're solving it with the car as being part of
basically of your environment and you're affecting it and you're directly using feedback to do it.
You're not involving any models of car travel at all. And when you're making breakfast, you just
do whatever worked yesterday. The holistic answer to almost every problem is, if it worked last time,
do it again. So you've spoken about abstraction and that's something that scientists use a lot.
And in pure model space, it's what scientists would use to refer to all sorts of things.
So that relates to reduction, isn't it? Absolutely. Yes. In fact, reduction is a
synonym, if you will, to abstraction. And the reason I'm using a different word is because
abstraction comes with a prior meaning which is used in mathematics and by others in physics,
and abstraction in that sense is magic. It's just expecting to exist. I mean, some of you,
we abstract this to that. Okay, fine. Nobody asked how you did it. Whereas if you're doing AI,
you have to know how to do it. You have to actually have an answer for that magic part.
And so the answer to that is the abstraction is done using reduction. And I'll describe
reduction in some detail in a moment. But basically, we call it reduction just to remind ourselves
that the kind of abstractions we do, we can't just declare them to be magic. We have to actually
know what we're doing. And reduction is a recipe for how to do this abstraction, if you will.
And the recipe is a little bit surprising, which is that if you have, for instance,
all these pixels and you're trying to figure out what is there a cat in the picture or whatever,
you can't, the key point is, can you figure out what matters? What's important in the picture?
What's noise? What's irrelevant? What is important? What do we have to pay attention to?
What is worth learning? And what can we ignore even? What's salient? Yeah, what is salient?
That is exactly the term we use. And so it's a tricky question. And the surprising answer
is that it is easier to figure out what is not salient. It's easier to figure out what's irrelevant.
And what you do is you throw away that which is irrelevant. And if you do that enough, you're left
to that which is salient. So that's a very handy trick. And that's exactly the trick that we use
in deep neural networks. When you get it, let's take the deep learning example, you get basically
a picture of a cat in the form of pixels or something like that. And you recognize things
that are basically, okay, so here is a big gray area and that looks irrelevant because I've seen
those before. And you basically, you throw away what you can. And then you say, okay, here's something
looks like a little gray line. And the next level up says it's a cat's whisker. And the next level
up says there's a cat's face and so on. You can discover these things by throwing away everything
else at each layer. And the trick about reduction in multiple layers is that you cannot throw,
you don't know what's relevant and what's irrelevant until you understand stuff at a certain
level. So you can understand some stuff at the pixel level, but not much. But if you
find sub patterns, you can understand the sub patterns higher up and so on and so on.
And so you at every layer, the soonest, the soonest, you discover that this part of the image is
irrelevant, or this integral middle middle representation is irrelevant, you throw that
away. And what's left is basically this trumpet, if you will, of stuff that starts wide and ends
up being saying it's a cat at the very top. And that's roughly how deep learning works. My AI
personal or primary at artificial-understanding.com discusses this in five chapters into quite some
details basically shows that all neural networks or deep neural networks have this ability to do
discarding of the relevant multiple steps and ending up with that, which matters.
That's interesting. So I mean, the reduction of similarity here is like, you know, the divided
and conquer approach in software development. And, you know, the falsification approach within
our philosophy of science. So I mean, it's a similar strategy there.
Similar, I don't know if it's similar. Yeah, there are similarities, but we are in this process,
we're basically using guessing a lot. I mean, we're not scientific about it. We are, we have
very arbitrary ways of deciding what matters and what doesn't. At least they look arbitrary to
somebody with a scientific background. And there are no guarantees. I mean, understanding is,
like I said, is jumping to conclusions on scant evidence. And it is not repeatable. It is not,
doesn't give you all the answer. It doesn't give you the best answer. You have no guarantees of
any of those things, doesn't even guarantee you can terminate. So we have, it's a very different
strategy. And this is one of the big problems in trying to get people to switch to neural
networks in general, or to switch to organic learning is that we have to give up, we have to
take our scientific education that we have and put it in the box and close the lid. Because none of
the things that we talked about, like you said, falsification, et cetera, hypothesis, those things.
Yeah, there kind of are hypotheses in the system, but they are made by the machine. They're not made
by us. So you have to basically, what you're doing when you're working on neural networks,
is that you're creating a framework that can learn. And this framework then learns and whatever it
learns, it's basically in the corpus and whatever, and it's decided by your algorithm what it keeps
and what it throws away. But it's, it's, there's nothing, there's no guarantees. One corollary
of that is that all AI's are fallible. All intelligences are fallible. There is no perfect
intelligence. That basically the dream of a super intelligence that never makes a mistake is the
reductionist pipe dream and I, it's never going to happen. Can I do a little demo here? Yeah,
yeah, I think I was thinking it would do it. Yeah, let's start it. Let me do a little bit of
screen share here and I'll tell you what's going on. Fantastic. Well, it's great to actually cover
that reductionism and holism too. Yeah, it's very, very important. You can't really go into AI
without knowing what's going on. So here I have, I hope you can see a green screen.
So I can see, I can see you like that still, but I can see the green. Hang on, hang on,
hang on, let me push one more button here. There we go. How about this? I do, I see the green screen.
All right, so here's just a regular Unix shell, then I have a program which happens to be called
UN2, which I like to run. And it's basically my artificial understanding, if you will, is my,
it's running the organic learning algorithm is trying to read a book. In fact, it's trying to
read multiple books, it reads a book in a few minutes. What does that stand for?
It's the learning characters, it's how many characters it has learned, how many characters
it read from the book so far. So it's read 16,000 characters right now from the books.
There'll be more things to show on that line in 500,000 characters because the 500,000 is the,
is the, we run tests, we run tests every half a megabyte of characters. So a megabyte is roughly
a book a little bit more than a book. So we read the three quarters of a book and we run a test
and we read another half a megabyte and we run another test. It's the same test. The system,
we can use the same test every time because the system doesn't remember the test. We turn off the
learning by setting one goal into false and then it cannot understand, that can it learn from the
test case. So we can use the same test over and over. So we read half a megabyte, we test it,
then we test it again every half a megabyte. We'll get back to that in a moment.
Yeah, we've got another few minutes before that. Yeah, it's going to take a few minutes before it
gets there. So let's stop the share and go back to the camera. So let's talk about fault tolerance
just briefly then. I think that's that's useful. In the terms of like, you know, we have models
that don't gratefully degrade at the edges of their ability and hold more of a holistic approach
might be able to make educated guesses or sort of guesstimates the edge there, which might,
I'm not sure if educated guesses is the right word, but learned guesses, which might be more
accurate. Those are, those are actually, those are, those are good terms. What there is basically,
there's two kinds of mistake and computers rarely make a mistake. Okay. But when they make a mistake,
it is like you said that the edges of their competence of the model that there are basically
running and those mistakes that they make, they are expensive, spectacular, hard to immediately
correct and hard to remedy and they don't learn from them. But when a human makes a mistake,
like suppose you're learning to walk and you kind of got the hang of it, but it's gives thresholds
and stairs and other things gives you problems. And so you basically every time you make a little
mistake, this mistake is small and easily correctable and immediately remediable and it's not
spectacular. It's like, just like, looks like, looks like you stumble a little bit. And so,
and you learn from every mistake you make. And that's one of the key distinctions in my mind
about whether something is a general intelligence or not. It's a general intelligence if it can
learn from his mistakes. It can't, it's not general. Another important distinction is whether you
use models or not. If you're making models of the world, or if you take a model, make a model of
a small part of the world, you're limiting your computer to only working on that small
piece of the domain, a small piece of the world. Whereas if you start out with an empty machine
that doesn't know anything, then it can be general. I mean, the model is not general. The model is
whatever, whatever your program, that's all it can do. But if you start with the machine as empty
and it learns everything as it goes, then it can potentially learn anything. And all general
intelligence are learning machines. You mean like a blank slate? I start with a blank slate. My
machines are empty when they start. You saw here, we started with zero characters. It knew nothing
about English or Chinese or Japanese when it started, but if you give it English books, it
will learn English. If you give Japanese books, it will learn Japanese. So it's a completely blank
slate when it starts and it learns whatever you give it. So it's an artificial general understanding.
What do you say? Would you say that's a requirement for a general intelligence?
It is absolutely yes, a requirement for general intelligence that it is a blank slate.
Well, humans aren't blank slates. But therefore, I imagine that you say that humans aren't
general intelligence. Well, they are, no, the newborn child is pretty much a blank slate.
I mean, there's details. We can argue about the details. We can argue about physiology,
whatever, developmental stuff. Outside of the scope of this discussion, in theory,
the theoretical intelligences are all blank slates in the beginning and they have to learn
everything. Because if they don't have the capability to learn everything, they can never be
general. Okay. So if they do start with models, the models that they start with will hinder
generality in the future. Is that what you mean? Exactly. Yes. All models of the world are detrimental.
And it is really, I mean, people argue that you should start with models because they give you a
bootstrap so that you can get going faster. And they are deluded because if you're trying to measure
what intelligence your system is learning, the stuff that you put in at the beginning,
the reduction is model-based stuff, is going to overwhelm all the measurements you're trying to
do. So you can't actually tell if your learning algorithm is doing anything or if you're learning
or if all the results come from your models. And it's really hard to tease those apart. So you're
doing this service by starting from models. And also, it is very hard to build a system
that both learns and has models and can basically merge the information from both.
I have not seen a way to do that. Well, I guess people are trying. I think Kristen
Thornton is doing something like that. And Ben Gertzel is trying to achieve that with the
rev-open God. Yes, yes. So you think they're doing... I think they're doing the wrong things.
And in general, for the layperson, if you want to estimate the chance of
an AI project success, you count the number of boxes in their block diagram. And if they
have more than three, they're likely not going to make it. I mean, if your idea about intelligence
is to have an intelligent machine, an AI, an AGI, is to have 30 boxes which have labels like
language understanding unit and prediction unit and reasoning unit.
You don't know how to implement a single one of those, generally. You can do the high-level
reductions, break down what to perceive intelligence to be, but it is not the reality. A lot of
stuff has to be like interacting, interwoven into one single unit. And that's in the brain. It's
basically neurons and synapses signaling to each other. That's all there is, a single black box
with neurons and synapses in it. And if that doesn't do 99% of what you want, you probably
have a poor outcome, or then you haven't understood the problem.
Like interesting progress in neuroscience is discovering more and more about how the connector
works and how these neural patterns work. Would you predict that once we get finer grained abilities
to take 3D videos in real time of what the brain's going on, we'll be able to use those algorithms
to improve upon the date neural network? I'm willing to bet money that that will be too late.
We will already have AI by the time that's possible, mainly because basically I'm on a
faster track than neuroscience is. I'm starting with epistemology. I figure out how we can learn
at all what intelligence is, how reduction works, how can you implement reduction in machine. If
you have those as your primitives, you can do a lot of progress and you don't have to actually
look at neuroscience much. I look at it every now and then for purposes of type breaking, whatever,
or implementation hints, but basically epistemology gives you way more implementation hints than
neuroscience does at this point. And so I think I'm way ahead of a lot of other people who are
building neuron based systems that are closer to what they perceive the brain to be. An example
is spiking neurons, which the brain basically uses. I have figured out that the spiking neurons
is there to solve a very complicated problem that I'm not going to get into, but I have solved it
in a different way. And so I don't have to do spiking. I basically a single zero one impulse
signaling from neuron to neuron through synapse. And that works just fine.
And so the neuroscience will lead you astray. You can find, based on epistemology,
you can find shorter, faster answers. Sure. I mean, evolution got to where it was,
not by completely redesigning. My sound bite for that is that the story about what the
brain has to do is much shorter than the story about what the brain does.
Okay. I wonder if your test is complete. Let's go see. Let's go take a look at my experiment. Yes.
Share screen.
So what we see here is that it has actually, I can't see the screen. Oh, I'm sorry. There's
also there's always this button called share screen. I forget the press. Okay, here we are.
You have basically it says here that after the test you see, which is an unsupervised
Congress test has run for four, after 434 seconds, it had read half a million characters,
342 seconds were spent learning. The rest was to spend testing. And it ran 0.37 tests per second.
And it got 19 things wrong. The error rate is 19%. Now, what is the problem we're trying to solve
here? Let's take a look at that. So here are basically, this is how this is the test that
is running. I'm giving it two sentences, which are labeled like P1 and P2. You can see them
highlighted in blue here. And they are the same sentence, except in the blue highlighted part.
The first, it says the captain was sick and prudence forced me to consider possibilities.
And the other one, it says the captain was and prudence sick forced me to consider possibilities.
They're basically, we take in the first sentence, duplicated it, and then we have mangled it a
bit to make it look like ugly English. So I call this the ugly English test. And then the computer
basically reads both of those and it has to decide which one is uglier. And here it says that the
correct one is number one. Well, it just ran through a hundred of these sentences, roughly at this
complexity, and it got 81 of them correct after having read one book. So if I gave you a book in
Turkish that you've never seen before, some other language you've never seen before,
could you, after reading one book, start figuring out what's good and bad Turkish? I mean, I actually
have a slide for that. Let me see if I can find the slide. It would be interesting to try. Right.
But I wouldn't be knowing what I was reading. I mean, if I read a book in Turkish. Right.
There is, so I probably know a little bit. There is a bit of a, there is, there is
uncertainty about the, what should I call it, this uncertainty about the validity of the test
or how deep you can really, can you really call it understanding? So these are all good questions.
But I sold it on and basically say it's good enough for me. Take a look at this. Here's basically
one sentence and then we have moved the word around and you can easily tell which one of those
is correct. Right. Which one is the best English? That's easy for you to tell. Well, try again.
So this is Finnish and you have no clue which sentence is correct Finnish and which one is ugly
Finnish. So the only difference between the top two sentences and the bottom two sentences is that
in one case you know language and the other don't. So that's why we believe this is a good test for
language understanding. This is used in the industry. It's published in 2012 by Noah Smith.
So it's, other people are using it too. And there's, now there's a number of other tests
that I'm also looking at. I've done several of them. But for our everyday work, the ugly English
test you're seeing here is, is basically what we rely on from, from day to day to tell whether
we're doing better or worse. And I should possibly, we don't, we may not have time to look at all of
this, but the spoiler is basically that this thing will go down to 3% in a matter of hours.
50 minutes or so we will get to 5%. And, and, and we will see.
We can keep it running in the background. And yeah, we keep it running. It's going to,
if you wait another 20,000 characters, start running tests again, and it's just, it's not
very spectacular, but it's different. Yeah, you see the, the, the, it's going to better,
it'll have a better, yeah. So two, one, go. Here it starts switching to the test and it runs test
case zero, one, two, three, et cetera. And when it gets to a hundred, it prints another line and
it's going to say record 11 or something like that. So that's how that works. And, and basically
we have, we've been running these tests. I mean, this test is, I've been running this test for
maybe six years, seven years. And we've done many, many other tests. Amazon sentiment,
Amazon review sentiment is one we've done. We looked at the, at what's called the closed
problem with the Z. We're now looking at the glue set of problems, which are basically
the second generation, if you will. I mean, most of the tests that happened,
the most of the existing tests that are used in the industry are, are too simple for us. And we
don't, we can't really handle those because they are made for the old kind of NLP kind of systems
that, that got by with different surface understanding. And we, we want to basically
the whole paragraph to get really into the problem statement. And, and that was a question,
I wanted to bring up too, like if we can just do a quick interlude, there's something called
natural language understanding and image understanding. So what do they mean by understanding,
and how is it different from your? Well, let's look at image understanding briefly. I mean,
if I share a screen again.
So, let's look at this 95, 96, 97, 98, 99, 111%. And it reads 1200 characters per second,
1300 characters per second still, it doesn't actually go down that much. And that's a good sign
because when, if you read a lot of books, you don't read slower. And our system basically is constant
speed. It drops a little bit, but in theory, it's constant speed. And when you get after you read
100 books, 100 reverse is going to go at the same speed. Anyways, what I wanted to show was this.
So this is from already from 2012 or 13 as an image that you can give to a computer and comes
back with this bounding boxes, identifying things in the image and says woman in white dress,
standing with tennis racket, two people in green behind her, you really can't fault that for being
incorrect. It's quite amazing. And this is basically this was impossible until 2012.
This was only possible to do with deep running. We had no clue how to approach the problem before
then. And this is reduction from an image to internal concepts that can then be associated
with colors and things like tennis racket and woman and rest and all of these things.
It's all learned by the computer. And that's a sentence if you put all those words together.
Yeah, correct. If being a sentence may be more luck than design, but basically that's
roughly what it is. Oops, I don't have a mouse here. So is that version of understanding closer
to what you see as understanding? I would say that that version of understanding is basically
understanding for vision, for images. That's state of the art image understanding. And the
fact that you can go to an internal presentation and then to text, that is correct. That is in
my mind, that is a very good sign that there is some kind of understanding going on. We don't
know our own understanding how it works because it's subconscious. So who are we to say that we
are any better? It's a lot of jumping to conclusions. You see if a cat whisker, yes, there is a cat.
That's the kind of stuff we do all the time. And that's the kind of stuff we are basically
raising our computers to learn teaching our computers how to do.
Interestingly, the deep dreams that came out, what was going on there? What was going on behind
the scenes? The use of deep learning and what it had already understood is like being
salient in images before or being sort of woven into sort of an artistic rendition.
Yes, deep dream was very fascinating. And it's one of the bigger wonders of deep
dream that most people don't know this is that if you look carefully at the deep dream
generated image, there are no cut and paste lines in there. It didn't do it by cut and
paste. It did it by synthesizing every pixel in a context. So it's not just pasting dog
snouts and everything. It basically thinks that this is a good place to have a dog snout,
then it creates one from scratch. Yeah, I think of deep dream as basically a
looking glass into the vision understanding intelligence and looking at intermediate hypotheses
that it has discarded. So they are unlikely to survive all the way to the top. If you go,
there's a depth parameter in deep dream and you go to the top, I mean you, it says dog or
something like that. And the dogs snouts are there on everything in the middle.
That's not, I mean, I don't do use deep dream myself. I'm a bit on thin ice here,
but that's basically how I think of it. We have similar effects in the text area. We
basically trying to generate text and we're trying to generate language from deep structures.
So we have our system read a paragraph of text and then we look at the intermediate
structures in short term memory to figure out what did it understand from this paragraph.
What did it think was the most important terms at the top level? And then we say,
can you make me a sentence from what you know in the brain, but use half as many words and that
would be a summarizer, which would be a real product. So that's one thing we're working on.
And so we look at different levels in the system and we have different
kinds of text representation in the middle and it doesn't all make sense,
just like deep dream doesn't make sense. It's interesting from an artistic point of view to
look at, but yeah, it's not finding all the telly and aspects and representing them in a
sort of an accurate way. It's more like a cool image at the end. Yeah. What about
a natural language understanding? Do you think what's going on there is relevant to
the term understanding that you're using? I expect so here. Yeah, first I have to clear up the little
confusion that some people use natural language understanding as meaning voice understanding
and that is unfortunate, but that's not something I'm going to try to correct. I'm just going to
mention it. I use understanding basically as a differentiator to natural language processing.
NLP is using reductionist models of language such as grammars and word lists and synonym lists
and stemming and all of these garbage to try to understand language piecemeal by models.
They make models of everything, parts of speech, etc. And just knowing that something is the
subject of a sentence doesn't really help you understand it, does it? And natural language
understanding in my book is basically trying to go directly to something that is an understanding
very similar to the way we have in the brain, where everything is built out of patterns of text
and overlapping sentence fragments and overlapping intermediate concepts. And then at the top level
you have basically, we know the down to the one true concept for what this paragraph is about or
whatever. And so that's a different view, if you will, of how to treat language. And it is
radically different. It's as different from NLP as deep learning is from early vision experiments.
Okay, well back to the sort of the fault tolerance aspect. In the world, you've brought up in your
writing, I think the frame problem, where the world changes while we build models. And so if we
have a model that we build, given a certain context in a given time, that context will likely and
often does change, even language like scripts over time. Yes, the world changes behind your back.
That's a short formulation of the frame problem, which was written by John McCarthy and Pat Hayes
back in the 60s. And in some sense, the AI community observed this frame problem paper and they said,
hmm, this is really damning. We basically can't make scientific AI. What are we going to do about
it? Well, not a damn thing. We're going to ignore it. I'm going to use toy problems. So they didn't
learn their lesson back in the 60s, 70s, 80s, 90s. And now we're basically at the situation where I
can say the frame problem is damning all of the model based AIs as being fallible. It dams every
intelligence, no matter how you implement it as being fallible, because the world changes behind
your back and you're going to make a mistake. And this affects things like the danger of AI
becoming super intelligent and killing off humanity just for good measure. It's not going to do that.
There is no absolute perfect omniscient godlike infallible omniscient super intelligence. That's
just another reductionist pipe dream. It doesn't exist. It will never exist. It can't work.
We would have to, if you wanted to have something that understood everything scientifically,
you'd have to track every water molecule in the ocean. And this is basically people who say, well,
let's do that. I call that the computronium approach, where you want to turn the solar system
into computronium so you can simulate the ocean. Well, I want AI in my lifetime, so I'm working
with solutions that don't require a computronium. Okay. It would be nice to have, wouldn't it?
Yeah, now that would be to just lead us astray. I mean, we create systems that continuously
learn from their own mistakes in the world. Yes, they would make mistakes just like everybody
else, but by learning from them rather than having to make a new model from scratch,
from the outside or whatever, we can build a true intelligence as true as we are.
Interestingly, Ray Kurzweil doesn't believe that journal intelligence is a useful term. He thinks
that narrow AI, at least last time I checked, will broaden and then intersect with other narrow
IS. And I guess the trunks of these narrow trees will widen until we get a thick enough
box. Actually, you'll have a system that will be able to be channeled. So, working on the
generality problem or the understanding problem I mentioned too, to him is bollic. That may be
a bastardization of his view. Sorry, Kurzweil, if I'm misinterpreting you. But yeah, what would
you say to that? Well, Kurzweil was in AI heavily even before 2012 and that colors his
view of the world into being largely reductionist. I don't know if he has changed after 2012 or not.
He's at Google, so he at least gets getting influenced by those. But as I understand it,
he's not working on in the brain group, etc. Who knows? The point is that the terminology even
used about these problems, the term general and narrow AI, etc., they were okay back in the
olden days in some sense. Narrow AI was everything we did, which was the partial solution to the
AI problem. And general AI was this thing that nobody knew how to do that would basically encompass
everything. And this is the term coined by Peter Voss and Gertzel mostly. But it was okay back then,
but right now I would like to say that the split into narrow and AI, etc., narrow versus AI,
is not productive anymore. It's not going to help us. It's not going to provide implementation hints.
We don't know how to make a general AI. What I do know is basically there's a difference between
machines that understand and machines that reason. And we have to start with the understanding
machines. And you cannot reason about that, which we do not understand. And all of the old AI systems,
they basically were stuck behind this misunderstanding that they thought that they could
build everything with reasoning. And that's just not true. And so nowadays, I like to say, okay,
I don't like to talk about AI at all. One way or the other. My systems are called understanding
machines. They are not narrow AI, general AI, anything like that. They just understand they
don't, they are not AIs because they don't reason. I claim basically that my systems are not ever
going to be dangerous because they can't reason. They can't be full blown intelligence. All they're
good for is understanding text. And that's a very limited task. It's a very limited task that
worths a trillion dollars because all the industries want to use text understanding for something.
Everybody. Yeah. So solve a simpler problem basically.
Yeah, yeah. So where do you see, when do you see this being achieved? Having like a good enough
natural language understanding, but it will do everything that you can imagine a company wanting
to do with it. That is understanding the global Wikipedia and then be able to put the pieces
together even where humans have it. We are getting very close. And I'm not necessarily the only one
working on these things at this manner. Google just really just advertised another big step forward
in the BERT system, which beats most natural language pros, NLP, and some so-called blue
benchmarks quite handsomely. So people are making progress towards these understanding machines.
And I feel like I'm pretty well along with my stuff. And we are actually working on a product
that will provide understanding as a service, if you will, in the Amazon cloud or in the
Google cloud, where you send in a piece of text and you get back a set of numbers,
which you can treat as a Python set, if you will, if you're programming in Python. And you can then
do set operations on those numbers. And basically those numbers mean, those numbers tell you what
a sentence meant. The way you use this is that you send, suppose you wanted to like
detect harassment in postings on Facebook or elsewhere. What you could do is you could enter
a bunch of harassment statements into the system and you get back sets of numbers for each one of
those. And you store those numbers away. And then when you get this posting, you send this
posting to my system and it comes back with another set of numbers. And if there's a significant
overlap to the harassment numbers, it's harassment. And it doesn't matter how the people are phrased
using it is basically what we're striving to in our service is what we call semantic consistency
or semantic repeatability. We are getting you the same tokens back no matter how they phrase it.
And that would simply, you can directly use, if you're building a chatbot or any kind of
system that requires the use of language, you can directly put those tokens into your business
logic using a Python set theory or other things like that, set operations and basically sort out
whether it's category A or B or whatever you want to do with it. Right. So the number from one to 100
represents its likelihood of being harassment is that correct? No, no, no, sorry. It's going to be
there's going to be, I don't know, 60 numbers coming back and 14 of those are common for every
harassment case. Okay. And it's easy for you to determine which ones those 14 are if you give it
20 different statements. And those are the ones you put aside and those are the ones you detect
in everything that comes back. So you have to do a little bit of work afterwards. I mean, we could do
it for you. We know but we will rather just post sample programs to GitHub about how to do it
because it gives you more control in your end. We just provide a service where you feed text in
and you get concepts out. So this is available for people now to use? No, no, we're basically we
have a few months left to go on this. First of all, we need to get our numbers a little bit higher
than we have them. But I mean, 4% is not quite good enough. There's a second level of semantics
that we need to get to which we haven't gotten to yet. But we know exactly how to do it. At the
moment, the speed of development in my company is basically a matter of money. We don't have quite
the resources to go full tilt forward on this. We have to do it with one researcher instead
of two, etc, etc. So it's, we could, this could be a few months away if we just had the funding put
it that way. So I don't know how much you can actually talk about it. But what
are some fundamental ways that your system of approaching understanding is different from
other people to you feel maybe on the right track or on a close to right track? How would you describe
like, would you say Peter Voss is on a similar track to you?
No, Peter Voss is definitely not on a similar track. Peter Voss systems, as far as I have seen
them, are reductionists. For instance, they are not general. They, I asked him if his system could
be made to work in a different language. And he said, no, we had to reprogram everything,
which indicates that there are language models in there. My systems can learn Japanese,
French, Spanish, whatever, because they start from scratch. And so Peter systems don't. And so
they're therefore I conclude, but I don't know if it's entirely correct or not, but I conclude
that those systems are mostly reductionist, at least. I can talk about organic learning a little
bit. Would you like that? Yeah, yeah, yeah, sure. Organic learning. So organic learning is my name
for this thing you see running behind me. It's the fourth name I have for it used to be called
artificial intuition once upon a time. But the success of deep learning was so big, I couldn't
resist the temptation to rename it again. Basically, like I said, the deep learning got
popular in 2012 and everybody tens of thousands of researchers are working on it now. And what's
said in my opinion is that everybody seems that deep learning works. It's the first deep neural
network solution that they've ever seen. And nobody's looking for any others. They all think
that that's like the only thing that work. Well, there's LSTM, there's a number of variants of
deep learning, but they're all in the same family, they all use these arrays of tensors and arrays
of floating point numbers that they push through gener graphics processors. Nobody's looking for
anything else in the deep neural network space. Why? There could be much better systems out there.
We just happened to stumble upon deep learning as the first one. I mean, happened to stumble is
probably unfair to Hinton, but because he spent 34 years working to get to this point,
just like I spent 17 years to get to organic learning to working. But still,
why doesn't anybody else believe that there's alternatives to deep learning that could work?
So organic learning is such an alternative. And I started writing it in 2001, January 1 in fact.
And I've been working on it for 17 plus years now. It's 2018, so yeah. But on July 19th of 2017,
it actually started working to a quite reasonable degree. I had never got an under 24% error rate,
and here you saw this one started out at 19. So we're doing a lot better than we were. That's
because we understand more about these things. I say we because I didn't work on this alone. Over
these 17 years, I had seven other people help me out on and off over the time. And I paid some of
them salary and a lot of them have worked for free. And so on. But it's not just me, it's a teamwork.
Although I have written out of the 21 experimental systems, this is gadget number 21 as we call it.
Four or three of them were written by other people in their entirety. And I wrote all the others.
I wrote this one. And so organic learning is basically it's a deep neural network,
just like deep learning is. But it's much more natural. It's much more brain like if you will.
And if you want to write a neural network simulation in a computer, you're more likely to
start with neuron and neuron and connect them with the synapse individual neurons and individual
synapse. That's like the most obvious model you can think of. And it doesn't work. What do you do?
You try to find more efficiencies. And this is my speculation. What happened with Hinton is that
he basically figured out that if he could use deep, if he could use a matrix algebra, he could
solve a lot more connections faster. So he went that route and that started working. But in
essence, the discrete neuron model where you have discrete neurons, discrete Java objects,
in my case, or discrete any kind of object in some other language. That is a more natural or
brain like or a more obvious starting point. The reason we don't have them was that just like
Hinton stuff, just like neural networks in general in the 80s and 90s didn't work because
our machines were too small. And our corpora were too small. The same was true for these discrete
models. And so I basically picked up the charts of neural networks that we had from the 80s and 90s
of the discrete kind. And I started working on those in 2001. And by 2016, the machines have
caught up in size. And now it was a viable solution, just like Hinton stuff got viable in 2006.
So it's just a parallel, it's just a fruit of a different tree. Now let's talk about the details
here. So what I have is instead of arrays of floating point numbers, I have individual neurons.
And I start out with an empty machine. And when I read a letter like T, I create the neuron. It's
the first time I see the T, I create a neuron for the T, which is now the T input neuron.
And it will then try to signal to other neurons and there aren't any there. So it makes a new one
and tries and so on. And basically for every letter you read, if you have an input neuron,
you reuse it. And you reuse your path of signaling. And if you don't have an input neuron,
you make a new one. And so in general, that's the strategy. If you have a neuron in the right
place, you use it, otherwise you make one and put it there. So we call it a connectome algorithm,
an algorithm that basically makes decisions about when to make new neurons and where to connect
the synapses. And it starts out empty. And then it grows in something we can compare to organic
growth in the jungle. And that's why it's called organic learning. You mean organic growth in the
brain or organic growth in the jungle, like trees? Yes, organic growth in a jungle like trees.
Our system basically grows without any central control, without any rules. It has basically,
it's all emergent. The structure that we get from reading a hundred books is basically an
emergent effect of what's in the books. Now, emergence is a bit of a dirty word among
some crowds. What do you think are going wrong with it? I mean, like, what is it? Why do people
don't like, dislike the idea of emergence? Because it's, it's contrary to reductionism,
it can't be, basically, emergence is, if you have a system of emergent effects and you break
it down into pieces, the effect goes away. Take an emergent effect such as, such as, I don't know,
quality or radio reception. I mean, you have a radio, you take it apart into pieces, and you
try to explain where the song singing was coming from. It's not there in the pieces.
The quality, lifespan are things that are emergent effects of all the components in the car or all
the components in the human body, etc. And then they, it can't be determined by taking things
apart and making models. And so I have a video called called bizarre systems, where I basically
discuss this problem that the emergence is one of, of 10 factors that makes science impossible.
Not having in data is the most biggest one for, for all the AI problems. You don't never have
complete input data. But emergence is definitely a problem. Yeah. So, so how does your system
differ from hybrid deep learning systems? Is there something around the traditional deep
learning model? Either hybridized needs to, well, it's not hybrid in any real sense. I mean,
deep learning is, is a holistic system just like my system is it, it solves the problem in the problem
domain. It doesn't build, it doesn't use external models. It builds internal models. If I actually
make a difference between models, which I claim are context free and patterns, which I claim are
mixed with context or content. In the beginning, you at the bottom, you have content coming in
like pixels or characters. And then you form concepts in the middle that are expressed as
patterns. And they have some residual attachment to the input, such as it was in this specific case.
But then you go abstract more and more and more. And when you abstract it enough, you have a context
free version of whatever you have, like you have the prototypical chair, or you have the
prototypical car or whatever. That's your highest level context free model of it, of a car.
And it's different from my car or my chair, which is what we started with, because those
had residual context of being in my house and my garage. So, so to me, models are basically just
to stop most things. But people say that models are all over the brains of fine. So, but the systems
do is they're basically building their own models at some level. In my system, I say they're
building overlapping, multiple overlapping recursive patterns. And those patterns eventually thin out
to become just the highest level concepts that exist. And those can be close enough to models
if we look at them, if they don't have any residual context. Okay, this is more to what you mean by
partial reductions. Yes. Yeah, you go halfway. I mean, you play pen tennis with partial reductions,
because you don't have to go to the model of a tennis ball to hit it, you can just go to hit this
ball. Like, like if you're playing billiards, you might be really like intuitive at doing it.
But at some point in the game, you might be partially reductionistic, right? So there are
some people who play entirely by they sit down and like hit the thing. And it's always, I mean,
you should watch Ronio Sullivan play play snooker sometime. I mean, yeah. So some people they play,
they basically have played enough that all of the stuff that they need to play billiards well is in
their mind is all short circuits at low levels that do exactly the right thing. And some people,
they could basically they wish they had a, what should I call it, a ruler and the degree on the
table so they could measure the reflecting angle. And they are playing it reductionistically to some
extent. But it's basically it's pool is a simple enough problem that you can solve it both ways.
There's others like that, like if you want to shoot a cannon, you can shoot the cannon and
modern computers on cannons will basically shoot down an airplane out of the sky by basically by
by computing everything completely reductionistically. But if you're in a war and you're shooting with a
gun, or just shooting at the range of whatever you don't have computers to help you aim the thing,
you're shooting holistically. You have to basically practice until you shoot better. That's the only
way you don't measure angles at the shoot at the shooting range. I'm not sure if I'm correct, but
one of the first uses of the terms of computers were people who would calculate trajectories
of cannon. Exactly. Yes. Yes. Yes. They had a desire to increase the hit rate.
And the way they thought that they were going to do that was by computation and it works. It works,
but you can't it's you don't have that on a handgun. I mean, yeah, you have a sight at best. But yeah,
shooting from the hip and shooting from the hip or whatever. That's definitely holistic.
So where to from here in terms of like holistic development or understanding machines?
What are the what are some of the most all the problems that need to be solved? Are they all
been worked on? Do you think or is there some things that we don't quite know what to solve?
Is there still unknown unknowns or there's no unknowns that aren't things right?
I can in my opinion, I mean, I'm looking at why what I'm doing, I think that I have a handle on
understanding. The next phase for my company is language. It's language generation that you
basically can take the like I said, your deep generation from brain state to a sentence that
comes out. That would be very useful even for debugging. But that's basically that's what
everybody's waiting for. If we had that, we could make question answering systems and stuff like
that. And the third phase is called contiguous rolling topic mixed initiative dialogue,
where the sound computer as clarifying questions and sometimes takes the initiative in a conversation.
And that's much like the movie her or that's like serify point oh, which doesn't exist yet,
but which I would love to be able to build. And so that's those are like incremental steps,
if you will, towards something we can talk to which we can perceive as intelligent.
No touring test necessary. I mean, we know, we know this is serify point oh, we still know it's
intelligent. It's going to be obvious in the first three sentences. So that's, and I think that these
incremental steps are small enough that that a company less than 50 people could do it in five
years, all the way to the top. I mean, three or months away or less from publishing an understanding
machine. Interesting. So do you think you'd be able to translate what you're doing with natural
language understanding into like code understanding, like, or, you know, being able to program and
yes, these are all these are all things we wish we could do and we don't know yet how good they
will be at this. And we don't know how much of a computer you need for this. I mean, I'm estimating
that you can get the regular I mean, you can estimate you I'm estimating you can get most of
English enough of English to be useful in most situations in a machine with about a terabyte
of memory ram. Maybe maybe and Intel is basically making ships that are getting up there. So so we
might see understanding machines in your cell phones within the next I don't know four or five
years who knows depends on how popular the guess how quickly this develops and so on. By the way,
I can see on my screen here that we have a little well we've gone out to 99% error at the moment. So
yeah, we don't have to look at it. Nice. Yeah. Well, yeah, I guess. Like, I'm just wondering,
like, in what ways do you think the salient aspects in which you disagree with some of your
with with some of the peers that you most respect?
Well, the, yeah, the, I don't the,
all of the people, all of the people who basically built models of something with models of the
world, with models of language, built models of blocks, worlds or whatever. They were good
results in their day, but they are not really on the path to anything we can call a AI. And I
basically disagree with everything. I mean, nowadays, remember, I was doing AI before that I've been
doing expert systems and other useful things myself, but they were not really understanding
machines. They didn't understand anything. They were handmade programs that I made that then just
did what they did. And it looked impressive. So we called it AI. But yeah, I basically say that,
yes, AI has many interesting techniques and some of them are still quite useful,
like pathfinding in your in your car, for instance, is is done using an algorithm that was regarded
as AI 10 years ago. But the big difference is basically between me and everybody who insists
that intelligence has to be logical in some sense. And that's that's the biggest. I mean,
I say intelligence has to be pre scientific or unscientific. That's the only because we never,
for instance, you have to jump to conclusions of scant evidence because scant evidence is all you'll
ever have in real world. I mean, AI's have to relieve in our world, they can't live in the pure
mathematical space. That will be silly. They have to live in our world. They have to be the fact
that the world changes behind your back, they have to deal with uncertainty, they have to deal
conflicting inputs, etc. And in a graceful manner. And that's basically that throws science out the
window, you have to do pre scientific stuff. And it's called pre scientific for two reasons. One
is that people in the world were not scientific before 1650, we only invented science in 1650,
but they were still intelligent. Okay, think about that. And the second thing is that you have to do
the unscientific understanding before you can apply the scientific reasoning on top. So don't
reason about any something you don't understand, but the understanding is still holistic, still
basically best effort based on available evidence and we're happy. We will be shown to be wrong
many times. There's no way around it. No, have you sort of, what do you think of Steven Wolfram's
new kind of science? Oh, Wolfram is some similarities. No, no, none whatsoever. I don't
think I like the ideas. I'm impressed by his cellular automata and stuff. And he's a very,
very good programmer and a very good reductionist. And we should let him do what he's doing.
I don't think he's going to have any impact on long term understanding machines.
And that's true for most of the people who have done good jobs as reductionists, they should stay
where they are and keep doing it. I hope to see a generation of basically of, I don't know,
students all over the place graduate with ideas that science isn't the end all of everything.
And that we can actually get somewhere with on scientific methods, pre-scientific methods,
such as neural networks. Interesting that you don't classify that as science. A lot of people
would. And I'm just wondering there, if you're frightening away people, I think it's not going
I realized that it's controversial and I kind of like the idea of being controversial in
this manner, but it's the absolute truth. If you insist on doing everything scientific,
you are going to lose it. Science, for instance, I have on my artificial intuition page, the old
one, I have a whole chapter called the trade off. And the trade off is basically that if you want
to be scientific, then you have, you have optimality, you can find all the answers. Sorry,
you can find the best answer, your completeness, you can find all the answers, your repeatability,
you get the same answer every time. You're parsimony, you're not wasting your resources. You
have transparency process, which means you understand how the answer was arrived at.
And you have a screwability of the answer, which means you understand the answer after you have it.
This is true for basically for science and everybody loves these features. You can find the
best solution, you can find all of them. When you are switching away from scientific methods,
for instance, if you switch into something as simple as a genetic algorithm,
you have thrown all six of them away. You can't use them. You have none of those features anymore.
You have gone holistic. Genetic algorithms, genetic programming, deep learning and organic learning,
they're all basically part of a new wave of solving problems that are a holistic way of
solving problems in the problem domain. And now those six guarantees are gone. And instead, you
have very interesting features such as the ability to do reduction, the ability and abstraction,
the ability to self-repair if, sorry, let me take it in order. I can't remember the
reduction, abstraction, saliency, prediction, self-repair, all kinds of cool stuff. You can
read my blog about that, that you don't have in the scientific realm. If you do think scientifically,
you can't do reduction. Because reduction is a suit to be done before you go scientific.
So it's like, okay, so we've covered a lot of ground here. Is there anything that we haven't
covered that you think it was bringing up? We can take one last look at my experiment. How's that?
So what we can see here is after
running for 3,000 seconds, it has read three megabytes of text. It spent 2,383 seconds learning
at the kilobyte plus per second, and it reached 6% error rate in that time. If it went for another
megabyte or two, it would basically hit the 4% mark, which is highlighted in blue here. This is
zero, this is 4%, and 100% is two feet to the right. But that's basically, that's what we're
going from 19% to 6% in basically reading something like four books. The first thing it got,
on line three, it seemed to have got slightly worse. Oh, yes, yes. It found some, it discovered that it
had jumped to conclusions and found better conclusions to jump to. Okay, so it's all prepared.
It's a little bit of catastrophic forgetting. Basically, we see this all the time. I mean, if
I had had a straight line going here, I probably had faked the data, but yeah, this is what happens
every time you train. You have some kind of up and down. And at the moment, one of the troubles
we're having with the system, one of the problems we're having with the system right now is that
it will bounce. It will go down to 2%, 3%, and stay there for a while. And then we start over
learning and it goes back out again, and we end up with 5%, 6%. And so we want to basically have
a completely certain ways of preventing the over learning from that.
Okay, well, yeah, so it's been wonderful and I've learned a lot. Actually,
I've done a lot of reading before this, so it does make sense to me. Is there any concluding
remarks or if you want to, like, if there's a compressed version of something you'd like
people to take away? If not, if they were watching until the end here, then there's nothing.
Well, I guess my main remark would be that these things are happening. Some things are
happening faster than you think. And the things that you're going to see happening soon are going
to be very different from what you expected, especially if what you expect is based on pre-2012
ideas about AI. Those are obsolete. You have to basically start at 2012 and see what we can do
with the methods that we discovered after that, because those are going to be the foundation
that we're building all our AI and understanding machine work on hands after.
Well, a lot of people are considering that they'll see part of the human intelligence or some kind
of singularity, and it's not something you completely agree with. I mean, maybe part of the
human, but like, not a boom. There are hard limits on the frame problem puts a hard limit
on intelligence, because at some point all AI, all every intelligence will make mistakes because
of the frame problem, because the world changed and they had a bad model. So, absolute intelligence
is impossible. Absolutely perfect intelligence is impossible. And so, I'm estimating that, yes,
we will have very competent machines. They will know a lot of stuff. They will be able to operate
very quickly. They will be able to do complementary searches for what they don't know very quickly.
All of these are good things, but they are not really better at problem solving than we are.
And they may not, yeah, so there's all kinds of little limits on what, in what way they can be
better. And yes, they will be better than humans, but not radically so and definitely not infinitely
so. So, from those points of view, with that background, for instance, consider this that
for the next decade or so, the most common way, the most common understanding machine at least,
we'll live in the cloud. It will be a machine that we give a document like a web page and say,
evaluate this for quality. And it comes back and says the quality is such and such. And then we
kill it because it's done its job and we don't want it to learn anything from it. So, the average
lifespan for an AI in the next decade is going to be 50 milliseconds or something. And those are
going to be read-only intelligences. We basically, we make an intelligence, we teach it everything
we wanted to know at the factory, so to speak. And then we sell it to whoever, Google Amazon,
whatever wants to put it in the cloud for any purpose. And they make a million of those and
they pay us licenses for whatever. But those are going to be basically frozen machines. They don't
learn anything else. They serve their, they basically understand text and they're doing that job
and nothing else. And they are not dangerous. And they're not, they're not gonna, yeah,
they're not gonna get to be dangerous. We have, they're sort of narrow in a sense, right? No,
they're not narrow. They are fully general machines. They just, we just froze them. They're
frozen general intelligences. And you don't let them just, you know, go on thinking that surely
somebody will do that. So surely someone will try an experiment and see what that, what happened.
Oh yeah, we're gonna do it ourselves too. I mean, we want to move forward on this thing. But,
like I said, for the next decade or so, we're gonna see a lot of intelligences that are very
ephemeral, very short lived. They do a single task and they do it well. They are not AGI's,
they are understanding machines, but that's still worth a lot of money.
What about the decade after that? I won't even dare to predict what's gonna happen five years
soon. It's been, yeah, it's been wonderful speaking to you, Monica.
This was fun. Thank you very much. Good questions.
And you know, it's great. So keep up the great work. And we're going looking forward to seeing
what the world looks like when we have more, more understanding in our machines. I think it is a
rather overlooked problem. And people like chasing things like generality and, and just, you know,
some of the common problems in AI that aren't necessarily focusing on the understanding aspect.
Yeah, generality is a solved problem in my opinion. Read my blog.
Okay. Sure. Yeah, yeah. We'll put some links to this underneath the video here. So thanks very
everybody for tuning in and listening. And yes, it's been wonderful. And please remember to subscribe
because there'll be more of these types of videos in the future. Thank you. Bye.
