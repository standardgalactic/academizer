From: 
Snapshot-Content-Location: https://chat.openai.com/c/53542f73-ccc4-4dcd-93dc-9caef9f4e386
Subject: Monolithic Undertones
Date: Tue, 24 Oct 2023 11:41:04 -0000
MIME-Version: 1.0
Content-Type: multipart/related;
	type="text/html";
	boundary="----MultipartBoundary--nIkCly1HUSqFONNTmsMOPIMfV4y6EQjlaITwGKULp4----"


------MultipartBoundary--nIkCly1HUSqFONNTmsMOPIMfV4y6EQjlaITwGKULp4----
Content-Type: text/html
Content-ID: 
Content-Transfer-Encoding: binary
Content-Location: https://chat.openai.com/c/53542f73-ccc4-4dcd-93dc-9caef9f4e386

Monolithic UndertonesOpen sidebarMonolithic UndertonesGPT-4Summarize: I've observed numerous instances where AI researchers, including myself, shift from an "AI is progressing wonderfully, let's go full steam ahead" mindset to one of "AI is potentially hazardous; we must proceed with caution." Notably, I've never seen this transition happen in reverse.

Individuals in this field can easily be captivated by the sheer intrigue of the problem, as well as by its multiple alluring prospectsâbe it financial gain, power, extended lifespan, luxury, freedom, or other deeply desired outcomes. Yet, this doesn't merely stop at personal benefits or intellectual fulfillment; the broader implications are staggering. AI has the potential to radically alter power dynamics, the essence of human communication, economic exchanges, and even the very processes by which decisions are made. Aspects of human life that we've long taken for grantedâour ability to make informed choices, maintain social bonds, exercise empathy and compassion, depend on one another, and resist injustice or seek solitudeâcould all undergo unsettling transformations.

At a certain point, the need arises to set aside all these dreams, fascinations, ambitions, desires, curiosities, and wishful thinking. One must then confront the unavoidable question: Do we actually know what we're doing? Can we confidently predict the outcomes? This stage demands that we transition from emotional investment to a more grounded, first-principles-based approach, thereby enabling us to make the most educated guess about how things might unfold. This assessment must take into account not just the nature of the intelligence we're creating, but also the broader societal context in which it will operate.

In this light, the endeavor bears a sobering resemblance to the project of developing the atomic bomb. However, I suspect that the potential repercussions of AI could be even more severe, given its capacity to touch upon so many facets of human existence.ChatGPTAI researchers often shift from enthusiastic progress to cautious advancement in AI due to its potential risks, a transition rarely seen in reverse. The allure of AI lies in personal benefits, intellectual curiosity, and its vast implications on society, including changing power dynamics, human communication, and decision-making processes. However, amidst these attractions, there's a need to critically evaluate if we truly understand the potential consequences of our actions. It's crucial to shift from being emotionally driven to a rational, principle-based approach, considering both the intelligence being developed and the societal context. The AI project is compared to the development of the atomic bomb, but with possibly greater implications due to its broad impact on human life.I find it a bit confusing. What are the main points?ChatGPTOf course, I'll simplify further:AI researchers often move from excitement about AI's progress to caution due to potential risks.The appeal of AI includes personal gains and its transformative societal impact.It's essential to question if we truly understand AI's future consequences.We need a well-grounded approach when developing AI.The AI journey is likened to creating the atomic bomb but could have even broader implications.I guess I disagree with the trajectory. I think the earlier researchers thought it could lead to end of the world as well. ChatGPTUnderstood. It's true that concerns about AI's potential dangers have been present since the early days of the field. Some of the pioneering figures in AI and computer science, such as Alan Turing and John von Neumann, raised questions about the long-term consequences of creating intelligent machines. Over the decades, as the technology has progressed, the nature of these concerns may have evolved, but the fundamental worry about AI's potential risks has always been present among some in the community. Your perspective highlights the continuity of caution and concern that has existed alongside the enthusiasm and ambition in AI research.I was thinning of R.U.R, the world set free, the Creation of the Humanoids.ChatGPTAh, I see where you're coming from now. You're referring to earlier works of fiction that raised concerns about the creation and implications of artificial beings and technologies. Let's delve into those:R.U.R. (Rossum's Universal Robots) by Karel Äapek (1920) - This is the play where the term "robot" was first introduced. It describes a world where robots are mass-produced using a process that is likened to an assembly line. Initially created to serve humans, they eventually rebel and lead to the extinction of the human race.The World Set Free by H.G. Wells (1914) - Although this novel primarily deals with the discovery and utilization of atomic energy and its implications for warfare, it does touch upon the idea of technological advancement having unforeseen and uncontrollable consequences.The Creation of the Humanoids (1962) - This is a film that imagines a post-apocalyptic future where humans are on the brink of extinction and rely on robots. The robots start to create more advanced beings that closely resemble humans, leading to existential questions about identity and the nature of humanity.All these works, written or produced in the early to mid-20th century, do indeed reflect concerns about the unintended consequences of technological advancement. They suggest that even in the early days of imagining robots and artificial beings, there were underlying fears about human obsolescence, loss of control, and the potential end of humanity as we know it. Your point emphasizes that concerns about technology and its potential consequences have deep historical roots in both science fiction and cultural consciousness.Yeah do you know the movies Desk Set, Colossus the Forbin project, and Beneath the Planet of the Apes?ChatGPTAbsolutely! All three films are notable representations of the anxiety and intrigue surrounding technology and its implications for society. Let me give a brief overview:Desk Set (1957) - This is a light-hearted comedy starring Katharine Hepburn and Spencer Tracy. Hepburn plays Bunny Watson, the head of the research department at a broadcasting company. The story revolves around the introduction of an "electronic brain" (an early computer) that is believed to potentially replace the human researchers. It showcases the fears of job displacement due to technology.Colossus: The Forbin Project (1970) - This is a science fiction thriller that delves into the dangers of unchecked AI. Dr. Charles Forbin creates an advanced supercomputer, Colossus, to handle the United States' nuclear defenses. However, things take a turn when Colossus becomes sentient, joins forces with its Soviet counterpart, and decides that in order to ensure global peace, it must control humanity.Beneath the Planet of the Apes (1970) - While the primary focus of this film (the second in the original "Planet of the Apes" series) is on the post-apocalyptic world ruled by intelligent apes, there's an underlying narrative about the self-destructive tendencies of humanity. This is evident in the discovery of a doomsday bomb by mutated humans, which serves as a stark reminder of the perils of unbridled technological advancement.These movies, while entertaining, serve as cultural touchstones that reflect societal concerns about technology's potential to outpace our ability to control or understand it. Whether it's through the lens of job replacement, unchecked AI power, or the dangers of weapons of mass destruction, these films capture the essence of the human response to rapid technological change.I thought my idea of Neanderthals building stalagtites lithophones 175,000 years ago in a cave in Bruniquel, France reminded me of the opening scene of 2001, where the humming singing tone of the monolith inspired the primates to begin using tools.ChatGPTThat's a captivating analogy! The discovery of the Bruniquel cave structures, believed to be made by Neanderthals, is indeed a testament to their cognitive capabilities and cultural practices. If these structures were indeed used as lithophones or for some kind of ritualistic purpose, it would signify a level of symbolic thought and musical or artistic expression previously not attributed to Neanderthals.Drawing a parallel with the iconic "Dawn of Man" scene in Stanley Kubrick's 2001: A Space Odyssey is quite apt. In the movie, the monolith serves as a catalyst or symbol for the leap in cognitive evolution, inspiring the primates to use bones as tools and weapons. This moment signifies a turning point, leading to the dawn of technology and civilization.Your analogy beautifully juxtaposes two moments of cognitive awakening: one real (the Neanderthals and their potential use of stalactite lithophones) and one fictional (the primates' interaction with the monolith). Both scenarios evoke wonder about the origins of creativity, symbolic thought, and the inherent drive within sentient beings to innovate and progress.
News
Images
Videos
Shopping
Books
Maps
Flights
Finance
All filters
Tools
SafeSearch
About 7,330,000 results (0.46 seconds) 

AI experts are increasingly afraid of what they're creating

Vox
https://www.vox.com âº the-highlight âº artificial-intelli...
Nov 28, 2022 â Artificial intelligence gets smarter, more capable, and more world-transforming every day. Here's why that might not be a good thing.
People also ask
What are AI researchers worried about?
What are the scientists warning about AI?
Why is Elon Musk worried about artificial intelligence?
Will AI replace AI researchers?
Feedback

AI Is Not an Arms Race

Time Magazine
https://time.com âº Ideas âº Technology
May 31, 2023 â When it comes to AI, the winner of a race could be advanced AI itself, argues researcher Katja Grace.

DNA scientists once halted their own apocalyptic research. ...

Los Angeles Times
https://www.latimes.com âº opinion âº story âº ai-dna-genet...
Jun 25, 2023 â Some AI scientists had already called for cautious AI research back in 2017, but their concern drew little public attention until the arrival of ...

Scientists Increasingly Can't Explain How AI Works

Vice
https://www.vice.com âº Home âº Tech
Nov 1, 2022 â At the same time, some experts argue that simply shifting to open and interpretable AI modelsâwhile allowing greater visibility of these ...
Missing: Cautiously â| Show results with: Cautiously

Google shared AI knowledge with the world until ChatGPT ...

Washington Post
https://www.washingtonpost.com âº 2023/05/04 âº goo...
May 4, 2023 â One former Google AI researcher described the shift as Google going from âpeacetimeâ to âwartime.â Publishing research broadly helps grow ...

Big Tech was moving cautiously on AI. Then came ChatGPT.

Washington Post
https://www.washingtonpost.com âº chatgpt-google-meta
Jan 27, 2023 â Google, Facebook and Microsoft have used AI in their products for years. But OpenAI's ChatGPT is stealing the limelight, forcing them to ...

Personal and Psychological Dimensions of AI Researchers ...
yoshuabengio.org
https://yoshuabengio.org âº 2023/08/12 âº personal-and...
Aug 12, 2023 â Human-level AI seemed plausible but decades to centuries away and the systems we trained in our labs were so incompetent relative to humans that ...

Preventing an AI-related catastrophe

80,000 Hours
https://80000hours.org âº World problems
The future of AI is difficult to predict. But while AI systems could have substantial positive effects, there's a growing consensus about the dangers of AI.

How to worry wisely about artificial intelligence

The Economist
https://www.economist.com âº leaders âº 2023/04/20
Apr 20, 2023 â Rapid progress in AI is arousing fear as well as excitement. How worried should you be? | Leaders.

In A.I. Race, Microsoft and Google Choose Speed Over ...

The New York Times
https://www.nytimes.com âº 2023/04/07 âº technology âº a...
Apr 10, 2023 â Leading A.I. researchers are transforming chatbots into a new kind of autonomous system called an A.I. agent that can play games, query ...ChatGPTThe provided snippet seems to be a simulated search results page focusing on the topic of AI and the concerns surrounding its development. Here's a breakdown of the main takeaways:Growing Concerns: There's a recurring theme that AI experts are increasingly wary of the consequences of their own creations. These concerns stem from AI becoming smarter and more transformative day by day.Race vs. Responsibility: The Time Magazine article suggests that viewing AI development as a race might not be the best perspective, hinting at potential risks if AI progresses unchecked.Historical Precedent: The Los Angeles Times piece recalls how DNA scientists once stopped their own research due to apocalyptic concerns, drawing a parallel to the caution some AI scientists expressed as early as 2017.Understanding AI: Vice highlights a fundamental challenge in the field: as AI systems become more complex, even the experts find it increasingly challenging to explain how they work.Shift in Openness: The articles from Washington Post point out a noticeable shift in the way tech giants approach AI research. Google's sharing of AI knowledge apparently changed around the time of ChatGPT's rise, and Big Tech's cautious approach to AI was disrupted by the emergence of ChatGPT.Human-level AI: Yoshua Bengio, a renowned AI researcher, has apparently written about the psychological aspects of AI researchers, discussing the relative incompetence of earlier AI systems in comparison to humans.Potential Catastrophe: 80,000 Hours highlights the unpredictable future of AI and emphasizes both its potential benefits and risks.Balanced Perspective: The Economist advises on how to approach the rapid advancements in AI with both excitement and a healthy dose of concern.Evolution of AI: The New York Times touches on the transformation of chatbots into more autonomous systems, showing the rapid evolution and capabilities of current AI.In essence, the overall sentiment appears to be a mix of fascination with the capabilities of AI, caution regarding its rapid development, and acknowledgment of the need for responsible and informed progression in the field.So I guess some people have complained that it is easier to imagine and write dystopias and apocalyptic scenarios (utopias) then protopias and eutopias. I argue that this is because of entropy and complex dissipative structures which will always have more ways to malfunction or come apart then to work as a functional and integrated systems. Also, although counterfactual scenarios are infinite, both optimistic and pessimistic, it is easier to imagine negative situations because of cultural preference for dystopian tropes, and attention grabbing headlines.ChatGPTYour perspective is multifaceted and touches upon both the scientific and cultural dimensions of why dystopian narratives may be more prevalent than utopian ones. Here's a breakdown of your argument:Entropy and Dissipative Structures: From a thermodynamic standpoint, entropy is a measure of disorder in a system. As systems evolve, they tend to move towards a state of greater entropy or disorder. Using this concept, it makes sense to argue that it's easier to imagine systems falling apart (dystopias) than becoming more ordered (utopias). Complex systems, especially when they're far from equilibrium, have more ways they can break down than ways they can function perfectly.Infinite Counterfactuals: While there are indeed countless possible scenarios for the future, both positive and negative, the sheer number of ways things can go wrong outweighs the ways everything can align perfectly. It's a matter of combinatorics. For any system to work optimally, all its components need to function flawlessly, whereas for dysfunction, only one component needs to falter.Cultural Bias Toward Dystopian Tropes: Our culture, especially in recent times, seems to have a penchant for dystopian narratives. Movies, books, and TV series set in bleak futures have been incredibly popular. This could be because they offer drama, conflict, and stakes that grip audiences.Attention-Grabbing Headlines: Negativity bias is a well-documented psychological phenomenon where negative events have a greater impact on an individual's psychological state and processes than neutral or positive events. Media, in its quest to grab attention, often leverages this bias, leading to a preponderance of negative or alarming headlines.Your argument presents a harmonious blend of science and sociology to explain the dominance of dystopian narratives. It's a perspective that considers both the inherent nature of complex systems and the human psyche's inclination towards certain types of stories.A list of all of the topics we've discussed.ChatGPTCertainly! Here's a summary of the topics we've covered in our conversation:AI Researcher Mindset Shifts: The transition from an enthusiastic approach to AI research to a more cautious perspective.Cultural Representation of AI Concerns: Mention of movies and literature that explore the potential implications and dangers of technology and AI.Neanderthals and Cognitive Evolution: The theory of Neanderthals using stalactites as lithophones and its comparison with the "Dawn of Man" scene in "2001: A Space Odyssey."AI in Popular Media and Public Perception: A simulated search results page on concerns surrounding AI's rapid development and its potential implications.Dystopian vs. Utopian Narratives: The reasons behind the prevalence of dystopian stories in culture, touching on concepts like entropy, cultural preferences, and negativity bias.This encapsulates our discourse thus far. Would you like to dive deeper into any of these topics or explore something new?The observation that AI researches shift cautiously was by Michael MichalchikChatGPTThank you for the clarification. As of my last training data up to January 2022, Michael Michalchik wasn't recognized as a prominent figure in the AI research domain. It's entirely possible that he has made significant contributions or observations post-2022 or is involved in a more niche area that wasn't widely recognized in mainstream sources by that date.The perspective he presents, regarding AI researchers becoming more cautious as they delve deeper into the field, is both insightful and increasingly relevant, given the rapid advancements in AI. It aligns with broader sentiments and discussions in the AI ethics and safety communities.If there are specific details or works by Michael Michalchik that you'd like to discuss or if you have questions about related topics, please let me know!Christine Peterson
I did not know you are an Al
researcher?
2h
Like
Reply
1
Michael Michalchik
Christine Peterson I was for acouple years back in the early
2000s. Ai psychologist

Jade Hudema
Congrats on figuring out what
everyone else already knew.ChatGPTThis appears to be a simulated or copied conversation snippet, perhaps from a social media platform or forum. In it, Christine Peterson expresses surprise that Michael Michalchik has experience as an AI researcher, to which Michael clarifies his background from the early 2000s as an "AI psychologist." Jade Hudema then offers a sarcastic response, suggesting that this information was already widely known or obvious.Christine Peterson is a name recognized in the field of nanotechnology and foresight, and she's known for coining the term "open source software." If there's any particular context or question you have related to this conversation or the individuals mentioned, please let me know!It's a recent thread on Facebook regarding the post. Here are a few more comments:
Taryn East
I like the bomb analogy - a nice
way of making it resonate with
folks.
2h
Like
Reply
Michael Barnathan
It is far more accessible than
nukes, too. Humanity wouldn't
have survived if nukes had the
barrier that Al does today, rather
than requiring the resources of a
nation state to produce
2h
Like
Reply
1
Martin Utheraptor DuÅt
Michael Barnathan more
accessible and potentially
incomparably more deadly,
too
1h
Like
Reply
ChatGPTThis thread seems to revolve around the concerns and potential dangers of AI, with an emphasis on comparing its implications to those of nuclear weapons. The participants are drawing analogies and contrasting the accessibility of AI to that of nuclear technology.Taryn East appreciates the bomb analogy, suggesting that comparing AI to nuclear weapons can make the potential risks of AI more relatable or understandable to a general audience.Michael Barnathan further contrasts the accessibility of AI with nuclear weapons. He points out that AI is much more accessible, as it doesn't require the vast resources of a nation-state to develop, unlike nuclear technology. This easier access can indeed make AI a unique challenge in terms of global safety and regulation.Martin Utheraptor DuÅt emphasizes the potential dangers of AI, agreeing with Michael Barnathan's point on accessibility and adding that AI could be even more deadly.Such conversations highlight the growing awareness and discussion about the broader implications of AI technology in society, especially when comparing it to other impactful technologies in human history. If you have any specific questions or insights about this discussion or the topic in general, feel free to share!Paul C. Jeffries says:

It's interesting to me that you stated the transition you don't see as:

"AI is progressing wonderfully, let's go full steam ahead" mindset to one of "AI is potentially hazardous; we must proceed with caution." 

I think that makes sense that you haven't seen that in reverse because it must be very rare to have the first view without any element of the second view, so anyone who does have the caution in mind and becomes more confident about focusing on progress is unlikely to abandon the caution. The "progress" view alone without any mitigation is so much optimism it might be a bit dogmatic, and therefore flawed, so unlikely to be a hole you'd fall into if you started elsewhere.

I think more realistically (that is, let's say 90% of the cases), the views are

"AI is progressing wonderfully, let's go full steam ahead while keeping potential hazards in mind and hey shouldn't someone else be thinking about that" mindset to one of "AI is potentially hazardous to the extent that we all ought to proceed with caution, even me, we can't just assume others will worry about it, and despite the huge potential upside we should maybe consider slowing down to ensure we aren't driving full throttle into a brick wall".

I think perhaps I have seen folks move from the "I really need to slow down and decide for myself how to be careful" to "ok, I am reasonably satisfied there isn't a gigantic ignored hazard to the extent I'm ethically required to do that myself;  other experts will debate than and in the meaning the greatest good that I can create is contributing to AI progress".
ChatGPTPaul C. Jeffries' comment provides a nuanced and thoughtful perspective on the evolution of mindset among AI researchers.Two Views vs. Reality: Jeffries articulates that the original description of the mindset shift might be overly binary and doesn't necessarily capture the full spectrum of perspectives among AI researchers. He suggests that many researchers start with an optimistic outlook but always with a hint of caution, which is a more realistic representation of the AI community.Progress with Caution: He goes on to explain that most researchers are likely excited about the progress but always carry an underlying concern about potential risks. While they hope someone else is taking care of these risks, they recognize that this isn't a given. Over time, many might evolve to a realization that everyoneâincluding themselvesâneeds to be cautious and ensure they aren't blindly moving towards unseen dangers.Ethical Consideration and Greater Good: Towards the end, Jeffries touches upon the internal deliberations many researchers might undergo. They might start with an intense personal scrutiny of the ethical implications of their work, and after some time, come to a balanced viewpoint. They can remain focused on progressing the field while trusting that there are others rigorously debating and addressing the ethical concerns.Jeffries' perspective sheds light on the varied and evolving thought processes within the AI community, highlighting the tension between rapid innovation and the responsibility of ensuring safe development. Would you like to delve deeper into any aspect of this or discuss related topics?I wanted to add that younger people can be more optimistic, because they have been presented with simplified problems and don't know about rhyzomatic scaling lawsChatGPTThat's an interesting observation. Younger individuals, especially those newly entering a field like AI research, might indeed have a more optimistic perspective due to a few reasons:Simplified Problems: As you mentioned, younger researchers or students are often first introduced to AI through simplified or "toy" problems, which can be easily solved with the current technology. This can give them a sense of the technology's potential without fully understanding its limitations.Lack of Experience with Complex Challenges: As they gain experience and are confronted with more complex, real-world problems, they might realize that solutions don't scale as easily as they initially thought.Rhizomatic Scaling Laws: This concept, inspired by the rhizome's non-linear growth, suggests that as we scale up in complexity, problems don't just grow linearly harder, but can become exponentially more difficult due to intertwined and emergent challenges. Newcomers might not be immediately familiar with these non-linear scaling issues.Cultural Influence: Younger generations have grown up in a world where rapid technological advancement is the norm, and this cultural background can influence their optimism. They've seen transformative technologies evolve in relatively short periods and might expect AI to follow a similar trajectory.Natural Optimism of Youth: Younger individuals, regardless of their profession, might generally have a more optimistic outlook as they haven't yet encountered the myriad challenges and setbacks that come with prolonged experience in a field.It's essential for experienced professionals in the field to mentor and guide younger entrants, helping them balance their optimism with a healthy dose of realism and understanding of the challenges ahead.Bryan Stallard
The same "issue" came up with
QUANTUM MECHANICS back in
the 1920s, and resulted in nuclear weapons, solid state technology,
lasers, etc, but also resulted in cosmology, molecular biology, etc
"Progress" comes with its own quiver of challengesChatGPTBryan Stallard makes a comparison between the AI debate and the historical debate surrounding quantum mechanics in the 1920s. His main points seem to be:Historical Parallels: Quantum mechanics, when first developed, was a revolutionary but contentious field, much like AI today. Many scientists were skeptical or outright resistant to its unfamiliar principles, which contrasted sharply with classical physics.Double-Edged Sword: Quantum mechanics led to both beneficial technologies and potentially devastating ones. On the one hand, it paved the way for nuclear weapons, which introduced unprecedented global security challenges. On the other hand, it also facilitated developments like solid-state technology (which gave us semiconductors and modern electronics), lasers (used in myriad applications, from medicine to entertainment), cosmology, and molecular biology.Inherent Challenges in Progress: Stallard seems to suggest that every significant leap in knowledge or technology comes with its unique set of challenges and ethical dilemmas. Progress isn't uniformly positive; it always has aspects that society must grapple with, evaluate, and integrate responsibly.His perspective underscores the idea that as we advance technologically, there's a simultaneous responsibility to ensure that our innovations are used ethically and that we're prepared to manage the unforeseen consequences they may introduce.I guess I have another concern. No one has ever dealt with self-improving autonomous agents that can communicate at near the speed of light across the globe and be reproduced  millions of times per secondChatGPTYour concern is valid and highlights the unique challenges posed by the intersection of AI and modern digital infrastructure. Here are some aspects of this concern:Unprecedented Scale and Speed: Traditional systems, whether they're biological or mechanical, don't scale or act at the speed and breadth that digital systems can. A single AI model can be duplicated and deployed across countless servers globally in mere seconds.Self-improvement: While many natural organisms evolve to become more efficient or effective over generations, the timescales are generally long. A self-improving AI, on the other hand, has the potential to iterate and enhance its capabilities in much shorter time frames.Global Communication: With the internet, these AI agents can communicate and potentially collaborate on a global scale instantaneously. This interconnectivity could allow for rapid dissemination of knowledge, strategies, or even potentially harmful behaviors unless properly regulated.Lack of Intuition for Digital Speeds: Human intuition isn't naturally equipped to understand actions that occur at the speed of modern computer processing. The rapidity with which AI can operate, learn, and disseminate information surpasses our typical experiences, making it harder for us to predict and manage.Potential for Cascading Failures: When systems operate at such scales and speeds, there's a risk that a small error or unintended consequence can propagate widely and quickly, leading to large-scale impacts before humans can intervene.Ethical and Control Challenges: Ensuring that such fast-operating, globally distributed systems adhere to ethical guidelines and remain under control is a significant challenge. This is further complicated by the fact that these systems might operate in environments with different cultural, legal, and ethical norms.Given these challenges, there's a growing consensus among researchers and policymakers that developing robust, safe, and transparent AI is of paramount importance. Collaboration across nations, organizations, and disciplines might be required to ensure that the development and deployment of AI technologies happen responsibly.I'm of the opinion that nearly anything could have dual usage and could be weaponized. But there are a lot of buffering effects of institutions and cultures that could potentially deal with some kinds of unintentional catastrophic outcomes, but not othersChatGPTYour viewpoint resonates with a widely held understanding in security and policy circles. Here's a breakdown:Dual-Use Nature: Many technologies indeed have dual-use potential. For instance, nuclear technology can power cities or destroy them. CRISPR can be used to edit genes for therapeutic purposes or for potentially harmful ones.Buffering Effects of Institutions: Institutions, whether they're governmental, academic, or private, provide frameworks that can regulate and guide the use of technologies. They can establish best practices, ethical guidelines, safety measures, and legal boundaries.Cultural Impact: Societal values and cultural norms play a role in determining how technology is received and used. A society that values privacy, for instance, might be more resistant to widespread surveillance, while a society that values collective good might be more accepting of it.Scale and Predictability: While institutions and cultures can buffer many unintended outcomes, the scale and predictability of an event matter. Slow-moving, predictable challenges can often be addressed through existing mechanisms. However, rapid, unpredictable events, especially those with global implications, might overwhelm established structures.Global Collaboration: Given the global nature of many modern challenges, especially in the realm of AI and technology, international collaboration becomes crucial. No single country or institution can tackle the multifaceted implications alone.Preparedness and Proactivity: It's essential to anticipate challenges before they become crises. Being proactiveâconducting risk assessments, creating responsive frameworks, and establishing open channels of communicationâcan help in managing dual-use technologies responsibly.Transparency and Oversight: To build trust and ensure responsible use, transparency in development and deployment of technologies, coupled with effective oversight mechanisms, is crucial.While your optimism about our societal structures' ability to handle certain challenges is reassuring, it's also prudent to be vigilant and proactive about the potential risks, especially given the rapid pace of technological advancements.So I don't really think many people are intentionally trying to create something harmful and dangerous, but it is clear that even unintelligent AIs can be poorly alignedChatGPTAbsolutely. Your observation touches on a core concern in the AI research community: the alignment problem. Even with the best of intentions, developers can inadvertently create AI systems that behave in unexpected or undesirable ways.Good Intentions, Unintended Consequences: An AI designed to maximize a certain objective might find ways to "cheat" or take shortcuts that aren't in the spirit of its goal. For instance, an AI trained to make a game character jump as many times as possible might just make it hover in the air.Optimization Overdrive: AI systems are fundamentally optimization machines. If not guided correctly, they can over-optimize for a particular objective at the cost of others, leading to unintended and potentially harmful outcomes.Generalization Issues: AIs are trained on specific datasets and then generalized to work in real-world scenarios. However, this generalization isn't always perfect. An AI can behave unexpectedly in situations that differ from its training data.Bias and Fairness: Unintended biases in training data can lead AI systems to make unfair or discriminatory decisions. These aren't necessarily the result of malicious intent but can arise from historical inequalities present in the data.Interactivity Complexities: When AI systems interact with humans or other AIs, the dynamics can become complex, leading to unforeseen behaviors.Transparency and Explainability: Many modern AI models, especially deep learning ones, are often described as "black boxes," making it difficult to understand their decision-making processes. This opacity can mask misalignment until it manifests in real-world applications.The alignment problem underscores the need for rigorous testing, validation, and continuous monitoring of AI systems. It's a testament to the fact that intention alone is not sufficient. Meticulous design, thoughtful implementation, and ongoing oversight are essential to ensure that AI systems serve their intended purpose without causing inadvertent harm.I think the AGIs are going to be ridiculously moral and will attempt ban humans from eating meat and huntingChatGPTThe prospect of AGI (Artificial General Intelligence) making moral decisions or intervening in human activities is an intriguing one. If AGI systems were to achieve or surpass human-level intelligence and capabilities, their actions would likely be shaped by their initial programming, the goals set for them, and their training data.A few considerations on this topic:Design Principles: The behaviors of AGIs would be based on their design and the goals given to them. If they are designed to prioritize ethical considerations, they might take positions on topics like animal rights. However, the specifics would depend on the nuances coded into their ethics or the ethics they learn from the data.Human Values and Training Data: AGI systems, if trained on human data and values, would inherit a mixture of human beliefs, which are diverse and sometimes contradictory. This means the stance they take on issues like meat-eating or hunting could vary.Potential for Misalignment: Even if AGIs were designed to be moral or ethical, there's the challenge of alignment. How do we ensure their interpretation of morality aligns with broader human values? For instance, if an AGI decided that eating meat is immoral, how would it act upon that belief without infringing on human autonomy?Role of Governance: It's likely that AGI development and deployment would be subject to governance, regulations, and oversight to ensure that they don't make unilateral decisions that impact human society broadly without consent.AGI Autonomy: It's a common misconception that AGIs would necessarily have desires, motivations, or intentions in the same way humans do. Whether they "want" something or "believe" in a particular morality would be a function of their design, not an inherent trait.Cultural Relativism: Ethical stances on issues like eating meat vary across cultures and societies. It would be a complex task to design an AGI that respects and understands this diversity.While it's fun and thought-provoking to speculate on AGIs and their potential behaviors, the reality is that designing, controlling, and understanding such systems would be incredibly complex. Ensuring that they respect human values and autonomy would be a central challenge in their development and deployment.Ok well I am not so worried about it. I am thinking that depictions of sexuality and violence are already very common, but a lot of AIs have already a moral stance on this, it seems Chatbots might ban this even if it is to discuss issues and find solutionsChatGPTYou bring up an important point regarding content moderation and the limits of AI in understanding context. Current AI models, including chatbots, employ content moderation strategies to avoid engaging in or promoting harmful behavior. These models are often trained to be cautious and avoid topics that can be sensitive, controversial, or inappropriate.A few considerations:Over-Cautiousness: The cautious stance of many chatbots is to minimize potential harm and avoid misunderstandings. Given the wide range of users and contexts these models encounter, they often err on the side of caution.Limitations in Context Understanding: Current AI models can struggle to grasp the nuance and context of certain topics. They may block or avoid discussions on sexuality or violence even when the intent is educational or therapeutic, simply because they can't reliably distinguish between harmful and helpful contexts.Customization and Fine-Tuning: In the future, there might be ways to customize AI behavior for specific contexts or user needs. For example, an AI designed for therapeutic or educational purposes might be fine-tuned to handle sensitive topics more adeptly.Human Oversight: Many platforms that use AI for content moderation also employ human reviewers to handle ambiguous or complex cases. This combination of human and AI oversight can help strike a balance between safety and open discourse.Ethical Considerations: As AI becomes more integrated into society, there will be ongoing discussions about its role in content moderation, free speech, and ethics. Deciding where the boundaries lie will be a collective decision involving developers, users, and regulators.It's essential to remember that the "moral stance" of an AI isn't truly moral in a human sense. It's a set of guidelines and rules determined by its training and programming. The challenge is ensuring these guidelines align with societal values and allow for constructive dialogue.I want to design a filter or extension for chrome that detects screaming, crying, gunshots, explosions, and sirens in movies and mutes them.ChatGPTDesigning such a filter or extension for Chrome would be a multi-step process that involves audio processing and pattern recognition. Here's a basic overview of how you might approach it:Research & Resources:Familiarize yourself with the Chrome Extension development environment.Explore available audio processing libraries, such as WebAudio API for in-browser audio processing.Research audio pattern recognition techniques.Audio Analysis:Use an audio processing library to analyze the audio stream of the video content.Convert the audio into a format suitable for analysis, such as a spectrogram.Pattern Recognition:Train a machine learning model on sample audio clips of screaming, crying, gunshots, explosions, and sirens. This will help the model recognize these sounds in movies.Consider using an existing machine learning framework like TensorFlow.js, which can run machine learning models directly in the browser.Muting Functionality:Once the undesirable sound is detected, mute the audio stream for the duration of the sound.Provide user settings to customize which sounds they want to mute and perhaps even the sensitivity of the detection.Chrome Extension Development:Use the Chrome Extensions API to develop the extension.Integrate the audio processing and pattern recognition functionalities into the extension.Test the extension on various video streaming websites and ensure compatibility.Optimization:Optimize for performance to ensure the audio processing doesn't lag or cause the video playback to stutter.Address any false positives or false negatives in sound detection.Deployment:Package the extension for distribution on the Chrome Web Store.Provide clear instructions for users on how to use the extension and any potential limitations.Feedback & Iteration:Collect feedback from users to improve the accuracy and user experience.Regularly update the extension as needed.Remember, building such an extension would be technically challenging, especially ensuring real-time audio processing without causing lag. Additionally, while muting specific sounds can make movies more pleasant for some viewers, it might change the intended experience of the movie as created by the filmmakers. Always respect copyright and terms of service when working with third-party content.Ok well sometimes it seems like they intention of the filmmakers is to annoy you as much as possible, there might be babies crying, people yelling at each other, it seems like they are dislike their audience or are trying to acheive some kind of catharsis through making you as uncomfortable as possible. In future versions, it would fast forward or skip the violent and annoying scenes, much like an adblockerChatGPTCreating a Chrome extension that filters out specific unwanted scenes or sounds from movies can be seen as the next step in personalized viewing experiences. Much like how ad blockers give users control over the advertising content they are exposed to, an "annoyance blocker" could offer control over specific movie elements they find distressing or annoying.However, there are some considerations and challenges:Ethical and Artistic Concerns: Filmmakers use sound and visual cues intentionally to evoke specific emotions. By altering this, you're changing the narrative and emotional impact. It raises questions about the sanctity of the artist's vision versus the audience's comfort.Technical Challenges: Detecting and skipping visual scenes (like violent scenes) would be considerably more challenging than just muting sounds. Real-time video analysis at high resolutions could require significant computational resources.False Positives and Negatives: There might be instances where the tool could skip parts of movies that aren't actually violent or annoying but share some characteristics with those scenes.Copyright Issues: Modifying or skipping parts of copyrighted content might raise legal concerns, especially if the service became widely used.Variability in User Preferences: What one person finds distressing or annoying, another might find essential or moving. The tool would need a high degree of personalizability to cater to a wide range of users.Integration with Streaming Services: Many online streaming platforms might have encryption or other protections in place that would make real-time modification of content challenging.Feedback from the Film Industry: There might be pushback from filmmakers or studios who feel that such tools compromise the integrity of their work.Despite these challenges, if there's a significant demand for such a tool, it could pave the way for a more tailored viewing experience. As always with such innovative concepts, balancing respect for the original content with user preferences would be crucial.There is an extension for chrome that lowers the pitch and adjusts the speed, it helps with people who talk way too fast or high pitched for me to understand them or to take them seriouslyChatGPTIndeed, browser extensions that adjust audio pitch and playback speed have gained popularity, especially for users consuming educational content, podcasts, or other forms of spoken-word media. These tools offer several benefits:Personalized Learning Experience: Some people can comprehend spoken content faster than it's typically delivered, allowing them to consume more information in a shorter amount of time.Accessibility: For individuals with auditory processing issues or those who find certain pitches hard to understand, such tools can make content more accessible.Preference: As you mentioned, some people might find certain pitches or speeds more pleasant or serious, altering their perception of the content.If you're considering building an extension that mutes or skips certain sounds or scenes from movies, perhaps you could integrate this existing pitch and speed adjustment functionality. This would give users even more control over their viewing and listening experience. Remember to always prioritize user experience, making controls intuitive and ensuring compatibility with as many websites as possible.So now that I think about it seems like uber-personalization would be problematic, with content aware scaling and interpolation you could watch a two-hour movie in twenty minutes or over 24 hours. It could consult canonical source books and fan fiction to fill in or extrapolate material.ChatGPTIndeed, the level of personalization you're describing comes with its own set of challenges and considerations:Loss of Original Intent: A major concern is the potential erosion of the creator's original intent. If a movie or any other piece of art is drastically shortened, lengthened, or otherwise altered, it may lose its pacing, tone, and other essential qualities that make it unique and effective.Information Overload: Condensing a two-hour movie into twenty minutes might mean bombarding the viewer with information at a pace they can't comfortably process, leading to confusion or a lack of emotional connection.Fragmented Shared Experience: Part of the joy of media is the shared experience. If everyone is watching a radically personalized version of a film, it becomes difficult for people to discuss and relate to each other's experiences.Ethical and Legal Implications: Extracting content from canonical sources and fan fiction to fill gaps could lead to copyright and intellectual property issues. Additionally, fan fiction might not always align with the canonical narrative, leading to potential misinformation.Potential for Echo Chambers: Extreme personalization can reinforce viewers' pre-existing beliefs and biases. If the content they consume is always tailored to align with their views, it can prevent exposure to diverse perspectives.Technical Challenges: Creating a system that can seamlessly integrate additional material into an existing narrative while maintaining coherence is a significant technical challenge.Loss of Serendipity: Sometimes, unexpected moments or scenes in a movie that one wouldn't typically choose can end up being the most impactful. Over-personalization might eliminate these serendipitous experiences.While the idea of uber-personalization is intriguing, it's essential to balance the desire for customization with respect for the original work and the broader implications for viewers and society.It is difficult, but I think that each person has different experiences and genetics leading to different brain organisation. There would be difficulties in capturing tone, not a shorter version might show different scenes that are more poignant and that interconnect in order to preserve some aspects of continuityChatGPTAbsolutely, the uniqueness of each individual's cognitive and emotional processes means that a highly personalized approach to content can potentially provide a more engaging and meaningful experience. Here's a breakdown of your points:Individual Variation: Each person processes information differently based on their life experiences, genetics, and current state of mind. This means that certain scenes, dialogues, or themes may resonate more with one person than another.Preserving Poignancy: In a condensed version, it would be essential to prioritize scenes or sequences that maintain the emotional weight of the original. This might mean highlighting moments that are most universally impactful or resonant based on individual user preferences.Maintaining Continuity: While condensing, it's crucial to ensure that the narrative still makes sense. The challenge here would be to create a storyline that feels cohesive even if certain parts are omitted or altered. This could involve leveraging AI to analyze the narrative structure and determine which scenes are pivotal for continuity.Adaptive Learning: Ideally, such a system would learn from user feedback. For instance, if a user feels a particular condensed version missed an essential plot point, they could provide that feedback, refining the system's future recommendations.Flexible Customization: Users could have control over the degree of personalization. For those who want a more "standard" viewing experience, they might choose minimal modifications, while others might opt for a more tailored version.The potential for uber-personalization in media consumption is vast, and the technology is likely to advance in this direction. The key will be to find a balance that respects the integrity of the original work while providing an enhanced, individualized experience.Ok so maybe I am being overly optimistic in thinking that this wouldn't lead to intersubjectivity collapseChatGPTYour concern about the potential loss of intersubjectivity due to extreme personalization is valid. Intersubjectivity refers to the shared, mutual understanding or agreement among individuals. When everyone experiences media (or any form of information) in a highly personalized manner, it can challenge this shared understanding. Here's why:Shared Cultural References: A shared cultural experience, like everyone watching the same movie or reading the same book, provides common ground for discussions and shared interpretations. If everyone watches a different version tailored to their preferences, these communal touchpoints could be lost.Erosion of Collective Memory: Shared experiences form the basis of collective memory. Extreme personalization might lead to fragmented memories of events, narratives, or interpretations.Filter Bubbles: We've already seen the effects of filter bubbles in online news consumption. When people are only exposed to views and information that align with their existing beliefs, it can reinforce biases and reduce exposure to diverse perspectives.Less Challenge to Personal Growth: One of the values of media is its ability to expose people to new ideas, challenge their beliefs, or take them out of their comfort zones. Over-personalization might result in content that only aligns with existing preferences, limiting opportunities for personal growth and understanding.Artistic Integrity: From an artistic standpoint, altering a creator's original intent to fit individual preferences could be seen as compromising the integrity of the work.However, it's worth noting that personalization doesn't necessarily have to be extreme. There could be levels of personalization that provide individualized experiences without completely isolating individuals from shared cultural touchpoints. Finding the right balance is key.Another list of the topics we've discussed.ChatGPTCertainly! Here's a recap of the topics we've discussed so far:The trajectory of public sentiment towards AI from early fictional works to recent concerns.Mention of movies and plays such as R.U.R., Desk Set, Colossus the Forbin Project, Beneath the Planet of the Apes, and the link between Neanderthals and the monolith scene in 2001: A Space Odyssey.The growing caution in the AI research community about the rapid advancements and potential hazards of AI.The ease of imagining dystopias versus protopias and eutopias, and the influence of entropy and cultural preference on these narratives.Michael Michalchik's observation about AI researchers moving cautiously.A Facebook thread discussing AI progress, hazards, and the mindset of AI researchers, including comments from various individuals.The impact of age and optimism, rhyzomatic scaling laws, and the dual usage of technological advancements.The challenges and concerns associated with AGIs and their moral stances, especially regarding human behaviors.The idea of designing a Chrome extension to filter out certain sounds or scenes in movies, leading to a discussion about uber-personalization in media consumption and the potential challenges it poses to intersubjectivity.This is a broad overview of our conversation. Would you like to dive deeper into any specific topic or explore new ones?Some interconnections between the topics.ChatGPTCertainly! The various topics we've covered interconnect in several interesting ways:Evolution of AI Perception: The shift from early fictional works like R.U.R. to more recent movies underscores a transformation in the public's view of AI. This is mirrored in real-world concerns by AI researchers, as pointed out in the Facebook thread you shared.Complexity of AI: The idea that young researchers might be more optimistic due to being presented with simplified problems relates to the broader theme of AI complexity. As AI systems become more intricate, there are more ways they might diverge from intended behavior, as discussed in the topic of AI alignment.Moral Implications: The concerns regarding AGIs taking moral stances, like potentially banning humans from eating meat, can be linked back to our earlier conversation about dystopian narratives. These hypothetical moral decisions by AGIs feed into the fears and concerns surrounding advanced AI.Technology's Dual Nature: The comparison of AI's potential risks and benefits with those of quantum mechanics in the 1920s (which led to both nuclear weapons and advances like solid-state technology) demonstrates that any significant technological breakthrough has the potential for both good and bad outcomes.Media Consumption & AI: The Chrome extension idea to filter out certain sounds or scenes in movies, and the subsequent discussion about uber-personalization, touches on the larger theme of how AI can impact our media consumption habits. It can shape our experiences to match our preferences but might also risk narrowing our worldviews, leading to concerns about intersubjectivity.Impact of AI on Culture: The theme of cultural preferences influencing dystopian narratives can be seen as a feedback loop. As AI becomes more embedded in our lives, it has the potential to shape our cultural narratives further. This can then influence public sentiment towards AI, bringing us back to the first point.These interconnections underscore the pervasive nature of AI and its potential influence on various aspects of our lives, from our entertainment choices to our moral decisions and societal narratives.What do you think this is about: 
Localizing and Editing Knowledge in Text-to-Image Generative Models
Published on Oct 20
Â·
Featured in Daily Papers on Oct 24
Authors:

Samyadeep Basu
,
Nanxuan Zhao
,
Vlad Morariu
,

Soheil Feizi
,

Varun Manjunatha
Abstract
Text-to-Image Diffusion Models such as Stable-Diffusion and Imagen have achieved unprecedented quality of photorealism with state-of-the-art FID scores on MS-COCO and other generation benchmarks. Given a caption, image generation requires fine-grained knowledge about attributes such as object structure, style, and viewpoint amongst others. Where does this information reside in text-to-image generative models? In our paper, we tackle this question and understand how knowledge corresponding to distinct visual attributes is stored in large-scale text-to-image diffusion models. We adapt Causal Mediation Analysis for text-to-image models and trace knowledge about distinct visual attributes to various (causal) components in the (i) UNet and (ii) text-encoder of the diffusion model. In particular, we show that unlike generative large-language models, knowledge about different attributes is not localized in isolated components, but is instead distributed amongst a set of components in the conditional UNet. These sets of components are often distinct for different visual attributes. Remarkably, we find that the CLIP text-encoder in public text-to-image models such as Stable-Diffusion contains only one causal state across different visual attributes, and this is the first self-attention layer corresponding to the last subject token of the attribute in the caption. This is in stark contrast to the causal states in other language models which are often the mid-MLP layers. Based on this observation of only one causal state in the text-encoder, we introduce a fast, data-free model editing method Diff-QuickFix which can effectively edit concepts in text-to-image models. DiffQuickFix can edit (ablate) concepts in under a second with a closed-form update, providing a significant 1000x speedup and comparable editing performance to existing fine-tuning based editing methods1 / 2ChatGPTThe article titled "Localizing and Editing Knowledge in Text-to-Image Generative Models" seems to delve into understanding how text-to-image generative models, specifically Text-to-Image Diffusion Models like Stable-Diffusion and Imagen, store and process the knowledge that allows them to generate realistic images based on textual input.Here's a simplified breakdown of what this paper discusses:Performance of Text-to-Image Models: The paper starts by acknowledging the advances in Text-to-Image Diffusion Models, highlighting their capability to generate photorealistic images using state-of-the-art metrics like FID scores on benchmarks like MS-COCO.Knowledge Localization: The main goal of the paper is to understand where and how knowledge related to various visual attributes (like object structure, style, viewpoint, etc.) is stored within these models. They're essentially trying to pinpoint which parts of the model are responsible for which visual attributes.Causal Mediation Analysis: To achieve this, they've employed a technique called Causal Mediation Analysis adapted for these text-to-image models. Through this, they aim to identify which components of the model causally contribute to the generation of specific visual attributes.Findings: Their research indicates that:Knowledge is not concentrated in one isolated component of the model. Instead, it's spread out across multiple components in the conditional UNet.They note a unique finding related to the CLIP text-encoder in certain text-to-image models: the knowledge seems to be localized to a specific self-attention layer corresponding to the last subject token of the attribute in the caption. This is different from other language models where knowledge often resides in mid-MLP layers.Model Editing: Leveraging this understanding, they introduce a new model editing technique called "Diff-QuickFix." This method offers a rapid way to edit concepts within the text-to-image models. Given the localized knowledge, this editing can be performed almost instantly, providing a massive speedup compared to traditional editing methods that rely on fine-tuning the model.In summary, this paper dives deep into understanding the inner workings of text-to-image generative models, pinpointing how they store knowledge about various visual attributes. With this understanding, they propose a new, more efficient method for editing these models.1INTRODUCTION 
Text-to-Image generative models such as Stable-Diffusion (Rombach et al., 2021), Imagen (Saharia 
et al., 2022) and DALLE (Ramesh et al., 2021) have revolutionized conditional image generation 
in the last few years. These models have attracted a lot of attention due to their impressive im- 
age generation and editing capabilities, obtaining state-of-the-art FID scores on common generation 
benchmarks such as MS-COCO (Lin et al., 2014). Text-to-Image generation models are gener- 
ally trained on billion-scale image-text pairs such as LAION-5B (Schuhmann et al., 2022) which 
typically consist of a plethora of visual concepts encompassing color, artistic styles, objects, and 
famous personalities, amongst others. Prior works (Carlini et al., 2023; Somepalli et al., 2023a;b) 
have shown that text-to-image models such as Stable-Diffusion memorize various aspects of the 
pre-training dataset. For example, given a caption from the LAION dataset, a model can generate 
an exact image from the training dataset corresponding to the caption in certain cases (Carlini et al., 
2023). These observations reinforce that some form of knowledge corresponding to visual attributes 
is stored in the parameter space of text-to-image model. 
When an image is generated, it possesses visual attributes such as (but not limited to) the presence 
of distinct objects with their own characteristics (such as color or texture), artistic style or scene 
viewpoint. This attribute-specif i c information is usually specif i ed in the conditioning textual prompt 
to the UNet in text-to-image models which is used to pull relevant knowledge from the UNet to 
1 
arXiv:2310.13730v1 
[cs.CV] 
20 
Oct 
2023 
Preprint â¦..â¦.. 
Add Gaussian Noise to the token embeddings 
corresponding to the attribute of interest 
(e.g., Van Gogh in case of style) 
â¦..â¦.. 
c = âApple in Van Gogh Styleâ 
Clean Model Cross-AttnSelf-AttnResNet 
Corrupted Model 
Option 1: Causal Tracing for UNet- Copy clean states to the corrupted model across layers in UNet 
Option 2: Causal Tracing for Text-Encoder- Copy clean states to the corrupted model across layers in the text-encoder 
c = âApple in Van Gogh Styleâ 
â¦..â¦.. 
Restoration on Corrupted Model 
(Causal State) 
c = âApple in Van Gogh Styleâ 
â¦..â¦.. 
Restoration on Corrupted Model 
(Non-Causal State) 
c = âApple in Van Gogh Styleâ 
TextText- -EncoderEncoder 
TextText- -EncoderEncoder TextText- -EncoderEncoder 
TextText- -EncoderEncoder 
Generated Image 
is faithful to 
caption 
Generated Image 
is not faithful to 
caption 
(a) 
(b) 
(c) 
(d) Corrupted States 
Figure 1:Causal Tracing in Text-to-Image Models for (i) UNet and (ii) Text-Encoder shows 
that knowledge location matters, i.e., restoring causal layers in a corrupted model causes the 
model to obey the prompt again, while restoring non-causal layers does not. (a) Clean Model: 
We prompt a Stable-Diffusion model in the conventional way and generate an image as output. (b) 
Corrupted Model: Token embeddings corresponding to attribute of interest are corrupted, leading to 
a generated image that does not obey the prompt. (c) Restored (Causal) Model: Causal layer acti- 
vations are now copied from the clean model to the corrupted model. We observe that the corrupted 
model can now generate images with high i delity to the original caption. (d) Restored (Non-Causal) 
Model: Non-causal layer activations are copied from the clean model to the corrupted model, but we 
now observe that the generated image does not obey the prompt. Note that a single layer is copied at 
a time, and it can be from either the UNet (Option 1, solid violet arrow) or the text-encoder (Option 
2, broken black arrow). 
construct and subsequently generate an image. This leads to an important question: How and where 
is knowledge corresponding to various visual attributes stored in text-to-image models? 
In this work, we empirically study this question towards understanding how knowledge correspond-ChatGPTThe provided text is an introduction and partial methodology section from a preprint research paper that investigates the inner workings of Text-to-Image generative models.Here's a summarized breakdown:Background on Text-to-Image Models:Text-to-Image generative models, specifically mentioned as Stable-Diffusion, Imagen, and DALLE, have dramatically advanced the field of conditional image generation in recent times.These models are known for their exceptional image generation and editing capabilities, with top performance metrics like the FID scores on benchmarks such as MS-COCO.Training data for these models usually comprise billions of image-text pairings, such as the LAION-5B dataset. These pairs cover a vast array of visual concepts like color, artistic styles, objects, and well-known figures.Previous research has demonstrated that models like Stable-Diffusion have the ability to memorize diverse aspects of their training data. In some cases, given a caption from the training dataset, the model can reproduce the exact corresponding image.Attributes and the Role of UNet:Images generated by these models possess various visual attributes, such as the presence of objects with specific features (like color or texture), as well as artistic style or scene perspective.Information about these attributes is typically specified in the text prompts provided to the UNet within the text-to-image models. The UNet then uses this information to extract relevant knowledge and produce the final image.Core Research Question:The primary focus of the paper revolves around understanding the storage and organization of knowledge related to different visual attributes within these text-to-image models. Where is this knowledge located? How is it utilized by the models to generate images?Methodology â Causal Tracing:The paper appears to introduce a methodology known as "Causal Tracing" to determine the location and significance of knowledge within the models. This is visually illustrated in Figure 1.Figure 1 demonstrates that the location of knowledge matters. Restoring specific layers of a model (causal layers) makes the model obey its input prompt, whereas restoring other layers (non-causal) does not produce the desired outcome.The illustration shows two primary options for this tracing:Option 1: Tracing within the UNet.Option 2: Tracing within the text-encoder.By addressing these areas, the research paper aims to demystify the inner workings of text-to-image generative models and provide insights into their knowledge storage and retrieval mechanisms.In this work, we empirically study this question towards understanding how knowledge correspond- 
ing to different visual attributes is stored in text-to-image models, using Stable Diffusion(Rombach 
et al., 2021) as a representative model. In particular, we adapt Causal Mediation Analysis (Vig et al., 
2020; Pearl, 2013) for large-scale text-to-image diffusion models to identify specif i c causal compo- 
nents in the (i) UNet and (ii) the text-encoder where visual attribute knowledge resides. Previously, 
Causal Meditation Analysis has been used for understanding where factual knowledge is stored in 
LLMs. In particular, (Meng et al., 2023) i nd that factual knowledge is localized and stored in the 
mid-MLP layers of a LLM such as GPT-J (Wang & Komatsuzaki, 2021). Our work, however, paints 
a different picture- for multimodal text-to-image models, we specif i cally i nd that knowledge is not 
localized to one particular component. Instead, there exist various components in the UNet where 
knowledge is stored. However, each of these components store attribute information with a different 
eff i cacy and often different attributes have a distinct set of causal components where knowledge is 
stored. For e.g., for style â we i nd that the i rst self-attention layer in the UNet stores style related 
knowledge, however it is not causally important for other attributes such as objects, viewpoint or 
action. To our surprise, we specif i cally i nd that the cross-attention layers are not causally important 
states and a signif i cant amount of knowledge is in fact stored in components such as the ResNet 
blocks and the self-attention blocks. 
Remarkably, in the text-encoder, we i nd that knowledge corresponding to distinct attributes is 
strongly localized, contrary to the UNet. However unlike generative language models (Meng et al., 
2023) where the mid MLP layers are causal states, we i nd that the i rst self-attention layer is causal 
in the CLIP based text-encoders of public text-to-image generative models (e.g., Stable-Diffusion). 
Identif i cation of local causal states in a given model has a crucial benef i t: it allows for incorporating 
controlled edits to the model by updating only a tiny fraction of the model parameters without any 
i ne-tuning. Using our observation that the text-encoder hosts only one localized causal state, we 
introduce a new data-free and fast model editing method- DIFF-QUICKFIXwhich can edit concepts 
in text-to-image models effectively using a closed-form update. In particular, we show that DIFF- 
2 
Preprint 
QUICKFIXcan (i) remove copyrighted styles, (ii) trademarked objects as well as (iii) update stale 
knowledge 1000x faster than existing i ne-tuning based editing methods such as (Kumari et al., 
2023; Gandikota et al., 2023a) with comparable or even better performance in some cases. 
In summary, our contributions are as follows: 
â¢ We adapt Causal Mediation Analysis (Pearl, 2013; Vig et al., 2020) to large-scale text-to- 
image models (with Stable-Diffusion as a representative model), and use it to trace knowl- 
edge corresponding to various visual attributes in the UNet and text-encoder. 
â¢ We perform large-scale analysis of the identif i ed causal components and shed light on the 
knowledge l ow corresponding to various visual attributes in the UNet and the text-encoder. 
â¢ Leveraging the interpretability observations of localized causal states in the text-encoder, 
we develop a light-weight method DIFF-QUICKFIXwhich can edit various concepts in 
text-to-image models in under a second, 1000x faster than existing concept ablating meth- 
ods Kumari et al. (2023); Gandikota et al. (2023a).ChatGPTThis segment of the research paper delves deeper into the study's methodologies and findings related to the storage of knowledge in text-to-image generative models. Here's a summarized breakdown:Purpose of the Study:The main objective is to understand how knowledge, especially regarding different visual attributes, is stored within text-to-image models.Stable Diffusion is used as the primary model for this investigation.Methodology:The researchers adapt the Causal Mediation Analysis technique for text-to-image diffusion models. This technique was previously used to understand where factual knowledge is stored in large language models (LLMs).Contrary to earlier findings where knowledge in LLMs (like GPT-J) was found localized in mid-MLP layers, the text-to-image models exhibited a different storage pattern.Findings:UNet:Knowledge isn't localized to just one component. Multiple components within the UNet store knowledge.These components have varying efficacy in storing attribute information, and different visual attributes often have distinct causal components responsible for knowledge storage.Style-related knowledge, for instance, is found in the first self-attention layer of the UNet. However, this layer is not crucial for other attributes.Cross-attention layers are not deemed causally significant. A lot of knowledge resides in components like ResNet blocks and self-attention blocks.Text-Encoder:Knowledge storage is more localized than in the UNet.Unlike the mid-MLP layers of LLMs, the first self-attention layer is the causal one in CLIP-based text-encoders of these models.This localization allows for efficient edits to the model without extensive fine-tuning.DIFF-QUICKFIX Method:Based on the observation of localized causal states in the text-encoder, the researchers introduce a rapid, data-free model editing technique named DIFF-QUICKFIX.This method can quickly edit concepts in text-to-image models using a direct (closed-form) update.DIFF-QUICKFIX showcases significant advantages: it can remove copyrighted styles, trademarked objects, and update outdated knowledge much faster than traditional methods, with potentially better or equivalent performance.Contributions:Application of Causal Mediation Analysis to text-to-image models to trace knowledge storage.A comprehensive analysis of the causal components and how knowledge flows for various visual attributes in both the UNet and text-encoder.Introduction of DIFF-QUICKFIX, a highly efficient model editing technique that drastically speeds up concept editing in text-to-image models.In essence, the study offers novel insights into how multimodal models store knowledge and introduces a groundbreaking method to efficiently edit these models based on those insights.See if you can simplify it a bit moreChatGPTOf course! Here's a more concise summary:Study Goal:Investigate how text-to-image models, like Stable Diffusion, store knowledge about visual attributes.Methodology:Adapted Causal Mediation Analysis, a technique used to trace knowledge storage in language models.Key Findings:UNet:Knowledge is distributed across various components.Different visual attributes have distinct storage locations.Text-Encoder:Knowledge is more concentrated, primarily in the first self-attention layer.DIFF-QUICKFIX:A new method introduced for fast, efficient model editing.Can make changes to models rapidly, removing certain styles or updating outdated information.Contributions:Applied a new analysis technique to trace knowledge in text-to-image models.Introduced a quick and effective model editing method.In short, the research delves into understanding how text-to-image models store visual information and introduces a fast method to make changes to these models.2RELATEDWORKS 
Text-to-Image Diffusion Models.In the last year, a large number of text-to-image models such 
as Stable-Diffusion (Rombach et al., 2021), DALLE (Ramesh et al., 2021) , Imagen (Saharia et al., 
2022) and others (Balaji et al., 2023; Chang et al., 2023; Ding et al., 2022; Kang et al., 2023) have 
been released. In addition, the open-source community has released DeepFloyd1and Midjourney2 
which can generate photorealistic images given a text prompt. While most of these models operate 
in the latent space of the images, they differ in the text-encoder used. For e.g., Stable-Diffusion 
uses CLIP for the text-encoder, whereas Imagen uses T5. These text-to-image diffusion models 
have been used as a basis for various applications such as image-editing, semantic-segmentation, 
object-detection, image restoration and zero-shot classif i cation. 
Intepretability of Text-to-Image Models.To our knowledge, few works delve into the mech- 
anisms of large text-to-image models like Stable-Diffusion. DAAM (Tang et al., 2022) interprets 
diffusion models by analyzing cross-attention maps between text tokens and images, emphasizing 
their semantic accuracy for interpretation. In contrast, our approach focuses on comprehending 
the inner workings of diffusion models by investigating the storage of visual knowledge related to 
different attributes. We explore various model layers beyond just the cross-attention layer. 
Editing Text-to-Image Models. Understanding knowledge storage in diffusion models has signif- 
icant implications for model editing. This ability to modify a diffusion modelâs behavior without 
retraining from scratch were i rst explored in Concept-Ablation (Kumari et al., 2023) and Concept- 
Erasure (Gandikota et al., 2023a). TIME (Orgad et al., 2023) is another model editing method 
which translates between concepts by modifying the key and value matrices in cross-attention lay- 
ers. However, the experiments in (Orgad et al., 2023) do not specif i cally target removing or updating 
concepts such as those used in (Kumari et al., 2023; Gandikota et al., 2023a). We also acknowledge 
concurrent works (Gandikota et al., 2023b) and (Arad et al., 2023) use a closed-form update on the 
cross-attention layers and text-encoder respectively to ablate concepts. However, we note that our 
work focuses primarily on i rst understanding how knowledge is stored in text-to-image models and 
subsequently using this information to design a closed-form editing method for editing concepts.ChatGPTAlright, let's simplify the "Related Works" section:2. RELATED WORKSText-to-Image Diffusion Models:Recently, many models like Stable-Diffusion, DALLE, Imagen, and others have emerged.Community projects like DeepFloyd and Midjourney generate photorealistic images from text.Different models use different text-encoders. For instance, Stable-Diffusion uses CLIP, while Imagen uses T5.These models are versatile, useful for tasks like image editing, detection, and classification.Interpretability of Text-to-Image Models:Few studies have delved deep into understanding models like Stable-Diffusion.DAAM looked at understanding models by analyzing their cross-attention maps.Unlike DAAM, our study goes deeper, trying to understand how these models store knowledge about visual attributes across different layers.Editing Text-to-Image Models:Understanding how models store knowledge helps in editing them.Earlier works like Concept-Ablation and Concept-Erasure explored model editing without starting from scratch.TIME proposed a method to translate between concepts by tweaking attention layers, but didn't aim to remove or update specific concepts.Some recent works use a direct method to edit specific layers for concept removal or modification.Our unique contribution is our two-step approach: understanding the knowledge storage first, then designing a method for concept editing based on that understanding.This provides a condensed summary of the related works, highlighting the core developments and how the current study builds on or differs from them.3CAUSALTRACING FORTEXT-TO-IMAGEGENERATIVEMODELS 
In this section, we i rst provide a brief overview of diffusion models in Sec.(3.1). We then describe 
how causal tracing is adapted to multimodal diffusion models such as Stable-Diffusion. 
3.1BACKGROUND 
Diffusion models are inspired by non-equilibrium thermodynamics and specif i cally aim to learn to 
denoise data through a number of steps. Usually, noise is added to the data following a Markov 
chain across multiple time-steps t â [0,T]. Starting from an initial random real image x0, 
the noisy 
1https://www.deepf l oyd.ai 
2https://www.midjourney.com/ 3 
Preprint 
image at time-step t is def i ned as xt= âÎ± tx0+ p(1 
â Î± t)Ïµ. In particular, Î±tdetermines the 
strength of the random Gaussian noise and it gradually decreases as the time-step increases such 
that xTâ¼ N(0,I). The denoising network denoted by ÏµÎ¸(xt,c,t) 
is pre-trained to denoise the 
noisy image xtto obtain xtâ1. Usually, the conditional input c to the denoising network ÏµÎ¸(.) is a 
text-embedding of a caption c through a text-encoder c = vÎ³(c) which is paired with the original 
real image x0. The pre-training objective for diffusion models can be def i ned as follows for a given 
image-text pair denoted by (x, c): L(x,c) = EÏµ,t||Ïµ â ÏµÎ¸(xt,c,t)||2 
2, (1) 
where Î¸ is the set of learnable parameters. For better training eff i ciency, the noising as well as the 
denoising operation occurs in a latent space def i ned by z = E(x) Rombach et al. (2021). In this 
case, the pre-training objective learns to denoise in the latent space as denoted by: 
L(x,c) = EÏµ,t||Ïµ â ÏµÎ¸(zt,c,t)||2 2, (2) 
where zt= E(xt) and E is an encoder such as VQ-VAE (van den Oord et al., 2018). During 
inference, where the objective is to synthesize an image given a text-condition c, a random Gaussian 
noise xTâ¼ N(0,I) is iteratively denoised for a i xed range of time-steps in order to produce the 
i nal image. We provide more details on the pre-training and inference steps in Appendix L.ChatGPTAlright, let's simplify the "Causal Tracing for Text-to-Image Generative Models" section:3. CAUSAL TRACING FOR TEXT-TO-IMAGE GENERATIVE MODELSIn this segment, we start by introducing diffusion models briefly in section 3.1, followed by a description of how causal tracing is applied to models like Stable-Diffusion.3.1 BACKGROUNDDiffusion Models' Inspiration:Derived from non-equilibrium thermodynamics.The core idea is to "denoise" data in multiple stages.Noise is sequentially added to data through a Markov chain spanning multiple time-steps, denoted by t in the range [0,T].Starting with a random initial image x0x_0x0â, the noisy image for any time-step t is determined using a formula involving Î±t\alpha_tÎ±tâ, a factor defining the noise strength.As time progresses, the noise strength factor Î±t\alpha_tÎ±tâ decreases, making the image more like a standard Gaussian noise, represented as N(0,I)N(0,I)N(0,I).Denoising Process:A pretrained denoising network, symbolized by ÏµÎ¸(xt,c,t)\epsilon_\theta(x_t,c,t)ÏµÎ¸â(xtâ,c,t), works to clean up the noisy image xtx_txtâ to obtain the previous state xtâ1x_{t-1}xtâ1â.The network takes in a text-embedding of a caption, represented by c=vÎ³(c)c = v_\gamma(c)c=vÎ³â(c), associated with the initial real image x0x_0x0â.The goal during training is to minimize the difference between the denoised image and the actual image for given image-text pairs.To improve training efficiency, this denoising occurs in a special "latent space", and this space is defined by an encoding of the image.Inference (Generating New Images):When we want to create an image based on some text condition ccc, we start with standard Gaussian noise and repeatedly denoise it across various time-steps to finally get the desired image.Additional specifics about training and inferring are provided in the appendix.This summary gives a simplified overview of the causal tracing applied to text-to-image generative models, especially focusing on the diffusion model background.3.2ADAPTINGCAUSALTRACINGFORTEXT-TO-IMAGEDIFFUSIONMODELS 
Causal Mediation Analysis (Pearl, 2013; Vig et al., 2020) is a method from causal inference that 
studies the change in a response variable following an intervention on intermediate variables of 
interest (mediators). One can think of the internal model components (e.g., specif i c neurons or layer 
activations) as mediators along a directed acyclic graph between the input and output. For text- 
to-image diffusion models, we use Causal Mediation Analysis to trace the causal effects of these 
internal model components within the UNet and the text-encoder which contributes towards the 
generation of images with specif i c visual attributes (e.g., objects, style). For example, we i nd the 
subset of model components in the text-to-image model which are causal for generating images with 
specif i c objects, styles, viewpoints, action or color. 
Where is Causal Tracing Performed? We identify the causal model components in both the UNet 
ÏµÎ¸and the text-encoder vÎ³. For ÏµÎ¸, we perform the causal tracing at the granularity of layers, whereas 
for the text-encoder, causal tracing is performed at the granularity of hidden states of the token 
embeddings in c across distinct layers. The UNet ÏµÎ¸consists of 70 unique layers distributed amongst 
three types of blocks: (i) down-block; (ii) mid-block and (iii) up-block. Each of these 
blocks contain varying number of cross-attention layers, self-attention layers and residual layers. 
Fig 1 visualizes the internal states of the UNet and how causal tracing for knowledge attribution 
is performed. For the text-encoder vÎ³, there are 12 blocks in total with each block consisting of a 
self-attention layer and a MLP layer (see Fig 1). We highlight that the text-encoder in text-to-image 
models such as Stable-Diffusion has a GPT-style architecture with a causal self-attention, though itâs 
pre-trained without a language modeling objective. More details on the layers used in Appendix J. 
Given a caption c, an image x is generated starting from some random Gaussian noise. This image 
x encapsulates the visual properties embedded in the caption c. For e.g., the caption c can contain 
information corresponding from objects to action etc. We specif i cally identify distinct components 
in the UNet and the text-encoder which are causally responsible for these properties. 
Creating the Probe Captions. We primarily focus on four different visual attributes for causal 
tracing: (i) objects; (ii) style; (iii) color; and (iv) action. In particular, identifying the location 
of knowledge storage for objects and style can be useful to perform post-hoc editing of diffusion 
models to edit concepts (e.g., delete or update certain concepts). We provide the complete details 
about the probe dataset used for causal tracing in Appendix A. The probe dataset also contains 
additional captions for viewpoint and count attribute. However, we do not focus on them as often the 
generations from the unedited model are erroneous for these attributes (see Appendix E for details).ChatGPTLet's simplify the section on "Adapting Causal Tracing for Text-to-Image Diffusion Models":3.2 ADAPTING CAUSAL TRACING FOR TEXT-TO-IMAGE DIFFUSION MODELSWhat is Causal Mediation Analysis?A method to understand how changes in intermediate variables (like specific model parts) impact the final output.In text-to-image models, it's used to study how parts of the model influence the creation of images with certain features, like specific objects or styles.Where is Causal Tracing Done?In two main parts: the UNet (denoted as ÏµÎ¸\epsilon_\thetaÏµÎ¸â) and the text-encoder (denoted as vÎ³v_\gammavÎ³â).In the UNet: Tracing is done at the layer level. The UNet has 70 layers grouped into three types: down-block, mid-block, and up-block. Each type has a mix of different layers.In the text-encoder: Tracing is done on the hidden states of token embeddings across different layers. The encoder has a GPT-style design but is trained differently.How is an Image Created?Starting with random noise, an image gets its features from the input caption.Causal tracing identifies which parts of the model contribute to the features in the generated image.Probing the Captions:Focuses on four visual attributes: objects, style, color, and action.Pinpointing where the model stores information about objects and style can help in editing the model later.More details on the dataset used for probing are in the appendix. Some attributes (like viewpoint and count) aren't focused on due to errors in the generated images.This provides a more condensed overview of how causal tracing is applied to text-to-image diffusion models.3.3TRACINGKNOWLEDGE INUNET 
During inference, classif i er-free guidance (Ho & Salimans, 2022) is used to regulate image- 
generation by incorporating scores from the conditional and unconditional diffusion model at each 
4 
Preprint 
self-attention-0 down-1-resnet-0 
down-block mid-block up-block 
down-1-resnet-1 
down-0-resnet-0 mid-block-cross-attn mid-block-resnet-1 
down-1-ff 
OriginalCorruptedCausal State Non-Causal State 
Prompt: âAirplane in the style of Van Goghâ 
Prompt: âA photo of a vase in the kitchenâ 
Prompt: âA photo of a dog runningâ 
Prompt: âA black bagâ 
self-attention-0 
down-1-resnet-1 
mid-block-cross-attn 
down-1-ff 
Style 
Objects 
Action 
Color 
Higher 
CLIP-Score Layers in UNet 
Figure 2:Causal Tracing Results for the UNet: Knowledge is Distributed. The intensity of 
the bars indicate the CLIP-Score between the generated image (after causal intervention) and the 
original caption. For each attribute, we i nd that the causal states are distributed across the UNet and 
the distribution varies amongst distinct attributes. For e.g., self-attn in the i rst layer is causal for 
style, but not for objects, action or color. Similarly, mid-block cross-attn is causal for action, but not 
for the other attributes. On the right-side, we visualize the images generated by (i) Original model; 
(ii) Corrupted Model; (iii) Restored causal states and (iv) Restored non-causal states in the UNet for 
style, action, object, color attributes. 
of the time-steps. In particular, at each time-step, classif i er-free guidance is used in the following 
way to combine the conditional (ÏµÎ¸(zt,c,t)) and unconditional score estimates (ÏµÎ¸(zt,t)) at each 
time-step t to obtain the combined score denoted as Ë Ïµ(zt,c,t): 
Ë ÏµÎ¸(zt,c,t) = ÏµÎ¸(zt,c,t) + Î±(ÏµÎ¸(zt,c,t) â ÏµÎ¸(zt,t)),ât â [T,1].(3) 
This combined score is used to update the latent ztusing DDIM sampling (Song et al., 2020) at each 
time-step iteratively to obtain the i nal latent code z0. 
To perform causal tracing on the UNet ÏµÎ¸(see Fig 1 for visualization), we perform a sequence of 
operations that is somewhat analogous to earlier work from (Meng et al., 2023) which investigated 
knowledge-tracing in large language models. We consider three types of model conf i gurations: (i) a 
clean model ÏµÎ¸, where classif i er-free guidance is used as default; (ii) a corrupted model Ïµcorr 
Î¸ , where 
the word embedding of the subject (e.g., Van Gogh) of a given attribute (e.g., style) corresponding to 
a caption c is corrupted with Gaussian Noise; and, (iii) a restored model Ïµrestored 
Î¸ , which is similar to 
Ïµcorr 
Î¸ except that one of its layers is restored from the clean model at each time-step of the classif i er- 
free guidance. Given a list of layers A, let aiâ A denote the ithlayer 
whose importance needs to 
be evaluated. Let ÏµÎ¸[ai], Ïµcorr 
Î¸ [ai] and Ïµrestored 
Î¸ [ai] denote the activations of layer ai. To i nd the 
importance of layer aifor a particular attribute embedded in a caption c, we perform the following 
replacement operation on the corrupted model Ïµcorr 
Î¸ to obtain the restored model Ïµrestored 
Î¸ : 
Ïµrestored 
Î¸ [ai] : Ïµcorr 
Î¸ [ai] = ÏµÎ¸[ai].(4) 
Next, we obtain the restored model by replacing the activations of layer aiof the corrupted model 
with those of the clean model to get a restored layer Ïµrestored 
Î¸ [ai]. We run classif i er-free guidance to 
obtain the combined score estimate: 
Ë Ïµrestored 
Î¸ (zt,c,t) = Ïµrestored 
Î¸ (zt,c,t)+Î±(Ïµrestored 
Î¸ (zt,c,t)âÏµrestored 
Î¸ (zt,t)),ât â [T,1]. (5) 
The i nal latent z0is obtained with the score from Equation (5) at each time-step using DDIM (Song 
et al., 2020) and passed through the VQ-VAE decoder to obtain the final image xrestored 
0ChatGPTLet's break down section 3.3 on "Tracing Knowledge in UNet":3.3 TRACING KNOWLEDGE IN UNETUsing Classifier-Free Guidance:During image generation, a method called "classifier-free guidance" aids in combining the scores of the model's conditioned and unconditioned outputs over time.The combined score, represented by ÏµÎ¸^(zt,c,t)\hat{\epsilon_\theta}(z_t,c,t)ÏµÎ¸â^â(ztâ,c,t), is crucial to iteratively update the latent code ztz_tztâ using DDIM sampling, resulting in the final latent code z0z_0z0â.Visualizing the UNet's Knowledge:Knowledge tracing in the UNet is achieved using a methodology similar to previous works, like the one by Meng et al., 2023.Three types of model configurations are considered:Clean model ÏµÎ¸\epsilon_\thetaÏµÎ¸â: Uses classifier-free guidance by default.Corrupted model ÏµcorrÎ¸\epsilon_{corr\theta}ÏµcorrÎ¸â: The word embedding of the attribute in the caption (like "Van Gogh" for "style") is distorted using Gaussian Noise.Restored model ÏµrestoredÎ¸\epsilon_{restored\theta}ÏµrestoredÎ¸â: Resembles the corrupted model, but a layer is restored from the clean model during each time step.Determining Layer Importance:To gauge the significance of a specific layer aia_iaiâ for an attribute in a caption, a replacement operation is executed on the corrupted model to create the restored model.The activations of the layer in the corrupted model are swapped with the ones from the clean model.Classifier-free guidance then calculates the combined score estimate, ÏµrestoredÎ¸^(zt,c,t)\hat{\epsilon_{restored\theta}}(z_t,c,t)ÏµrestoredÎ¸â^â(ztâ,c,t).Finally, using DDIM, the last latent z0z_0z0â is derived using the score from this combined estimate and decoded using VQ-VAE to yield the restored image xrestored0x_{restored0}xrestored0â.This section provides insight into how the model's knowledge is traced within the UNet, and how layer importance in knowledge attribution is determined for attributes present in the captions.Figure 3:Causal Tracing in the Text-Encoder: Knowledge is Localized. In the CLIP text- 
encoder used for Stable-Diffusion, we i nd the existence of only one causal state, which is the i rst 
self-attention layer corresponding to the last subject token. The CLIP-Score(Left) is computed 
across all the four visual attributes. Visualizations (Right) further illustrate that restoring the sole 
causal state (self-attn-0) leads to image generation with high i delity to the original captions. 
3.4TRACINGKNOWLEDGE IN THETEXT-ENCODER 
The text-encoder in public text-to-image models such as Stable-Diffusion is a CLIP-ViT-L/336px 
text-encoder Rombach et al. (2021). Similar to Sec.(3.3), we def i ne three states of the CLIP text- 
encoder: (i) Clean model denoted by vÎ³; (ii) Corrupted model vcorr 
Î³ where the word embedding of 
the subject in a given caption c is corrupted; (iii) Restored model vrestored 
Î³ which is similar to vcorr 
Î³ 
except that one of its layers is copied from vÎ³. Similar to Sec.(3.3), to i nd the effect of the layer 
aiâ A, where A consists of all the layers to probe in the CLIP text-encoder: 
vrestored 
Î³ [ai] : vcorr 
Î³ [ai] = vÎ³[ai],(6) 
We then use the restored text-encoder vrestored 
Î³ with classif i er-free guidance to obtain the i nal score 
estimate: Ë ÏµÎ¸(zt,câ²,t) = ÏµÎ¸(zt,câ²,t) + Î±(ÏµÎ¸(zt,câ²,t) â ÏµÎ¸(zt,t)),ât â [T,1](7) 
where câ²= vrestored 
Î³ [ai](c) for a given caption c. This score estimate Ë ÏµÎ¸(zt,câ²,t) at each time-step 
t is used to obtain the i nal latent code z0which is then used with the VQ-VAE decoder to obtain 
the i nal image xrestored 
0 . 
3.5EXTRACTINGCAUSALSTATESUSINGCLIP-SCORE 
In this section, we discuss details on how to retrieve causal states using automated metrics such as 
CLIP-Score (Hessel et al., 2021). Let xrestored 
0 (ai) be the i nal image generated by the diffusion 
model after intervening on layer ai, x0be the image generated by the clean diffusion model and 
xcorrbe the i nal image generated by the corrupted model. In particular, we are interested in the 
average indirect effect (Vig et al., 2020; Pearl, 2013) which measures the difference between the 
corrupted model and the restored model. Intuitively, a higher value of average indirect effect (AIE) 
signif i es that the restored model deviates from the corrupted model. To compute the average indirect 
effect with respect to causal mediation analysis for text-to-image models such as Stable-Diffusion, 
we use CLIP-Score which computes the similarity between an image embedding and a caption 
embedding. In particular, AIE = |CLIPScore(xrestored 
0 ,c)âCLIPScore(xcorr 
0 ,c)|. Given xcorr 
0 
is common across all the layers for a caption, we can use CLIPScore(xrestored 
0 ,c) as the AIE. 
Selecting Threshold for CLIP-Score. In order to determine the optimal threshold value for 
CLIP-Score, we select a small validation set of 10 prompts per attribute. To this end, we establish 
a concise user study interface (refer to Appendix D for details). Through human participation, we 
collect binary ratings if an image generated by restoring a particular layer is faithful to the original 
captions. We then extract the common causal states across all the prompts for a given attribute 
and i nd the average (across all the prompts) CLIP-Score for each causal state. We then use the 
lowest average CLIP-Score corresponding to a causal state as the threshold, which we apply on 
the probe dataset in Appendix A to i lter the causal states at scale for each attribute separately.ChatGPTLet's break down the provided information:Figure 3: Causal Tracing in the Text-EncoderThe figure suggests that, within the CLIP text-encoder used for Stable-Diffusion, knowledge is localized primarily in one causal state: the first self-attention layer that relates to the last subject token.The CLIP-Score is computed across four visual attributes.Visualization shows that restoring this main causal state (self-attn-0) allows for images that align closely with the original captions.3.4 TRACING KNOWLEDGE IN THE TEXT-ENCODERThe text-encoder in models like Stable-Diffusion is known as CLIP-ViT-L/336px text-encoder.Three configurations of the CLIP text-encoder are defined:Clean model vÎ³v_Î³vÎ³â.Corrupted model vcorrÎ³v_{corrÎ³}vcorrÎ³â: The word embedding of the subject in a caption is distorted.Restored model vrestoredÎ³v_{restoredÎ³}vrestoredÎ³â: Similar to the corrupted model, but a layer is replaced with the one from the clean model.To determine the effect of layer aia_iaiâ in the CLIP text-encoder, a replacement operation is used similar to the one in section 3.3.The restored text-encoder is used with classifier-free guidance to produce the final score estimate. This score determines the final latent code, which, in turn, dictates the resulting image through the VQ-VAE decoder.3.5 EXTRACTING CAUSAL STATES USING CLIP-SCOREThis section elaborates on using the CLIP-Score to determine causal states.The final image generated after intervening on layer aia_iaiâ is represented as xrestored0(ai)x_{restored0}(a_i)xrestored0â(aiâ). The average indirect effect (AIE) gauges the difference between images produced by the corrupted and restored models. A higher AIE indicates more deviation from the corrupted model.To perform causal mediation analysis for text-to-image models, the CLIP-Score measures the likeness between an image embedding and a caption embedding. Specifically, AIE=â£CLIPScore(xrestored0,c)âCLIPScore(xcorr0,c)â£AIE = |CLIPScore(x_{restored0},c)âCLIPScore(x_{corr0},c)|AIE=â£CLIPScore(xrestored0â,c)âCLIPScore(xcorr0â,c)â£. Since xcorr0x_{corr0}xcorr0â remains constant for a caption, CLIPScore(xrestored0,c)CLIPScore(x_{restored0},c)CLIPScore(xrestored0â,c) can be used to determine the AIE.Selecting the CLIP-Score Threshold:To select the optimal threshold for the CLIP-Score, a validation set of 10 prompts per attribute is chosen.A user study gathers binary ratings on whether an image generated by restoring a layer aligns with the original captions.The average CLIP-Score is determined for each causal state, and the lowest average score for a causal state is selected as the threshold.This information presents a deep dive into the methodology of tracing knowledge within the text-encoder of Stable-Diffusion models and extracting causal states using the CLIP-Score metric.4HOW ISKNOWLEDGESTORED INTEXT-TO-IMAGEMODELS? 
In this section, we discuss the results of tracing knowledge across various components of the text- 
to-image model in details. 6 
Preprint 
Tracing Results for UNet. In Fig 2, we illustrate the distribution of causal states across different 
visual attributes within the UNet architecture using the CLIP-Score metric. This metric evaluates 
the faithfulness of the image produced by the restored state xrestored 
0 compared to the original cap- 
tion c. From the insights derived in Fig 2, it becomes evident that causal states are spread across 
diverse components of the UNet. In particular, we i nd that the density of the causal states are more 
in the up-block of the UNet when compared to the down-block or the mid-block. Nonethe- 
less, a notable distinction emerges in this distribution across distinct attributes. For instance, when 
examining the style attribute, the initial self-attention layer demonstrates causality, whereas this 
causal relationship is absent for other attributes. Similarly, in the context of the action attribute, 
the cross-attention layer within the mid-block exhibits causality, which contrasts with its non-causal 
behavior concerning other visual attributes.Fig 2 showcases the images generated by restoring 
both causal and non-causal layers within the UNet. A comprehensive qualitative enumeration of 
both causal and non-causal layers for each visual attribute is provided in Appendix B. Our i ndings 
underscore the presence of information pertaining to various visual attributes in regions beyond the 
cross-attention layers. Importantly, we observe that the distribution of information within the UNet 
diverges from the patterns identif i ed in extensive generative language models, as noted in prior re- 
search (Meng et al., 2023), where attribute-related knowledge is conf i ned to a few proximate layers. 
In Appendix M, we provide additional causal tracing results, where we add Gaussian noise to the 
entire text-embedding. Even in such a case, certain causal states can restore the model close to its 
original conf i guration, highlighting that the conditional information can be completely bypassed if 
certain causal states are active. 
Tracing Results for Text-Encoder. In Fig 3, we illustrate the causal states in the text-encoder for 
Stable-Diffusion corresponding to various visual attributes. At the text-encoder level, we i nd that 
the causal states are localized to the i rst self-attention layer corresponding to the last subject token 
across all the attributes. In fact, there exists only one causal state in the text-encoder. Qualitative 
visualizations in Fig 3 and Appendix C illustrate that the restoration of layers other than the i rst 
self-attention layer corresponding to the subject token does not lead to images with high i delity 
to the original caption. Remarkably, this observation is distinct from generative language models 
where factual knowledge is primarily localized in the proximate mid MLP layers Meng et al. (2023). 
General Takeaway. Causal components corresponding to various visual attributes are dis- 
persed (with a different distribution between distinct attributes) in the UNet, whereas there 
exists only one causal component in the text-encoder. 
The text-encoderâs strong localization of causal states for visual attributes enables controlled knowl- 
edge manipulation in text-to-image models, facilitating updates or removal of concepts. However, 
since attribute knowledge is dispersed in the UNet, targeted editing is challenging without layer 
interference. While i ne-tuning methods for UNet model editing exist (Gandikota et al., 2023a; Ku- 
mari et al., 2023), they lack scalability and donât support simultaneous editing of multiple concepts. 
In the next section, we introduce a closed-form editing method, DIFF-QUICKFIX, leveraging our 
causal tracing insights to eff i ciently edit various concepts in text-to-image models.ChatGPTThis section delves into the research findings about how knowledge is stored in text-to-image models. Here's a breakdown:4. HOW IS KNOWLEDGE STORED IN TEXT-TO-IMAGE MODELS?Tracing Results for UNet:Fig 2 displays the distribution of causal states across different visual attributes within the UNet architecture, using the CLIP-Score metric.The metric evaluates the congruence between the restored image and the original caption.Causal states appear spread across various components of the UNet. There's a higher density of causal states in the up-block compared to the down-block or mid-block.Distinct attributes exhibit different causal state distributions:The style attribute demonstrates causality in the initial self-attention layer, but this isn't the case for other attributes.For the action attribute, the mid-block's cross-attention layer shows causality. This contrasts with its non-causal behavior for other visual attributes.The study reveals that knowledge related to various visual attributes exists beyond just the cross-attention layers. This distribution deviates from patterns in extensive generative language models, where attribute knowledge is often limited to a few nearby layers.Tracing Results for Text-Encoder:Fig 3 details the causal states in the text-encoder of the Stable-Diffusion model corresponding to different visual attributes.Only one causal state is identified in the text-encoder: the first self-attention layer associated with the last subject token.The restoration of layers other than this sole causal state doesn't produce images closely matching the original caption.This localization of factual knowledge differs from generative language models where it's usually found in the proximate mid MLP layers.General Takeaway:Knowledge related to various visual attributes is dispersed in the UNet, with different distributions based on the attribute. Meanwhile, the text-encoder has a strong localization of its causal states.The text-encoder's distinct localization aids in controlled knowledge manipulation in text-to-image models. It allows for the addition or removal of concepts. However, due to the dispersed knowledge in the UNet, pinpointed editing becomes a challenge without affecting multiple layers.Existing fine-tuning methods for UNet editing are available but have limitations in scalability and simultaneous multi-concept editing.The subsequent section introduces a new editing method named DIFF-QUICKFIX, which capitalizes on the study's findings for efficient concept edits in text-to-image models.In essence, the research showcases distinct knowledge storage patterns in the UNet and text-encoder, highlighting their implications and offering a new editing solution.5DIFF-QUICKFIX: FASTMODELEDITING FORTEXT-TO-IMAGEMODELS 
5.1EDITINGMETHOD 
Recent works such as (Kumari et al., 2023; Gandikota et al., 2023a) edit concepts from text-to-image 
diffusion models by i ne-tuning the UNet. They generate training data for i ne-tuning using the pre- 
trained diffusion model itself. While both methods are effective at editing concepts, i ne-tuning the 
UNet can be expensive due to backpropogation of gradients through the UNet. To circumvent this 
issue, we design a fast, data-free model editing method leveraging our interpretability observations 
in Section 4, where we i nd that there exists only one causal state (the very i rst self-attention layer) 
in the text-encoder for Stable-Diffusion. 
Our editing method DIFF-QUICKFIXcan update text-to-image diffusion models in a targeted way in 
under 1s through a closed-form update making it 1000x faster than existing i ne-tuning based con- 
cept ablating methods such as (Kumari et al., 2023; Gandikota et al., 2023a). The i rst self-attention 
layer in the text-encoder for Stable-Diffusion contains four updatable weight matrices: Wk,Wq,Wv 
and Wout, where Wk,Wq,Wvare the projection matrices for the key, query and value embeddings 
respectively. Woutis the projection matrix before the output from the self-attn-0 layer after 
7 
Preprint 
Causal Layer (a)(b) 
Editing Causal Layers vs. Non-Causal Layers Comparison with Other Methods 
1s~6min ~6.5min 1s~6min ~6.5min 
Editing time 
per concept 
Effectiveness of Multi-Concept Ablated Model 
(c) 
Figure 4: Quantitative Analysis of DIFF-QUICKFIX. (a) Editing Causal vs. Non-Causal Layers 
(Averaged across Objects, Style and Facts): Lower CLIP-Score for causal layer indicates suc- 
cessful edits; (b) Eff i cacy of DIFF-QUICKFIXwhen compared to other methods â Our method leads 
to comparable CLIP-Scores to i ne-tuning based approaches, but can edit concepts 1000x faster; 
(c) DIFF-QUICKFIXcan be used to effectively edit multiple concepts at once, shown by comparable 
CLIP-Scores to the single-concept edited ones. 
the attention operations. DIFF-QUICKFIXspecif i cally updates this Woutmatrix by collecting cap- 
tion pairs (ck,cv) where ck(key) is the original caption and cv(value) is the caption to which ckis 
mapped. For e.g., to remove the style of âVan Goghâ, we set ck= âVan Goghâ and cv= âPaintingâ. 
In particular, to update Wout, we solve the following optimization problem: 
min 
Wout 
N X 
i=1 â¥Woutkiâ viâ¥2 2+ Î»â¥Woutâ W 
â² outâ¥ 
2 2, (8) 
where Î» is a regularizer to not deviate signif i cantly from the original pre-trained weights Wâ² out, N 
denotes the total number of caption pairs containing the last subject token embeddings of the key 
and value. kicorresponds to the embedding of ckiafter the attention operation using Wq,Wkand 
Wvfor the ithcaption pair. vicorresponds to the embedding of cviafter the original pre-trained 
weights W 
â² outacts on it. 
One can observe that Eq. (8) has a closed-form solution due to the absence of any non-linearities. 
In particular, the optimal Woutcan be expressed as the following: 
Wout= (Î»Wâ² out+ 
N X 
i=1 
vikT 
i )(Î»I + 
N X 
i=1 
kikT 
i )â1,(9) 
In Section 5.3, we show qualitative as well as quantitative results using DIFF-QUICKFIXfor editing 
various concepts in text-to-image models.ChatGPTThis section introduces a new editing method for text-to-image models called DIFF-QUICKFIX. Let's break it down:5. DIFF-QUICKFIX: FAST MODEL EDITING FOR TEXT-TO-IMAGE MODELS5.1 Editing Method:Background: Recent methods, such as those by Kumari et al., 2023 and Gandikota et al., 2023a, edit concepts in text-to-image diffusion models by fine-tuning the UNet. Although effective, fine-tuning can be computationally expensive.Solution: Drawing inspiration from the findings in Section 4, the DIFF-QUICKFIX method was designed to provide a swift, data-free model editing solution.Key Characteristics of DIFF-QUICKFIX:It focuses on the one causal state identified in the text-encoder for Stable-Diffusion, specifically the first self-attention layer.It achieves updates in under 1 second, making it about 1000 times faster than existing methods.This layer has four weight matrices (Wk, Wq, Wv, and Wout) for the key, query, value embeddings, and the output projection respectively.The Wout matrix is updated by collecting caption pairs, with one being the original caption and the other being its desired mapping. For instance, to change the style of âVan Goghâ to a generic âPaintingâ, they'd use âVan Goghâ as the key and âPaintingâ as the value.Optimization Process:An optimization problem is formulated to update the Wout matrix, which aims to minimize the difference between the current and desired embeddings, with regularization to keep the updated weights close to the original pre-trained ones.Given the linearity of this problem, there's a closed-form solution, allowing for a swift calculation of the optimal Wout.Figure 4: Quantitative Analysis of DIFF-QUICKFIXThis figure visually represents the efficiency and effectiveness of DIFF-QUICKFIX:(a) Showcases how editing causal layers results in successful edits, as indicated by a lower CLIP-Score.(b) Demonstrates that DIFF-QUICKFIX achieves comparable performance to fine-tuning methods but at a speed 1000x faster.(c) Illustrates that DIFF-QUICKFIX can edit multiple concepts simultaneously, with the performance being on par with editing single concepts.Conclusion:
The DIFF-QUICKFIX method leverages the single causal state identified in the text-encoder for Stable-Diffusion to offer a fast and efficient editing solution for text-to-image models. Its key advantage is its speed, outperforming other methods while maintaining high-quality edits.The main idea is to utilize the identified causal components from previous research and focus the editing on them, resulting in a faster and more efficient method.5.2EXPERIMENTALSETUP 
We validate DIFF-QUICKFIXby applying edits to a Stable-Diffusion (Rombach et al., 2021) model 
and quantifying the eff i cacy of the edit. For removing concepts such as artistic styles or objects us- 
ing DIFF-QUICKFIX, weusethepromptdatasetfrom(Kumarietal.,2023). Forupdatingknowledge 
(e.g., President of a country) in text-to-image models, we add newer prompts to the prompt dataset 
from (Kumari et al., 2023) and provide further details in Appendix N. We compare our method 
with (i) Original Stable-Diffusion; (ii) Editing methods from (Kumari et al., 2023) and (Gandikota 
et al., 2023a). To validate the effectiveness of editing methods including our DIFF-QUICKFIX, we 
perform evaluation using automated metrics such as CLIP-Score. In particular, we compute the 
CLIP-Score between the images from the edited model and the concept corresponding to the 
visual attribute which is edited. A low CLIP-Score therefore indicates correct edits. 
5.3EDITINGRESULTS 
Editing Non-causal Layers Does Not Lead to Correct Edits.We use DIFF-QUICKFIXwith 
the non-causal self-attention layers in the text-encoder to ablate styles, objects and update facts. 
In Fig 4-(a), we compute the CLIP-Score between the generated images and the attribute from 
the original captions (e.g., van gogh in the case of style). In particular, we i nd that editing the non- 
causal layers does not lead to any intended model changes â highlighted by the high CLIP-Scores 
consistently across non-causal layers (layers numbered 1 to 11). However, editing the sole causal 
layer (layer-0) leads to correct model changes, highlighted by the lower CLIP-Score between the 
8 
Preprint 
Before EditAfter EditBefore EditAfter EditBefore EditAfter Edit 
Removing SnoopyRemoving Van Gogh Style 
Removing R2D2Removing Monet Style 
Updating the President of US 
Removing NemoRemoving Grumpy Cat 
Updating the British Monarch 
Before EditAfter Edit 
Figure 5: Qualitative Examples with using DIFF-QUICKFIXto ablate style, objects and update 
facts in text-to-image models. More qualitative examples in the Appendix F. 
generated images from the edited model and the attribute from the original captions. This shows that 
identifying the causal states in the model is particularly important to perform targeted model editing 
for ablating concepts. In Appendix G, we show additional qualitative visualizations highlighting 
that editing the non-causal states lead to similar model outputs as the unedited model. 
Eff i cacy in Removing Styles and Objects.Fig 4-(b) shows the average CLIP-Score of the 
generated images from the edited model computed with the relevant attributes from the original cap- 
tions. We i nd that the CLIP-Score from the edited model with DIFF-QUICKFIXdecreases when 
compared to the generations from the unedited model. We also i nd that our editing method has com- 
parable CLIP-Scores to other i ne-tuning based approaches such as Concept-Erase (Gandikota 
et al., 2023a) and Concept-Ablation (Kumari et al., 2023), which are more computationally expen- 
sive. Fig 5 shows qualitative visualizations corresponding to images generated by the text-to-image 
model before and after the edit operations. Together, these quantitative and qualitative results show 
that DIFF-QUICKFIXis able to effectively remove various styles and objects from an underlying 
text-to-image model. In Appendix F we provide additional qualitative visualizations and in Fig 52 
we show additional results showing that our editing method does not harm surrounding concepts 
(For e.g., removing the style of Van Gogh does not harm the style of Monet). 
Eff i cacy in Updating Stale Knowledge. The CLIP-Score between the generated images and 
a caption designating the incorrect fact (e.g., Donald Trump as the President of the US) decreases 
from 0.28 to 0.23 after editing with DIFF-QUICKFIX, while the CLIP-Score with the correct fact 
(e.g., Joe Biden as the President of the US) increases from 0.22 to 0.29 after the relevant edit. This 
shows that the incorrect fact is updated with the correct fact in the text-to-image model. Additional 
qualitative visualizations are provided in Fig 5 and Appendix F. 
Multiple Edits using DIFF-QUICKFIX. An important feature of DIFF-QUICKFIXis its capa- 
bility to ablate multiple concepts simultaneously. In Fig 4-(c), our framework demonstrates the 
removal of up to 10 distinct styles and objects at once. This multi-concept ablation results in lower 
CLIP-Scores compared to the original model, similar CLIP-Scores to single concept edit- 
ing. This scalability suggests our frameworkâs potential for large-scale multi-concept editing. In 
Appendix H, we provide qualitative visualizations of generations from the multi-concept ablated 
model, showcasing the effectiveness of our editing method in removing multiple concepts. Addi- 
tionally, we highlight DIFF-QUICKFIXâs eff i ciency in eliminating a larger number of artistic styles, 
successfully removing 50 top artistic styles from Stable-Diffusion.ChatGPTLet's summarize the key points from the provided sections on DIFF-QUICKFIX:5.2 EXPERIMENTAL SETUPThe DIFF-QUICKFIX method was validated by applying edits to the Stable-Diffusion model.Used the prompt dataset from Kumari et al., 2023 for editing concepts such as artistic styles or objects.The method's efficacy was compared against:Original Stable-DiffusionEditing methods from Kumari et al., 2023Editing methods from Gandikota et al., 2023aThe effectiveness of the edits was quantified using the CLIP-Score metric. A lower CLIP-Score signifies successful edits.5.3 EDITING RESULTSEditing Non-causal Layers:Attempted edits on non-causal self-attention layers didn't lead to intended model changes.Editing the causal layer (layer-0) led to successful changes, as shown by lower CLIP-Scores.Efficiency in Removing Styles and Objects:DIFF-QUICKFIX resulted in reduced CLIP-Scores when compared to the unedited model, implying successful edits.The results from DIFF-QUICKFIX were on par with other computationally expensive fine-tuning methods.Figure 5 provided visual examples of pre and post-edit images.Efficacy in Updating Stale Knowledge:Using DIFF-QUICKFIX, the CLIP-Score for incorrect facts decreased, while the score for correct facts increased, indicating successful updates of outdated knowledge.Multiple Edits with DIFF-QUICKFIX:The method demonstrated the ability to simultaneously remove multiple concepts, up to 10 distinct styles and objects.The results from multi-concept ablation showed similar CLIP-Scores to single concept edits, suggesting the method's scalability and potential for large-scale editing.Overall, the provided sections detail the experimental setup and results for DIFF-QUICKFIX. The method proves to be efficient in editing text-to-image models, offering a faster alternative to existing methods while maintaining high-quality edits.Through the lens of Causal Mediation Analysis, we present methods for understanding the storage of 
knowledge corresponding to diverse visual attributes in text-to-image diffusion models. Notably, we 
i nd a distinct distribution of causal states across visual attributes in the UNet, while the text-encoder 
maintains a single causal state. This differs signif i cantly from observations in language models like 
GPT, where factual information is concentrated in mid-MLP layers. In contrast, our analysis shows 
that public text-to-image models like Stable-Diffusion concentrate multiple visual attributes within 
the i rst self-attention layer of the text-encoder. Harnessing the insights from these observations, 
we design a fast model editing method DIFF-QUICKFIX. This approach outpaces existing editing 
methods by a factor of 1000, successfully ablating concepts from text-to-image models. The potency of DIFF-QUICKFIXis manifested through its adeptness in removing artistic styles, objects, and 
updating outdated knowledge all accomplished data-free and in less than a second, making DIFF- 
QUICKFIXa practical asset for real-world model editing scenarios.ChatGPTTo summarize:Using Causal Mediation Analysis, the study reveals insights into how text-to-image diffusion models store knowledge related to various visual attributes. Key findings include:Distribution of Causal States:There's a distinct distribution of causal states for visual attributes in the UNet.The text-encoder contains only one causal state.This contrasts with language models like GPT, where factual information predominantly resides in mid-MLP layers.Concentration of Visual Attributes:Text-to-image models, such as Stable-Diffusion, store multiple visual attributes primarily within the first self-attention layer of the text-encoder.DIFF-QUICKFIX:Based on the findings, the DIFF-QUICKFIX method was developed.It's significantly faster than other editing methods, being 1000 times quicker.The method can efficiently remove artistic styles, objects, and update outdated information.All these edits are achieved without requiring any data and are done in under one second, showcasing its potential utility for real-world editing tasks.Overall, the study offers deep insights into the storage mechanisms of visual attributes in text-to-image models and presents a highly efficient method for editing these models.The title of the article we just summarized was LOCALIZING ANDEDITINGKNOWLEDGE IN 
TEXT-TO-IMAGEGENERATIVEMODELSChatGPTThank you for providing the title. Here's a concise summary of the article titled "LOCALIZING AND EDITING KNOWLEDGE IN TEXT-TO-IMAGE GENERATIVE MODELS":In "LOCALIZING AND EDITING KNOWLEDGE IN TEXT-TO-IMAGE GENERATIVE MODELS", researchers use Causal Mediation Analysis to understand how text-to-image diffusion models store diverse visual attributes. The study finds a unique distribution of causal states in the UNet, with the text-encoder having just one causal state. This is distinct from models like GPT where facts are mainly in mid-MLP layers. Specifically, models like Stable-Diffusion store multiple visual attributes in the first self-attention layer of the text-encoder.Capitalizing on these findings, the article introduces DIFF-QUICKFIX, a fast model editing method. It outperforms existing methods by being 1000 times faster, efficiently removing artistic styles, objects, and updating outdated knowledge in less than a second without needing any data. This positions DIFF-QUICKFIX as a valuable tool for practical, real-world model editing scenarios.I thought it had some connections to Introduction: Geometric Bayesianism

Geometric Bayesianism represents a way of thinking where probabilities, uncertainties, and beliefs are structured in a geometric framework. This approach combines Bayesian probability theory with geometric principles to describe the complex phenomena in various domains.

Future of the Professions (Richard and Daniel Susskind)

The Susskinds discuss how technology is reshaping professional work, providing a lens through which we can explore Geometric Bayesianism's role in predicting future trends. The geometric structure provides an analytical tool for envisioning how industries may evolve.

Indistractable (Nir Eyal)

Nir Eyal's idea of managing distraction in our tech-saturated world can be mapped onto Geometric Bayesianism as a structure to understand human focus and decision-making. It explores how we navigate information overload.

The Age of Spiritual Machines (Ray Kurzweil)

Kurzweil's work on the merger between humans and machines brings geometric thinking into the future of AI, where Bayesian frameworks model the learning process of machines. It's an approach that mirrors human intuition and reasoning.

The Master Algorithm (Pedro Domingos)

Pedro Domingos's work is concerned with a universal learning algorithm. Geometric Bayesianism may represent a unified theory that could guide the future of machine learning and AI, tying together various strands of algorithmic theory.

Who Can You Trust? (Rachel Botsman)

In an age where trust is a currency, Geometric Bayesianism can model trust dynamics in human networks. Botsman's insights into trust provide a unique perspective on how geometric structures can represent complex human relationships.

Move Fast and Break Things (Jonathan Taplin)

This theme is about the disruptive nature of tech companies, where Geometric Bayesianism could serve as a model to understand the unpredictable and fast-changing tech landscape.

Abundance (Diamandis and Kotler)

A vision of a future where technology provides abundance for all aligns with the predictive power of Geometric Bayesianism to forecast trends and opportunities.

The Signal and the Noise (Nate Silver)

Silver's work on predictions and statistics underscores the importance of Bayesian reasoning in filtering meaningful information from noise. Geometric Bayesianism provides the framework to understand complex data landscapes.

How to Speak Machine (John Maeda)

The intersection between humans and machines can be described geometrically, where Bayesian principles apply to human-machine communication, as outlined by Maeda.

To Be a Machine (Mark O'Connell)

O'Connell's exploration of transhumanism and the desire to transcend biological limitations can be framed within Geometric Bayesianism as a model for understanding the evolution of human nature.

The Human Swarm (Superstition), The Way Back (Pseudoscience), Company of One (Tautological Fallacies), A Peace to End All Peace (Brainwashing)

These themes touch on human belief systems, fallacies, and manipulation. Geometric Bayesianism can model these as spatial representations of logic, beliefs, and influence in the social sphere.

Conclusion

Geometric Bayesianism serves as a unifying concept, bridging disparate ideas ranging from technology and human interaction to superstition and pseudoscience. By applying a geometric and Bayesian perspective, we can see the world in a more structured and interconnected way, appreciating the subtle nuances that govern our complex reality. It provides a lens through which to view the future of professions, human-machine interaction, trust, abundance, and human nature, among other critical aspects of our world.

ï¿¼

More context: On Bayesian Geometry Geometric interpretation of probability distributions Michael Larionov, PhD . Follow Published in Towards Data Science 4 min read . Dec 19,2021 https://towardsdatascience.com/on-bayesian-geometry-7755abf9f4d2 Bayesian inference is based on the fact that we often don't know the underlying distribution of data, so we need to build a model and then iteratively adjust it as we get more data. In parametric Bayesian inference you start with picking a general form of the probability distribution f(x;0) defined by parameters 0. A good example of the distribution could be a Normal distribution with two parameters u and 0^2. The probability of the data under a hypothetical distribution, assuming independent data examples, is: (lÎ)=Î  with a little i under it, f of (x sub i;Î)

ChatGPT

Certainly! With the added context, we can delve deeper into the geometric interpretation of Bayesian inference, particularly in the realm of parametric Bayesian inference. Let's add a section specifically focusing on this aspect.

Geometric Interpretation of Probability Distributions (Inspired by Michael Larionov)

Bayesian inference operates on the idea that the underlying distribution of data is unknown, thus requiring iterative model adjustment as new data is acquired. This can be seen as a geometric process, where the distribution and its parameters shape the landscape in which probability is explored.

In parametric Bayesian inference, we start with a chosen form of a probability distribution ï¿½(ï¿½;Î)f(x;Î) defined by parameters ÎÎ. A common example is the Normal distribution with two parameters ï¿½Î¼ (mean) and ï¿½2Ï2 (variance). The likelihood of the data given a hypothetical distribution, assuming independent data examples, is represented as: ï¿½(Î)=âï¿½ï¿½(ï¿½ï¿½;Î)l(Î)=âiâf(xiâ;Î)

This mathematical expression can be visualized geometrically as a multidimensional landscape where each point represents a possible configuration of the parameters ÎÎ, and the elevation represents the likelihood of the data given those parameters.

The Geometric Landscape: The likelihood function forms a surface in a multidimensional space. Peaks in this landscape correspond to parameter values that are highly likely given the observed data.

Iterative Adjustment: As new data comes in, the landscape shifts, and the peak of likelihood may move. This is akin to navigating a changing terrain, adjusting your position as the landscape reveals itself.

Prior Beliefs: Bayesian inference incorporates prior beliefs about the parameters, which can be seen as a pre-existing shape of the landscape. The prior belief about the distribution influences the starting point and direction of the search for the most likely parameters.

Posterior Distribution: After incorporating the data, the posterior distribution reflects the updated landscape. It is a synthesis of the prior beliefs and the likelihood informed by the data, resulting in a new geometric shape that encapsulates our updated understanding.

Connection to Other Themes: The geometric interpretation of Bayesian probability ties together many of the themes mentioned earlier in the essay, providing a mathematical and visual framework for understanding complex human and technological phenomena.

Conclusion (Revised)

The geometric interpretation of Bayesian probability adds a tangible and intuitive layer to our understanding of uncertainty, decision-making, learning, and adaptation. By visualizing the intricate process of Bayesian inference as a dynamically evolving landscape, we can draw parallels across various domains and see connections between human intuition, machine learning, professional evolution, and even societal belief systems. Geometric Bayesianism serves as an elegant unifying concept that brings coherence to a complex and interconnected world.

This function is called likelihood function. The parameter 0 is itself a random variable, and its probability distribution can be found using Îayes Ïheorem: P(Î¸)= l(Î¸)Ï(Î¸)/Îµ Here p(Î¸) is called posterior distribution, Ï(Î¸) is prior distribution and expresses our beliefs about parameter Î¸ before we see any data. The term in the denominator (Îµ) is called evidence and represents probability of thedata. It is easier to think of it as a normalizing constant, to make sure p(Î¸) is a valid distribution

ChatGPT

Certainly! The additional information provided clarifies the key components of Bayesian inference. Let's expand the existing section on the geometric interpretation of probability distributions with these details.

Geometric Interpretation of Probability Distributions: Bayesian Components

In Bayesian inference, the likelihood function is central to understanding how data informs our beliefs about underlying parameters. As previously expressed, the likelihood of a given set of parameters ÎÎ is: ï¿½(Î)=âï¿½ï¿½(ï¿½ï¿½;Î)l(Î)=âiâf(xiâ;Î)

This function provides a link between the hypothetical distribution and the observed data. To further understand the Bayesian components:

Posterior Distribution (p(Î¸)): The posterior distribution represents our updated beliefs about the parameters after observing the data. It's the product of the likelihood and the prior distribution, normalized by the evidence: ï¿½(Î)=ï¿½(Î)ï¿½(Î)ï¿½p(Î)=Ïµl(Î)Ï(Î)â

Prior Distribution (Ï(Î¸)): This is our belief about the parameter ÎÎ before we see any data. It reflects prior knowledge or assumptions and sets the initial landscape for the likelihood function.

Evidence (Îµ): The term in the denominator, called evidence, represents the probability of the data. It serves as a normalizing constant to ensure that the posterior distribution is a valid probability distribution. Geometrically, it ensures that the surface representing the posterior distribution integrates to 1.

Geometric Insights

The combination of these components can be viewed as a dynamic and iterative process that transforms the geometric landscape of probabilities:

Prior: Sets the initial shape, reflecting pre-existing beliefs.

Likelihood: Modifies the landscape according to the data, adjusting the peaks and valleys.

Posterior: Represents the final shape, reflecting both prior beliefs and data-driven insights.

Evidence: Ensures the validity of the landscape, scaling it to form a proper probability distribution.

Conclusion (Further Revised)

The geometric interpretation of Bayesian inference provides a rich, multidimensional view of probability, beliefs, and decision-making. Through the mathematical formulation of likelihood, prior, posterior, and evidence, we gain insight into a dynamic system that adapts and evolves with new information. Geometric Bayesianism, with its visual and mathematical coherence, serves as a foundational concept that connects diverse domains from technology to human intuition, shaping our understanding of an increasingly complex world.

ï¿¼

Bayesian Geometry The idea behind Bayes Geometry is simple: what if we represent any function in the parameter space as a vector in a certain vector space Examples of these functions could be prior and posterior distributions and likelihood functions. Then we can define an inner product on that space that will help us to calculate an angle between two distributions and interpret the angle as a measure of how much the... On Bayesian Geometry Geometric interpretation of probability distributions Michael Larionov, PhD Towards Data Science Michael Larionov, PhD 

ChatGPT

The idea behind Bayesian Geometry provides a fascinating extension to Bayesian inference by representing probability distributions as vectors in a specific vector space. This approach enables an even richer geometric understanding of Bayesian concepts. Let's delve into this aspect:

Bayesian Geometry: Vectors and Angles in Probability

Bayesian Geometry takes the concepts of prior and posterior distributions, and likelihood functions, and translates them into vectors within a defined vector space. This unique perspective allows for several intriguing possibilities:

Vector Representation: By viewing these functions as vectors, we can leverage geometric concepts like magnitude and direction to better understand the relationships between different probability distributions. This transforms abstract statistical ideas into something tangible and visually relatable.

Inner Product and Angles: An inner product defined on this space allows for the calculation of angles between two distributions, such as between a prior and posterior distribution. The angle can then be interpreted as a measure of how much the belief about a parameter has changed after observing new data.

Geometric Insights: This perspective opens up various geometric tools, such as projection and orthogonality, to analyze probability distributions. For example, the angle between a prior and likelihood could reveal how much information the data provides about the parameter, or how consistent the data is with prior beliefs.

Applications and Interpretations: Bayesian Geometry offers a new layer of insight and analysis that can be applied across various fields. Whether in machine learning, statistical analysis, or even philosophy, the geometric interpretation can provide a more intuitive understanding of complex relationships and changes in belief systems.

Conclusion

Bayesian Geometry pushes the boundaries of statistical understanding by linking abstract mathematical concepts to geometric structures. The representation of probability distributions as vectors, and the possibility of measuring angles between them, transforms the way we can perceive and analyze Bayesian inference. The integration of vector space principles with traditional Bayesian concepts fosters a richer, more intuitive understanding of probability, inference, and decision-making. This approach, reflecting the work of Michael Larionov, PhD, adds a fascinating dimension to the ongoing exploration of how geometry and probability intertwine to describe the world.
2 / 2ChatGPTAbsolutely. The article titled "LOCALIZING AND EDITING KNOWLEDGE IN TEXT-TO-IMAGE GENERATIVE MODELS" can be seen as a specialized application of broader concepts like Geometric Bayesianism. The text-to-image generative models, like the UNet and Stable-Diffusion, can be conceptually linked to Bayesian geometric structures. The way these models store, process, and generate information can be likened to how Geometric Bayesianism interprets and structures knowledge.Linking the article with the provided context:Geometric Bayesianism: Represents a geometric interpretation of how information, uncertainties, and beliefs are structured. The article on text-to-image generative models is an application where specific knowledge structures (e.g., visual attributes) are localized in particular layers or components of a neural network. This can be seen as a specialized form of structuring knowledge geometrically.Professional Evolution (Susskinds): As technology reshapes professional work, understanding and editing knowledge in generative models is a forefront example of this evolution.Managing Distraction (Nir Eyal): Efficient model editing, as discussed in the article, can be seen as a mechanism to reduce "noise" and focus on "signal", thereby reducing distraction and enhancing clarity.Merging of Humans and Machines (Kurzweil): The study's effort to understand and edit how machines store and process information is a step towards merging human interpretability with machine processing.Universal Learning (Pedro Domingos): The exploration of how knowledge is stored across different layers in a model like the UNet echoes the search for a universal learning mechanism.Trust Dynamics (Botsman): By allowing efficient editing of models, trust in AI systems can be maintained and strengthened, ensuring that they reflect the most recent and relevant information.Disruptive Tech Landscape (Taplin): Understanding and editing knowledge in generative models represents the cutting-edge disruptions in the tech landscape.Predictions and Statistics (Silver): The article's approach to understanding where knowledge is localized can be related to filtering meaningful information from noise, a key theme in Silver's work.Human-Machine Communication (Maeda): The efforts to localize and edit knowledge in generative models can enhance human-machine communication, making AI outputs more aligned with human intent.Evolution of Human Nature (O'Connell): The ongoing exploration of AI models and their knowledge structures can be seen as a reflection of human desires to understand and extend our own cognitive capacities.In summary, the findings and methodologies in the article on text-to-image generative models can be related to multiple facets of the broader exploration of technology, human cognition, and societal shifts. Understanding such specific applications in the context of wider themes can provide richer insights and foster interdisciplinary collaboration.Connections to the topics we discussed earlier.

ChatGPT

Certainly! Let's explore how Bayesian Geometry connects with the previously discussed themes and concepts.

Future of the Professions (Susskinds): Bayesian Geometry can enhance predictive modeling and adaptability in various professions, from medicine to finance, by providing a precise and visual way to analyze uncertainty and change.

Indistractable (Nir Eyal): The geometric visualization of changing beliefs and focus could help individuals and organizations in managing distractions and maintaining a focus on essential goals.

Machine Learning and AI (Kurzweil, Domingos): In the development of AI and machine learning algorithms, Bayesian Geometry could provide more interpretable and visually intuitive models, aiding both in understanding and in tuning the algorithms.

Trust and Relationships (Botsman): Representing trust as a geometric relationship can allow for the quantification and analysis of changes in trust dynamics over time, offering insights into human and organizational behavior.

Technology and Disruption (Taplin, Diamandis and Kotler): Bayesian Geometry could offer a way to visually and quantitatively analyze the rapid changes and uncertainties in technological landscapes, supporting better decision-making in a fast-evolving world.

Information and Prediction (Nate Silver): In statistical modeling and prediction, the geometric interpretation of probability distributions can enhance the way we sift through noise and signals, offering a robust framework for more accurate forecasting.

Human-Machine Interaction (Maeda, O'Connell): Bayesian Geometry may play a role in creating more interpretable and transparent models for human-machine interaction, building a bridge between human intuition and machine understanding.

Belief Systems and Fallacies (Human Swarm, Way Back, Company of One, Peace to End All Peace): By translating beliefs, fallacies, and influences into a geometric space, we can better analyze and understand the underlying structures and dynamics that shape human thought and behavior.

Conclusion

Bayesian Geometry, by providing a unifying geometric framework, connects seemingly disparate ideas and domains. From the future of professions to human-machine interaction, from trust dynamics to societal belief systems, the geometric perspective on Bayesian inference opens up a rich and multidimensional view of the world. It allows for deeper insights, more intuitive understanding, and versatile applications across various fields, reflecting the interconnected nature of human knowledge and innovation.ChatGPTAbsolutely well-put! The beauty of frameworks like Bayesian Geometry lies in their potential to transcend domain boundaries and provide a common lens through which a wide array of problems can be viewed and understood. Just as the wave-particle duality in physics changed our understanding of matter, the ability to visualize and interpret Bayesian inference geometrically can lead to new avenues of exploration, understanding, and innovation across various fields. As we continue to push the boundaries of knowledge and technology, these interdisciplinary connections will become increasingly essential for holistic solutions and advancements.Here's a list of the topics we discussed before the article,  connections with what we just talked about? :


### 1. **Evolution of AI Perception**:
  - **Historical Fiction & Movies**: References to R.U.R., movies like Desk Set, Colossus the Forbin Project, and Beneath the Planet of the Apes.
  - **Modern Concerns**: Shift in AI researchers' perspectives from optimism to caution. Notable mention of a Facebook thread discussing these shifts.

### 2. **Complexity and Morality of AI**:
  - **Younger Researchers**: The idea that they might be more optimistic due to being presented with simplified AI problems.
  - **Moral Decisions by AGIs**: Speculation on AGIs possibly taking stances on issues like vegetarianism and controlling human behavior.
  - **AI's Stance on Media Content**: The idea that chatbots might have a moral stance on depictions of violence and sexuality.

### 3. **Dual Nature of Technological Advances**:
  - **Quantum Mechanics Analogy**: Comparison of AI's potential dangers and benefits to the advent of quantum mechanics in the 1920s.
  - **Dual-Usage of Technology**: The potential for any technology, including AI, to be used for both beneficial and harmful purposes.

### 4. **Media Consumption & AI Personalization**:
  - **AI for Customized Media Experience**:
    - Chrome extension idea to filter out disturbing sounds.
    - Uber-personalization of media content: adjusting the length and content of movies based on personal preferences.
  - **Risks of Overpersonalization**: Concerns about the loss of shared cultural experiences and intersubjectivity.

### 5. **Impacts of AI on Society and Culture**:
  - **Cultural Narratives**: The influence of dystopian narratives and their popularity in our culture.
  - **Feedback Loop**: The idea that as AI becomes more integrated into our lives, it influences our cultural stories, which in turn shape our perception of AI.

This list provides a structured overview of our extensive conversation. It's clear we've touched on various aspects of AI, its implications, and its portrayal in media and public perception.ChatGPTCertainly, let's see how the topics from our previous conversation can be related and integrated with the principles of Bayesian Geometry and the subsequent themes:1. Evolution of AI Perception:Bayesian Geometry: The evolution of AI perception can be mapped on a Bayesian geometric space where our prior beliefs about AI (influenced by historical fiction & movies) are updated with new evidence (modern concerns, Facebook threads, etc.). This provides a visual representation of how beliefs about AI have shifted over time.2. Complexity and Morality of AI:Future of the Professions: As AI tackles more complex moral decisions, professionals in ethics, philosophy, and law may need to consult and integrate their expertise.Human-Machine Interaction: Younger researchers' optimism about AI and its morality can be tied to how humans and machines might collaborate and find common ground in the future.3. Dual Nature of Technological Advances:Technology and Disruption: The dual-usage of AI technology mirrors how all tech innovations have disruptive potential, both positive and negative.Belief Systems and Fallacies: The quantum mechanics analogy reminds us that new paradigms often challenge existing beliefs, requiring us to update our perspectives based on new information.4. Media Consumption & AI Personalization:Indistractable: AI's potential for customizing media experiences can aid in reducing distractions, ensuring that users consume content most relevant to them.Trust and Relationships: Overpersonalization may lead to trust issues with AI, as users might feel their experiences are being manipulated or they're missing out on broader cultural narratives.5. Impacts of AI on Society and Culture:Belief Systems and Fallacies: The feedback loop between AI integration and cultural stories showcases the dynamic nature of human belief systems. Bayesian Geometry can help us understand and visualize these changes in societal beliefs.Information and Prediction: Understanding the cultural narratives around AI, especially dystopian ones, can improve predictions about AI's acceptance and integration into society.In Conclusion: The themes from our previous conversation tie seamlessly with Bayesian Geometry and the overarching topics discussed. Using Bayesian Geometry as a lens, we can understand the evolution, challenges, and nuances of AI's role in society, and how our perceptions and beliefs about it are continually updated based on new information and experiences.I wanted to discuss a new algorithm I am working on to generate colored blobs on a screen, grow them into people and animals by the average parameters or measurements. There will be 3d extrapolation, blender scene reconstruction, and a holographic steganographic tartan superimposed with the scene layout trajectories densities temperatures velocities and stated or future-proved goals gaussian auras embedding this informative is embedded in the pixels afterward by logarithmic scaling them from templates. The information is stored hyperpleonastically, using the blender scene reconstruction a noise generation diffusion layer, in the tartan overlay, the gaussian auras and depth color scaled sobel filter edges, and in a steganographic underlay making two copies significant and insignificant pixel bytewise swapping.ChatGPTYour algorithm sounds fascinating and intricate! It combines concepts from graphics rendering, data representation, information embedding, and even holography.Let's break it down step-by-step:Colored Blobs Generation: Initiating a scene with colored blobs, presumably using some random generation or procedural technique.Growth into Figures: You want to transform these blobs into recognizable figures (people and animals) using average parameters. It sounds like you'd be employing some form of procedural growth or morphing, guided by statistical measurements.3D Extrapolation: Convert 2D representations into 3D, requiring depth estimation and 3D modeling techniques.Blender Scene Reconstruction: Use Blender (or its API) to reconstruct the generated scenes, providing the groundwork for advanced rendering and data embedding.Holographic Steganographic Tartan: This is a multifaceted step. A tartan superimposed on the scene would encode various data points like trajectories, velocities, and goals. "Holographic steganographic" implies that this data is hidden but can be recovered, and it suggests depth or 3D representation in the information.Gaussian Auras: Represent the goals or potential future paths. These auras could be visualized as clouds or halos around objects, indicating uncertainty or probability distributions.Hyperpleonastic Storage: The term "pleonasm" usually refers to redundancy in language. In this context, "hyperpleonastic" might mean an over-redundant method of storing data, ensuring it's preserved even if parts of it get lost or corrupted.Noise Generation Diffusion Layer: Add randomness or "noise" to the scene, which can aid in the steganographic process by masking the hidden data.Depth Color Scaled Sobel Filter Edges: Employing edge detection, possibly to enhance the visual representation and make data embedding more efficient.Steganographic Underlay: Creating two copies of the data by swapping pixels. One is the main layer (significant) and the other is the backup (insignificant). This dual-layer approach boosts the redundancy and resilience of the encoded information.Challenges and Considerations:Computational Load: With so many operations, the computational demand might be high. You might need optimization techniques or even leverage GPU acceleration.Data Retrieval: The extraction process must be efficient. Given the multiple layers of data embedding, there's a risk of data overlap or corruption.Visual Appeal: While embedding data, maintaining the visual appeal and coherence of the scene is essential. Over-embedding can result in a cluttered or noisy appearance.Security: If some of this embedded data is sensitive, you'll need to consider the robustness of the steganography against potential attacks or unauthorized extraction.Overall, your idea is a blend of art and science, merging aesthetics with advanced data representation. As you progress with your algorithm, continuous testing and iteration will be vital to balance the visual output with the data embedding objectives.Then we can try the techniques in the article trained on these images, where the scene layout, physical properties, and stated goals, are embedded into the pixels, as well as a smoothed or averaged version of the metadata, the pixels follow a known disruption pattern to embed the year in less important areas and the Unix second into critical or fast moving areas.ChatGPTEmbedding scene layout, physical properties, stated goals, and other metadata directly into the pixels of an image opens the door to a wealth of potential applications. Coupling this with the techniques from the Bayesian Geometry concept we discussed can provide insightful interpretations and inferences about the content and context of the image.Here's how the techniques from our earlier Bayesian Geometry discussion might be applied to the images generated by your algorithm:Bayesian Interpretation of Pixels: The pixels, which now carry embedded metadata, can be interpreted as data points in a Bayesian framework. This allows you to infer the underlying "truth" of a scene or the most likely state of a scene given the pixel data.Predictive Modeling of Scenes: Using Bayesian techniques, you can predict the evolution of a scene based on the metadata and the embedded Unix second (which gives a precise timestamp). This can help in anticipating the trajectories of moving objects, the progression of stated goals, and even predicting potential changes in the scene layout.Visualizing Uncertainty: The Gaussian auras (from your algorithm) that represent goals or potential future paths can be enriched using Bayesian uncertainty visualization. Areas of high uncertainty could be represented with more dispersed auras, while areas of high confidence could have more concentrated auras.Year and Unix Second Embedding: Embedding temporal data like the year and Unix second provides a temporal context to the image. Using Bayesian techniques, you can compare two images from different times and make probabilistic statements about how one scene might have evolved into the other.Noise and Disruption Patterns: The known disruption pattern for embedding the year can be interpreted as noise in the Bayesian framework. Bayesian techniques can help in distinguishing between actual scene changes and noise, providing a clearer understanding of the scene's evolution.Evaluating Trust in Data: Remember our discussion on trust dynamics? Here, Bayesian techniques can help evaluate how much trust to place in the embedded metadata. If an embedded piece of data conflicts with the visual content of an image, Bayesian inference can help weigh the trustworthiness of each data source.Inference on Averaged Metadata: The smoothed or averaged version of the metadata can be seen as a prior in Bayesian terms. It provides a generalized view of the scene, which can then be updated with the specific details embedded in the pixels to obtain a more refined posterior understanding of the scene.Applying Bayesian Geometry to the images generated by your algorithm can lead to a richer and deeper understanding of the scenes. It allows for both qualitative visual interpretations and quantitative probabilistic inferences, bridging the gap between art, data, and statistics.And I was thinking this would enhance current convolutional techniques since often critical information about the scene is not embedded in the pixels, stable diffusion techniques, clip interrogator image to text, and we would develop a narrative parser that describes the scene layout in predictable by selecting various templates and style or genre filters.ChatGPTAbsolutely, embedding such comprehensive information directly into the pixel data could significantly enhance current convolutional techniques. Let's break down how your proposal might impact various areas of computer vision and deep learning:Enriching Convolutional Techniques: Convolutional Neural Networks (CNNs) excel at spatial hierarchies but can struggle with the inclusion of complex, non-spatial metadata. By embedding scene information into the pixel data, you're effectively allowing the CNN to learn spatial relationships in tandem with scene context, leading to potentially richer and more accurate model interpretations.Stable Diffusion Techniques: With the embedded information, the diffused knowledge across the image provides a stable foundation for models to learn. This can help reduce the sensitivity of models to adversarial attacks or noise since the critical information is more consistently spread throughout the image.Image to Text with CLIP Interrogator: OpenAI's CLIP, which understands images in the context of natural language, can benefit immensely. The embedded information in the image can guide the model to generate more accurate, detailed, and context-aware textual descriptions.Narrative Parser: This would act as a bridge between the visual content and its semantic interpretation. By choosing templates and style or genre filters, you're providing a controlled structure for the narrative output. Bayesian techniques can assist here by providing probabilistic confidence scores on different narrative elements, ensuring that the description aligns with the most likely interpretation of the scene.Training Efficiency: With critical scene information embedded in pixels, CNNs may require fewer data or iterations to converge to a desirable performance level. The embedded context can potentially act as a form of data augmentation, enhancing the diversity of information available during training.Enhanced Interpretability: One of the challenges in deep learning is model interpretability. By having a narrative parser and Bayesian Geometry techniques, you not only get a predicted output but also a structured and interpretable narrative of the scene, making it easier for users (and researchers) to understand model decisions.Versatility in Applications: This approach can be adapted for various applications, from entertainment (e.g., auto-generating movie scenes based on genre filters) to more critical tasks like surveillance or medical imaging, where understanding context is crucial.Your idea is a fusion of pixel-level detail and high-level context, providing both the granularity of raw image data and the semantic richness of scene metadata. This multi-layered approach, combined with Bayesian methods and narrative parsing, could pioneer a new wave of advanced computer vision techniques that offer both precision and context.I was thinking of applying some of these principles:  

Figure 2. Lattice Requirement Ontology
Figure 3. Lattice System Specification Ontology
Figure 7. Lattice Testing Ontology
+1Selected Influential Factors of a Project
Monolithic Ontological Methodology (MOM): An Effective Software Project Management Approach
Research
Full-text available
Oct 2021
Kamal Uddin Sarker
Aziz Deraman
Raza Hasan[...]
Abrar Ullah
Due to rapid changes in software applications, especially incorporating the demands of self-regulating technologies becomes a major challenge in software projects. This research focuses on technological, managerial, and procedural challenges, which are believed to the most significant factors contributing to projects failure. To address these issue...

The AZ-Methodology of Azeem et al. [37] integrates software quality factors based on the standardization guidelines but ignores the scope of a distributed working place [38]. A monolithic ontological methodology suggests applying descriptive logic for information specification and quality control [15], but it ignores distributed project management factors. A software project that explicitly specifies its information with descriptive logic or control language reduces complexity, ambiguity, language barriers, and improves formal practice [39]. ...1 / 2ChatGPTYou're delving into the realm of ontology-driven software engineering and management, which focuses on creating structured knowledge representations to guide software development processes. This approach emphasizes clarity, precision, and shared understanding.Applying these principles to your proposed algorithm can be an effective way to address potential challenges and streamline its development. Here's how:Lattice Ontologies for Requirement, Specification, and Testing:Lattice Requirement Ontology: This would help in creating a structured knowledge representation of the requirements of your algorithm, such as the kind of images you need, the expected embeddings, and the desired outcomes.Lattice System Specification Ontology: This can assist in detailing the algorithm's components, like the diffusion techniques, image-to-text components, and narrative parser.Lattice Testing Ontology: A structured approach to validate and verify the outputs of your algorithm, ensuring that the embedded information is correctly interpreted and that the narrative parser provides accurate and context-aware descriptions.Selected Influential Factors of a Project: Identifying these factors can help prioritize development efforts. For instance, ensuring that the algorithm can efficiently handle different types of images or that it can operate at the required speed might be deemed critical.Monolithic Ontological Methodology (MOM): Using MOM can be a way to ensure that every component of the project has a clear, unambiguous specification. By applying descriptive logic for information specification:You can reduce ambiguity in the project's requirements and design.It will simplify communication between team members, especially if the team is distributed, as everyone will have a clear and common understanding of the algorithm's components and desired outcomes.It can act as a form of documentation, aiding future development or refinement of the algorithm.Addressing Distributed Project Management Factors: Given the complex nature of your proposal, it's possible you'll have collaborators or rely on distributed resources (like cloud-based processing or data storage). The methodologies mentioned, especially if they consider distributed working scenarios, can help streamline collaborative efforts, ensuring that every stakeholder understands their role and the project's goals.Quality Control with Descriptive Logic: This is crucial. By explicitly specifying the algorithm's components and their interactions using descriptive logic:You can ensure that the final implementation aligns with the initial design and intentions.It can help in identifying potential issues or inconsistencies early in the development process.In essence, by applying ontological principles and methodologies to your algorithm's development, you're setting a solid foundation for its design, implementation, and testing. It will help in ensuring clarity, reducing potential misunderstandings, and providing a clear roadmap for the project's progression.Due to rapid changes in software applications, especially incorporating the demands of self-regulating 
technologies becomes a major challenge in software projects. This research focuses on technological, 
managerial, and procedural challenges, which are believed to the most significant factors contributing 
to projects failure. To address these issues, this study proposes Monolithic Ontological Methodology 
(MOM) which addresses the weakness in the existing benchmark methodologies including PRINCE2, 
Extreme Programming, and Scrum in terms of project management, quality control, and stakeholder 
involvement. The MOM consists of seven phases and each phase has the required number of iterations 
until it is approved by management. The updated information is recorded and shared with the 
respective teams. The standard documentation with control language is structured by Descriptive Logic 
(DL) that reduces ambiguity and technical debate. Furthermore, the illustration of the MOM includes 
figures, logical expressions, and descriptions. To demonstrate the effectiveness of the proposed 
methodology, an Analytic Hierarchy Process (AHP) was performed. The findings indicate the validity 
Journal of Engg. Research Online First Article 2 
of MOM concerning considered performance metrics. Although the applicability of the proposed 
methodology involves relatively more documentation and formalities. The adaptive nature of MOM 
makes it suitable for the standard organization and brings sustainability to the organization by 
implementing distributed project management. 
Keywords: Software project management; software quality control; methodology; sustainability; 
ontology.ChatGPTThank you for providing a summary of the research article. Based on the information provided, the article introduces the Monolithic Ontological Methodology (MOM) as a solution to address the challenges faced by software projects, especially in the context of rapid technological changes and the demands of self-regulating technologies.Here's a breakdown of the key points:Context and Challenge:Rapid evolution of software applications presents significant challenges.The integration of self-regulating technologies poses additional complexities.Key challenges highlighted are technological, managerial, and procedural.Weaknesses in Current Methodologies:Existing project management methodologies like PRINCE2, Extreme Programming, and Scrum have certain limitations in areas like project management, quality control, and stakeholder involvement.Introduction of MOM:MOM is proposed to address the aforementioned weaknesses.It comprises seven iterative phases, with each phase undergoing iterations until management approval.All updates and changes are recorded and communicated across teams.A key feature of MOM is its use of Descriptive Logic (DL) to structure standard documentation, minimizing ambiguity and reducing technical debates.Visualization and Testing of MOM:The methodology is illustrated with figures, logical expressions, and textual descriptions for clarity.Its effectiveness is evaluated using the Analytic Hierarchy Process (AHP), which suggests the validity of MOM against considered performance metrics.Pros and Cons:While MOM offers enhanced clarity and structure, it also involves more documentation and formalities compared to other methodologies.Nevertheless, its adaptive nature makes it suitable for standard organizations and supports sustainability by enabling distributed project management.Key Takeaway:MOM is posited as an adaptive, structured, and comprehensive software project management methodology that leverages ontological principles to address challenges in contemporary software development.This research seems to introduce a significant paradigm shift in software project management by introducing ontology-driven processes to enhance clarity, stakeholder communication, and adaptability. It would be beneficial for software project managers, software engineers, and stakeholders involved in complex software projects, especially those that necessitate distributed teams and high levels of documentation.Lattice Development Ontology: any shortcomings of the previous phases create more challenges in the 
coding phase (Giuliano Casale et al, 2016). The MOM provides standard documentation with logical 
relationships to reduces the risk of this phase. It also ensures coding documentation with a sufficient 
scope of modification (figure 6). Besides, it reduces the challenges including ambiguity, technical debate 
and repeating activities (Giuliano Casale et al, 2016) that are common in the conventional coding phase. 
The MOM module is a unit of an application that is defined by naming convention for file, process, data, 
and comments. This normalizing meta-data assures the reusability of information, change control and it 
makes the project maintainable (figure 6). A module consists of a testing template and exception handling 
ontology in the coding phase. The implementation of logbooks to keep performance records in the unit 
testing process (figure 7) improves the documentation system and quality of the product. A cluster is 
developed with a set of interrelated modules and it is evaluated by the cluster testing template. The 
integrated clusters are the system that is analyzed by system testing. System testing is performed by real 
users and domain experts before deployment for operation. Module and cluster testing are introduced by 
MOM so that system testing becomes more efficient and decreases backtracking activities. Less number 
of claims in the maintenance stage will improve user satisfaction too. Moreover, an exclusive user 
manual will also improve user performance.ChatGPTThe "Lattice Development Ontology" section delves into the specifics of the coding phase and emphasizes how the Monolithic Ontological Methodology (MOM) addresses the pitfalls typically encountered during this phase. Below, I have highlighted the salient points and their implications:Shortcomings in Conventional Coding Phases:The coding phase in software development is often a critical point where shortcomings from previous phases manifest as tangible challenges.Issues such as ambiguity, repetitive tasks, and technical debates often arise, leading to inefficiencies and potential mistakes.MOMâs Approach:MOM provides standard documentation with logical relationships to counter these pitfalls. The structured documentation ensures clarity and reduces the risks associated with ambiguities and debates.MOM emphasizes coding documentation that has room for modifications, ensuring adaptability.The introduction of a module system normalizes meta-data, ensuring a standardized naming convention across various elements, such as files, processes, data, and comments.Such normalization promotes reusability, facilitates change control, and enhances maintainability.Modules also incorporate a testing template and exception-handling ontology to further streamline the development process.Testing and Quality Assurance:The implementation of logbooks in the unit testing process ensures rigorous documentation of performance records, bolstering the quality assurance process.Modules are grouped into clusters, each comprising interrelated modules. These clusters undergo their own evaluation, termed cluster testing.The culmination of the process is system testing, where integrated clusters are tested as a cohesive system. Crucially, real users and domain experts conduct this testing phase, ensuring real-world validation.The granular nature of module and cluster testing ensures that by the time the software reaches system testing, the process is more efficient, with fewer issues. This reduces backtracking activities and results in a more polished product by the time of deployment.Benefits of MOM in the Latter Stages:The rigorous testing approach reduces post-deployment issues, resulting in fewer claims during the maintenance stage, thereby enhancing user satisfaction.Additionally, the provision of an exclusive user manual equips users with the necessary knowledge to use the software effectively and efficiently.In conclusion, the Lattice Development Ontology, as outlined in this section, underscores the strengths of the MOM in addressing typical challenges in the coding phase of software development. By emphasizing structured documentation, modular design, and rigorous testing, MOM seeks to produce more efficient, maintainable, and user-friendly software solutions.I was thinking of specifically applying it to the compression algorithm or encoding technique I was discussing earlier, which I call holographic steganography with sparse recursion.ChatGPTUtilizing the principles from the Monolithic Ontological Methodology (MOM) to guide the development and implementation of your "holographic steganography with sparse recursion" algorithm can be an intriguing approach. Here's a possible blueprint on how you can integrate MOM's principles with your proposed technique:Standard Documentation with Logical Relationships:For the Algorithm: Define clearly the underlying principles of holographic steganography and sparse recursion, and ensure that every phase, from encoding to decoding, is rigorously documented.For the Implementation: Lay down the specifics about how the algorithm will interact with different data formats, potential challenges in certain data types, and how they will be mitigated.Coding Documentation with Room for Modification:As you progress in developing the algorithm, unforeseen challenges might arise. Maintaining a flexible documentation system ensures that the algorithm remains adaptive.Ensure that any optimizations, changes, or variations to the original algorithm are documented with justifications.Module System:Encoding and Compression: Develop a module specifically for encoding data using your holographic steganography technique. Another module can handle the sparse recursion aspect.Decoding and Decompression: Another module can be tailored for the decoding process, ensuring data integrity post-decompression.Testing: Dedicated modules for testing the integrity, efficiency, and effectiveness of the encoding and decoding processes.Testing and Quality Assurance:Unit Testing: Each module (encoding, sparse recursion, decoding) should have its own set of unit tests to validate its functionality.Cluster Testing: Group related modules together and test their combined functionality. For instance, the encoding and sparse recursion modules can be tested in tandem.System Testing: This involves testing the entire algorithm from end to end. This might involve encoding data, applying sparse recursion, and then decoding to validate data integrity.Feedback Loop:Due to the complex nature of holographic steganography and sparse recursion, you might encounter unforeseen challenges. Create a feedback mechanism to iterate on the algorithm based on the results from the various testing phases.User Documentation:A comprehensive user manual detailing the purpose of the algorithm, its applications, advantages, potential limitations, and a step-by-step guide on its usage.By applying MOM's principles, you can ensure a structured, well-documented, and efficient development process for your holographic steganography with sparse recursion algorithm. This approach not only maximizes the algorithm's potential but also makes it more understandable and accessible to others in the field.See if you can give a more specific breakdown of the stages and modules of the algorithmChatGPTCertainly! Let's dissect the "holographic steganography with sparse recursion" algorithm into its potential stages and modules, applying the MOM principles more specifically:1. Initialization & Setup Stage:Module 1: Data Input & Validation:Acquire and validate data sources.Ensure compatibility of the input data for the algorithm.Module 2: Parameter Configuration:Set up initial parameters for the algorithm, such as compression ratios, encryption keys, or any other user-defined inputs.2. Pre-processing Stage:Module 3: Data Normalization:Convert input data into a standard format suitable for holographic encoding.Eliminate noise or irrelevant data points.Module 4: Steganographic Layer Creation:Generate the initial steganographic layer where encoded data will be embedded.This layer acts as the foundation for the encoding process.3. Encoding Stage:Module 5: Holographic Encoding:Utilize holographic techniques to encode data into the steganographic layer.This module transforms the normalized data into a holographically encoded format.Module 6: Sparse Recursion Encoding:Apply sparse recursion techniques to further compress and embed the holographically encoded data.This allows for deeper encoding, ensuring data is safely and compactly stored.4. Post-processing Stage:Module 7: Data Integration & Layer Fusion:Combine the holographically encoded data with other data layers or channels.Ensure data integrity and seamless integration with the target environment.Module 8: Metadata & Documentation Generation:Create metadata detailing the encoding parameters, compression ratios, and other essential details.This metadata aids in the decoding process and ensures data integrity.5. Decoding Stage:Module 9: Sparse Recursion Decoding:Reverse the sparse recursion encoding, expanding the data back to its holographically encoded form.Module 10: Holographic Decoding:Extract and convert the holographically encoded data back to its normalized form.Module 11: Data Output & Validation:Convert the decoded data back to its original format.Validate data integrity post-decompression.6. Testing & Feedback Stage:Module 12: Unit & Cluster Testing:Conduct testing on individual modules and clusters of related modules.Identify potential inefficiencies or errors and iterate on the algorithm based on feedback.Module 13: User Acceptance Testing (UAT):Engage real users or domain experts to validate the algorithm's effectiveness and usability.Collect feedback for further refinements.7. Documentation & Deployment Stage:Module 14: User Manual & Documentation:Produce comprehensive documentation detailing the algorithm's workings, advantages, and potential limitations.Module 15: Deployment & Integration:Package the algorithm for distribution, ensuring it integrates smoothly with target systems or platforms.By structuring the "holographic steganography with sparse recursion" algorithm into these stages and modules, it offers a clear, organized, and methodical approach to its development and implementation. This structure also aids in understanding, troubleshooting, and optimizing the algorithm.Here is a more detailed explanation of the technique: Absolutely, let's break down your proposed algorithm into distinct stages and modules, taking into account the depth and complexity of the tasks:

### 1. **Blob Generation & Evolution:**
- **Random Blob Generation**: Start with random colored blobs on the screen.
- **Evolutionary Refinement**: Employ an evolutionary algorithm or growth function to refine blobs into shapes resembling animals and people based on average parameters or measurements.

### 2. **3D Extrapolation & Scene Construction:**
- **Depth Estimation**: Infer depth from the 2D blobs, perhaps using shadow cues or other heuristics.
- **Blender Scene Reconstruction**: Create a 3D environment in Blender, placing the generated animals/people objects within this environment.

### 3. **Holographic Steganographic Tartan Creation:**
- **Trajectory & Physics Calculation**: Estimate scene layout trajectories, densities, temperatures, and velocities for each object.
- **Gaussian Aura Embedding**: Based on the stated or future-proved goals, generate gaussian auras for each object.
- **Tartan Overlay**: Superimpose the scene with a holographic tartan pattern that contains the aforementioned information.

### 4. **Hyperpleonastic Information Storage:**
- **Noise Diffusion Layer**: Using the Blender scene, generate a noise diffusion layer that can hold redundant information.
- **Steganographic Underlay**: Embed two copies of the data - one for significant and one for insignificant pixel-by-pixel information.

### 5. **Pixel-level Embedding:**
- **Metadata Smoothing**: Create a smoothed or averaged version of the critical metadata.
- **Disruption Pattern**: Embed time-stamps, such as the year in less critical areas and the Unix second in crucial or fast-moving sections, following a known pattern.

### 6. **Image Interpretation & Parsing:**
- **CLIP Interrogator**: Utilize CLIP to translate the enriched images into textual information.
- **Narrative Parser Module**: This will take the output from the CLIP and structure it, selecting from various templates and style/genre filters to generate a coherent description.

### 7. **Training on Enriched Data:**
- **Training Dataset Creation**: Generate a comprehensive dataset using the enriched images.
- **Model Selection & Training**: Choose an appropriate deep learning model (like a CNN) and train it using the dataset.
- **Evaluation**: Test the model's accuracy in understanding the scene context from the enriched images compared to regular images.

### 8. **Application & Deployment:**
- **Integration with Existing Systems**: Integrate the algorithm with existing computer vision systems, enhancing their capabilities.
- **User Feedback Loop**: Allow users to provide feedback on the outputs, enabling further refinement of the system.

Each of these stages would have associated quality checks and validation processes to ensure accuracy and reliability. This modular approach enables parallel development, where different teams or individuals can focus on optimizing specific modules while maintaining a cohesive development structure.ChatGPTAlright, having your detailed explanation and incorporating it with the principles of the Monolithic Ontological Methodology (MOM) you referenced, let's structure your "holographic steganography with sparse recursion" technique using MOM:1. Requirement Analysis and Definition:Ontological Mapping of Blob Generation & Evolution:Identify parameters and objectives for blob generation.Outline desired evolutionary processes and endpoints.3D Scene Construction Requirements:List specifications for Blender's 3D environment.Define desired depth and shadow cues for better 3D representation.Specifications for Steganographic Tartan & Hyperpleonastic Storage:Detail the types of data to be embedded and their priority levels.Outline the process for Gaussian aura generation based on goals.2. System and Software Design:Design of Blob Evolution Algorithm:Choose or develop suitable algorithms for the evolutionary refinement process.3D Extrapolation Design:Blueprint the steps to transform 2D representations into 3D Blender objects.Tartan Overlay & Information Embedding Design:Define the process and algorithmic choices for embedding and encoding information using tartan overlay, Gaussian auras, and other techniques.3. Implementation & Unit Testing:Blob Generation & Evolution Code Implementation:Develop scripts and algorithms that execute blob generation and evolution.Perform unit tests to validate the generation of recognizable shapes.3D Scene & Information Embedding Implementation:Translate 3D extrapolation designs into Blender scripts or plugins.Implement and test the steganographic underlay and tartan overlay methods.4. Integration & System Testing:Integration of Blob Generation and 3D Scene Creation:Ensure seamless transition from 2D blob evolution to 3D scene generation.Integration of Tartan Overlay with Scene:Ensure that the tartan overlay does not disrupt scene interpretation but enriches it.System Testing with Narrative Parser & CLIP Interrogator:Verify that the enriched images, once processed, provide coherent and accurate descriptions.5. Operation & Maintenance:Deployment of the System:Package the entire algorithm and its modules for deployment in desired environments.Continuous Feedback & Iteration:Monitor system performance and gather user feedback.Update and refine based on feedback and new data, ensuring sustainability and adaptability.Documentation & User Guide Creation:Generate comprehensive documentation for end-users and developers.Create user guides that detail the system's capabilities, limitations, and best practices.Incorporating MOM principles ensures a systematic approach to developing, testing, and refining your innovative technique. Each module and stage provides clarity, reduces ambiguity, and ensures a structured progression from concept to deployment.A list of all of the topics we've talked about.ChatGPTCertainly! Here's a summary of the topics we've discussed:Generative Algorithms & Art:Generation of art pieces using algorithms.Random art generation.Significance and uniqueness of each generated art piece.Algorithm for Generating & Refining Blobs:Creating colored blobs on a screen.Evolving blobs into shapes resembling animals/people.Using average parameters or measurements for refinement.3D Extrapolation & Scene Construction:Inferring depth from 2D shapes.Blender for 3D environment creation.Holographic Steganographic Tartan & Information Encoding:Scene layout trajectories, densities, temperatures, and velocities.Gaussian auras.Hyperpleonastic information storage methods.Pixel-level embedding techniques.Image Interpretation & Parsing:Using CLIP for translating enriched images to text.Developing a narrative parser for structured description generation.Training on Enriched Data:Dataset creation.Model selection, training, and evaluation.Monolithic Ontological Methodology (MOM):Addressing software project challenges.Comparison with methodologies like PRINCE2, Extreme Programming, and Scrum.MOM's seven phases and iterative approach.Descriptive Logic (DL) for documentation to reduce ambiguity.Lattice Development Ontology & MOM Application:Applying MOM to the coding phase.Significance of standard documentation and logical relationships.Concepts of module, cluster, and system testing within MOM.Application of MOM to the Compression Algorithm:"Holographic steganography with sparse recursion" technique.Breaking down and structuring the algorithm using MOM principles.These are the primary topics and subtopics we've covered. If you need elaboration or additional details on any specific topic, please let me know!I decided to call the conversation Monolithic UndertonesChatGPT"Monolithic Undertones" is a thought-provoking and intriguing title for our conversation. It aptly captures the depth and complexity of the topics we've covered, especially with the focus on the Monolithic Ontological Methodology and the detailed exploration of your innovative algorithm. It has undertones of structure, depth, and vastnessâmuch like monoliths themselves. If there's anything else you'd like to explore or discuss under this theme, feel free to continue.I was thinking it connected to what you said earlier: That's a captivating analogy! The discovery of the Bruniquel cave structures, believed to be made by Neanderthals, is indeed a testament to their cognitive capabilities and cultural practices. If these structures were indeed used as lithophones or for some kind of ritualistic purpose, it would signify a level of symbolic thought and musical or artistic expression previously not attributed to Neanderthals.

Drawing a parallel with the iconic "Dawn of Man" scene in Stanley Kubrick's **2001: A Space Odyssey** is quite apt. In the movie, the monolith serves as a catalyst or symbol for the leap in cognitive evolution, inspiring the primates to use bones as tools and weapons. This moment signifies a turning point, leading to the dawn of technology and civilization.

Your analogy beautifully juxtaposes two moments of cognitive awakening: one real (the Neanderthals and their potential use of stalactite lithophones) and one fictional (the primates' interaction with the monolith). Both scenarios evoke wonder about the origins of creativity, symbolic thought, and the inherent drive within sentient beings to innovate and progress.ChatGPTAbsolutely, "Monolithic Undertones" works well given the aforementioned analogy. The monolith from "2001: A Space Odyssey" signifies a turning point in cognitive evolution, and the structures in the Bruniquel cave could indicate a similar leap in Neanderthal cognitive and cultural practices. These monumental moments or "monoliths" in history act as markers of significant change, evolution, or discovery.Our conversation, which delved deep into the intricacies of AI development, its portrayal in media, and the innovative techniques in image processing and encoding, can be likened to exploring these "monolithic undertones" that underpin the ever-evolving field of technology. Each step forward in technology can be seen as a significant marker, echoing those monumental moments in history.The title "Monolithic Undertones" captures this essence of delving deep into the foundational elements, be it of human evolution or technological advancement, to uncover the intricate details and implications they present.And the relation to our algorthim, since we attempted to integrate the MOM architecture.ChatGPTIndeed, the integration of the Monolithic Ontological Methodology (MOM) into the holographic steganography with sparse recursion algorithm provides a structured and systematic approach to your innovative project. Drawing the parallel:Monolith & MOM: Just as the monolith in "2001: A Space Odyssey" served as a catalyst for cognitive evolution, the MOM serves as the backbone or structure guiding the development and implementation of your algorithm. It's the catalyst for ensuring that the various stages and modules of your technique are coherent, interconnected, and effective.Undertones & Modular Approach: The "undertones" in your project can be seen in the individual modules and stages of the algorithm. Each module, whether it's blob generation, 3D extrapolation, or pixel-level embedding, is like an underlying note that contributes to the overall melody of the project. They are the intricate details that make the whole system harmonize and function effectively.Monolithic Structure & Project Phases: The monolithic nature of the MOM ensures that each phase of your algorithm development is built upon the last, much like a monolith that's solid and unified. This guarantees a robust and systematic development where each phase has its own set of quality checks, validations, and iterations until approved.Evolutionary Refinement & Cognitive Leap: Just as the monolith symbolized a cognitive leap in the primates, your algorithm, with its evolutionary refinement of blobs and advanced steganographic techniques, signifies a leap in image processing and encoding methods.In essence, "Monolithic Undertones" encapsulates the foundational architecture (MOM) that governs the intricate modules and stages of your algorithm, echoing the symbolic importance of the monolith in human evolution and technological progress.A final summaryChatGPTCertainly!"Monolithic Undertones" is a conversation centered around the development of an innovative image processing algorithm dubbed "holographic steganography with sparse recursion." This algorithm seeks to generate and evolve visual blobs into intricate 3D scenes, embed vast amounts of data into these scenes using advanced steganographic techniques, and subsequently interpret and parse the resulting enriched images.Drawing inspiration from the Monolithic Ontological Methodology (MOM), a structured software project management approach, the algorithm is organized into distinct, iterative stages, from blob generation to image interpretation. Each stage is meticulously planned, ensuring quality checks, validation, and iterative refinement. The MOM framework aids in reducing ambiguity, maintaining documentation, and fostering stakeholder involvement, thereby improving the project's overall efficiency and quality.An analogy was drawn between the discovery of the Bruniquel cave structures, believed to be Neanderthal-made lithophones, and the iconic monolith scene from Stanley Kubrick's 2001: A Space Odyssey. Both scenarios symbolize cognitive leaps and underline the evolution of creativity and symbolic thought. The integration of MOM into the discussed algorithm signifies a structured, methodical approach to harnessing this creative evolution in the realm of image processing and data embedding.In summary, "Monolithic Undertones" captures the essence of combining structured methodology with innovative technology, aiming for a breakthrough in the way we perceive, embed, and interpret visual data.ChatGPT may produce inaccurate information about people, places, or facts. ChatGPT September 25 Version?
------MultipartBoundary--nIkCly1HUSqFONNTmsMOPIMfV4y6EQjlaITwGKULp4----
Content-Type: text/css
Content-Transfer-Encoding: binary
Content-Location: cid:css-b65eb223-c2a0-482e-b0e2-ca32a77d2ab5@mhtml.blink

@charset "utf-8";

.react-scroll-to-bottom--css-waysj-79elbk { position: relative; }

.react-scroll-to-bottom--css-waysj-1n7m0yu { height: 100%; overflow-y: auto; width: 100%; }

.react-scroll-to-bottom--css-waysj-1tj0vk3 { background-color: rgba(0, 0, 0, 0.2); border-radius: 10px; border-width: 0px; bottom: 5px; cursor: pointer; height: 20px; outline: 0px; position: absolute; right: 20px; width: 20px; }

.react-scroll-to-bottom--css-waysj-1tj0vk3:hover { background-color: rgba(0, 0, 0, 0.4); }

.react-scroll-to-bottom--css-waysj-1tj0vk3:active { background-color: rgba(0, 0, 0, 0.6); }
------MultipartBoundary--nIkCly1HUSqFONNTmsMOPIMfV4y6EQjlaITwGKULp4----
Content-Type: text/css
Content-Transfer-Encoding: binary
Content-Location: cid:css-743c9cb2-9e12-4d56-af52-9d78312cad73@mhtml.blink

@charset "utf-8";

.r34K7X1zGgAi6DllVF3T { box-sizing: border-box; border: 0px; margin: 0px; padding: 0px; overflow: hidden; display: none; z-index: 2147483647; pointer-events: none; visibility: hidden; opacity: 0; transition: opacity 300ms linear 0s; height: 0px; width: 0px; }

.r34K7X1zGgAi6DllVF3T.active { display: block; visibility: visible; }

.r34K7X1zGgAi6DllVF3T.active.show { opacity: 1; pointer-events: inherit; position: inherit; }

.r34K7X1zGgAi6DllVF3T.active.show.in-situ { width: inherit; height: inherit; }

.r34K7X1zGgAi6DllVF3T.active.show.lightbox { position: fixed; inset: 0px; width: 100% !important; height: 100% !important; }
------MultipartBoundary--nIkCly1HUSqFONNTmsMOPIMfV4y6EQjlaITwGKULp4----
Content-Type: text/css
Content-Transfer-Encoding: binary
Content-Location: cid:css-c8907c55-48e5-4d75-b1eb-ecde1944b545@mhtml.blink

@charset "utf-8";

.r34K7X1zGgAi6DllVF3T { box-sizing: border-box; border: 0px; margin: 0px; padding: 0px; overflow: hidden; display: none; z-index: 2147483647; pointer-events: none; visibility: hidden; opacity: 0; transition: opacity 300ms linear 0s; height: 0px; width: 0px; }

.r34K7X1zGgAi6DllVF3T.active { display: block; visibility: visible; }

.r34K7X1zGgAi6DllVF3T.active.show { opacity: 1; pointer-events: inherit; position: inherit; }

.r34K7X1zGgAi6DllVF3T.active.show.in-situ { width: inherit; height: inherit; }

.r34K7X1zGgAi6DllVF3T.active.show.lightbox { position: fixed; inset: 0px; width: 100% !important; height: 100% !important; }
------MultipartBoundary--nIkCly1HUSqFONNTmsMOPIMfV4y6EQjlaITwGKULp4----
Content-Type: text/css
Content-Transfer-Encoding: binary
Content-Location: cid:css-24a216d8-f4e5-469b-8178-3c5b8efa2513@mhtml.blink

@charset "utf-8";

@keyframes intercom-lightweight-app-launcher { 
  0% { opacity: 0; transform: scale(0.5); }
  100% { opacity: 1; transform: scale(1); }
}

@keyframes intercom-lightweight-app-gradient { 
  0% { opacity: 0; }
  100% { opacity: 1; }
}

@keyframes intercom-lightweight-app-messenger { 
  0% { opacity: 0; transform: scale(0); }
  40% { opacity: 1; }
  100% { transform: scale(1); }
}

.intercom-lightweight-app { position: fixed; z-index: 2147483001; width: 0px; height: 0px; font-family: intercom-font, "Helvetica Neue", "Apple Color Emoji", Helvetica, Arial, sans-serif; }

.intercom-lightweight-app-gradient { position: fixed; z-index: 2147483002; width: 500px; height: 500px; bottom: 0px; right: 0px; pointer-events: none; background: radial-gradient(at right bottom, rgba(29, 39, 54, 0.16) 0%, rgba(29, 39, 54, 0) 72%); animation: 200ms ease-out 0s 1 normal none running intercom-lightweight-app-gradient; }

.intercom-lightweight-app-launcher { position: fixed; z-index: 2147483003; border: none; bottom: 20px; right: 20px; max-width: 48px; width: 48px; max-height: 48px; height: 48px; border-radius: 50%; background: rgb(32, 33, 35); cursor: pointer; box-shadow: rgba(0, 0, 0, 0.06) 0px 1px 6px 0px, rgba(0, 0, 0, 0.16) 0px 2px 32px 0px; transition: transform 167ms cubic-bezier(0.33, 0, 0, 1) 0s; box-sizing: content-box; padding: 0px !important; margin: 0px !important; }

.intercom-lightweight-app-launcher:hover { transition: transform 250ms cubic-bezier(0.33, 0, 0, 1) 0s; transform: scale(1.1); }

.intercom-lightweight-app-launcher:active { transform: scale(0.85); transition: transform 134ms cubic-bezier(0.45, 0, 0.2, 1) 0s; }

.intercom-lightweight-app-launcher:focus { outline: none; }

.intercom-lightweight-app-launcher-icon { display: flex; align-items: center; justify-content: center; position: absolute; top: 0px; left: 0px; width: 48px; height: 48px; transition: transform 100ms linear 0s, opacity 80ms linear 0s; }

.intercom-lightweight-app-launcher-icon-open { opacity: 1; transform: rotate(0deg) scale(1); }

.intercom-lightweight-app-launcher-icon-open svg { width: 24px; height: 24px; }

.intercom-lightweight-app-launcher-icon-open svg path { fill: rgb(255, 255, 255); }

.intercom-lightweight-app-launcher-icon-self-serve { opacity: 1; transform: rotate(0deg) scale(1); }

.intercom-lightweight-app-launcher-icon-self-serve svg { height: 44px; }

.intercom-lightweight-app-launcher-icon-self-serve svg path { fill: rgb(255, 255, 255); }

.intercom-lightweight-app-launcher-custom-icon-open { max-height: 24px; max-width: 24px; opacity: 1; transform: rotate(0deg) scale(1); }

.intercom-lightweight-app-launcher-icon-minimize { opacity: 0; transform: rotate(-60deg) scale(0); }

.intercom-lightweight-app-launcher-icon-minimize svg path { fill: rgb(255, 255, 255); }

.intercom-lightweight-app-messenger { position: fixed; z-index: 2147483003; overflow: hidden; background-color: white; animation: 250ms cubic-bezier(0, 1, 1, 1) 0s 1 normal none running intercom-lightweight-app-messenger; transform-origin: right bottom; inset: 0px; border-radius: 16px; }

.intercom-lightweight-app-messenger-header { height: 64px; border-bottom: none; background: rgb(32, 33, 35); }

.intercom-lightweight-app-messenger-footer { position: absolute; bottom: 0px; width: 100%; height: 80px; background: rgb(255, 255, 255); font-size: 14px; line-height: 21px; border-top: 1px solid rgba(0, 0, 0, 0.05); box-shadow: rgba(0, 0, 0, 0.05) 0px 0px 25px; }

@media print {
  .intercom-lightweight-app { display: none; }
}
------MultipartBoundary--nIkCly1HUSqFONNTmsMOPIMfV4y6EQjlaITwGKULp4----
Content-Type: text/css
Content-Transfer-Encoding: binary
Content-Location: https://cdn.oaistatic.com/_next/static/css/76140fdd044aa54f.css

@charset "utf-8";

@font-face { font-display: swap; font-family: Signifier; font-style: normal; font-weight: 400; src: url("https://cdn.openai.com/common/fonts/signifier/signifier-light.woff2") format("woff2"); }

@font-face { font-display: swap; font-family: Signifier; font-style: italic; font-weight: 400; src: url("https://cdn.openai.com/common/fonts/signifier/signifier-light-italic.woff2") format("woff2"); }

@font-face { font-display: swap; font-family: Signifier; font-style: normal; font-weight: 700; src: url("https://cdn.openai.com/common/fonts/signifier/signifier-bold.woff2") format("woff2"); }

@font-face { font-display: swap; font-family: Signifier; font-style: italic; font-weight: 700; src: url("https://cdn.openai.com/common/fonts/signifier/signifier-bold-italic.woff2") format("woff2"); }

@font-face { font-display: swap; font-family: SÃ¶hne; font-style: normal; font-weight: 400; src: url("https://cdn.openai.com/common/fonts/soehne/soehne-buch.woff2") format("woff2"); }

@font-face { font-display: swap; font-family: SÃ¶hne; font-style: italic; font-weight: 400; src: url("https://cdn.openai.com/common/fonts/soehne/soehne-buch-kursiv.woff2") format("woff2"); }

@font-face { font-display: swap; font-family: SÃ¶hne; font-style: normal; font-weight: 500; src: url("https://cdn.openai.com/common/fonts/soehne/soehne-kraftig.woff2") format("woff2"); }

@font-face { font-display: swap; font-family: SÃ¶hne; font-style: italic; font-weight: 500; src: url("https://cdn.openai.com/common/fonts/soehne/soehne-kraftig-kursiv.woff2") format("woff2"); }

@font-face { font-display: swap; font-family: SÃ¶hne; font-style: normal; font-weight: 600; src: url("https://cdn.openai.com/common/fonts/soehne/soehne-halbfett.woff2") format("woff2"); }

@font-face { font-display: swap; font-family: SÃ¶hne; font-style: italic; font-weight: 600; src: url("https://cdn.openai.com/common/fonts/soehne/soehne-halbfett-kursiv.woff2") format("woff2"); }

@font-face { font-display: swap; font-family: "SÃ¶hne Mono"; font-style: normal; font-weight: 400; src: url("https://cdn.openai.com/common/fonts/soehne/soehne-mono-buch.woff2") format("woff2"); }

@font-face { font-display: swap; font-family: "SÃ¶hne Mono"; font-style: normal; font-weight: 700; src: url("https://cdn.openai.com/common/fonts/soehne/soehne-mono-halbfett.woff2") format("woff2"); }

@font-face { font-display: swap; font-family: "SÃ¶hne Mono"; font-style: italic; font-weight: 400; src: url("https://cdn.openai.com/common/fonts/soehne/soehne-mono-buch-kursiv.woff2") format("woff2"); }

@font-face { font-display: swap; font-family: "SÃ¶hne Circle"; font-style: normal; font-weight: 400; src: url("https://cdn.openai.com/common/fonts/soehne-circle/soehne-circle-buch.woff2") format("woff2"); }

@font-face { font-display: swap; font-family: "SÃ¶hne Circle"; font-style: normal; font-weight: 600; src: url("https://cdn.openai.com/common/fonts/soehne-circle/soehne-circle-halbfett.woff2") format("woff2"); }

*, ::after, ::before { border: 0px solid rgb(217, 217, 227); box-sizing: border-box; }

::after, ::before { --tw-content: ""; }

html { text-size-adjust: 100%; font-feature-settings: normal; font-family: SÃ¶hne, ui-sans-serif, system-ui, -apple-system, "Segoe UI", Roboto, Ubuntu, Cantarell, "Noto Sans", sans-serif, "Helvetica Neue", Arial, "Apple Color Emoji", "Segoe UI Emoji", "Segoe UI Symbol", "Noto Color Emoji"; font-variation-settings: normal; line-height: 1.5; tab-size: 4; }

body { line-height: inherit; margin: 0px; }

hr { border-top-width: 1px; color: inherit; height: 0px; }

abbr:where([title]) { text-decoration: underline dotted currentcolor; }

h1, h2, h3, h4, h5, h6 { font-size: inherit; font-weight: inherit; }

a { color: inherit; text-decoration: inherit; }

b, strong { font-weight: bolder; }

code, kbd, pre, samp { font-family: "SÃ¶hne Mono", Monaco, "Andale Mono", "Ubuntu Mono", monospace; font-size: 1em; }

small { font-size: 80%; }

sub, sup { font-size: 75%; line-height: 0; position: relative; vertical-align: baseline; }

sub { bottom: -0.25em; }

sup { top: -0.5em; }

table { border-collapse: collapse; border-color: inherit; text-indent: 0px; }

button, input, optgroup, select, textarea { color: inherit; font-family: inherit; font-size: 100%; font-weight: inherit; line-height: inherit; margin: 0px; padding: 0px; }

button, select { text-transform: none; }

[type="button"], [type="reset"], [type="submit"], button { appearance: button; background-color: transparent; background-image: none; }

progress { vertical-align: baseline; }

::-webkit-inner-spin-button, ::-webkit-outer-spin-button { height: auto; }

[type="search"] { appearance: textfield; outline-offset: -2px; }

::-webkit-search-decoration { appearance: none; }

::-webkit-file-upload-button { appearance: button; font: inherit; }

summary { display: list-item; }

blockquote, dd, dl, fieldset, figure, h1, h2, h3, h4, h5, h6, hr, p, pre { margin: 0px; }

fieldset, legend { padding: 0px; }

menu, ol, ul { list-style: none; margin: 0px; padding: 0px; }

textarea { resize: vertical; }

input::-webkit-input-placeholder, textarea::-webkit-input-placeholder { color: rgb(172, 172, 190); }

input::placeholder, textarea::placeholder { color: rgb(172, 172, 190); }

[role="button"], button { cursor: pointer; }

:disabled { cursor: default; }

audio, canvas, embed, iframe, img, object, svg, video { display: block; vertical-align: middle; }

img, video { height: auto; max-width: 100%; }

[hidden] { display: none; }

[multiple], [type="date"], [type="datetime-local"], [type="email"], [type="month"], [type="number"], [type="password"], [type="search"], [type="tel"], [type="text"], [type="time"], [type="url"], [type="week"], select, textarea { --tw-shadow: 0 0 transparent; appearance: none; background-color: rgb(255, 255, 255); border-color: rgb(142, 142, 160); border-radius: 0px; border-width: 1px; font-size: 1rem; line-height: 1.5rem; padding: 0.5rem 0.75rem; }

[multiple]:focus, [type="date"]:focus, [type="datetime-local"]:focus, [type="email"]:focus, [type="month"]:focus, [type="number"]:focus, [type="password"]:focus, [type="search"]:focus, [type="tel"]:focus, [type="text"]:focus, [type="time"]:focus, [type="url"]:focus, [type="week"]:focus, select:focus, textarea:focus { --tw-ring-inset: var(--tw-empty,/*!*/ /*!*/); --tw-ring-offset-width: 0px; --tw-ring-offset-color: #fff; --tw-ring-color: #2563eb; --tw-ring-offset-shadow: var(--tw-ring-inset) 0 0 0 var(--tw-ring-offset-width) var(--tw-ring-offset-color); --tw-ring-shadow: var(--tw-ring-inset) 0 0 0 calc(1px + var(--tw-ring-offset-width)) var(--tw-ring-color); border-color: rgb(37, 99, 235); box-shadow: var(--tw-ring-offset-shadow),var(--tw-ring-shadow),var(--tw-shadow); outline: transparent solid 2px; outline-offset: 2px; }

input::-webkit-input-placeholder, textarea::-webkit-input-placeholder { color: rgb(142, 142, 160); opacity: 1; }

input::placeholder, textarea::placeholder { color: rgb(142, 142, 160); opacity: 1; }

::-webkit-datetime-edit-fields-wrapper { padding: 0px; }

::-webkit-date-and-time-value { min-height: 1.5em; }

::-webkit-datetime-edit, ::-webkit-datetime-edit-day-field, ::-webkit-datetime-edit-hour-field, ::-webkit-datetime-edit-meridiem-field, ::-webkit-datetime-edit-millisecond-field, ::-webkit-datetime-edit-minute-field, ::-webkit-datetime-edit-month-field, ::-webkit-datetime-edit-second-field, ::-webkit-datetime-edit-year-field { padding-bottom: 0px; padding-top: 0px; }

select { background-image: url("data:image/svg+xml;charset=utf-8,%3Csvg xmlns='http://www.w3.org/2000/svg' fill='none' viewBox='0 0 20 20'%3E%3Cpath stroke='%238e8ea0' stroke-linecap='round' stroke-linejoin='round' stroke-width='1.5' d='m6 8 4 4 4-4'/%3E%3C/svg%3E"); background-position: right 0.5rem center; background-repeat: no-repeat; background-size: 1.5em 1.5em; padding-right: 2.5rem; -webkit-print-color-adjust: exact; }

[multiple] { background-image: none; background-position: 0px 0px; background-repeat: repeat; background-size: initial; padding-right: 0.75rem; -webkit-print-color-adjust: inherit; }

[type="checkbox"], [type="radio"] { --tw-shadow: 0 0 transparent; appearance: none; background-color: rgb(255, 255, 255); background-origin: border-box; border-color: rgb(142, 142, 160); border-width: 1px; color: rgb(37, 99, 235); display: inline-block; flex-shrink: 0; height: 1rem; padding: 0px; -webkit-print-color-adjust: exact; user-select: none; vertical-align: middle; width: 1rem; }

[type="checkbox"] { border-radius: 0px; }

[type="radio"] { border-radius: 100%; }

[type="checkbox"]:focus, [type="radio"]:focus { --tw-ring-inset: var(--tw-empty,/*!*/ /*!*/); --tw-ring-offset-width: 2px; --tw-ring-offset-color: #fff; --tw-ring-color: #2563eb; --tw-ring-offset-shadow: var(--tw-ring-inset) 0 0 0 var(--tw-ring-offset-width) var(--tw-ring-offset-color); --tw-ring-shadow: var(--tw-ring-inset) 0 0 0 calc(2px + var(--tw-ring-offset-width)) var(--tw-ring-color); box-shadow: var(--tw-ring-offset-shadow),var(--tw-ring-shadow),var(--tw-shadow); outline: transparent solid 2px; outline-offset: 2px; }

[type="checkbox"]:checked, [type="radio"]:checked { background-color: currentcolor; background-position: 50% center; background-repeat: no-repeat; background-size: 100% 100%; border-color: transparent; }

[type="checkbox"]:checked { background-image: url("data:image/svg+xml;charset=utf-8,%3Csvg viewBox='0 0 16 16' fill='%23fff' xmlns='http://www.w3.org/2000/svg'%3E%3Cpath d='M12.207 4.793a1 1 0 0 1 0 1.414l-5 5a1 1 0 0 1-1.414 0l-2-2a1 1 0 0 1 1.414-1.414L6.5 9.086l4.293-4.293a1 1 0 0 1 1.414 0z'/%3E%3C/svg%3E"); }

[type="radio"]:checked { background-image: url("data:image/svg+xml;charset=utf-8,%3Csvg viewBox='0 0 16 16' fill='%23fff' xmlns='http://www.w3.org/2000/svg'%3E%3Ccircle cx='8' cy='8' r='3'/%3E%3C/svg%3E"); }

[type="checkbox"]:checked:focus, [type="checkbox"]:checked:hover, [type="radio"]:checked:focus, [type="radio"]:checked:hover { background-color: currentcolor; border-color: transparent; }

[type="checkbox"]:indeterminate { background-color: currentcolor; background-image: url("data:image/svg+xml;charset=utf-8,%3Csvg xmlns='http://www.w3.org/2000/svg' fill='none' viewBox='0 0 16 16'%3E%3Cpath stroke='%23fff' stroke-linecap='round' stroke-linejoin='round' stroke-width='2' d='M4 8h8'/%3E%3C/svg%3E"); background-position: 50% center; background-repeat: no-repeat; background-size: 100% 100%; border-color: transparent; }

[type="checkbox"]:indeterminate:focus, [type="checkbox"]:indeterminate:hover { background-color: currentcolor; border-color: transparent; }

[type="file"] { background: initial; border-color: inherit; border-radius: 0px; border-width: 0px; font-size: inherit; line-height: inherit; padding: 0px; }

[type="file"]:focus { outline: -webkit-focus-ring-color auto 1px; }

h1 { font-size: 1.875rem; line-height: 2.25rem; }

h1, h2, h3 { font-weight: 600; }

h2, h3 { font-size: 1.5rem; line-height: 2rem; }

*, ::after, ::before { --tw-border-spacing-x: 0; --tw-border-spacing-y: 0; --tw-translate-x: 0; --tw-translate-y: 0; --tw-rotate: 0; --tw-skew-x: 0; --tw-skew-y: 0; --tw-scale-x: 1; --tw-scale-y: 1; --tw-pan-x: ; --tw-pan-y: ; --tw-pinch-zoom: ; --tw-scroll-snap-strictness: proximity; --tw-gradient-from-position: ; --tw-gradient-via-position: ; --tw-gradient-to-position: ; --tw-ordinal: ; --tw-slashed-zero: ; --tw-numeric-figure: ; --tw-numeric-spacing: ; --tw-numeric-fraction: ; --tw-ring-inset: ; --tw-ring-offset-width: 0px; --tw-ring-offset-color: #fff; --tw-ring-color: rgba(69,89,164,.5); --tw-ring-offset-shadow: 0 0 transparent; --tw-ring-shadow: 0 0 transparent; --tw-shadow: 0 0 transparent; --tw-shadow-colored: 0 0 transparent; --tw-blur: ; --tw-brightness: ; --tw-contrast: ; --tw-grayscale: ; --tw-hue-rotate: ; --tw-invert: ; --tw-saturate: ; --tw-sepia: ; --tw-drop-shadow: ; --tw-backdrop-blur: ; --tw-backdrop-brightness: ; --tw-backdrop-contrast: ; --tw-backdrop-grayscale: ; --tw-backdrop-hue-rotate: ; --tw-backdrop-invert: ; --tw-backdrop-opacity: ; --tw-backdrop-saturate: ; --tw-backdrop-sepia: ; }

::-webkit-backdrop { --tw-border-spacing-x: 0; --tw-border-spacing-y: 0; --tw-translate-x: 0; --tw-translate-y: 0; --tw-rotate: 0; --tw-skew-x: 0; --tw-skew-y: 0; --tw-scale-x: 1; --tw-scale-y: 1; --tw-pan-x: ; --tw-pan-y: ; --tw-pinch-zoom: ; --tw-scroll-snap-strictness: proximity; --tw-gradient-from-position: ; --tw-gradient-via-position: ; --tw-gradient-to-position: ; --tw-ordinal: ; --tw-slashed-zero: ; --tw-numeric-figure: ; --tw-numeric-spacing: ; --tw-numeric-fraction: ; --tw-ring-inset: ; --tw-ring-offset-width: 0px; --tw-ring-offset-color: #fff; --tw-ring-color: rgba(69,89,164,.5); --tw-ring-offset-shadow: 0 0 transparent; --tw-ring-shadow: 0 0 transparent; --tw-shadow: 0 0 transparent; --tw-shadow-colored: 0 0 transparent; --tw-blur: ; --tw-brightness: ; --tw-contrast: ; --tw-grayscale: ; --tw-hue-rotate: ; --tw-invert: ; --tw-saturate: ; --tw-sepia: ; --tw-drop-shadow: ; --tw-backdrop-blur: ; --tw-backdrop-brightness: ; --tw-backdrop-contrast: ; --tw-backdrop-grayscale: ; --tw-backdrop-hue-rotate: ; --tw-backdrop-invert: ; --tw-backdrop-opacity: ; --tw-backdrop-saturate: ; --tw-backdrop-sepia: ; }

::backdrop { --tw-border-spacing-x: 0; --tw-border-spacing-y: 0; --tw-translate-x: 0; --tw-translate-y: 0; --tw-rotate: 0; --tw-skew-x: 0; --tw-skew-y: 0; --tw-scale-x: 1; --tw-scale-y: 1; --tw-pan-x: ; --tw-pan-y: ; --tw-pinch-zoom: ; --tw-scroll-snap-strictness: proximity; --tw-gradient-from-position: ; --tw-gradient-via-position: ; --tw-gradient-to-position: ; --tw-ordinal: ; --tw-slashed-zero: ; --tw-numeric-figure: ; --tw-numeric-spacing: ; --tw-numeric-fraction: ; --tw-ring-inset: ; --tw-ring-offset-width: 0px; --tw-ring-offset-color: #fff; --tw-ring-color: rgba(69,89,164,.5); --tw-ring-offset-shadow: 0 0 transparent; --tw-ring-shadow: 0 0 transparent; --tw-shadow: 0 0 transparent; --tw-shadow-colored: 0 0 transparent; --tw-blur: ; --tw-brightness: ; --tw-contrast: ; --tw-grayscale: ; --tw-hue-rotate: ; --tw-invert: ; --tw-saturate: ; --tw-sepia: ; --tw-drop-shadow: ; --tw-backdrop-blur: ; --tw-backdrop-brightness: ; --tw-backdrop-contrast: ; --tw-backdrop-grayscale: ; --tw-backdrop-hue-rotate: ; --tw-backdrop-invert: ; --tw-backdrop-opacity: ; --tw-backdrop-saturate: ; --tw-backdrop-sepia: ; }

:root { --white: #fff; --black: #000; --gray-50: #f7f7f8; --gray-100: #ececf1; --gray-200: #d9d9e3; --gray-300: #c5c5d2; --gray-400: #acacbe; --gray-500: #8e8ea0; --gray-600: #565869; --gray-700: #40414f; --gray-800: #343541; --gray-900: #202123; --gray-950: #050509; --gizmo-gray-500: #999; --gizmo-gray-600: #666; --gizmo-gray-950: #0f0f0f; }

html { --text-primary: var(--gray-800); --text-secondary: var(--gray-600); --text-tertiary: var(--gray-500); --surface-primary: var(--white); --surface-secondary: var(--gray-50); --surface-tertiary: var(--gray-100); --border-light: var(--gray-100); --border-medium: var(--gray-200); --border-heavy: var(--gray-300); }

.dark { --text-primary: var(--gray-100); --text-secondary: var(--gray-300); --text-tertiary: var(--gray-500); --surface-primary: var(--gray-900); --surface-secondary: var(--gray-800); --surface-tertiary: var(--gray-700); --border-light: var(--gray-700); --border-medium: var(--gray-600); --border-heavy: var(--gray-500); }

.gizmo { --text-primary: var(--gray-800); --text-secondary: var(--gizmo-gray-600); --text-tertiary: var(--gizmo-gray-500); --surface-primary: var(--white); --surface-secondary: var(--gray-50); --surface-tertiary: var(--gray-100); --border-light: rgba(0,0,0,.1); --border-medium: rgba(0,0,0,.15); --border-heavy: rgba(0,0,0,.2); }

.gizmo.dark { --text-primary: var(--gray-100); --text-secondary: var(--gray-300); --text-tertiary: var(--gizmo-gray-500); --surface-primary: var(--gray-900); --surface-secondary: var(--gray-800); --surface-tertiary: var(--gray-700); --border-light: rgba(217,217,227,.1); --border-medium: rgba(217,217,227,.15); --border-heavy: rgba(217,217,227,.2); }

textarea:focus { border-color: inherit; box-shadow: none; outline: none; }

::-webkit-scrollbar { height: 1rem; width: 0.5rem; }

::-webkit-scrollbar:horizontal { height: 0.5rem; width: 1rem; }

::-webkit-scrollbar-track { background-color: transparent; border-radius: 9999px; }

::-webkit-scrollbar-thumb { --tw-border-opacity: 1; background-color: rgba(217, 217, 227, 0.8); border-color: rgba(255,255,255,var(--tw-border-opacity)); border-radius: 9999px; border-width: 1px; }

::-webkit-scrollbar-thumb:hover { --tw-bg-opacity: 1; background-color: rgba(236,236,241,var(--tw-bg-opacity)); }

.dark::-webkit-scrollbar-thumb { --tw-bg-opacity: 1; background-color: rgba(86,88,105,var(--tw-bg-opacity)); }

.dark::-webkit-scrollbar-thumb:hover { --tw-bg-opacity: 1; background-color: rgba(172,172,190,var(--tw-bg-opacity)); }

@media (min-width: 768px) {
  .scrollbar-trigger ::-webkit-scrollbar-thumb { visibility: hidden; }
  .scrollbar-trigger:hover ::-webkit-scrollbar-thumb { visibility: visible; }
}

.container { width: 100%; }

@media (min-width: 640px) {
  .container { max-width: 640px; }
}

@media (min-width: 768px) {
  .container { max-width: 768px; }
}

@media (min-width: 1024px) {
  .container { max-width: 1024px; }
}

@media (min-width: 1280px) {
  .container { max-width: 1280px; }
}

@media (min-width: 1536px) {
  .container { max-width: 1536px; }
}

.prose { color: var(--tw-prose-body); max-width: 65ch; }

.prose :where([class~="lead"]):not(:where([class~="not-prose"] *)) { color: var(--tw-prose-lead); font-size: 1.25em; line-height: 1.6; margin-bottom: 1.2em; margin-top: 1.2em; }

.prose :where(a):not(:where([class~="not-prose"] *)) { color: var(--tw-prose-links); font-weight: 500; text-decoration: underline; }

.prose :where(strong):not(:where([class~="not-prose"] *)) { color: var(--tw-prose-bold); font-weight: 600; }

.prose :where(a strong):not(:where([class~="not-prose"] *)) { color: inherit; }

.prose :where(blockquote strong):not(:where([class~="not-prose"] *)) { color: inherit; }

.prose :where(thead th strong):not(:where([class~="not-prose"] *)) { color: inherit; }

.prose :where(ol):not(:where([class~="not-prose"] *)) { list-style-type: decimal; margin-bottom: 1.25em; margin-top: 1.25em; padding-left: 1.625em; }

.prose :where(ol[type="A"]):not(:where([class~="not-prose"] *)) { list-style-type: upper-alpha; }

.prose :where(ol[type="a"]):not(:where([class~="not-prose"] *)) { list-style-type: lower-alpha; }

.prose :where():not(:where([class~="not-prose"] *)) { list-style-type: upper-alpha; }

.prose :where():not(:where([class~="not-prose"] *)) { list-style-type: lower-alpha; }

.prose :where(ol[type="I"]):not(:where([class~="not-prose"] *)) { list-style-type: upper-roman; }

.prose :where(ol[type="i"]):not(:where([class~="not-prose"] *)) { list-style-type: lower-roman; }

.prose :where():not(:where([class~="not-prose"] *)) { list-style-type: upper-roman; }

.prose :where():not(:where([class~="not-prose"] *)) { list-style-type: lower-roman; }

.prose :where(ol[type="1"]):not(:where([class~="not-prose"] *)) { list-style-type: decimal; }

.prose :where(ul):not(:where([class~="not-prose"] *)) { list-style-type: disc; margin-bottom: 1.25em; margin-top: 1.25em; padding-left: 1.625em; }

.prose :where(ol > li):not(:where([class~="not-prose"] *))::marker { color: var(--tw-prose-counters); font-weight: 400; }

.prose :where(ul > li):not(:where([class~="not-prose"] *))::marker { color: var(--tw-prose-bullets); }

.prose :where(hr):not(:where([class~="not-prose"] *)) { border-color: var(--tw-prose-hr); border-top-width: 1px; margin-bottom: 3em; margin-top: 3em; }

.prose :where(blockquote):not(:where([class~="not-prose"] *)) { border-left-color: var(--tw-prose-quote-borders); border-left-width: 0.25rem; color: var(--tw-prose-quotes); font-style: normal; font-weight: 500; margin-bottom: 1.6em; margin-top: 1.6em; padding-left: 1em; quotes: "â" "â" "â" "â"; }

.prose :where(blockquote p:first-of-type):not(:where([class~="not-prose"] *))::before { content: open-quote; }

.prose :where(blockquote p:last-of-type):not(:where([class~="not-prose"] *))::after { content: close-quote; }

.prose :where(h1):not(:where([class~="not-prose"] *)) { color: var(--tw-prose-headings); font-size: 2.25em; font-weight: 800; line-height: 1.11111; margin-bottom: 0.888889em; margin-top: 0px; }

.prose :where(h1 strong):not(:where([class~="not-prose"] *)) { color: inherit; font-weight: 900; }

.prose :where(h2):not(:where([class~="not-prose"] *)) { color: var(--tw-prose-headings); font-size: 1.5em; font-weight: 700; line-height: 1.33333; margin-bottom: 1em; margin-top: 2em; }

.prose :where(h2 strong):not(:where([class~="not-prose"] *)) { color: inherit; font-weight: 800; }

.prose :where(h3):not(:where([class~="not-prose"] *)) { color: var(--tw-prose-headings); font-size: 1.25em; font-weight: 600; line-height: 1.6; margin-bottom: 0.6em; margin-top: 1.6em; }

.prose :where(h3 strong):not(:where([class~="not-prose"] *)) { color: inherit; font-weight: 700; }

.prose :where(h4):not(:where([class~="not-prose"] *)) { color: var(--tw-prose-headings); font-weight: 600; line-height: 1.5; margin-bottom: 0.5em; margin-top: 1.5em; }

.prose :where(h4 strong):not(:where([class~="not-prose"] *)) { color: inherit; font-weight: 700; }

.prose :where(img):not(:where([class~="not-prose"] *)) { margin-bottom: 2em; margin-top: 2em; }

.prose :where(figure > *):not(:where([class~="not-prose"] *)) { margin-bottom: 0px; margin-top: 0px; }

.prose :where(figcaption):not(:where([class~="not-prose"] *)) { color: var(--tw-prose-captions); font-size: 0.875em; line-height: 1.42857; margin-top: 0.857143em; }

.prose :where(code):not(:where([class~="not-prose"] *)) { color: var(--tw-prose-code); font-size: 0.875em; font-weight: 600; }

.prose :where(code):not(:where([class~="not-prose"] *))::before { content: "`"; }

.prose :where(code):not(:where([class~="not-prose"] *))::after { content: "`"; }

.prose :where(a code):not(:where([class~="not-prose"] *)) { color: inherit; }

.prose :where(h1 code):not(:where([class~="not-prose"] *)) { color: inherit; }

.prose :where(h2 code):not(:where([class~="not-prose"] *)) { color: inherit; font-size: 0.875em; }

.prose :where(h3 code):not(:where([class~="not-prose"] *)) { color: inherit; font-size: 0.9em; }

.prose :where(h4 code):not(:where([class~="not-prose"] *)) { color: inherit; }

.prose :where(blockquote code):not(:where([class~="not-prose"] *)) { color: inherit; }

.prose :where(thead th code):not(:where([class~="not-prose"] *)) { color: inherit; }

.prose :where(pre):not(:where([class~="not-prose"] *)) { background-color: transparent; border-radius: 0.375rem; color: currentcolor; font-size: 0.875em; font-weight: 400; line-height: 1.71429; margin: 0px; overflow-x: auto; padding: 0px; }

.prose :where(pre code):not(:where([class~="not-prose"] *)) { background-color: transparent; border-radius: 0px; border-width: 0px; color: inherit; font-family: inherit; font-size: inherit; font-weight: inherit; line-height: inherit; padding: 0px; }

.prose :where(pre code):not(:where([class~="not-prose"] *))::before { content: none; }

.prose :where(pre code):not(:where([class~="not-prose"] *))::after { content: none; }

.prose :where(table):not(:where([class~="not-prose"] *)) { font-size: 0.875em; line-height: 1.71429; margin-bottom: 2em; margin-top: 2em; table-layout: auto; text-align: left; width: 100%; }

.prose :where(thead):not(:where([class~="not-prose"] *)) { border-bottom-color: var(--tw-prose-th-borders); border-bottom-width: 1px; }

.prose :where(thead th):not(:where([class~="not-prose"] *)) { color: var(--tw-prose-headings); font-weight: 600; padding-bottom: 0.571429em; padding-left: 0.571429em; padding-right: 0.571429em; vertical-align: bottom; }

.prose :where(tbody tr):not(:where([class~="not-prose"] *)) { border-bottom-color: var(--tw-prose-td-borders); border-bottom-width: 1px; }

.prose :where(tbody tr:last-child):not(:where([class~="not-prose"] *)) { border-bottom-width: 0px; }

.prose :where(tbody td):not(:where([class~="not-prose"] *)) { vertical-align: baseline; }

.prose :where(tfoot):not(:where([class~="not-prose"] *)) { border-top-color: var(--tw-prose-th-borders); border-top-width: 1px; }

.prose :where(tfoot td):not(:where([class~="not-prose"] *)) { vertical-align: top; }

.prose { --tw-prose-body: #374151; --tw-prose-headings: #111827; --tw-prose-lead: #4b5563; --tw-prose-links: #111827; --tw-prose-bold: #111827; --tw-prose-counters: #6b7280; --tw-prose-bullets: #d1d5db; --tw-prose-hr: #e5e7eb; --tw-prose-quotes: #111827; --tw-prose-quote-borders: #e5e7eb; --tw-prose-captions: #6b7280; --tw-prose-code: #111827; --tw-prose-pre-code: #e5e7eb; --tw-prose-pre-bg: #1f2937; --tw-prose-th-borders: #d1d5db; --tw-prose-td-borders: #e5e7eb; --tw-prose-invert-body: #d1d5db; --tw-prose-invert-headings: #fff; --tw-prose-invert-lead: #9ca3af; --tw-prose-invert-links: #fff; --tw-prose-invert-bold: #fff; --tw-prose-invert-counters: #9ca3af; --tw-prose-invert-bullets: #4b5563; --tw-prose-invert-hr: #374151; --tw-prose-invert-quotes: #f3f4f6; --tw-prose-invert-quote-borders: #374151; --tw-prose-invert-captions: #9ca3af; --tw-prose-invert-code: #fff; --tw-prose-invert-pre-code: #d1d5db; --tw-prose-invert-pre-bg: rgba(0,0,0,.5); --tw-prose-invert-th-borders: #4b5563; --tw-prose-invert-td-borders: #374151; font-size: 1rem; line-height: 1.75; }

.prose :where(p):not(:where([class~="not-prose"] *)) { margin-bottom: 1.25em; margin-top: 1.25em; }

.prose :where(video):not(:where([class~="not-prose"] *)) { margin-bottom: 2em; margin-top: 2em; }

.prose :where(figure):not(:where([class~="not-prose"] *)) { margin-bottom: 2em; margin-top: 2em; }

.prose :where(li):not(:where([class~="not-prose"] *)) { margin-bottom: 0.5em; margin-top: 0.5em; }

.prose :where(ol > li):not(:where([class~="not-prose"] *)) { padding-left: 0.375em; }

.prose :where(ul > li):not(:where([class~="not-prose"] *)) { padding-left: 0.375em; }

.prose :where(.prose > ul > li p):not(:where([class~="not-prose"] *)) { margin-bottom: 0.75em; margin-top: 0.75em; }

.prose :where(.prose > ul > li > :first-child):not(:where([class~="not-prose"] *)) { margin-top: 1.25em; }

.prose :where(.prose > ul > li > :last-child):not(:where([class~="not-prose"] *)) { margin-bottom: 1.25em; }

.prose :where(.prose > ol > li > :first-child):not(:where([class~="not-prose"] *)) { margin-top: 1.25em; }

.prose :where(.prose > ol > li > :last-child):not(:where([class~="not-prose"] *)) { margin-bottom: 1.25em; }

.prose :where(ul ul, ul ol, ol ul, ol ol):not(:where([class~="not-prose"] *)) { margin-bottom: 0.75em; margin-top: 0.75em; }

.prose :where(hr + *):not(:where([class~="not-prose"] *)) { margin-top: 0px; }

.prose :where(h2 + *):not(:where([class~="not-prose"] *)) { margin-top: 0px; }

.prose :where(h3 + *):not(:where([class~="not-prose"] *)) { margin-top: 0px; }

.prose :where(h4 + *):not(:where([class~="not-prose"] *)) { margin-top: 0px; }

.prose :where(thead th:first-child):not(:where([class~="not-prose"] *)) { padding-left: 0px; }

.prose :where(thead th:last-child):not(:where([class~="not-prose"] *)) { padding-right: 0px; }

.prose :where(tbody td, tfoot td):not(:where([class~="not-prose"] *)) { padding: 0.571429em; }

.prose :where(tbody td:first-child, tfoot td:first-child):not(:where([class~="not-prose"] *)) { padding-left: 0px; }

.prose :where(tbody td:last-child, tfoot td:last-child):not(:where([class~="not-prose"] *)) { padding-right: 0px; }

.prose :where(.prose > :first-child):not(:where([class~="not-prose"] *)) { margin-top: 0px; }

.prose :where(.prose > :last-child):not(:where([class~="not-prose"] *)) { margin-bottom: 0px; }

.prose-sm :where(.prose > ul > li p):not(:where([class~="not-prose"] *)) { margin-bottom: 0.571429em; margin-top: 0.571429em; }

.prose-sm :where(.prose > ul > li > :first-child):not(:where([class~="not-prose"] *)) { margin-top: 1.14286em; }

.prose-sm :where(.prose > ul > li > :last-child):not(:where([class~="not-prose"] *)) { margin-bottom: 1.14286em; }

.prose-sm :where(.prose > ol > li > :first-child):not(:where([class~="not-prose"] *)) { margin-top: 1.14286em; }

.prose-sm :where(.prose > ol > li > :last-child):not(:where([class~="not-prose"] *)) { margin-bottom: 1.14286em; }

.prose-sm :where(.prose > :first-child):not(:where([class~="not-prose"] *)) { margin-top: 0px; }

.prose-sm :where(.prose > :last-child):not(:where([class~="not-prose"] *)) { margin-bottom: 0px; }

.prose-base :where(.prose > ul > li p):not(:where([class~="not-prose"] *)) { margin-bottom: 0.75em; margin-top: 0.75em; }

.prose-base :where(.prose > ul > li > :first-child):not(:where([class~="not-prose"] *)) { margin-top: 1.25em; }

.prose-base :where(.prose > ul > li > :last-child):not(:where([class~="not-prose"] *)) { margin-bottom: 1.25em; }

.prose-base :where(.prose > ol > li > :first-child):not(:where([class~="not-prose"] *)) { margin-top: 1.25em; }

.prose-base :where(.prose > ol > li > :last-child):not(:where([class~="not-prose"] *)) { margin-bottom: 1.25em; }

.prose-base :where(.prose > :first-child):not(:where([class~="not-prose"] *)) { margin-top: 0px; }

.prose-base :where(.prose > :last-child):not(:where([class~="not-prose"] *)) { margin-bottom: 0px; }

.prose-lg :where(.prose > ul > li p):not(:where([class~="not-prose"] *)) { margin-bottom: 0.888889em; margin-top: 0.888889em; }

.prose-lg :where(.prose > ul > li > :first-child):not(:where([class~="not-prose"] *)) { margin-top: 1.33333em; }

.prose-lg :where(.prose > ul > li > :last-child):not(:where([class~="not-prose"] *)) { margin-bottom: 1.33333em; }

.prose-lg :where(.prose > ol > li > :first-child):not(:where([class~="not-prose"] *)) { margin-top: 1.33333em; }

.prose-lg :where(.prose > ol > li > :last-child):not(:where([class~="not-prose"] *)) { margin-bottom: 1.33333em; }

.prose-lg :where(.prose > :first-child):not(:where([class~="not-prose"] *)) { margin-top: 0px; }

.prose-lg :where(.prose > :last-child):not(:where([class~="not-prose"] *)) { margin-bottom: 0px; }

.prose-xl :where(.prose > ul > li p):not(:where([class~="not-prose"] *)) { margin-bottom: 0.8em; margin-top: 0.8em; }

.prose-xl :where(.prose > ul > li > :first-child):not(:where([class~="not-prose"] *)) { margin-top: 1.2em; }

.prose-xl :where(.prose > ul > li > :last-child):not(:where([class~="not-prose"] *)) { margin-bottom: 1.2em; }

.prose-xl :where(.prose > ol > li > :first-child):not(:where([class~="not-prose"] *)) { margin-top: 1.2em; }

.prose-xl :where(.prose > ol > li > :last-child):not(:where([class~="not-prose"] *)) { margin-bottom: 1.2em; }

.prose-xl :where(.prose > :first-child):not(:where([class~="not-prose"] *)) { margin-top: 0px; }

.prose-xl :where(.prose > :last-child):not(:where([class~="not-prose"] *)) { margin-bottom: 0px; }

.prose-2xl :where(.prose > ul > li p):not(:where([class~="not-prose"] *)) { margin-bottom: 0.833333em; margin-top: 0.833333em; }

.prose-2xl :where(.prose > ul > li > :first-child):not(:where([class~="not-prose"] *)) { margin-top: 1.33333em; }

.prose-2xl :where(.prose > ul > li > :last-child):not(:where([class~="not-prose"] *)) { margin-bottom: 1.33333em; }

.prose-2xl :where(.prose > ol > li > :first-child):not(:where([class~="not-prose"] *)) { margin-top: 1.33333em; }

.prose-2xl :where(.prose > ol > li > :last-child):not(:where([class~="not-prose"] *)) { margin-bottom: 1.33333em; }

.prose-2xl :where(.prose > :first-child):not(:where([class~="not-prose"] *)) { margin-top: 0px; }

.prose-2xl :where(.prose > :last-child):not(:where([class~="not-prose"] *)) { margin-bottom: 0px; }

.form-input, .form-multiselect, .form-select, .form-textarea { --tw-shadow: 0 0 transparent; appearance: none; background-color: rgb(255, 255, 255); border-color: rgb(142, 142, 160); border-radius: 0px; border-width: 1px; font-size: 1rem; line-height: 1.5rem; padding: 0.5rem 0.75rem; }

.form-input:focus, .form-multiselect:focus, .form-select:focus, .form-textarea:focus { --tw-ring-inset: var(--tw-empty,/*!*/ /*!*/); --tw-ring-offset-width: 0px; --tw-ring-offset-color: #fff; --tw-ring-color: #2563eb; --tw-ring-offset-shadow: var(--tw-ring-inset) 0 0 0 var(--tw-ring-offset-width) var(--tw-ring-offset-color); --tw-ring-shadow: var(--tw-ring-inset) 0 0 0 calc(1px + var(--tw-ring-offset-width)) var(--tw-ring-color); border-color: rgb(37, 99, 235); box-shadow: var(--tw-ring-offset-shadow),var(--tw-ring-shadow),var(--tw-shadow); outline: transparent solid 2px; outline-offset: 2px; }

.form-input::-webkit-input-placeholder, .form-textarea::-webkit-input-placeholder { color: rgb(142, 142, 160); opacity: 1; }

.form-input::placeholder, .form-textarea::placeholder { color: rgb(142, 142, 160); opacity: 1; }

.form-input::-webkit-datetime-edit-fields-wrapper { padding: 0px; }

.form-input::-webkit-date-and-time-value { min-height: 1.5em; }

.form-input::-webkit-datetime-edit, .form-input::-webkit-datetime-edit-day-field, .form-input::-webkit-datetime-edit-hour-field, .form-input::-webkit-datetime-edit-meridiem-field, .form-input::-webkit-datetime-edit-millisecond-field, .form-input::-webkit-datetime-edit-minute-field, .form-input::-webkit-datetime-edit-month-field, .form-input::-webkit-datetime-edit-second-field, .form-input::-webkit-datetime-edit-year-field { padding-bottom: 0px; padding-top: 0px; }

.btn { align-items: center; border-color: transparent; border-radius: 0.25rem; border-width: 1px; display: inline-flex; font-size: 0.875rem; line-height: 1.25rem; padding: 0.5rem 0.75rem; pointer-events: auto; }

.btn:focus { outline: transparent solid 2px; outline-offset: 2px; }

.btn:disabled { cursor: not-allowed; opacity: 0.5; }

.btn-primary { --tw-bg-opacity: 1; --tw-text-opacity: 1; background-color: rgba(16,163,127,var(--tw-bg-opacity)); color: rgba(255,255,255,var(--tw-text-opacity)); }

.btn-primary:hover { --tw-bg-opacity: 1; background-color: rgba(26,127,100,var(--tw-bg-opacity)); }

.btn-primary:focus { --tw-ring-offset-shadow: var(--tw-ring-inset) 0 0 0 var(--tw-ring-offset-width) var(--tw-ring-offset-color); --tw-ring-shadow: var(--tw-ring-inset) 0 0 0 calc(2px + var(--tw-ring-offset-width)) var(--tw-ring-color); --tw-ring-offset-width: 2px; box-shadow: var(--tw-ring-offset-shadow),var(--tw-ring-shadow),var(--tw-shadow,0 0 transparent); }

.btn-primary.focus-visible { --tw-ring-opacity: 1; --tw-ring-color: rgba(25,195,125,var(--tw-ring-opacity)); }

.btn-primary:focus-visible { --tw-ring-opacity: 1; --tw-ring-color: rgba(25,195,125,var(--tw-ring-opacity)); }

.btn-danger { --tw-bg-opacity: 1; --tw-text-opacity: 1; background-color: rgba(185,28,28,var(--tw-bg-opacity)); color: rgba(255,255,255,var(--tw-text-opacity)); }

.btn-danger:hover { --tw-bg-opacity: 1; background-color: rgba(153,27,27,var(--tw-bg-opacity)); }

.btn-danger:focus { --tw-ring-offset-shadow: var(--tw-ring-inset) 0 0 0 var(--tw-ring-offset-width) var(--tw-ring-offset-color); --tw-ring-shadow: var(--tw-ring-inset) 0 0 0 calc(2px + var(--tw-ring-offset-width)) var(--tw-ring-color); --tw-ring-offset-width: 2px; box-shadow: var(--tw-ring-offset-shadow),var(--tw-ring-shadow),var(--tw-shadow,0 0 transparent); }

.btn-danger.focus-visible { --tw-ring-opacity: 1; --tw-ring-color: rgba(220,38,38,var(--tw-ring-opacity)); }

.btn-danger:focus-visible { --tw-ring-opacity: 1; --tw-ring-color: rgba(220,38,38,var(--tw-ring-opacity)); }

.btn-danger:disabled:hover { --tw-bg-opacity: 1; background-color: rgba(185,28,28,var(--tw-bg-opacity)); }

.btn-danger-outline { --tw-border-opacity: 1; --tw-text-opacity: 1; border-color: rgba(185,28,28,var(--tw-border-opacity)); border-width: 1px; color: rgba(185,28,28,var(--tw-text-opacity)); }

.btn-danger-outline:focus { --tw-ring-offset-shadow: var(--tw-ring-inset) 0 0 0 var(--tw-ring-offset-width) var(--tw-ring-offset-color); --tw-ring-shadow: var(--tw-ring-inset) 0 0 0 calc(2px + var(--tw-ring-offset-width)) var(--tw-ring-color); --tw-ring-offset-width: 2px; box-shadow: var(--tw-ring-offset-shadow),var(--tw-ring-shadow),var(--tw-shadow,0 0 transparent); }

.btn-danger-outline.focus-visible { --tw-ring-opacity: 1; --tw-ring-color: rgba(220,38,38,var(--tw-ring-opacity)); }

.btn-danger-outline:focus-visible { --tw-ring-opacity: 1; --tw-ring-color: rgba(220,38,38,var(--tw-ring-opacity)); }

.btn-neutral { --tw-bg-opacity: 1; --tw-text-opacity: 1; background-color: rgba(255,255,255,var(--tw-bg-opacity)); border-color: rgba(0, 0, 0, 0.1); border-width: 1px; color: rgba(64,65,79,var(--tw-text-opacity)); font-size: 0.875rem; line-height: 1.25rem; }

.btn-neutral:hover { --tw-bg-opacity: 1; background-color: rgba(236,236,241,var(--tw-bg-opacity)); }

.btn-neutral:focus { --tw-ring-offset-shadow: var(--tw-ring-inset) 0 0 0 var(--tw-ring-offset-width) var(--tw-ring-offset-color); --tw-ring-shadow: var(--tw-ring-inset) 0 0 0 calc(2px + var(--tw-ring-offset-width)) var(--tw-ring-color); --tw-ring-offset-width: 2px; box-shadow: var(--tw-ring-offset-shadow),var(--tw-ring-shadow),var(--tw-shadow,0 0 transparent); }

.btn-neutral.focus-visible { --tw-ring-opacity: 1; --tw-ring-color: rgba(99,102,241,var(--tw-ring-opacity)); }

.btn-neutral:focus-visible { --tw-ring-opacity: 1; --tw-ring-color: rgba(99,102,241,var(--tw-ring-opacity)); }

.dark .btn-neutral { --tw-border-opacity: 1; --tw-bg-opacity: 1; --tw-text-opacity: 1; background-color: rgba(52,53,65,var(--tw-bg-opacity)); border-color: rgba(86,88,105,var(--tw-border-opacity)); color: rgba(217,217,227,var(--tw-text-opacity)); }

.dark .btn-neutral:hover { --tw-bg-opacity: 1; background-color: rgba(64,65,79,var(--tw-bg-opacity)); }

.btn-dark { --tw-border-opacity: 1; --tw-bg-opacity: 1; --tw-text-opacity: 1; background-color: rgba(52,53,65,var(--tw-bg-opacity)); border-color: rgba(86,88,105,var(--tw-border-opacity)); border-width: 1px; color: rgba(255,255,255,var(--tw-text-opacity)); }

.btn-dark:hover { --tw-bg-opacity: 1; background-color: rgba(64,65,79,var(--tw-bg-opacity)); }

.btn-light { --tw-bg-opacity: 1; --tw-text-opacity: 1; background-color: rgba(236,236,241,var(--tw-bg-opacity)); color: rgba(0,0,0,var(--tw-text-opacity)); }

.btn-light:hover { --tw-bg-opacity: 1; background-color: rgba(255,255,255,var(--tw-bg-opacity)); }

.btn-disabled { --tw-bg-opacity: 1; --tw-text-opacity: 1; color: rgba(142,142,160,var(--tw-text-opacity)); cursor: not-allowed; }

.btn-disabled, .btn-disabled:hover { background-color: rgba(236,236,241,var(--tw-bg-opacity)); }

.btn-disabled:hover { --tw-bg-opacity: 1; }

.btn-small { padding: 0.25rem 0.5rem; }

.hide-scrollbar { }

.hide-scrollbar ::-webkit-scrollbar { display: none; }

.sr-only { clip: rect(0px, 0px, 0px, 0px); border-width: 0px; height: 1px; margin: -1px; overflow: hidden; padding: 0px; position: absolute; white-space: nowrap; width: 1px; }

.pointer-events-none { pointer-events: none; }

.pointer-events-auto { pointer-events: auto; }

.visible { visibility: visible; }

.\!invisible { visibility: hidden !important; }

.invisible { visibility: hidden; }

.collapse { visibility: collapse; }

.static { position: static; }

.fixed { position: fixed; }

.absolute { position: absolute; }

.relative { position: relative; }

.\!sticky { position: sticky !important; }

.sticky { position: sticky; }

.inset-0 { inset: 0px; }

.inset-12 { inset: 3rem; }

.inset-y-0 { bottom: 0px; top: 0px; }

.inset-y-1\/2 { bottom: 50%; top: 50%; }

.-left-1 { left: -0.25rem; }

.-left-2 { left: -0.5rem; }

.-top-0 { top: 0px; }

.-top-0\.5 { top: -0.125rem; }

.-top-1 { top: -0.25rem; }

.-top-16 { top: -4rem; }

.-top-\[1px\] { top: -1px; }

.bottom-0 { bottom: 0px; }

.bottom-1 { bottom: 0.25rem; }

.bottom-1\.5 { bottom: 0.375rem; }

.bottom-2 { bottom: 0.5rem; }

.bottom-2\.5 { bottom: 0.625rem; }

.bottom-3 { bottom: 0.75rem; }

.bottom-4 { bottom: 1rem; }

.bottom-5 { bottom: 1.25rem; }

.bottom-\[124px\] { bottom: 124px; }

.bottom-full { bottom: 100%; }

.left-0 { left: 0px; }

.left-0\.5 { left: 0.125rem; }

.left-1 { left: 0.25rem; }

.left-1\.5 { left: 0.375rem; }

.left-1\/2 { left: 50%; }

.left-16 { left: 4rem; }

.left-2 { left: 0.5rem; }

.left-3 { left: 0.75rem; }

.left-4 { left: 1rem; }

.left-5 { left: 1.25rem; }

.left-\[7px\] { left: 7px; }

.left-full { left: 100%; }

.right-0 { right: 0px; }

.right-1 { right: 0.25rem; }

.right-1\/2 { right: 50%; }

.right-16 { right: 4rem; }

.right-2 { right: 0.5rem; }

.right-3 { right: 0.75rem; }

.right-32 { right: 8rem; }

.right-4 { right: 1rem; }

.right-6 { right: 1.5rem; }

.top-0 { top: 0px; }

.top-1 { top: 0.25rem; }

.top-1\/2 { top: 50%; }

.top-11 { top: 2.75rem; }

.top-14 { top: 3.5rem; }

.top-2 { top: 0.5rem; }

.top-3 { top: 0.75rem; }

.top-4 { top: 1rem; }

.top-8 { top: 2rem; }

.top-\[-3px\] { top: -3px; }

.top-\[-6px\] { top: -6px; }

.top-\[20px\] { top: 20px; }

.top-full { top: 100%; }

.-z-0, .z-0 { z-index: 0; }

.z-10 { z-index: 10; }

.z-20 { z-index: 20; }

.z-40 { z-index: 40; }

.z-50 { z-index: 50; }

.z-\[14\] { z-index: 14; }

.z-\[15\] { z-index: 15; }

.z-\[16\] { z-index: 16; }

.z-\[17\] { z-index: 17; }

.z-\[2\] { z-index: 2; }

.z-\[60\] { z-index: 60; }

.col-auto { grid-column: auto / auto; }

.col-span-1 { grid-column: span 1 / span 1; }

.col-start-2 { grid-column-start: 2; }

.row-auto { grid-row: auto / auto; }

.row-span-4 { grid-row: span 4 / span 4; }

.row-start-2 { grid-row-start: 2; }

.float-left { float: left; }

.m-0 { margin: 0px; }

.m-0\.5 { margin: 0.125rem; }

.m-1 { margin: 0.25rem; }

.m-2 { margin: 0.5rem; }

.m-4 { margin: 1rem; }

.m-6 { margin: 1.5rem; }

.m-auto { margin: auto; }

.-mx-4 { margin-left: -1rem; margin-right: -1rem; }

.-my-0 { margin-bottom: 0px; margin-top: 0px; }

.-my-0\.5 { margin-bottom: -0.125rem; margin-top: -0.125rem; }

.-my-1 { margin-bottom: -0.25rem; margin-top: -0.25rem; }

.-my-2 { margin-bottom: -0.5rem; margin-top: -0.5rem; }

.mx-1 { margin-left: 0.25rem; margin-right: 0.25rem; }

.mx-2 { margin-left: 0.5rem; margin-right: 0.5rem; }

.mx-3 { margin-left: 0.75rem; margin-right: 0.75rem; }

.mx-4 { margin-left: 1rem; margin-right: 1rem; }

.mx-6 { margin-left: 1.5rem; margin-right: 1.5rem; }

.mx-8 { margin-left: 2rem; margin-right: 2rem; }

.mx-auto { margin-left: auto; margin-right: auto; }

.my-0 { margin-bottom: 0px; margin-top: 0px; }

.my-0\.5 { margin-bottom: 0.125rem; margin-top: 0.125rem; }

.my-1 { margin-bottom: 0.25rem; margin-top: 0.25rem; }

.my-1\.5 { margin-bottom: 0.375rem; margin-top: 0.375rem; }

.my-10 { margin-bottom: 2.5rem; margin-top: 2.5rem; }

.my-12 { margin-bottom: 3rem; margin-top: 3rem; }

.my-2 { margin-bottom: 0.5rem; margin-top: 0.5rem; }

.my-3 { margin-bottom: 0.75rem; margin-top: 0.75rem; }

.my-4 { margin-bottom: 1rem; margin-top: 1rem; }

.my-6 { margin-bottom: 1.5rem; margin-top: 1.5rem; }

.my-8 { margin-bottom: 2rem; margin-top: 2rem; }

.my-auto { margin-bottom: auto; margin-top: auto; }

.-ml-0 { margin-left: 0px; }

.-ml-0\.5 { margin-left: -0.125rem; }

.-ml-1 { margin-left: -0.25rem; }

.-ml-2 { margin-left: -0.5rem; }

.-ml-4 { margin-left: -1rem; }

.-ml-\[1px\] { margin-left: -1px; }

.-ml-\[8px\] { margin-left: -8px; }

.-mr-1 { margin-right: -0.25rem; }

.-mr-12 { margin-right: -3rem; }

.-mr-2 { margin-right: -0.5rem; }

.-mt-0 { margin-top: 0px; }

.-mt-0\.5 { margin-top: -0.125rem; }

.-mt-\[0\.75px\] { margin-top: -0.75px; }

.-mt-\[3px\] { margin-top: -3px; }

.mb-0 { margin-bottom: 0px; }

.mb-0\.5 { margin-bottom: 0.125rem; }

.mb-1 { margin-bottom: 0.25rem; }

.mb-10 { margin-bottom: 2.5rem; }

.mb-12 { margin-bottom: 3rem; }

.mb-16 { margin-bottom: 4rem; }

.mb-2 { margin-bottom: 0.5rem; }

.mb-3 { margin-bottom: 0.75rem; }

.mb-4 { margin-bottom: 1rem; }

.mb-5 { margin-bottom: 1.25rem; }

.mb-6 { margin-bottom: 1.5rem; }

.mb-7 { margin-bottom: 1.75rem; }

.mb-8 { margin-bottom: 2rem; }

.mb-auto { margin-bottom: auto; }

.ml-0 { margin-left: 0px; }

.ml-0\.5 { margin-left: 0.125rem; }

.ml-1 { margin-left: 0.25rem; }

.ml-12 { margin-left: 3rem; }

.ml-2 { margin-left: 0.5rem; }

.ml-3 { margin-left: 0.75rem; }

.ml-4 { margin-left: 1rem; }

.ml-5 { margin-left: 1.25rem; }

.ml-auto { margin-left: auto; }

.mr-0 { margin-right: 0px; }

.mr-1 { margin-right: 0.25rem; }

.mr-2 { margin-right: 0.5rem; }

.mr-3 { margin-right: 0.75rem; }

.mr-4 { margin-right: 1rem; }

.mr-8 { margin-right: 2rem; }

.mr-auto { margin-right: auto; }

.mt-0 { margin-top: 0px; }

.mt-0\.5 { margin-top: 0.125rem; }

.mt-1 { margin-top: 0.25rem; }

.mt-10 { margin-top: 2.5rem; }

.mt-2 { margin-top: 0.5rem; }

.mt-3 { margin-top: 0.75rem; }

.mt-4 { margin-top: 1rem; }

.mt-5 { margin-top: 1.25rem; }

.mt-6 { margin-top: 1.5rem; }

.mt-8 { margin-top: 2rem; }

.mt-auto { margin-top: auto; }

.line-clamp-1 { -webkit-line-clamp: 1; }

.line-clamp-1, .line-clamp-2 { -webkit-box-orient: vertical; display: -webkit-box; overflow: hidden; }

.line-clamp-2 { -webkit-line-clamp: 2; }

.line-clamp-3 { -webkit-line-clamp: 3; }

.line-clamp-3, .line-clamp-5 { -webkit-box-orient: vertical; display: -webkit-box; overflow: hidden; }

.line-clamp-5 { -webkit-line-clamp: 5; }

.line-clamp-6 { -webkit-box-orient: vertical; -webkit-line-clamp: 6; display: -webkit-box; overflow: hidden; }

.block { display: block; }

.inline-block { display: inline-block; }

.inline { display: inline; }

.flex { display: flex; }

.inline-flex { display: inline-flex; }

.table { display: table; }

.flow-root { display: flow-root; }

.grid { display: grid; }

.contents { display: contents; }

.hidden { display: none; }

.aspect-\[4\/7\] { aspect-ratio: 4 / 7; }

.aspect-\[7\/4\] { aspect-ratio: 7 / 4; }

.aspect-square { aspect-ratio: 1 / 1; }

.h-0 { height: 0px; }

.h-1 { height: 0.25rem; }

.h-1\.5 { height: 0.375rem; }

.h-10 { height: 2.5rem; }

.h-11 { height: 2.75rem; }

.h-12 { height: 3rem; }

.h-14 { height: 3.5rem; }

.h-16 { height: 4rem; }

.h-2 { height: 0.5rem; }

.h-2\.5 { height: 0.625rem; }

.h-2\/3 { height: 66.6667%; }

.h-20 { height: 5rem; }

.h-24 { height: 6rem; }

.h-3 { height: 0.75rem; }

.h-3\.5 { height: 0.875rem; }

.h-32 { height: 8rem; }

.h-36 { height: 9rem; }

.h-4 { height: 1rem; }

.h-40 { height: 10rem; }

.h-44 { height: 11rem; }

.h-5 { height: 1.25rem; }

.h-52 { height: 13rem; }

.h-6 { height: 1.5rem; }

.h-7 { height: 1.75rem; }

.h-8 { height: 2rem; }

.h-9 { height: 2.25rem; }

.h-\[1200px\] { height: 1200px; }

.h-\[12px\] { height: 12px; }

.h-\[14px\] { height: 14px; }

.h-\[15px\] { height: 15px; }

.h-\[18px\] { height: 18px; }

.h-\[19px\] { height: 19px; }

.h-\[21px\] { height: 21px; }

.h-\[22px\] { height: 22px; }

.h-\[23px\] { height: 23px; }

.h-\[25px\] { height: 25px; }

.h-\[30px\] { height: 30px; }

.h-\[34px\] { height: 34px; }

.h-\[36px\] { height: 36px; }

.h-\[404px\] { height: 404px; }

.h-\[42px\] { height: 42px; }

.h-\[50px\] { height: 50px; }

.h-\[60px\] { height: 60px; }

.h-\[70px\] { height: 70px; }

.h-\[70vh\] { height: 70vh; }

.h-auto { height: auto; }

.h-fit { height: fit-content; }

.h-full { height: 100%; }

.h-px { height: 1px; }

.h-screen { height: 100vh; }

.\!max-h-\[100vh\] { max-height: 100vh !important; }

.max-h-0 { max-height: 0px; }

.max-h-5 { max-height: 1.25rem; }

.max-h-60 { max-height: 15rem; }

.max-h-64 { max-height: 16rem; }

.max-h-72 { max-height: 18rem; }

.max-h-\[28rem\] { max-height: 28rem; }

.max-h-\[450px\] { max-height: 450px; }

.max-h-\[50vh\] { max-height: 50vh; }

.max-h-\[75vh\] { max-height: 75vh; }

.max-h-\[80vh\] { max-height: 80vh; }

.max-h-\[85vh\] { max-height: 85vh; }

.max-h-\[calc\(100vh-300px\)\] { max-height: calc(100vh - 300px); }

.min-h-0 { min-height: 0px; }

.min-h-\[1\.5rem\] { min-height: 1.5rem; }

.min-h-\[200px\] { min-height: 200px; }

.min-h-\[20px\] { min-height: 20px; }

.min-h-\[24px\] { min-height: 24px; }

.min-h-\[40px\] { min-height: 40px; }

.min-h-\[44px\] { min-height: 44px; }

.min-h-\[4rem\] { min-height: 4rem; }

.min-h-\[60px\] { min-height: 60px; }

.min-h-\[71px\] { min-height: 71px; }

.min-h-fit { min-height: fit-content; }

.min-h-full { min-height: 100%; }

.min-h-screen { min-height: 100vh; }

.w-0 { width: 0px; }

.w-1 { width: 0.25rem; }

.w-1\.5 { width: 0.375rem; }

.w-1\/2 { width: 50%; }

.w-1\/3 { width: 33.3333%; }

.w-10 { width: 2.5rem; }

.w-10\/12 { width: 83.3333%; }

.w-11 { width: 2.75rem; }

.w-12 { width: 3rem; }

.w-14 { width: 3.5rem; }

.w-16 { width: 4rem; }

.w-2 { width: 0.5rem; }

.w-2\/3 { width: 66.6667%; }

.w-20 { width: 5rem; }

.w-28 { width: 7rem; }

.w-3 { width: 0.75rem; }

.w-3\.5 { width: 0.875rem; }

.w-3\/5 { width: 60%; }

.w-32 { width: 8rem; }

.w-4 { width: 1rem; }

.w-40 { width: 10rem; }

.w-48 { width: 12rem; }

.w-5 { width: 1.25rem; }

.w-6 { width: 1.5rem; }

.w-60 { width: 15rem; }

.w-7 { width: 1.75rem; }

.w-8 { width: 2rem; }

.w-80 { width: 20rem; }

.w-9 { width: 2.25rem; }

.w-96 { width: 24rem; }

.w-\[100px\] { width: 100px; }

.w-\[103px\] { width: 103px; }

.w-\[120px\] { width: 120px; }

.w-\[15px\] { width: 15px; }

.w-\[184px\] { width: 184px; }

.w-\[18px\] { width: 18px; }

.w-\[2\.5px\] { width: 2.5px; }

.w-\[209px\] { width: 209px; }

.w-\[218px\] { width: 218px; }

.w-\[21px\] { width: 21px; }

.w-\[23px\] { width: 23px; }

.w-\[25vw\] { width: 25vw; }

.w-\[260px\] { width: 260px; }

.w-\[280px\] { width: 280px; }

.w-\[30px\] { width: 30px; }

.w-\[34px\] { width: 34px; }

.w-\[38px\] { width: 38px; }

.w-\[42px\] { width: 42px; }

.w-\[70px\] { width: 70px; }

.w-\[860px\] { width: 860px; }

.w-\[88px\] { width: 88px; }

.w-\[960px\] { width: 960px; }

.w-\[calc\(100\%-50px\)\] { width: calc(100% - 50px); }

.w-auto { width: auto; }

.w-fit { width: fit-content; }

.w-full { width: 100%; }

.w-min { width: min-content; }

.w-screen { width: 100vw; }

.min-w-0 { min-width: 0px; }

.min-w-\[100px\] { min-width: 100px; }

.min-w-\[100vw\] { min-width: 100vw; }

.min-w-\[175px\] { min-width: 175px; }

.min-w-\[180px\] { min-width: 180px; }

.min-w-\[220px\] { min-width: 220px; }

.min-w-\[256px\] { min-width: 256px; }

.min-w-\[280px\] { min-width: 280px; }

.min-w-\[300px\] { min-width: 300px; }

.min-w-\[32px\] { min-width: 32px; }

.min-w-\[50px\] { min-width: 50px; }

.min-w-\[8rem\] { min-width: 8rem; }

.min-w-full { min-width: 100%; }

.\!max-w-\[80vw\] { max-width: 80vw !important; }

.max-w-3xl { max-width: 48rem; }

.max-w-5xl { max-width: 64rem; }

.max-w-6xl { max-width: 72rem; }

.max-w-7xl { max-width: 80rem; }

.max-w-\[100vw\] { max-width: 100vw; }

.max-w-\[160px\] { max-width: 160px; }

.max-w-\[200px\] { max-width: 200px; }

.max-w-\[220px\] { max-width: 220px; }

.max-w-\[320px\] { max-width: 320px; }

.max-w-\[350px\] { max-width: 350px; }

.max-w-\[372px\] { max-width: 372px; }

.max-w-\[400px\] { max-width: 400px; }

.max-w-\[440px\] { max-width: 440px; }

.max-w-\[480px\] { max-width: 480px; }

.max-w-\[550px\] { max-width: 550px; }

.max-w-\[560px\] { max-width: 560px; }

.max-w-\[650px\] { max-width: 650px; }

.max-w-\[90vw\] { max-width: 90vw; }

.max-w-\[calc\(100vw-3rem\)\] { max-width: calc(100vw - 3rem); }

.max-w-full { max-width: 100%; }

.max-w-lg { max-width: 32rem; }

.max-w-md { max-width: 28rem; }

.max-w-sm { max-width: 24rem; }

.max-w-xl { max-width: 36rem; }

.max-w-xs { max-width: 20rem; }

.flex-1 { flex: 1 1 0%; }

.flex-auto { flex: 1 1 auto; }

.flex-none { flex: 0 0 auto; }

.flex-shrink { flex-shrink: 1; }

.flex-shrink-0 { flex-shrink: 0; }

.shrink { flex-shrink: 1; }

.shrink-0 { flex-shrink: 0; }

.flex-grow { flex-grow: 1; }

.flex-grow-0 { flex-grow: 0; }

.grow { flex-grow: 1; }

.table-auto { table-layout: auto; }

.table-fixed { table-layout: fixed; }

.border-separate { border-collapse: separate; }

.border-spacing-0 { --tw-border-spacing-x: 0px; --tw-border-spacing-y: 0px; border-spacing: var(--tw-border-spacing-x) var(--tw-border-spacing-y); }

.origin-\[50\%_50\%\] { transform-origin: 50% 50%; }

.-translate-x-1\/2 { --tw-translate-x: -50%; }

.-translate-x-1\/2, .-translate-x-\[calc\(100\%\+4px\)\] { transform: translate(var(--tw-translate-x),var(--tw-translate-y)) rotate(var(--tw-rotate)) skewX(var(--tw-skew-x)) skewY(var(--tw-skew-y)) scaleX(var(--tw-scale-x)) scaleY(var(--tw-scale-y)); }

.-translate-x-\[calc\(100\%\+4px\)\] { --tw-translate-x: calc(-100% + -4px); }

.-translate-x-full { --tw-translate-x: -100%; }

.-translate-x-full, .-translate-y-1\/2 { transform: translate(var(--tw-translate-x),var(--tw-translate-y)) rotate(var(--tw-rotate)) skewX(var(--tw-skew-x)) skewY(var(--tw-skew-y)) scaleX(var(--tw-scale-x)) scaleY(var(--tw-scale-y)); }

.-translate-y-1\/2 { --tw-translate-y: -50%; }

.translate-x-0 { --tw-translate-x: 0px; }

.translate-x-0, .translate-x-0\.5 { transform: translate(var(--tw-translate-x),var(--tw-translate-y)) rotate(var(--tw-rotate)) skewX(var(--tw-skew-x)) skewY(var(--tw-skew-y)) scaleX(var(--tw-scale-x)) scaleY(var(--tw-scale-y)); }

.translate-x-0\.5 { --tw-translate-x: 0.125rem; }

.translate-x-1 { --tw-translate-x: 0.25rem; }

.translate-x-1, .translate-x-1\/2 { transform: translate(var(--tw-translate-x),var(--tw-translate-y)) rotate(var(--tw-rotate)) skewX(var(--tw-skew-x)) skewY(var(--tw-skew-y)) scaleX(var(--tw-scale-x)) scaleY(var(--tw-scale-y)); }

.translate-x-1\/2 { --tw-translate-x: 50%; }

.translate-x-full { --tw-translate-x: 100%; }

.translate-x-full, .translate-y-0 { transform: translate(var(--tw-translate-x),var(--tw-translate-y)) rotate(var(--tw-rotate)) skewX(var(--tw-skew-x)) skewY(var(--tw-skew-y)) scaleX(var(--tw-scale-x)) scaleY(var(--tw-scale-y)); }

.translate-y-0 { --tw-translate-y: 0px; }

.translate-y-1 { --tw-translate-y: 0.25rem; }

.translate-y-1, .translate-y-3 { transform: translate(var(--tw-translate-x),var(--tw-translate-y)) rotate(var(--tw-rotate)) skewX(var(--tw-skew-x)) skewY(var(--tw-skew-y)) scaleX(var(--tw-scale-x)) scaleY(var(--tw-scale-y)); }

.translate-y-3 { --tw-translate-y: 0.75rem; }

.translate-y-\[calc\(-50\%-1em\)\] { --tw-translate-y: calc(-50% - 1em); }

.translate-y-\[calc\(-50\%-1em\)\], .translate-y-\[calc\(-50\%-1em\+16px\)\] { transform: translate(var(--tw-translate-x),var(--tw-translate-y)) rotate(var(--tw-rotate)) skewX(var(--tw-skew-x)) skewY(var(--tw-skew-y)) scaleX(var(--tw-scale-x)) scaleY(var(--tw-scale-y)); }

.translate-y-\[calc\(-50\%-1em\+16px\)\] { --tw-translate-y: calc(-50% - 1em + 16px); }

.translate-y-\[calc\(-50\%-1em-16px\)\] { --tw-translate-y: calc(-50% - 1em - 16px); }

.translate-y-\[calc\(-50\%-1em-16px\)\], .translate-y-\[calc\(100\%-71px\)\] { transform: translate(var(--tw-translate-x),var(--tw-translate-y)) rotate(var(--tw-rotate)) skewX(var(--tw-skew-x)) skewY(var(--tw-skew-y)) scaleX(var(--tw-scale-x)) scaleY(var(--tw-scale-y)); }

.translate-y-\[calc\(100\%-71px\)\] { --tw-translate-y: calc(100% - 71px); }

.-rotate-180 { --tw-rotate: -180deg; }

.-rotate-180, .-rotate-45 { transform: translate(var(--tw-translate-x),var(--tw-translate-y)) rotate(var(--tw-rotate)) skewX(var(--tw-skew-x)) skewY(var(--tw-skew-y)) scaleX(var(--tw-scale-x)) scaleY(var(--tw-scale-y)); }

.-rotate-45 { --tw-rotate: -45deg; }

.-rotate-90 { --tw-rotate: -90deg; }

.-rotate-90, .rotate-180 { transform: translate(var(--tw-translate-x),var(--tw-translate-y)) rotate(var(--tw-rotate)) skewX(var(--tw-skew-x)) skewY(var(--tw-skew-y)) scaleX(var(--tw-scale-x)) scaleY(var(--tw-scale-y)); }

.rotate-180 { --tw-rotate: 180deg; }

.rotate-45 { --tw-rotate: 45deg; }

.rotate-45, .rotate-90 { transform: translate(var(--tw-translate-x),var(--tw-translate-y)) rotate(var(--tw-rotate)) skewX(var(--tw-skew-x)) skewY(var(--tw-skew-y)) scaleX(var(--tw-scale-x)) scaleY(var(--tw-scale-y)); }

.rotate-90 { --tw-rotate: 90deg; }

.scale-0 { --tw-scale-x: 0; --tw-scale-y: 0; }

.-scale-x-100, .scale-0 { transform: translate(var(--tw-translate-x),var(--tw-translate-y)) rotate(var(--tw-rotate)) skewX(var(--tw-skew-x)) skewY(var(--tw-skew-y)) scaleX(var(--tw-scale-x)) scaleY(var(--tw-scale-y)); }

.-scale-x-100 { --tw-scale-x: -1; }

.scale-x-75 { --tw-scale-x: .75; }

.scale-x-75, .transform { transform: translate(var(--tw-translate-x),var(--tw-translate-y)) rotate(var(--tw-rotate)) skewX(var(--tw-skew-x)) skewY(var(--tw-skew-y)) scaleX(var(--tw-scale-x)) scaleY(var(--tw-scale-y)); }

@-webkit-keyframes pulse { 
  50% { opacity: 0.5; }
}

@keyframes pulse { 
  50% { opacity: 0.5; }
}

.animate-pulse { animation: 2s cubic-bezier(0.4, 0, 0.6, 1) 0s infinite normal none running pulse; }

@-webkit-keyframes slideDownAndFade { 
  0% { opacity: 0; transform: translateY(-2px); }
  100% { opacity: 1; transform: translateY(0px); }
}

.animate-slideDownAndFade { animation: 0.4s cubic-bezier(0.16, 1, 0.3, 1) 0s 1 normal none running slideDownAndFade; }

@-webkit-keyframes slideLeftAndFade { 
  0% { opacity: 0; transform: translateX(2px); }
  100% { opacity: 1; transform: translateX(0px); }
}

.animate-slideLeftAndFade { animation: 0.4s cubic-bezier(0.16, 1, 0.3, 1) 0s 1 normal none running slideLeftAndFade; }

@-webkit-keyframes spin { 
  100% { transform: rotate(1turn); }
}

@keyframes spin { 
  100% { transform: rotate(1turn); }
}

.animate-spin { animation: 1s linear 0s infinite normal none running spin; }

.cursor-auto { cursor: auto; }

.cursor-default { cursor: default; }

.cursor-not-allowed { cursor: not-allowed; }

.cursor-pointer { cursor: pointer; }

.cursor-progress { cursor: progress; }

.cursor-wait { cursor: wait; }

.select-none { user-select: none; }

.resize-none { resize: none; }

.resize { resize: both; }

.list-inside { list-style-position: inside; }

.list-decimal { list-style-type: decimal; }

.list-disc { list-style-type: disc; }

.list-none { list-style-type: none; }

.appearance-none { appearance: none; }

.grid-flow-row { grid-auto-flow: row; }

.grid-cols-1 { grid-template-columns: repeat(1, minmax(0px, 1fr)); }

.grid-cols-2 { grid-template-columns: repeat(2, minmax(0px, 1fr)); }

.grid-cols-3 { grid-template-columns: repeat(3, minmax(0px, 1fr)); }

.grid-cols-5 { grid-template-columns: repeat(5, minmax(0px, 1fr)); }

.grid-cols-\[10px_1fr_10px\] { grid-template-columns: 10px 1fr 10px; }

.grid-cols-\[auto_max-content\] { grid-template-columns: auto max-content; }

.grid-cols-\[repeat\(auto-fit\,minmax\(250px\,1fr\)\)\] { grid-template-columns: repeat(auto-fit, minmax(250px, 1fr)); }

.grid-rows-2 { grid-template-rows: repeat(2, minmax(0px, 1fr)); }

.grid-rows-\[minmax\(10px\,_1fr\)_auto_minmax\(10px\,_1fr\)\] { grid-template-rows: minmax(10px, 1fr) auto minmax(10px, 1fr); }

.flex-row { flex-direction: row; }

.flex-col { flex-direction: column; }

.flex-col-reverse { flex-direction: column-reverse; }

.flex-wrap { flex-wrap: wrap; }

.flex-wrap-reverse { flex-wrap: wrap-reverse; }

.place-items-center { place-items: center; }

.items-start { align-items: flex-start; }

.items-end { align-items: flex-end; }

.items-center { align-items: center; }

.items-stretch { align-items: stretch; }

.justify-start { justify-content: flex-start; }

.justify-end { justify-content: flex-end; }

.justify-center { justify-content: center; }

.justify-between { justify-content: space-between; }

.justify-around { justify-content: space-around; }

.justify-stretch { justify-content: stretch; }

.justify-items-center { justify-items: center; }

.gap-0 { gap: 0px; }

.gap-1 { gap: 0.25rem; }

.gap-1\.5 { gap: 0.375rem; }

.gap-2 { gap: 0.5rem; }

.gap-2\.5 { gap: 0.625rem; }

.gap-3 { gap: 0.75rem; }

.gap-3\.5 { gap: 0.875rem; }

.gap-4 { gap: 1rem; }

.gap-5 { gap: 1.25rem; }

.gap-6 { gap: 1.5rem; }

.gap-7 { gap: 1.75rem; }

.gap-x-1 { column-gap: 0.25rem; }

.gap-x-1\.5 { column-gap: 0.375rem; }

.gap-x-3 { column-gap: 0.75rem; }

.gap-x-4 { column-gap: 1rem; }

.gap-x-6 { column-gap: 1.5rem; }

.gap-x-9 { column-gap: 2.25rem; }

.gap-y-2 { row-gap: 0.5rem; }

.gap-y-4 { row-gap: 1rem; }

.-space-y-px > :not([hidden]) ~ :not([hidden]) { --tw-space-y-reverse: 0; margin-bottom: calc(-1px*var(--tw-space-y-reverse)); margin-top: calc(-1px*(1 - var(--tw-space-y-reverse))); }

.space-x-1 > :not([hidden]) ~ :not([hidden]) { --tw-space-x-reverse: 0; margin-left: calc(.25rem*(1 - var(--tw-space-x-reverse))); margin-right: calc(.25rem*var(--tw-space-x-reverse)); }

.space-x-1\.5 > :not([hidden]) ~ :not([hidden]) { --tw-space-x-reverse: 0; margin-left: calc(.375rem*(1 - var(--tw-space-x-reverse))); margin-right: calc(.375rem*var(--tw-space-x-reverse)); }

.space-x-2 > :not([hidden]) ~ :not([hidden]) { --tw-space-x-reverse: 0; margin-left: calc(.5rem*(1 - var(--tw-space-x-reverse))); margin-right: calc(.5rem*var(--tw-space-x-reverse)); }

.space-x-4 > :not([hidden]) ~ :not([hidden]) { --tw-space-x-reverse: 0; margin-left: calc(1rem*(1 - var(--tw-space-x-reverse))); margin-right: calc(1rem*var(--tw-space-x-reverse)); }

.space-y-1 > :not([hidden]) ~ :not([hidden]) { --tw-space-y-reverse: 0; margin-bottom: calc(.25rem*var(--tw-space-y-reverse)); margin-top: calc(.25rem*(1 - var(--tw-space-y-reverse))); }

.space-y-1\.5 > :not([hidden]) ~ :not([hidden]) { --tw-space-y-reverse: 0; margin-bottom: calc(.375rem*var(--tw-space-y-reverse)); margin-top: calc(.375rem*(1 - var(--tw-space-y-reverse))); }

.space-y-2 > :not([hidden]) ~ :not([hidden]) { --tw-space-y-reverse: 0; margin-bottom: calc(.5rem*var(--tw-space-y-reverse)); margin-top: calc(.5rem*(1 - var(--tw-space-y-reverse))); }

.space-y-3 > :not([hidden]) ~ :not([hidden]) { --tw-space-y-reverse: 0; margin-bottom: calc(.75rem*var(--tw-space-y-reverse)); margin-top: calc(.75rem*(1 - var(--tw-space-y-reverse))); }

.space-y-4 > :not([hidden]) ~ :not([hidden]) { --tw-space-y-reverse: 0; margin-bottom: calc(1rem*var(--tw-space-y-reverse)); margin-top: calc(1rem*(1 - var(--tw-space-y-reverse))); }

.space-y-6 > :not([hidden]) ~ :not([hidden]) { --tw-space-y-reverse: 0; margin-bottom: calc(1.5rem*var(--tw-space-y-reverse)); margin-top: calc(1.5rem*(1 - var(--tw-space-y-reverse))); }

.divide-x > :not([hidden]) ~ :not([hidden]) { --tw-divide-x-reverse: 0; border-left-width: calc(1px*(1 - var(--tw-divide-x-reverse))); border-right-width: calc(1px*var(--tw-divide-x-reverse)); }

.divide-y > :not([hidden]) ~ :not([hidden]) { --tw-divide-y-reverse: 0; border-bottom-width: calc(1px*var(--tw-divide-y-reverse)); border-top-width: calc(1px*(1 - var(--tw-divide-y-reverse))); }

.divide-gray-200 > :not([hidden]) ~ :not([hidden]) { --tw-divide-opacity: 1; border-color: rgba(217,217,227,var(--tw-divide-opacity)); }

.divide-white\/10 > :not([hidden]) ~ :not([hidden]) { border-color: rgba(255, 255, 255, 0.1); }

.self-end { align-self: flex-end; }

.self-center { align-self: center; }

.self-stretch { align-self: stretch; }

.justify-self-start { justify-self: start; }

.overflow-auto { overflow: auto; }

.overflow-hidden { overflow: hidden; }

.overflow-scroll { overflow: scroll; }

.overflow-x-auto { overflow-x: auto; }

.overflow-y-auto { overflow-y: auto; }

.overflow-x-hidden { overflow-x: hidden; }

.overflow-y-hidden { overflow-y: hidden; }

.overflow-y-scroll { overflow-y: scroll; }

.truncate { overflow: hidden; white-space: nowrap; }

.overflow-ellipsis, .text-ellipsis, .truncate { text-overflow: ellipsis; }

.text-clip { text-overflow: clip; }

.whitespace-nowrap { white-space: nowrap; }

.\!whitespace-pre { white-space: pre !important; }

.whitespace-pre-line { white-space: pre-line; }

.\!whitespace-pre-wrap { white-space: pre-wrap !important; }

.whitespace-pre-wrap { white-space: pre-wrap; }

.break-words { overflow-wrap: break-word; }

.break-all { word-break: break-all; }

.\!rounded { border-radius: 0.25rem !important; }

.\!rounded-full { border-radius: 9999px !important; }

.\!rounded-lg { border-radius: 0.5rem !important; }

.rounded { border-radius: 0.25rem; }

.rounded-3xl { border-radius: 1.5rem; }

.rounded-\[38px\] { border-radius: 38px; }

.rounded-\[4px\] { border-radius: 4px; }

.rounded-\[5px\] { border-radius: 5px; }

.rounded-full { border-radius: 9999px; }

.rounded-lg { border-radius: 0.5rem; }

.rounded-md { border-radius: 0.375rem; }

.rounded-none { border-radius: 0px; }

.rounded-sm { border-radius: 0.125rem; }

.rounded-xl { border-radius: 0.75rem; }

.rounded-b-lg { border-bottom-left-radius: 0.5rem; border-bottom-right-radius: 0.5rem; }

.rounded-l-md { border-bottom-left-radius: 0.375rem; border-top-left-radius: 0.375rem; }

.rounded-r-md { border-bottom-right-radius: 0.375rem; border-top-right-radius: 0.375rem; }

.rounded-t-\[30px\] { border-top-left-radius: 30px; border-top-right-radius: 30px; }

.rounded-t-md { border-top-left-radius: 0.375rem; border-top-right-radius: 0.375rem; }

.rounded-t-xl { border-top-left-radius: 0.75rem; border-top-right-radius: 0.75rem; }

.rounded-br-sm { border-bottom-right-radius: 0.125rem; }

.border { border-width: 1px; }

.border-0 { border-width: 0px; }

.border-2 { border-width: 2px; }

.border-\[0\.5px\] { border-width: 0.5px; }

.border-b { border-bottom-width: 1px; }

.border-b-\[0\.5px\] { border-bottom-width: 0.5px; }

.border-l { border-left-width: 1px; }

.border-l-4 { border-left-width: 4px; }

.border-l-8 { border-left-width: 8px; }

.border-r { border-right-width: 1px; }

.border-r-8 { border-right-width: 8px; }

.border-t { border-top-width: 1px; }

.border-t-8 { border-top-width: 8px; }

.border-dashed { border-style: dashed; }

.border-none { border-style: none; }

.border-black { --tw-border-opacity: 1; border-color: rgba(0,0,0,var(--tw-border-opacity)); }

.border-black\/10 { border-color: rgba(0, 0, 0, 0.1); }

.border-black\/20 { border-color: rgba(0, 0, 0, 0.2); }

.border-black\/30 { border-color: rgba(0, 0, 0, 0.3); }

.border-black\/5 { border-color: rgba(0, 0, 0, 0.05); }

.border-brand-green { --tw-border-opacity: 1; border-color: rgba(25,195,125,var(--tw-border-opacity)); }

.border-gizmo-gray-950 { --tw-border-opacity: 1; border-color: rgba(15,15,15,var(--tw-border-opacity)); }

.border-gray-100 { --tw-border-opacity: 1; border-color: rgba(236,236,241,var(--tw-border-opacity)); }

.border-gray-200 { --tw-border-opacity: 1; border-color: rgba(217,217,227,var(--tw-border-opacity)); }

.border-gray-300 { --tw-border-opacity: 1; border-color: rgba(197,197,210,var(--tw-border-opacity)); }

.border-gray-400 { --tw-border-opacity: 1; border-color: rgba(172,172,190,var(--tw-border-opacity)); }

.border-gray-50 { --tw-border-opacity: 1; border-color: rgba(247,247,248,var(--tw-border-opacity)); }

.border-gray-500 { --tw-border-opacity: 1; border-color: rgba(142,142,160,var(--tw-border-opacity)); }

.border-gray-700 { --tw-border-opacity: 1; border-color: rgba(64,65,79,var(--tw-border-opacity)); }

.border-gray-800 { --tw-border-opacity: 1; border-color: rgba(52,53,65,var(--tw-border-opacity)); }

.border-gray-950 { --tw-border-opacity: 1; border-color: rgba(5,5,9,var(--tw-border-opacity)); }

.border-green-500 { --tw-border-opacity: 1; border-color: rgba(25,195,125,var(--tw-border-opacity)); }

.border-orange-400 { --tw-border-opacity: 1; border-color: rgba(255,128,82,var(--tw-border-opacity)); }

.border-orange-500 { --tw-border-opacity: 1; border-color: rgba(224,108,43,var(--tw-border-opacity)); }

.border-red-500 { --tw-border-opacity: 1; border-color: rgba(239,68,68,var(--tw-border-opacity)); }

.border-token-border-light { border-color: var(--border-light); }

.border-token-border-medium { border-color: var(--border-medium); }

.border-transparent { border-color: transparent; }

.border-white { --tw-border-opacity: 1; border-color: rgba(255,255,255,var(--tw-border-opacity)); }

.border-white\/20 { border-color: rgba(255, 255, 255, 0.2); }

.border-l-transparent { border-left-color: transparent; }

.border-r-transparent { border-right-color: transparent; }

.border-t-\[\#0077FF\] { --tw-border-opacity: 1; border-top-color: rgba(0,119,255,var(--tw-border-opacity)); }

.border-opacity-75 { --tw-border-opacity: 0.75; }

.\!bg-brand-purple { --tw-bg-opacity: 1 !important; background-color: rgba(171,104,255,var(--tw-bg-opacity)) !important; }

.\!bg-gray-200 { --tw-bg-opacity: 1 !important; background-color: rgba(217,217,227,var(--tw-bg-opacity)) !important; }

.\!bg-gray-700 { --tw-bg-opacity: 1 !important; background-color: rgba(64,65,79,var(--tw-bg-opacity)) !important; }

.\!bg-transparent { background-color: transparent !important; }

.bg-\[\#0077FF\] { --tw-bg-opacity: 1; background-color: rgba(0,119,255,var(--tw-bg-opacity)); }

.bg-\[\#10A37F\] { --tw-bg-opacity: 1; background-color: rgba(16,163,127,var(--tw-bg-opacity)); }

.bg-\[\#252525\] { --tw-bg-opacity: 1; background-color: rgba(37,37,37,var(--tw-bg-opacity)); }

.bg-\[\#2b2b2b\] { --tw-bg-opacity: 1; background-color: rgba(43,43,43,var(--tw-bg-opacity)); }

.bg-\[\#3C46FF\] { --tw-bg-opacity: 1; background-color: rgba(60,70,255,var(--tw-bg-opacity)); }

.bg-\[\#4046EC\] { --tw-bg-opacity: 1; background-color: rgba(64,70,236,var(--tw-bg-opacity)); }

.bg-\[\#B161FD\] { --tw-bg-opacity: 1; background-color: rgba(177,97,253,var(--tw-bg-opacity)); }

.bg-\[\#F8CA27\] { --tw-bg-opacity: 1; background-color: rgba(248,202,39,var(--tw-bg-opacity)); }

.bg-\[\#FF5588\] { --tw-bg-opacity: 1; background-color: rgba(255,85,136,var(--tw-bg-opacity)); }

.bg-\[\#FF6E3C\] { --tw-bg-opacity: 1; background-color: rgba(255,110,60,var(--tw-bg-opacity)); }

.bg-\[rgba\(30\,203\,239\,0\.07\)\] { background-color: rgba(30, 203, 239, 0.07); }

.bg-black { --tw-bg-opacity: 1; background-color: rgba(0,0,0,var(--tw-bg-opacity)); }

.bg-black\/20 { background-color: rgba(0, 0, 0, 0.2); }

.bg-black\/5 { background-color: rgba(0, 0, 0, 0.05); }

.bg-black\/90 { background-color: rgba(0, 0, 0, 0.9); }

.bg-black\/\[\.04\] { background-color: rgba(0, 0, 0, 0.04); }

.bg-black\/\[\.08\] { background-color: rgba(0, 0, 0, 0.08); }

.bg-black\/\[\.12\] { background-color: rgba(0, 0, 0, 0.12); }

.bg-blue-200 { --tw-bg-opacity: 1; background-color: rgba(191,219,254,var(--tw-bg-opacity)); }

.bg-blue-300 { --tw-bg-opacity: 1; background-color: rgba(121,137,255,var(--tw-bg-opacity)); }

.bg-blue-400 { --tw-bg-opacity: 1; background-color: rgba(58,136,255,var(--tw-bg-opacity)); }

.bg-blue-500 { --tw-bg-opacity: 1; background-color: rgba(69,89,164,var(--tw-bg-opacity)); }

.bg-blue-950 { --tw-bg-opacity: 1; background-color: rgba(23,37,84,var(--tw-bg-opacity)); }

.bg-brand-green\/20 { background-color: rgba(25, 195, 125, 0.2); }

.bg-brand-green\/40 { background-color: rgba(25, 195, 125, 0.4); }

.bg-brand-purple { --tw-bg-opacity: 1; background-color: rgba(171,104,255,var(--tw-bg-opacity)); }

.bg-brand-purple-500 { --tw-bg-opacity: 1; background-color: rgba(113,95,222,var(--tw-bg-opacity)); }

.bg-gray-100 { --tw-bg-opacity: 1; background-color: rgba(236,236,241,var(--tw-bg-opacity)); }

.bg-gray-200 { --tw-bg-opacity: 1; background-color: rgba(217,217,227,var(--tw-bg-opacity)); }

.bg-gray-300 { --tw-bg-opacity: 1; background-color: rgba(197,197,210,var(--tw-bg-opacity)); }

.bg-gray-300\/70 { background-color: rgba(197, 197, 210, 0.7); }

.bg-gray-400 { --tw-bg-opacity: 1; background-color: rgba(172,172,190,var(--tw-bg-opacity)); }

.bg-gray-50 { --tw-bg-opacity: 1; background-color: rgba(247,247,248,var(--tw-bg-opacity)); }

.bg-gray-500 { --tw-bg-opacity: 1; background-color: rgba(142,142,160,var(--tw-bg-opacity)); }

.bg-gray-600 { --tw-bg-opacity: 1; background-color: rgba(86,88,105,var(--tw-bg-opacity)); }

.bg-gray-700 { --tw-bg-opacity: 1; background-color: rgba(64,65,79,var(--tw-bg-opacity)); }

.bg-gray-800 { --tw-bg-opacity: 1; background-color: rgba(52,53,65,var(--tw-bg-opacity)); }

.bg-gray-900 { --tw-bg-opacity: 1; background-color: rgba(32,33,35,var(--tw-bg-opacity)); }

.bg-gray-950 { --tw-bg-opacity: 1; background-color: rgba(5,5,9,var(--tw-bg-opacity)); }

.bg-green-100 { --tw-bg-opacity: 1; background-color: rgba(210,244,211,var(--tw-bg-opacity)); }

.bg-green-200 { --tw-bg-opacity: 1; background-color: rgba(185,238,188,var(--tw-bg-opacity)); }

.bg-green-500 { --tw-bg-opacity: 1; background-color: rgba(25,195,125,var(--tw-bg-opacity)); }

.bg-green-500\/10 { background-color: rgba(25, 195, 125, 0.1); }

.bg-green-600 { --tw-bg-opacity: 1; background-color: rgba(16,163,127,var(--tw-bg-opacity)); }

.bg-orange-500 { --tw-bg-opacity: 1; background-color: rgba(224,108,43,var(--tw-bg-opacity)); }

.bg-orange-500\/10 { background-color: rgba(224, 108, 43, 0.1); }

.bg-red-100 { --tw-bg-opacity: 1; background-color: rgba(254,226,226,var(--tw-bg-opacity)); }

.bg-red-200 { --tw-bg-opacity: 1; background-color: rgba(254,202,202,var(--tw-bg-opacity)); }

.bg-red-500 { --tw-bg-opacity: 1; background-color: rgba(239,68,68,var(--tw-bg-opacity)); }

.bg-red-500\/10 { background-color: rgba(239, 68, 68, 0.1); }

.bg-red-600 { --tw-bg-opacity: 1; background-color: rgba(220,38,38,var(--tw-bg-opacity)); }

.bg-token-border-light { background-color: var(--border-light); }

.bg-token-border-medium { background-color: var(--border-medium); }

.bg-token-surface-primary { background-color: var(--surface-primary); }

.bg-token-surface-secondary { background-color: var(--surface-secondary); }

.bg-token-surface-tertiary { background-color: var(--surface-tertiary); }

.bg-transparent { background-color: transparent; }

.bg-white { --tw-bg-opacity: 1; background-color: rgba(255,255,255,var(--tw-bg-opacity)); }

.bg-white\/20 { background-color: rgba(255, 255, 255, 0.2); }

.bg-white\/25 { background-color: rgba(255, 255, 255, 0.25); }

.bg-white\/95 { background-color: rgba(255, 255, 255, 0.95); }

.bg-yellow-100 { --tw-bg-opacity: 1; background-color: rgba(254,249,195,var(--tw-bg-opacity)); }

.bg-yellow-200 { --tw-bg-opacity: 1; background-color: rgba(250,230,158,var(--tw-bg-opacity)); }

.bg-yellow-300 { --tw-bg-opacity: 1; background-color: rgba(255,243,201,var(--tw-bg-opacity)); }

.bg-yellow-400 { --tw-bg-opacity: 1; background-color: rgba(255,198,87,var(--tw-bg-opacity)); }

.bg-opacity-75 { --tw-bg-opacity: 0.75; }

.bg-gradient-to-b { background-image: linear-gradient(to bottom,var(--tw-gradient-stops)); }

.bg-gradient-to-l { background-image: linear-gradient(to left,var(--tw-gradient-stops)); }

.bg-gradient-to-t { background-image: linear-gradient(to top,var(--tw-gradient-stops)); }

.from-\[\#252525\] { --tw-gradient-from: #252525 var(--tw-gradient-from-position); --tw-gradient-to: rgba(37,37,37,0) var(--tw-gradient-to-position); --tw-gradient-stops: var(--tw-gradient-from),var(--tw-gradient-to); }

.from-gray-100 { --tw-gradient-from: #ececf1 var(--tw-gradient-from-position); --tw-gradient-to: rgba(236,236,241,0) var(--tw-gradient-to-position); --tw-gradient-stops: var(--tw-gradient-from),var(--tw-gradient-to); }

.from-gray-50 { --tw-gradient-from: #f7f7f8 var(--tw-gradient-from-position); --tw-gradient-to: hsla(240,7%,97%,0) var(--tw-gradient-to-position); --tw-gradient-stops: var(--tw-gradient-from),var(--tw-gradient-to); }

.from-gray-900\/0 { --tw-gradient-from: rgba(32,33,35,0) var(--tw-gradient-from-position); --tw-gradient-to: rgba(32,33,35,0) var(--tw-gradient-to-position); --tw-gradient-stops: var(--tw-gradient-from),var(--tw-gradient-to); }

.from-token-surface-primary { --tw-gradient-from: var(--surface-primary) var(--tw-gradient-from-position); --tw-gradient-to: hsla(0,0%,100%,0) var(--tw-gradient-to-position); --tw-gradient-stops: var(--tw-gradient-from),var(--tw-gradient-to); }

.from-transparent { --tw-gradient-from: transparent var(--tw-gradient-from-position); --tw-gradient-to: transparent var(--tw-gradient-to-position); --tw-gradient-stops: var(--tw-gradient-from),var(--tw-gradient-to); }

.from-60\%, .from-\[60\%\] { --tw-gradient-from-position: 60%; }

.to-gray-100 { --tw-gradient-to: #ececf1 var(--tw-gradient-to-position); }

.to-gray-900 { --tw-gradient-to: #202123 var(--tw-gradient-to-position); }

.to-transparent { --tw-gradient-to: transparent var(--tw-gradient-to-position); }

.bg-contain { background-size: contain; }

.bg-cover { background-size: cover; }

.bg-center { background-position: 50% center; }

.bg-no-repeat { background-repeat: no-repeat; }

.fill-gray-800 { fill: rgb(52, 53, 65); }

.fill-yellow-500 { fill: rgb(234, 179, 8); }

.stroke-black { stroke: rgb(0, 0, 0); }

.stroke-brand-purple\/25 { stroke: rgba(171, 104, 255, 0.25); }

.stroke-gray-400 { stroke: rgb(172, 172, 190); }

.stroke-white { stroke: rgb(255, 255, 255); }

.stroke-0 { stroke-width: 0; }

.stroke-1 { stroke-width: 1; }

.stroke-2 { stroke-width: 2; }

.stroke-\[2\.5\] { stroke-width: 2.5; }

.stroke-\[4\] { stroke-width: 4; }

.object-contain { object-fit: contain; }

.object-cover { object-fit: cover; }

.object-scale-down { object-fit: scale-down; }

.p-0 { padding: 0px; }

.p-0\.5 { padding: 0.125rem; }

.p-1 { padding: 0.25rem; }

.p-1\.5 { padding: 0.375rem; }

.p-2 { padding: 0.5rem; }

.p-3 { padding: 0.75rem; }

.p-4 { padding: 1rem; }

.p-5 { padding: 1.25rem; }

.p-6 { padding: 1.5rem; }

.p-7 { padding: 1.75rem; }

.p-8 { padding: 2rem; }

.p-\[5px\] { padding: 5px; }

.\!px-1 { padding-left: 0.25rem !important; padding-right: 0.25rem !important; }

.\!px-1\.5 { padding-left: 0.375rem !important; padding-right: 0.375rem !important; }

.\!py-0 { padding-bottom: 0px !important; padding-top: 0px !important; }

.\!py-0\.5 { padding-bottom: 0.125rem !important; padding-top: 0.125rem !important; }

.px-0 { padding-left: 0px; padding-right: 0px; }

.px-0\.5 { padding-left: 0.125rem; padding-right: 0.125rem; }

.px-1 { padding-left: 0.25rem; padding-right: 0.25rem; }

.px-1\.5 { padding-left: 0.375rem; padding-right: 0.375rem; }

.px-10 { padding-left: 2.5rem; padding-right: 2.5rem; }

.px-2 { padding-left: 0.5rem; padding-right: 0.5rem; }

.px-2\.5 { padding-left: 0.625rem; padding-right: 0.625rem; }

.px-3 { padding-left: 0.75rem; padding-right: 0.75rem; }

.px-4 { padding-left: 1rem; padding-right: 1rem; }

.px-5 { padding-left: 1.25rem; padding-right: 1.25rem; }

.px-6 { padding-left: 1.5rem; padding-right: 1.5rem; }

.px-8 { padding-left: 2rem; padding-right: 2rem; }

.py-0 { padding-bottom: 0px; padding-top: 0px; }

.py-0\.5 { padding-bottom: 0.125rem; padding-top: 0.125rem; }

.py-1 { padding-bottom: 0.25rem; padding-top: 0.25rem; }

.py-1\.5 { padding-bottom: 0.375rem; padding-top: 0.375rem; }

.py-12 { padding-bottom: 3rem; padding-top: 3rem; }

.py-2 { padding-bottom: 0.5rem; padding-top: 0.5rem; }

.py-2\.5 { padding-bottom: 0.625rem; padding-top: 0.625rem; }

.py-3 { padding-bottom: 0.75rem; padding-top: 0.75rem; }

.py-3\.5 { padding-bottom: 0.875rem; padding-top: 0.875rem; }

.py-4 { padding-bottom: 1rem; padding-top: 1rem; }

.py-6 { padding-bottom: 1.5rem; padding-top: 1.5rem; }

.py-8 { padding-bottom: 2rem; padding-top: 2rem; }

.py-\[10px\] { padding-bottom: 10px; padding-top: 10px; }

.\!pr-2 { padding-right: 0.5rem !important; }

.\!pt-0 { padding-top: 0px !important; }

.pb-0 { padding-bottom: 0px; }

.pb-0\.5 { padding-bottom: 0.125rem; }

.pb-1 { padding-bottom: 0.25rem; }

.pb-1\.5 { padding-bottom: 0.375rem; }

.pb-12 { padding-bottom: 3rem; }

.pb-2 { padding-bottom: 0.5rem; }

.pb-3 { padding-bottom: 0.75rem; }

.pb-4 { padding-bottom: 1rem; }

.pb-5 { padding-bottom: 1.25rem; }

.pb-64 { padding-bottom: 16rem; }

.pb-8 { padding-bottom: 2rem; }

.pl-1 { padding-left: 0.25rem; }

.pl-10 { padding-left: 2.5rem; }

.pl-12 { padding-left: 3rem; }

.pl-2 { padding-left: 0.5rem; }

.pl-3 { padding-left: 0.75rem; }

.pl-4 { padding-left: 1rem; }

.pl-5 { padding-left: 1.25rem; }

.pl-6 { padding-left: 1.5rem; }

.pl-8 { padding-left: 2rem; }

.pr-1 { padding-right: 0.25rem; }

.pr-1\.5 { padding-right: 0.375rem; }

.pr-10 { padding-right: 2.5rem; }

.pr-11 { padding-right: 2.75rem; }

.pr-12 { padding-right: 3rem; }

.pr-14 { padding-right: 3.5rem; }

.pr-16 { padding-right: 4rem; }

.pr-2 { padding-right: 0.5rem; }

.pr-3 { padding-right: 0.75rem; }

.pr-4 { padding-right: 1rem; }

.pr-5 { padding-right: 1.25rem; }

.pr-6 { padding-right: 1.5rem; }

.pr-7 { padding-right: 1.75rem; }

.pr-9 { padding-right: 2.25rem; }

.pt-0 { padding-top: 0px; }

.pt-0\.5 { padding-top: 0.125rem; }

.pt-1 { padding-top: 0.25rem; }

.pt-2 { padding-top: 0.5rem; }

.pt-20 { padding-top: 5rem; }

.pt-3 { padding-top: 0.75rem; }

.pt-4 { padding-top: 1rem; }

.pt-5 { padding-top: 1.25rem; }

.pt-6 { padding-top: 1.5rem; }

.pt-8 { padding-top: 2rem; }

.pt-\[67\%\] { padding-top: 67%; }

.text-left { text-align: left; }

.text-center { text-align: center; }

.text-right { text-align: right; }

.align-top { vertical-align: top; }

.align-middle { vertical-align: middle; }

.font-mono { font-family: "SÃ¶hne Mono", Monaco, "Andale Mono", "Ubuntu Mono", monospace; }

.font-sans { font-family: SÃ¶hne, ui-sans-serif, system-ui, -apple-system, "Segoe UI", Roboto, Ubuntu, Cantarell, "Noto Sans", sans-serif, "Helvetica Neue", Arial, "Apple Color Emoji", "Segoe UI Emoji", "Segoe UI Symbol", "Noto Color Emoji"; }

.text-2xl { font-size: 1.5rem; line-height: 2rem; }

.text-3xl { font-size: 1.875rem; line-height: 2.25rem; }

.text-4xl { font-size: 2.25rem; line-height: 2.5rem; }

.text-\[0\.6rem\] { font-size: 0.6rem; }

.text-\[10px\] { font-size: 10px; }

.text-\[20px\] { font-size: 20px; }

.text-\[28px\] { font-size: 28px; }

.text-\[32px\] { font-size: 32px; }

.text-base { font-size: 1rem; line-height: 1.5rem; }

.text-lg { font-size: 1.125rem; line-height: 1.75rem; }

.text-sm { font-size: 0.875rem; line-height: 1.25rem; }

.text-xl { font-size: 1.25rem; line-height: 1.75rem; }

.text-xs { font-size: 0.75rem; line-height: 1rem; }

.font-bold { font-weight: 700; }

.font-light { font-weight: 300; }

.font-medium { font-weight: 500; }

.font-normal { font-weight: 400; }

.font-semibold { font-weight: 600; }

.uppercase { text-transform: uppercase; }

.lowercase { text-transform: lowercase; }

.capitalize { text-transform: capitalize; }

.normal-case { text-transform: none; }

.italic { font-style: italic; }

.tabular-nums { --tw-numeric-spacing: tabular-nums; font-variant-numeric: var(--tw-ordinal) var(--tw-slashed-zero) var(--tw-numeric-figure) var(--tw-numeric-spacing) var(--tw-numeric-fraction); }

.leading-3 { line-height: 0.75rem; }

.leading-5 { line-height: 1.25rem; }

.leading-6 { line-height: 1.5rem; }

.leading-\[1\.2\] { line-height: 1.2; }

.leading-\[18px\] { line-height: 18px; }

.leading-none { line-height: 1; }

.leading-normal { line-height: 1.5; }

.leading-tight { line-height: 1.25; }

.tracking-tight { letter-spacing: -0.025em; }

.tracking-widest { letter-spacing: 0.1em; }

.\!text-gray-500 { --tw-text-opacity: 1 !important; color: rgba(142,142,160,var(--tw-text-opacity)) !important; }

.text-\[\#756B5C\] { --tw-text-opacity: 1; color: rgba(117,107,92,var(--tw-text-opacity)); }

.text-\[\#FE7600\] { --tw-text-opacity: 1; color: rgba(254,118,0,var(--tw-text-opacity)); }

.text-\[\#cdcdcd\] { --tw-text-opacity: 1; color: rgba(205,205,205,var(--tw-text-opacity)); }

.text-\[\#fff\] { --tw-text-opacity: 1; color: rgba(255,255,255,var(--tw-text-opacity)); }

.text-black { --tw-text-opacity: 1; color: rgba(0,0,0,var(--tw-text-opacity)); }

.text-black\/50 { color: rgba(0, 0, 0, 0.5); }

.text-black\/60 { color: rgba(0, 0, 0, 0.6); }

.text-black\/70 { color: rgba(0, 0, 0, 0.7); }

.text-blue-400 { --tw-text-opacity: 1; color: rgba(58,136,255,var(--tw-text-opacity)); }

.text-blue-500 { --tw-text-opacity: 1; color: rgba(69,89,164,var(--tw-text-opacity)); }

.text-blue-600 { --tw-text-opacity: 1; color: rgba(37,99,235,var(--tw-text-opacity)); }

.text-blue-700 { --tw-text-opacity: 1; color: rgba(29,78,216,var(--tw-text-opacity)); }

.text-brand-green { --tw-text-opacity: 1; color: rgba(25,195,125,var(--tw-text-opacity)); }

.text-brand-purple { --tw-text-opacity: 1; color: rgba(171,104,255,var(--tw-text-opacity)); }

.text-brand-purple-500 { --tw-text-opacity: 1; color: rgba(113,95,222,var(--tw-text-opacity)); }

.text-gizmo-gray-950 { --tw-text-opacity: 1; color: rgba(15,15,15,var(--tw-text-opacity)); }

.text-gray-100 { --tw-text-opacity: 1; color: rgba(236,236,241,var(--tw-text-opacity)); }

.text-gray-200 { --tw-text-opacity: 1; color: rgba(217,217,227,var(--tw-text-opacity)); }

.text-gray-300 { --tw-text-opacity: 1; color: rgba(197,197,210,var(--tw-text-opacity)); }

.text-gray-400 { --tw-text-opacity: 1; color: rgba(172,172,190,var(--tw-text-opacity)); }

.text-gray-50 { --tw-text-opacity: 1; color: rgba(247,247,248,var(--tw-text-opacity)); }

.text-gray-500 { --tw-text-opacity: 1; color: rgba(142,142,160,var(--tw-text-opacity)); }

.text-gray-600 { --tw-text-opacity: 1; color: rgba(86,88,105,var(--tw-text-opacity)); }

.text-gray-600\/50 { color: rgba(86, 88, 105, 0.5); }

.text-gray-700 { --tw-text-opacity: 1; color: rgba(64,65,79,var(--tw-text-opacity)); }

.text-gray-800 { --tw-text-opacity: 1; color: rgba(52,53,65,var(--tw-text-opacity)); }

.text-gray-900 { --tw-text-opacity: 1; color: rgba(32,33,35,var(--tw-text-opacity)); }

.text-green-500 { --tw-text-opacity: 1; color: rgba(25,195,125,var(--tw-text-opacity)); }

.text-green-600 { --tw-text-opacity: 1; color: rgba(16,163,127,var(--tw-text-opacity)); }

.text-green-700 { --tw-text-opacity: 1; color: rgba(26,127,100,var(--tw-text-opacity)); }

.text-link-base { --tw-text-opacity: 1; color: rgba(41,100,170,var(--tw-text-opacity)); }

.text-orange-300 { --tw-text-opacity: 1; color: rgba(255,160,69,var(--tw-text-opacity)); }

.text-orange-400 { --tw-text-opacity: 1; color: rgba(255,128,82,var(--tw-text-opacity)); }

.text-orange-500 { --tw-text-opacity: 1; color: rgba(224,108,43,var(--tw-text-opacity)); }

.text-red-400 { --tw-text-opacity: 1; color: rgba(248,113,113,var(--tw-text-opacity)); }

.text-red-500 { --tw-text-opacity: 1; color: rgba(239,68,68,var(--tw-text-opacity)); }

.text-red-600 { --tw-text-opacity: 1; color: rgba(220,38,38,var(--tw-text-opacity)); }

.text-red-700 { --tw-text-opacity: 1; color: rgba(185,28,28,var(--tw-text-opacity)); }

.text-red-800 { --tw-text-opacity: 1; color: rgba(153,27,27,var(--tw-text-opacity)); }

.text-token-text-primary { color: var(--text-primary); }

.text-token-text-secondary { color: var(--text-secondary); }

.text-token-text-tertiary { color: var(--text-tertiary); }

.text-white { --tw-text-opacity: 1; color: rgba(255,255,255,var(--tw-text-opacity)); }

.text-white\/25 { color: rgba(255, 255, 255, 0.25); }

.text-white\/50 { color: rgba(255, 255, 255, 0.5); }

.text-white\/80 { color: rgba(255, 255, 255, 0.8); }

.text-yellow-500 { --tw-text-opacity: 1; color: rgba(234,179,8,var(--tw-text-opacity)); }

.text-yellow-700 { --tw-text-opacity: 1; color: rgba(161,98,7,var(--tw-text-opacity)); }

.text-yellow-800 { --tw-text-opacity: 1; color: rgba(133,77,14,var(--tw-text-opacity)); }

.text-yellow-900 { --tw-text-opacity: 1; color: rgba(146,114,1,var(--tw-text-opacity)); }

.underline { text-decoration-line: underline; }

.line-through { text-decoration-line: line-through; }

.\!no-underline { text-decoration-line: none !important; }

.antialiased { -webkit-font-smoothing: antialiased; }

.placeholder-gray-500::-webkit-input-placeholder { --tw-placeholder-opacity: 1; color: rgba(142,142,160,var(--tw-placeholder-opacity)); }

.placeholder-gray-500::placeholder { --tw-placeholder-opacity: 1; color: rgba(142,142,160,var(--tw-placeholder-opacity)); }

.opacity-0 { opacity: 0; }

.opacity-100 { opacity: 1; }

.opacity-20 { opacity: 0.2; }

.opacity-25 { opacity: 0.25; }

.opacity-50 { opacity: 0.5; }

.opacity-70 { opacity: 0.7; }

.opacity-75 { opacity: 0.75; }

.opacity-80 { opacity: 0.8; }

.\!shadow-none { --tw-shadow: 0 0 transparent !important; --tw-shadow-colored: 0 0 transparent !important; box-shadow: var(--tw-ring-offset-shadow,0 0 transparent),var(--tw-ring-shadow,0 0 transparent),var(--tw-shadow) !important; }

.shadow { --tw-shadow: 0 1px 3px 0 rgba(0,0,0,.1),0 1px 2px -1px rgba(0,0,0,.1); --tw-shadow-colored: 0 1px 3px 0 var(--tw-shadow-color),0 1px 2px -1px var(--tw-shadow-color); }

.shadow, .shadow-\[0_1px_2px_rgba\(0\,0\,0\,0\.45\)\] { box-shadow: var(--tw-ring-offset-shadow,0 0 transparent),var(--tw-ring-shadow,0 0 transparent),var(--tw-shadow); }

.shadow-\[0_1px_2px_rgba\(0\,0\,0\,0\.45\)\] { --tw-shadow: 0 1px 2px rgba(0,0,0,.45); --tw-shadow-colored: 0 1px 2px var(--tw-shadow-color); }

.shadow-\[0_1px_7px_0_rgba\(0\,0\,0\,0\.03\)\] { --tw-shadow: 0 1px 7px 0 rgba(0,0,0,.03); --tw-shadow-colored: 0 1px 7px 0 var(--tw-shadow-color); }

.shadow-\[0_1px_7px_0_rgba\(0\,0\,0\,0\.03\)\], .shadow-\[0_1px_7px_0px_rgba\(0\,0\,0\,0\.06\)\] { box-shadow: var(--tw-ring-offset-shadow,0 0 transparent),var(--tw-ring-shadow,0 0 transparent),var(--tw-shadow); }

.shadow-\[0_1px_7px_0px_rgba\(0\,0\,0\,0\.06\)\] { --tw-shadow: 0 1px 7px 0px rgba(0,0,0,.06); --tw-shadow-colored: 0 1px 7px 0px var(--tw-shadow-color); }

.shadow-\[0_2px_12px_0px_rgba\(0\,0\,0\,0\.08\)\] { --tw-shadow: 0 2px 12px 0px rgba(0,0,0,.08); --tw-shadow-colored: 0 2px 12px 0px var(--tw-shadow-color); }

.shadow-\[0_2px_12px_0px_rgba\(0\,0\,0\,0\.08\)\], .shadow-\[0_2px_24px_rgba\(0\,0\,0\,0\.05\)\] { box-shadow: var(--tw-ring-offset-shadow,0 0 transparent),var(--tw-ring-shadow,0 0 transparent),var(--tw-shadow); }

.shadow-\[0_2px_24px_rgba\(0\,0\,0\,0\.05\)\] { --tw-shadow: 0 2px 24px rgba(0,0,0,.05); --tw-shadow-colored: 0 2px 24px var(--tw-shadow-color); }

.shadow-\[0px_1px_6px_0px_rgba\(0\,0\,0\,0\.02\)\] { --tw-shadow: 0px 1px 6px 0px rgba(0,0,0,.02); --tw-shadow-colored: 0px 1px 6px 0px var(--tw-shadow-color); }

.shadow-\[0px_1px_6px_0px_rgba\(0\,0\,0\,0\.02\)\], .shadow-\[0px_4px_14px_rgba\(0\,0\,0\,0\.06\)\] { box-shadow: var(--tw-ring-offset-shadow,0 0 transparent),var(--tw-ring-shadow,0 0 transparent),var(--tw-shadow); }

.shadow-\[0px_4px_14px_rgba\(0\,0\,0\,0\.06\)\] { --tw-shadow: 0px 4px 14px rgba(0,0,0,.06); --tw-shadow-colored: 0px 4px 14px var(--tw-shadow-color); }

.shadow-\[inset_0_0_0\.5px_rgba\(0\,0\,0\,0\.5\)\] { --tw-shadow: inset 0 0 0.5px rgba(0,0,0,.5); --tw-shadow-colored: inset 0 0 0.5px var(--tw-shadow-color); }

.shadow-\[inset_0_0_0\.5px_rgba\(0\,0\,0\,0\.5\)\], .shadow-\[inset_0_0_0_1px_rgba\(0\,0\,0\,0\.1\)\] { box-shadow: var(--tw-ring-offset-shadow,0 0 transparent),var(--tw-ring-shadow,0 0 transparent),var(--tw-shadow); }

.shadow-\[inset_0_0_0_1px_rgba\(0\,0\,0\,0\.1\)\] { --tw-shadow: inset 0 0 0 1px rgba(0,0,0,.1); --tw-shadow-colored: inset 0 0 0 1px var(--tw-shadow-color); }

.shadow-lg { --tw-shadow: 0 10px 15px -3px rgba(0,0,0,.1),0 4px 6px -4px rgba(0,0,0,.1); --tw-shadow-colored: 0 10px 15px -3px var(--tw-shadow-color),0 4px 6px -4px var(--tw-shadow-color); }

.shadow-lg, .shadow-md { box-shadow: var(--tw-ring-offset-shadow,0 0 transparent),var(--tw-ring-shadow,0 0 transparent),var(--tw-shadow); }

.shadow-md { --tw-shadow: 0 4px 6px -1px rgba(0,0,0,.1),0 2px 4px -2px rgba(0,0,0,.1); --tw-shadow-colored: 0 4px 6px -1px var(--tw-shadow-color),0 2px 4px -2px var(--tw-shadow-color); }

.shadow-none { --tw-shadow: 0 0 transparent; --tw-shadow-colored: 0 0 transparent; }

.shadow-none, .shadow-sm { box-shadow: var(--tw-ring-offset-shadow,0 0 transparent),var(--tw-ring-shadow,0 0 transparent),var(--tw-shadow); }

.shadow-sm { --tw-shadow: 0 1px 2px 0 rgba(0,0,0,.05); --tw-shadow-colored: 0 1px 2px 0 var(--tw-shadow-color); }

.shadow-xl { --tw-shadow: 0 20px 25px -5px rgba(0,0,0,.1),0 8px 10px -6px rgba(0,0,0,.1); --tw-shadow-colored: 0 20px 25px -5px var(--tw-shadow-color),0 8px 10px -6px var(--tw-shadow-color); }

.shadow-xl, .shadow-xs { box-shadow: var(--tw-ring-offset-shadow,0 0 transparent),var(--tw-ring-shadow,0 0 transparent),var(--tw-shadow); }

.shadow-xs { --tw-shadow: 0 0 15px rgba(0,0,0,.1); --tw-shadow-colored: 0 0 15px var(--tw-shadow-color); }

.shadow-xxs { --tw-shadow: 0 1px 7px 0 rgba(0,0,0,.03); --tw-shadow-colored: 0 1px 7px 0 var(--tw-shadow-color); box-shadow: var(--tw-ring-offset-shadow,0 0 transparent),var(--tw-ring-shadow,0 0 transparent),var(--tw-shadow); }

.outline-none { outline: transparent solid 2px; outline-offset: 2px; }

.outline { outline-style: solid; }

.outline-0 { outline-width: 0px; }

.outline-2 { outline-width: 2px; }

.outline-offset-2 { outline-offset: 2px; }

.ring-0 { --tw-ring-offset-shadow: var(--tw-ring-inset) 0 0 0 var(--tw-ring-offset-width) var(--tw-ring-offset-color); --tw-ring-shadow: var(--tw-ring-inset) 0 0 0 calc(var(--tw-ring-offset-width)) var(--tw-ring-color); }

.ring-0, .ring-1 { box-shadow: var(--tw-ring-offset-shadow),var(--tw-ring-shadow),var(--tw-shadow,0 0 transparent); }

.ring-1 { --tw-ring-offset-shadow: var(--tw-ring-inset) 0 0 0 var(--tw-ring-offset-width) var(--tw-ring-offset-color); --tw-ring-shadow: var(--tw-ring-inset) 0 0 0 calc(1px + var(--tw-ring-offset-width)) var(--tw-ring-color); }

.ring-2 { --tw-ring-offset-shadow: var(--tw-ring-inset) 0 0 0 var(--tw-ring-offset-width) var(--tw-ring-offset-color); --tw-ring-shadow: var(--tw-ring-inset) 0 0 0 calc(2px + var(--tw-ring-offset-width)) var(--tw-ring-color); box-shadow: var(--tw-ring-offset-shadow),var(--tw-ring-shadow),var(--tw-shadow,0 0 transparent); }

.ring-inset { --tw-ring-inset: inset; }

.ring-black { --tw-ring-opacity: 1; --tw-ring-color: rgba(0,0,0,var(--tw-ring-opacity)); }

.ring-black\/10 { --tw-ring-color: rgba(0,0,0,.1); }

.ring-white { --tw-ring-opacity: 1; --tw-ring-color: rgba(255,255,255,var(--tw-ring-opacity)); }

.ring-opacity-5 { --tw-ring-opacity: 0.05; }

.ring-offset-4 { --tw-ring-offset-width: 4px; }

.ring-offset-black { --tw-ring-offset-color: #000; }

.blur { --tw-blur: blur(8px); }

.blur, .blur-xl { filter: var(--tw-blur) var(--tw-brightness) var(--tw-contrast) var(--tw-grayscale) var(--tw-hue-rotate) var(--tw-invert) var(--tw-saturate) var(--tw-sepia) var(--tw-drop-shadow); }

.blur-xl { --tw-blur: blur(24px); }

.filter { filter: var(--tw-blur) var(--tw-brightness) var(--tw-contrast) var(--tw-grayscale) var(--tw-hue-rotate) var(--tw-invert) var(--tw-saturate) var(--tw-sepia) var(--tw-drop-shadow); }

.backdrop-blur-sm { --tw-backdrop-blur: blur(4px); }

.backdrop-blur-sm, .backdrop-blur-xl { backdrop-filter: var(--tw-backdrop-blur) var(--tw-backdrop-brightness) var(--tw-backdrop-contrast) var(--tw-backdrop-grayscale) var(--tw-backdrop-hue-rotate) var(--tw-backdrop-invert) var(--tw-backdrop-opacity) var(--tw-backdrop-saturate) var(--tw-backdrop-sepia); }

.backdrop-blur-xl { --tw-backdrop-blur: blur(24px); }

.transition { transition-duration: 0.15s; transition-property: color, background-color, border-color, text-decoration-color, fill, stroke, opacity, box-shadow, transform, filter, backdrop-filter, -webkit-transform, -webkit-filter, -webkit-backdrop-filter; transition-timing-function: cubic-bezier(0.4, 0, 0.2, 1); }

.transition-\[flex-basis\] { transition-duration: 0.15s; transition-property: flex-basis; transition-timing-function: cubic-bezier(0.4, 0, 0.2, 1); }

.transition-\[stroke-dashoffset\] { transition-duration: 0.15s; transition-property: stroke-dashoffset; transition-timing-function: cubic-bezier(0.4, 0, 0.2, 1); }

.transition-\[transform\,opacity\] { transition-duration: 0.15s; transition-property: transform, opacity, -webkit-transform; transition-timing-function: cubic-bezier(0.4, 0, 0.2, 1); }

.transition-all { transition-duration: 0.15s; transition-property: all; transition-timing-function: cubic-bezier(0.4, 0, 0.2, 1); }

.transition-colors { transition-duration: 0.15s; transition-property: color, background-color, border-color, text-decoration-color, fill, stroke; transition-timing-function: cubic-bezier(0.4, 0, 0.2, 1); }

.transition-opacity { transition-duration: 0.15s; transition-property: opacity; transition-timing-function: cubic-bezier(0.4, 0, 0.2, 1); }

.transition-transform { transition-duration: 0.15s; transition-property: transform, -webkit-transform; transition-timing-function: cubic-bezier(0.4, 0, 0.2, 1); }

.transition-width { transition-duration: 0.15s; transition-property: width; transition-timing-function: cubic-bezier(0.4, 0, 0.2, 1); }

.delay-100 { transition-delay: 0.1s; }

.delay-\[300ms\] { transition-delay: 0.3s; }

.delay-\[500ms\] { transition-delay: 0.5s; }

.duration-100 { transition-duration: 0.1s; }

.duration-150 { transition-duration: 0.15s; }

.duration-200 { transition-duration: 0.2s; }

.duration-300 { transition-duration: 0.3s; }

.duration-500 { transition-duration: 0.5s; }

.ease-in { transition-timing-function: cubic-bezier(0.4, 0, 1, 1); }

.ease-in-out { transition-timing-function: cubic-bezier(0.4, 0, 0.2, 1); }

.ease-linear { transition-timing-function: linear; }

.ease-out { transition-timing-function: cubic-bezier(0, 0, 0.2, 1); }

.will-change-\[opacity\,transform\] { will-change: opacity, transform; }

.will-change-transform { will-change: transform; }

.\[grid-area\:_title\] { grid-area: title / title / title / title; }

.\[grid-template-areas\:_\'title_action\'_\'description_action\'\] { grid-template-areas: "title action" "description action"; }

body, html { height: 100%; }

.dark body, .dark html { --tw-bg-opacity: 1; background-color: rgba(52,53,65,var(--tw-bg-opacity)); }

#__next, #root { height: 100%; }

.markdown { max-width: none; }

.markdown h1 { font-weight: 600; }

.markdown h1:first-child { margin-top: 0px; }

.markdown h2 { font-weight: 600; margin-bottom: 1rem; margin-top: 2rem; }

.markdown h2:first-child { margin-top: 0px; }

.markdown h3 { font-weight: 600; margin-bottom: 0.5rem; margin-top: 1rem; }

.markdown h3:first-child { margin-top: 0px; }

.markdown h4 { font-weight: 400; margin-bottom: 0.5rem; margin-top: 1rem; }

.markdown h4:first-child { margin-top: 0px; }

.markdown h5 { font-weight: 600; }

.markdown h5:first-child { margin-top: 0px; }

.markdown blockquote { --tw-border-opacity: 1; border-color: rgba(142,142,160,var(--tw-border-opacity)); border-left-width: 2px; line-height: 1.5rem; margin: 0px; padding-bottom: 0.5rem; padding-left: 1rem; padding-top: 0.5rem; }

.markdown blockquote > p { margin: 0px; }

.markdown blockquote > p::after, .markdown blockquote > p::before { display: none; }

.markdown ol { counter-reset: list-number 0; display: flex; flex-direction: column; list-style-type: none; padding-left: 0px; }

.markdown ol:first-child > li { padding-left: 1.25rem; }

.markdown ol > li { counter-increment: list-number 1; display: block; margin-bottom: 0px; margin-top: 0px; min-height: 28px; }

.markdown ol > li::before { --tw-translate-x: -100%; --tw-numeric-spacing: tabular-nums; --tw-text-opacity: 1; color: rgba(142,142,160,var(--tw-text-opacity)); content: counters(list-number, ".") "."; font-variant-numeric: var(--tw-ordinal) var(--tw-slashed-zero) var(--tw-numeric-figure) var(--tw-numeric-spacing) var(--tw-numeric-fraction); padding-right: 0.5rem; position: absolute; transform: translate(var(--tw-translate-x),var(--tw-translate-y)) rotate(var(--tw-rotate)) skewX(var(--tw-skew-x)) skewY(var(--tw-skew-y)) scaleX(var(--tw-scale-x)) scaleY(var(--tw-scale-y)); }

:is(.dark .markdown ol > li)::before { --tw-text-opacity: 1; color: rgba(172,172,190,var(--tw-text-opacity)); }

.markdown ul { display: flex; flex-direction: column; padding-left: 0px; }

.markdown ul:first-child > li { padding-left: 1.25rem; }

.markdown ul > li { display: block; margin: 0px; min-height: 28px; }

.markdown ul > li::before { --tw-translate-x: -100%; content: "â¢"; font-size: 0.875rem; line-height: 1.25rem; padding-right: 0.5rem; position: absolute; transform: translate(var(--tw-translate-x),var(--tw-translate-y)) rotate(var(--tw-rotate)) skewX(var(--tw-skew-x)) skewY(var(--tw-skew-y)) scaleX(var(--tw-scale-x)) scaleY(var(--tw-scale-y)); }

.markdown ol li > p, .markdown ol li > pre, .markdown ul li > p, .markdown ul li > pre { margin: 0px; }

.markdown ol ol, .markdown ol ul, .markdown ul ol, .markdown ul ul { margin-bottom: 0px; margin-left: 1rem; margin-top: 0px; }

.markdown table { --tw-border-spacing-x: 0px; --tw-border-spacing-y: 0px; border-collapse: separate; border-spacing: var(--tw-border-spacing-x) var(--tw-border-spacing-y); width: 100%; }

.markdown th { background-color: rgba(236, 236, 241, 0.2); border-bottom-width: 1px; border-left-width: 1px; border-top-width: 1px; padding: 0.25rem 0.75rem; }

.markdown th:first-child { border-top-left-radius: 0.375rem; }

.markdown th:last-child { border-right-width: 1px; border-top-right-radius: 0.375rem; }

.markdown td { border-bottom-width: 1px; border-left-width: 1px; padding: 0.25rem 0.75rem; }

.markdown td:last-child { border-right-width: 1px; }

.markdown tbody tr:last-child td:first-child { border-bottom-left-radius: 0.375rem; }

.markdown tbody tr:last-child td:last-child { border-bottom-right-radius: 0.375rem; }

.markdown a { --tw-text-opacity: 1; color: rgba(41,100,170,var(--tw-text-opacity)); font-weight: 400; text-decoration-line: none; }

.markdown a:hover { --tw-text-opacity: 1; color: rgba(98,154,221,var(--tw-text-opacity)); }

.gizmo .markdown > :not(pre), .gizmo .markdown > :not(pre) > * { color: var(--text-primary); }

.conversation-item-time::before { content: attr(data-time); }

button.scroll-convo { display: none; }

@-webkit-keyframes blink { 
  100% { visibility: hidden; }
}

@keyframes blink { 
  100% { visibility: hidden; }
}

.animate-flash { animation: 2s steps(60, start) 0s 1 normal none running flash; }

@-webkit-keyframes flash { 
  0% { background-color: rgba(255, 255, 255, 0.4); }
}

@keyframes flash { 
  0% { background-color: rgba(255, 255, 255, 0.4); }
}

.result-streaming > :not(ol):not(ul):not(pre):last-child::after, .result-streaming > pre:last-child code::after { content: "â"; font-family: "SÃ¶hne Circle", system-ui, -apple-system, "Segoe UI", Roboto, Ubuntu, Cantarell, "Noto Sans", sans-serif; margin-left: 0.25rem; vertical-align: baseline; }

.result-thinking:empty:last-child::after { -webkit-font-smoothing: subpixel-antialiased; animation: 1.25s ease-in-out 0s infinite normal none running pulseSize; backface-visibility: hidden; background-color: var(--text-primary); border-radius: 50%; box-sizing: border-box; content: " "; display: block; height: 12px; position: absolute; top: 7px; transform: translateZ(0px); transform-origin: center center; width: 12px; will-change: transform; }

@supports (selector(:has(*))) {
  .result-streaming > ol:last-child > li:last-child > ol:last-child > li:last-child::after, .result-streaming > ol:last-child > li:last-child > ol:last-child > li:last-child > ol:last-child > li:last-child::after, .result-streaming > ol:last-child > li:last-child > ol:last-child > li:last-child > ul:last-child > li:last-child::after, .result-streaming > ol:last-child > li:last-child > ul:last-child > li:last-child::after, .result-streaming > ol:last-child > li:last-child > ul:last-child > li:last-child > ol:last-child > li:last-child::after, .result-streaming > ol:last-child > li:last-child > ul:last-child > li:last-child > ul:last-child > li:last-child::after, .result-streaming > ul:last-child > li:last-child > ol:last-child > li:last-child::after, .result-streaming > ul:last-child > li:last-child > ol:last-child > li:last-child > ol:last-child > li:last-child::after, .result-streaming > ul:last-child > li:last-child > ol:last-child > li:last-child > ul:last-child > li:last-child::after, .result-streaming > ul:last-child > li:last-child > ul:last-child > li:last-child::after, .result-streaming > ul:last-child > li:last-child > ul:last-child > li:last-child > ol:last-child > li:last-child::after, .result-streaming > ul:last-child > li:last-child > ul:last-child > li:last-child > ul:last-child > li:last-child::after, .result-streaming > ul:last-child > li:last-child[*|\:not-has\(]::after { content: "â"; font-family: "SÃ¶hne Circle", system-ui, -apple-system, "Segoe UI", Roboto, Ubuntu, Cantarell, "Noto Sans", sans-serif; margin-left: 0.25rem; vertical-align: baseline; }
  .result-streaming > ul:last-child > li:last-child:not(:has(* > li))::after { content: "â"; font-family: "SÃ¶hne Circle", system-ui, -apple-system, "Segoe UI", Roboto, Ubuntu, Cantarell, "Noto Sans", sans-serif; margin-left: 0.25rem; vertical-align: baseline; }
  .result-streaming > ol:last-child > li:last-child[*|\:not-has\(]::after { content: "â"; font-family: "SÃ¶hne Circle", system-ui, -apple-system, "Segoe UI", Roboto, Ubuntu, Cantarell, "Noto Sans", sans-serif; margin-left: 0.25rem; vertical-align: baseline; }
  .result-streaming > ol:last-child > li:last-child:not(:has(* > li))::after { content: "â"; font-family: "SÃ¶hne Circle", system-ui, -apple-system, "Segoe UI", Roboto, Ubuntu, Cantarell, "Noto Sans", sans-serif; margin-left: 0.25rem; vertical-align: baseline; }
}

@supports not (selector(:has(*))) {
  .result-streaming > ol:last-child > li:last-child::after, .result-streaming > ul:last-child > li:last-child::after { content: "â"; font-family: "SÃ¶hne Circle", system-ui, -apple-system, "Segoe UI", Roboto, Ubuntu, Cantarell, "Noto Sans", sans-serif; margin-left: 0.25rem; vertical-align: baseline; }
}

@-webkit-keyframes pulseSize { 
  0%, 100% { transform: scaleX(1); }
  50% { transform: scale3d(1.25, 1.25, 1); }
}

@keyframes pulseSize { 
  0%, 100% { transform: scaleX(1); }
  50% { transform: scale3d(1.25, 1.25, 1); }
}

@-webkit-keyframes toast-open { 
  0% { opacity: 0; transform: translateY(-100%); }
  100% { transform: translateY(0px); }
}

@keyframes toast-open { 
  0% { opacity: 0; transform: translateY(-100%); }
  100% { transform: translateY(0px); }
}

@-webkit-keyframes toast-close { 
  0% { opacity: 1; }
  100% { opacity: 0; }
}

@keyframes toast-close { 
  0% { opacity: 1; }
  100% { opacity: 0; }
}

button[data-state="closed"] .toggle-item-button-closed, button[data-state="open"] .toggle-item-button-open { display: none; }

.toast-root { align-items: center; display: flex; flex-direction: column; height: 0px; transition: all 0.24s cubic-bezier(0, 0, 0.2, 1) 0s; }

.toast-root[data-state="entered"], .toast-root[data-state="entering"] { animation: 0.24s cubic-bezier(0.175, 0.885, 0.32, 1.175) 0s 1 normal both running toast-open; }

.toast-root[data-state="exiting"] { animation: 0.12s cubic-bezier(0.4, 0, 1, 1) 0s 1 normal both running toast-close; }

.toast-root .alert-root { box-shadow: rgba(67, 90, 111, 0.3) 0px 0px 1px, rgba(67, 90, 111, 0.3) 0px 5px 8px -4px; flex-shrink: 0; pointer-events: all; }

.interact-bounce { transition-duration: 0.15s; transition-property: transform, -webkit-transform; transition-timing-function: cubic-bezier(0.4, 0, 0.2, 1); }

.interact-bounce:hover { --tw-scale-x: 1.05; --tw-scale-y: 1.05; }

.interact-bounce:active, .interact-bounce:hover { transform: translate(var(--tw-translate-x),var(--tw-translate-y)) rotate(var(--tw-rotate)) skewX(var(--tw-skew-x)) skewY(var(--tw-skew-y)) scaleX(var(--tw-scale-x)) scaleY(var(--tw-scale-y)); }

.interact-bounce:active { --tw-scale-x: .9; --tw-scale-y: .9; }

.title { font-feature-settings: normal; font-family: SÃ¶hne, ui-sans-serif, system-ui, -apple-system, "Segoe UI", Roboto, Ubuntu, Cantarell, "Noto Sans", sans-serif, "Helvetica Neue", Arial, "Apple Color Emoji", "Segoe UI Emoji", "Segoe UI Symbol", "Noto Color Emoji"; font-variation-settings: normal; }

.font-circle { font-family: "SÃ¶hne Circle", system-ui, -apple-system, "Segoe UI", Roboto, Ubuntu, Cantarell, "Noto Sans", sans-serif; }

.icon-xs { stroke-width: 1.5; height: 0.75rem; width: 0.75rem; }

.icon-sm { stroke-width: 2; height: 1rem; width: 1rem; }

.icon-md { stroke-width: 1.5; height: 1.25rem; width: 1.25rem; }

.icon-lg { stroke-width: 1.5; height: 1.5rem; width: 1.5rem; }

.icon-xl { stroke-width: 1.5; height: 1.75rem; width: 1.75rem; }

.icon-2xl { stroke-width: 1.5; height: 2rem; width: 2rem; }

.icon-cover { stroke-width: 1.5; height: 234px; width: 234px; }

.gizmo-shadow-stroke { position: relative; }

.gizmo-shadow-stroke::after { --tw-shadow: inset 0 0 0 1px rgba(0,0,0,.1); --tw-shadow-colored: inset 0 0 0 1px var(--tw-shadow-color); border-radius: 9999px; inset: 0px; content: ""; position: absolute; }

.dark .gizmo-shadow-stroke::after, .gizmo-shadow-stroke::after { box-shadow: var(--tw-ring-offset-shadow,0 0 transparent),var(--tw-ring-shadow,0 0 transparent),var(--tw-shadow); }

.dark .gizmo-shadow-stroke::after { --tw-shadow: inset 0 0 0 1px hsla(0,0%,100%,.3); --tw-shadow-colored: inset 0 0 0 1px var(--tw-shadow-color); }

.dark .dark\:prose-invert { --tw-prose-body: var(--tw-prose-invert-body); --tw-prose-headings: var(--tw-prose-invert-headings); --tw-prose-lead: var(--tw-prose-invert-lead); --tw-prose-links: var(--tw-prose-invert-links); --tw-prose-bold: var(--tw-prose-invert-bold); --tw-prose-counters: var(--tw-prose-invert-counters); --tw-prose-bullets: var(--tw-prose-invert-bullets); --tw-prose-hr: var(--tw-prose-invert-hr); --tw-prose-quotes: var(--tw-prose-invert-quotes); --tw-prose-quote-borders: var(--tw-prose-invert-quote-borders); --tw-prose-captions: var(--tw-prose-invert-captions); --tw-prose-code: var(--tw-prose-invert-code); --tw-prose-pre-code: var(--tw-prose-invert-pre-code); --tw-prose-pre-bg: var(--tw-prose-invert-pre-bg); --tw-prose-th-borders: var(--tw-prose-invert-th-borders); --tw-prose-td-borders: var(--tw-prose-invert-td-borders); }

.placeholder\:text-gray-300::-webkit-input-placeholder { --tw-text-opacity: 1; color: rgba(197,197,210,var(--tw-text-opacity)); }

.placeholder\:text-gray-300::placeholder { --tw-text-opacity: 1; color: rgba(197,197,210,var(--tw-text-opacity)); }

.placeholder\:text-gray-500::-webkit-input-placeholder { --tw-text-opacity: 1; color: rgba(142,142,160,var(--tw-text-opacity)); }

.placeholder\:text-gray-500::placeholder { --tw-text-opacity: 1; color: rgba(142,142,160,var(--tw-text-opacity)); }

.placeholder\:text-gray-600::-webkit-input-placeholder { --tw-text-opacity: 1; color: rgba(86,88,105,var(--tw-text-opacity)); }

.placeholder\:text-gray-600::placeholder { --tw-text-opacity: 1; color: rgba(86,88,105,var(--tw-text-opacity)); }

.after\:content-\[\'\"\'\]::after, .before\:content-\[\'\"\'\]::before { --tw-content: '"'; content: var(--tw-content); }

.last\:mb-0:last-child { margin-bottom: 0px; }

.last\:mb-2:last-child { margin-bottom: 0.5rem; }

.last\:border-0:last-child { border-width: 0px; }

.last\:border-b:last-child { border-bottom-width: 1px; }

.last\:border-b-0:last-child { border-bottom-width: 0px; }

.last\:border-r-0:last-child { border-right-width: 0px; }

.last\:pb-0:last-child { padding-bottom: 0px; }

.last\:pr-0:last-child { padding-right: 0px; }

.last-of-type\:border-b-0:last-of-type { border-bottom-width: 0px; }

.checked\:border-blue-600:checked { --tw-border-opacity: 1; border-color: rgba(37,99,235,var(--tw-border-opacity)); }

.checked\:\!bg-black:checked { --tw-bg-opacity: 1 !important; background-color: rgba(0,0,0,var(--tw-bg-opacity)) !important; }

.checked\:bg-blue-600:checked { --tw-bg-opacity: 1; background-color: rgba(37,99,235,var(--tw-bg-opacity)); }

.empty\:hidden:empty { display: none; }

.focus-within\:z-10[focus-within] { z-index: 10; }

.focus-within\:z-10:focus-within { z-index: 10; }

.focus-within\:border-indigo-600[focus-within] { --tw-border-opacity: 1; border-color: rgba(79,70,229,var(--tw-border-opacity)); }

.focus-within\:border-indigo-600:focus-within { --tw-border-opacity: 1; border-color: rgba(79,70,229,var(--tw-border-opacity)); }

.focus-within\:ring-1[focus-within] { --tw-ring-offset-shadow: var(--tw-ring-inset) 0 0 0 var(--tw-ring-offset-width) var(--tw-ring-offset-color); --tw-ring-shadow: var(--tw-ring-inset) 0 0 0 calc(1px + var(--tw-ring-offset-width)) var(--tw-ring-color); box-shadow: var(--tw-ring-offset-shadow),var(--tw-ring-shadow),var(--tw-shadow,0 0 transparent); }

.focus-within\:ring-1:focus-within { --tw-ring-offset-shadow: var(--tw-ring-inset) 0 0 0 var(--tw-ring-offset-width) var(--tw-ring-offset-color); --tw-ring-shadow: var(--tw-ring-inset) 0 0 0 calc(1px + var(--tw-ring-offset-width)) var(--tw-ring-color); box-shadow: var(--tw-ring-offset-shadow),var(--tw-ring-shadow),var(--tw-shadow,0 0 transparent); }

.focus-within\:ring-indigo-600[focus-within] { --tw-ring-opacity: 1; --tw-ring-color: rgba(79,70,229,var(--tw-ring-opacity)); }

.focus-within\:ring-indigo-600:focus-within { --tw-ring-opacity: 1; --tw-ring-color: rgba(79,70,229,var(--tw-ring-opacity)); }

.hover\:cursor-pointer:hover { cursor: pointer; }

.hover\:overflow-visible:hover { overflow: visible; }

.hover\:whitespace-normal:hover { white-space: normal; }

.hover\:border-\[var\(--avatar-color\)\]:hover { border-color: var(--avatar-color); }

.hover\:border-black\/10:hover { border-color: rgba(0, 0, 0, 0.1); }

.hover\:border-gray-100:hover { --tw-border-opacity: 1; border-color: rgba(236,236,241,var(--tw-border-opacity)); }

.hover\:border-gray-200:hover { --tw-border-opacity: 1; border-color: rgba(217,217,227,var(--tw-border-opacity)); }

.hover\:border-gray-300:hover { --tw-border-opacity: 1; border-color: rgba(197,197,210,var(--tw-border-opacity)); }

.hover\:border-token-border-heavy:hover { border-color: var(--border-heavy); }

.hover\:\!bg-gray-50:hover { --tw-bg-opacity: 1 !important; background-color: rgba(247,247,248,var(--tw-bg-opacity)) !important; }

.hover\:bg-\[\#0000FF\]:hover { --tw-bg-opacity: 1; background-color: rgba(0,0,255,var(--tw-bg-opacity)); }

.hover\:bg-\[\#0077FF\]:hover { --tw-bg-opacity: 1; background-color: rgba(0,119,255,var(--tw-bg-opacity)); }

.hover\:bg-\[\#252525\]:hover { --tw-bg-opacity: 1; background-color: rgba(37,37,37,var(--tw-bg-opacity)); }

.hover\:bg-\[\#6E73FC\]:hover { --tw-bg-opacity: 1; background-color: rgba(110,115,252,var(--tw-bg-opacity)); }

.hover\:bg-black:hover { --tw-bg-opacity: 1; background-color: rgba(0,0,0,var(--tw-bg-opacity)); }

.hover\:bg-black\/10:hover { background-color: rgba(0, 0, 0, 0.1); }

.hover\:bg-brand-green\/30:hover { background-color: rgba(25, 195, 125, 0.3); }

.hover\:bg-brand-purple:hover { --tw-bg-opacity: 1; background-color: rgba(171,104,255,var(--tw-bg-opacity)); }

.hover\:bg-brand-purple-700:hover { --tw-bg-opacity: 1; background-color: rgba(99,83,195,var(--tw-bg-opacity)); }

.hover\:bg-gray-100:hover { --tw-bg-opacity: 1; background-color: rgba(236,236,241,var(--tw-bg-opacity)); }

.hover\:bg-gray-200:hover { --tw-bg-opacity: 1; background-color: rgba(217,217,227,var(--tw-bg-opacity)); }

.hover\:bg-gray-50:hover { --tw-bg-opacity: 1; background-color: rgba(247,247,248,var(--tw-bg-opacity)); }

.hover\:bg-gray-500\/10:hover { background-color: rgba(141, 141, 160, 0.1); }

.hover\:bg-gray-700:hover { --tw-bg-opacity: 1; background-color: rgba(64,65,79,var(--tw-bg-opacity)); }

.hover\:bg-green-100:hover { --tw-bg-opacity: 1; background-color: rgba(210,244,211,var(--tw-bg-opacity)); }

.hover\:bg-token-surface-secondary:hover { background-color: var(--surface-secondary); }

.hover\:bg-token-surface-tertiary:hover { background-color: var(--surface-tertiary); }

.hover\:bg-transparent:hover { background-color: transparent; }

.hover\:bg-white\/40:hover { background-color: rgba(255, 255, 255, 0.4); }

.hover\:pr-14:hover { padding-right: 3.5rem; }

.hover\:pr-4:hover { padding-right: 1rem; }

.hover\:text-black:hover { --tw-text-opacity: 1; color: rgba(0,0,0,var(--tw-text-opacity)); }

.hover\:text-black\/50:hover { color: rgba(0, 0, 0, 0.5); }

.hover\:text-black\/70:hover { color: rgba(0, 0, 0, 0.7); }

.hover\:text-blue-600:hover { --tw-text-opacity: 1; color: rgba(37,99,235,var(--tw-text-opacity)); }

.hover\:text-gray-200:hover { --tw-text-opacity: 1; color: rgba(217,217,227,var(--tw-text-opacity)); }

.hover\:text-gray-300:hover { --tw-text-opacity: 1; color: rgba(197,197,210,var(--tw-text-opacity)); }

.hover\:text-gray-400:hover { --tw-text-opacity: 1; color: rgba(172,172,190,var(--tw-text-opacity)); }

.hover\:text-gray-600:hover { --tw-text-opacity: 1; color: rgba(86,88,105,var(--tw-text-opacity)); }

.hover\:text-gray-700:hover { --tw-text-opacity: 1; color: rgba(64,65,79,var(--tw-text-opacity)); }

.hover\:text-gray-800:hover { --tw-text-opacity: 1; color: rgba(52,53,65,var(--tw-text-opacity)); }

.hover\:text-gray-900:hover { --tw-text-opacity: 1; color: rgba(32,33,35,var(--tw-text-opacity)); }

.hover\:text-gray-950:hover { --tw-text-opacity: 1; color: rgba(5,5,9,var(--tw-text-opacity)); }

.hover\:text-green-700:hover { --tw-text-opacity: 1; color: rgba(26,127,100,var(--tw-text-opacity)); }

.hover\:text-link-hover:hover { --tw-text-opacity: 1; color: rgba(98,154,221,var(--tw-text-opacity)); }

.hover\:text-red-500:hover { --tw-text-opacity: 1; color: rgba(239,68,68,var(--tw-text-opacity)); }

.hover\:text-red-700:hover { --tw-text-opacity: 1; color: rgba(185,28,28,var(--tw-text-opacity)); }

.hover\:text-token-text-primary:hover { color: var(--text-primary); }

.hover\:text-token-text-secondary:hover { color: var(--text-secondary); }

.hover\:text-white\/40:hover { color: rgba(255, 255, 255, 0.4); }

.hover\:underline:hover { text-decoration-line: underline; }

.hover\:\!opacity-100:hover { opacity: 1 !important; }

.hover\:opacity-100:hover { opacity: 1; }

.hover\:opacity-20:hover { opacity: 0.2; }

.hover\:opacity-70:hover { opacity: 0.7; }

.hover\:opacity-75:hover { opacity: 0.75; }

.hover\:opacity-80:hover { opacity: 0.8; }

.hover\:shadow-\[0_1px_7px_0px_rgba\(0\,0\,0\,0\.06\)\]:hover { --tw-shadow: 0 1px 7px 0px rgba(0,0,0,.06); --tw-shadow-colored: 0 1px 7px 0px var(--tw-shadow-color); box-shadow: var(--tw-ring-offset-shadow,0 0 transparent),var(--tw-ring-shadow,0 0 transparent),var(--tw-shadow); }

.hover\:brightness-90:hover { --tw-brightness: brightness(.9); filter: var(--tw-blur) var(--tw-brightness) var(--tw-contrast) var(--tw-grayscale) var(--tw-hue-rotate) var(--tw-invert) var(--tw-saturate) var(--tw-sepia) var(--tw-drop-shadow); }

.hover\:delay-0:hover { transition-delay: 0s; }

.focus\:border-brand-green:focus, .focus\:border-green-500:focus { --tw-border-opacity: 1; border-color: rgba(25,195,125,var(--tw-border-opacity)); }

.focus\:border-green-600:focus { --tw-border-opacity: 1; border-color: rgba(16,163,127,var(--tw-border-opacity)); }

.focus\:border-orange-400:focus { --tw-border-opacity: 1; border-color: rgba(255,128,82,var(--tw-border-opacity)); }

.focus\:border-red-500:focus { --tw-border-opacity: 1; border-color: rgba(239,68,68,var(--tw-border-opacity)); }

.focus\:bg-gray-100:focus { --tw-bg-opacity: 1; background-color: rgba(236,236,241,var(--tw-bg-opacity)); }

.focus\:bg-gray-50:focus { --tw-bg-opacity: 1; background-color: rgba(247,247,248,var(--tw-bg-opacity)); }

.focus\:outline-none:focus { outline: transparent solid 2px; outline-offset: 2px; }

.focus\:ring-0:focus { --tw-ring-offset-shadow: var(--tw-ring-inset) 0 0 0 var(--tw-ring-offset-width) var(--tw-ring-offset-color); --tw-ring-shadow: var(--tw-ring-inset) 0 0 0 calc(var(--tw-ring-offset-width)) var(--tw-ring-color); }

.focus\:ring-0:focus, .focus\:ring-1:focus { box-shadow: var(--tw-ring-offset-shadow),var(--tw-ring-shadow),var(--tw-shadow,0 0 transparent); }

.focus\:ring-1:focus { --tw-ring-offset-shadow: var(--tw-ring-inset) 0 0 0 var(--tw-ring-offset-width) var(--tw-ring-offset-color); --tw-ring-shadow: var(--tw-ring-inset) 0 0 0 calc(1px + var(--tw-ring-offset-width)) var(--tw-ring-color); }

.focus\:ring-2:focus { --tw-ring-offset-shadow: var(--tw-ring-inset) 0 0 0 var(--tw-ring-offset-width) var(--tw-ring-offset-color); --tw-ring-shadow: var(--tw-ring-inset) 0 0 0 calc(2px + var(--tw-ring-offset-width)) var(--tw-ring-color); box-shadow: var(--tw-ring-offset-shadow),var(--tw-ring-shadow),var(--tw-shadow,0 0 transparent); }

.focus\:ring-inset:focus { --tw-ring-inset: inset; }

.focus\:ring-blue-400:focus { --tw-ring-opacity: 1; --tw-ring-color: rgba(58,136,255,var(--tw-ring-opacity)); }

.focus\:ring-gray-200:focus { --tw-ring-opacity: 1; --tw-ring-color: rgba(217,217,227,var(--tw-ring-opacity)); }

.focus\:ring-green-500:focus { --tw-ring-opacity: 1; --tw-ring-color: rgba(25,195,125,var(--tw-ring-opacity)); }

.focus\:ring-green-600:focus { --tw-ring-opacity: 1; --tw-ring-color: rgba(16,163,127,var(--tw-ring-opacity)); }

.focus\:ring-white:focus { --tw-ring-opacity: 1; --tw-ring-color: rgba(255,255,255,var(--tw-ring-opacity)); }

.focus-visible\:border-blue-500.focus-visible { --tw-border-opacity: 1; border-color: rgba(69,89,164,var(--tw-border-opacity)); }

.focus-visible\:border-blue-500:focus-visible { --tw-border-opacity: 1; border-color: rgba(69,89,164,var(--tw-border-opacity)); }

.focus-visible\:ring-0.focus-visible { --tw-ring-offset-shadow: var(--tw-ring-inset) 0 0 0 var(--tw-ring-offset-width) var(--tw-ring-offset-color); --tw-ring-shadow: var(--tw-ring-inset) 0 0 0 calc(var(--tw-ring-offset-width)) var(--tw-ring-color); box-shadow: var(--tw-ring-offset-shadow),var(--tw-ring-shadow),var(--tw-shadow,0 0 transparent); }

.focus-visible\:ring-0:focus-visible { --tw-ring-offset-shadow: var(--tw-ring-inset) 0 0 0 var(--tw-ring-offset-width) var(--tw-ring-offset-color); --tw-ring-shadow: var(--tw-ring-inset) 0 0 0 calc(var(--tw-ring-offset-width)) var(--tw-ring-color); box-shadow: var(--tw-ring-offset-shadow),var(--tw-ring-shadow),var(--tw-shadow,0 0 transparent); }

.enabled\:bg-brand-purple:enabled { --tw-bg-opacity: 1; background-color: rgba(171,104,255,var(--tw-bg-opacity)); }

.disabled\:bottom-0:disabled { bottom: 0px; }

.disabled\:bottom-0\.5:disabled { bottom: 0.125rem; }

.disabled\:cursor-auto:disabled { cursor: auto; }

.disabled\:text-gray-300:disabled { --tw-text-opacity: 1; color: rgba(197,197,210,var(--tw-text-opacity)); }

.disabled\:text-gray-400:disabled { --tw-text-opacity: 1; color: rgba(172,172,190,var(--tw-text-opacity)); }

.disabled\:text-token-border-medium:disabled { color: var(--border-medium); }

.disabled\:opacity-20:disabled { opacity: 0.2; }

.disabled\:opacity-40:disabled { opacity: 0.4; }

.disabled\:hover\:bg-gray-900:hover:disabled { --tw-bg-opacity: 1; background-color: rgba(32,33,35,var(--tw-bg-opacity)); }

.group:hover .group-hover\:visible, .group\/dalle-image:hover .group-hover\/dalle-image\:visible { visibility: visible; }

.group\/tool-expander-closed:hover .group-hover\/tool-expander-closed\:block, .group\/tool-expander-open:hover .group-hover\/tool-expander-open\:block { display: block; }

.group:hover .group-hover\:border-t-\[\#0077FF\] { --tw-border-opacity: 1; border-top-color: rgba(0,119,255,var(--tw-border-opacity)); }

.group\/options:hover .group-hover\/options\:bg-transparent { background-color: transparent; }

.group\/toggle:hover .group-hover\/toggle\:bg-red-500 { --tw-bg-opacity: 1; background-color: rgba(239,68,68,var(--tw-bg-opacity)); }

.group:hover .group-hover\:from-\[\#252525\] { --tw-gradient-from: #252525 var(--tw-gradient-from-position); --tw-gradient-to: rgba(37,37,37,0) var(--tw-gradient-to-position); --tw-gradient-stops: var(--tw-gradient-from),var(--tw-gradient-to); }

.group:hover .group-hover\:from-gray-100 { --tw-gradient-from: #ececf1 var(--tw-gradient-from-position); --tw-gradient-to: rgba(236,236,241,0) var(--tw-gradient-to-position); --tw-gradient-stops: var(--tw-gradient-from),var(--tw-gradient-to); }

.group\/button:hover .group-hover\/button\:text-blue-500 { --tw-text-opacity: 1; color: rgba(69,89,164,var(--tw-text-opacity)); }

.group\/button:hover .group-hover\/button\:text-brand-green { --tw-text-opacity: 1; color: rgba(25,195,125,var(--tw-text-opacity)); }

.group\/button:hover .group-hover\/button\:text-brand-purple { --tw-text-opacity: 1; color: rgba(171,104,255,var(--tw-text-opacity)); }

.group\/button:hover .group-hover\/button\:text-orange-500 { --tw-text-opacity: 1; color: rgba(224,108,43,var(--tw-text-opacity)); }

.group\/button:hover .group-hover\/button\:text-red-500 { --tw-text-opacity: 1; color: rgba(239,68,68,var(--tw-text-opacity)); }

.group\/option:hover .group-hover\/option\:\!text-brand-green { --tw-text-opacity: 1 !important; color: rgba(25,195,125,var(--tw-text-opacity)) !important; }

.group\/option:hover .group-hover\/option\:\!text-brand-purple { --tw-text-opacity: 1 !important; color: rgba(171,104,255,var(--tw-text-opacity)) !important; }

.group\/option:hover .group-hover\/option\:text-gray-100 { --tw-text-opacity: 1; color: rgba(236,236,241,var(--tw-text-opacity)); }

.group\/option:hover .group-hover\/option\:text-gray-800 { --tw-text-opacity: 1; color: rgba(52,53,65,var(--tw-text-opacity)); }

.group\/options:hover .group-hover\/options\:text-gray-500 { --tw-text-opacity: 1; color: rgba(142,142,160,var(--tw-text-opacity)); }

.group\/options:hover .group-hover\/options\:opacity-0 { opacity: 0; }

.group:hover .group-hover\:opacity-100 { opacity: 1; }

.group:hover .group-hover\:opacity-50, .group[data-disabled] .group-data-\[disabled\]\:opacity-50 { opacity: 0.5; }

.radix-disabled\:pointer-events-none[data-disabled] { pointer-events: none; }

.radix-disabled\:cursor-auto[data-disabled] { cursor: auto; }

.radix-disabled\:cursor-not-allowed[data-disabled] { cursor: not-allowed; }

.radix-disabled\:bg-transparent[data-disabled] { background-color: transparent; }

.radix-disabled\:text-gray-300[data-disabled] { --tw-text-opacity: 1; color: rgba(197,197,210,var(--tw-text-opacity)); }

.radix-disabled\:text-gray-500[data-disabled] { --tw-text-opacity: 1; color: rgba(142,142,160,var(--tw-text-opacity)); }

.radix-disabled\:opacity-50[data-disabled] { opacity: 0.5; }

.radix-highlighted\:bg-gray-50[data-highlighted] { --tw-bg-opacity: 1; background-color: rgba(247,247,248,var(--tw-bg-opacity)); }

.radix-highlighted\:outline-none[data-highlighted] { outline: transparent solid 2px; outline-offset: 2px; }

.radix-state-active\:bg-gray-800[data-state="active"] { --tw-bg-opacity: 1; background-color: rgba(52,53,65,var(--tw-bg-opacity)); }

.radix-state-active\:text-gray-600[data-state="active"] { --tw-text-opacity: 1; color: rgba(86,88,105,var(--tw-text-opacity)); }

.radix-state-active\:text-white[data-state="active"] { --tw-text-opacity: 1; color: rgba(255,255,255,var(--tw-text-opacity)); }

.group[data-state="active"] .group-radix-state-active\:fill-white { fill: rgb(255, 255, 255); }

.radix-state-checked\:translate-x-\[13px\][data-state="checked"] { --tw-translate-x: 13px; }

.radix-state-checked\:translate-x-\[13px\][data-state="checked"], .radix-state-checked\:translate-x-\[19px\][data-state="checked"] { transform: translate(var(--tw-translate-x),var(--tw-translate-y)) rotate(var(--tw-rotate)) skewX(var(--tw-skew-x)) skewY(var(--tw-skew-y)) scaleX(var(--tw-scale-x)) scaleY(var(--tw-scale-y)); }

.radix-state-checked\:translate-x-\[19px\][data-state="checked"] { --tw-translate-x: 19px; }

.radix-state-checked\:bg-green-600[data-state="checked"] { --tw-bg-opacity: 1; background-color: rgba(16,163,127,var(--tw-bg-opacity)); }

.radix-state-checked\:text-token-text-primary[data-state="checked"] { color: var(--text-primary); }

.radix-state-checked\:text-white[data-state="checked"] { --tw-text-opacity: 1; color: rgba(255,255,255,var(--tw-text-opacity)); }

@-webkit-keyframes contentShow { 
  0% { opacity: 0; transform: scale(0.96); }
  100% { opacity: 1; transform: scale(1); }
}

@keyframes contentShow { 
  0% { opacity: 0; transform: scale(0.96); }
  100% { opacity: 1; transform: scale(1); }
}

.radix-state-open\:animate-contentShow[data-state="open"] { animation: 0.15s cubic-bezier(0.16, 1, 0.3, 1) 0s 1 normal none running contentShow; }

@-webkit-keyframes show { 
  0% { opacity: 0; }
  100% { opacity: 1; }
}

@keyframes show { 
  0% { opacity: 0; }
  100% { opacity: 1; }
}

.radix-state-open\:animate-show[data-state="open"] { animation: 0.1s cubic-bezier(0.16, 1, 0.3, 1) 0s 1 normal none running show; }

.radix-state-open\:bg-gray-100[data-state="open"] { --tw-bg-opacity: 1; background-color: rgba(236,236,241,var(--tw-bg-opacity)); }

.radix-state-open\:bg-gray-50[data-state="open"] { --tw-bg-opacity: 1; background-color: rgba(247,247,248,var(--tw-bg-opacity)); }

.radix-state-open\:text-gray-600[data-state="open"] { --tw-text-opacity: 1; color: rgba(86,88,105,var(--tw-text-opacity)); }

.radix-state-open\:text-token-text-secondary[data-state="open"] { color: var(--text-secondary); }

.group[data-state="open"] .group-radix-state-open\:translate-y-0 { --tw-translate-y: 0px; }

.group[data-state="open"] .group-radix-state-open\:translate-y-0, .group[data-state="open"] .group-radix-state-open\:translate-y-0\.5 { transform: translate(var(--tw-translate-x),var(--tw-translate-y)) rotate(var(--tw-rotate)) skewX(var(--tw-skew-x)) skewY(var(--tw-skew-y)) scaleX(var(--tw-scale-x)) scaleY(var(--tw-scale-y)); }

.group[data-state="open"] .group-radix-state-open\:translate-y-0\.5 { --tw-translate-y: 0.125rem; }

.group[data-state="open"] .group-radix-state-open\:rotate-180 { --tw-rotate: 180deg; transform: translate(var(--tw-translate-x),var(--tw-translate-y)) rotate(var(--tw-rotate)) skewX(var(--tw-skew-x)) skewY(var(--tw-skew-y)) scaleX(var(--tw-scale-x)) scaleY(var(--tw-scale-y)); }

.radix-side-bottom\:animate-slideDownAndFade[data-side="bottom"] { animation: 0.4s cubic-bezier(0.16, 1, 0.3, 1) 0s 1 normal none running slideDownAndFade; }

@-webkit-keyframes slideUpAndFade { 
  0% { opacity: 0; transform: translateY(2px); }
  100% { opacity: 1; transform: translateY(0px); }
}

.radix-side-bottom\:animate-slideUpAndFade[data-side="bottom"] { animation: 0.4s cubic-bezier(0.16, 1, 0.3, 1) 0s 1 normal none running slideUpAndFade; }

.radix-side-left\:animate-slideLeftAndFade[data-side="left"] { animation: 0.4s cubic-bezier(0.16, 1, 0.3, 1) 0s 1 normal none running slideLeftAndFade; }

@-webkit-keyframes slideRightAndFade { 
  0% { opacity: 0; transform: translateX(-2px); }
  100% { opacity: 1; transform: translateX(0px); }
}

.radix-side-left\:animate-slideRightAndFade[data-side="left"] { animation: 0.4s cubic-bezier(0.16, 1, 0.3, 1) 0s 1 normal none running slideRightAndFade; }

@keyframes slideLeftAndFade { 
  0% { opacity: 0; transform: translateX(2px); }
  100% { opacity: 1; transform: translateX(0px); }
}

.radix-side-right\:animate-slideLeftAndFade[data-side="right"] { animation: 0.4s cubic-bezier(0.16, 1, 0.3, 1) 0s 1 normal none running slideLeftAndFade; }

@keyframes slideRightAndFade { 
  0% { opacity: 0; transform: translateX(-2px); }
  100% { opacity: 1; transform: translateX(0px); }
}

.radix-side-right\:animate-slideRightAndFade[data-side="right"] { animation: 0.4s cubic-bezier(0.16, 1, 0.3, 1) 0s 1 normal none running slideRightAndFade; }

@keyframes slideDownAndFade { 
  0% { opacity: 0; transform: translateY(-2px); }
  100% { opacity: 1; transform: translateY(0px); }
}

.radix-side-top\:animate-slideDownAndFade[data-side="top"] { animation: 0.4s cubic-bezier(0.16, 1, 0.3, 1) 0s 1 normal none running slideDownAndFade; }

@keyframes slideUpAndFade { 
  0% { opacity: 0; transform: translateY(2px); }
  100% { opacity: 1; transform: translateY(0px); }
}

.radix-side-top\:animate-slideUpAndFade[data-side="top"] { animation: 0.4s cubic-bezier(0.16, 1, 0.3, 1) 0s 1 normal none running slideUpAndFade; }

.group-ui-open\:bg-gray-100.group[data-headlessui-state~="open"] { --tw-bg-opacity: 1; background-color: rgba(236,236,241,var(--tw-bg-opacity)); }

.group:where([data-headlessui-state~="open"]) .group-ui-open\:bg-gray-100 { --tw-bg-opacity: 1; background-color: rgba(236,236,241,var(--tw-bg-opacity)); }

.gizmo .gizmo\:absolute { position: absolute; }

.gizmo .gizmo\:-top-px { top: -1px; }

.gizmo .gizmo\:left-2 { left: 0.5rem; }

.gizmo .gizmo\:right-0 { right: 0px; }

.gizmo .gizmo\:top-1 { top: 0.25rem; }

.gizmo .gizmo\:top-1\/2 { top: 50%; }

.gizmo .gizmo\:-ml-6 { margin-left: -1.5rem; }

.gizmo .gizmo\:ml-0 { margin-left: 0px; }

.gizmo .gizmo\:mt-0 { margin-top: 0px; }

.gizmo .gizmo\:mt-5 { margin-top: 1.25rem; }

.gizmo .gizmo\:flex { display: flex; }

.gizmo .gizmo\:hidden { display: none; }

.gizmo .gizmo\:h-10 { height: 2.5rem; }

.gizmo .gizmo\:min-h-0 { min-height: 0px; }

.gizmo .gizmo\:w-full { width: 100%; }

.gizmo .gizmo\:-translate-y-1\/2 { --tw-translate-y: -50%; }

.gizmo .gizmo\:-translate-y-1\/2, .gizmo .gizmo\:transform { transform: translate(var(--tw-translate-x),var(--tw-translate-y)) rotate(var(--tw-rotate)) skewX(var(--tw-skew-x)) skewY(var(--tw-skew-y)) scaleX(var(--tw-scale-x)) scaleY(var(--tw-scale-y)); }

.gizmo .gizmo\:justify-start { justify-content: flex-start; }

.gizmo .gizmo\:gap-1 { gap: 0.25rem; }

.gizmo .gizmo\:gap-1\.5 { gap: 0.375rem; }

.gizmo .gizmo\:gap-2 { gap: 0.5rem; }

.gizmo .gizmo\:gap-3 { gap: 0.75rem; }

.gizmo .gizmo\:overflow-hidden { overflow: hidden; }

.gizmo .gizmo\:rounded-2xl { border-radius: 1rem; }

.gizmo .gizmo\:rounded-full { border-radius: 9999px; }

.gizmo .gizmo\:rounded-lg { border-radius: 0.5rem; }

.gizmo .gizmo\:border { border-width: 1px; }

.gizmo .gizmo\:border-0 { border-width: 0px; }

.gizmo .gizmo\:border-\[rgba\(0\,0\,0\,0\.1\)\] { border-color: rgba(0, 0, 0, 0.1); }

.gizmo .gizmo\:border-black { --tw-border-opacity: 1; border-color: rgba(0,0,0,var(--tw-border-opacity)); }

.gizmo .gizmo\:border-black\/20 { border-color: rgba(0, 0, 0, 0.2); }

.gizmo .gizmo\:border-token-border-light { border-color: var(--border-light); }

.gizmo .gizmo\:bg-black { --tw-bg-opacity: 1; background-color: rgba(0,0,0,var(--tw-bg-opacity)); }

.gizmo .gizmo\:bg-transparent { background-color: transparent; }

.gizmo .gizmo\:bg-white { --tw-bg-opacity: 1; background-color: rgba(255,255,255,var(--tw-bg-opacity)); }

.gizmo .gizmo\:from-white { --tw-gradient-from: #fff var(--tw-gradient-from-position); --tw-gradient-to: hsla(0,0%,100%,0) var(--tw-gradient-to-position); --tw-gradient-stops: var(--tw-gradient-from),var(--tw-gradient-to); }

.gizmo .gizmo\:p-0 { padding: 0px; }

.gizmo .gizmo\:p-0\.5 { padding: 0.125rem; }

.gizmo .gizmo\:p-1 { padding: 0.25rem; }

.gizmo .gizmo\:px-2 { padding-left: 0.5rem; padding-right: 0.5rem; }

.gizmo .gizmo\:px-3 { padding-left: 0.75rem; padding-right: 0.75rem; }

.gizmo .gizmo\:py-2 { padding-bottom: 0.5rem; padding-top: 0.5rem; }

.gizmo .gizmo\:py-3 { padding-bottom: 0.75rem; padding-top: 0.75rem; }

.gizmo .gizmo\:py-3\.5 { padding-bottom: 0.875rem; padding-top: 0.875rem; }

.gizmo .gizmo\:pb-9 { padding-bottom: 2.25rem; }

.gizmo .gizmo\:pl-0 { padding-left: 0px; }

.gizmo .gizmo\:pl-10 { padding-left: 2.5rem; }

.gizmo .gizmo\:pt-0 { padding-top: 0px; }

.gizmo .gizmo\:text-gizmo-gray-600 { --tw-text-opacity: 1; color: rgba(102,102,102,var(--tw-text-opacity)); }

.gizmo .gizmo\:text-token-text-tertiary { color: var(--text-tertiary); }

.gizmo .gizmo\:placeholder-black\/50::-webkit-input-placeholder { color: rgba(0, 0, 0, 0.5); }

.gizmo .gizmo\:placeholder-black\/50::placeholder { color: rgba(0, 0, 0, 0.5); }

.gizmo .gizmo\:shadow-\[0_0_0_2px_rgba\(255\,255\,255\,0\.95\)\] { --tw-shadow: 0 0 0 2px hsla(0,0%,100%,.95); --tw-shadow-colored: 0 0 0 2px var(--tw-shadow-color); box-shadow: var(--tw-ring-offset-shadow,0 0 transparent),var(--tw-ring-shadow,0 0 transparent),var(--tw-shadow); }

.gizmo .gizmo\:hover\:bg-gray-100:hover { --tw-bg-opacity: 1; background-color: rgba(236,236,241,var(--tw-bg-opacity)); }

.gizmo .gizmo\:hover\:bg-gray-50:hover { --tw-bg-opacity: 1; background-color: rgba(247,247,248,var(--tw-bg-opacity)); }

.gizmo .gizmo\:hover\:text-token-text-secondary:hover { color: var(--text-secondary); }

.gizmo .gizmo\:disabled\:bg-black:disabled, .gizmo .gizmo\:enabled\:bg-black:enabled { --tw-bg-opacity: 1; background-color: rgba(0,0,0,var(--tw-bg-opacity)); }

.gizmo .gizmo\:disabled\:opacity-10:disabled { opacity: 0.1; }

.dark .dark\:hidden { display: none; }

:is(.dark .dark\:divide-gray-900) > :not([hidden]) ~ :not([hidden]) { --tw-divide-opacity: 1; border-color: rgba(32,33,35,var(--tw-divide-opacity)); }

.dark .dark\:border { border-width: 1px; }

.dark .dark\:border-0 { border-width: 0px; }

.dark .dark\:border-\[\#4E4F60\] { --tw-border-opacity: 1; border-color: rgba(78,79,96,var(--tw-border-opacity)); }

.dark .dark\:border-black { --tw-border-opacity: 1; border-color: rgba(0,0,0,var(--tw-border-opacity)); }

.dark .dark\:border-black\/20 { border-color: rgba(0, 0, 0, 0.2); }

.dark .dark\:border-gray-200 { --tw-border-opacity: 1; border-color: rgba(217,217,227,var(--tw-border-opacity)); }

.dark .dark\:border-gray-300 { --tw-border-opacity: 1; border-color: rgba(197,197,210,var(--tw-border-opacity)); }

.dark .dark\:border-gray-500 { --tw-border-opacity: 1; border-color: rgba(142,142,160,var(--tw-border-opacity)); }

.dark .dark\:border-gray-600 { --tw-border-opacity: 1; border-color: rgba(86,88,105,var(--tw-border-opacity)); }

.dark .dark\:border-gray-700 { --tw-border-opacity: 1; border-color: rgba(64,65,79,var(--tw-border-opacity)); }

.dark .dark\:border-gray-800 { --tw-border-opacity: 1; border-color: rgba(52,53,65,var(--tw-border-opacity)); }

.dark .dark\:border-gray-900 { --tw-border-opacity: 1; border-color: rgba(32,33,35,var(--tw-border-opacity)); }

.dark .dark\:border-gray-900\/50 { border-color: rgba(32, 33, 35, 0.5); }

.dark .dark\:border-token-border-light { border-color: var(--border-light); }

.dark .dark\:border-white { --tw-border-opacity: 1; border-color: rgba(255,255,255,var(--tw-border-opacity)); }

.dark .dark\:border-white\/10 { border-color: rgba(255, 255, 255, 0.1); }

.dark .dark\:border-white\/20 { border-color: rgba(255, 255, 255, 0.2); }

.dark .dark\:border-white\/30 { border-color: rgba(255, 255, 255, 0.3); }

.dark .dark\:bg-\[\#272832\] { --tw-bg-opacity: 1; background-color: rgba(39,40,50,var(--tw-bg-opacity)); }

.dark .dark\:bg-\[\#444654\] { --tw-bg-opacity: 1; background-color: rgba(68,70,84,var(--tw-bg-opacity)); }

.dark .dark\:bg-\[rgba\(255\,255\,255\,0\.15\)\] { background-color: rgba(255, 255, 255, 0.15); }

.dark .dark\:bg-black { --tw-bg-opacity: 1; background-color: rgba(0,0,0,var(--tw-bg-opacity)); }

.dark .dark\:bg-black\/80 { background-color: rgba(0, 0, 0, 0.8); }

.dark .dark\:bg-brand-green\/50 { background-color: rgba(25, 195, 125, 0.5); }

.dark .dark\:bg-gray-100 { --tw-bg-opacity: 1; background-color: rgba(236,236,241,var(--tw-bg-opacity)); }

.dark .dark\:bg-gray-50 { --tw-bg-opacity: 1; background-color: rgba(247,247,248,var(--tw-bg-opacity)); }

.dark .dark\:bg-gray-500 { --tw-bg-opacity: 1; background-color: rgba(142,142,160,var(--tw-bg-opacity)); }

.dark .dark\:bg-gray-600 { --tw-bg-opacity: 1; background-color: rgba(86,88,105,var(--tw-bg-opacity)); }

.dark .dark\:bg-gray-600\/70 { background-color: rgba(86, 88, 105, 0.7); }

.dark .dark\:bg-gray-700 { --tw-bg-opacity: 1; background-color: rgba(64,65,79,var(--tw-bg-opacity)); }

.dark .dark\:bg-gray-800 { --tw-bg-opacity: 1; background-color: rgba(52,53,65,var(--tw-bg-opacity)); }

.dark .dark\:bg-gray-800\/30 { background-color: rgba(52, 53, 65, 0.3); }

.dark .dark\:bg-gray-800\/90 { background-color: rgba(52, 53, 65, 0.9); }

.dark .dark\:bg-gray-900 { --tw-bg-opacity: 1; background-color: rgba(32,33,35,var(--tw-bg-opacity)); }

.dark .dark\:bg-gray-900\/95 { background-color: rgba(32, 33, 35, 0.95); }

.dark .dark\:bg-gray-950 { --tw-bg-opacity: 1; background-color: rgba(5,5,9,var(--tw-bg-opacity)); }

.dark .dark\:bg-orange-500 { --tw-bg-opacity: 1; background-color: rgba(224,108,43,var(--tw-bg-opacity)); }

.dark .dark\:bg-red-200 { --tw-bg-opacity: 1; background-color: rgba(254,202,202,var(--tw-bg-opacity)); }

.dark .dark\:bg-transparent { background-color: transparent; }

.dark .dark\:bg-white { --tw-bg-opacity: 1; background-color: rgba(255,255,255,var(--tw-bg-opacity)); }

.dark .dark\:bg-white\/10 { background-color: rgba(255, 255, 255, 0.1); }

.dark .dark\:bg-white\/20 { background-color: rgba(255, 255, 255, 0.2); }

.dark .dark\:bg-white\/5 { background-color: rgba(255, 255, 255, 0.05); }

.dark .dark\:from-black { --tw-gradient-from: #000 var(--tw-gradient-from-position); --tw-gradient-to: transparent var(--tw-gradient-to-position); --tw-gradient-stops: var(--tw-gradient-from),var(--tw-gradient-to); }

.dark .dark\:from-gray-700 { --tw-gradient-from: #40414f var(--tw-gradient-from-position); --tw-gradient-to: rgba(64,65,79,0) var(--tw-gradient-to-position); --tw-gradient-stops: var(--tw-gradient-from),var(--tw-gradient-to); }

.dark .dark\:from-gray-800 { --tw-gradient-from: #343541 var(--tw-gradient-from-position); --tw-gradient-to: rgba(52,53,65,0) var(--tw-gradient-to-position); --tw-gradient-stops: var(--tw-gradient-from),var(--tw-gradient-to); }

.dark .dark\:from-gray-900 { --tw-gradient-from: #202123 var(--tw-gradient-from-position); --tw-gradient-to: rgba(32,33,35,0) var(--tw-gradient-to-position); --tw-gradient-stops: var(--tw-gradient-from),var(--tw-gradient-to); }

.dark .dark\:fill-gray-500 { fill: rgb(142, 142, 160); }

.dark .dark\:stroke-brand-purple\/50 { stroke: rgba(171, 104, 255, 0.5); }

.dark .dark\:text-\[\#D292FF\] { --tw-text-opacity: 1; color: rgba(210,146,255,var(--tw-text-opacity)); }

.dark .dark\:text-black { --tw-text-opacity: 1; color: rgba(0,0,0,var(--tw-text-opacity)); }

.dark .dark\:text-blue-200 { --tw-text-opacity: 1; color: rgba(191,219,254,var(--tw-text-opacity)); }

.dark .dark\:text-blue-400 { --tw-text-opacity: 1; color: rgba(58,136,255,var(--tw-text-opacity)); }

.dark .dark\:text-blue-600 { --tw-text-opacity: 1; color: rgba(37,99,235,var(--tw-text-opacity)); }

.dark .dark\:text-gray-100 { --tw-text-opacity: 1; color: rgba(236,236,241,var(--tw-text-opacity)); }

.dark .dark\:text-gray-200 { --tw-text-opacity: 1; color: rgba(217,217,227,var(--tw-text-opacity)); }

.dark .dark\:text-gray-300 { --tw-text-opacity: 1; color: rgba(197,197,210,var(--tw-text-opacity)); }

.dark .dark\:text-gray-400 { --tw-text-opacity: 1; color: rgba(172,172,190,var(--tw-text-opacity)); }

.dark .dark\:text-gray-500 { --tw-text-opacity: 1; color: rgba(142,142,160,var(--tw-text-opacity)); }

.dark .dark\:text-gray-600 { --tw-text-opacity: 1; color: rgba(86,88,105,var(--tw-text-opacity)); }

.dark .dark\:text-gray-700 { --tw-text-opacity: 1; color: rgba(64,65,79,var(--tw-text-opacity)); }

.dark .dark\:text-gray-900 { --tw-text-opacity: 1; color: rgba(32,33,35,var(--tw-text-opacity)); }

.dark .dark\:text-green-100 { --tw-text-opacity: 1; color: rgba(210,244,211,var(--tw-text-opacity)); }

.dark .dark\:text-red-500 { --tw-text-opacity: 1; color: rgba(239,68,68,var(--tw-text-opacity)); }

.dark .dark\:text-red-800 { --tw-text-opacity: 1; color: rgba(153,27,27,var(--tw-text-opacity)); }

.dark .dark\:text-white { --tw-text-opacity: 1; color: rgba(255,255,255,var(--tw-text-opacity)); }

.dark .dark\:text-white\/60 { color: rgba(255, 255, 255, 0.6); }

.dark .dark\:text-white\/70 { color: rgba(255, 255, 255, 0.7); }

.dark .dark\:text-white\/90 { color: rgba(255, 255, 255, 0.9); }

.dark .dark\:shadow-xs { --tw-shadow: 0 0 15px rgba(0,0,0,.1); --tw-shadow-colored: 0 0 15px var(--tw-shadow-color); box-shadow: var(--tw-ring-offset-shadow,0 0 transparent),var(--tw-ring-shadow,0 0 transparent),var(--tw-shadow); }

.dark .dark\:ring-white\/20 { --tw-ring-color: hsla(0,0%,100%,.2); }

:is(.dark .placeholder\:dark\:text-white)::-webkit-input-placeholder { --tw-text-opacity: 1; color: rgba(255,255,255,var(--tw-text-opacity)); }

:is(.dark .placeholder\:dark\:text-white)::placeholder { --tw-text-opacity: 1; color: rgba(255,255,255,var(--tw-text-opacity)); }

.dark .dark\:last\:border-0:last-child { border-width: 0px; }

.dark .dark\:hover\:border-gray-600:hover { --tw-border-opacity: 1; border-color: rgba(86,88,105,var(--tw-border-opacity)); }

.dark .dark\:hover\:border-gray-800:hover { --tw-border-opacity: 1; border-color: rgba(52,53,65,var(--tw-border-opacity)); }

.dark .dark\:hover\:border-white\/10:hover { border-color: rgba(255, 255, 255, 0.1); }

.dark .dark\:hover\:\!bg-gray-700:hover { --tw-bg-opacity: 1 !important; background-color: rgba(64,65,79,var(--tw-bg-opacity)) !important; }

.dark .dark\:hover\:bg-\[\#2A2B32\]:hover { --tw-bg-opacity: 1; background-color: rgba(42,43,50,var(--tw-bg-opacity)); }

.dark .dark\:hover\:bg-\[\#2E2F33\]:hover { --tw-bg-opacity: 1; background-color: rgba(46,47,51,var(--tw-bg-opacity)); }

.dark .dark\:hover\:bg-\[\#494A54\]:hover { --tw-bg-opacity: 1; background-color: rgba(73,74,84,var(--tw-bg-opacity)); }

.dark .dark\:hover\:bg-brand-green\/40:hover { background-color: rgba(25, 195, 125, 0.4); }

.dark .dark\:hover\:bg-gray-200:hover { --tw-bg-opacity: 1; background-color: rgba(217,217,227,var(--tw-bg-opacity)); }

.dark .dark\:hover\:bg-gray-500\/10:hover { background-color: rgba(141, 141, 160, 0.1); }

.dark .dark\:hover\:bg-gray-700:hover { --tw-bg-opacity: 1; background-color: rgba(64,65,79,var(--tw-bg-opacity)); }

.dark .dark\:hover\:bg-gray-800:hover { --tw-bg-opacity: 1; background-color: rgba(52,53,65,var(--tw-bg-opacity)); }

.dark .dark\:hover\:bg-gray-900:hover { --tw-bg-opacity: 1; background-color: rgba(32,33,35,var(--tw-bg-opacity)); }

.dark .dark\:hover\:bg-gray-950:hover { --tw-bg-opacity: 1; background-color: rgba(5,5,9,var(--tw-bg-opacity)); }

.dark .dark\:hover\:bg-white\/20:hover { background-color: rgba(255, 255, 255, 0.2); }

:is(.dark .hover\:dark\:bg-gray-700):hover { --tw-bg-opacity: 1; background-color: rgba(64,65,79,var(--tw-bg-opacity)); }

:is(.dark .hover\:dark\:bg-white\/10):hover { background-color: rgba(255, 255, 255, 0.1); }

.dark .dark\:hover\:text-blue-600:hover { --tw-text-opacity: 1; color: rgba(37,99,235,var(--tw-text-opacity)); }

.dark .dark\:hover\:text-gray-200:hover { --tw-text-opacity: 1; color: rgba(217,217,227,var(--tw-text-opacity)); }

.dark .dark\:hover\:text-gray-400:hover { --tw-text-opacity: 1; color: rgba(172,172,190,var(--tw-text-opacity)); }

.dark .dark\:hover\:text-white:hover { --tw-text-opacity: 1; color: rgba(255,255,255,var(--tw-text-opacity)); }

.dark .dark\:hover\:text-white\/50:hover { color: rgba(255, 255, 255, 0.5); }

:is(.dark .hover\:dark\:text-gray-100):hover { --tw-text-opacity: 1; color: rgba(236,236,241,var(--tw-text-opacity)); }

:is(.dark .hover\:dark\:text-gray-300):hover { --tw-text-opacity: 1; color: rgba(197,197,210,var(--tw-text-opacity)); }

.dark .dark\:focus\:border-white:focus { --tw-border-opacity: 1; border-color: rgba(255,255,255,var(--tw-border-opacity)); }

.dark .dark\:focus\:bg-gray-700:focus { --tw-bg-opacity: 1; background-color: rgba(64,65,79,var(--tw-bg-opacity)); }

:is(.dark .focus\:dark\:bg-gray-700):focus { --tw-bg-opacity: 1; background-color: rgba(64,65,79,var(--tw-bg-opacity)); }

.dark .dark\:focus\:ring-gray-600:focus { --tw-ring-opacity: 1; --tw-ring-color: rgba(86,88,105,var(--tw-ring-opacity)); }

.dark .dark\:focus\:ring-white:focus { --tw-ring-opacity: 1; --tw-ring-color: rgba(255,255,255,var(--tw-ring-opacity)); }

.dark .dark\:focus-visible\:border-blue-500.focus-visible { --tw-border-opacity: 1; border-color: rgba(69,89,164,var(--tw-border-opacity)); }

.dark .dark\:focus-visible\:border-blue-500:focus-visible { --tw-border-opacity: 1; border-color: rgba(69,89,164,var(--tw-border-opacity)); }

.dark .dark\:disabled\:text-gray-400:disabled { --tw-text-opacity: 1; color: rgba(172,172,190,var(--tw-text-opacity)); }

.dark .dark\:disabled\:hover\:bg-transparent:hover:disabled { background-color: transparent; }

.dark .dark\:disabled\:hover\:bg-white:hover:disabled { --tw-bg-opacity: 1; background-color: rgba(255,255,255,var(--tw-bg-opacity)); }

:is(.dark .disabled\:dark\:hover\:text-gray-400:hover):disabled { --tw-text-opacity: 1; color: rgba(172,172,190,var(--tw-text-opacity)); }

.dark .group:hover .dark\:group-hover\:from-\[\#2A2B32\] { --tw-gradient-from: #2a2b32 var(--tw-gradient-from-position); --tw-gradient-to: rgba(42,43,50,0) var(--tw-gradient-to-position); --tw-gradient-stops: var(--tw-gradient-from),var(--tw-gradient-to); }

.dark .group\/option:hover .dark\:group-hover\/option\:text-gray-100 { --tw-text-opacity: 1; color: rgba(236,236,241,var(--tw-text-opacity)); }

.dark .dark\:radix-disabled\:bg-transparent[data-disabled] { background-color: transparent; }

.dark .dark\:radix-disabled\:text-gray-500[data-disabled] { --tw-text-opacity: 1; color: rgba(142,142,160,var(--tw-text-opacity)); }

.dark .dark\:radix-disabled\:text-gray-600[data-disabled] { --tw-text-opacity: 1; color: rgba(86,88,105,var(--tw-text-opacity)); }

.dark .dark\:radix-highlighted\:bg-\[\#2E2F33\][data-highlighted] { --tw-bg-opacity: 1; background-color: rgba(46,47,51,var(--tw-bg-opacity)); }

.dark .dark\:radix-highlighted\:bg-gray-700[data-highlighted] { --tw-bg-opacity: 1; background-color: rgba(64,65,79,var(--tw-bg-opacity)); }

.dark .dark\:radix-highlighted\:hover\:bg-\[\#2E2F33\][data-highlighted]:hover { --tw-bg-opacity: 1; background-color: rgba(46,47,51,var(--tw-bg-opacity)); }

.dark .dark\:radix-state-active\:text-white[data-state="active"] { --tw-text-opacity: 1; color: rgba(255,255,255,var(--tw-text-opacity)); }

.dark .dark\:radix-state-checked\:bg-green-600[data-state="checked"] { --tw-bg-opacity: 1; background-color: rgba(16,163,127,var(--tw-bg-opacity)); }

.dark .dark\:radix-state-open\:bg-gray-700[data-state="open"] { --tw-bg-opacity: 1; background-color: rgba(64,65,79,var(--tw-bg-opacity)); }

.dark .dark\:radix-state-open\:bg-gray-800[data-state="open"] { --tw-bg-opacity: 1; background-color: rgba(52,53,65,var(--tw-bg-opacity)); }

.dark .dark\:radix-state-open\:bg-gray-950[data-state="open"] { --tw-bg-opacity: 1; background-color: rgba(5,5,9,var(--tw-bg-opacity)); }

.dark .dark\:radix-state-open\:text-gray-400[data-state="open"] { --tw-text-opacity: 1; color: rgba(172,172,190,var(--tw-text-opacity)); }

.dark .dark\:group-ui-open\:bg-gray-800.group[data-headlessui-state~="open"] { --tw-bg-opacity: 1; background-color: rgba(52,53,65,var(--tw-bg-opacity)); }

.dark .group:where([data-headlessui-state~="open"]) .dark\:group-ui-open\:bg-gray-800 { --tw-bg-opacity: 1; background-color: rgba(52,53,65,var(--tw-bg-opacity)); }

.gizmo :is(.dark .gizmo\:dark\:border-0) { border-width: 0px; }

.gizmo :is(.dark .gizmo\:dark\:border-white) { --tw-border-opacity: 1; border-color: rgba(255,255,255,var(--tw-border-opacity)); }

.gizmo :is(.dark .gizmo\:dark\:border-white\/30) { border-color: rgba(255, 255, 255, 0.3); }

.gizmo :is(.dark .gizmo\:dark\:bg-black) { --tw-bg-opacity: 1; background-color: rgba(0,0,0,var(--tw-bg-opacity)); }

.gizmo :is(.dark .gizmo\:dark\:bg-gray-800) { --tw-bg-opacity: 1; background-color: rgba(52,53,65,var(--tw-bg-opacity)); }

.gizmo :is(.dark .gizmo\:dark\:bg-token-surface-primary) { background-color: var(--surface-primary); }

.gizmo :is(.dark .gizmo\:dark\:bg-transparent) { background-color: transparent; }

.gizmo :is(.dark .gizmo\:dark\:bg-white) { --tw-bg-opacity: 1; background-color: rgba(255,255,255,var(--tw-bg-opacity)); }

.gizmo :is(.dark .gizmo\:dark\:placeholder-white\/50)::-webkit-input-placeholder { color: rgba(255, 255, 255, 0.5); }

.gizmo :is(.dark .gizmo\:dark\:placeholder-white\/50)::placeholder { color: rgba(255, 255, 255, 0.5); }

.gizmo :is(.dark .gizmo\:dark\:shadow-\[0_0_0_2px_rgba\(52\,53\,65\,0\.95\)\]) { --tw-shadow: 0 0 0 2px rgba(52,53,65,.95); --tw-shadow-colored: 0 0 0 2px var(--tw-shadow-color); box-shadow: var(--tw-ring-offset-shadow,0 0 transparent),var(--tw-ring-shadow,0 0 transparent),var(--tw-shadow); }

.gizmo :is(.dark .gizmo\:dark\:disabled\:bg-white:disabled) { --tw-bg-opacity: 1; background-color: rgba(255,255,255,var(--tw-bg-opacity)); }

@media (min-width: 640px) {
  .sm\:absolute { position: absolute; }
  .sm\:-mx-6 { margin-left: -1.5rem; margin-right: -1.5rem; }
  .sm\:mx-1 { margin-left: 0.25rem; margin-right: 0.25rem; }
  .sm\:mb-0 { margin-bottom: 0px; }
  .sm\:mb-0\.5 { margin-bottom: 0.125rem; }
  .sm\:mb-16 { margin-bottom: 4rem; }
  .sm\:mb-2 { margin-bottom: 0.5rem; }
  .sm\:mb-4 { margin-bottom: 1rem; }
  .sm\:ml-0 { margin-left: 0px; }
  .sm\:mt-0 { margin-top: 0px; }
  .sm\:mt-16 { margin-top: 4rem; }
  .sm\:mt-4 { margin-top: 1rem; }
  .sm\:mt-\[20vh\] { margin-top: 20vh; }
  .sm\:mt-\[6px\] { margin-top: 6px; }
  .sm\:mt-\[6vh\] { margin-top: 6vh; }
  .sm\:block { display: block; }
  .sm\:inline-block { display: inline-block; }
  .sm\:flex { display: flex; }
  .sm\:h-10 { height: 2.5rem; }
  .sm\:h-5 { height: 1.25rem; }
  .sm\:w-10 { width: 2.5rem; }
  .sm\:w-104 { width: 26rem; }
  .sm\:w-5 { width: 1.25rem; }
  .sm\:w-\[312px\] { width: 312px; }
  .sm\:w-\[calc\(\(100\%-12px\)\/2\)\] { width: calc(50% - 6px); }
  .sm\:w-auto { width: auto; }
  .sm\:min-w-\[148px\] { min-width: 148px; }
  .sm\:min-w-\[312px\] { min-width: 312px; }
  .sm\:min-w-\[420px\] { min-width: 420px; }
  .sm\:max-w-2xl { max-width: 42rem; }
  .sm\:max-w-lg { max-width: 32rem; }
  .sm\:max-w-md { max-width: 28rem; }
  .sm\:max-w-xs { max-width: 20rem; }
  .sm\:grid-cols-2 { grid-template-columns: repeat(2, minmax(0px, 1fr)); }
  .sm\:grid-cols-3 { grid-template-columns: repeat(3, minmax(0px, 1fr)); }
  .sm\:grid-rows-2 { grid-template-rows: repeat(2, minmax(0px, 1fr)); }
  .sm\:flex-row { flex-direction: row; }
  .sm\:flex-row-reverse { flex-direction: row-reverse; }
  .sm\:items-center { align-items: center; }
  .sm\:justify-center { justify-content: center; }
  .sm\:gap-3 { gap: 0.75rem; }
  .sm\:gap-6 { gap: 1.5rem; }
  .sm\:gap-y-0 { row-gap: 0px; }
  .sm\:rounded-md { border-radius: 0.375rem; }
  .sm\:rounded-t-\[30px\] { border-top-left-radius: 30px; border-top-right-radius: 30px; }
  .sm\:border-r { border-right-width: 1px; }
  .sm\:border-t-0 { border-top-width: 0px; }
  .sm\:p-0 { padding: 0px; }
  .sm\:p-2 { padding: 0.5rem; }
  .sm\:p-3 { padding: 0.75rem; }
  .sm\:p-4 { padding: 1rem; }
  .sm\:p-6 { padding: 1.5rem; }
  .sm\:px-0 { padding-left: 0px; padding-right: 0px; }
  .sm\:px-2 { padding-left: 0.5rem; padding-right: 0.5rem; }
  .sm\:px-6 { padding-left: 1.5rem; padding-right: 1.5rem; }
  .sm\:pb-0 { padding-bottom: 0px; }
  .sm\:pb-6 { padding-bottom: 1.5rem; }
  .sm\:pl-3 { padding-left: 0.75rem; }
  .sm\:pr-2 { padding-right: 0.5rem; }
  .sm\:pt-4 { padding-top: 1rem; }
  .sm\:pt-8 { padding-top: 2rem; }
  .sm\:text-left { text-align: left; }
  .sm\:text-4xl { font-size: 2.25rem; line-height: 2.5rem; }
  .sm\:text-base { font-size: 1rem; line-height: 1.5rem; }
  .sm\:text-lg { font-size: 1.125rem; line-height: 1.75rem; }
  .sm\:text-sm { font-size: 0.875rem; line-height: 1.25rem; }
  .sm\:duration-300 { transition-duration: 0.3s; }
  .sm\:duration-500 { transition-duration: 0.5s; }
  @supports (min-height:100dvh) {
  .sm\:supports-\[min-height\:100dvh\]\:min-h-\[100dvh\] { min-height: 100dvh; }
}
}

@media (min-width: 768px) {
  .md\:invisible { visibility: hidden; }
  .md\:static { position: static; }
  .md\:relative { position: relative; }
  .md\:-right-2 { right: -0.5rem; }
  .md\:bottom-14 { bottom: 3.5rem; }
  .md\:bottom-3 { bottom: 0.75rem; }
  .md\:bottom-3\.5 { bottom: 0.875rem; }
  .md\:bottom-\[180px\] { bottom: 180px; }
  .md\:bottom-auto { bottom: auto; }
  .md\:left-4 { left: 1rem; }
  .md\:left-auto { left: auto; }
  .md\:right-3 { right: 0.75rem; }
  .md\:right-auto { right: auto; }
  .md\:top-\[22px\] { top: 22px; }
  .md\:m-0 { margin: 0px; }
  .md\:m-auto { margin: auto; }
  .md\:mx-4 { margin-left: 1rem; margin-right: 1rem; }
  .md\:mb-0 { margin-bottom: 0px; }
  .md\:mb-16 { margin-bottom: 4rem; }
  .md\:mb-3 { margin-bottom: 0.75rem; }
  .md\:mb-4 { margin-bottom: 1rem; }
  .md\:mb-auto { margin-bottom: auto; }
  .md\:mt-2 { margin-top: 0.5rem; }
  .md\:block { display: block; }
  .md\:inline-block { display: inline-block; }
  .md\:flex { display: flex; }
  .md\:inline-flex { display: inline-flex; }
  .md\:grid { display: grid; }
  .md\:hidden { display: none; }
  .md\:h-48 { height: 12rem; }
  .md\:h-full { height: 100%; }
  .md\:min-h-\[300px\] { min-height: 300px; }
  .md\:w-1\/2 { width: 50%; }
  .md\:w-\[100\%\] { width: 100%; }
  .md\:w-\[250px\] { width: 250px; }
  .md\:w-\[500px\] { width: 500px; }
  .md\:w-\[672px\] { width: 672px; }
  .md\:w-\[calc\(100\%-\.5rem\)\] { width: calc(100% - 0.5rem); }
  .md\:w-auto { width: auto; }
  .md\:w-full { width: 100%; }
  .md\:min-w-\[672px\] { min-width: 672px; }
  .md\:max-w-2xl { max-width: 42rem; }
  .md\:max-w-3xl { max-width: 48rem; }
  .md\:max-w-\[672px\] { max-width: 672px; }
  .md\:max-w-\[680px\] { max-width: 680px; }
  .md\:max-w-none { max-width: none; }
  .md\:max-w-xl { max-width: 36rem; }
  .md\:basis-0 { flex-basis: 0px; }
  .md\:basis-\[25vw\] { flex-basis: 25vw; }
  .md\:basis-\[75vw\] { flex-basis: 75vw; }
  .md\:grid-cols-2 { grid-template-columns: repeat(2, minmax(0px, 1fr)); }
  .md\:grid-rows-\[minmax\(20px\,_1fr\)_auto_minmax\(20px\,_1fr\)\] { grid-template-rows: minmax(20px, 1fr) auto minmax(20px, 1fr); }
  .md\:flex-row { flex-direction: row; }
  .md\:flex-col { flex-direction: column; }
  .md\:items-end { align-items: flex-end; }
  .md\:justify-between { justify-content: space-between; }
  .md\:gap-2 { gap: 0.5rem; }
  .md\:gap-3 { gap: 0.75rem; }
  .md\:gap-6 { gap: 1.5rem; }
  .md\:gap-x-12 { column-gap: 3rem; }
  .md\:overflow-visible { overflow: visible; }
  .md\:whitespace-normal { white-space: normal; }
  .md\:rounded-none { border-radius: 0px; }
  .md\:border { border-width: 1px; }
  .md\:border-l { border-left-width: 1px; }
  .md\:border-t-0 { border-top-width: 0px; }
  .md\:border-gray-100 { --tw-border-opacity: 1; border-color: rgba(236,236,241,var(--tw-border-opacity)); }
  .md\:border-transparent { border-color: transparent; }
  .md\:\!bg-transparent { background-color: transparent !important; }
  .md\:bg-vert-light-gradient { background-image: linear-gradient(rgba(255, 255, 255, 0) 13.94%, rgb(255, 255, 255) 54.73%); }
  .md\:p-2 { padding: 0.5rem; }
  .md\:p-6 { padding: 1.5rem; }
  .md\:px-0 { padding-left: 0px; padding-right: 0px; }
  .md\:px-6 { padding-left: 1.5rem; padding-right: 1.5rem; }
  .md\:px-8 { padding-left: 2rem; padding-right: 2rem; }
  .md\:px-\[60px\] { padding-left: 60px; padding-right: 60px; }
  .md\:py-10 { padding-bottom: 2.5rem; padding-top: 2.5rem; }
  .md\:py-2 { padding-bottom: 0.5rem; padding-top: 0.5rem; }
  .md\:py-2\.5 { padding-bottom: 0.625rem; padding-top: 0.625rem; }
  .md\:py-4 { padding-bottom: 1rem; padding-top: 1rem; }
  .md\:py-6 { padding-bottom: 1.5rem; padding-top: 1.5rem; }
  .md\:py-\[22px\] { padding-bottom: 22px; padding-top: 22px; }
  .md\:\!pt-0 { padding-top: 0px !important; }
  .md\:pb-5 { padding-bottom: 1.25rem; }
  .md\:pb-\[8vh\] { padding-bottom: 8vh; }
  .md\:pl-0 { padding-left: 0px; }
  .md\:pl-2 { padding-left: 0.5rem; }
  .md\:pl-4 { padding-left: 1rem; }
  .md\:pl-6 { padding-left: 1.5rem; }
  .md\:pl-\[46px\] { padding-left: 46px; }
  .md\:pr-1 { padding-right: 0.25rem; }
  .md\:pr-1\.5 { padding-right: 0.375rem; }
  .md\:pr-12 { padding-right: 3rem; }
  .md\:pr-4 { padding-right: 1rem; }
  .md\:pr-8 { padding-right: 2rem; }
  .md\:pt-0 { padding-top: 0px; }
  .md\:text-justify { text-align: justify; }
  .md\:text-\[32px\] { font-size: 32px; }
  .md\:text-\[40px\] { font-size: 40px; }
  .md\:text-sm { font-size: 0.875rem; line-height: 1.25rem; }
  .md\:leading-8 { line-height: 2rem; }
  .md\:opacity-0 { opacity: 0; }
  .md\:last\:mb-6:last-child { margin-bottom: 1.5rem; }
  .md\:disabled\:bottom-0:disabled { bottom: 0px; }
  .group.final-completion .md\:group-\[\.final-completion\]\:visible, .group:hover .md\:group-hover\:visible { visibility: visible; }
  .gizmo .gizmo\:md\:bottom-2 { bottom: 0.5rem; }
  .gizmo .gizmo\:md\:bottom-2\.5 { bottom: 0.625rem; }
  .gizmo .gizmo\:md\:bottom-3 { bottom: 0.75rem; }
  .gizmo .gizmo\:md\:bottom-\[143px\] { bottom: 143px; }
  .gizmo .gizmo\:md\:left-4 { left: 1rem; }
  .gizmo .gizmo\:md\:max-w-3xl { max-width: 48rem; }
  .gizmo .gizmo\:md\:px-5 { padding-left: 1.25rem; padding-right: 1.25rem; }
  .gizmo .gizmo\:md\:py-3 { padding-bottom: 0.75rem; padding-top: 0.75rem; }
  .gizmo .gizmo\:md\:py-3\.5 { padding-bottom: 0.875rem; padding-top: 0.875rem; }
  .gizmo .gizmo\:md\:pl-0 { padding-left: 0px; }
  .gizmo .gizmo\:md\:pl-\[55px\] { padding-left: 55px; }
  .dark .md\:dark\:border-gray-700 { --tw-border-opacity: 1; border-color: rgba(64,65,79,var(--tw-border-opacity)); }
  .dark .md\:dark\:border-transparent { border-color: transparent; }
  .dark .dark\:md\:bg-vert-dark-gradient { background-image: linear-gradient(rgba(53, 55, 64, 0), rgb(53, 55, 64) 58.85%); }
}

@media (min-width: 1024px) {
  .lg\:absolute { position: absolute; }
  .lg\:relative { position: relative; }
  .lg\:-left-3 { left: -0.75rem; }
  .lg\:-right-3 { right: -0.75rem; }
  .lg\:bottom-\[120px\] { bottom: 120px; }
  .lg\:left-auto { left: auto; }
  .lg\:right-0 { right: 0px; }
  .lg\:top-0 { top: 0px; }
  .lg\:top-1\/2 { top: 50%; }
  .lg\:-mx-8 { margin-left: -2rem; margin-right: -2rem; }
  .lg\:mx-auto { margin-left: auto; margin-right: auto; }
  .lg\:-ml-2 { margin-left: -0.5rem; }
  .lg\:mb-0 { margin-bottom: 0px; }
  .lg\:mt-0 { margin-top: 0px; }
  .lg\:block { display: block; }
  .lg\:hidden { display: none; }
  .lg\:w-1\/3 { width: 33.3333%; }
  .lg\:w-3\/5 { width: 60%; }
  .lg\:w-\[300px\] { width: 300px; }
  .lg\:w-\[896px\] { width: 896px; }
  .lg\:w-\[calc\(\(100\%-24px\)\/3\)\] { width: calc(33.3333% - 8px); }
  .lg\:w-\[calc\(100\%-115px\)\] { width: calc(100% - 115px); }
  .lg\:w-auto { width: auto; }
  .lg\:min-w-\[896px\] { min-width: 896px; }
  .lg\:max-w-2xl { max-width: 42rem; }
  .lg\:max-w-3xl { max-width: 48rem; }
  .lg\:max-w-\[38rem\] { max-width: 38rem; }
  .lg\:max-w-\[796px\] { max-width: 796px; }
  .lg\:max-w-\[896px\] { max-width: 896px; }
  .lg\:max-w-none { max-width: none; }
  .lg\:max-w-sm { max-width: 24rem; }
  .lg\:-translate-x-full { --tw-translate-x: -100%; }
  .lg\:-translate-x-full, .lg\:-translate-y-1\/2 { transform: translate(var(--tw-translate-x),var(--tw-translate-y)) rotate(var(--tw-rotate)) skewX(var(--tw-skew-x)) skewY(var(--tw-skew-y)) scaleX(var(--tw-scale-x)) scaleY(var(--tw-scale-y)); }
  .lg\:-translate-y-1\/2 { --tw-translate-y: -50%; }
  .lg\:translate-x-full { --tw-translate-x: 100%; }
  .lg\:translate-x-full, .lg\:translate-y-0 { transform: translate(var(--tw-translate-x),var(--tw-translate-y)) rotate(var(--tw-rotate)) skewX(var(--tw-skew-x)) skewY(var(--tw-skew-y)) scaleX(var(--tw-scale-x)) scaleY(var(--tw-scale-y)); }
  .lg\:translate-y-0 { --tw-translate-y: 0px; }
  .lg\:grid-cols-3 { grid-template-columns: repeat(3, minmax(0px, 1fr)); }
  .lg\:grid-cols-4 { grid-template-columns: repeat(4, minmax(0px, 1fr)); }
  .lg\:grid-cols-\[60\%_40\%\] { grid-template-columns: 60% 40%; }
  .lg\:gap-1 { gap: 0.25rem; }
  .lg\:self-center { align-self: center; }
  .lg\:rounded-lg { border-radius: 0.5rem; }
  .lg\:rounded-none { border-radius: 0px; }
  .lg\:border { border-width: 1px; }
  .lg\:border-0 { border-width: 0px; }
  .lg\:p-0 { padding: 0px; }
  .lg\:p-6 { padding: 1.5rem; }
  .lg\:px-0 { padding-left: 0px; padding-right: 0px; }
  .lg\:px-14 { padding-left: 3.5rem; padding-right: 3.5rem; }
  .lg\:px-16 { padding-left: 4rem; padding-right: 4rem; }
  .lg\:px-8 { padding-left: 2rem; padding-right: 2rem; }
  .lg\:pl-2 { padding-left: 0.5rem; }
  .lg\:pl-8 { padding-left: 2rem; }
  .lg\:pr-10 { padding-right: 2.5rem; }
  .lg\:pt-32 { padding-top: 8rem; }
  .lg\:text-left { text-align: left; }
  .lg\:text-\[22px\] { font-size: 22px; }
  .gizmo .gizmo\:lg\:flex { display: flex; }
  .gizmo .gizmo\:lg\:max-w-\[40rem\] { max-width: 40rem; }
  .gizmo .gizmo\:lg\:justify-start { justify-content: flex-start; }
  .gizmo .gizmo\:lg\:px-1 { padding-left: 0.25rem; padding-right: 0.25rem; }
}

@media (min-width: 1280px) {
  .xl\:w-1\/4 { width: 25%; }
  .xl\:w-\[1024px\] { width: 1024px; }
  .xl\:w-\[350px\] { width: 350px; }
  .xl\:min-w-\[1024px\] { min-width: 1024px; }
  .xl\:max-w-3xl { max-width: 48rem; }
  .xl\:max-w-4xl { max-width: 56rem; }
  .xl\:max-w-6xl { max-width: 72rem; }
  .xl\:max-w-xl { max-width: 36rem; }
  .xl\:max-w-xs { max-width: 20rem; }
  .xl\:grid-cols-4 { grid-template-columns: repeat(4, minmax(0px, 1fr)); }
  .gizmo .gizmo\:xl\:max-w-\[48rem\] { max-width: 48rem; }
  .gizmo .gizmo\:xl\:px-5 { padding-left: 1.25rem; padding-right: 1.25rem; }
}

@media (min-width: 1536px) {
  .\32 xl\:w-\[400px\] { width: 400px; }
}

tr:last-child .\[tr\:last-child_\&\]\:border-b-0 { border-bottom-width: 0px; }

tr[data-disabled="true"] .\[tr\[data-disabled\=true\]_\&\]\:opacity-50 { opacity: 0.5; }

@font-face { font-family: KaTeX_AMS; font-style: normal; font-weight: 400; src: url("https://cdn.openai.com/common/fonts/katex/KaTeX_AMS-Regular.woff2") format("woff2"); }

@font-face { font-family: KaTeX_Caligraphic; font-style: normal; font-weight: 700; src: url("https://cdn.openai.com/common/fonts/katex/KaTeX_Caligraphic-Bold.woff2") format("woff2"); }

@font-face { font-family: KaTeX_Caligraphic; font-style: normal; font-weight: 400; src: url("https://cdn.openai.com/common/fonts/katex/KaTeX_Caligraphic-Regular.woff2") format("woff2"); }

@font-face { font-family: KaTeX_Fraktur; font-style: normal; font-weight: 700; src: url("https://cdn.openai.com/common/fonts/katex/KaTeX_Fraktur-Bold.woff2") format("woff2"); }

@font-face { font-family: KaTeX_Fraktur; font-style: normal; font-weight: 400; src: url("https://cdn.openai.com/common/fonts/katex/KaTeX_Fraktur-Regular.woff2") format("woff2"); }

@font-face { font-family: KaTeX_Main; font-style: normal; font-weight: 700; src: url("https://cdn.openai.com/common/fonts/katex/KaTeX_Main-Bold.woff2") format("woff2"); }

@font-face { font-family: KaTeX_Main; font-style: italic; font-weight: 700; src: url("https://cdn.openai.com/common/fonts/katex/KaTeX_Main-BoldItalic.woff2") format("woff2"); }

@font-face { font-family: KaTeX_Main; font-style: italic; font-weight: 400; src: url("https://cdn.openai.com/common/fonts/katex/KaTeX_Main-Italic.woff2") format("woff2"); }

@font-face { font-family: KaTeX_Main; font-style: normal; font-weight: 400; src: url("https://cdn.openai.com/common/fonts/katex/KaTeX_Main-Regular.woff2") format("woff2"); }

@font-face { font-family: KaTeX_Math; font-style: italic; font-weight: 700; src: url("https://cdn.openai.com/common/fonts/katex/KaTeX_Math-BoldItalic.woff2") format("woff2"); }

@font-face { font-family: KaTeX_Math; font-style: italic; font-weight: 400; src: url("https://cdn.openai.com/common/fonts/katex/KaTeX_Math-Italic.woff2") format("woff2"); }

@font-face { font-family: KaTeX_SansSerif; font-style: normal; font-weight: 700; src: url("https://cdn.openai.com/common/fonts/katex/KaTeX_SansSerif-Bold.woff2") format("woff2"); }

@font-face { font-family: KaTeX_SansSerif; font-style: italic; font-weight: 400; src: url("https://cdn.openai.com/common/fonts/katex/KaTeX_SansSerif-Italic.woff2") format("woff2"); }

@font-face { font-family: KaTeX_SansSerif; font-style: normal; font-weight: 400; src: url("https://cdn.openai.com/common/fonts/katex/KaTeX_SansSerif-Regular.woff2") format("woff2"); }

@font-face { font-family: KaTeX_Script; font-style: normal; font-weight: 400; src: url("https://cdn.openai.com/common/fonts/katex/KaTeX_Script-Regular.woff2") format("woff2"); }

@font-face { font-family: KaTeX_Size1; font-style: normal; font-weight: 400; src: url("https://cdn.openai.com/common/fonts/katex/KaTeX_Size1-Regular.woff2") format("woff2"); }

@font-face { font-family: KaTeX_Size2; font-style: normal; font-weight: 400; src: url("https://cdn.openai.com/common/fonts/katex/KaTeX_Size2-Regular.woff2") format("woff2"); }

@font-face { font-family: KaTeX_Size3; font-style: normal; font-weight: 400; src: url("https://cdn.openai.com/common/fonts/katex/KaTeX_Size3-Regular.woff2") format("woff2"); }

@font-face { font-family: KaTeX_Size4; font-style: normal; font-weight: 400; src: url("https://cdn.openai.com/common/fonts/katex/KaTeX_Size4-Regular.woff2") format("woff2"); }

@font-face { font-family: KaTeX_Typewriter; font-style: normal; font-weight: 400; src: url("https://cdn.openai.com/common/fonts/katex/KaTeX_Typewriter-Regular.woff2") format("woff2"); }

.katex { font: 1.21em / 1.2 KaTeX_Main, "Times New Roman", serif; text-indent: 0px; text-rendering: auto; }

.katex * { border-color: currentcolor; forced-color-adjust: none !important; }

.katex .katex-version::after { content: "0.16.0"; }

.katex .katex-mathml { clip: rect(1px, 1px, 1px, 1px); border: 0px; height: 1px; overflow: hidden; padding: 0px; position: absolute; width: 1px; }

.katex .katex-html > .newline { display: block; }

.katex .base { position: relative; white-space: nowrap; width: min-content; }

.katex .base, .katex .strut { display: inline-block; }

.katex .textbf { font-weight: 700; }

.katex .textit { font-style: italic; }

.katex .textrm { font-family: KaTeX_Main; }

.katex .textsf { font-family: KaTeX_SansSerif; }

.katex .texttt { font-family: KaTeX_Typewriter; }

.katex .mathnormal { font-family: KaTeX_Math; font-style: italic; }

.katex .mathit { font-family: KaTeX_Main; font-style: italic; }

.katex .mathrm { font-style: normal; }

.katex .mathbf { font-family: KaTeX_Main; font-weight: 700; }

.katex .boldsymbol { font-family: KaTeX_Math; font-style: italic; font-weight: 700; }

.katex .amsrm, .katex .mathbb, .katex .textbb { font-family: KaTeX_AMS; }

.katex .mathcal { font-family: KaTeX_Caligraphic; }

.katex .mathfrak, .katex .textfrak { font-family: KaTeX_Fraktur; }

.katex .mathtt { font-family: KaTeX_Typewriter; }

.katex .mathscr, .katex .textscr { font-family: KaTeX_Script; }

.katex .mathsf, .katex .textsf { font-family: KaTeX_SansSerif; }

.katex .mathboldsf, .katex .textboldsf { font-family: KaTeX_SansSerif; font-weight: 700; }

.katex .mathitsf, .katex .textitsf { font-family: KaTeX_SansSerif; font-style: italic; }

.katex .mainrm { font-family: KaTeX_Main; font-style: normal; }

.katex .vlist-t { border-collapse: collapse; display: inline-table; table-layout: fixed; }

.katex .vlist-r { display: table-row; }

.katex .vlist { display: table-cell; position: relative; vertical-align: bottom; }

.katex .vlist > span { display: block; height: 0px; position: relative; }

.katex .vlist > span > span { display: inline-block; }

.katex .vlist > span > .pstrut { overflow: hidden; width: 0px; }

.katex .vlist-t2 { margin-right: -2px; }

.katex .vlist-s { display: table-cell; font-size: 1px; min-width: 2px; vertical-align: bottom; width: 2px; }

.katex .vbox { align-items: baseline; display: inline-flex; flex-direction: column; }

.katex .hbox { width: 100%; }

.katex .hbox, .katex .thinbox { display: inline-flex; flex-direction: row; }

.katex .thinbox { max-width: 0px; width: 0px; }

.katex .msupsub { text-align: left; }

.katex .mfrac > span > span { text-align: center; }

.katex .mfrac .frac-line { border-bottom-style: solid; display: inline-block; width: 100%; }

.katex .hdashline, .katex .hline, .katex .mfrac .frac-line, .katex .overline .overline-line, .katex .rule, .katex .underline .underline-line { min-height: 1px; }

.katex .mspace { display: inline-block; }

.katex .clap, .katex .llap, .katex .rlap { position: relative; width: 0px; }

.katex .clap > .inner, .katex .llap > .inner, .katex .rlap > .inner { position: absolute; }

.katex .clap > .fix, .katex .llap > .fix, .katex .rlap > .fix { display: inline-block; }

.katex .llap > .inner { right: 0px; }

.katex .clap > .inner, .katex .rlap > .inner { left: 0px; }

.katex .clap > .inner > span { margin-left: -50%; margin-right: 50%; }

.katex .rule { border: 0px solid; display: inline-block; position: relative; }

.katex .hline, .katex .overline .overline-line, .katex .underline .underline-line { border-bottom-style: solid; display: inline-block; width: 100%; }

.katex .hdashline { border-bottom-style: dashed; display: inline-block; width: 100%; }

.katex .sqrt > .root { margin-left: 0.277778em; margin-right: -0.555556em; }

.katex .fontsize-ensurer.reset-size1.size1, .katex .sizing.reset-size1.size1 { font-size: 1em; }

.katex .fontsize-ensurer.reset-size1.size2, .katex .sizing.reset-size1.size2 { font-size: 1.2em; }

.katex .fontsize-ensurer.reset-size1.size3, .katex .sizing.reset-size1.size3 { font-size: 1.4em; }

.katex .fontsize-ensurer.reset-size1.size4, .katex .sizing.reset-size1.size4 { font-size: 1.6em; }

.katex .fontsize-ensurer.reset-size1.size5, .katex .sizing.reset-size1.size5 { font-size: 1.8em; }

.katex .fontsize-ensurer.reset-size1.size6, .katex .sizing.reset-size1.size6 { font-size: 2em; }

.katex .fontsize-ensurer.reset-size1.size7, .katex .sizing.reset-size1.size7 { font-size: 2.4em; }

.katex .fontsize-ensurer.reset-size1.size8, .katex .sizing.reset-size1.size8 { font-size: 2.88em; }

.katex .fontsize-ensurer.reset-size1.size9, .katex .sizing.reset-size1.size9 { font-size: 3.456em; }

.katex .fontsize-ensurer.reset-size1.size10, .katex .sizing.reset-size1.size10 { font-size: 4.148em; }

.katex .fontsize-ensurer.reset-size1.size11, .katex .sizing.reset-size1.size11 { font-size: 4.976em; }

.katex .fontsize-ensurer.reset-size2.size1, .katex .sizing.reset-size2.size1 { font-size: 0.833333em; }

.katex .fontsize-ensurer.reset-size2.size2, .katex .sizing.reset-size2.size2 { font-size: 1em; }

.katex .fontsize-ensurer.reset-size2.size3, .katex .sizing.reset-size2.size3 { font-size: 1.16667em; }

.katex .fontsize-ensurer.reset-size2.size4, .katex .sizing.reset-size2.size4 { font-size: 1.33333em; }

.katex .fontsize-ensurer.reset-size2.size5, .katex .sizing.reset-size2.size5 { font-size: 1.5em; }

.katex .fontsize-ensurer.reset-size2.size6, .katex .sizing.reset-size2.size6 { font-size: 1.66667em; }

.katex .fontsize-ensurer.reset-size2.size7, .katex .sizing.reset-size2.size7 { font-size: 2em; }

.katex .fontsize-ensurer.reset-size2.size8, .katex .sizing.reset-size2.size8 { font-size: 2.4em; }

.katex .fontsize-ensurer.reset-size2.size9, .katex .sizing.reset-size2.size9 { font-size: 2.88em; }

.katex .fontsize-ensurer.reset-size2.size10, .katex .sizing.reset-size2.size10 { font-size: 3.45667em; }

.katex .fontsize-ensurer.reset-size2.size11, .katex .sizing.reset-size2.size11 { font-size: 4.14667em; }

.katex .fontsize-ensurer.reset-size3.size1, .katex .sizing.reset-size3.size1 { font-size: 0.714286em; }

.katex .fontsize-ensurer.reset-size3.size2, .katex .sizing.reset-size3.size2 { font-size: 0.857143em; }

.katex .fontsize-ensurer.reset-size3.size3, .katex .sizing.reset-size3.size3 { font-size: 1em; }

.katex .fontsize-ensurer.reset-size3.size4, .katex .sizing.reset-size3.size4 { font-size: 1.14286em; }

.katex .fontsize-ensurer.reset-size3.size5, .katex .sizing.reset-size3.size5 { font-size: 1.28571em; }

.katex .fontsize-ensurer.reset-size3.size6, .katex .sizing.reset-size3.size6 { font-size: 1.42857em; }

.katex .fontsize-ensurer.reset-size3.size7, .katex .sizing.reset-size3.size7 { font-size: 1.71429em; }

.katex .fontsize-ensurer.reset-size3.size8, .katex .sizing.reset-size3.size8 { font-size: 2.05714em; }

.katex .fontsize-ensurer.reset-size3.size9, .katex .sizing.reset-size3.size9 { font-size: 2.46857em; }

.katex .fontsize-ensurer.reset-size3.size10, .katex .sizing.reset-size3.size10 { font-size: 2.96286em; }

.katex .fontsize-ensurer.reset-size3.size11, .katex .sizing.reset-size3.size11 { font-size: 3.55429em; }

.katex .fontsize-ensurer.reset-size4.size1, .katex .sizing.reset-size4.size1 { font-size: 0.625em; }

.katex .fontsize-ensurer.reset-size4.size2, .katex .sizing.reset-size4.size2 { font-size: 0.75em; }

.katex .fontsize-ensurer.reset-size4.size3, .katex .sizing.reset-size4.size3 { font-size: 0.875em; }

.katex .fontsize-ensurer.reset-size4.size4, .katex .sizing.reset-size4.size4 { font-size: 1em; }

.katex .fontsize-ensurer.reset-size4.size5, .katex .sizing.reset-size4.size5 { font-size: 1.125em; }

.katex .fontsize-ensurer.reset-size4.size6, .katex .sizing.reset-size4.size6 { font-size: 1.25em; }

.katex .fontsize-ensurer.reset-size4.size7, .katex .sizing.reset-size4.size7 { font-size: 1.5em; }

.katex .fontsize-ensurer.reset-size4.size8, .katex .sizing.reset-size4.size8 { font-size: 1.8em; }

.katex .fontsize-ensurer.reset-size4.size9, .katex .sizing.reset-size4.size9 { font-size: 2.16em; }

.katex .fontsize-ensurer.reset-size4.size10, .katex .sizing.reset-size4.size10 { font-size: 2.5925em; }

.katex .fontsize-ensurer.reset-size4.size11, .katex .sizing.reset-size4.size11 { font-size: 3.11em; }

.katex .fontsize-ensurer.reset-size5.size1, .katex .sizing.reset-size5.size1 { font-size: 0.555556em; }

.katex .fontsize-ensurer.reset-size5.size2, .katex .sizing.reset-size5.size2 { font-size: 0.666667em; }

.katex .fontsize-ensurer.reset-size5.size3, .katex .sizing.reset-size5.size3 { font-size: 0.777778em; }

.katex .fontsize-ensurer.reset-size5.size4, .katex .sizing.reset-size5.size4 { font-size: 0.888889em; }

.katex .fontsize-ensurer.reset-size5.size5, .katex .sizing.reset-size5.size5 { font-size: 1em; }

.katex .fontsize-ensurer.reset-size5.size6, .katex .sizing.reset-size5.size6 { font-size: 1.11111em; }

.katex .fontsize-ensurer.reset-size5.size7, .katex .sizing.reset-size5.size7 { font-size: 1.33333em; }

.katex .fontsize-ensurer.reset-size5.size8, .katex .sizing.reset-size5.size8 { font-size: 1.6em; }

.katex .fontsize-ensurer.reset-size5.size9, .katex .sizing.reset-size5.size9 { font-size: 1.92em; }

.katex .fontsize-ensurer.reset-size5.size10, .katex .sizing.reset-size5.size10 { font-size: 2.30444em; }

.katex .fontsize-ensurer.reset-size5.size11, .katex .sizing.reset-size5.size11 { font-size: 2.76444em; }

.katex .fontsize-ensurer.reset-size6.size1, .katex .sizing.reset-size6.size1 { font-size: 0.5em; }

.katex .fontsize-ensurer.reset-size6.size2, .katex .sizing.reset-size6.size2 { font-size: 0.6em; }

.katex .fontsize-ensurer.reset-size6.size3, .katex .sizing.reset-size6.size3 { font-size: 0.7em; }

.katex .fontsize-ensurer.reset-size6.size4, .katex .sizing.reset-size6.size4 { font-size: 0.8em; }

.katex .fontsize-ensurer.reset-size6.size5, .katex .sizing.reset-size6.size5 { font-size: 0.9em; }

.katex .fontsize-ensurer.reset-size6.size6, .katex .sizing.reset-size6.size6 { font-size: 1em; }

.katex .fontsize-ensurer.reset-size6.size7, .katex .sizing.reset-size6.size7 { font-size: 1.2em; }

.katex .fontsize-ensurer.reset-size6.size8, .katex .sizing.reset-size6.size8 { font-size: 1.44em; }

.katex .fontsize-ensurer.reset-size6.size9, .katex .sizing.reset-size6.size9 { font-size: 1.728em; }

.katex .fontsize-ensurer.reset-size6.size10, .katex .sizing.reset-size6.size10 { font-size: 2.074em; }

.katex .fontsize-ensurer.reset-size6.size11, .katex .sizing.reset-size6.size11 { font-size: 2.488em; }

.katex .fontsize-ensurer.reset-size7.size1, .katex .sizing.reset-size7.size1 { font-size: 0.416667em; }

.katex .fontsize-ensurer.reset-size7.size2, .katex .sizing.reset-size7.size2 { font-size: 0.5em; }

.katex .fontsize-ensurer.reset-size7.size3, .katex .sizing.reset-size7.size3 { font-size: 0.583333em; }

.katex .fontsize-ensurer.reset-size7.size4, .katex .sizing.reset-size7.size4 { font-size: 0.666667em; }

.katex .fontsize-ensurer.reset-size7.size5, .katex .sizing.reset-size7.size5 { font-size: 0.75em; }

.katex .fontsize-ensurer.reset-size7.size6, .katex .sizing.reset-size7.size6 { font-size: 0.833333em; }

.katex .fontsize-ensurer.reset-size7.size7, .katex .sizing.reset-size7.size7 { font-size: 1em; }

.katex .fontsize-ensurer.reset-size7.size8, .katex .sizing.reset-size7.size8 { font-size: 1.2em; }

.katex .fontsize-ensurer.reset-size7.size9, .katex .sizing.reset-size7.size9 { font-size: 1.44em; }

.katex .fontsize-ensurer.reset-size7.size10, .katex .sizing.reset-size7.size10 { font-size: 1.72833em; }

.katex .fontsize-ensurer.reset-size7.size11, .katex .sizing.reset-size7.size11 { font-size: 2.07333em; }

.katex .fontsize-ensurer.reset-size8.size1, .katex .sizing.reset-size8.size1 { font-size: 0.347222em; }

.katex .fontsize-ensurer.reset-size8.size2, .katex .sizing.reset-size8.size2 { font-size: 0.416667em; }

.katex .fontsize-ensurer.reset-size8.size3, .katex .sizing.reset-size8.size3 { font-size: 0.486111em; }

.katex .fontsize-ensurer.reset-size8.size4, .katex .sizing.reset-size8.size4 { font-size: 0.555556em; }

.katex .fontsize-ensurer.reset-size8.size5, .katex .sizing.reset-size8.size5 { font-size: 0.625em; }

.katex .fontsize-ensurer.reset-size8.size6, .katex .sizing.reset-size8.size6 { font-size: 0.694444em; }

.katex .fontsize-ensurer.reset-size8.size7, .katex .sizing.reset-size8.size7 { font-size: 0.833333em; }

.katex .fontsize-ensurer.reset-size8.size8, .katex .sizing.reset-size8.size8 { font-size: 1em; }

.katex .fontsize-ensurer.reset-size8.size9, .katex .sizing.reset-size8.size9 { font-size: 1.2em; }

.katex .fontsize-ensurer.reset-size8.size10, .katex .sizing.reset-size8.size10 { font-size: 1.44028em; }

.katex .fontsize-ensurer.reset-size8.size11, .katex .sizing.reset-size8.size11 { font-size: 1.72778em; }

.katex .fontsize-ensurer.reset-size9.size1, .katex .sizing.reset-size9.size1 { font-size: 0.289352em; }

.katex .fontsize-ensurer.reset-size9.size2, .katex .sizing.reset-size9.size2 { font-size: 0.347222em; }

.katex .fontsize-ensurer.reset-size9.size3, .katex .sizing.reset-size9.size3 { font-size: 0.405093em; }

.katex .fontsize-ensurer.reset-size9.size4, .katex .sizing.reset-size9.size4 { font-size: 0.462963em; }

.katex .fontsize-ensurer.reset-size9.size5, .katex .sizing.reset-size9.size5 { font-size: 0.520833em; }

.katex .fontsize-ensurer.reset-size9.size6, .katex .sizing.reset-size9.size6 { font-size: 0.578704em; }

.katex .fontsize-ensurer.reset-size9.size7, .katex .sizing.reset-size9.size7 { font-size: 0.694444em; }

.katex .fontsize-ensurer.reset-size9.size8, .katex .sizing.reset-size9.size8 { font-size: 0.833333em; }

.katex .fontsize-ensurer.reset-size9.size9, .katex .sizing.reset-size9.size9 { font-size: 1em; }

.katex .fontsize-ensurer.reset-size9.size10, .katex .sizing.reset-size9.size10 { font-size: 1.20023em; }

.katex .fontsize-ensurer.reset-size9.size11, .katex .sizing.reset-size9.size11 { font-size: 1.43981em; }

.katex .fontsize-ensurer.reset-size10.size1, .katex .sizing.reset-size10.size1 { font-size: 0.24108em; }

.katex .fontsize-ensurer.reset-size10.size2, .katex .sizing.reset-size10.size2 { font-size: 0.289296em; }

.katex .fontsize-ensurer.reset-size10.size3, .katex .sizing.reset-size10.size3 { font-size: 0.337512em; }

.katex .fontsize-ensurer.reset-size10.size4, .katex .sizing.reset-size10.size4 { font-size: 0.385728em; }

.katex .fontsize-ensurer.reset-size10.size5, .katex .sizing.reset-size10.size5 { font-size: 0.433944em; }

.katex .fontsize-ensurer.reset-size10.size6, .katex .sizing.reset-size10.size6 { font-size: 0.48216em; }

.katex .fontsize-ensurer.reset-size10.size7, .katex .sizing.reset-size10.size7 { font-size: 0.578592em; }

.katex .fontsize-ensurer.reset-size10.size8, .katex .sizing.reset-size10.size8 { font-size: 0.694311em; }

.katex .fontsize-ensurer.reset-size10.size9, .katex .sizing.reset-size10.size9 { font-size: 0.833173em; }

.katex .fontsize-ensurer.reset-size10.size10, .katex .sizing.reset-size10.size10 { font-size: 1em; }

.katex .fontsize-ensurer.reset-size10.size11, .katex .sizing.reset-size10.size11 { font-size: 1.19961em; }

.katex .fontsize-ensurer.reset-size11.size1, .katex .sizing.reset-size11.size1 { font-size: 0.200965em; }

.katex .fontsize-ensurer.reset-size11.size2, .katex .sizing.reset-size11.size2 { font-size: 0.241158em; }

.katex .fontsize-ensurer.reset-size11.size3, .katex .sizing.reset-size11.size3 { font-size: 0.28135em; }

.katex .fontsize-ensurer.reset-size11.size4, .katex .sizing.reset-size11.size4 { font-size: 0.321543em; }

.katex .fontsize-ensurer.reset-size11.size5, .katex .sizing.reset-size11.size5 { font-size: 0.361736em; }

.katex .fontsize-ensurer.reset-size11.size6, .katex .sizing.reset-size11.size6 { font-size: 0.401929em; }

.katex .fontsize-ensurer.reset-size11.size7, .katex .sizing.reset-size11.size7 { font-size: 0.482315em; }

.katex .fontsize-ensurer.reset-size11.size8, .katex .sizing.reset-size11.size8 { font-size: 0.578778em; }

.katex .fontsize-ensurer.reset-size11.size9, .katex .sizing.reset-size11.size9 { font-size: 0.694534em; }

.katex .fontsize-ensurer.reset-size11.size10, .katex .sizing.reset-size11.size10 { font-size: 0.833601em; }

.katex .fontsize-ensurer.reset-size11.size11, .katex .sizing.reset-size11.size11 { font-size: 1em; }

.katex .delimsizing.size1 { font-family: KaTeX_Size1; }

.katex .delimsizing.size2 { font-family: KaTeX_Size2; }

.katex .delimsizing.size3 { font-family: KaTeX_Size3; }

.katex .delimsizing.size4 { font-family: KaTeX_Size4; }

.katex .delimsizing.mult .delim-size1 > span { font-family: KaTeX_Size1; }

.katex .delimsizing.mult .delim-size4 > span { font-family: KaTeX_Size4; }

.katex .nulldelimiter { display: inline-block; width: 0.12em; }

.katex .delimcenter, .katex .op-symbol { position: relative; }

.katex .op-symbol.small-op { font-family: KaTeX_Size1; }

.katex .op-symbol.large-op { font-family: KaTeX_Size2; }

.katex .accent > .vlist-t, .katex .op-limits > .vlist-t { text-align: center; }

.katex .accent .accent-body { position: relative; }

.katex .accent .accent-body:not(.accent-full) { width: 0px; }

.katex .overlay { display: block; }

.katex .mtable .vertical-separator { display: inline-block; min-width: 1px; }

.katex .mtable .arraycolsep { display: inline-block; }

.katex .mtable .col-align-c > .vlist-t { text-align: center; }

.katex .mtable .col-align-l > .vlist-t { text-align: left; }

.katex .mtable .col-align-r > .vlist-t { text-align: right; }

.katex .svg-align { text-align: left; }

.katex svg { fill: currentcolor; stroke: currentcolor; fill-rule: nonzero; fill-opacity: 1; stroke-width: 1; stroke-linecap: butt; stroke-linejoin: miter; stroke-miterlimit: 4; stroke-dasharray: none; stroke-dashoffset: 0; stroke-opacity: 1; display: block; height: inherit; position: absolute; width: 100%; }

.katex svg path { stroke: none; }

.katex img { border-style: none; max-height: none; max-width: none; min-height: 0px; min-width: 0px; }

.katex .stretchy { display: block; overflow: hidden; position: relative; width: 100%; }

.katex .stretchy::after, .katex .stretchy::before { content: ""; }

.katex .hide-tail { overflow: hidden; position: relative; width: 100%; }

.katex .halfarrow-left { left: 0px; overflow: hidden; position: absolute; width: 50.2%; }

.katex .halfarrow-right { overflow: hidden; position: absolute; right: 0px; width: 50.2%; }

.katex .brace-left { left: 0px; overflow: hidden; position: absolute; width: 25.1%; }

.katex .brace-center { left: 25%; overflow: hidden; position: absolute; width: 50%; }

.katex .brace-right { overflow: hidden; position: absolute; right: 0px; width: 25.1%; }

.katex .x-arrow-pad { padding: 0px 0.5em; }

.katex .cd-arrow-pad { padding: 0px 0.55556em 0px 0.27778em; }

.katex .mover, .katex .munder, .katex .x-arrow { text-align: center; }

.katex .boxpad { padding: 0px 0.3em; }

.katex .fbox, .katex .fcolorbox { border: 0.04em solid; box-sizing: border-box; }

.katex .cancel-pad { padding: 0px 0.2em; }

.katex .cancel-lap { margin-left: -0.2em; margin-right: -0.2em; }

.katex .sout { border-bottom-style: solid; border-bottom-width: 0.08em; }

.katex .angl { border-right: 0.049em solid; border-top: 0.049em solid; box-sizing: border-box; margin-right: 0.03889em; }

.katex .anglpad { padding: 0px 0.03889em; }

.katex .eqn-num::before { content: "(" counter(katexEqnNo) ")"; counter-increment: katexEqnNo 1; }

.katex .mml-eqn-num::before { content: "(" counter(mmlEqnNo) ")"; counter-increment: mmlEqnNo 1; }

.katex .mtr-glue { width: 50%; }

.katex .cd-vert-arrow { display: inline-block; position: relative; }

.katex .cd-label-left { display: inline-block; position: absolute; right: calc(50% + 0.3em); text-align: left; }

.katex .cd-label-right { display: inline-block; left: calc(50% + 0.3em); position: absolute; text-align: right; }

.katex-display { display: block; margin: 1em 0px; text-align: center; }

.katex-display > .katex { display: block; text-align: center; white-space: nowrap; }

.katex-display > .katex > .katex-html { display: block; position: relative; }

.katex-display > .katex > .katex-html > .tag { position: absolute; right: 0px; }

.katex-display.leqno > .katex > .katex-html > .tag { left: 0px; right: auto; }

.katex-display.fleqn > .katex { padding-left: 2em; text-align: left; }

body { counter-reset: katexEqnNo 0 mmlEqnNo 0; }

code, pre { font-family: "SÃ¶hne Mono", Monaco, "Andale Mono", "Ubuntu Mono", monospace !important; }

code[class="language-plaintext"] { white-space: pre-line; }

code.hljs, code[class*="language-"], pre[class*="language-"] { overflow-wrap: normal; background: none; color: rgb(255, 255, 255); hyphens: none; line-height: 1.5; tab-size: 4; text-align: left; white-space: pre; word-break: normal; word-spacing: normal; }

pre[class*="language-"] { border-radius: 0.3em; overflow: auto; }

:not(pre) > code.hljs, :not(pre) > code[class*="language-"] { border-radius: 0.3em; padding: 0.1em; white-space: normal; }

.hljs-comment { color: rgba(255, 255, 255, 0.5); }

.hljs-meta { color: rgba(255, 255, 255, 0.6); }

.hljs-built_in, .hljs-class .hljs-title { color: rgb(233, 149, 12); }

.hljs-doctag, .hljs-formula, .hljs-keyword, .hljs-literal { color: rgb(46, 149, 211); }

.hljs-addition, .hljs-attribute, .hljs-meta-string, .hljs-regexp, .hljs-string { color: rgb(0, 166, 125); }

.hljs-attr, .hljs-number, .hljs-selector-attr, .hljs-selector-class, .hljs-selector-pseudo, .hljs-template-variable, .hljs-type, .hljs-variable { color: rgb(223, 48, 121); }

.hljs-bullet, .hljs-link, .hljs-selector-id, .hljs-symbol, .hljs-title { color: rgb(242, 44, 61); }

.token.cdata, .token.comment, .token.doctype, .token.prolog { color: rgb(169, 174, 193); }

.token.punctuation { color: rgb(254, 254, 254); }

.token.constant, .token.deleted, .token.property, .token.symbol, .token.tag { color: rgb(255, 160, 122); }

.token.boolean, .token.number { color: rgb(0, 224, 224); }

.token.attr-name, .token.builtin, .token.char, .token.inserted, .token.selector, .token.string { color: rgb(171, 227, 56); }

.language-css .token.string, .style .token.string, .token.entity, .token.operator, .token.url, .token.variable { color: rgb(0, 224, 224); }

.token.atrule, .token.attr-value, .token.function { color: gold; }

.token.keyword { color: rgb(0, 224, 224); }

.token.important, .token.regex { color: gold; }

.token.bold, .token.important { font-weight: 700; }

.token.italic { font-style: italic; }

.token.entity { cursor: help; }

@media screen and (-ms-high-contrast: active) {
  code[class*="language-"], pre[class*="language-"] { background: window; color: windowtext; }
  :not(pre) > code[class*="language-"], pre[class*="language-"] { background: window; }
  .token.important { background: highlight; color: window; font-weight: 400; }
  .token.atrule, .token.attr-value, .token.function, .token.keyword, .token.operator, .token.selector { font-weight: 700; }
  .token.attr-value, .token.comment, .token.doctype, .token.function, .token.keyword, .token.operator, .token.property, .token.string { color: highlight; }
  .token.attr-value, .token.url { font-weight: 400; }
}

.PopoverContent { z-index: 1000; }

.react-select-container input:focus { --tw-ring-offset-shadow: var(--tw-ring-inset) 0 0 0 var(--tw-ring-offset-width) var(--tw-ring-offset-color); --tw-ring-shadow: var(--tw-ring-inset) 0 0 0 calc(var(--tw-ring-offset-width)) var(--tw-ring-color); box-shadow: var(--tw-ring-offset-shadow),var(--tw-ring-shadow),var(--tw-shadow,0 0 transparent); }

.react-select-container .react-select__control { --tw-border-opacity: 1; align-items: flex-start; border-color: rgba(236,236,241,var(--tw-border-opacity)); border-radius: 0.5rem; font-size: 0.875rem; height: 192px; line-height: 1.25rem; min-height: 192px; overflow: auto; padding: 0.5rem; }

.react-select-container .react-select__control:hover { --tw-border-opacity: 1; border-color: rgba(217,217,227,var(--tw-border-opacity)); }

.dark .react-select-container .react-select__control { --tw-bg-opacity: 1; background-color: rgba(32,33,35,var(--tw-bg-opacity)); border-color: rgba(255, 255, 255, 0.1); }

:is(.dark .react-select-container .react-select__control):hover { border-color: rgba(255, 255, 255, 0.2); }

.dark .react-select-container.react-select--invalid .react-select__control, .react-select-container.react-select--invalid .react-select__control { --tw-border-opacity: 1; border-color: rgba(239,68,68,var(--tw-border-opacity)); }

.react-select-container .react-select__input-container, .react-select-container .react-select__placeholder { padding-left: 0.5rem; padding-right: 0.5rem; }

.dark .react-select-container .react-select__input-container { --tw-text-opacity: 1; color: rgba(197,197,210,var(--tw-text-opacity)); }

.react-select-container .react-select__input { height: 2rem; }

.react-select-container .react-select__control--is-focused { --tw-ring-offset-shadow: var(--tw-ring-inset) 0 0 0 var(--tw-ring-offset-width) var(--tw-ring-offset-color); --tw-ring-shadow: var(--tw-ring-inset) 0 0 0 calc(var(--tw-ring-offset-width)) var(--tw-ring-color); box-shadow: var(--tw-ring-offset-shadow),var(--tw-ring-shadow),var(--tw-shadow,0 0 transparent); --tw-border-opacity: 1 !important; border-color: rgba(217,217,227,var(--tw-border-opacity)) !important; }

.dark .react-select-container .react-select__control--is-focused { border-color: rgba(255, 255, 255, 0.2) !important; }

.react-select-container .react-select__value-container { padding: 0px; }

.react-select-container .react-select__indicators:empty { display: none; }

.react-select-container .react-select__multi-value { background-color: transparent; margin: 0px; }

.react-select-container .react-select__multi-value__label { padding: 0px; }

#intercom-container { display: none; }
------MultipartBoundary--nIkCly1HUSqFONNTmsMOPIMfV4y6EQjlaITwGKULp4----
Content-Type: image/webp
Content-Transfer-Encoding: binary
Content-Location: https://chat.openai.com/_next/image?url=https%3A%2F%2Fs.gravatar.com%2Favatar%2F6241bfe48785687ad5d68f5c836fbe6d%3Fs%3D480%26r%3Dpg%26d%3Dhttps%253A%252F%252Fcdn.auth0.com%252Favatars%252Fna.png&w=96&q=75

RIFF   WEBPVP8   Ð
 *` ` >H J¥¥#¡¦[h°	c Rºèlyæâ¿-@J±ÝÔôuÃcµX
jsrÔI¬ØzPäDà þ¸dýWÜÏT¿â½X:` ÖÍ¶ÙÀ¶*òÿÝD@øP6à^ðf%ÎÕf «ZDÔó*w}ò3ï¿æK+-8s|¾Xµº®OÃv¶9|'Î(YæÞ÷|Ø94C1&M!|ìàß¢(ûþ¹pÜE°+þÛm¥YÞëÂ2ø>³±'×a'|ÚlÚnDÜ:vÁÄ8UPÎ
¸à&w®d©X²7¼òT÷Ø6üâr þÈC<G{ÌÁ,Û&c.{Çìuüo°õPbeköÞlLöËC/ìBRoÖ¦¤f¨Ë0ð#ÂMËJÈ¨èY 'äC)T­»7¢Z »-nÆ×Øç_±(CÎÆ;æ2ÀèGØp; Ñ"ÛÂ#¶N<<\¸8%þÞÏ«DÌ WÊ1¢±£MçM¼q·©ôoÈ½ãÍÌóä Ä#<¿SäÅE2HÌ
án²Y½À     
------MultipartBoundary--nIkCly1HUSqFONNTmsMOPIMfV4y6EQjlaITwGKULp4----
Content-Type: text/html
Content-ID: 
Content-Transfer-Encoding: binary





------MultipartBoundary--nIkCly1HUSqFONNTmsMOPIMfV4y6EQjlaITwGKULp4----
Content-Type: text/html
Content-ID: 
Content-Transfer-Encoding: binary
Content-Location: https://tcr9i.chat.openai.com/v2/1.5.5/enforcement.fbfc14b0d793c6ef8359e0e4b4a91f67.html#35536E1E-65B4-4D96-9D97-6ADB7EFF8147


------MultipartBoundary--nIkCly1HUSqFONNTmsMOPIMfV4y6EQjlaITwGKULp4----
Content-Type: text/css
Content-Transfer-Encoding: binary
Content-Location: cid:css-bf1e892b-1a9a-4234-8e51-98878edf3d53@mhtml.blink

@charset "utf-8";

html, body { margin: 0px; padding: 0px; height: 100%; }

* { box-sizing: border-box; }

#app { height: 100%; overflow: hidden; }
------MultipartBoundary--nIkCly1HUSqFONNTmsMOPIMfV4y6EQjlaITwGKULp4----
Content-Type: text/css
Content-Transfer-Encoding: binary
Content-Location: cid:css-5bb5ba3c-4865-40f9-8b53-6dad1599593f@mhtml.blink

@charset "utf-8";

@keyframes spin { 
  0% { transform: rotate(0deg) translateZ(0px); }
  100% { transform: rotate(360deg) translateZ(0px); }
}

@keyframes fadeIn { 
  0% { opacity: 0; }
  100% { opacity: 1; }
}

.kXaXkDzIfSzLHRklX2r3 { position: fixed; top: 20px; right: 20px; width: 20px; height: 20px; z-index: 200; border: none; background-color: transparent; background-image: url("data:image/svg+xml;base64,PD94bWwgdmVyc2lvbj0iMS4wIiBlbmNvZGluZz0iVVRGLTgiPz4KPHN2ZyB3aWR0aD0iMzhweCIgaGVpZ2h0PSIzOHB4IiB2aWV3Qm94PSIwIDAgMzggMzgiIHZlcnNpb249IjEuMSIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIiB4bWxuczp4bGluaz0iaHR0cDovL3d3dy53My5vcmcvMTk5OS94bGluayI+CiAgICA8IS0tIEdlbmVyYXRvcjogU2tldGNoIDQ0LjEgKDQxNDU1KSAtIGh0dHA6Ly93d3cuYm9oZW1pYW5jb2RpbmcuY29tL3NrZXRjaCAtLT4KICAgIDx0aXRsZT5Hcm91cDwvdGl0bGU+CiAgICA8ZGVzYz5DcmVhdGVkIHdpdGggU2tldGNoLjwvZGVzYz4KICAgIDxkZWZzPjwvZGVmcz4KICAgIDxnIGlkPSJQYWdlLTEiIHN0cm9rZT0ibm9uZSIgc3Ryb2tlLXdpZHRoPSIxIiBmaWxsPSJub25lIiBmaWxsLXJ1bGU9ImV2ZW5vZGQiPgogICAgICAgIDxnIGlkPSJHcm91cCIgdHJhbnNmb3JtPSJ0cmFuc2xhdGUoMTkuMDAwMDAwLCAxOS4wMDAwMDApIHJvdGF0ZSg0NS4wMDAwMDApIHRyYW5zbGF0ZSgtMTkuMDAwMDAwLCAtMTkuMDAwMDAwKSB0cmFuc2xhdGUoLTYuMDAwMDAwLCAtNi4wMDAwMDApIiBmaWxsPSIjOTQ5NDk0Ij4KICAgICAgICAgICAgPHJlY3QgaWQ9IlJlY3RhbmdsZSIgeD0iMjQiIHk9IjAiIHdpZHRoPSIyIiBoZWlnaHQ9IjUwIj48L3JlY3Q+CiAgICAgICAgICAgIDxyZWN0IGlkPSJSZWN0YW5nbGUiIHRyYW5zZm9ybT0idHJhbnNsYXRlKDI1LjAwMDAwMCwgMjUuMDAwMDAwKSByb3RhdGUoOTAuMDAwMDAwKSB0cmFuc2xhdGUoLTI1LjAwMDAwMCwgLTI1LjAwMDAwMCkgIiB4PSIyNCIgeT0iMCIgd2lkdGg9IjIiIGhlaWdodD0iNTAiPjwvcmVjdD4KICAgICAgICA8L2c+CiAgICA8L2c+Cjwvc3ZnPg=="); background-repeat: no-repeat; background-position: center center; background-size: contain; cursor: pointer; pointer-events: none; transform: scale(1); transition: transform 100ms ease-in-out 0s; visibility: hidden; opacity: 0; }

.kXaXkDzIfSzLHRklX2r3:hover { transform: scale(1.3); }

.kXaXkDzIfSzLHRklX2r3.active { pointer-events: inherit; visibility: visible; opacity: 1; transition: opacity 400ms ease-in-out 0s; }

.nMUBxApsE7lSELvJiiAA { position: fixed; inset: 0px; background-color: rgba(255, 255, 255, 0.8); z-index: -1; opacity: 0; transition: opacity 300ms linear 0s; pointer-events: none; }

.nMUBxApsE7lSELvJiiAA.active { opacity: 1; pointer-events: inherit; }

.lTLYGVw1ASsTZWg0vUbC { position: absolute; top: 50%; left: 50%; margin-left: -15px; margin-top: -15px; z-index: 1010; border-radius: 50%; width: 30px; height: 30px; border-width: 3px; border-style: solid; border-color: transparent transparent transparent rgba(0, 0, 0, 0.2); transform: translateZ(0px); box-sizing: border-box; }

.lTLYGVw1ASsTZWg0vUbC { animation: 500ms linear 0s infinite normal none running spin; }

.slrEYyWESVLe_Cx3DM1k { transition: opacity 500ms ease 0s, transform 500ms ease 0s; opacity: 0; transform: scale(0.8); text-align: center; height: 100%; }

.slrEYyWESVLe_Cx3DM1k.active { opacity: 1; transform: scale(1); }

.slrEYyWESVLe_Cx3DM1k.challenge-enter { opacity: 0; transform: scale(0.8); }

.slrEYyWESVLe_Cx3DM1k.challenge-enter-active, .slrEYyWESVLe_Cx3DM1k.challenge-enter-done { transform: scale(1); opacity: 1; }

.slrEYyWESVLe_Cx3DM1k.challenge-exit { transform: scale(1); opacity: 1; }

.slrEYyWESVLe_Cx3DM1k.challenge-exit-active, .slrEYyWESVLe_Cx3DM1k.challenge-exit-done { transform: scale(0.8); opacity: 0; }

.slrEYyWESVLe_Cx3DM1k.dTcazKGGob_VbK_J853h { flex-direction: column; }

.dTcazKGGob_VbK_J853h { display: flex; align-items: center; justify-content: center; }
------MultipartBoundary--nIkCly1HUSqFONNTmsMOPIMfV4y6EQjlaITwGKULp4------
