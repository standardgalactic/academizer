From: 
Snapshot-Content-Location: https://chat.openai.com/c/06a6638b-afab-49e9-b998-dd273eb5145d
Subject: Neural Basis of Future Prediction
Date: Sat, 23 Sep 2023 03:10:43 -0000
MIME-Version: 1.0
Content-Type: multipart/related;
	type="text/html";
	boundary="----MultipartBoundary--vS53nJOjydVm2jf5zsp0U2JCPSCh5LRhvu8f55va6M----"


------MultipartBoundary--vS53nJOjydVm2jf5zsp0U2JCPSCh5LRhvu8f55va6M----
Content-Type: text/html
Content-ID: 
Content-Transfer-Encoding: binary
Content-Location: https://chat.openai.com/c/06a6638b-afab-49e9-b998-dd273eb5145d

Neural Basis of Future PredictionOpen sidebarNeural Basis of Future PredictionGPT-4Summarize: arXiv:2305.11772 (cs)
[Submitted on 19 May 2023]
Neural Foundations of Mental Simulation: Future Prediction of Latent Representations on Dynamic Scenes
Aran Nayebi, Rishi Rajalingham, Mehrdad Jazayeri, Guangyu Robert Yang
Download PDF
Humans and animals have a rich and flexible understanding of the physical world, which enables them to infer the underlying dynamical trajectories of objects and events, plausible future states, and use that to plan and anticipate the consequences of actions. However, the neural mechanisms underlying these computations are unclear. We combine a goal-driven modeling approach with dense neurophysiological data and high-throughput human behavioral readouts to directly impinge on this question. Specifically, we construct and evaluate several classes of sensory-cognitive networks to predict the future state of rich, ethologically-relevant environments, ranging from self-supervised end-to-end models with pixel-wise or object-centric objectives, to models that future predict in the latent space of purely static image-based or dynamic video-based pretrained foundation models. We find strong differentiation across these model classes in their ability to predict neural and behavioral data both within and across diverse environments. In particular, we find that neural responses are currently best predicted by models trained to predict the future state of their environment in the latent space of pretrained foundation models optimized for dynamic scenes in a self-supervised manner. Notably, models that future predict in the latent space of video foundation models that are optimized to support a diverse range of sensorimotor tasks, reasonably match both human behavioral error patterns and neural dynamics across all environmental scenarios that we were able to test. Overall, these findings suggest that the neural mechanisms and behaviors of primate mental simulation are thus far most consistent with being optimized to future predict on dynamic, reusable visual representations that are useful for embodied AI more generally1 / 1ChatGPTNeural Foundations of Mental Simulation: Future Prediction of Latent Representations on Dynamic Scenes by Aran Nayebi, Rishi Rajalingham, Mehrdad Jazayeri, Guangyu Robert Yang:This research investigates the neural basis for humans and animals' ability to understand the physical world and predict future states. The team employed a modeling technique integrated with dense neural data and human behavior metrics. They tested various models aimed at predicting the future states of complex environments. The results highlighted that the most accurate neural responses come from models that forecast the future based on latent spaces of pretrained dynamic scene models in a self-supervised manner. Importantly, models predicting using latent video foundation models, designed for diverse sensorimotor tasks, align well with human behavior and neural patterns across different scenarios. This indicates that primate brain functions related to mental simulation are likely optimized for predictions using dynamic visual representations beneficial for general embodied AI.1 / 12Related Work 
Mental simulations have been studied at the level of neural activity only very recently. Prior 
human neuroimaging studies [Zacks, 2008, Fischer et al., 2016, Pramod et al., 2022] in the past 
decades showed elevated levels of blood-oxygen-level-dependent (BOLD) signal to mental simulation, 
although they do not have the required resolution to verify that these dynamics are actually represented 
in underlying neural activity. Rajalingham et al. [2022b] was the i rst study to show that neural 
dynamics recorded from macaque dorsomedial frontal cortex (DMFC), track the occluded ball by 
comparing these dynamics to Recurrent Neural Networks (RNNs) that simulate the occluded ballâs 
position in Mental-Pong. However, monkeys can perform these tasks without substantial training, 
suggesting that they are already equipped with the necessary neural foundations for mental simulation 
in this environment. Therefore, we aim to also build networks that are not explicitly trained on Mental- 
Pong itself, but are tasked to generalize to this novel setting as a test of their general understanding of 
physical scene dynamics â chief l y developed through three factors: their architecture, optimization 
objective, and pretraining on a naturalistic environment. 
2 
Additionally, we constrain our models by evaluating them against high-throughput human behavioral 
data (from Bear et al. [2021]) in more naturalistic, 3D environments than Mental-Pong alone, which 
goes beyond prior behavioral studies that either rely on a narrow range of physical scenarios [Shepard 
and Metzler, 1971, Cooperau and Shepard, 1973], such as block towers with several cubes of different 
colors [Groth et al., 2018, Li et al., 2016], or 2D environments that may not generalize to the real 
world [Bakhtin et al., 2019]. A key challenge to addressing these questions is a common standard 
to evaluating the everyday physical scene understanding and neural predictivity of these models, 
especially since they are usually trained on vastly different scenarios and input types. Towards 
this end, we require models to operate under similar constraints as the brain, namely(i)to take in 
unstructured visual inputs across a range of physical phenomena,(ii)to generate physical predictions 
for each scenario (e.g. producing âbehavioral outputsâ), and(iii)to consist of internal units that can 
be compared to biological units (e.g. containing âartif i cial neuronsâ). 
Taken together, these three requirements encompass a large class of functionally reasonable hypothe- 
ses that we call âsensory-cognitive networksâ, and includes the two broad approaches mentioned in 
Â§1. However, they do exclude some approaches â for example, particle-relation graph neural network 
dynamics predictors [Battaglia et al., 2016, 2018, Li et al., 2019, Sanchez-Gonzalez et al., 2020] 
that take the ground truth simulator state as input (which may not be readily available in real-world 
situations, failing to satisfy requirement (i)) or probabilistic programs [Battaglia et al., 2013, Hamrick 
et al., 2016, Ullman et al., 2017] (which fail to satisfy requirements (i) and (iii)). Nonetheless, we 
believe these latter approaches are a useful guide for building improved models from pixels that 
satisfy the three requirements â especially in terms of assessing whether the prediction problem 
lies at the level of vision, or the dynamics that interfaces with it. For example, Bear et al. [2021] 
demonstrated that graph neural networks with access to the ground truth simulator state best match 
human-level physical predictions across a diverse range of scenarios, which indicates that the onus 
mainly rests on learning a good visual representation, as these models assume perfect observability 
of physical dynamics. This latter observation further motivates our enforcement of requirement (i) 
and our approach of evaluating a range of visual foundation models, pretrained on a diverse set of 
scenarios, which we expand on below.1 / 1ChatGPTRelated Work on Mental Simulations:Background:Neural activity associated with mental simulations has recently been explored.Previous human neuroimaging studies (e.g., Zacks, 2008; Fischer et al., 2016) showed elevated BOLD signals during mental simulation, but lacked sufficient resolution to determine if these dynamics genuinely represented underlying neural activity.Rajalingham et al. (2022b) demonstrated that neural dynamics from macaque dorsomedial frontal cortex (DMFC) can track an occluded ball in Mental-Pong using RNNs. Monkeys naturally perform these tasks without much training, suggesting an inherent neural foundation for mental simulations.Research Goals:The researchers aim to create networks not just trained on Mental-Pong, but ones that can generalize to such settings based on three factors: their architecture, optimization objective, and naturalistic environment pretraining.Model Constraints:Models are assessed against human behavioral data in more naturalistic, 3D environments (using data from Bear et al., 2021). This surpasses previous studies which relied on simpler scenarios.A challenge is to find a standard to evaluate physical scene understanding and neural predictability across diverse models.The models should:Handle unstructured visual inputs across physical phenomena.Generate physical predictions.Have internal units comparable to biological units.Scope of Study:The study focuses on "sensory-cognitive networks", which adhere to the three aforementioned requirements.Some approaches, like particle-relation graph neural networks and probabilistic programs, are excluded due to their inability to meet all the requirements.Other Insights:Although not meeting all requirements, some models, like those using graph neural networks, can closely match human-level physical predictions. This suggests that a primary challenge lies in developing an effective visual representation as these models already assume a clear understanding of physical dynamics.This insight guided the researchers to emphasize the importance of a good visual foundation in their models and evaluations.1 / 13Methods 
To tackle this question, we took a hypothesis-driven approach and built sensory-cognitive networks 
that performed mental simulations of their environment in a variety of different ways (schematized 
in Figure 1). Specif i cally, we tasked models to operate on the Physion dataset [Bear et al., 2021], 
a large-scale video dataset that focuses on everyday physical understanding, consisting of eight 
different scenarios in a simulated Unity3D-based environment (the ThreeDWorld simulator [Gan 
et al., 2021]) with roughly 2,000 examples each. Models are trained on all eight scenarios, which 
are: Dominoes, Support, Collide, Contain, Drop, Link, Roll, and Drape; which together cover many 
scenarios involving rigid and soft bodies. 
We additionally train a subset of models on Kinetics 700 [Carreira et al., 2019], which consists of 
over 500,000 training videos from real-world (rather than simulated) scenes from 700 different action 
categories, in order to assess whether a different dataset of increased scale is benef i cial or not, relative 
to Physion. 
We consider models from several classes: 
1. End-to-end self-supervised future prediction: 
(a) Pixel-wise:This class of models consists of three parts: encoder, dynamics, and 
decoder; which are altogether trained end-to-end with a pixel-wise loss to predict 
the next frame. We consider a current state-of-the-art model in this model family, 
FitVid [Babaeizadeh et al., 2021], and we train it on Physion (with and without 
temporal augmentations such as RandAugment [Cubuk et al., 2020]); along with an 
earlier variant, SVG [Denton and Fergus, 2018], that can be additionally trained at 
larger image scales. 
(b) Object-centric:ThisclassofmodelsisbasedontheContrastiveLearningofStructured 
World Models framework (âC-SWMâ [Kipf et al., 2020]) which contrastively learns 
object-centric latent states. It consists of an encoder (whose size we vary: âsmallâ, 
âmediumâ, and âlargeâ) that outputs a i xed number of object slotsN = 10, and a graph 
3 
Observed + Simulated 
??? 
Time ? 
2. Macaque Neurophysiology: Mental-Pong AP 
M 
L 
DMFC 
Model Evaluations ball paddle occluder 
Time 
Observed epoch 
(1240Â±350 ms) 
Occluded epoch 
(895Â±270 ms) 
Feedback 
1.Human Behavior: Physion Object Contact Prediction (OCP) 
Yes/No? NO 
acc. = 0.89 
... 
Observed Stimuli 
Time 
Unobserved Outcome 
last frame true label 
cuestimulus YES 
acc. = 0.96 
... 
Example 
Scenarios 
Model Pretraining 
Physion 
Sensory-Cognitive Hypothesis Classes 
End-to-End Future Prediction: 
Latent Future Prediction: 
DominoesSupport 
LinkDrape 
2. Dynamics Training Stage 
1.Pretraining Stage Pixel-wise 
Ego4D, etc T+1 
T+1 
Ground Truth 
Prediction 
Foundation Model T 
EncoderDecoder Object-centric 
Encoder 
(A) 
(B) 
Inputs 
Figure 1:Model Pretraining and Evaluation Pipeline:We built and evaluate sensory-cognitive 
networks for mental simulation.(A)Models are pretrained to predict the future state of naturalistic 
environments in a variety of ways. Their dynamics are then compared to high-throughput human 
behavioral judgements and dense primate neurophysiological response dynamics. Model pretraining 
(green) can be (1) two-stage, or (2) end-to-end. For two-stage, as in the case of latent future prediction 
models, there is i rst visual network pretraining, then dynamics network pretraining.(B)Evaluations 
are multi-fold. (1) Comparison to human behavior in OCP, (2) Comparison to neural activity in 
Mental-Pong. In both cases, the model weights are i xed, so this is a strong test of generalization. 
neural network forward dynamics module that operates on this representation. Since 
the dynamics module itself does not have temporal dependencies (only spatial), we 
enable temporal dependencies by passing frames in the input channel dimension of the 
encoder, as a sliding window of temporal dimension ofTcontext frames to predict the 
object-centric latent at timestepT + 1. This model family is one instantiation of the 
cognitive theory that humans tend to reason about scenes with regards to objects and 
their relations (e.g. âSpelke core knowledgeâ [Baillargeon et al., 1985, Spelke, 1990, 
Spelke and Kinzler, 2007]). These Gestalt principles therefore may provide physical 
knowledge that are better-positioned to generalize than more i ne-grained (pixel-wise) 
prediction â a hypothesis that we explicitly evaluate on high-throughput neural and 
behavioral data, in what follows.1 / 1ChatGPTMethods for Building Sensory-Cognitive Networks for Mental Simulation:Overview:A hypothesis-driven approach is taken to create sensory-cognitive networks to perform mental simulations in various ways.Models operate on the Physion dataset (Bear et al., 2021) â a large-scale video dataset containing eight different scenarios, designed for everyday physical understanding.Dataset:Physion: Contains eight scenarios - Dominoes, Support, Collide, Contain, Drop, Link, Roll, and Drape - simulated in Unity3D-based ThreeDWorld simulator. Each scenario offers around 2,000 examples.Kinetics 700 (Carreira et al., 2019): A collection of over 500,000 training videos from real-world scenes across 700 different action categories. Used to determine the benefits of a different dataset of larger scale compared to Physion.Model Classes:End-to-end self-supervised future prediction:Pixel-wise:Comprises three segments: encoder, dynamics, and decoder.Trained end-to-end with a pixel-wise loss for next frame prediction.Use of FitVid (Babaeizadeh et al., 2021) as the current best model in this category, trained on Physion with/without temporal augmentations.SVG (Denton and Fergus, 2018) is another model in this category, scalable to larger image dimensions.Object-centric:Uses the Contrastive Learning of Structured World Models framework (C-SWM by Kipf et al., 2020).This framework learns object-centric latent states contrastively.Contains an encoder, which outputs a fixed number of object slots (N = 10), and a forward dynamics module, which operates on the encoder's output.Models are aligned with the cognitive theory suggesting that humans reason about scenes concerning objects and their relations, known as âSpelke core knowledgeâ.This approach could offer physical knowledge that might generalize better than pixel-wise predictions.Model Pretraining and Evaluation Pipeline (Figure 1):The figure displays the process of building and evaluating the sensory-cognitive networks.Models are first pretrained for predicting future states in naturalistic environments.Their dynamics are then juxtaposed against human behavioral judgements and primate neurophysiological response dynamics.Evaluations involve comparisons to human behavior and neural activity. Model weights remain consistent, presenting a robust test for generalization.1 / 12. Latent self-supervised future prediction: 
These models consist of two parts: an encoderEthat 
produces a latent state spacehand a 
dynamics moduleDthat 
predicts the future state purely in the latent spacehofE.Eis 
i xed 
and pretrained to perform a challenging vision task (âfoundation modelâ), through which it 
learns a partial, implicit representation of the physical world. However, sinceEis 
not trained 
to do explicit physical simulation, we train additional dynamicsDon 
Physion, that can later 
be ârolled outâ an arbitrary number of steps. More concretely, givenTcontext frames,E 
produces the latent sequenceh1:T, from whichDis 
trained to predicthT+1.Dis 
relatively 
simple, being either an LSTM [Hochreiter and Schmidhuber, 1997] or a continuous-time 
RNN (CTRNN) [Miller and Fumarola, 2012]; or âNo Dynamicsâ as a control, which always 
outputs the last context frame latent of E. 
We consider a variety of foundation encoders, divided into two primary classes based on 
their type of large-scale pretraining dataset: 
4 
(a) Image Foundation Models (Static scenes): 
â¢Standard Convolutional Neural Networks (CNNs) VGG16 [Simonyan and Zisser- 
man, 2014] and ResNet-50 [He et al., 2016] trained on ImageNet with a supervised 
categorization objective. 
â¢Vision Transformer (ViT) [Dosovitskiy et al., 2021] based architectures such as 
DeiT [Touvron et al., 2021] and DINO [Caron et al., 2021] trained on ImageNet 
with a self-supervised objective. 
â¢DINOv2 [Oquab et al., 2023], which is a very recent ViT trained on a larger curated 
dataset called LVD-142M (142 million images). 
â¢CLIP [Radford et al., 2021], which is a ViT trained on 400 million image-text pairs 
curated from the Internet (250 times larger than ImageNet). 
(b) Video Foundation Models (Dynamic scenes): 
â¢R3M[Nairetal.,2022], whichisaResNet-50architecturetrainedwithatemporally- 
contrastive video-language alignment objective on 5 million frames from a subset 
of the recent large-scale Ego4D human video dataset [Grauman et al., 2022]. 
â¢VIP [Ma et al., 2023], which is a ResNet-50 architecture trained with a goal- 
conditioned value function objective on 5 million frames from a subset of Ego4D. 
â¢VC-1 [Majumdar et al., 2023], which is a very recent ViT trained on 7 different 
egocentric video sources (over 5.6 million frames, including Ego4D) relevant to 
sensorimotor skills, using a self-supervised masked autoencoding (MAE) [He et al., 
2022] objective. 
In total, these networks encompass both qualitatively distinct hypotheses (pixel-wise vs. object- 
centric vs. latent future prediction), alongside several variations within each hypothesis class. This 
combination of diverse networks allows us to differentiate hypothesis classes across a range of 
functionally reasonable instantiations of each hypothesis.1 / 1ChatGPTLatent Self-Supervised Future Prediction Models:Structure:Consists of an encoder (E) that produces a latent state space hhh and a dynamics module (D) predicting the future state within the latent space of EEE.EEE is fixed and pretrained on a challenging vision task, resulting in a foundation model. Through this, EEE develops a partial, implicit representation of the physical world.Since EEE isnât tailored for explicit physical simulation, an additional dynamics DDD is trained on Physion.Using TcontextT_{context}Tcontextâ frames, EEE provides the latent sequence h1:Th_{1:T}h1:Tâ, and DDD is trained to predict hT+1h_{T+1}hT+1â.Dynamics DDD is kept relatively simple:LSTM [Hochreiter and Schmidhuber, 1997]Continuous-time RNN (CTRNN) [Miller and Fumarola, 2012]Or "No Dynamics" as a control, which outputs the last context frame latent of EEE.Foundation Encoders:Divided into two main classes based on their pretraining dataset type:Image Foundation Models (Static scenes):Standard CNNs:VGG16 [Simonyan and Zisserman, 2014]ResNet-50 [He et al., 2016], both trained on ImageNet with supervised categorization.Vision Transformer (ViT) based architectures:DeiT [Touvron et al., 2021]DINO [Caron et al., 2021], trained on ImageNet with a self-supervised objective.DINOv2 [Oquab et al., 2023]: A recent ViT on a bigger curated dataset, LVD-142M.CLIP [Radford et al., 2021]: ViT trained on 400 million image-text pairs from the Internet, a dataset 250 times larger than ImageNet.Video Foundation Models (Dynamic scenes):R3M [Nair et al., 2022]: A ResNet-50 model trained with a temporally-contrastive video-language alignment objective on 5 million frames from the Ego4D human video dataset subset.VIP [Ma et al., 2023]: A ResNet-50 model trained1 / 14Comparison to Human Physical Judgements 
4.1OCP Task Evaluation 
In the object contact prediction (OCP) task [Bear et al., 2021], each evaluation scenario involves 
a red âagentâ object and a yellow âpatientâ object (which did not appear during model training), 
and both humans and models are tasked to predict the probability that they will come into contact. 
This prediction requires understanding of the relevant physical phenomenon in the given scenario, 
corresponding to a higher-order readout of the underlying scene dynamics. 
4.2End-to-end pixel-wise future predictors best explain human behavior in the same 
environment 
The model comparison results are summarized in Figure 2. In Figure 2A, we examine held-out 
accuracy of models across all eight scenarios in the OCP task, where chance performance is 50%. 
We can see that humans are quite reliable on this task, attaining 74.04% average accuracy across 
scenarios (grey horizontal line). Furthermore, the best models that approach human accuracy are 
models that are trained end-to-end with pixel-wise losses (FitVid and SVG). Despite their differences 
in architecture, both FitVid and SVG are comparable to one another in their OCP test set accuracy 
(61.12%-63.31%), attaining non-signif i cant differences in the distribution of their accuracies across 
scenarios(Pairedt-test, minimumBonferronicorrectedp-valueof0.444acrosspairwisecomparisons). 
For a i xed architecture, training with increased image size helps improve accuracy somewhat on 
the OCP prediction task (SVG128 Ã 128, 
rightmost periwinkle bar, vs. SVG64 Ã 64, 
leftmost 
one), along with temporal augmentations for a high capacity model (FitVid, rightmost dark green 
bar vs. leftmost one), but these differences are overall non-signif i cant (Pairedt-testp = 0.461and 
p = 0.196within the SVG and FitVid architectures, respectively). However, not all end-to-end 
models match human accuracy well. In particular, we see that the object-centric C-SWM class 
of models matches human accuracies least well compared to other model classes, despite varying 
the encoder size. The class of latent future prediction models overall better matches the explicitly 
object-centric C-SWM models, but there does not appear to be strong differentiation across models, 
and also not much differentiation between having a dynamics module and using the encoder latents 
5 
(A)(B) 
Human Accuracy 
Human-to-Human Consistency 
(C) Râ0.8587, p << 0.001 
End-to-EndLatent Future Prediction 
64x64 
Pixel-wise C-SWM 
128x128 64x64 64x64 
+ RandAugment 
Image Foundation ModelsVideo Foundation Models 
SVG FitVid Small Large 
Medium No Dynamics LSTM 
Dynamics CTRNN 
Dynamics 
VGG16ResNet-50DeiTDINO DINOv2 
Object 
-centric No 
Dynamics LSTM 
Dynamics CTRNN 
Dynamics 
CLIPVIPVC-1R3M 
CTRNN 
Dynamics 
LSTM 
Dynamics End-to-EndLatent Future Prediction 
C-SWMSVG 
FitVidVGG16ResNet-50DeiTDINO DINOv2 CLIPVIPVC-1R3M 
64x64 
Pixel-wise 
128x128 64x64 64x64 
+ RandAugment 
Image Foundation ModelsVideo Foundation Models 
Small Large 
Medium No Dynamics LSTM 
Dynamics CTRNN 
Dynamics 
Object 
-centric No Dynamics LSTM Dynamics CTRNN 
Dynamics CTRNN 
Dynamics 
LSTM Dynamics 
End-to-End 
Image Foundation Models 
Video Foundation Models 
Heldout 
OCP Accuracy Correlation 
to Average 
Human 
Response (Pearsonâs 
R) 
Heldout OCP Accuracy 
Correlation 
to Average 
Human 
Response (Pearsonâs 
R) 
Figure 2:Model Comparisons to Human Physical Judgements: (A)Each model is compared to 
binary (âyes/noâ) accuracy on the OCP task (chance is 50%). Grey horizontal bar represents the 
human accuracy on this task. Mean and s.e.m. across all eight Physion scenarios, weighted by number 
of stimuli in each scenario. Hatched bars represent models trained on the Kinetics 700 dataset, rather 
than on Physion.(B)Each model is compared to human subject judgement probabilities for the OCP 
task, via Pearsonâs correlation from the Logistic Regression classif i er trained on each of the model 
dynamics. Grey is the correlation of human judgement probabilities to each other. Mean and s.e.m. 
across all eight Physion scenarios, weighted by number of stimuli in each scenario.(C)A modelâs 
match to human judgement probabilities is strongly correlated with its OCP task accuracy. 
directly (rightmost two bars vs. leftmost bar in each group). This suggests that at least for the OCP 
task, either Physion is not suff i ciently high-variation enough to train the dynamics module or most of 
the predictivity comes from the physical scene understanding encapsulated in the foundation model 
encoder latents. The latter appears to be the case, since training the dynamics module on a larger-scale 
dataset such as Kinetics 700 for a subset of the models (VIP+LSTM/CTRNN, VC-1+LSTM/CTRNN, 
and R3M+LSTM/CTRNN; hatched bars) does not improve OCP accuracy over the base encoder 
latents either, relative to training them on Physion (Pairedt-test, minimum Bonferroni corrected 
p-value of 0.066 across architectures). 
Additionally, we can look at i ner-grained error patterns of the probabilities reported by humans 
compared to those of models (rather than accuracies alone), summarized in Figure 2B. Here we see 
that despite the metric being more detailed, humans are quite consistent with each other, suggesting 
that this type of behavioral metric is quite reliable (grey horizontal line). In fact, all models appear to 
be further from the human-to-human consistency than the OCP accuracy. However, overall, similar 
trends appear to hold across models as with the OCP accuracy measure â where the end-to-end 
pixel-wise models (FitVid and SVG) match these consistency scores the best across models, and the 
object-centric C-SWM models match them the least well. From an AI perspective, it is actually quite 
relevant to work towards matching human error patterns, as it is highly correlated with the primarily 
performance based measure of OCP accuracy, as seen in Figure 2C (R â 0.8587,p ? 0.001).1 / 1ChatGPTComparison to Human Physical JudgementsObject Contact Prediction (OCP) Task Evaluation:Involves a red "agent" and a yellow "patient" object.Both humans and models predict the probability of these objects coming into contact.Requires understanding the physical phenomenon in the scenario to interpret scene dynamics.End-to-end pixel-wise predictors align best with human behavior:Figure 2A:Examines the accuracy of models in all eight OCP scenarios.Human accuracy averaged at 74.04%.Models like FitVid and SVG, which are trained end-to-end with pixel-wise losses, were closest to human accuracy with test set accuracy between 61.12%-63.31%.Training with larger image sizes slightly improves OCP task accuracy.Object-centric C-SWM models align least with human accuracy.Latent future prediction models seem better than C-SWM but lack strong differentiation.For the OCP task, Physion might not have enough variation to train the dynamics module, or the predictivity might largely come from the scene understanding in the foundation model encoder latents.Training the dynamics module on larger datasets, like Kinetics 700, doesn't necessarily improve OCP accuracy compared to training on Physion.Figure 2B:Looks at finer-grained error patterns.Humans remain consistent with each other, making this a reliable metric.All models are further from human-to-human consistency than from OCP accuracy.Still, end-to-end pixel-wise models (FitVid and SVG) match human consistency scores best, while C-SWM models do least well.Matching human error patterns is essential for AI since it correlates highly with OCP accuracy.Figure 2C:Illustrates a strong correlation between a model's match to human judgment probabilities and its OCP task accuracy with R â 0.8587 and p << 0.001.Summary:
End-to-end pixel-wise future prediction models, especially FitVid and SVG, perform best in aligning with human judgment for the OCP task. Despite the diverse models explored, consistent trends emerge: models that capture detailed visual dynamics closely resemble human prediction patterns. This is particularly evident when comparing end-to-end models and object-centric models, with the latter showing less alignment with human judgements. The results underscore the importance of aligning AI models with human behavioral patterns, especially in tasks that require understanding complex physical interactions.1 / 15Comparison to Dense Neurophysiological Data 
To gain more insight into model generalization, we compared the above models to neural dynamics 
recorded from macaque dorsomedial frontal cortex (DMFC), which was shown by Rajalingham 
6 
et al. [2022b] to simulate the ballâs position in Mental-Pong while behind an occluder, until it was 
intercepted by the paddle. 
5.1Inter-animal consistency and neural behavioral decoders 
For each of the 79 conditions (different randomized start position of the ball), we present the frames 
in the visible epoch (the time up until the ball reaches the occluder) as context frames to the models, 
and unroll the model dynamics during the occluded epoch of the condition. We then build detailed, 
physical mappings (schematized in Figure 3A) from the committed model layer latent dynamics to 
match every single neuron in the population, and the ground truth ball state while occluded, when 
mental simulation takes place. We enforce the requirement, established in prior work [Nayebi et al., 
2021, Cao and Yamins, 2021], that models are at least as similar to a given animalâs neural responses 
as two conspecif i csâ neural responses are to each other. When we perform these mappings, we see 
that both the inter-animal neural predictivity consistency and ground truth ball state decoding from 
DMFC is quite high (grey horizontal lines in Figures 3B and C, respectively), indicating that these 
are very reliable neural and behavioral measures of the mental simulation phenomenon. 
(A)(B) 
(C) (D) 
ball paddle occluder 
Time 
Observed epoch 
(1240Â±350 ms) 
Occluded epoch 
(895Â±270 ms) 
Feedback 
DMFC DMFC 
Monkey M Monkey P 
Model 
LM P 
LModel 
P 
? 
LModel A 
? 
? 
LP M 
? 
LMBall 
? LPBall 
? 
LModelBall 
? End-to-End 
Image Foundation Models 
Video Foundation Models 
DMFC Predictivity 
C-SWMSVG 
FitVidVGG16ResNet-50DeiT DINO DINOv2 CLIPVIPVC-1R3M 
End-to-EndLatent Future Prediction 
Pixel-wise 
Image Foundation Models 
Video Foundation Models 
Object 
-centric 
Small Large 
Medium No Dynamics LSTM 
Dynamics CTRNN 
Dynamics No Dynamics LSTM 
Dynamics CTRNN 
Dynamics CTRNN 
Dynamics 
LSTM 
Dynamics 
64x64 128x128 64x64 64x64+ 
RandAugment 
Râ0.683, p << 0.001 
Ball Position 
+ 
Velocity 
Predictivity (Pearsonâs 
R) Ball Position 
+ Velocity 
Predictivity (Pearsonâs 
R) Neural Predictivity 
(Pearsonâs R) 
End-to-EndLatent Future Prediction 
Inter-animal Consistency 
C-SWMSVG 
FitVidVGG16ResNet-50DeiT DINODINOv2 CLIPVIPVC-1R3M 
Pixel-wise 
Image Foundation Models 
Video Foundation Models 
64x64 128x128 64x64 64x64+ RandAugment Small Large 
Medium 
Object 
-centric No 
Dynamics LSTM Dynamics CTRNN 
Dynamics No Dynamics LSTM 
Dynamics CTRNN 
Dynamics CTRNN 
Dynamics 
LSTM 
Dynamics 
Oracles 
Ball 
Position Ball 
Position 
+ 
Velocity 
Ball 
Velocity 
Neural 
Predictivity (Pearsonâs 
R) 
Encoder 
Dynamics 
Figure 3:Model Comparisons to Neural Response Dynamics:(A)We measure how similar 
model dynamics (red) are to each primateâs DMFC neural dynamics up to at least how (linearly) 
similar primates are to each other, via the linear transformLN(black arrows). We also assess how 
well models can decode ball state (position and velocity) up to how well it can be decoded from 
DMFC via the linear transformLB(dark blue arrows).(B)DMFC test set neural predictivity of each 
model, averaged across i ve train-test splits. Median and s.e.m. across1,889recorded units. For each 
of the latent future prediction models, there are three bars: the i rst corresponding to i nal context 
encoder latent itself (no dynamics); the last two bars with LSTM or CTRNN dynamics, respectively. 
For the VIP, VC-1, and R3M based models, we also train the LSTM and CTRNN dynamics on 
Kinetics 700, presented in the last two additional hatched bars. The grey horizontal bar represents how 
well each primateâs DMFC can explain the otherâs via the linear transformLNschematized in (A), 
setting a consistency ceiling on model predictivity. Position, velocity, and position + velocity oracles 
are the ground truth ball state values for those variables while it is occluded (and not visible to the 
primate during game play).(C)Ball position and velocity predictivity of each model, averaged across 
i ve train-test splits, from the best DMFC i tting model layer used in (B) via the linear transformLB 
schematized in (A). The grey horizontal bar represents the ball position and velocity predictivity from 
DMFC units. Median and s.e.m. across four quantities: the ground truth(x,y)position and velocity 
of the ball while it was occluded (in degrees of visual angle).(D)Mental-Pong ball position and 
velocity predictivity (y-axis) vs. neural predictivity (x-axis). Dotted golden line represents best linear 
i t to the data. 71 / 1ChatGPTThe provided passage outlines a comparison of machine learning models to neural data gathered from the dorsomedial frontal cortex (DMFC) of macaques. This comparison aims to understand how well the models can replicate or generalize the mental processes observed in primates. Here's a summary of the main points:Comparison to Dense Neurophysiological DataPurpose: The study sought to understand the model's generalization by comparing its dynamics with the neural dynamics of the macaque DMFC. This part of the brain has been previously observed to simulate a ball's position in a game called "Mental-Pong" when it's occluded from view.Procedure:Inter-animal consistency and neural behavioral decoders:The models were presented with visible frames from the game and then unrolled during the occluded period.Physical mappings were constructed between the models' latent dynamics and the neural responses of the monkeys.These mappings aimed to determine if models can match the consistency between neural responses of different individual animals.Results:Neural Consistency: Both the inter-animal neural predictivity and the ground truth ball state decoding from the DMFC was found to be high, emphasizing the reliability of these measures for mental simulation.Model Comparisons to Neural Response Dynamics:Figure 3A describes how model dynamics are compared with primate DMFC neural dynamics and how models can decode the ball state.Figure 3B showcases the DMFC test set neural predictivity for each model. Some models were also trained on the Kinetics 700 dataset. The grey bar indicates the consistency between primate DMFCs.Figure 3C represents the model's ability to predict the ball's position and velocity, using the best fitting model layer from Figure 3B.Figure 3D plots the predictivity of ball position and velocity against neural predictivity, indicating a correlation between the two.Overall, this section emphasizes the value of comparing AI models with real neural data to understand their generalization capabilities. This could provide insights into how well the models are simulating or understanding the task in a way similar to how an actual brain might process it.1 / 15.2Neural response predictivity strongly separates models 
When we map model units to DMFC units across 79 conditions (different randomized ball start 
position), we see in Figure 3B that across all architectures, only dynamically equipped models from 
the video foundation model class class explains DMFC dynamics best (46.34-48.83% explained 
variance), specif i cally in the latent space of the VC-1 and R3M encoders. However, the VC-1 and 
R3M encoderâs latents alone are insuff i cient to explain the data (19.03% and 24.79% explained 
variance, respectively). Training end-to-end on Physion with either a pixel-wise loss (SVG and 
FitVid; 14.44%-25.35% explained variance) or an object-centric loss (C-SWM with varying encoder 
sizes; 13.69%-23.05% explained variance) is not suff i cient either, indicating a strong constraint on 
the neural mechanisms of mental simulation being performed on a suitable latent space. However, it 
is not the case that any latent space works, since all dynamics trained on encoder latents trained on 
static images, regardless of their imageset scale, explained at most 29.67% of the variance. This is 
suggestive that for a relatively small but time-varying stimulus such as the Pong ball, models that are 
trained on static images may not be equipped to handle the temporal coherence of a single object 
moving through time, as prior studies have mainly examined generalization of ImageNet-optimized 
CNNs to novel yet static, single-object scenes [Hong et al., 2016]. 
Moreover, even for video optimized foundation models, we see differentiation at the level of loss 
function. In particular, goal-conditioned self-supervised learning on videos, as instantiated by VIP 
(which differs from R3M primarily in the choice of objective function, as it shares the same ResNet-50 
backbone and is also trained on Ego4D), fails to match DMFC neural dynamics well (7.37%-18.12% 
explained variance). We also see parsimony at the level of the architecture and training dataset of 
the dynamics module, as the VC-1/R3M+CTRNN either attains comparable or improved neural 
predictivity over the more sophisticated VC-1/R3M+LSTM (middle vs. rightmost bars for each 
encoder type), and even when training on a larger dataset like Kinetics 700 (Figure S1) â suggesting 
that simple dynamics can be suff i cient at explaining neural response dynamics given the appropriate 
visual encoder. Both the VC-1/R3M+LSTM and VC-1/R3M+CTRNN models approach the neural 
predictivity of the ground truth ball position oracle model (50.74% explained variance, leftmost 
golden bar in Figure 3B), with the joint position and velocity oracle performing the best (60.65% 
explained variance, rightmost golden bar). This suggests that a substantial amount of the neural 
response variability is devoted to simulating the ballâs state while it is occluded, as opposed to other 
static features of the environment. 
In Figure 3C, we see that the most neurally predictive dynamically-equipped VC-1/R3M-based 
models generalize to the Mental-Pong task, approaching DMFCâs ability to track the ground truth 
position and velocity of the ball while it is occluded, despite not being explicitly trained in this 
environment. In particular, there is a linear relationship (R â 0.683,p ? 0.001) between the modelâs 
ability to generalize to the Mental-Pong environment and its ability to predict DMFC neural dynamics 
(Figure 3D), indicating that predicting the underlying neural dynamics is in fact behaviorally relevant 
to effectively simulating the ballâs dynamics.1 / 1ChatGPTThe passage further delves into the comparison between various machine learning models and neural data from the dorsomedial frontal cortex (DMFC) of macaques. The goal is to discern which models are more effective at replicating or understanding the neural dynamics related to the Mental-Pong task. Here's a concise summary:Neural Response Predictivity Strongly Separates ModelsOverview:Comparisons were made by mapping model units to DMFC units across 79 conditions, each representing different randomized ball start positions.Findings:Model Predictivity:Dynamically equipped models from the video foundation class (specifically, the latent space of the VC-1 and R3M encoders) best explain DMFC dynamics.Models trained on static images, irrespective of the dataset's scale, can only explain a limited variance, suggesting they might not be equipped for time-varying stimuli like the moving Pong ball.Video optimized models show differentiation based on their loss function. Notably, the VIP model, which uses goal-conditioned self-supervised learning, doesn't match DMFC dynamics well.Simpler dynamics (VC-1/R3M+CTRNN) perform comparably or even better than more sophisticated dynamics (VC-1/R3M+LSTM). This indicates that the correct choice of visual encoder can lead to simpler models explaining neural response dynamics effectively.The models' neural predictivity is close to the predictivity of the ground truth ball position oracle model, indicating much of the neural activity is dedicated to simulating the ball's position when occluded.Generalization to the Mental-Pong Task:The most neurally predictive dynamically-equipped models (VC-1/R3M-based) can generalize to the Mental-Pong task and approach the DMFC's ability to track the ball's position and velocity when it's occluded.There's a significant linear relationship between a model's capability to generalize to the Mental-Pong task and its ability to predict DMFC neural dynamics. This relationship suggests that predicting underlying neural dynamics is crucial for effectively simulating the ballâs movement.In essence, this study underscores the importance of choosing the right model, with an emphasis on dynamic video models, to accurately reflect neural processes as observed in primates. The study also reveals that not just any model will suffice; specific configurations and training paradigms are needed to approach the kind of neural predictivity seen in actual neural data.1 / 16Dynamically Equipped Sensorimotor Foundation Models Can Match Both 
Human Behavioral and Neural Response Patterns Across Environments 
Both the OCP human behavioral error pattern metric and the DMFC Mental-Pong neural predictivity 
metric are linearly correlated to their corresponding behavior of OCP accuracy and ball position 
prediction (as seen in Figures 2C and 3D), which suggests that these more i ne-grained metrics are 
relevant to the underlying behavioral goal that they represent. But how do they relate to each other, 
and is there a model class that can match both reasonably well? 
As we can see in Figure 4A, the two behavioral goals of Mental-Pong ball state predictivity and 
Physion OCP accuracy do not appear to be very related across models (R â â0.243,p â 0.129). 
This suggests that accuracy within environment, despite being to novel scenarios, does not imply 
that the same model will generalize to completely new environmental dynamics. In particular, it is 
important to consider generalization to new environments, since the end-to-end pixel models such as 
SVG and FitVid subtly overf i t to this scenario (attaining the highest OCP accuracy), but they fail to 
generalize as well to the completely out-of-distribution Mental-Pong setting (rightmost periwinkle 
and dark green points in Figure 4A). For example, for FitVid, this failure is visualizable, as it predicts 
the Mental-Pong ball to be static, regardless of enlarging the ball during evaluation (Figure S2). 
8 
(A) (B) VC-1+CTRNN 
VC-1+LSTM 
Better Models 
End-to-End 
Image Foundation Models 
Video Foundation Models 
Ball Position 
+ Velocity 
Predictivity (Pearsonâs 
R) Heldout OCP Accuracy 
Neural 
Predictivity (Pearsonâs 
R) Correlation to Average Human Response 
(Pearsonâs R) 
Figure 4:Sensorimotor Foundation Model Can Match Both Metrics: (A)Ball state (position 
and velocity) predictivity in Mental-Pong (y-axis) vs. OCP accuracy in Physion (x-axis). Across 
models, these two metrics appear to be largely independent.(B)The dynamically-equipped VC-1 
models (VC-1+LSTM/CTRNN trained on either Physion or Kinetics 700) can reasonably match 
neural response dynamics in Mental-Pong and match human error patterns in the OCP task, relative 
to other models. Dotted grey line is the unity line to indicate that better models will occupy the top 
right of this scatter plot. 
Delving into the more challenging, i ne-grained measures of DMFC Mental-Pong neural predictivity 
and correlation to human error patterns, we observe in particular that the dynamically-equipped 
VC-1-based models (VC-1+CTRNN and VC-1+LSTM in cyan) both reasonably match human error 
patterns in Physion and Mental-Pong DMFC neural dynamics (Figure 4B). 
Just as VC-1 on its own does not universally dominate on every specif i c sensorimotor task, but instead 
outperforms the best prior existing visual foundation models on average across the 17 CortexBench 
tasks [Majumdar et al., 2023], our i nding that future prediction in the latent space of this model 
class reasonably matches both human behavioral patterns and neural responses is therefore consistent 
with this observation. Taken together, our results suggest that future prediction in the latent space of 
video foundation models for embodied AI is a promising paradigm for developing models of physical 
dynamics that are both a better match to neural and behavioral recordings, and can structurally 
generalize across diverse environments and scenarios within. 
7Discussion 
Overall, we i nd that structural generalization to novel environments and matching dense neurophysi- 
ological data to be a strong constraint on models of physical simulation. Going forward, we believe 
that we are at a crucial turning point whereby foundation models that engage with the visually rich, 
dynamic scenes that humans and animals naturally interface with, will be jointly critical for progress 
in embodied AI and neuroscience, addressing the recent call to action to build AI that can be as 
grounded in the physical world as animals are [Zador et al., 2023]. On our existing benchmarks, 
there are a few ways that we envision our current models can be improved. First, we believe that 
training video foundation models with other self-supervised methods [Balestriero et al., 2023] that 
better leverage temporal relationships on naturalistic, sensorimotor tasks would yield better latent rep- 
resentations. This is based on the observation that dynamically-equipped R3M models best matched 
DMFC neural dynamics compared to the MAE objective of VC-1 and goal-conditioned objective of 
VIP. However, likely R3Mâs match to human behavioral patterns in the OCP task could be improved 
by pretraining its temporally-contrastive loss on a larger video dataset similar to CortexBench, rather 
than Ego4D alone. Furthermore, the dynamics architecture could benef i t from better leveraging a 
more âfactorizedâ representation of temporally-active state variables (as suggested by the high neural 
predictivity of the joint, ground truth position + velocity oracle in Figure 3B). One method could be 
by including multiple timescales of hierarchy, with some recent neurobiological evidence of this type 
of temporal hierarchy already existing in frontal cortex [Sarafyazd and Jazayeri, 2019]. 
In addition to building improved models, we also want to benchmark them against neural recordings 
from primates in more complex environments that involve multi-object scenarios, both procedurally 
generated at scale in 2D and 3D virtual settings [Watters et al., 2021, Gan et al., 2021], and also 
involving real world objects. 91 / 1ChatGPTThe passage focuses on evaluating how well different models can match both human behavioral error patterns and neural response patterns, particularly for tasks that involve predicting ball state in a game of "Mental-Pong" and object-centric perception (OCP) accuracy in another environment called "Physion".Key Points:Model Evaluations on Two Metrics:The OCP human behavioral error pattern and DMFC (a region in the brain) Mental-Pong neural predictivity are both linearly correlated to their associated behaviors of OCP accuracy and ball position prediction.However, a model's accuracy in one environment (e.g., Physion) does not necessarily imply it will perform well in a completely different environment (e.g., Mental-Pong).Inter-model Variability:Models like SVG and FitVid, which are end-to-end pixel models, might overfit to specific scenarios. For instance, FitVid predicts the Mental-Pong ball to be static.Dynamically-Equipped VC-1 Models:The VC-1 models equipped with LSTM and CTRNN architectures can effectively match neural response dynamics in Mental-Pong and also correlate well with human error patterns in the OCP task.VC-1, while not universally superior in every task, does generally perform better than other models on a variety of sensorimotor tasks.Insights:The results emphasize the potential of using video foundation models' latent space for predicting future events. These models can mimic both human behavioral patterns and neural responses, and they have the potential to generalize across various environments.Discussion and Future Directions:The focus should be on developing foundation models that can engage with dynamic visual scenarios similar to what humans and animals experience.There's a potential to improve models by training video foundation models with self-supervised methods that take advantage of temporal relationships in sensorimotor tasks.Dynamically-equipped R3M models show promise, especially when compared to other objectives.Future models could benefit from incorporating more factorized representations of temporally-active state variables.The research aims to extend benchmarking to neural recordings from primates in more complicated environments, involving multi-object scenarios in both virtual and real-world settings.In essence, the study underscores the importance of dynamically-equipped sensorimotor foundation models in matching human behavioral and neural response patterns. While some models perform well in specific environments, achieving broad generalization remains a challenge. However, models like VC-1 show promise in this direction.1 / 1A summary of the main points of the whole article.1 / 1ChatGPTThe article evaluates the effectiveness of various AI models in mimicking human behavioral and neural response patterns, especially in tasks like predicting ball movement in "Mental-Pong" and object-centric perception (OCP) accuracy in "Physion."Main Points:Model Evaluation Metrics:AI models were evaluated based on how well they match the OCP human behavioral error pattern and DMFC neural predictivity in the Mental-Pong task.Inter-model Performance:Accuracy in one environment doesn't guarantee performance in a different environment.Some models, like SVG and FitVid, might overfit to specific scenarios, limiting their generalization capabilities.Promising Models:Dynamically-equipped VC-1 models (with LSTM and CTRNN architectures) stand out, demonstrating the ability to match human neural responses and behavioral patterns across tasks effectively.Insights and Implications:Models trained in the latent space of video foundations show potential in future prediction tasks, aligning closely with human behavioral patterns and neural responses.Future Directions:There's an emphasis on developing models that engage with dynamic visual scenarios.Enhancement strategies include leveraging temporal relationships in training and refining the representation of temporally-active state variables.The goal is to expand benchmarking to more complex environments and compare AI models to neural recordings from primates in varied settings.In summary, the research underscores the potential and challenges of AI models in simulating human behavioral and neural dynamics. While certain models, particularly VC-1, display promising results, achieving broad and consistent generalization across diverse scenarios remains an ongoing challenge.1 / 1The article was titled Neural Foundations of Mental Simulation: Future Prediction of Latent Representations on Dynamic Scenes1 / 1ChatGPTTitle: Neural Foundations of Mental Simulation: Future Prediction of Latent Representations on Dynamic ScenesSummary:The study delves into the assessment of AI models in replicating human behavioral and neural responses, focusing on tasks like the prediction of ball movements in "Mental-Pong" and the accuracy of object-centric perception (OCP) in the "Physion" scenario.Key Takeaways:Evaluation Criteria:Models were gauged based on their alignment with the OCP human behavioral error pattern and DMFC neural predictivity during the Mental-Pong exercise.Performance Variability:Achieving high accuracy in one environmental setting does not guarantee similar success in another.Some models, particularly SVG and FitVid, might be too specialized for certain situations, hindering their adaptability to diverse contexts.Standout Models:AI models that are dynamically-equipped, especially those based on the VC-1 architecture (coupled with LSTM and CTRNN), showcased a significant ability to emulate human neural activities and behavioral tendencies across varied tasks.Research Insights:AI models trained to recognize latent patterns within video foundation data have shown potential for predicting future events, aligning well with both human behavior and neural activities.Looking Ahead:The study emphasizes the need to develop AI models that are attuned to dynamic visual scenes.Future improvement strategies might involve harnessing temporal relationships more effectively during the training phase and refining the models' handling of time-sensitive state variables.The ultimate objective is to widen the benchmark spectrum to encompass more intricate environments, comparing AI performance against neural data gathered from primates in these settings.In essence, the research highlights both the potential and challenges associated with AI models in simulating human neural and behavioral patterns. While certain models, notably VC-1, show promise, ensuring consistent and wide-ranging adaptability across diverse environments remains a challenge.1 / 1What was their explanation of a neural correlate?1 / 1ChatGPTThe term "neural correlate" generally refers to a measurable neural phenomenon (e.g., brain activity patterns, neuron firing rates) that corresponds to specific conscious experiences or behaviors. In the context provided, the study examined the neural predictivity of various AI models by measuring how closely the models' outputs matched the "neural correlates" observed in the dorsomedial frontal cortex (DMFC) during the Mental-Pong task. Essentially, the study aimed to assess how well AI models can predict or simulate the brain activity patterns associated with specific mental simulations or behaviors.1 / 1So something was blocking them from seeing the ball flying across the screen?1 / 1ChatGPTYes, in the provided information, it appears that the Mental-Pong task involved tracking the position of a pong ball, likely with periods where the ball was occluded (or hidden) from view. The goal of this task was to study the brain's (or DMFC's) ability to mentally simulate or predict the ball's trajectory even when it was not directly visible. By comparing this with the predictions made by various AI models, the researchers were able to assess the models' ability to simulate or predict similar dynamic events as the human brain.1 / 1Connections with the following: PLoS One. 2014; 9(5): e96534.
Published online 2014 May 2. doi: 10.1371/journal.pone.0096534
PMCID: PMC4008581
PMID: 24789353
The Neural Basis of Event Simulation: An fMRI Study
Yukihito Yomogida, 1 , 2 , * Motoaki Sugiura, 3 , 4 Yoritaka Akimoto, 3 Carlos Makoto Miyauchi, 3 and Ryuta Kawashima 3 , 5 , 6
Cosimo Urgesi, Editor
Author information Article notes Copyright and License information PMC Disclaimer
Associated Data
Supplementary Materials
Go to:
Abstract
Event simulation (ES) is the situational inference process in which perceived event features such as objects, agents, and actions are associated in the brain to represent the whole situation. ES provides a common basis for various cognitive processes, such as perceptual prediction, situational understanding/prediction, and social cognition (such as mentalizing/trait inference). Here, functional magnetic resonance imaging was used to elucidate the neural substrates underlying important subdivisions within ES. First, the study investigated whether ES depends on different neural substrates when it is conducted explicitly and implicitly. Second, the existence of neural substrates specific to the future-prediction component of ES was assessed. Subjects were shown contextually related object pictures implying a situation and performed several pictureâword-matching tasks. By varying task goals, subjects were made to infer the implied situation implicitly/explicitly or predict the future consequence of that situation. The results indicate that, whereas implicit ES activated the lateral prefrontal cortex and medial/lateral parietal cortex, explicit ES activated the medial prefrontal cortex, posterior cingulate cortex, and medial/lateral temporal cortex. Additionally, the left temporoparietal junction plays an important role in the future-prediction component of ES. These findings enrich our understanding of the neural substrates of the implicit/explicit/predictive aspects of ES-related cognitive processes.

Go to:
Introduction
To cope with dynamically changing physical and social environments, both current situations and future transitions must be inferred from fractionally perceived environmental information. Recently, such situational inferences have been proposed to be accomplished when partially perceived event features are associated in the brain to represent the whole situation [1]â[3]. Those event features include information concerning objects, agents, actions, mental states, and places that constitute a situation. By matching those event features with the event knowledge constructed through the accumulation of experience, the most likely situation is inferred. At the same time, other event features likely to appear in that situation but not perceived at that moment are also inferred based on the event knowledge. Via such completion inference, the whole situation is represented in the brain as a simulation. The situational inference process explained above is called event simulation (ES). ES is thought to provide a common basis for various cognitive processes. Those include simple perceptual prediction, situational understanding/prediction previously termed âevent schema/scriptâ, or complex social cognition such as âtheory of mindâ or trait inference [2], [4]. Thus, clarifying the detailed neural basis of ES would contribute to our understanding of these cognitive processes.

Although previous studies have assessed the neural basis underlying ES-related cognitive processes, such as language-based situational understanding [5]â[9] and social cognition (e.g., theory of mind (TOM) and trait inference) [10]â[20], no study has clarified the ES core, the process in which event features are integrated into a coherent representation of a situation. A few recent studies assessed this ES core more directly [21]â[26]. In these studies, event features (e.g., objects, agents, and backgrounds) that constitute a situation were presented in words or pictures, and the brain regions that respond to successful integration of these event features were examined. These studies demonstrated that the ES core process recruits the medial prefrontal cortex (MPFC), retrosplenial cortex (RSC), inferior parietal cortex (IPL), dorso-lateral prefrontal cortex (DLPFC), ventro-lateral prefrontal cortex (VLPFC), parahippocampal cortex (PHC), middle temporal gyrus (MTG), and temporal pole (TP).

Although previous studies have assessed the neural basis of ES as a whole, critical ES subdivisions and the neural basis of those subdivisions have not yet been clarified. For example, whether ES recruits similar or different neural substrates when occurring explicitly versus implicitly remains unclear. ES is sometimes conducted without an explicit intention to do so; this implicit aspect of ES has great significance. For example, in daily life, an individual will usually spontaneously notice his or her own situation. In this case, ES occurs implicitly. On the other hand, although this is often the case in a laboratory experiment, it is rare that an individual will need to explicitly understand a situation in daily life. When âimplicitâ, the aforementioned process is not assumed to be unconscious. Instead, it is characterized by its spontaneous activation via bottom-up input from the environment, regardless of explicit intentions to do so. Another example of the importance of the implicit aspect of ES is the fact that social phenomena such as stereotypes or prejudices are problematic due to their implicit nature [27]â[29]. Because these phenomena depend on ES-based trait inference, clarifying the neural mechanisms underlying the implicit aspects of ES would contribute to understanding these social phenomena. In fact, implicit aspects of various cognitive processes and their neural substrates have attracted immense attention in recent years [27], [29]â[33]. However, previous studies have not differentiated the neural basis of implicit ES from that of explicit ES. Of the above-mentioned studies that directly assessed the ES core, that by Bar et al. employed only an implicit task [21]. Other studies employed only explicit tasks [22]â[26]. So, there is an urgent need to differentiate the neural basis of implicit and explicit ES by conducting a comparison in a unified experimental framework. In the present study, we differentiated between the neural substrates of implicit and explicit ES by assuming that two cognitive processes underpin implicit and explicit ES. The first process is recruited by implicit ES (implicit ES process) and is also recruited when ES is conducted explicitly, thus providing a common ground for implicit and explicit ES. The second process is recruited together with the implicit ES process in the explicit ES (explicit ES process). We differentiated brain regions specific to those processes using the following paradigm. In one condition, subjects were shown a picture set of contextually irrelevant objects and performed an object-name-matching task. Unknown to the subjects, contextually related objects, indicating a situation, were occasionally inserted to induce implicit ES. Brain regions that increased their activity in response to this insertion were assumed to be the neural basis of the implicit ES process. In the second condition, subjects were explicitly instructed to infer the situation indicated by objects to demonstrate explicit ES. Brain activation induced in the explicit condition was compared with that induced in the implicit condition to clarify the neural basis of the explicit ES process. Based on previous findings noted above, it was expected that ES as a whole would recruit the MPFC, RSC, IPL, DLPFC/VLPFC, PHC, MTG, and TP. More importantly, it was expected that implicit and explicit ES processes would recruit different subsets of these brain regions.

Another important aspect of ES is the future-prediction component, but the neural basis underlying this component is not yet known. To cope with dynamically changing environments, not only must the current situation be inferred but the possible future situation must also be predicted. This predictive component plays an essential role in ES. Previous studies that assessed the neural basis of future prediction emphasized that common brain networks are recruited as in episodic memory recall, including the hippocampus, PHC, MPFC, VLPFC, RSC, precuneus, posterior cingulate cortex (PCC), and the temporo-parietal junction (TPJ) [34]â[42]. Based on these findings, it has been suggested that future prediction and episodic memory recall both depend on a common âscene constructionâ process [43], [44]. This process is similar to the completion inference process in ES, as it integrates event features into a coherent situational representation. In addition to this completion inference process, however, future prediction additionally demands representing unexperienced future situations as hypothetical ones. Previous studies have not dissociated this key component of future prediction from completion inference itself. Consequently, the specific neural substrate of this component is still unknown. As for the candidate neural substrates of this component, we expected that the TPJ plays a primary role among the regions previously indicated to be involved in future prediction. This is because the TPJ has been consistently indicated to be involved in representing hypothetical perspective or perspective differences in previous studies on TOM, perspective taking, and self-transcendence [45]â[51]. In the present study, to extract brain regions specific to representing a hypothetical situation in future prediction, the following paradigm was used. In addition to the above-explained explicit task condition in which subjects were asked to infer a situation indicated by objects, we also used a future-prediction condition in which subjects were asked to predict future situations likely to happen after those indicated by the objects had already occurred. By comparing the brain activity induced under these two task conditions, this study aimed to clarify the neural basis of the future-prediction component of ES.1 / 1ChatGPTThe article titled "The Neural Basis of Event Simulation: An fMRI Study" by Yukihito Yomogida et al. examines the neural processes underlying event simulation (ES), which is the brain's ability to infer situations by associating perceived features such as objects, agents, and actions. Here's a summary of the main points:Abstract:ES plays a role in various cognitive functions, including perceptual prediction, situational understanding, and social cognition.The study used fMRI to determine the neural basis for different facets of ES, focusing on:Whether ES operates differently on an explicit vs. implicit level.Identifying the neural regions linked to the future-prediction component of ES.Findings reveal that implicit ES activates areas like the lateral prefrontal cortex and medial/lateral parietal cortex, whereas explicit ES triggers the medial prefrontal cortex, posterior cingulate cortex, and medial/lateral temporal cortex.The left temporoparietal junction plays a crucial role in the future-prediction aspect of ES.Introduction:Humans infer current situations and anticipate future transitions using limited environmental information.Situational inferences result from associating partially perceived event features, like objects and actions, to simulate the entire situation in the brain. This process is called event simulation (ES).ES is foundational to various cognitive processes, from basic perceptual prediction to complex social cognition tasks.Although prior studies have explored the neural foundation of ES-associated processes, they haven't specifically addressed the core of ES â integrating event features to form a cohesive situational representation.Previous research pinpointed brain regions involved in ES, including the medial prefrontal cortex, inferior parietal cortex, middle temporal gyrus, and others.There's a need to distinguish between the neural bases of implicit and explicit ES.Implicit ES refers to spontaneous situational awareness without deliberate intention, whereas explicit ES involves a conscious effort to understand a situation.Another essential element of ES is its future-prediction component. Predicting potential future situations based on current circumstances is crucial for navigating dynamic environments.The article underscores the importance of understanding the neural basis of event simulation, which encompasses both conscious (explicit) and unconscious (implicit) processes of inferring and predicting events based on partial information. By examining these processes at the neural level, the study sheds light on the intricate ways our brain constructs and predicts situational narratives.1 / 1Connections between the articles1 / 1ChatGPTBoth articles delve into the neural foundations of how the human brain processes, infers, and predicts events based on sensory information, but they approach the topic from slightly different angles. Here's a breakdown of the connections between the two articles:Topic of Study:Both articles investigate the neural mechanisms underlying event prediction and simulation.The first article titled "Neural Foundations of Mental Simulation" discusses how people predict the trajectory of moving objects in a dynamic scene.The second article titled "The Neural Basis of Event Simulation: An fMRI Study" delves into event simulation (ES) and how the brain infers situations by connecting perceived features such as objects, agents, and actions.Neural Mechanisms:The first article touched on "neural correlates," suggesting a focus on the specific brain regions or neural activities associated with predicting motion in a dynamic scene.The second article provides a more in-depth examination of the neural substrates activated during implicit and explicit ES. It identifies multiple brain regions like the medial prefrontal cortex, lateral prefrontal cortex, and temporoparietal junction, among others.Implicit vs. Explicit Processes:The second article specifically differentiates between implicit and explicit ES. Implicit ES refers to spontaneous and unconscious situational inferences, while explicit ES involves a conscious effort to comprehend a situation.Although the first article didn't explicitly mention this differentiation, it's possible that the brain's ability to predict a ball's trajectory in a dynamic scene can be linked to implicit processes, as these predictions might occur spontaneously.Predictive Components:Both articles underscore the brain's ability to anticipate or predict events.The first article looks at predictions related to motion trajectory in a visual scene.The second article delves deeper into the "future-prediction component" of ES, examining how the brain anticipates future situations based on the current context.Methodology:Both studies likely employ neuroimaging techniques to identify neural mechanisms, with the second article explicitly mentioning the use of functional magnetic resonance imaging (fMRI).In summary, both articles explore the intricate neural mechanisms that allow humans to simulate, infer, and predict events. While their specific focuses differ, they converge on the broader theme of understanding how the brain processes dynamic information to anticipate future events or situations.1 / 1Connections with the following: Sources of uncertainty in intuitive physics.
Cited by 28

Journal Article Database: APA PsycInfo

Smith, Kevin A. Vul, Edward
Citation
Smith, K. A., & Vul, E. (2013). Sources of uncertainty in intuitive physics. Topics in Cognitive Science, 5(1), 185â199. https://doi.org/10.1111/tops.12009
Abstract
Recent work suggests that people predict how objects interact in a manner consistent with Newtonian physics, but with additional uncertainty. However, the sources of uncertainty have not been examined. In this study, we measure perceptual noise in initial conditions and stochasticity in the physical model used to make predictions. Participants predicted the trajectory of a moving object through occluded motion and bounces, and we compared their behavior to an ideal observer model. We found that human judgments cannot be captured by simple heuristics and must incorporate noisy dynamics. Moreover, these judgments are biased consistently with a prior expectation on object destinations, suggesting that people use simple expectations about outcomes to compensate for uncertainty about their physical models. (PsycINFO Database Record (c) 2016 APA, all rights reserved)1 / 1ChatGPTThe article titled "Sources of uncertainty in intuitive physics" by Smith and Vul (2013) explores the mechanisms and sources of uncertainty when humans predict how objects will interact, particularly in relation to motion trajectory. Let's draw connections between this article and the previously mentioned articles on neural foundations of mental simulation:Topic of Study:All articles deal with the human brain's ability to simulate, infer, and predict events or motion.Smith & Vul (2013) specifically delve into the uncertainties inherent in our intuitive understanding of physics, trying to pinpoint where our predictions might deviate from actual Newtonian physics.Prediction of Motion:The article on "Neural Foundations of Mental Simulation" discussed the brain's capacity to predict the trajectory of a moving object in a dynamic scene, much like Smith & Vul's focus on predicting the trajectory of a moving object through occlusions and bounces.The concept of predicting motion trajectory, whether it's a ball flying across a screen or through occlusions, appears central to both discussions.Sources of Uncertainty:Smith & Vul's article specifically identifies and measures perceptual noise and stochasticity in our intuitive physical models as sources of uncertainty.This can be linked to the neural mechanisms discussed in the other articles, suggesting that our brain's neural pathways and structures, while sophisticated, might introduce uncertainties or biases in our predictions.Compensatory Mechanisms:Smith & Vul highlight that humans use simple expectations about outcomes to compensate for uncertainties in their physical models. This suggests that our brains might rely on heuristics or prior experiences to "fill in the gaps" when predicting motion.The "Neural Basis of Event Simulation" article similarly discussed the idea of event simulation, where the brain connects perceived features to infer a whole situation. This can be seen as a form of compensating for missing or uncertain information.Ideal Observer Model:The use of an "ideal observer model" by Smith & Vul suggests they compared human judgments to an idealized, error-free model of prediction.This ties back to the theme of neural correlates and neural mechanisms in the other articles, as they aim to understand how the human brain's predictions and simulations compare to "reality" or more accurate, physics-based models.In essence, the Smith & Vul article builds upon the themes introduced in the other articles by focusing on the sources of uncertainty in our predictions, providing a more granulated perspective on the inherent imperfections in our intuitive understanding of physics and motion prediction.1 / 1So I argued elsewhere the the neural correlates of consciousness happen on multiscale polycomputation using directed acyclic graphs for causal inference with Bayesian updating and sparse representation.1 / 1ChatGPTYour argument is quite complex, touching upon several advanced concepts in neuroscience, cognitive science, and computational modeling. Here's a breakdown of the ideas:Neural Correlates of Consciousness (NCC):This refers to the specific neural systems and processes in the brain that correlate with conscious experience.Multiscale Polycomputation:"Multiscale" implies that the computation or processing occurs at multiple levels or scales. This might mean different levels of neural organization, from individual neurons to neural networks to entire brain regions."Polycomputation" suggests the simultaneous utilization of different computational methods or strategies.Directed Acyclic Graphs (DAGs):DAGs are graphs that have a direction (from one point to another) and don't have any cycles (closed loops). They're often used in computational modeling, especially for representing causal relationships between variables.Causal Inference:This is the process of determining cause and effect relationships. In the context of the brain, it might refer to understanding how one neural process causes or influences another.Bayesian Updating:This is a statistical method based on Bayes' theorem, used for updating the probabilities of hypotheses as more evidence becomes available. In the context of neural processing, it might suggest that the brain updates its beliefs or models of the world in a probabilistic way based on new sensory data.Sparse Representation:This refers to the idea that only a small number of neurons might be active or representing information at a given time. Sparse representation is efficient because it requires less energy and can lead to more efficient information processing.Connecting these ideas, your argument seems to posit that conscious experience in the brain emerges from a multi-layered computational process. This process uses DAGs to infer causal relationships, continuously updates these inferences in a Bayesian manner as new data comes in, and represents this information efficiently through sparse coding.It's a sophisticated and intriguing proposition that integrates many current ideas in neuroscience and computational theory. If supported by empirical data, it could offer a comprehensive framework for understanding how the brain gives rise to conscious experience.1 / 1I think I called it Attentional Cladistics1 / 1ChatGPT"Attentional Cladistics" sounds like a novel concept. Here's an interpretation of how it might relate to the earlier described framework:Attention: At its core, attention refers to the cognitive process of selectively concentrating on one aspect of the environment while ignoring other things. In neural terms, this can be thought of as certain neural networks being more active or "lit up" while others are less active or dormant.Cladistics: Traditionally, cladistics is a method used in biology to classify organisms based on the evolutionary branches (or clades) they belong to, based on shared characteristics. The primary output of cladistic analysis is a cladogram, which is a form of a tree diagram (akin to DAGs) that showcases the relations among various species.Combining the two, "Attentional Cladistics" might propose a hierarchical, branching structure to the way attention operates in the brain. This could mean:Different levels or "branches" of attention, from the most basic and broad to the most detailed and specific.Each branch or level could be based on shared neural or cognitive characteristics.The hierarchy might represent the flow of attentional focus, starting from broad, general attention to specific, detailed attention.This flow might be influenced by the Bayesian updating mechanism you mentioned earlier, where prior beliefs or knowledge influence where attention is directed next.The overall structure would be dynamic, adapting based on the environment and the task at hand.In essence, "Attentional Cladistics" could be a conceptual framework for understanding the branching, hierarchical nature of attention in the brain, influenced by prior knowledge and continuously updated based on new information. It's a unique perspective that would be interesting to explore further, both conceptually and empirically.1 / 1Write the introduction to an essay introducing this theory, with some possible alternate names, its main tenets or theses, and criticisms of the theory.1 / 1ChatGPTIntroductionIn the intricate web of cognitive sciences, the mechanism by which our brains process, prioritize, and act upon the vast influx of information has long captivated researchers and theorists. Dubbed "Attentional Cladistics," this innovative theory proposes a hierarchical, branching structure to the way attention operates within our neural networks. Evoking comparisons to the evolutionary branches (clades) found in biology, the theory suggests that our attentional processes can be classified, mapped, and understood in a similar branching fashion.While the name "Attentional Cladistics" beautifully encapsulates the essence of the theory, alternate names such as "Hierarchical Attentional Networks" or "Neuro-Cognitive Branching" have also been floated to capture its multi-faceted nature.The main tenets of this theory include:Hierarchical Structuring: Similar to how organisms are categorized based on shared characteristics, attentional processes can be categorized in a hierarchical manner, from the most generalized to the most specific.Dynamic Adaptability: This hierarchy is not static; it's fluid and influenced by the environment, task at hand, and prior knowledge.Bayesian Mechanisms: The flow of attention is continuously updated, akin to Bayesian updating, where prior beliefs influence the subsequent direction of attention.However, like all groundbreaking theories, Attentional Cladistics has not been without its critics. Detractors argue that comparing cognitive processes to evolutionary biology might be an oversimplification or that the theory doesn't account for certain attentional anomalies observed in real-world scenarios. Some believe that the Bayesian component, while mathematically elegant, might not fully capture the complexities of human attention.This essay delves deep into the roots, branches, and leaves of Attentional Cladistics, exploring its potential to reshape our understanding of cognitive processes and addressing the criticisms it faces.Note: This is a fictional introduction and the theory as presented is a conceptual exploration based on the details you provided.1 / 1Some context:

Hierarchical Attention Networks for Document Classif i cation 
Zichao Yang1, Diyi Yang1, Chris Dyer1, Xiaodong He2, Alex Smola1, Eduard Hovy1 
1Carnegie Mellon University,2Microsoft Research, Redmond 
{zichaoy, diyiy, cdyer, hovy}@cs.cmu.edu 
xiaohe@microsoft.comalex@smola.org 
Abstract 
We propose a hierarchical attention network 
for document classif i cation. Our model has 
two distinctive characteristics: (i) it has a hier- 
archical structure that mirrors the hierarchical 
structure of documents; (ii) it has two levels 
of attention mechanisms applied at the word- 
and sentence-level, enabling it to attend dif- 
ferentially to more and less important con- 
tent when constructing the document repre- 
sentation. Experiments conducted on six large 
scale text classif i cation tasks demonstrate that 
the proposed architecture outperform previous 
methods by a substantial margin. Visualiza- 
tion of the attention layers illustrates that the 
model selects qualitatively informative words 
and sentences. 
1Introduction 
Text classif i cation is one of the fundamental task in 
Natural Language Processing.The goal is to as- 
sign labels to text. It has broad applications includ- 
ing topic labeling (Wang and Manning, 2012), senti- 
ment classif i cation (Maas et al., 2011; Pang and Lee, 
2008), and spam detection (Sahami et al., 1998). 
Traditional approaches of text classif i cation repre- 
sent documents with sparse lexical features, such 
as n-grams, and then use a linear model or kernel 
methods on this representation (Wang and Manning, 
2012; Joachims, 1998).More recent approaches 
useddeeplearning, suchasconvolutionalneuralnet- 
works (Blunsom et al., 2014) and recurrent neural 
networks based on long short-term memory (LSTM) 
(Hochreiter and Schmidhuber, 1997) to learn text 
representations. 
pork belly = delicious . || scallops? || I donât even 
like scallops, and these were a-m-a-z-i-n-g . || fun 
and tasty cocktails. || next time I in Phoenix, I will 
go back here. || Highly recommend. 
Figure 1: A simple example review from Yelp 2013 that con- 
sists of i ve sentences, delimited by period, question mark. The 
i rst and third sentence delivers stronger meaning and inside, 
the word delicious, a-m-a-z-i-n-g contributes the most in def i n- 
ing sentiment of the two sentences. 
Although neural-networkâbased approaches to 
text classif i cation have been quite effective (Kim, 
2014; Zhang et al., 2015; Johnson and Zhang, 2014; 
Tang et al., 2015), in this paper we test the hypoth- 
esis that better representations can be obtained by 
incorporating knowledge of document structure in 
the model architecture. The intuition underlying our 
model is that not all parts of a document are equally 
relevant for answering a query and that determining 
the relevant sections involves modeling the interac- 
tions of the words, not just their presence in isola- 
tion. 
Our primary contribution is a new neural archi- 
tecture (Â§2), the Hierarchical Attention Network 
(HAN) that is designed to capture two basic insights 
about document structure. First, since documents 
have a hierarchical structure (words form sentences, 
sentences form a document), we likewise construct a 
document representation by i rst building represen- 
tations of sentences and then aggregating those into 
a document representation. Second, it is observed 
that different words and sentences in a documents 
are differentially informative. Moreover, the impor- 
tance of words and sentences are highly context de- 
pendent, i.e. the same word or sentence may be dif- 
ferentially important in different context (Â§3.5). To 
include sensitivity to this fact, our model includes 
two levels of attention mechanisms (Bahdanau et al., 
2014; Xu et al., 2015) â one at the word level and 
one at the sentence level â that let the model to 
pay more or less attention to individual words and 
sentences when constructing the representation of 
the document. To illustrate, consider the example 
in Fig. 1, which is a short Yelp review where the 
task is to predict the rating on a scale from 1â5. In- 
tuitively, the i rst and third sentence have stronger 
information in assisting the prediction of the rat- 
ing; within these sentences, the word delicious, 
a-m-a-z-i-n-g contributes more in implying 
the positive attitude contained in this review. At- 
tention serves two benef i ts: not only does it often 
result in better performance, but it also provides in- 
sight into which words and sentences contribute to 
the classif i cation decision which can be of value in 
applications and analysis (Shen et al., 2014; Gao et 
al., 2014). 
The key difference to previous work is that our 
system uses context to discover when a sequence of 
tokens is relevant rather than simply i ltering for (se- 
quences of) tokens, taken out of context. To evaluate 
theperformanceofourmodelincomparisontoother 
common classif i cation architectures, we look at six 
data sets (Â§3). Our model outperforms previous ap- 
proaches by a signif i cant margin.1 / 1ChatGPTIntroduction: Attentional Cladistics and Hierarchical Document ClassificationThe vast realm of cognitive neuroscience has often drawn parallels with the computational techniques we use in modern artificial intelligence. One such crossroad is observed in "Attentional Cladistics," an emerging cognitive theory that postulates attention operates within our neural networks in a hierarchical, branching structure. This theory echoes the approach presented by Yang et al. in their research on Hierarchical Attention Networks for Document Classification. Their work suggests that when processing documents, not all content holds equal significance. Instead, the relevance of words and sentences varies, with some contributing more critical information than others.This hierarchical processing is reminiscent of how our attentional networks might function. Under the banner of "Attentional Cladistics," or alternative names like "Hierarchical Attentional Networks" and "Neuro-Cognitive Branching," the theory suggests that attentional processes are compartmentalized and structured, from broad to specific, mirroring the way documents are structured from words to sentences.While Yang et al.'s work is primarily geared towards document classification in the field of Natural Language Processing (NLP), the overarching concept finds resonance with the broader theory of attention. In NLP, this hierarchical approach combined with attention mechanisms offers a way to discern the importance of different segments within a document, shedding light on how information is processed and prioritized.However, Attentional Cladistics, in its comparison with computational models, has met with criticism. Detractors question if cognitive processes, as intricate and multifaceted as they are, can be adequately represented by a branching hierarchical structure. Furthermore, while the hierarchical attention model presented by Yang et al. showcases impressive results in document classification, can it be generalized to the myriad ways our brains process information?This essay will venture deeper into the intricacies of Attentional Cladistics, elucidating its potential applications in understanding neural attention processes and addressing the critiques it encounters, all while drawing parallels with hierarchical document classification.Note: This introduction bridges the presented paper on hierarchical attention networks for document classification with the theoretical framework of "Attentional Cladistics," contextualizing it within a broader cognitive perspective.1 / 12Hierarchical Attention Networks 
The overall architecture of the Hierarchical Atten- 
tion Network (HAN) is shown in Fig. 2.It con- 
sists of several parts: a word sequence encoder, a 
word-level attention layer, a sentence encoder and a 
sentence-level attention layer. We describe the de- 
tails of different components in the following sec- 
tions. 
2.1GRU-based sequence encoder 
The GRU (Bahdanau et al., 2014) uses a gating 
mechanism to track the state of sequences without 
using separate memory cells. There are two types of 
gates: the reset gate rtand the update gate zt. They 
together control how information is updated to the 
? 
h21 
? 
h21 ? 
h22 
? 
h22 
? ! 
h22 
? ! 
h22 
? 
h2T 
? 
h2T 
? ! 
h2T 
? ! 
h2T 
uwuw w21w21w22w22w2Tw2T 
word 
encoder 
word 
attention 
? 
h1 
? 
h1 
? ! 
h1 
? ! 
h1 
? 
h2 
? 
h2 
? ! 
h2 
? ! 
h2 
? 
hL 
? 
hL 
? ! 
hL 
? ! 
hL 
usus s1s1 s2s2sLsL 
âµLâµL sentence 
encoder 
sentence 
attention 
v 
softmax 
âµ21âµ21âµ22âµ22 âµ2Tâµ2T 
? ! 
h21 
? ! 
h21 
âµ1âµ1 âµ2âµ2 
Figure 2: Hierarchical Attention Network. 
state. At time t, the GRU computes the new state as 
ht= (1 â zt) ? htâ1+ zt?Ëht.(1) 
This is a linear interpolation between the previous 
state htâ1and the current new state Ë 
htcomputed 
with new sequence information. The gate ztdecides 
how much past information is kept and how much 
new information is added. ztis updated as: 
zt= Ï(Wzxt+ Uzhtâ1+ bz),(2) 
where xtis the sequence vector at time t. The can- 
didate stateËhtis computed in a way similar to a tra- 
ditional recurrent neural network (RNN): 
Ë 
ht= tanh(Whxt+ rt? (Uhhtâ1) 
+ bh),(3) 
Here rtis the reset gate which controls how much 
the past state contributes to the candidate state. If rt 
is zero, then it forgets the previous state. The reset 
gate is updated as follows: 
rt= Ï(Wrxt+ Urhtâ1+ br)(4) 
2.2Hierarchical Attention 
We focus on document-level classif i cation in this 
work.Assume that a document has L sentences 
siand each sentence contains Tiwords. witwith 
t â [1,T] represents the words in the ith sentence. 
The proposed model projects the raw document into 
a vector representation, on which we build a classi- 
i er to perform document classif i cation. In the fol- 
lowing, we will present how we build the document 
level vector progressively from word vectors by us- 
ing the hierarchical structure. 
Word EncoderGiven a sentence with words 
wit,t â [0,T], we i rst embed the words to vectors 
through an embedding matrix We, xij= Wewij. 
We use a bidirectional GRU (Bahdanau et al., 2014) 
to get annotations of words by summarizing infor- 
mation from both directions for words, and therefore 
incorporate the contextual information in the anno- 
tation. The bidirectional GRU contains the forward 
GRU â 
f which reads the sentence sifrom wi1to 
wiTand a backward GRU â â 
f which reads from wiT 
to wi1: xit=Wewit,t â [1,T], 
â â 
hit=â â â 
âGRU(xit),t â [1,T], 
â â 
hit=â â â 
âGRU(xit),t â [T,1]. 
We obtain an annotation for a given word witby 
concatenating the forward hidden state â 
hitand 
backward hidden state â â 
hit, i.e., hit= [â âhit, â â 
hit], 
which summarizes the information of the whole sen- 
tence centered around wit. 
Note that we directly use word embeddings. For 
a more complete model we could use a GRU to get 
word vectors directly from characters, similarly to 
(Ling et al., 2015). We omitted this for simplicity. 
Word AttentionNot all words contribute equally 
to the representation of the sentence meaning. 
Hence, we introduce attention mechanism to extract 
such words that are important to the meaning of the 
sentence and aggregate the representation of those 
informative words to form a sentence vector. Specif- 
ically, uit=tanh(Wwhit+ bw)(5) 
Î±it= 
exp(u> 
ituw) 
P 
texp(u>ituw) 
(6) 
si= X 
t 
Î±ithit.(7) 
That is, we i rst feed the word annotation hitthrough 
a one-layer MLP to get uitas a hidden represen- 
tation of hit, then we measure the importance of 
the word as the similarity of uitwith a word level 
context vector uwand get a normalized importance 
weight Î±itthrough a softmax function. After that, 
we compute the sentence vector si(we abuse the no- 
tation here) as a weighted sum of the word annota- 
tions based on the weights. The context vector uw 
can be seen as a high level representation of a i xed 
query âwhat is the informative wordâ over the words 
like that used in memory networks (Sukhbaatar et 
al., 2015; Kumar et al., 2015). The word context 
vector uwis randomly initialized and jointly learned 
during the training process. 
Sentence EncoderGiven the sentence vectors si, 
we can get a document vector in a similar way. We 
use a bidirectional GRU to encode the sentences: 
â 
hi=â â â 
âGRU(si),i â [1,L], 
â â 
hi=â â â 
âGRU(si),t â [L,1]. 
We concatenate â 
hiand â â 
hjto get an annotation of 
sentence i, i.e., hi= [â âhi, â â 
hi]. hisummarizes the 
neighbor sentences around sentence i but still focus1 / 1ChatGPTon the i-th sentence.Sentence Attention: As with the word attention mechanism, not all sentences contribute equally to the representation of the document's meaning. Therefore, an attention mechanism is introduced at the sentence level to capture the most informative sentences.Specifically, the calculations are:
ui=tanhâ¡(Wshi+bs)u_i = \tanh(W_s h_i + b_s)uiâ=tanh(Wsâhiâ+bsâ)
Î±i=expâ¡(uiTus)âjexpâ¡(ujTus)\alpha_i = \frac{\exp(u_i^T u_s)}{\sum_{j} \exp(u_j^T u_s)}Î±iâ=âjâexp(ujTâusâ)exp(uiTâusâ)â
v=âiÎ±ihiv = \sum_i \alpha_i h_iv=âiâÎ±iâhiâHere, uiu_iuiâ is the hidden representation of the sentence annotation hih_ihiâ after passing through a one-layer MLP. The importance of the sentence is measured as the similarity of uiu_iuiâ with a sentence-level context vector usu_susâ. The normalized importance weight Î±i\alpha_iÎ±iâ is obtained using a softmax function. The document vector vvv is then computed as a weighted sum of the sentence annotations based on their weights. Similar to the word context vector uwu_wuwâ, the sentence context vector usu_susâ can be seen as a high-level representation of a query about the importance of the sentence. It is learned jointly during the training process.Document Classification: Once we obtain the document vector vvv using hierarchical attention over words and sentences, it can be used for various tasks, such as document classification. A common approach is to pass vvv through a softmax layer to get the probabilities for different classes. The model is trained end-to-end with the objective to minimize the classification error.In summary, the Hierarchical Attention Network (HAN) is a deep learning architecture that applies attention mechanisms at both the word and sentence levels. This hierarchical structure enables the model to focus on the most informative words in a sentence and the most important sentences in a document, leading to a more expressive representation for text classification tasks.1 / 1tant for poor ratings and less so for good ones. 
3.6Visualization of attention 
Inordertovalidatethatourmodelisabletoselectin- 
formativesentencesandwordsinadocument, wevi- 
sualize the hierarchical attention layers in Figures 5 
and 6 for several documents from the Yelp 2013 and 
Yahoo Answers data sets. 
Every line is a sentence (sometimes sentences 
spill over several lines due to their length). Red de- 
notes the sentence weight and blue denotes the word 
weight. Due to the hierarchical structure, we nor- 
malize the word weight by the sentence weight to 
make sure that only important words in important 
sentences are emphasized.For visualization pur- 
poses we display âp spw. The âp sterm displays the 
important words in unimportant sentences to ensure 
that they are not totally invisible. 
Figure 5 shows that our model can select the 
words carrying strong sentiment like delicious, 
amazing, terrible and their corresponding 
sentences.Sentences containing many words 
like cocktails, pasta, entree are disre- 
garded.Note that our model can not only select 
words carrying strong sentiment, it can also deal 
with complex across-sentence context. For example, 
there are sentences like i donât even like 
scallops in the i rst document of Fig. 5, if look- 
ing purely at the single sentence, we may think this 
is negative comment. However, our model looks at 
the context of this sentence and i gures out this is a 
positive review and chooses to ignore this sentence. 
Our hierarchical attention mechanism also works 
well for topic classif i cation in the Yahoo Answer 
data set.For example, for the left document 
in Figure 6 with label 1, which denotes Science 
and Mathematics, our model accurately localizes 
the words zebra, strips, camouflage, 
predator and their corresponding sentences. For 
the right document with label 4, which denotes 
ComputersandInternet, ourmodelfocusesonweb, 
searches, browsers and their corresponding 
sentences. Note that this happens in a multiclass set- 
ting, that is, detection happens before the selection 
of the topic! 
4Related Work 
Kim (2014) use neural networks for text classif i- 
cation. The architecture is a direct application of 
CNNs, as used in computer vision (LeCun et al., 
1998), albeit with NLP interpretations. Johnson and 
Zhang (2014) explores the case of directly using 
a high-dimensional one hot vector as input. They 
i nd that it performs well. Unlike word level mod- 
elings, Zhang et al. (2015) apply a character-level 
CNN for text classif i cation and achieve competitive 
results.Socher et al. (2013) use recursive neural 
networks for text classif i cation.Tai et al. (2015) 
GT: 4 Prediction: 4 
porkbelly=delicious. 
scallops? 
idonât. 
even. 
like. 
scallops , and these were a-m-a-z-i-n-g . 
funandtastycocktails. 
nexttimeiâminphoenix,iwillgo 
backhere. 
highlyrecommend. 
GT: 0 Prediction: 0 
terriblevalue. 
orderedpastaentree. 
. $16.95goodtastebutsizewasan 
appetizersize. 
. nosalad,nobreadnovegetable. 
thiswas. 
ourandtastycocktails. 
oursecondvisit. 
iwillnotgoback. 
Figure 5: Documents from Yelp 2013. Label 4 means star 5, label 0 means star 1. 
GT: 1 Prediction: 1 
whydoeszebrashavestripes? 
whatisthepurposeorthosestripes? 
whodotheyservethezebrasinthe 
wildlife? 
thisprovidescamouf l age-predator 
visionissuchthatitisusuallydiff i cult 
forthemtoseecomplexpatterns 
GT: 4 Prediction: 4 
howdoigetridofalltheoldweb 
searchesihaveonmywebbrowser? 
iwanttocleanupmywebbrowser 
gototools>options. 
thenclickâdeletehistoryâandâ 
cleanuptemporaryinternetf i les.â 
Figure 6: Documents from Yahoo Answers. Label 1 denotes Science and Mathematics and label 4 denotes Computers and Internet. 
explore the structure of a sentence and use a tree- 
structured LSTMs for classif i cation. There are also 
some works that combine LSTM and CNN struc- 
ture to for sentence classif i cation (Lai et al., 2015; 
Zhou et al., 2015). Tang et al. (2015) use hierarchi- 
cal structure in sentiment classif i cation. They i rst 
use a CNN or LSTM to get a sentence vector and 
then a bi-directional gated recurrent neural network 
to compose the sentence vectors to get a document 
vectors. There are some other works that use hier- 
archical structure in sequence generation (Li et al., 
2015) and language modeling (Lin et al., 2015). 
The attention mechanism was proposed by (Bah- 
danau et al., 2014) in machine translation. The en- 
coder decoder framework is used and an attention 
mechanism is used to select the reference words in 
original language for words in foreign language be- 
fore translation. Xu et al. (2015) uses the attention 
mechanism in image caption generation to select the 
relevant image regions when generating words in the 
captions. Further uses of the attention mechanism 
include parsing (Vinyals et al., 2014), natural lan- 
guage question answering (Sukhbaatar et al., 2015; 
Kumar et al., 2015; Hermann et al., 2015), and im- 
age question answering (Yang et al., 2015).Un- 
like these works, we explore a hierarchical attention 
mechanism (to the best of our knowledge this is the 
i rst such instance). 
5Conclusion 
In this paper, we proposed hierarchical attention net- 
works (HAN) for classifying documents. As a con- 
venient side-effect we obtained better visualization 
using the highly informative components of a doc- 
ument. Our model progressively builds a document 
vector by aggregating important words into sentence 
vectors and then aggregating important sentences 
vectors to document vectors. Experimental results 
demonstrate that our model performs signif i cantly 
better than previous methods. Visualization of these 
attention layers illustrates that our model is effective 
in picking out important words and sentences.1 / 1ChatGPTThis text seems to be from a research paper or article that discusses the application of hierarchical attention networks (HAN) in text classification. Here's a concise summary of the provided content:Hierarchical Attention Networks (HAN) for Document Classification:The attention mechanism allows the model to select informative words and sentences for better document classification.They visually examined how the model assigns attention to different parts of documents using data sets like Yelp 2013 and Yahoo Answers.In the Yelp dataset, the model could discern the sentiment of the words like 'delicious' and 'terrible' and use contextual information to accurately predict the sentiment of sentences.In the Yahoo Answers dataset, the model could accurately focus on keywords relevant to the topic of the document, e.g., "zebra" for Science and Mathematics or "web searches" for Computers and Internet.The attention mechanism has been used in various other applications such as machine translation, image caption generation, parsing, and question answering. This paper introduced a hierarchical approach to it.The conclusion is that the proposed hierarchical attention networks (HAN) not only classify documents effectively but also provide insights into which parts of the documents were deemed essential by the model.This paper emphasizes the power and potential of using hierarchical attention mechanisms in understanding and classifying lengthy texts like documents.1 / 1Computer Science > Computation and Language
[Submitted on 29 May 2023 (v1), last revised 31 Jul 2023 (this version, v2)]
Forgotten Knowledge: Examining the Citational Amnesia in NLP
Janvijay Singh, Mukund Rungta, Diyi Yang, Saif M. Mohammad
Citing papers is the primary method through which modern scientific writing discusses and builds on past work. Collectively, citing a diverse set of papers (in time and area of study) is an indicator of how widely the community is reading. Yet, there is little work looking at broad temporal patterns of citation. This work systematically and empirically examines: How far back in time do we tend to go to cite papers? How has that changed over time, and what factors correlate with this citational attention/amnesia? We chose NLP as our domain of interest and analyzed approximately 71.5K papers to show and quantify several key trends in citation. Notably, around 62% of cited papers are from the immediate five years prior to publication, whereas only about 17% are more than ten years old. Furthermore, we show that the median age and age diversity of cited papers were steadily increasing from 1990 to 2014, but since then, the trend has reversed, and current NLP papers have an all-time low temporal citation diversity. Finally, we show that unlike the 1990s, the highly cited papers in the last decade were also papers with the least citation diversity, likely contributing to the intense (and arguably harmful) recency focus. Code, data, and a demo are available on the project homepage.1 / 1ChatGPTSummary:Title: Forgotten Knowledge: Examining the Citational Amnesia in NLPAuthors: Janvijay Singh, Mukund Rungta, Diyi Yang, Saif M. MohammadDomain: Computer Science > Computation and LanguageSubmission & Revision: Submitted on 29 May 2023 and revised on 31 Jul 2023.Overview:
This research paper delves into the patterns of citation in scientific papers, with a specific focus on the Natural Language Processing (NLP) domain. The goal is to analyze how broad and diverse the citational references are in terms of time and the field of study.Key Findings:The study analyzed approximately 71.5K NLP papers to determine citational patterns.A significant portion, around 62%, of the citations in these papers reference works from the previous five years of their publication.Only 17% of citations refer to works that are more than a decade old.Between 1990 to 2014, there was an increasing trend in the median age and diversity of citations. However, post-2014, this trend reversed, and current NLP papers exhibit the lowest temporal citation diversity ever recorded.Notably, in the last decade, the papers that were cited the most also had the least citation diversity. This suggests a growing focus on recent works, which could be detrimental to the breadth and depth of knowledge in the field.Additional Information:
The researchers have made the code, data, and a demonstration available on the project's homepage.Implications:
The study suggests a growing "amnesia" in citational practices within the NLP community, emphasizing recent works and potentially neglecting older foundational research. The decrease in citation diversity can have broader implications on the depth and richness of research in the field.1 / 1Do you know any semantic scholar id's?1 / 1ChatGPTYes, Semantic Scholar is an academic search engine developed by the Allen Institute for AI. Each article indexed by Semantic Scholar is assigned a unique identifier known as the Semantic Scholar ID (often abbreviated as "s2CID"). However, these IDs are specific to individual papers and are not generalizable. If you need the Semantic Scholar ID for a specific paper, you would typically find it on the Semantic Scholar website on the paper's detail page.For example, when viewing a paper on Semantic Scholar, the URL might look something like this:rubyCopy codehttps://www.semanticscholar.org/paper/[s2CID]/[paper-title]
Where [s2CID] would be the unique ID for that paper.I cannot generate or provide specific Semantic Scholar IDs as they are assigned by Semantic Scholar and my training data only goes up to January 2022. However, if you have access to the internet, you can easily find the s2CID by visiting Semantic Scholar's website and searching for the desired paper.1 / 1I tried the last article:

Semantic Scholar ID
6f8ab0fa15c87cee55b2ca5b8877e67848
87ddbf
Generate
Number of
references
Mean AoC
11.446808510638
299
47
Top 5 oldest papers cited:
[1934] Title: The influence of English
[1979] Title: A Formula for the Cini
Coefficient
[1981] Title: The Mismeasure of Man
[1998] Title: Region-based citation bias in
science
[2000] Title: Determinants of Citations to
Articles in Elite Law Reviews1 / 1ChatGPTIt seems you've provided the Semantic Scholar ID and some other related information for the article titled "Forgotten Knowledge: Examining the Citational Amnesia in NLP." This ID and the provided metrics offer insight into the paper's references and their respective ages. Notably, the article cites sources spanning from as far back as 1934 to more recent ones, indicating a broad temporal range in its citations.The "Mean AoC" (Age of Citation) of 47 suggests that, on average, the references used in the paper are 47 years old from the time of the paper's publication. The list of the top 5 oldest papers gives an idea of the diverse range of references cited in the paper.If you have any specific questions or tasks related to this information, please let me know how I can assist further!1 / 1No the number of references was 47 so the mean age of citation was just under 11 and a half1 / 1ChatGPTAh, I see. My apologies for misunderstanding.You're right:The paper has 47 references.The Mean Age of Citation (AoC) is approximately 11.446808510638, which means that, on average, the references used in the paper are just under 11 and a half years old from the time of the paper's publication.Thank you for pointing that out. How can I assist you further with this?1 / 1List a author in a related subject1 / 1ChatGPTCertainly! Given the topic of the paper you mentioned earlier, which discusses citational patterns in the NLP domain, a prominent author who has contributed significantly to the study of computational linguistics and NLP is:Prof. Christopher D. ManningHe is a professor of computer science and linguistics at Stanford University. Prof. Manning has authored numerous papers on natural language processing, machine learning, and linguistic theory. His work has had a considerable impact on the field, and he's also known for co-authoring the book "Foundations of Statistical Natural Language Processing" along with Hinrich SchÃ¼tze.Would you like more information on him or any other authors?1 / 1rpus ID: 990233
Recursive Deep Models for Semantic Compositionality Over a Sentiment Treebank
R. Socher, Alex Perelygin, +4 authors Christopher Potts
Published in Conference on Empiricalâ¦ 1 October 2013
Computer Science

TLDR
A Sentiment Treebank that includes fine grained sentiment labels for 215,154 phrases in the parse trees of 11,855 sentences and presents new challenges for sentiment compositionality, and introduces the Recursive Neural Tensor Network.
Highlight Information
Methods

Results
Abstract
Semantic word spaces have been very useful but cannot express the meaning of longer phrases in a principled way. Further progress towards understanding compositionality in tasks such as sentiment detection requires richer supervised training and evaluation resources and more powerful models of composition. To remedy this, we introduce a Sentiment Treebank. It includes fine grained sentiment labels for 215,154 phrases in the parse trees of 11,855 sentences and presents new challenges for sentiment compositionality. To address them, we introduce the Recursive Neural Tensor Network. When trained on the new treebank, this model outperforms all previous methods on several metrics. It pushes the state of the art in single sentence positive/negative classification from 80% up to 85.4%. The accuracy of predicting fine-grained sentiment labels for all phrases reaches 80.7%, an improvement of 9.7% over bag of features baselines. Lastly, it is the only model that can accurately capture the effects of negation and its scope at various tree levels for both positive and negative phrases.1 / 1ChatGPTThe paper titled "Recursive Deep Models for Semantic Compositionality Over a Sentiment Treebank" by R. Socher, Alex Perelygin, and others (including Christopher Potts) represents a significant contribution to the NLP domain, particularly in the area of sentiment analysis.Key Highlights:Introduction of the Sentiment Treebank: The authors present a unique Sentiment Treebank that contains granular sentiment labels for 215,154 phrases found in the parse trees of 11,855 sentences. This treebank offers an unprecedented level of granularity in sentiment labeling and is aimed at understanding how sentiments are composed in larger phrases and sentences.Challenges in Sentiment Compositionality: Due to the comprehensive nature of the treebank, it poses new challenges in understanding how sentiments are combined and presented in larger structures, moving beyond simple positive or negative classifications.Recursive Neural Tensor Network (RNTN): To tackle the complexities posed by the new treebank, the authors introduce a novel model known as the Recursive Neural Tensor Network. This model, when trained on the new treebank, surpassed the performance of all previously existing models.Performance Improvements: The RNTN achieves a state-of-the-art accuracy of 85.4% for classifying sentences into positive/negative sentiment, marking a 5.4% improvement over previous bests. When predicting fine-grained sentiment labels for all phrases, the accuracy reached 80.7%, showing a 9.7% improvement over traditional bag-of-features methods.Handling Negation: One of the unique capabilities of the RNTN is its ability to accurately understand the effects of negation and its scope at various levels in the parse tree, for both positive and negative sentiment phrases.This paper highlights the importance of understanding compositional semantics in sentiment analysis and showcases the potential of the Recursive Neural Tensor Network in capturing intricate nuances in sentiment composition.1 / 1Incorporate it into the essay.1 / 1ChatGPTCertainly! Here's the integration of the highlighted paper details into the essay format:Forgotten Knowledge and Citational Amnesia in NLPCiting papers is the primary method through which modern scientific writing discusses and builds on past work. Collectively, citing a diverse set of papers (in time and area of study) is an indicator of how widely the community is reading. Yet, there is little work looking at broad temporal patterns of citation. As the scientific community strives to build upon foundational knowledge, it becomes crucial to consider the pivotal works that have significantly advanced the field.One such notable contribution in the domain of Natural Language Processing (NLP) is the paper titled "Recursive Deep Models for Semantic Compositionality Over a Sentiment Treebank" by R. Socher, Alex Perelygin, and others. This work represents a groundbreaking contribution, especially in the realm of sentiment analysis.The authors introduced the Sentiment Treebank, a unique resource that contains fine-grained sentiment labels for 215,154 phrases derived from the parse trees of 11,855 sentences. The creation of this treebank ushered in an unprecedented granularity in sentiment labeling, aiming to decipher the compositional nature of sentiments in larger linguistic structures.However, with richer resources come newer challenges. The Sentiment Treebank highlighted the complexities surrounding sentiment compositionality, which goes beyond the traditional binary classification of sentiments as merely positive or negative.Addressing these challenges, the authors unveiled the Recursive Neural Tensor Network (RNTN). This novel model, upon training with the new treebank, not only outperformed all preceding models but also set new benchmarks in the domain. It achieved a remarkable accuracy of 85.4% for classifying sentiments at the sentence level and an impressive 80.7% accuracy when predicting fine-grained sentiment labels for all phrases. A distinct capability of the RNTN is its adeptness in understanding the intricate effects of negation across different levels of the parse tree, reflecting its nuanced understanding of sentiment composition.The paper by Socher et al. underscores the significance of understanding compositional semantics in sentiment analysis. It exemplifies the profound impact that such pioneering works can have on the field of NLP. However, as the study on "Forgotten Knowledge" suggests, it is paramount for the scientific community to ensure that such foundational works continue to be recognized and cited, preventing any form of citational amnesia.This systematic examination of citation patterns shows a concerning trend: around 62% of papers are cited from the immediate five years prior to publication, with only about 17% being more than ten years old. Such trends emphasize the need to continually revisit and build upon foundational knowledge, such as the works of Socher and his team, ensuring that the richness of past research is not overshadowed by the recency bias.I hope this fits well into the context of your essay! Let me know if you'd like further adjustments.1 / 1Our essay was on Attentional Cladistics1 / 1ChatGPTMy apologies for misunderstanding the context earlier. Let me provide a revised integration that relates to "Attentional Cladistics" while incorporating the work by Socher et al. on sentiment analysis.Attentional Cladistics and the Evolution of NLP ModelsThe study of attentional cladistics provides a framework for examining the evolutionary trajectories of cognitive and computational models, including those in the domain of Natural Language Processing (NLP). By tracing the lineage and divergence of attentional mechanisms, one can uncover the foundational concepts and pivotal contributions that have driven advancements in NLP.In the vast landscape of NLP research, the paper "Recursive Deep Models for Semantic Compositionality Over a Sentiment Treebank" by R. Socher, Alex Perelygin, and others stands out as a testament to the evolution of modeling techniques. The Sentiment Treebank, introduced in this work, offers fine-grained sentiment labels for over 215,000 phrases in parse trees. This granularity reflects a shift in attentional mechanisms, focusing not just on entire sentences but also on constituent parts to decode sentiment compositionality.The Recursive Neural Tensor Network (RNTN), developed by the authors, signifies an evolutionary leap in NLP models. Its ability to capture the effects of negation at various tree levels for both positive and negative phrases showcases the progression in attention-based techniques. This progression can be likened to a branching in cladistics, where the RNTN represents a novel branch diverging from traditional models, emphasizing nuanced, attention-driven parsing.However, as with any evolutionary study, the attentional cladistics approach reminds us to not only focus on the latest offshoots but also to recognize and understand the lineage from which they arise. The trend, as highlighted in previous discussions on "Forgotten Knowledge," suggests a strong recency bias in citations. Yet, tracing back to foundational works like that of Socher et al. is crucial to understand the evolutionary trajectory of attention mechanisms in NLP.In conclusion, while attentional cladistics offers a lens to examine the evolutionary pathways of cognitive models, it is essential to maintain a holistic view, acknowledging both pioneering works and the latest advancements. This ensures a comprehensive understanding of the development and divergence of attentional mechanisms in the ever-evolving field of NLP.I hope this revision aligns better with the theme of your essay on "Attentional Cladistics". Let me know if you need any further adjustments.1 / 1Certainly! Here's a condensed summary:

We embarked on an exploration of **Attentional Cladistics**, a theoretical framework examining the evolutionary progression of cognitive attention patterns. Drawing inspiration from biological cladistics, this framework seeks to understand the divergence of attention strategies over time.

Key components include:
- **Ancestral State**: A foundational attention pattern from which others derive.
- **Aperture & Intensity**: Scope and depth of attention respectively.
- **Value System**: The priority given to specific stimuli.
  
We touched upon **Visual Foraging**, the process of visually seeking out specific stimuli, akin to animals foraging for food. This concept parallels how attention might have evolved, adapting to efficiently locate and process relevant information.

Adding depth, we discussed Arnie Cox's idea of **Mimetic Reproduction** â appreciating music through physical imitation, like tapping or humming. This serves as an example of how deeply rooted and embodied our cognitive responses can be.

The journey of understanding attention's evolution is intricate, drawing from cognitive science, evolutionary biology, and personal introspection. While still a budding concept, Attentional Cladistics offers a potential pathway to bridge these disciplines, providing insights into human cognition and its development over time.1 / 1ChatGPTUsing the condensed summary as a foundation, I will incorporate the work by Socher et al. on sentiment analysis, drawing connections between the intricate nature of attention mechanisms and advancements in NLP:We embarked on an exploration of Attentional Cladistics, a theoretical framework examining the evolutionary progression of cognitive attention patterns, inspired by biological cladistics. This framework seeks to trace the divergence and lineage of attention strategies over time.Key components that map the territory include:Ancestral State: The foundational attention pattern, which serves as the base for all subsequent evolutions.Aperture & Intensity: Parameters defining the scope and depth of our attention.Value System: The innate or learned priority we assign to specific stimuli.To offer a tangible representation, we delved into Visual Foraging, where one's attention shifts in seeking out specific visual stimuli, paralleling animals' behavior in the wild. This process provides a visual analogy for understanding how attentional mechanisms have evolved to efficiently locate and process relevant information in diverse environments.In the realm of Natural Language Processing (NLP), there's an evident parallel. The work by R. Socher and his team on "Recursive Deep Models for Semantic Compositionality Over a Sentiment Treebank" exemplifies the complex interplay of attention mechanisms in computational models. Their Sentiment Treebank captures fine-grained sentiments of phrases, directing our attention to the nuanced emotions conveyed in language. Just as in visual foraging where the eye seeks specific cues, their model hones in on particular linguistic elements, efficiently decoding sentiment compositionality.Adding another layer, we contemplated Arnie Cox's notion of Mimetic Reproduction, where our enjoyment of music emerges from physical imitation, such as tapping or humming. This idea drives home the point of how embodied and primal our cognitive responses are, stretching across various domains from music to language processing.In conclusion, the evolution of attention, be it in the natural or computational domain, is multifaceted. Drawing from cognitive science, evolutionary biology, and advancements in NLP, Attentional Cladistics promises a comprehensive lens to understand human cognition's intricate dance and its transformation over epochs.I hope this integration fits seamlessly into the context of your essay. Please let me know if you require further adjustments or refinements.1 / 1Its relations to the main theme of this conversation: Neural Basis of Future Prediction1 / 1ChatGPTAh, I see. Given that the main theme is the "Neural Basis of Future Prediction," let's pivot the discussion to focus on how attentional strategies (as observed through Attentional Cladistics) and advancements in NLP (like sentiment analysis) may shed light on our neural predisposition to predict the future. Here's an adaptation:Central to our exploration is the theme of the Neural Basis of Future Prediction. At the heart of this lies the evolutionary progression of cognitive attention patterns, which we have come to term Attentional Cladistics. Drawing parallels with biological cladistics, this theoretical framework attempts to decode how our brains have evolved attention strategies over time, enhancing our capacity for predicting future events.Understanding the neural underpinnings:Ancestral State: The primal neural patterns serve as a roadmap to our attention's evolution, indicating how early humans might have made predictions based on immediate stimuli.Aperture & Intensity: These define our neural bandwidth and how deeply we process information, dictating our ability to predict intricate future scenarios.Value System: Our neural circuitry has always been tuned to prioritize specific stimuli, ensuring survival and now, in modern contexts, helping us predict socio-emotional responses.One of the paradigms that closely mirrors our innate predictive mechanisms is Visual Foraging. Just as our ancestors might have predicted the presence of prey or threats based on visual cues, we, in today's digital age, make predictions based on visual information.Delving into the domain of Natural Language Processing, R. Socher and colleagues' work on "Recursive Deep Models for Semantic Compositionality Over a Sentiment Treebank" resonates deeply with our theme. Their computational model, through sentiment analysis, anticipates human emotional reactions based on linguistic cues. Such advancements in NLP could be seen as an emulation of our brain's predictive capabilities, forecasting emotions and reactions based on textual data.Furthermore, the Mimetic Reproduction concept introduced by Arnie Cox provides an avenue to understand how our neural structures might have evolved to predict and replicate auditory patterns, creating a sense of familiarity and prediction when it comes to music.In wrapping up, it's imperative to understand that the human brain's intricate dance with attention mechanisms, rooted deeply in our evolutionary past, plays a pivotal role in our unceasing endeavor to predict the future. Both naturally evolved and artificial systems provide a tapestry of insights into how prediction is embedded in our neural fabric.This repositioning should align better with the theme of "Neural Basis of Future Prediction". Let me know if further changes are necessary!1 / 1It was good but could I get a simplified version?1 / 1ChatGPTCertainly! Here's a more concise adaptation:Neural Basis of Future Prediction is deeply intertwined with the evolution of our attention patterns, termed Attentional Cladistics. By tracing how our attention evolved, we gain insights into our innate ability to anticipate future events.Key Points:Our primal Ancestral State offers clues about how early humans predicted based on stimuli.Aperture & Intensity reflect our capacity to foresee intricate future scenarios by determining our focus and depth of information processing.The Value System highlights how we're wired to prioritize certain stimuli, aiding prediction.In today's digital world, Visual Foraging mirrors our predictive mechanisms: using visual cues to anticipate outcomes, much like our ancestors did for survival.R. Socher's research on sentiment analysis, predicting emotional responses from text, hints at how modern technology emulates our brain's foresight. Additionally, Mimetic Reproduction showcases our neural ability to predict and mimic familiar auditory patterns.In essence, both our evolutionary attention patterns and advancements in NLP underscore the human brain's intrinsic drive to predict the future.This version is streamlined while still touching upon the main points relative to the theme.1 / 1Ok so give a thorough outline of what a future book on the subject might cover.1 / 1ChatGPTCertainly! Here's a comprehensive outline for a book on the Neural Basis of Future Prediction and its intertwining with evolutionary attention patterns:I. IntroductionA. Brief on Neural Prediction and its Evolutionary ImportanceB. Aim and Scope of the BookII. Foundations of Neural PredictionA. Basics of Neural ProcessesB. The Brainâs Predictive NatureC. Evolutionary Significance of PredictionIII. Attentional Cladistics: Tracing Evolutionary Attention PatternsA. Introduction to Cladistics in BiologyB. Adapting Cladistics for Cognitive EvolutionC. Ancestral State of Attention & Predictive CapacityIV. Mechanisms of Neural PredictionA. Neural Plasticity and LearningB. Aperture & Intensity: Decoding Depth and FocusC. The Role of the Value System in Prioritizing StimuliV. Real-world Applications of Neural PredictionA. Visual Foraging in the Digital AgeB. Mimetic Reproduction: Predicting and Replicating Auditory PatternsC. NLP & Sentiment Analysis: Modern Tech Emulating Brain's Predictive AbilitiesVI. Future Prediction in Various SpeciesA. Comparative Study of Predictive Abilities across SpeciesB. Evolutionary Advantages and Trade-offsC. Lessons from Nature: Predictions in EcosystemsVII. Case Studies: Attentional Cladistics in PracticeA. Early Human Societies: Predicting Weather and Animal MovementsB. Modern Society: Predicting Market Trends based on Visual CuesC. Technology: Predictive Text and Algorithms Mimicking Neural ProcessesVIII. The Ethics of Neural PredictionA. Benefits of Enhanced Predictive AbilitiesB. The Dangers of Over-reliance on Predictive TechC. Balancing Human Intuition with Algorithmic PredictionsIX. Future DirectionsA. Advancements in Brain-Computer Interfaces for PredictionB. Potential of Enhancing Natural Predictive AbilitiesC. Ethical Considerations for Enhanced PredictionsX. ConclusionA. The Journey of Understanding the Neural Basis of PredictionB. Potential Implications for Society and Future GenerationsAppendicesA. Glossary of TermsB. Recommended Further ReadingsC. ReferencesThis outline provides a multi-faceted exploration of the topic, integrating evolutionary perspectives, real-world applications, and ethical considerations for a comprehensive understanding.1 / 1End the essay on Attentional Cladistics.1 / 1ChatGPTConcluding Thoughts on Attentional CladisticsIn the journey through the neural landscapes of prediction, the framework of Attentional Cladistics offers profound insights. Rooted in the essence of evolutionary biology, this paradigm paints a vivid picture of how attention mechanisms might have branched and evolved, adapting to the demands of an ever-changing environment. As our species faced novel challenges, our brains developed intricate patterns of attention, each serving a unique predictive function.This evolution of attention isn't merely a tale of the past but is deeply woven into our contemporary life. From the rapid information-seeking behavior on digital platforms to the nuanced emotional predictions in interpersonal relationships, echoes of our ancestral predictive tendencies resonate.However, it's crucial to acknowledge the fluidity of this framework. As science advances, our understanding of neural prediction and its evolutionary context will refine. Attentional Cladistics stands not as an end but as a pivotal marker in the continuous exploration of the intricate dance between prediction and attention.In the future, as technology increasingly integrates with cognition, our attention patterns will continue to evolve, shaped by both biological heritage and artificial influences. It beckons us to be attentive observers, not just of the external world but of our inner cognitive tapestries, understanding where we come from and, more importantly, predicting where we might be heading.1 / 1Ok a list of its main theses1 / 1ChatGPTMain Theses of Attentional Cladistics:Evolutionary Framework: Attentional strategies and patterns have evolved over time, much like species in biological cladistics. These patterns have diverged based on different environmental and cognitive demands.Ancestral State of Attention: There exists a foundational attention pattern, a base state from which various attention strategies have evolved.Aperture & Intensity Dichotomy: Attention operates on two primary dimensions: the breadth (aperture) and depth (intensity). Different tasks and environments demand variations along these axes.Value Systems and Attention: Our attentional mechanisms prioritize stimuli based on an internal value system, which itself has evolved to maximize survival and adaptability.Visual Foraging Parallel: Modern information-seeking behaviors can be likened to ancestral foraging tactics, hinting at deep-rooted evolutionary origins.Embodied Cognition: Physical reactions, such as mimetic reproductions in response to stimuli like music, highlight the intricate connection between cognition and physical embodiment.Neural Predictive Foundations: At its core, attention serves as a predictive mechanism, guiding individuals toward relevant stimuli and away from potential threats.Attention in the Digital Age: Modern technology and the digital information age have influenced and altered human attention patterns, creating both challenges and opportunities for understanding the evolution of attention.Interdisciplinary Insights: The study of attentional cladistics benefits from integrating knowledge across disciplines, from neuroscience and evolutionary biology to psychology and anthropology.Future of Attention: As technological advancements continue to shape human experience, attentional patterns will evolve, demanding a continuous re-evaluation and refinement of the Attentional Cladistics framework.1 / 1A summary of the whole conversation.1 / 1ChatGPTCertainly! Here's a summary:The conversation began with a focus on Neural Basis of Future Prediction. The topic of Attentional Cladistics was introduced, presenting a theoretical framework that examines the evolutionary progression of cognitive attention patterns. The discussion expanded to explore the concepts of Visual Foraging and Mimetic Reproduction. The conversation then touched on a related paper by R. Socher et al. on sentiment analysis, illustrating the application of machine learning in related cognitive domains. Subsequently, the user requested a thorough outline for a potential book on the topic and sought to conclude an essay on Attentional Cladistics. The conversation ended with a list of the main theses of Attentional Cladistics, summarizing its core principles and contributions.1 / 1Give some of the mathematics prerequisites1 / 1ChatGPTFor understanding concepts like the Neural Basis of Future Prediction and Attentional Cladistics, especially if one were to dive into the computational or neuroscientific modeling aspects, several areas of mathematics are crucial. Here are some of the essential mathematical prerequisites:Calculus: Fundamental for understanding change and rates of change. Essential for understanding neural computations and many machine learning algorithms.Linear Algebra: Critical for understanding vectors, matrices, and their operations. Neural networks, at their core, involve a lot of matrix operations.Probability and Statistics: Essential for understanding uncertainty, distributions, and statistical testing. Concepts like Bayesian inference are particularly relevant in neuroscience and cognitive modeling.Differential Equations: Particularly important if delving into the detailed dynamics of neural systems.Information Theory: To understand concepts like entropy, which can be essential for topics like attention and prediction.Graph Theory: Relevant for understanding network structures, which are foundational in both neural networks in AI and neuronal networks in the brain.Optimization: Techniques like gradient descent are foundational in training neural networks.Complex Analysis: Useful when dealing with certain types of neural models, especially if one gets into the realm of oscillatory neural dynamics or neural synchronization.It's worth noting that the depth to which one needs to understand these topics depends on how deeply they plan to dive into the technical and modeling aspects. For a more conceptual understanding, a basic grasp of these topics might suffice. However, for hands-on research or development, a more thorough understanding would be beneficial.1 / 1And what kind of predictable hypotheses might attentional cladistics make about each of these aspects?1 / 1ChatGPTAttentional Cladistics, as a theoretical framework for the evolution of cognitive attention patterns, might have several implications or hypotheses related to these mathematical areas:Calculus:Hypothesis: As cognitive attention patterns evolved, the rate of change or adaptation to new stimuli might have varied. By understanding these rates, we can discern periods of rapid cognitive evolution or stagnation.Linear Algebra:Hypothesis: Vector spaces can represent different attentional states, and the transformations between these might represent shifts in attention or the evolution of attentional strategies over time.Probability and Statistics:Hypothesis: The probability distributions of attentional focuses might have changed over evolutionary time, reflecting adaptive strategies to different environments. Modern humans, for instance, might have a broader attention distribution due to the information-rich environments we inhabit.Differential Equations:Hypothesis: The dynamics of attentional shifts could be modeled using differential equations, representing how attention changes in response to both internal and external factors over time.Information Theory:Hypothesis: As cognitive systems evolved, the efficiency of information processing (entropy, mutual information) in attentional systems might have improved, optimizing the system for survival-relevant information.Graph Theory:Hypothesis: The connectivity and network structures of attention-related neural circuits could have grown more complex over evolutionary time, leading to richer and more diverse attentional patterns.Optimization:Hypothesis: Attentional systems have been optimized over evolutionary time to balance the trade-off between explorative (broad) and exploitative (focused) attention.Complex Analysis:Hypothesis: As attentional systems grew more sophisticated, they might have developed more intricate oscillatory dynamics or synchronization patterns, which can be described using complex analysis.It's worth noting that these hypotheses are speculative and serve as examples of how attentional cladistics might intersect with various mathematical disciplines. Actual predictions would require more detailed modeling and empirical validation.1 / 1It is a good start. Any final considerations?1 / 1ChatGPTCertainly! If one were to delve deeper into Attentional Cladistics, a few final considerations to bear in mind would be:Interdisciplinary Collaboration: Combining expertise from evolutionary biology, cognitive science, and mathematics can lead to more robust and holistic models.Empirical Validation: Any theoretical model or hypothesis drawn from Attentional Cladistics needs empirical evidence, perhaps from neuroimaging or behavioral studies, to validate its claims.Ethical Implications: If one understands the evolutionary trajectory of attention, it might be possible to manipulate or enhance it. Such capabilities bring forth ethical dilemmas regarding cognitive enhancement and the potential creation of attentional disparities in society.Technological Applications: Insights from Attentional Cladistics could be applied to improve AI attention mechanisms, making them more human-like or efficient based on evolutionary principles.Historical Context: It's essential to understand historical and environmental contexts when considering the evolution of attention. Factors like societal structure, technological advancements, and ecological changes play a role in how attentional patterns evolve.Future Predictions: As the world becomes increasingly digital and information-dense, predicting how attention will evolve in response to these changes could be a crucial application of Attentional Cladistics.By weaving together these considerations, one can approach Attentional Cladistics with a comprehensive perspective, ensuring that the exploration is both rigorous and ethically grounded.1 / 1That was a little overly general. Connections between the topics.1 / 1ChatGPTOf course, let's draw more direct connections between the topics:Interdisciplinary Collaboration & Empirical Validation: Combining expertise from various fields allows for the creation of rigorous experimental designs. For instance, knowledge from evolutionary biology could inform cognitive science researchers about potential ancestral attention patterns. Subsequently, these patterns can be empirically tested using neuroimaging techniques.Ethical Implications & Technological Applications: Insights from Attentional Cladistics might pave the way for advanced attention-modifying technologies. While such advancements could revolutionize fields like education or mental health, they also pose ethical challenges. For instance, if only a segment of the population has access to attention-enhancing tools, it could lead to cognitive inequalities.Historical Context & Future Predictions: By understanding how societal structures or technological advancements of the past influenced attention, we can make informed predictions about the future. For instance, if the rise of print media in the past led to prolonged focus (deep reading), what might the era of bite-sized digital information mean for future attention spans? Attentional Cladistics could offer predictions on these trajectories.Empirical Validation & Technological Applications: Findings from behavioral or neuroimaging studies on attention could be used to improve AI attention mechanisms. If certain evolutionary attention patterns are deemed more efficient or adaptive, they could be modeled in machine learning algorithms.These connections emphasize the intertwined nature of the topics and how a holistic approach to Attentional Cladistics necessitates understanding and leveraging the relationships between them.1 / 1Ok any other ideas that we mentioned that were unusual?1 / 1ChatGPTCertainly! Here are some of the more unusual ideas that emerged from our discussion on Attentional Cladistics:Visual Foraging: Drawing parallels between the way animals forage for food and how humans visually seek out specific stimuli. It's a fresh perspective that ties evolutionary survival mechanisms to cognitive processes.Mimetic Reproduction: Arnie Cox's idea that we understand and appreciate music through physical imitation, like tapping or humming. This suggests that some cognitive processes are deeply embodied and rooted in physical actions, rather than being purely mental.Neural Basis of Future Prediction: The concept of the brain not only processing the present but constantly making predictions about the future. This intertwines with attention as our predictions can determine where our attention gets directed.Recursive Deep Models for Sentiment Analysis: The inclusion of the work on sentiment treebanks and recursive neural tensor networks as a way to understand semantic compositionality can be seen as an analogy for how complex cognitive processes (like attention) can be broken down, analyzed, and modeled.Attentional "Fossils": Much like paleontologists study fossils to understand the past, we might look for "cognitive fossils" or remnants of past attention patterns in modern behavior. This could be in the form of cultural artifacts, rituals, or even persistent cognitive biases.These ideas, while unusual, can provide fresh perspectives and novel angles of approach to the study of attention and its evolutionary trajectory.1 / 1Thank you. One last summary.1 / 1ChatGPTOf course! Here's a concise summary of our discussion:We delved into the concept of Attentional Cladistics, a theoretical approach to understanding the evolutionary progression of cognitive attention patterns. Drawing parallels with biological cladistics, the aim is to trace the divergence of attention strategies over time. Key ideas explored included:Ancestral State of attention.Visual Foraging, equating visual stimulus seeking to animals foraging for food.Mimetic Reproduction, Arnie Cox's theory of understanding music via physical imitation.Connection to the Neural Basis of Future Prediction, suggesting our brains constantly forecast the future, impacting attention.Introduction of Recursive Deep Models to understand semantic compositionality, hinting at how we can analyze complex cognitive processes.Our discussion spanned from evolutionary roots to modern neural network models, offering fresh insights into the intricate nature of attention and its historical trajectory.1 / 1ChatGPT may produce inaccurate information about people, places, or facts. ChatGPT August 3 Version?Neural Basis of Future Prediction
------MultipartBoundary--vS53nJOjydVm2jf5zsp0U2JCPSCh5LRhvu8f55va6M----
Content-Type: text/css
Content-Transfer-Encoding: binary
Content-Location: cid:css-c5f9adbf-e41b-47b4-92cb-95ac5829107e@mhtml.blink

@charset "utf-8";

.react-scroll-to-bottom--css-zsjfu-79elbk { position: relative; }

.react-scroll-to-bottom--css-zsjfu-1n7m0yu { height: 100%; overflow-y: auto; width: 100%; }

.react-scroll-to-bottom--css-zsjfu-1tj0vk3 { background-color: rgba(0, 0, 0, 0.2); border-radius: 10px; border-width: 0px; bottom: 5px; cursor: pointer; height: 20px; outline: 0px; position: absolute; right: 20px; width: 20px; }

.react-scroll-to-bottom--css-zsjfu-1tj0vk3:hover { background-color: rgba(0, 0, 0, 0.4); }

.react-scroll-to-bottom--css-zsjfu-1tj0vk3:active { background-color: rgba(0, 0, 0, 0.6); }
------MultipartBoundary--vS53nJOjydVm2jf5zsp0U2JCPSCh5LRhvu8f55va6M----
Content-Type: text/css
Content-Transfer-Encoding: binary
Content-Location: cid:css-d3dc9906-1af9-4ecb-9e6d-d00b4f091d98@mhtml.blink

@charset "utf-8";

.r34K7X1zGgAi6DllVF3T { box-sizing: border-box; border: 0px; margin: 0px; padding: 0px; overflow: hidden; display: none; z-index: 2147483647; pointer-events: none; visibility: hidden; opacity: 0; transition: opacity 300ms linear 0s; height: 0px; width: 0px; }

.r34K7X1zGgAi6DllVF3T.active { display: block; visibility: visible; }

.r34K7X1zGgAi6DllVF3T.active.show { opacity: 1; pointer-events: inherit; position: inherit; }

.r34K7X1zGgAi6DllVF3T.active.show.in-situ { width: inherit; height: inherit; }

.r34K7X1zGgAi6DllVF3T.active.show.lightbox { position: fixed; inset: 0px; width: 100% !important; height: 100% !important; }
------MultipartBoundary--vS53nJOjydVm2jf5zsp0U2JCPSCh5LRhvu8f55va6M----
Content-Type: text/css
Content-Transfer-Encoding: binary
Content-Location: cid:css-b0383707-2d35-4453-8ce5-b17caff22506@mhtml.blink

@charset "utf-8";

@keyframes intercom-lightweight-app-launcher { 
  0% { opacity: 0; transform: scale(0.5); }
  100% { opacity: 1; transform: scale(1); }
}

@keyframes intercom-lightweight-app-gradient { 
  0% { opacity: 0; }
  100% { opacity: 1; }
}

@keyframes intercom-lightweight-app-messenger { 
  0% { opacity: 0; transform: scale(0); }
  40% { opacity: 1; }
  100% { transform: scale(1); }
}

.intercom-lightweight-app { position: fixed; z-index: 2147483001; width: 0px; height: 0px; font-family: intercom-font, "Helvetica Neue", "Apple Color Emoji", Helvetica, Arial, sans-serif; }

.intercom-lightweight-app-gradient { position: fixed; z-index: 2147483002; width: 500px; height: 500px; bottom: 0px; right: 0px; pointer-events: none; background: radial-gradient(at right bottom, rgba(29, 39, 54, 0.16) 0%, rgba(29, 39, 54, 0) 72%); animation: 200ms ease-out 0s 1 normal none running intercom-lightweight-app-gradient; }

.intercom-lightweight-app-launcher { position: fixed; z-index: 2147483003; border: none; bottom: 20px; right: 20px; max-width: 48px; width: 48px; max-height: 48px; height: 48px; border-radius: 50%; background: rgb(32, 33, 35); cursor: pointer; box-shadow: rgba(0, 0, 0, 0.06) 0px 1px 6px 0px, rgba(0, 0, 0, 0.16) 0px 2px 32px 0px; transition: transform 167ms cubic-bezier(0.33, 0, 0, 1) 0s; box-sizing: content-box; padding: 0px !important; margin: 0px !important; }

.intercom-lightweight-app-launcher:hover { transition: transform 250ms cubic-bezier(0.33, 0, 0, 1) 0s; transform: scale(1.1); }

.intercom-lightweight-app-launcher:active { transform: scale(0.85); transition: transform 134ms cubic-bezier(0.45, 0, 0.2, 1) 0s; }

.intercom-lightweight-app-launcher:focus { outline: none; }

.intercom-lightweight-app-launcher-icon { display: flex; align-items: center; justify-content: center; position: absolute; top: 0px; left: 0px; width: 48px; height: 48px; transition: transform 100ms linear 0s, opacity 80ms linear 0s; }

.intercom-lightweight-app-launcher-icon-open { opacity: 1; transform: rotate(0deg) scale(1); }

.intercom-lightweight-app-launcher-icon-open svg { width: 24px; height: 24px; }

.intercom-lightweight-app-launcher-icon-open svg path { fill: rgb(255, 255, 255); }

.intercom-lightweight-app-launcher-icon-self-serve { opacity: 1; transform: rotate(0deg) scale(1); }

.intercom-lightweight-app-launcher-icon-self-serve svg { height: 44px; }

.intercom-lightweight-app-launcher-icon-self-serve svg path { fill: rgb(255, 255, 255); }

.intercom-lightweight-app-launcher-custom-icon-open { max-height: 24px; max-width: 24px; opacity: 1; transform: rotate(0deg) scale(1); }

.intercom-lightweight-app-launcher-icon-minimize { opacity: 0; transform: rotate(-60deg) scale(0); }

.intercom-lightweight-app-launcher-icon-minimize svg path { fill: rgb(255, 255, 255); }

.intercom-lightweight-app-messenger { position: fixed; z-index: 2147483003; overflow: hidden; background-color: white; animation: 250ms cubic-bezier(0, 1, 1, 1) 0s 1 normal none running intercom-lightweight-app-messenger; transform-origin: right bottom; inset: 0px; border-radius: 16px; }

.intercom-lightweight-app-messenger-header { height: 64px; border-bottom: none; background: rgb(32, 33, 35); }

.intercom-lightweight-app-messenger-footer { position: absolute; bottom: 0px; width: 100%; height: 80px; background: rgb(255, 255, 255); font-size: 14px; line-height: 21px; border-top: 1px solid rgba(0, 0, 0, 0.05); box-shadow: rgba(0, 0, 0, 0.05) 0px 0px 25px; }

@media print {
  .intercom-lightweight-app { display: none; }
}
------MultipartBoundary--vS53nJOjydVm2jf5zsp0U2JCPSCh5LRhvu8f55va6M----
Content-Type: text/css
Content-Transfer-Encoding: binary
Content-Location: https://cdn.oaistatic.com/_next/static/css/27dc37929106858f.css

@charset "utf-8";

@font-face { font-display: swap; font-family: Signifier; font-style: normal; font-weight: 400; src: url("https://cdn.openai.com/common/fonts/signifier/signifier-light.woff2") format("woff2"); }

@font-face { font-display: swap; font-family: Signifier; font-style: italic; font-weight: 400; src: url("https://cdn.openai.com/common/fonts/signifier/signifier-light-italic.woff2") format("woff2"); }

@font-face { font-display: swap; font-family: Signifier; font-style: normal; font-weight: 700; src: url("https://cdn.openai.com/common/fonts/signifier/signifier-bold.woff2") format("woff2"); }

@font-face { font-display: swap; font-family: Signifier; font-style: italic; font-weight: 700; src: url("https://cdn.openai.com/common/fonts/signifier/signifier-bold-italic.woff2") format("woff2"); }

@font-face { font-display: swap; font-family: SÃ¶hne; font-style: normal; font-weight: 400; src: url("https://cdn.openai.com/common/fonts/soehne/soehne-buch.woff2") format("woff2"); }

@font-face { font-display: swap; font-family: SÃ¶hne; font-style: italic; font-weight: 400; src: url("https://cdn.openai.com/common/fonts/soehne/soehne-buch-kursiv.woff2") format("woff2"); }

@font-face { font-display: swap; font-family: SÃ¶hne; font-style: normal; font-weight: 500; src: url("https://cdn.openai.com/common/fonts/soehne/soehne-kraftig.woff2") format("woff2"); }

@font-face { font-display: swap; font-family: SÃ¶hne; font-style: italic; font-weight: 500; src: url("https://cdn.openai.com/common/fonts/soehne/soehne-kraftig-kursiv.woff2") format("woff2"); }

@font-face { font-display: swap; font-family: SÃ¶hne; font-style: normal; font-weight: 600; src: url("https://cdn.openai.com/common/fonts/soehne/soehne-halbfett.woff2") format("woff2"); }

@font-face { font-display: swap; font-family: SÃ¶hne; font-style: italic; font-weight: 600; src: url("https://cdn.openai.com/common/fonts/soehne/soehne-halbfett-kursiv.woff2") format("woff2"); }

@font-face { font-display: swap; font-family: "SÃ¶hne Mono"; font-style: normal; font-weight: 400; src: url("https://cdn.openai.com/common/fonts/soehne/soehne-mono-buch.woff2") format("woff2"); }

@font-face { font-display: swap; font-family: "SÃ¶hne Mono"; font-style: normal; font-weight: 700; src: url("https://cdn.openai.com/common/fonts/soehne/soehne-mono-halbfett.woff2") format("woff2"); }

@font-face { font-display: swap; font-family: "SÃ¶hne Mono"; font-style: italic; font-weight: 400; src: url("https://cdn.openai.com/common/fonts/soehne/soehne-mono-buch-kursiv.woff2") format("woff2"); }

@font-face { font-display: swap; font-family: "SÃ¶hne Circle"; font-style: normal; font-weight: 400; src: url("https://cdn.openai.com/common/fonts/soehne-circle/soehne-circle-buch.woff2") format("woff2"); }

@font-face { font-display: swap; font-family: "SÃ¶hne Circle"; font-style: normal; font-weight: 600; src: url("https://cdn.openai.com/common/fonts/soehne-circle/soehne-circle-halbfett.woff2") format("woff2"); }

*, ::after, ::before { border: 0px solid rgb(217, 217, 227); box-sizing: border-box; }

::after, ::before { --tw-content:""; }

html { text-size-adjust: 100%; font-feature-settings: normal; font-family: SÃ¶hne, ui-sans-serif, system-ui, -apple-system, "Segoe UI", Roboto, Ubuntu, Cantarell, "Noto Sans", sans-serif, "Helvetica Neue", Arial, "Apple Color Emoji", "Segoe UI Emoji", "Segoe UI Symbol", "Noto Color Emoji"; font-variation-settings: normal; line-height: 1.5; tab-size: 4; }

body { line-height: inherit; margin: 0px; }

hr { border-top-width: 1px; color: inherit; height: 0px; }

abbr:where([title]) { text-decoration: underline dotted currentcolor; }

h1, h2, h3, h4, h5, h6 { font-size: inherit; font-weight: inherit; }

a { color: inherit; text-decoration: inherit; }

b, strong { font-weight: bolder; }

code, kbd, pre, samp { font-family: "SÃ¶hne Mono", Monaco, "Andale Mono", "Ubuntu Mono", monospace; font-size: 1em; }

small { font-size: 80%; }

sub, sup { font-size: 75%; line-height: 0; position: relative; vertical-align: baseline; }

sub { bottom: -0.25em; }

sup { top: -0.5em; }

table { border-collapse: collapse; border-color: inherit; text-indent: 0px; }

button, input, optgroup, select, textarea { color: inherit; font-family: inherit; font-size: 100%; font-weight: inherit; line-height: inherit; margin: 0px; padding: 0px; }

button, select { text-transform: none; }

[type="button"], [type="reset"], [type="submit"], button { appearance: button; background-color: transparent; background-image: none; }

progress { vertical-align: baseline; }

::-webkit-inner-spin-button, ::-webkit-outer-spin-button { height: auto; }

[type="search"] { appearance: textfield; outline-offset: -2px; }

::-webkit-search-decoration { appearance: none; }

::-webkit-file-upload-button { appearance: button; font: inherit; }

summary { display: list-item; }

blockquote, dd, dl, fieldset, figure, h1, h2, h3, h4, h5, h6, hr, p, pre { margin: 0px; }

fieldset, legend { padding: 0px; }

menu, ol, ul { list-style: none; margin: 0px; padding: 0px; }

textarea { resize: vertical; }

input::-webkit-input-placeholder, textarea::-webkit-input-placeholder { color: rgb(172, 172, 190); }

input::placeholder, textarea::placeholder { color: rgb(172, 172, 190); }

[role="button"], button { cursor: pointer; }

:disabled { cursor: default; }

audio, canvas, embed, iframe, img, object, svg, video { display: block; vertical-align: middle; }

img, video { height: auto; max-width: 100%; }

[hidden] { display: none; }

[multiple], [type="date"], [type="datetime-local"], [type="email"], [type="month"], [type="number"], [type="password"], [type="search"], [type="tel"], [type="text"], [type="time"], [type="url"], [type="week"], select, textarea { --tw-shadow:0 0 transparent; appearance: none; background-color: rgb(255, 255, 255); border-color: rgb(142, 142, 160); border-radius: 0px; border-width: 1px; font-size: 1rem; line-height: 1.5rem; padding: 0.5rem 0.75rem; }

[multiple]:focus, [type="date"]:focus, [type="datetime-local"]:focus, [type="email"]:focus, [type="month"]:focus, [type="number"]:focus, [type="password"]:focus, [type="search"]:focus, [type="tel"]:focus, [type="text"]:focus, [type="time"]:focus, [type="url"]:focus, [type="week"]:focus, select:focus, textarea:focus { --tw-ring-inset:var(--tw-empty, ); --tw-ring-offset-width:0px; --tw-ring-offset-color:#fff; --tw-ring-color:#2563eb; --tw-ring-offset-shadow:var(--tw-ring-inset) 0 0 0 var(--tw-ring-offset-width) var(--tw-ring-offset-color); --tw-ring-shadow:var(--tw-ring-inset) 0 0 0 calc(1px + var(--tw-ring-offset-width)) var(--tw-ring-color); border-color: rgb(37, 99, 235); box-shadow: var(--tw-ring-offset-shadow),var(--tw-ring-shadow),var(--tw-shadow); outline: transparent solid 2px; outline-offset: 2px; }

input::-webkit-input-placeholder, textarea::-webkit-input-placeholder { color: rgb(142, 142, 160); opacity: 1; }

input::placeholder, textarea::placeholder { color: rgb(142, 142, 160); opacity: 1; }

::-webkit-datetime-edit-fields-wrapper { padding: 0px; }

::-webkit-date-and-time-value { min-height: 1.5em; }

::-webkit-datetime-edit, ::-webkit-datetime-edit-day-field, ::-webkit-datetime-edit-hour-field, ::-webkit-datetime-edit-meridiem-field, ::-webkit-datetime-edit-millisecond-field, ::-webkit-datetime-edit-minute-field, ::-webkit-datetime-edit-month-field, ::-webkit-datetime-edit-second-field, ::-webkit-datetime-edit-year-field { padding-bottom: 0px; padding-top: 0px; }

select { background-image: url("data:image/svg+xml;charset=utf-8,%3Csvg xmlns='http://www.w3.org/2000/svg' fill='none' viewBox='0 0 20 20'%3E%3Cpath stroke='%238e8ea0' stroke-linecap='round' stroke-linejoin='round' stroke-width='1.5' d='m6 8 4 4 4-4'/%3E%3C/svg%3E"); background-position: right 0.5rem center; background-repeat: no-repeat; background-size: 1.5em 1.5em; padding-right: 2.5rem; -webkit-print-color-adjust: exact; }

[multiple] { background-image: none; background-position: 0px 0px; background-repeat: repeat; background-size: initial; padding-right: 0.75rem; -webkit-print-color-adjust: inherit; }

[type="checkbox"], [type="radio"] { --tw-shadow:0 0 transparent; appearance: none; background-color: rgb(255, 255, 255); background-origin: border-box; border-color: rgb(142, 142, 160); border-width: 1px; color: rgb(37, 99, 235); display: inline-block; flex-shrink: 0; height: 1rem; padding: 0px; -webkit-print-color-adjust: exact; user-select: none; vertical-align: middle; width: 1rem; }

[type="checkbox"] { border-radius: 0px; }

[type="radio"] { border-radius: 100%; }

[type="checkbox"]:focus, [type="radio"]:focus { --tw-ring-inset:var(--tw-empty, ); --tw-ring-offset-width:2px; --tw-ring-offset-color:#fff; --tw-ring-color:#2563eb; --tw-ring-offset-shadow:var(--tw-ring-inset) 0 0 0 var(--tw-ring-offset-width) var(--tw-ring-offset-color); --tw-ring-shadow:var(--tw-ring-inset) 0 0 0 calc(2px + var(--tw-ring-offset-width)) var(--tw-ring-color); box-shadow: var(--tw-ring-offset-shadow),var(--tw-ring-shadow),var(--tw-shadow); outline: transparent solid 2px; outline-offset: 2px; }

[type="checkbox"]:checked, [type="radio"]:checked { background-color: currentcolor; background-position: 50% center; background-repeat: no-repeat; background-size: 100% 100%; border-color: transparent; }

[type="checkbox"]:checked { background-image: url("data:image/svg+xml;charset=utf-8,%3Csvg viewBox='0 0 16 16' fill='%23fff' xmlns='http://www.w3.org/2000/svg'%3E%3Cpath d='M12.207 4.793a1 1 0 0 1 0 1.414l-5 5a1 1 0 0 1-1.414 0l-2-2a1 1 0 0 1 1.414-1.414L6.5 9.086l4.293-4.293a1 1 0 0 1 1.414 0z'/%3E%3C/svg%3E"); }

[type="radio"]:checked { background-image: url("data:image/svg+xml;charset=utf-8,%3Csvg viewBox='0 0 16 16' fill='%23fff' xmlns='http://www.w3.org/2000/svg'%3E%3Ccircle cx='8' cy='8' r='3'/%3E%3C/svg%3E"); }

[type="checkbox"]:checked:focus, [type="checkbox"]:checked:hover, [type="radio"]:checked:focus, [type="radio"]:checked:hover { background-color: currentcolor; border-color: transparent; }

[type="checkbox"]:indeterminate { background-color: currentcolor; background-image: url("data:image/svg+xml;charset=utf-8,%3Csvg xmlns='http://www.w3.org/2000/svg' fill='none' viewBox='0 0 16 16'%3E%3Cpath stroke='%23fff' stroke-linecap='round' stroke-linejoin='round' stroke-width='2' d='M4 8h8'/%3E%3C/svg%3E"); background-position: 50% center; background-repeat: no-repeat; background-size: 100% 100%; border-color: transparent; }

[type="checkbox"]:indeterminate:focus, [type="checkbox"]:indeterminate:hover { background-color: currentcolor; border-color: transparent; }

[type="file"] { background: initial; border-color: inherit; border-radius: 0px; border-width: 0px; font-size: inherit; line-height: inherit; padding: 0px; }

[type="file"]:focus { outline: -webkit-focus-ring-color auto 1px; }

h1 { font-size: 1.875rem; line-height: 2.25rem; }

h1, h2, h3 { font-weight: 600; }

h2, h3 { font-size: 1.5rem; line-height: 2rem; }

*, ::after, ::before { --tw-border-spacing-x:0; --tw-border-spacing-y:0; --tw-translate-x:0; --tw-translate-y:0; --tw-rotate:0; --tw-skew-x:0; --tw-skew-y:0; --tw-scale-x:1; --tw-scale-y:1; --tw-pan-x: ; --tw-pan-y: ; --tw-pinch-zoom: ; --tw-scroll-snap-strictness:proximity; --tw-gradient-from-position: ; --tw-gradient-via-position: ; --tw-gradient-to-position: ; --tw-ordinal: ; --tw-slashed-zero: ; --tw-numeric-figure: ; --tw-numeric-spacing: ; --tw-numeric-fraction: ; --tw-ring-inset: ; --tw-ring-offset-width:0px; --tw-ring-offset-color:#fff; --tw-ring-color:rgba(69,89,164,0.5); --tw-ring-offset-shadow:0 0 transparent; --tw-ring-shadow:0 0 transparent; --tw-shadow:0 0 transparent; --tw-shadow-colored:0 0 transparent; --tw-blur: ; --tw-brightness: ; --tw-contrast: ; --tw-grayscale: ; --tw-hue-rotate: ; --tw-invert: ; --tw-saturate: ; --tw-sepia: ; --tw-drop-shadow: ; --tw-backdrop-blur: ; --tw-backdrop-brightness: ; --tw-backdrop-contrast: ; --tw-backdrop-grayscale: ; --tw-backdrop-hue-rotate: ; --tw-backdrop-invert: ; --tw-backdrop-opacity: ; --tw-backdrop-saturate: ; --tw-backdrop-sepia: ; }

::-webkit-backdrop { --tw-border-spacing-x:0; --tw-border-spacing-y:0; --tw-translate-x:0; --tw-translate-y:0; --tw-rotate:0; --tw-skew-x:0; --tw-skew-y:0; --tw-scale-x:1; --tw-scale-y:1; --tw-pan-x: ; --tw-pan-y: ; --tw-pinch-zoom: ; --tw-scroll-snap-strictness:proximity; --tw-gradient-from-position: ; --tw-gradient-via-position: ; --tw-gradient-to-position: ; --tw-ordinal: ; --tw-slashed-zero: ; --tw-numeric-figure: ; --tw-numeric-spacing: ; --tw-numeric-fraction: ; --tw-ring-inset: ; --tw-ring-offset-width:0px; --tw-ring-offset-color:#fff; --tw-ring-color:rgba(69,89,164,0.5); --tw-ring-offset-shadow:0 0 transparent; --tw-ring-shadow:0 0 transparent; --tw-shadow:0 0 transparent; --tw-shadow-colored:0 0 transparent; --tw-blur: ; --tw-brightness: ; --tw-contrast: ; --tw-grayscale: ; --tw-hue-rotate: ; --tw-invert: ; --tw-saturate: ; --tw-sepia: ; --tw-drop-shadow: ; --tw-backdrop-blur: ; --tw-backdrop-brightness: ; --tw-backdrop-contrast: ; --tw-backdrop-grayscale: ; --tw-backdrop-hue-rotate: ; --tw-backdrop-invert: ; --tw-backdrop-opacity: ; --tw-backdrop-saturate: ; --tw-backdrop-sepia: ; }

::backdrop { --tw-border-spacing-x:0; --tw-border-spacing-y:0; --tw-translate-x:0; --tw-translate-y:0; --tw-rotate:0; --tw-skew-x:0; --tw-skew-y:0; --tw-scale-x:1; --tw-scale-y:1; --tw-pan-x: ; --tw-pan-y: ; --tw-pinch-zoom: ; --tw-scroll-snap-strictness:proximity; --tw-gradient-from-position: ; --tw-gradient-via-position: ; --tw-gradient-to-position: ; --tw-ordinal: ; --tw-slashed-zero: ; --tw-numeric-figure: ; --tw-numeric-spacing: ; --tw-numeric-fraction: ; --tw-ring-inset: ; --tw-ring-offset-width:0px; --tw-ring-offset-color:#fff; --tw-ring-color:rgba(69,89,164,0.5); --tw-ring-offset-shadow:0 0 transparent; --tw-ring-shadow:0 0 transparent; --tw-shadow:0 0 transparent; --tw-shadow-colored:0 0 transparent; --tw-blur: ; --tw-brightness: ; --tw-contrast: ; --tw-grayscale: ; --tw-hue-rotate: ; --tw-invert: ; --tw-saturate: ; --tw-sepia: ; --tw-drop-shadow: ; --tw-backdrop-blur: ; --tw-backdrop-brightness: ; --tw-backdrop-contrast: ; --tw-backdrop-grayscale: ; --tw-backdrop-hue-rotate: ; --tw-backdrop-invert: ; --tw-backdrop-opacity: ; --tw-backdrop-saturate: ; --tw-backdrop-sepia: ; }

:root { --gray-0:#fff; --gray-50:#f7f7f8; --gray-100:#ececf1; --gray-200:#d9d9e3; --gray-300:#c5c5d2; --gray-400:#acacbe; --gray-500:#8e8ea0; --gray-600:#565869; --gray-700:#40414f; --gray-800:#343541; --gray-900:#202123; --gray-950:#050509; }

html { --text-primary:var(--gray-800); --text-secondary:var(--gray-600); --text-tertiary:var(--gray-500); --surface-primary:var(--gray-0); --surface-secondary:var(--gray-50); --surface-tertiary:var(--gray-100); --border-light:var(--gray-100); --border-medium:var(--gray-200); --border-heavy:var(--gray-300); }

html.dark { --text-primary:var(--gray-100); --text-secondary:var(--gray-300); --text-tertiary:var(--gray-500); --surface-primary:var(--gray-900); --surface-secondary:var(--gray-800); --surface-tertiary:var(--gray-700); --border-light:var(--gray-700); --border-medium:var(--gray-600); --border-heavy:var(--gray-500); }

textarea:focus { border-color: inherit; box-shadow: none; outline: none; }

::-webkit-scrollbar { height: 1rem; width: 0.5rem; }

::-webkit-scrollbar:horizontal { height: 0.5rem; width: 1rem; }

::-webkit-scrollbar-track { background-color: transparent; border-radius: 9999px; }

::-webkit-scrollbar-thumb { --tw-border-opacity:1; background-color: rgba(217, 217, 227, 0.8); border-color: rgba(255,255,255,var(--tw-border-opacity)); border-radius: 9999px; border-width: 1px; }

::-webkit-scrollbar-thumb:hover { --tw-bg-opacity:1; background-color: rgba(236,236,241,var(--tw-bg-opacity)); }

.dark::-webkit-scrollbar-thumb { --tw-bg-opacity:1; background-color: rgba(86,88,105,var(--tw-bg-opacity)); }

.dark::-webkit-scrollbar-thumb:hover { --tw-bg-opacity:1; background-color: rgba(172,172,190,var(--tw-bg-opacity)); }

@media (min-width: 768px) {
  .scrollbar-trigger ::-webkit-scrollbar-thumb { visibility: hidden; }
  .scrollbar-trigger:hover ::-webkit-scrollbar-thumb { visibility: visible; }
}

.container { width: 100%; }

@media (min-width: 640px) {
  .container { max-width: 640px; }
}

@media (min-width: 768px) {
  .container { max-width: 768px; }
}

@media (min-width: 1024px) {
  .container { max-width: 1024px; }
}

@media (min-width: 1280px) {
  .container { max-width: 1280px; }
}

@media (min-width: 1536px) {
  .container { max-width: 1536px; }
}

.prose { color: var(--tw-prose-body); max-width: 65ch; }

.prose :where([class~="lead"]):not(:where([class~="not-prose"] *)) { color: var(--tw-prose-lead); font-size: 1.25em; line-height: 1.6; margin-bottom: 1.2em; margin-top: 1.2em; }

.prose :where(a):not(:where([class~="not-prose"] *)) { color: var(--tw-prose-links); font-weight: 500; text-decoration: underline; }

.prose :where(strong):not(:where([class~="not-prose"] *)) { color: var(--tw-prose-bold); font-weight: 600; }

.prose :where(a strong):not(:where([class~="not-prose"] *)) { color: inherit; }

.prose :where(blockquote strong):not(:where([class~="not-prose"] *)) { color: inherit; }

.prose :where(thead th strong):not(:where([class~="not-prose"] *)) { color: inherit; }

.prose :where(ol):not(:where([class~="not-prose"] *)) { list-style-type: decimal; margin-bottom: 1.25em; margin-top: 1.25em; padding-left: 1.625em; }

.prose :where(ol[type="A"]):not(:where([class~="not-prose"] *)) { list-style-type: upper-alpha; }

.prose :where(ol[type="a"]):not(:where([class~="not-prose"] *)) { list-style-type: lower-alpha; }

.prose :where():not(:where([class~="not-prose"] *)) { list-style-type: upper-alpha; }

.prose :where():not(:where([class~="not-prose"] *)) { list-style-type: lower-alpha; }

.prose :where(ol[type="I"]):not(:where([class~="not-prose"] *)) { list-style-type: upper-roman; }

.prose :where(ol[type="i"]):not(:where([class~="not-prose"] *)) { list-style-type: lower-roman; }

.prose :where():not(:where([class~="not-prose"] *)) { list-style-type: upper-roman; }

.prose :where():not(:where([class~="not-prose"] *)) { list-style-type: lower-roman; }

.prose :where(ol[type="1"]):not(:where([class~="not-prose"] *)) { list-style-type: decimal; }

.prose :where(ul):not(:where([class~="not-prose"] *)) { list-style-type: disc; margin-bottom: 1.25em; margin-top: 1.25em; padding-left: 1.625em; }

.prose :where(ol > li):not(:where([class~="not-prose"] *))::marker { color: var(--tw-prose-counters); font-weight: 400; }

.prose :where(ul > li):not(:where([class~="not-prose"] *))::marker { color: var(--tw-prose-bullets); }

.prose :where(hr):not(:where([class~="not-prose"] *)) { border-color: var(--tw-prose-hr); border-top-width: 1px; margin-bottom: 3em; margin-top: 3em; }

.prose :where(blockquote):not(:where([class~="not-prose"] *)) { border-left-color: var(--tw-prose-quote-borders); border-left-width: 0.25rem; color: var(--tw-prose-quotes); font-style: normal; font-weight: 500; margin-bottom: 1.6em; margin-top: 1.6em; padding-left: 1em; quotes: "â" "â" "â" "â"; }

.prose :where(blockquote p:first-of-type):not(:where([class~="not-prose"] *))::before { content: open-quote; }

.prose :where(blockquote p:last-of-type):not(:where([class~="not-prose"] *))::after { content: close-quote; }

.prose :where(h1):not(:where([class~="not-prose"] *)) { color: var(--tw-prose-headings); font-size: 2.25em; font-weight: 800; line-height: 1.11111; margin-bottom: 0.888889em; margin-top: 0px; }

.prose :where(h1 strong):not(:where([class~="not-prose"] *)) { color: inherit; font-weight: 900; }

.prose :where(h2):not(:where([class~="not-prose"] *)) { color: var(--tw-prose-headings); font-size: 1.5em; font-weight: 700; line-height: 1.33333; margin-bottom: 1em; margin-top: 2em; }

.prose :where(h2 strong):not(:where([class~="not-prose"] *)) { color: inherit; font-weight: 800; }

.prose :where(h3):not(:where([class~="not-prose"] *)) { color: var(--tw-prose-headings); font-size: 1.25em; font-weight: 600; line-height: 1.6; margin-bottom: 0.6em; margin-top: 1.6em; }

.prose :where(h3 strong):not(:where([class~="not-prose"] *)) { color: inherit; font-weight: 700; }

.prose :where(h4):not(:where([class~="not-prose"] *)) { color: var(--tw-prose-headings); font-weight: 600; line-height: 1.5; margin-bottom: 0.5em; margin-top: 1.5em; }

.prose :where(h4 strong):not(:where([class~="not-prose"] *)) { color: inherit; font-weight: 700; }

.prose :where(img):not(:where([class~="not-prose"] *)) { margin-bottom: 2em; margin-top: 2em; }

.prose :where(figure > *):not(:where([class~="not-prose"] *)) { margin-bottom: 0px; margin-top: 0px; }

.prose :where(figcaption):not(:where([class~="not-prose"] *)) { color: var(--tw-prose-captions); font-size: 0.875em; line-height: 1.42857; margin-top: 0.857143em; }

.prose :where(code):not(:where([class~="not-prose"] *)) { color: var(--tw-prose-code); font-size: 0.875em; font-weight: 600; }

.prose :where(code):not(:where([class~="not-prose"] *))::before { content: "`"; }

.prose :where(code):not(:where([class~="not-prose"] *))::after { content: "`"; }

.prose :where(a code):not(:where([class~="not-prose"] *)) { color: inherit; }

.prose :where(h1 code):not(:where([class~="not-prose"] *)) { color: inherit; }

.prose :where(h2 code):not(:where([class~="not-prose"] *)) { color: inherit; font-size: 0.875em; }

.prose :where(h3 code):not(:where([class~="not-prose"] *)) { color: inherit; font-size: 0.9em; }

.prose :where(h4 code):not(:where([class~="not-prose"] *)) { color: inherit; }

.prose :where(blockquote code):not(:where([class~="not-prose"] *)) { color: inherit; }

.prose :where(thead th code):not(:where([class~="not-prose"] *)) { color: inherit; }

.prose :where(pre):not(:where([class~="not-prose"] *)) { background-color: transparent; border-radius: 0.375rem; color: currentcolor; font-size: 0.875em; font-weight: 400; line-height: 1.71429; margin: 0px; overflow-x: auto; padding: 0px; }

.prose :where(pre code):not(:where([class~="not-prose"] *)) { background-color: transparent; border-radius: 0px; border-width: 0px; color: inherit; font-family: inherit; font-size: inherit; font-weight: inherit; line-height: inherit; padding: 0px; }

.prose :where(pre code):not(:where([class~="not-prose"] *))::before { content: none; }

.prose :where(pre code):not(:where([class~="not-prose"] *))::after { content: none; }

.prose :where(table):not(:where([class~="not-prose"] *)) { font-size: 0.875em; line-height: 1.71429; margin-bottom: 2em; margin-top: 2em; table-layout: auto; text-align: left; width: 100%; }

.prose :where(thead):not(:where([class~="not-prose"] *)) { border-bottom-color: var(--tw-prose-th-borders); border-bottom-width: 1px; }

.prose :where(thead th):not(:where([class~="not-prose"] *)) { color: var(--tw-prose-headings); font-weight: 600; padding-bottom: 0.571429em; padding-left: 0.571429em; padding-right: 0.571429em; vertical-align: bottom; }

.prose :where(tbody tr):not(:where([class~="not-prose"] *)) { border-bottom-color: var(--tw-prose-td-borders); border-bottom-width: 1px; }

.prose :where(tbody tr:last-child):not(:where([class~="not-prose"] *)) { border-bottom-width: 0px; }

.prose :where(tbody td):not(:where([class~="not-prose"] *)) { vertical-align: baseline; }

.prose :where(tfoot):not(:where([class~="not-prose"] *)) { border-top-color: var(--tw-prose-th-borders); border-top-width: 1px; }

.prose :where(tfoot td):not(:where([class~="not-prose"] *)) { vertical-align: top; }

.prose { --tw-prose-body:#374151; --tw-prose-headings:#111827; --tw-prose-lead:#4b5563; --tw-prose-links:#111827; --tw-prose-bold:#111827; --tw-prose-counters:#6b7280; --tw-prose-bullets:#d1d5db; --tw-prose-hr:#e5e7eb; --tw-prose-quotes:#111827; --tw-prose-quote-borders:#e5e7eb; --tw-prose-captions:#6b7280; --tw-prose-code:#111827; --tw-prose-pre-code:#e5e7eb; --tw-prose-pre-bg:#1f2937; --tw-prose-th-borders:#d1d5db; --tw-prose-td-borders:#e5e7eb; --tw-prose-invert-body:#d1d5db; --tw-prose-invert-headings:#fff; --tw-prose-invert-lead:#9ca3af; --tw-prose-invert-links:#fff; --tw-prose-invert-bold:#fff; --tw-prose-invert-counters:#9ca3af; --tw-prose-invert-bullets:#4b5563; --tw-prose-invert-hr:#374151; --tw-prose-invert-quotes:#f3f4f6; --tw-prose-invert-quote-borders:#374151; --tw-prose-invert-captions:#9ca3af; --tw-prose-invert-code:#fff; --tw-prose-invert-pre-code:#d1d5db; --tw-prose-invert-pre-bg:rgba(0,0,0,0.5); --tw-prose-invert-th-borders:#4b5563; --tw-prose-invert-td-borders:#374151; font-size: 1rem; line-height: 1.75; }

.prose :where(p):not(:where([class~="not-prose"] *)) { margin-bottom: 1.25em; margin-top: 1.25em; }

.prose :where(video):not(:where([class~="not-prose"] *)) { margin-bottom: 2em; margin-top: 2em; }

.prose :where(figure):not(:where([class~="not-prose"] *)) { margin-bottom: 2em; margin-top: 2em; }

.prose :where(li):not(:where([class~="not-prose"] *)) { margin-bottom: 0.5em; margin-top: 0.5em; }

.prose :where(ol > li):not(:where([class~="not-prose"] *)) { padding-left: 0.375em; }

.prose :where(ul > li):not(:where([class~="not-prose"] *)) { padding-left: 0.375em; }

.prose :where(.prose > ul > li p):not(:where([class~="not-prose"] *)) { margin-bottom: 0.75em; margin-top: 0.75em; }

.prose :where(.prose > ul > li > :first-child):not(:where([class~="not-prose"] *)) { margin-top: 1.25em; }

.prose :where(.prose > ul > li > :last-child):not(:where([class~="not-prose"] *)) { margin-bottom: 1.25em; }

.prose :where(.prose > ol > li > :first-child):not(:where([class~="not-prose"] *)) { margin-top: 1.25em; }

.prose :where(.prose > ol > li > :last-child):not(:where([class~="not-prose"] *)) { margin-bottom: 1.25em; }

.prose :where(ul ul, ul ol, ol ul, ol ol):not(:where([class~="not-prose"] *)) { margin-bottom: 0.75em; margin-top: 0.75em; }

.prose :where(hr + *):not(:where([class~="not-prose"] *)) { margin-top: 0px; }

.prose :where(h2 + *):not(:where([class~="not-prose"] *)) { margin-top: 0px; }

.prose :where(h3 + *):not(:where([class~="not-prose"] *)) { margin-top: 0px; }

.prose :where(h4 + *):not(:where([class~="not-prose"] *)) { margin-top: 0px; }

.prose :where(thead th:first-child):not(:where([class~="not-prose"] *)) { padding-left: 0px; }

.prose :where(thead th:last-child):not(:where([class~="not-prose"] *)) { padding-right: 0px; }

.prose :where(tbody td, tfoot td):not(:where([class~="not-prose"] *)) { padding: 0.571429em; }

.prose :where(tbody td:first-child, tfoot td:first-child):not(:where([class~="not-prose"] *)) { padding-left: 0px; }

.prose :where(tbody td:last-child, tfoot td:last-child):not(:where([class~="not-prose"] *)) { padding-right: 0px; }

.prose :where(.prose > :first-child):not(:where([class~="not-prose"] *)) { margin-top: 0px; }

.prose :where(.prose > :last-child):not(:where([class~="not-prose"] *)) { margin-bottom: 0px; }

.prose-sm :where(.prose > ul > li p):not(:where([class~="not-prose"] *)) { margin-bottom: 0.571429em; margin-top: 0.571429em; }

.prose-sm :where(.prose > ul > li > :first-child):not(:where([class~="not-prose"] *)) { margin-top: 1.14286em; }

.prose-sm :where(.prose > ul > li > :last-child):not(:where([class~="not-prose"] *)) { margin-bottom: 1.14286em; }

.prose-sm :where(.prose > ol > li > :first-child):not(:where([class~="not-prose"] *)) { margin-top: 1.14286em; }

.prose-sm :where(.prose > ol > li > :last-child):not(:where([class~="not-prose"] *)) { margin-bottom: 1.14286em; }

.prose-sm :where(.prose > :first-child):not(:where([class~="not-prose"] *)) { margin-top: 0px; }

.prose-sm :where(.prose > :last-child):not(:where([class~="not-prose"] *)) { margin-bottom: 0px; }

.prose-base :where(.prose > ul > li p):not(:where([class~="not-prose"] *)) { margin-bottom: 0.75em; margin-top: 0.75em; }

.prose-base :where(.prose > ul > li > :first-child):not(:where([class~="not-prose"] *)) { margin-top: 1.25em; }

.prose-base :where(.prose > ul > li > :last-child):not(:where([class~="not-prose"] *)) { margin-bottom: 1.25em; }

.prose-base :where(.prose > ol > li > :first-child):not(:where([class~="not-prose"] *)) { margin-top: 1.25em; }

.prose-base :where(.prose > ol > li > :last-child):not(:where([class~="not-prose"] *)) { margin-bottom: 1.25em; }

.prose-base :where(.prose > :first-child):not(:where([class~="not-prose"] *)) { margin-top: 0px; }

.prose-base :where(.prose > :last-child):not(:where([class~="not-prose"] *)) { margin-bottom: 0px; }

.prose-lg :where(.prose > ul > li p):not(:where([class~="not-prose"] *)) { margin-bottom: 0.888889em; margin-top: 0.888889em; }

.prose-lg :where(.prose > ul > li > :first-child):not(:where([class~="not-prose"] *)) { margin-top: 1.33333em; }

.prose-lg :where(.prose > ul > li > :last-child):not(:where([class~="not-prose"] *)) { margin-bottom: 1.33333em; }

.prose-lg :where(.prose > ol > li > :first-child):not(:where([class~="not-prose"] *)) { margin-top: 1.33333em; }

.prose-lg :where(.prose > ol > li > :last-child):not(:where([class~="not-prose"] *)) { margin-bottom: 1.33333em; }

.prose-lg :where(.prose > :first-child):not(:where([class~="not-prose"] *)) { margin-top: 0px; }

.prose-lg :where(.prose > :last-child):not(:where([class~="not-prose"] *)) { margin-bottom: 0px; }

.prose-xl :where(.prose > ul > li p):not(:where([class~="not-prose"] *)) { margin-bottom: 0.8em; margin-top: 0.8em; }

.prose-xl :where(.prose > ul > li > :first-child):not(:where([class~="not-prose"] *)) { margin-top: 1.2em; }

.prose-xl :where(.prose > ul > li > :last-child):not(:where([class~="not-prose"] *)) { margin-bottom: 1.2em; }

.prose-xl :where(.prose > ol > li > :first-child):not(:where([class~="not-prose"] *)) { margin-top: 1.2em; }

.prose-xl :where(.prose > ol > li > :last-child):not(:where([class~="not-prose"] *)) { margin-bottom: 1.2em; }

.prose-xl :where(.prose > :first-child):not(:where([class~="not-prose"] *)) { margin-top: 0px; }

.prose-xl :where(.prose > :last-child):not(:where([class~="not-prose"] *)) { margin-bottom: 0px; }

.prose-2xl :where(.prose > ul > li p):not(:where([class~="not-prose"] *)) { margin-bottom: 0.833333em; margin-top: 0.833333em; }

.prose-2xl :where(.prose > ul > li > :first-child):not(:where([class~="not-prose"] *)) { margin-top: 1.33333em; }

.prose-2xl :where(.prose > ul > li > :last-child):not(:where([class~="not-prose"] *)) { margin-bottom: 1.33333em; }

.prose-2xl :where(.prose > ol > li > :first-child):not(:where([class~="not-prose"] *)) { margin-top: 1.33333em; }

.prose-2xl :where(.prose > ol > li > :last-child):not(:where([class~="not-prose"] *)) { margin-bottom: 1.33333em; }

.prose-2xl :where(.prose > :first-child):not(:where([class~="not-prose"] *)) { margin-top: 0px; }

.prose-2xl :where(.prose > :last-child):not(:where([class~="not-prose"] *)) { margin-bottom: 0px; }

.form-input, .form-multiselect, .form-select, .form-textarea { --tw-shadow:0 0 transparent; appearance: none; background-color: rgb(255, 255, 255); border-color: rgb(142, 142, 160); border-radius: 0px; border-width: 1px; font-size: 1rem; line-height: 1.5rem; padding: 0.5rem 0.75rem; }

.form-input:focus, .form-multiselect:focus, .form-select:focus, .form-textarea:focus { --tw-ring-inset:var(--tw-empty, ); --tw-ring-offset-width:0px; --tw-ring-offset-color:#fff; --tw-ring-color:#2563eb; --tw-ring-offset-shadow:var(--tw-ring-inset) 0 0 0 var(--tw-ring-offset-width) var(--tw-ring-offset-color); --tw-ring-shadow:var(--tw-ring-inset) 0 0 0 calc(1px + var(--tw-ring-offset-width)) var(--tw-ring-color); border-color: rgb(37, 99, 235); box-shadow: var(--tw-ring-offset-shadow),var(--tw-ring-shadow),var(--tw-shadow); outline: transparent solid 2px; outline-offset: 2px; }

.form-input::-webkit-input-placeholder, .form-textarea::-webkit-input-placeholder { color: rgb(142, 142, 160); opacity: 1; }

.form-input::placeholder, .form-textarea::placeholder { color: rgb(142, 142, 160); opacity: 1; }

.form-input::-webkit-datetime-edit-fields-wrapper { padding: 0px; }

.form-input::-webkit-date-and-time-value { min-height: 1.5em; }

.form-input::-webkit-datetime-edit, .form-input::-webkit-datetime-edit-day-field, .form-input::-webkit-datetime-edit-hour-field, .form-input::-webkit-datetime-edit-meridiem-field, .form-input::-webkit-datetime-edit-millisecond-field, .form-input::-webkit-datetime-edit-minute-field, .form-input::-webkit-datetime-edit-month-field, .form-input::-webkit-datetime-edit-second-field, .form-input::-webkit-datetime-edit-year-field { padding-bottom: 0px; padding-top: 0px; }

.btn { align-items: center; border-color: transparent; border-radius: 0.25rem; border-width: 1px; display: inline-flex; font-size: 0.875rem; line-height: 1.25rem; padding: 0.5rem 0.75rem; pointer-events: auto; }

.btn:focus { outline: transparent solid 2px; outline-offset: 2px; }

.btn:disabled { cursor: not-allowed; opacity: 0.5; }

.btn-primary { --tw-bg-opacity:1; --tw-text-opacity:1; background-color: rgba(16,163,127,var(--tw-bg-opacity)); color: rgba(255,255,255,var(--tw-text-opacity)); }

.btn-primary:hover { --tw-bg-opacity:1; background-color: rgba(26,127,100,var(--tw-bg-opacity)); }

.btn-primary:focus { --tw-ring-offset-shadow:var(--tw-ring-inset) 0 0 0 var(--tw-ring-offset-width) var(--tw-ring-offset-color); --tw-ring-shadow:var(--tw-ring-inset) 0 0 0 calc(2px + var(--tw-ring-offset-width)) var(--tw-ring-color); --tw-ring-offset-width:2px; box-shadow: var(--tw-ring-offset-shadow),var(--tw-ring-shadow),var(--tw-shadow,0 0 transparent); }

.btn-primary.focus-visible { --tw-ring-opacity:1; --tw-ring-color:rgba(25,195,125,var(--tw-ring-opacity)); }

.btn-primary:focus-visible { --tw-ring-opacity:1; --tw-ring-color:rgba(25,195,125,var(--tw-ring-opacity)); }

.btn-danger { --tw-bg-opacity:1; --tw-text-opacity:1; background-color: rgba(185,28,28,var(--tw-bg-opacity)); color: rgba(255,255,255,var(--tw-text-opacity)); }

.btn-danger:hover { --tw-bg-opacity:1; background-color: rgba(153,27,27,var(--tw-bg-opacity)); }

.btn-danger:focus { --tw-ring-offset-shadow:var(--tw-ring-inset) 0 0 0 var(--tw-ring-offset-width) var(--tw-ring-offset-color); --tw-ring-shadow:var(--tw-ring-inset) 0 0 0 calc(2px + var(--tw-ring-offset-width)) var(--tw-ring-color); --tw-ring-offset-width:2px; box-shadow: var(--tw-ring-offset-shadow),var(--tw-ring-shadow),var(--tw-shadow,0 0 transparent); }

.btn-danger.focus-visible { --tw-ring-opacity:1; --tw-ring-color:rgba(220,38,38,var(--tw-ring-opacity)); }

.btn-danger:focus-visible { --tw-ring-opacity:1; --tw-ring-color:rgba(220,38,38,var(--tw-ring-opacity)); }

.btn-danger:disabled:hover { --tw-bg-opacity:1; background-color: rgba(185,28,28,var(--tw-bg-opacity)); }

.btn-danger-outline { --tw-border-opacity:1; --tw-text-opacity:1; border-color: rgba(185,28,28,var(--tw-border-opacity)); border-width: 1px; color: rgba(185,28,28,var(--tw-text-opacity)); }

.btn-danger-outline:focus { --tw-ring-offset-shadow:var(--tw-ring-inset) 0 0 0 var(--tw-ring-offset-width) var(--tw-ring-offset-color); --tw-ring-shadow:var(--tw-ring-inset) 0 0 0 calc(2px + var(--tw-ring-offset-width)) var(--tw-ring-color); --tw-ring-offset-width:2px; box-shadow: var(--tw-ring-offset-shadow),var(--tw-ring-shadow),var(--tw-shadow,0 0 transparent); }

.btn-danger-outline.focus-visible { --tw-ring-opacity:1; --tw-ring-color:rgba(220,38,38,var(--tw-ring-opacity)); }

.btn-danger-outline:focus-visible { --tw-ring-opacity:1; --tw-ring-color:rgba(220,38,38,var(--tw-ring-opacity)); }

.btn-neutral { --tw-bg-opacity:1; --tw-text-opacity:1; background-color: rgba(255,255,255,var(--tw-bg-opacity)); border-color: rgba(0, 0, 0, 0.1); border-width: 1px; color: rgba(64,65,79,var(--tw-text-opacity)); font-size: 0.875rem; line-height: 1.25rem; }

.btn-neutral:hover { --tw-bg-opacity:1; background-color: rgba(236,236,241,var(--tw-bg-opacity)); }

.btn-neutral:focus { --tw-ring-offset-shadow:var(--tw-ring-inset) 0 0 0 var(--tw-ring-offset-width) var(--tw-ring-offset-color); --tw-ring-shadow:var(--tw-ring-inset) 0 0 0 calc(2px + var(--tw-ring-offset-width)) var(--tw-ring-color); --tw-ring-offset-width:2px; box-shadow: var(--tw-ring-offset-shadow),var(--tw-ring-shadow),var(--tw-shadow,0 0 transparent); }

.btn-neutral.focus-visible { --tw-ring-opacity:1; --tw-ring-color:rgba(99,102,241,var(--tw-ring-opacity)); }

.btn-neutral:focus-visible { --tw-ring-opacity:1; --tw-ring-color:rgba(99,102,241,var(--tw-ring-opacity)); }

.dark .btn-neutral { --tw-border-opacity:1; --tw-bg-opacity:1; --tw-text-opacity:1; background-color: rgba(52,53,65,var(--tw-bg-opacity)); border-color: rgba(86,88,105,var(--tw-border-opacity)); color: rgba(217,217,227,var(--tw-text-opacity)); }

.dark .btn-neutral:hover { --tw-bg-opacity:1; background-color: rgba(64,65,79,var(--tw-bg-opacity)); }

.btn-dark { --tw-border-opacity:1; --tw-bg-opacity:1; --tw-text-opacity:1; background-color: rgba(52,53,65,var(--tw-bg-opacity)); border-color: rgba(86,88,105,var(--tw-border-opacity)); border-width: 1px; color: rgba(255,255,255,var(--tw-text-opacity)); }

.btn-dark:hover { --tw-bg-opacity:1; background-color: rgba(64,65,79,var(--tw-bg-opacity)); }

.btn-light { --tw-bg-opacity:1; --tw-text-opacity:1; background-color: rgba(236,236,241,var(--tw-bg-opacity)); color: rgba(0,0,0,var(--tw-text-opacity)); }

.btn-light:hover { --tw-bg-opacity:1; background-color: rgba(255,255,255,var(--tw-bg-opacity)); }

.btn-disabled { --tw-bg-opacity:1; --tw-text-opacity:1; color: rgba(142,142,160,var(--tw-text-opacity)); cursor: not-allowed; }

.btn-disabled, .btn-disabled:hover { background-color: rgba(236,236,241,var(--tw-bg-opacity)); }

.btn-disabled:hover { --tw-bg-opacity:1; }

.btn-small { padding: 0.25rem 0.5rem; }

.hide-scrollbar { }

.hide-scrollbar ::-webkit-scrollbar { display: none; }

.sr-only { clip: rect(0px, 0px, 0px, 0px); border-width: 0px; height: 1px; margin: -1px; overflow: hidden; padding: 0px; position: absolute; white-space: nowrap; width: 1px; }

.pointer-events-none { pointer-events: none; }

.pointer-events-auto { pointer-events: auto; }

.visible { visibility: visible; }

.\!invisible { visibility: hidden !important; }

.invisible { visibility: hidden; }

.collapse { visibility: collapse; }

.static { position: static; }

.fixed { position: fixed; }

.absolute { position: absolute; }

.relative { position: relative; }

.\!sticky { position: sticky !important; }

.sticky { position: sticky; }

.inset-0 { inset: 0px; }

.inset-12 { inset: 3rem; }

.inset-y-0 { bottom: 0px; top: 0px; }

.inset-y-1\/2 { bottom: 50%; top: 50%; }

.-left-1 { left: -0.25rem; }

.-left-2 { left: -0.5rem; }

.-top-0 { top: 0px; }

.-top-0\.5 { top: -0.125rem; }

.-top-1 { top: -0.25rem; }

.-top-16 { top: -4rem; }

.-top-\[1px\] { top: -1px; }

.bottom-0 { bottom: 0px; }

.bottom-1 { bottom: 0.25rem; }

.bottom-1\.5 { bottom: 0.375rem; }

.bottom-2 { bottom: 0.5rem; }

.bottom-2\.5 { bottom: 0.625rem; }

.bottom-4 { bottom: 1rem; }

.bottom-5 { bottom: 1.25rem; }

.bottom-\[10px\] { bottom: 10px; }

.bottom-\[124px\] { bottom: 124px; }

.bottom-full { bottom: 100%; }

.left-0 { left: 0px; }

.left-1 { left: 0.25rem; }

.left-1\.5 { left: 0.375rem; }

.left-1\/2 { left: 50%; }

.left-16 { left: 4rem; }

.left-2 { left: 0.5rem; }

.left-4 { left: 1rem; }

.left-5 { left: 1.25rem; }

.left-full { left: 100%; }

.right-0 { right: 0px; }

.right-1 { right: 0.25rem; }

.right-12 { right: 3rem; }

.right-16 { right: 4rem; }

.right-2 { right: 0.5rem; }

.right-3 { right: 0.75rem; }

.right-4 { right: 1rem; }

.right-6 { right: 1.5rem; }

.top-0 { top: 0px; }

.top-1 { top: 0.25rem; }

.top-1\/2 { top: 50%; }

.top-11 { top: 2.75rem; }

.top-14 { top: 3.5rem; }

.top-2 { top: 0.5rem; }

.top-3 { top: 0.75rem; }

.top-4 { top: 1rem; }

.top-8 { top: 2rem; }

.top-\[-3px\] { top: -3px; }

.top-\[-6px\] { top: -6px; }

.top-\[20px\] { top: 20px; }

.top-full { top: 100%; }

.top-px { top: 1px; }

.-z-0, .z-0 { z-index: 0; }

.z-10 { z-index: 10; }

.z-20 { z-index: 20; }

.z-40 { z-index: 40; }

.z-50 { z-index: 50; }

.z-\[14\] { z-index: 14; }

.z-\[15\] { z-index: 15; }

.z-\[16\] { z-index: 16; }

.z-\[17\] { z-index: 17; }

.z-\[2\] { z-index: 2; }

.z-\[60\] { z-index: 60; }

.order-1 { order: 1; }

.order-2 { order: 2; }

.col-auto { grid-column: auto / auto; }

.col-span-1 { grid-column: span 1 / span 1; }

.col-start-2 { grid-column-start: 2; }

.row-auto { grid-row: auto / auto; }

.row-start-2 { grid-row-start: 2; }

.float-left { float: left; }

.m-0 { margin: 0px; }

.m-0\.5 { margin: 0.125rem; }

.m-1 { margin: 0.25rem; }

.m-2 { margin: 0.5rem; }

.m-4 { margin: 1rem; }

.m-6 { margin: 1.5rem; }

.m-auto { margin: auto; }

.-mx-4 { margin-left: -1rem; margin-right: -1rem; }

.-my-0 { margin-bottom: 0px; margin-top: 0px; }

.-my-0\.5 { margin-bottom: -0.125rem; margin-top: -0.125rem; }

.-my-1 { margin-bottom: -0.25rem; margin-top: -0.25rem; }

.-my-2 { margin-bottom: -0.5rem; margin-top: -0.5rem; }

.mx-1 { margin-left: 0.25rem; margin-right: 0.25rem; }

.mx-2 { margin-left: 0.5rem; margin-right: 0.5rem; }

.mx-3 { margin-left: 0.75rem; margin-right: 0.75rem; }

.mx-6 { margin-left: 1.5rem; margin-right: 1.5rem; }

.mx-8 { margin-left: 2rem; margin-right: 2rem; }

.mx-auto { margin-left: auto; margin-right: auto; }

.my-0 { margin-bottom: 0px; margin-top: 0px; }

.my-0\.5 { margin-bottom: 0.125rem; margin-top: 0.125rem; }

.my-1 { margin-bottom: 0.25rem; margin-top: 0.25rem; }

.my-1\.5 { margin-bottom: 0.375rem; margin-top: 0.375rem; }

.my-10 { margin-bottom: 2.5rem; margin-top: 2.5rem; }

.my-12 { margin-bottom: 3rem; margin-top: 3rem; }

.my-2 { margin-bottom: 0.5rem; margin-top: 0.5rem; }

.my-3 { margin-bottom: 0.75rem; margin-top: 0.75rem; }

.my-4 { margin-bottom: 1rem; margin-top: 1rem; }

.my-6 { margin-bottom: 1.5rem; margin-top: 1.5rem; }

.my-8 { margin-bottom: 2rem; margin-top: 2rem; }

.my-auto { margin-bottom: auto; margin-top: auto; }

.-ml-0 { margin-left: 0px; }

.-ml-0\.5 { margin-left: -0.125rem; }

.-ml-1 { margin-left: -0.25rem; }

.-ml-2 { margin-left: -0.5rem; }

.-ml-4 { margin-left: -1rem; }

.-ml-\[1px\] { margin-left: -1px; }

.-ml-\[8px\] { margin-left: -8px; }

.-mr-1 { margin-right: -0.25rem; }

.-mr-12 { margin-right: -3rem; }

.-mr-2 { margin-right: -0.5rem; }

.-mt-0 { margin-top: 0px; }

.-mt-0\.5 { margin-top: -0.125rem; }

.-mt-\[0\.75px\] { margin-top: -0.75px; }

.-mt-\[3px\] { margin-top: -3px; }

.mb-0 { margin-bottom: 0px; }

.mb-0\.5 { margin-bottom: 0.125rem; }

.mb-1 { margin-bottom: 0.25rem; }

.mb-10 { margin-bottom: 2.5rem; }

.mb-12 { margin-bottom: 3rem; }

.mb-16 { margin-bottom: 4rem; }

.mb-2 { margin-bottom: 0.5rem; }

.mb-3 { margin-bottom: 0.75rem; }

.mb-4 { margin-bottom: 1rem; }

.mb-5 { margin-bottom: 1.25rem; }

.mb-6 { margin-bottom: 1.5rem; }

.mb-7 { margin-bottom: 1.75rem; }

.mb-8 { margin-bottom: 2rem; }

.mb-9 { margin-bottom: 2.25rem; }

.mb-auto { margin-bottom: auto; }

.ml-0 { margin-left: 0px; }

.ml-0\.5 { margin-left: 0.125rem; }

.ml-1 { margin-left: 0.25rem; }

.ml-12 { margin-left: 3rem; }

.ml-2 { margin-left: 0.5rem; }

.ml-3 { margin-left: 0.75rem; }

.ml-5 { margin-left: 1.25rem; }

.ml-auto { margin-left: auto; }

.mr-0 { margin-right: 0px; }

.mr-1 { margin-right: 0.25rem; }

.mr-2 { margin-right: 0.5rem; }

.mr-3 { margin-right: 0.75rem; }

.mr-4 { margin-right: 1rem; }

.mr-8 { margin-right: 2rem; }

.mr-auto { margin-right: auto; }

.mt-0 { margin-top: 0px; }

.mt-0\.5 { margin-top: 0.125rem; }

.mt-1 { margin-top: 0.25rem; }

.mt-10 { margin-top: 2.5rem; }

.mt-14 { margin-top: 3.5rem; }

.mt-2 { margin-top: 0.5rem; }

.mt-3 { margin-top: 0.75rem; }

.mt-4 { margin-top: 1rem; }

.mt-5 { margin-top: 1.25rem; }

.mt-6 { margin-top: 1.5rem; }

.mt-8 { margin-top: 2rem; }

.mt-auto { margin-top: auto; }

.line-clamp-1 { -webkit-line-clamp: 1; }

.line-clamp-1, .line-clamp-2 { -webkit-box-orient: vertical; display: -webkit-box; overflow: hidden; }

.line-clamp-2 { -webkit-line-clamp: 2; }

.line-clamp-3 { -webkit-line-clamp: 3; }

.line-clamp-3, .line-clamp-6 { -webkit-box-orient: vertical; display: -webkit-box; overflow: hidden; }

.line-clamp-6 { -webkit-line-clamp: 6; }

.block { display: block; }

.inline-block { display: inline-block; }

.inline { display: inline; }

.flex { display: flex; }

.inline-flex { display: inline-flex; }

.table { display: table; }

.flow-root { display: flow-root; }

.grid { display: grid; }

.contents { display: contents; }

.hidden { display: none; }

.aspect-\[4\/7\] { aspect-ratio: 4 / 7; }

.aspect-\[7\/4\] { aspect-ratio: 7 / 4; }

.aspect-square { aspect-ratio: 1 / 1; }

.\!h-full { height: 100% !important; }

.h-0 { height: 0px; }

.h-1 { height: 0.25rem; }

.h-1\.5 { height: 0.375rem; }

.h-10 { height: 2.5rem; }

.h-11 { height: 2.75rem; }

.h-12 { height: 3rem; }

.h-14 { height: 3.5rem; }

.h-16 { height: 4rem; }

.h-2 { height: 0.5rem; }

.h-2\.5 { height: 0.625rem; }

.h-20 { height: 5rem; }

.h-24 { height: 6rem; }

.h-3 { height: 0.75rem; }

.h-3\/5 { height: 60%; }

.h-32 { height: 8rem; }

.h-36 { height: 9rem; }

.h-4 { height: 1rem; }

.h-40 { height: 10rem; }

.h-44 { height: 11rem; }

.h-5 { height: 1.25rem; }

.h-52 { height: 13rem; }

.h-6 { height: 1.5rem; }

.h-64 { height: 16rem; }

.h-7 { height: 1.75rem; }

.h-8 { height: 2rem; }

.h-9 { height: 2.25rem; }

.h-\[1200px\] { height: 1200px; }

.h-\[12px\] { height: 12px; }

.h-\[14px\] { height: 14px; }

.h-\[18px\] { height: 18px; }

.h-\[19px\] { height: 19px; }

.h-\[21px\] { height: 21px; }

.h-\[22px\] { height: 22px; }

.h-\[25px\] { height: 25px; }

.h-\[30px\] { height: 30px; }

.h-\[34px\] { height: 34px; }

.h-\[36px\] { height: 36px; }

.h-\[404px\] { height: 404px; }

.h-\[42px\] { height: 42px; }

.h-\[50px\] { height: 50px; }

.h-\[60px\] { height: 60px; }

.h-\[70px\] { height: 70px; }

.h-auto { height: auto; }

.h-fit { height: fit-content; }

.h-full { height: 100%; }

.h-px { height: 1px; }

.h-screen { height: 100vh; }

.max-h-0 { max-height: 0px; }

.max-h-32 { max-height: 8rem; }

.max-h-5 { max-height: 1.25rem; }

.max-h-60 { max-height: 15rem; }

.max-h-64 { max-height: 16rem; }

.max-h-72 { max-height: 18rem; }

.max-h-\[28rem\] { max-height: 28rem; }

.max-h-\[450px\] { max-height: 450px; }

.max-h-\[80vh\] { max-height: 80vh; }

.max-h-\[85vh\] { max-height: 85vh; }

.max-h-\[calc\(100vh-150px\)\] { max-height: calc(100vh - 150px); }

.max-h-\[calc\(100vh-300px\)\] { max-height: calc(100vh - 300px); }

.min-h-0 { min-height: 0px; }

.min-h-\[1\.5rem\] { min-height: 1.5rem; }

.min-h-\[200px\] { min-height: 200px; }

.min-h-\[20px\] { min-height: 20px; }

.min-h-\[24px\] { min-height: 24px; }

.min-h-\[40px\] { min-height: 40px; }

.min-h-\[44px\] { min-height: 44px; }

.min-h-\[4rem\] { min-height: 4rem; }

.min-h-\[60px\] { min-height: 60px; }

.min-h-\[71px\] { min-height: 71px; }

.min-h-fit { min-height: fit-content; }

.min-h-full { min-height: 100%; }

.min-h-screen { min-height: 100vh; }

.\!w-full { width: 100% !important; }

.w-0 { width: 0px; }

.w-1 { width: 0.25rem; }

.w-1\.5 { width: 0.375rem; }

.w-1\/2 { width: 50%; }

.w-1\/3 { width: 33.3333%; }

.w-10 { width: 2.5rem; }

.w-11 { width: 2.75rem; }

.w-12 { width: 3rem; }

.w-14 { width: 3.5rem; }

.w-16 { width: 4rem; }

.w-2 { width: 0.5rem; }

.w-20 { width: 5rem; }

.w-28 { width: 7rem; }

.w-3 { width: 0.75rem; }

.w-32 { width: 8rem; }

.w-4 { width: 1rem; }

.w-40 { width: 10rem; }

.w-48 { width: 12rem; }

.w-5 { width: 1.25rem; }

.w-6 { width: 1.5rem; }

.w-60 { width: 15rem; }

.w-64 { width: 16rem; }

.w-7 { width: 1.75rem; }

.w-8 { width: 2rem; }

.w-8\/12 { width: 66.6667%; }

.w-80 { width: 20rem; }

.w-9 { width: 2.25rem; }

.w-96 { width: 24rem; }

.w-\[103px\] { width: 103px; }

.w-\[120px\] { width: 120px; }

.w-\[184px\] { width: 184px; }

.w-\[18px\] { width: 18px; }

.w-\[2\.5px\] { width: 2.5px; }

.w-\[209px\] { width: 209px; }

.w-\[218px\] { width: 218px; }

.w-\[21px\] { width: 21px; }

.w-\[25vw\] { width: 25vw; }

.w-\[260px\] { width: 260px; }

.w-\[280px\] { width: 280px; }

.w-\[30px\] { width: 30px; }

.w-\[34px\] { width: 34px; }

.w-\[42px\] { width: 42px; }

.w-\[70px\] { width: 70px; }

.w-\[860px\] { width: 860px; }

.w-\[88px\] { width: 88px; }

.w-\[960px\] { width: 960px; }

.w-\[calc\(100\%-50px\)\] { width: calc(100% - 50px); }

.w-auto { width: auto; }

.w-fit { width: fit-content; }

.w-full { width: 100%; }

.w-min { width: min-content; }

.w-screen { width: 100vw; }

.min-w-0 { min-width: 0px; }

.min-w-\[100vw\] { min-width: 100vw; }

.min-w-\[175px\] { min-width: 175px; }

.min-w-\[180px\] { min-width: 180px; }

.min-w-\[220px\] { min-width: 220px; }

.min-w-\[256px\] { min-width: 256px; }

.min-w-\[280px\] { min-width: 280px; }

.min-w-\[300px\] { min-width: 300px; }

.min-w-\[32px\] { min-width: 32px; }

.min-w-\[50px\] { min-width: 50px; }

.min-w-\[8rem\] { min-width: 8rem; }

.min-w-full { min-width: 100%; }

.max-w-3xl { max-width: 48rem; }

.max-w-5xl { max-width: 64rem; }

.max-w-6xl { max-width: 72rem; }

.max-w-7xl { max-width: 80rem; }

.max-w-\[100vw\] { max-width: 100vw; }

.max-w-\[160px\] { max-width: 160px; }

.max-w-\[200px\] { max-width: 200px; }

.max-w-\[220px\] { max-width: 220px; }

.max-w-\[250px\] { max-width: 250px; }

.max-w-\[320px\] { max-width: 320px; }

.max-w-\[350px\] { max-width: 350px; }

.max-w-\[372px\] { max-width: 372px; }

.max-w-\[400px\] { max-width: 400px; }

.max-w-\[440px\] { max-width: 440px; }

.max-w-\[46rem\] { max-width: 46rem; }

.max-w-\[480px\] { max-width: 480px; }

.max-w-\[510px\] { max-width: 510px; }

.max-w-\[550px\] { max-width: 550px; }

.max-w-\[560px\] { max-width: 560px; }

.max-w-\[650px\] { max-width: 650px; }

.max-w-\[90vw\] { max-width: 90vw; }

.max-w-\[calc\(100vw-3rem\)\] { max-width: calc(100vw - 3rem); }

.max-w-full { max-width: 100%; }

.max-w-lg { max-width: 32rem; }

.max-w-md { max-width: 28rem; }

.max-w-sm { max-width: 24rem; }

.max-w-xl { max-width: 36rem; }

.max-w-xs { max-width: 20rem; }

.flex-1 { flex: 1 1 0%; }

.flex-auto { flex: 1 1 auto; }

.flex-none { flex: 0 0 auto; }

.flex-shrink { flex-shrink: 1; }

.flex-shrink-0 { flex-shrink: 0; }

.shrink { flex-shrink: 1; }

.shrink-0 { flex-shrink: 0; }

.flex-grow { flex-grow: 1; }

.flex-grow-0 { flex-grow: 0; }

.grow { flex-grow: 1; }

.basis-0 { flex-basis: 0px; }

.basis-\[25vw\] { flex-basis: 25vw; }

.basis-\[75vw\] { flex-basis: 75vw; }

.table-auto { table-layout: auto; }

.table-fixed { table-layout: fixed; }

.border-separate { border-collapse: separate; }

.border-spacing-0 { --tw-border-spacing-x:0px; --tw-border-spacing-y:0px; border-spacing: var(--tw-border-spacing-x) var(--tw-border-spacing-y); }

.origin-\[50\%_50\%\] { transform-origin: 50% 50%; }

.-translate-x-1\/2 { --tw-translate-x:-50%; }

.-translate-x-1\/2, .-translate-x-\[calc\(100\%\+4px\)\] { transform: translate(var(--tw-translate-x),var(--tw-translate-y)) rotate(var(--tw-rotate)) skewX(var(--tw-skew-x)) skewY(var(--tw-skew-y)) scaleX(var(--tw-scale-x)) scaleY(var(--tw-scale-y)); }

.-translate-x-\[calc\(100\%\+4px\)\] { --tw-translate-x:calc(-100% + -4px); }

.-translate-x-full { --tw-translate-x:-100%; }

.-translate-x-full, .-translate-y-1\/2 { transform: translate(var(--tw-translate-x),var(--tw-translate-y)) rotate(var(--tw-rotate)) skewX(var(--tw-skew-x)) skewY(var(--tw-skew-y)) scaleX(var(--tw-scale-x)) scaleY(var(--tw-scale-y)); }

.-translate-y-1\/2 { --tw-translate-y:-50%; }

.translate-x-0 { --tw-translate-x:0px; }

.translate-x-0, .translate-x-0\.5 { transform: translate(var(--tw-translate-x),var(--tw-translate-y)) rotate(var(--tw-rotate)) skewX(var(--tw-skew-x)) skewY(var(--tw-skew-y)) scaleX(var(--tw-scale-x)) scaleY(var(--tw-scale-y)); }

.translate-x-0\.5 { --tw-translate-x:0.125rem; }

.translate-x-1 { --tw-translate-x:0.25rem; }

.translate-x-1, .translate-x-1\/2 { transform: translate(var(--tw-translate-x),var(--tw-translate-y)) rotate(var(--tw-rotate)) skewX(var(--tw-skew-x)) skewY(var(--tw-skew-y)) scaleX(var(--tw-scale-x)) scaleY(var(--tw-scale-y)); }

.translate-x-1\/2 { --tw-translate-x:50%; }

.translate-x-full { --tw-translate-x:100%; }

.translate-x-full, .translate-y-0 { transform: translate(var(--tw-translate-x),var(--tw-translate-y)) rotate(var(--tw-rotate)) skewX(var(--tw-skew-x)) skewY(var(--tw-skew-y)) scaleX(var(--tw-scale-x)) scaleY(var(--tw-scale-y)); }

.translate-y-0 { --tw-translate-y:0px; }

.translate-y-1 { --tw-translate-y:0.25rem; }

.translate-y-1, .translate-y-3 { transform: translate(var(--tw-translate-x),var(--tw-translate-y)) rotate(var(--tw-rotate)) skewX(var(--tw-skew-x)) skewY(var(--tw-skew-y)) scaleX(var(--tw-scale-x)) scaleY(var(--tw-scale-y)); }

.translate-y-3 { --tw-translate-y:0.75rem; }

.translate-y-\[calc\(-50\%-1em\)\] { --tw-translate-y:calc(-50% - 1em); }

.translate-y-\[calc\(-50\%-1em\)\], .translate-y-\[calc\(-50\%-1em\+16px\)\] { transform: translate(var(--tw-translate-x),var(--tw-translate-y)) rotate(var(--tw-rotate)) skewX(var(--tw-skew-x)) skewY(var(--tw-skew-y)) scaleX(var(--tw-scale-x)) scaleY(var(--tw-scale-y)); }

.translate-y-\[calc\(-50\%-1em\+16px\)\] { --tw-translate-y:calc(-50% - 1em + 16px); }

.translate-y-\[calc\(-50\%-1em-16px\)\] { --tw-translate-y:calc(-50% - 1em - 16px); }

.translate-y-\[calc\(-50\%-1em-16px\)\], .translate-y-\[calc\(100\%-71px\)\] { transform: translate(var(--tw-translate-x),var(--tw-translate-y)) rotate(var(--tw-rotate)) skewX(var(--tw-skew-x)) skewY(var(--tw-skew-y)) scaleX(var(--tw-scale-x)) scaleY(var(--tw-scale-y)); }

.translate-y-\[calc\(100\%-71px\)\] { --tw-translate-y:calc(100% - 71px); }

.-rotate-180 { --tw-rotate:-180deg; }

.-rotate-180, .-rotate-45 { transform: translate(var(--tw-translate-x),var(--tw-translate-y)) rotate(var(--tw-rotate)) skewX(var(--tw-skew-x)) skewY(var(--tw-skew-y)) scaleX(var(--tw-scale-x)) scaleY(var(--tw-scale-y)); }

.-rotate-45 { --tw-rotate:-45deg; }

.-rotate-90 { --tw-rotate:-90deg; }

.-rotate-90, .rotate-180 { transform: translate(var(--tw-translate-x),var(--tw-translate-y)) rotate(var(--tw-rotate)) skewX(var(--tw-skew-x)) skewY(var(--tw-skew-y)) scaleX(var(--tw-scale-x)) scaleY(var(--tw-scale-y)); }

.rotate-180 { --tw-rotate:180deg; }

.rotate-45 { --tw-rotate:45deg; }

.rotate-45, .rotate-90 { transform: translate(var(--tw-translate-x),var(--tw-translate-y)) rotate(var(--tw-rotate)) skewX(var(--tw-skew-x)) skewY(var(--tw-skew-y)) scaleX(var(--tw-scale-x)) scaleY(var(--tw-scale-y)); }

.rotate-90 { --tw-rotate:90deg; }

.scale-0 { --tw-scale-x:0; --tw-scale-y:0; }

.scale-0, .scale-x-75 { transform: translate(var(--tw-translate-x),var(--tw-translate-y)) rotate(var(--tw-rotate)) skewX(var(--tw-skew-x)) skewY(var(--tw-skew-y)) scaleX(var(--tw-scale-x)) scaleY(var(--tw-scale-y)); }

.scale-x-75 { --tw-scale-x:0.75; }

.transform { transform: translate(var(--tw-translate-x),var(--tw-translate-y)) rotate(var(--tw-rotate)) skewX(var(--tw-skew-x)) skewY(var(--tw-skew-y)) scaleX(var(--tw-scale-x)) scaleY(var(--tw-scale-y)); }

@-webkit-keyframes pulse { 
  50% { opacity: 0.5; }
}

@keyframes pulse { 
  50% { opacity: 0.5; }
}

.animate-pulse { animation: 2s cubic-bezier(0.4, 0, 0.6, 1) 0s infinite normal none running pulse; }

@-webkit-keyframes slideDownAndFade { 
  0% { opacity: 0; transform: translateY(-2px); }
  100% { opacity: 1; transform: translateY(0px); }
}

.animate-slideDownAndFade { animation: 0.4s cubic-bezier(0.16, 1, 0.3, 1) 0s 1 normal none running slideDownAndFade; }

@-webkit-keyframes slideLeftAndFade { 
  0% { opacity: 0; transform: translateX(2px); }
  100% { opacity: 1; transform: translateX(0px); }
}

.animate-slideLeftAndFade { animation: 0.4s cubic-bezier(0.16, 1, 0.3, 1) 0s 1 normal none running slideLeftAndFade; }

@-webkit-keyframes slideUpAndFade { 
  0% { opacity: 0; transform: translateY(2px); }
  100% { opacity: 1; transform: translateY(0px); }
}

.animate-slideUpAndFade { animation: 0.4s cubic-bezier(0.16, 1, 0.3, 1) 0s 1 normal none running slideUpAndFade; }

@-webkit-keyframes spin { 
  100% { transform: rotate(1turn); }
}

@keyframes spin { 
  100% { transform: rotate(1turn); }
}

.animate-spin { animation: 1s linear 0s infinite normal none running spin; }

.cursor-auto { cursor: auto; }

.cursor-default { cursor: default; }

.cursor-not-allowed { cursor: not-allowed; }

.cursor-pointer { cursor: pointer; }

.cursor-progress { cursor: progress; }

.select-none { user-select: none; }

.resize-none { resize: none; }

.resize { resize: both; }

.list-inside { list-style-position: inside; }

.list-disc { list-style-type: disc; }

.list-none { list-style-type: none; }

.appearance-none { appearance: none; }

.grid-flow-row { grid-auto-flow: row; }

.grid-cols-1 { grid-template-columns: repeat(1, minmax(0px, 1fr)); }

.grid-cols-2 { grid-template-columns: repeat(2, minmax(0px, 1fr)); }

.grid-cols-3 { grid-template-columns: repeat(3, minmax(0px, 1fr)); }

.grid-cols-5 { grid-template-columns: repeat(5, minmax(0px, 1fr)); }

.grid-cols-\[10px_1fr_10px\] { grid-template-columns: 10px 1fr 10px; }

.grid-cols-\[repeat\(auto-fit\,minmax\(250px\,1fr\)\)\] { grid-template-columns: repeat(auto-fit, minmax(250px, 1fr)); }

.grid-rows-\[minmax\(10px\,_1fr\)_auto_minmax\(10px\,_1fr\)\] { grid-template-rows: minmax(10px, 1fr) auto minmax(10px, 1fr); }

.flex-row { flex-direction: row; }

.flex-col { flex-direction: column; }

.flex-col-reverse { flex-direction: column-reverse; }

.flex-wrap { flex-wrap: wrap; }

.flex-wrap-reverse { flex-wrap: wrap-reverse; }

.place-items-center { place-items: center; }

.items-start { align-items: flex-start; }

.items-end { align-items: flex-end; }

.items-center { align-items: center; }

.items-stretch { align-items: stretch; }

.justify-start { justify-content: flex-start; }

.justify-end { justify-content: flex-end; }

.justify-center { justify-content: center; }

.justify-between { justify-content: space-between; }

.justify-around { justify-content: space-around; }

.justify-stretch { justify-content: stretch; }

.justify-items-center { justify-items: center; }

.gap-0 { gap: 0px; }

.gap-1 { gap: 0.25rem; }

.gap-1\.5 { gap: 0.375rem; }

.gap-2 { gap: 0.5rem; }

.gap-2\.5 { gap: 0.625rem; }

.gap-3 { gap: 0.75rem; }

.gap-3\.5 { gap: 0.875rem; }

.gap-4 { gap: 1rem; }

.gap-5 { gap: 1.25rem; }

.gap-6 { gap: 1.5rem; }

.gap-7 { gap: 1.75rem; }

.gap-x-1 { column-gap: 0.25rem; }

.gap-x-1\.5 { column-gap: 0.375rem; }

.gap-x-3 { column-gap: 0.75rem; }

.gap-x-6 { column-gap: 1.5rem; }

.gap-x-9 { column-gap: 2.25rem; }

.gap-y-2 { row-gap: 0.5rem; }

.gap-y-4 { row-gap: 1rem; }

.space-x-1 > :not([hidden]) ~ :not([hidden]) { --tw-space-x-reverse:0; margin-left: calc(0.25rem*(1 - var(--tw-space-x-reverse))); margin-right: calc(0.25rem*var(--tw-space-x-reverse)); }

.space-x-1\.5 > :not([hidden]) ~ :not([hidden]) { --tw-space-x-reverse:0; margin-left: calc(0.375rem*(1 - var(--tw-space-x-reverse))); margin-right: calc(0.375rem*var(--tw-space-x-reverse)); }

.space-x-2 > :not([hidden]) ~ :not([hidden]) { --tw-space-x-reverse:0; margin-left: calc(0.5rem*(1 - var(--tw-space-x-reverse))); margin-right: calc(0.5rem*var(--tw-space-x-reverse)); }

.space-x-4 > :not([hidden]) ~ :not([hidden]) { --tw-space-x-reverse:0; margin-left: calc(1rem*(1 - var(--tw-space-x-reverse))); margin-right: calc(1rem*var(--tw-space-x-reverse)); }

.space-y-0 > :not([hidden]) ~ :not([hidden]) { --tw-space-y-reverse:0; margin-bottom: calc(0px*var(--tw-space-y-reverse)); margin-top: calc(0px*(1 - var(--tw-space-y-reverse))); }

.space-y-0\.5 > :not([hidden]) ~ :not([hidden]) { --tw-space-y-reverse:0; margin-bottom: calc(0.125rem*var(--tw-space-y-reverse)); margin-top: calc(0.125rem*(1 - var(--tw-space-y-reverse))); }

.space-y-1 > :not([hidden]) ~ :not([hidden]) { --tw-space-y-reverse:0; margin-bottom: calc(0.25rem*var(--tw-space-y-reverse)); margin-top: calc(0.25rem*(1 - var(--tw-space-y-reverse))); }

.space-y-1\.5 > :not([hidden]) ~ :not([hidden]) { --tw-space-y-reverse:0; margin-bottom: calc(0.375rem*var(--tw-space-y-reverse)); margin-top: calc(0.375rem*(1 - var(--tw-space-y-reverse))); }

.space-y-2 > :not([hidden]) ~ :not([hidden]) { --tw-space-y-reverse:0; margin-bottom: calc(0.5rem*var(--tw-space-y-reverse)); margin-top: calc(0.5rem*(1 - var(--tw-space-y-reverse))); }

.space-y-3 > :not([hidden]) ~ :not([hidden]) { --tw-space-y-reverse:0; margin-bottom: calc(0.75rem*var(--tw-space-y-reverse)); margin-top: calc(0.75rem*(1 - var(--tw-space-y-reverse))); }

.space-y-4 > :not([hidden]) ~ :not([hidden]) { --tw-space-y-reverse:0; margin-bottom: calc(1rem*var(--tw-space-y-reverse)); margin-top: calc(1rem*(1 - var(--tw-space-y-reverse))); }

.space-y-6 > :not([hidden]) ~ :not([hidden]) { --tw-space-y-reverse:0; margin-bottom: calc(1.5rem*var(--tw-space-y-reverse)); margin-top: calc(1.5rem*(1 - var(--tw-space-y-reverse))); }

.divide-x > :not([hidden]) ~ :not([hidden]) { --tw-divide-x-reverse:0; border-left-width: calc(1px*(1 - var(--tw-divide-x-reverse))); border-right-width: calc(1px*var(--tw-divide-x-reverse)); }

.divide-y > :not([hidden]) ~ :not([hidden]) { --tw-divide-y-reverse:0; border-bottom-width: calc(1px*var(--tw-divide-y-reverse)); border-top-width: calc(1px*(1 - var(--tw-divide-y-reverse))); }

.divide-gray-200 > :not([hidden]) ~ :not([hidden]) { --tw-divide-opacity:1; border-color: rgba(217,217,227,var(--tw-divide-opacity)); }

.divide-white\/10 > :not([hidden]) ~ :not([hidden]) { border-color: rgba(255, 255, 255, 0.1); }

.self-end { align-self: flex-end; }

.self-center { align-self: center; }

.self-stretch { align-self: stretch; }

.justify-self-start { justify-self: start; }

.overflow-auto { overflow: auto; }

.overflow-hidden { overflow: hidden; }

.overflow-scroll { overflow: scroll; }

.overflow-x-auto { overflow-x: auto; }

.overflow-y-auto { overflow-y: auto; }

.overflow-x-hidden { overflow-x: hidden; }

.overflow-y-hidden { overflow-y: hidden; }

.overflow-y-scroll { overflow-y: scroll; }

.truncate { overflow: hidden; white-space: nowrap; }

.overflow-ellipsis, .text-ellipsis, .truncate { text-overflow: ellipsis; }

.text-clip { text-overflow: clip; }

.whitespace-normal { white-space: normal; }

.whitespace-nowrap { white-space: nowrap; }

.\!whitespace-pre { white-space: pre !important; }

.whitespace-pre-line { white-space: pre-line; }

.\!whitespace-pre-wrap { white-space: pre-wrap !important; }

.whitespace-pre-wrap { white-space: pre-wrap; }

.break-words { overflow-wrap: break-word; }

.break-all { word-break: break-all; }

.\!rounded-2xl { border-radius: 1rem !important; }

.\!rounded-lg { border-radius: 0.5rem !important; }

.rounded { border-radius: 0.25rem; }

.rounded-2xl { border-radius: 1rem; }

.rounded-\[4px\] { border-radius: 4px; }

.rounded-\[5px\] { border-radius: 5px; }

.rounded-full { border-radius: 9999px; }

.rounded-lg { border-radius: 0.5rem; }

.rounded-md { border-radius: 0.375rem; }

.rounded-none { border-radius: 0px; }

.rounded-sm { border-radius: 0.125rem; }

.rounded-xl { border-radius: 0.75rem; }

.rounded-b-lg { border-bottom-left-radius: 0.5rem; border-bottom-right-radius: 0.5rem; }

.rounded-l-md { border-bottom-left-radius: 0.375rem; border-top-left-radius: 0.375rem; }

.rounded-r-md { border-bottom-right-radius: 0.375rem; border-top-right-radius: 0.375rem; }

.rounded-t-\[30px\] { border-top-left-radius: 30px; border-top-right-radius: 30px; }

.rounded-t-md { border-top-left-radius: 0.375rem; border-top-right-radius: 0.375rem; }

.rounded-t-xl { border-top-left-radius: 0.75rem; border-top-right-radius: 0.75rem; }

.rounded-br-sm { border-bottom-right-radius: 0.125rem; }

.border { border-width: 1px; }

.border-0 { border-width: 0px; }

.border-2 { border-width: 2px; }

.border-\[0\.5px\] { border-width: 0.5px; }

.border-b { border-bottom-width: 1px; }

.border-b-\[0\.5px\] { border-bottom-width: 0.5px; }

.border-l { border-left-width: 1px; }

.border-l-4 { border-left-width: 4px; }

.border-l-8 { border-left-width: 8px; }

.border-r { border-right-width: 1px; }

.border-r-0 { border-right-width: 0px; }

.border-r-8 { border-right-width: 8px; }

.border-t { border-top-width: 1px; }

.border-t-8 { border-top-width: 8px; }

.border-none { border-style: none; }

.border-\[rgba\(0\,0\,0\,\.05\)\] { border-color: rgba(0, 0, 0, 0.05); }

.border-\[rgba\(0\,0\,0\,\.15\)\] { border-color: rgba(0, 0, 0, 0.15); }

.border-black\/10 { border-color: rgba(0, 0, 0, 0.1); }

.border-black\/20 { border-color: rgba(0, 0, 0, 0.2); }

.border-black\/30 { border-color: rgba(0, 0, 0, 0.3); }

.border-black\/5 { border-color: rgba(0, 0, 0, 0.05); }

.border-brand-green { --tw-border-opacity:1; border-color: rgba(25,195,125,var(--tw-border-opacity)); }

.border-gray-100 { --tw-border-opacity:1; border-color: rgba(236,236,241,var(--tw-border-opacity)); }

.border-gray-200 { --tw-border-opacity:1; border-color: rgba(217,217,227,var(--tw-border-opacity)); }

.border-gray-300 { --tw-border-opacity:1; border-color: rgba(197,197,210,var(--tw-border-opacity)); }

.border-gray-400 { --tw-border-opacity:1; border-color: rgba(172,172,190,var(--tw-border-opacity)); }

.border-gray-50 { --tw-border-opacity:1; border-color: rgba(247,247,248,var(--tw-border-opacity)); }

.border-gray-500 { --tw-border-opacity:1; border-color: rgba(142,142,160,var(--tw-border-opacity)); }

.border-gray-700 { --tw-border-opacity:1; border-color: rgba(64,65,79,var(--tw-border-opacity)); }

.border-gray-800 { --tw-border-opacity:1; border-color: rgba(52,53,65,var(--tw-border-opacity)); }

.border-green-500 { --tw-border-opacity:1; border-color: rgba(25,195,125,var(--tw-border-opacity)); }

.border-orange-400 { --tw-border-opacity:1; border-color: rgba(255,128,82,var(--tw-border-opacity)); }

.border-orange-500 { --tw-border-opacity:1; border-color: rgba(224,108,43,var(--tw-border-opacity)); }

.border-red-500 { --tw-border-opacity:1; border-color: rgba(239,68,68,var(--tw-border-opacity)); }

.border-token-border-light { border-color: var(--border-light); }

.border-token-border-medium { border-color: var(--border-medium); }

.border-transparent { border-color: transparent; }

.border-white { --tw-border-opacity:1; border-color: rgba(255,255,255,var(--tw-border-opacity)); }

.border-white\/20 { border-color: rgba(255, 255, 255, 0.2); }

.border-l-transparent { border-left-color: transparent; }

.border-r-transparent { border-right-color: transparent; }

.border-t-\[\#0077FF\] { --tw-border-opacity:1; border-top-color: rgba(0,119,255,var(--tw-border-opacity)); }

.border-opacity-75 { --tw-border-opacity:0.75; }

.\!bg-brand-purple { --tw-bg-opacity:1 !important; background-color: rgba(171,104,255,var(--tw-bg-opacity)) !important; }

.\!bg-gray-200 { --tw-bg-opacity:1 !important; background-color: rgba(217,217,227,var(--tw-bg-opacity)) !important; }

.\!bg-gray-700 { --tw-bg-opacity:1 !important; background-color: rgba(64,65,79,var(--tw-bg-opacity)) !important; }

.\!bg-transparent { background-color: transparent !important; }

.bg-\[\#0077FF\] { --tw-bg-opacity:1; background-color: rgba(0,119,255,var(--tw-bg-opacity)); }

.bg-\[\#10A37F\] { --tw-bg-opacity:1; background-color: rgba(16,163,127,var(--tw-bg-opacity)); }

.bg-\[\#3C46FF\] { --tw-bg-opacity:1; background-color: rgba(60,70,255,var(--tw-bg-opacity)); }

.bg-\[\#4046EC\] { --tw-bg-opacity:1; background-color: rgba(64,70,236,var(--tw-bg-opacity)); }

.bg-\[\#B161FD\] { --tw-bg-opacity:1; background-color: rgba(177,97,253,var(--tw-bg-opacity)); }

.bg-\[\#F8CA27\] { --tw-bg-opacity:1; background-color: rgba(248,202,39,var(--tw-bg-opacity)); }

.bg-\[\#FF5588\] { --tw-bg-opacity:1; background-color: rgba(255,85,136,var(--tw-bg-opacity)); }

.bg-\[\#FF6E3C\] { --tw-bg-opacity:1; background-color: rgba(255,110,60,var(--tw-bg-opacity)); }

.bg-\[rgba\(30\,203\,239\,0\.07\)\] { background-color: rgba(30, 203, 239, 0.07); }

.bg-black { --tw-bg-opacity:1; background-color: rgba(0,0,0,var(--tw-bg-opacity)); }

.bg-black\/20 { background-color: rgba(0, 0, 0, 0.2); }

.bg-black\/5 { background-color: rgba(0, 0, 0, 0.05); }

.bg-black\/90 { background-color: rgba(0, 0, 0, 0.9); }

.bg-black\/\[\.04\] { background-color: rgba(0, 0, 0, 0.04); }

.bg-black\/\[\.08\] { background-color: rgba(0, 0, 0, 0.08); }

.bg-black\/\[\.12\] { background-color: rgba(0, 0, 0, 0.12); }

.bg-blue-200 { --tw-bg-opacity:1; background-color: rgba(191,219,254,var(--tw-bg-opacity)); }

.bg-blue-300 { --tw-bg-opacity:1; background-color: rgba(121,137,255,var(--tw-bg-opacity)); }

.bg-blue-500 { --tw-bg-opacity:1; background-color: rgba(69,89,164,var(--tw-bg-opacity)); }

.bg-blue-950 { --tw-bg-opacity:1; background-color: rgba(23,37,84,var(--tw-bg-opacity)); }

.bg-brand-green\/20 { background-color: rgba(25, 195, 125, 0.2); }

.bg-brand-green\/40 { background-color: rgba(25, 195, 125, 0.4); }

.bg-brand-purple { --tw-bg-opacity:1; background-color: rgba(171,104,255,var(--tw-bg-opacity)); }

.bg-gray-100 { --tw-bg-opacity:1; background-color: rgba(236,236,241,var(--tw-bg-opacity)); }

.bg-gray-200 { --tw-bg-opacity:1; background-color: rgba(217,217,227,var(--tw-bg-opacity)); }

.bg-gray-300 { --tw-bg-opacity:1; background-color: rgba(197,197,210,var(--tw-bg-opacity)); }

.bg-gray-300\/70 { background-color: rgba(197, 197, 210, 0.7); }

.bg-gray-400 { --tw-bg-opacity:1; background-color: rgba(172,172,190,var(--tw-bg-opacity)); }

.bg-gray-50 { --tw-bg-opacity:1; background-color: rgba(247,247,248,var(--tw-bg-opacity)); }

.bg-gray-500 { --tw-bg-opacity:1; background-color: rgba(142,142,160,var(--tw-bg-opacity)); }

.bg-gray-600 { --tw-bg-opacity:1; background-color: rgba(86,88,105,var(--tw-bg-opacity)); }

.bg-gray-700 { --tw-bg-opacity:1; background-color: rgba(64,65,79,var(--tw-bg-opacity)); }

.bg-gray-800 { --tw-bg-opacity:1; background-color: rgba(52,53,65,var(--tw-bg-opacity)); }

.bg-gray-900 { --tw-bg-opacity:1; background-color: rgba(32,33,35,var(--tw-bg-opacity)); }

.bg-green-100 { --tw-bg-opacity:1; background-color: rgba(210,244,211,var(--tw-bg-opacity)); }

.bg-green-200 { --tw-bg-opacity:1; background-color: rgba(185,238,188,var(--tw-bg-opacity)); }

.bg-green-500 { --tw-bg-opacity:1; background-color: rgba(25,195,125,var(--tw-bg-opacity)); }

.bg-green-500\/10 { background-color: rgba(25, 195, 125, 0.1); }

.bg-green-600 { --tw-bg-opacity:1; background-color: rgba(16,163,127,var(--tw-bg-opacity)); }

.bg-orange-500 { --tw-bg-opacity:1; background-color: rgba(224,108,43,var(--tw-bg-opacity)); }

.bg-orange-500\/10 { background-color: rgba(224, 108, 43, 0.1); }

.bg-red-100 { --tw-bg-opacity:1; background-color: rgba(254,226,226,var(--tw-bg-opacity)); }

.bg-red-200 { --tw-bg-opacity:1; background-color: rgba(254,202,202,var(--tw-bg-opacity)); }

.bg-red-500 { --tw-bg-opacity:1; background-color: rgba(239,68,68,var(--tw-bg-opacity)); }

.bg-red-500\/10 { background-color: rgba(239, 68, 68, 0.1); }

.bg-red-600 { --tw-bg-opacity:1; background-color: rgba(220,38,38,var(--tw-bg-opacity)); }

.bg-token-surface-primary { background-color: var(--surface-primary); }

.bg-token-surface-secondary { background-color: var(--surface-secondary); }

.bg-transparent { background-color: transparent; }

.bg-white { --tw-bg-opacity:1; background-color: rgba(255,255,255,var(--tw-bg-opacity)); }

.bg-white\/20 { background-color: rgba(255, 255, 255, 0.2); }

.bg-white\/25 { background-color: rgba(255, 255, 255, 0.25); }

.bg-white\/95 { background-color: rgba(255, 255, 255, 0.95); }

.bg-yellow-100 { --tw-bg-opacity:1; background-color: rgba(254,249,195,var(--tw-bg-opacity)); }

.bg-yellow-200 { --tw-bg-opacity:1; background-color: rgba(250,230,158,var(--tw-bg-opacity)); }

.bg-yellow-300 { --tw-bg-opacity:1; background-color: rgba(255,243,201,var(--tw-bg-opacity)); }

.bg-yellow-400 { --tw-bg-opacity:1; background-color: rgba(255,198,87,var(--tw-bg-opacity)); }

.bg-opacity-75 { --tw-bg-opacity:0.75; }

.bg-gradient-to-b { background-image: linear-gradient(to bottom,var(--tw-gradient-stops)); }

.bg-gradient-to-l { background-image: linear-gradient(to left,var(--tw-gradient-stops)); }

.bg-gradient-to-t { background-image: linear-gradient(to top,var(--tw-gradient-stops)); }

.from-gray-100 { --tw-gradient-from:#ececf1 var(--tw-gradient-from-position); --tw-gradient-to:rgba(236,236,241,0) var(--tw-gradient-to-position); --tw-gradient-stops:var(--tw-gradient-from),var(--tw-gradient-to); }

.from-gray-50 { --tw-gradient-from:#f7f7f8 var(--tw-gradient-from-position); --tw-gradient-to:hsla(240,7%,97%,0) var(--tw-gradient-to-position); --tw-gradient-stops:var(--tw-gradient-from),var(--tw-gradient-to); }

.from-gray-900\/0 { --tw-gradient-from:rgba(32,33,35,0) var(--tw-gradient-from-position); --tw-gradient-to:rgba(32,33,35,0) var(--tw-gradient-to-position); --tw-gradient-stops:var(--tw-gradient-from),var(--tw-gradient-to); }

.from-transparent { --tw-gradient-from:transparent var(--tw-gradient-from-position); --tw-gradient-to:transparent var(--tw-gradient-to-position); --tw-gradient-stops:var(--tw-gradient-from),var(--tw-gradient-to); }

.from-\[60\%\] { --tw-gradient-from-position:60%; }

.to-gray-100 { --tw-gradient-to:#ececf1 var(--tw-gradient-to-position); }

.to-gray-900 { --tw-gradient-to:#202123 var(--tw-gradient-to-position); }

.bg-contain { background-size: contain; }

.bg-cover { background-size: cover; }

.bg-center { background-position: 50% center; }

.bg-no-repeat { background-repeat: no-repeat; }

.fill-gray-800 { fill: rgb(52, 53, 65); }

.fill-yellow-500 { fill: rgb(234, 179, 8); }

.stroke-black { stroke: rgb(0, 0, 0); }

.stroke-gray-400 { stroke: rgb(172, 172, 190); }

.stroke-white { stroke: rgb(255, 255, 255); }

.stroke-0 { stroke-width: 0; }

.stroke-2 { stroke-width: 2; }

.stroke-\[2\.5\] { stroke-width: 2.5; }

.stroke-\[4\] { stroke-width: 4; }

.object-contain { object-fit: contain; }

.object-cover { object-fit: cover; }

.p-0 { padding: 0px; }

.p-0\.5 { padding: 0.125rem; }

.p-1 { padding: 0.25rem; }

.p-1\.5 { padding: 0.375rem; }

.p-2 { padding: 0.5rem; }

.p-3 { padding: 0.75rem; }

.p-4 { padding: 1rem; }

.p-5 { padding: 1.25rem; }

.p-6 { padding: 1.5rem; }

.p-7 { padding: 1.75rem; }

.p-\[3px\] { padding: 3px; }

.p-\[5px\] { padding: 5px; }

.\!px-1 { padding-left: 0.25rem !important; padding-right: 0.25rem !important; }

.\!px-1\.5 { padding-left: 0.375rem !important; padding-right: 0.375rem !important; }

.\!py-0 { padding-bottom: 0px !important; padding-top: 0px !important; }

.\!py-0\.5 { padding-bottom: 0.125rem !important; padding-top: 0.125rem !important; }

.px-0 { padding-left: 0px; padding-right: 0px; }

.px-0\.5 { padding-left: 0.125rem; padding-right: 0.125rem; }

.px-1 { padding-left: 0.25rem; padding-right: 0.25rem; }

.px-1\.5 { padding-left: 0.375rem; padding-right: 0.375rem; }

.px-10 { padding-left: 2.5rem; padding-right: 2.5rem; }

.px-2 { padding-left: 0.5rem; padding-right: 0.5rem; }

.px-2\.5 { padding-left: 0.625rem; padding-right: 0.625rem; }

.px-3 { padding-left: 0.75rem; padding-right: 0.75rem; }

.px-4 { padding-left: 1rem; padding-right: 1rem; }

.px-5 { padding-left: 1.25rem; padding-right: 1.25rem; }

.px-6 { padding-left: 1.5rem; padding-right: 1.5rem; }

.px-8 { padding-left: 2rem; padding-right: 2rem; }

.py-0 { padding-bottom: 0px; padding-top: 0px; }

.py-0\.5 { padding-bottom: 0.125rem; padding-top: 0.125rem; }

.py-1 { padding-bottom: 0.25rem; padding-top: 0.25rem; }

.py-1\.5 { padding-bottom: 0.375rem; padding-top: 0.375rem; }

.py-12 { padding-bottom: 3rem; padding-top: 3rem; }

.py-2 { padding-bottom: 0.5rem; padding-top: 0.5rem; }

.py-3 { padding-bottom: 0.75rem; padding-top: 0.75rem; }

.py-4 { padding-bottom: 1rem; padding-top: 1rem; }

.py-6 { padding-bottom: 1.5rem; padding-top: 1.5rem; }

.py-8 { padding-bottom: 2rem; padding-top: 2rem; }

.py-\[10px\] { padding-bottom: 10px; padding-top: 10px; }

.\!pr-2 { padding-right: 0.5rem !important; }

.\!pt-0 { padding-top: 0px !important; }

.pb-0 { padding-bottom: 0px; }

.pb-0\.5 { padding-bottom: 0.125rem; }

.pb-1 { padding-bottom: 0.25rem; }

.pb-1\.5 { padding-bottom: 0.375rem; }

.pb-10 { padding-bottom: 2.5rem; }

.pb-12 { padding-bottom: 3rem; }

.pb-16 { padding-bottom: 4rem; }

.pb-2 { padding-bottom: 0.5rem; }

.pb-3 { padding-bottom: 0.75rem; }

.pb-4 { padding-bottom: 1rem; }

.pb-5 { padding-bottom: 1.25rem; }

.pb-64 { padding-bottom: 16rem; }

.pb-8 { padding-bottom: 2rem; }

.pl-0 { padding-left: 0px; }

.pl-1 { padding-left: 0.25rem; }

.pl-10 { padding-left: 2.5rem; }

.pl-12 { padding-left: 3rem; }

.pl-2 { padding-left: 0.5rem; }

.pl-3 { padding-left: 0.75rem; }

.pl-4 { padding-left: 1rem; }

.pl-5 { padding-left: 1.25rem; }

.pl-6 { padding-left: 1.5rem; }

.pl-8 { padding-left: 2rem; }

.pr-1 { padding-right: 0.25rem; }

.pr-1\.5 { padding-right: 0.375rem; }

.pr-10 { padding-right: 2.5rem; }

.pr-11 { padding-right: 2.75rem; }

.pr-12 { padding-right: 3rem; }

.pr-14 { padding-right: 3.5rem; }

.pr-16 { padding-right: 4rem; }

.pr-2 { padding-right: 0.5rem; }

.pr-3 { padding-right: 0.75rem; }

.pr-4 { padding-right: 1rem; }

.pr-5 { padding-right: 1.25rem; }

.pr-6 { padding-right: 1.5rem; }

.pr-7 { padding-right: 1.75rem; }

.pr-9 { padding-right: 2.25rem; }

.pt-0 { padding-top: 0px; }

.pt-0\.5 { padding-top: 0.125rem; }

.pt-1 { padding-top: 0.25rem; }

.pt-2 { padding-top: 0.5rem; }

.pt-20 { padding-top: 5rem; }

.pt-3 { padding-top: 0.75rem; }

.pt-4 { padding-top: 1rem; }

.pt-5 { padding-top: 1.25rem; }

.pt-8 { padding-top: 2rem; }

.pt-\[36vh\] { padding-top: 36vh; }

.pt-\[67\%\] { padding-top: 67%; }

.text-left { text-align: left; }

.text-center { text-align: center; }

.text-right { text-align: right; }

.align-top { vertical-align: top; }

.align-middle { vertical-align: middle; }

.font-mono { font-family: "SÃ¶hne Mono", Monaco, "Andale Mono", "Ubuntu Mono", monospace; }

.font-sans { font-family: SÃ¶hne, ui-sans-serif, system-ui, -apple-system, "Segoe UI", Roboto, Ubuntu, Cantarell, "Noto Sans", sans-serif, "Helvetica Neue", Arial, "Apple Color Emoji", "Segoe UI Emoji", "Segoe UI Symbol", "Noto Color Emoji"; }

.text-2xl { font-size: 1.5rem; line-height: 2rem; }

.text-3xl { font-size: 1.875rem; line-height: 2.25rem; }

.text-4xl { font-size: 2.25rem; line-height: 2.5rem; }

.text-\[0\.8rem\] { font-size: 0.8rem; }

.text-\[10px\] { font-size: 10px; }

.text-\[13px\] { font-size: 13px; }

.text-\[20px\] { font-size: 20px; }

.text-\[28px\] { font-size: 28px; }

.text-\[32px\] { font-size: 32px; }

.text-base { font-size: 1rem; line-height: 1.5rem; }

.text-lg { font-size: 1.125rem; line-height: 1.75rem; }

.text-sm { font-size: 0.875rem; line-height: 1.25rem; }

.text-xl { font-size: 1.25rem; line-height: 1.75rem; }

.text-xs { font-size: 0.75rem; line-height: 1rem; }

.font-bold { font-weight: 700; }

.font-light { font-weight: 300; }

.font-medium { font-weight: 500; }

.font-normal { font-weight: 400; }

.font-semibold { font-weight: 600; }

.uppercase { text-transform: uppercase; }

.capitalize { text-transform: capitalize; }

.normal-case { text-transform: none; }

.italic { font-style: italic; }

.tabular-nums { --tw-numeric-spacing:tabular-nums; font-variant-numeric: var(--tw-ordinal) var(--tw-slashed-zero) var(--tw-numeric-figure) var(--tw-numeric-spacing) var(--tw-numeric-fraction); }

.leading-3 { line-height: 0.75rem; }

.leading-5 { line-height: 1.25rem; }

.leading-6 { line-height: 1.5rem; }

.leading-\[1\.2\] { line-height: 1.2; }

.leading-\[18px\] { line-height: 18px; }

.leading-none { line-height: 1; }

.leading-normal { line-height: 1.5; }

.leading-tight { line-height: 1.25; }

.tracking-tight { letter-spacing: -0.025em; }

.tracking-widest { letter-spacing: 0.1em; }

.\!text-gray-500 { --tw-text-opacity:1 !important; color: rgba(142,142,160,var(--tw-text-opacity)) !important; }

.text-\[\#756B5C\] { --tw-text-opacity:1; color: rgba(117,107,92,var(--tw-text-opacity)); }

.text-\[\#800080\] { --tw-text-opacity:1; color: rgba(128,0,128,var(--tw-text-opacity)); }

.text-\[\#FE7600\] { --tw-text-opacity:1; color: rgba(254,118,0,var(--tw-text-opacity)); }

.text-\[\#cdcdcd\] { --tw-text-opacity:1; color: rgba(205,205,205,var(--tw-text-opacity)); }

.text-\[\#fff\] { --tw-text-opacity:1; color: rgba(255,255,255,var(--tw-text-opacity)); }

.text-black { --tw-text-opacity:1; color: rgba(0,0,0,var(--tw-text-opacity)); }

.text-black\/50 { color: rgba(0, 0, 0, 0.5); }

.text-black\/60 { color: rgba(0, 0, 0, 0.6); }

.text-black\/70 { color: rgba(0, 0, 0, 0.7); }

.text-blue-400 { --tw-text-opacity:1; color: rgba(58,136,255,var(--tw-text-opacity)); }

.text-blue-500 { --tw-text-opacity:1; color: rgba(69,89,164,var(--tw-text-opacity)); }

.text-blue-600 { --tw-text-opacity:1; color: rgba(37,99,235,var(--tw-text-opacity)); }

.text-blue-700 { --tw-text-opacity:1; color: rgba(29,78,216,var(--tw-text-opacity)); }

.text-brand-green { --tw-text-opacity:1; color: rgba(25,195,125,var(--tw-text-opacity)); }

.text-brand-purple { --tw-text-opacity:1; color: rgba(171,104,255,var(--tw-text-opacity)); }

.text-gray-100 { --tw-text-opacity:1; color: rgba(236,236,241,var(--tw-text-opacity)); }

.text-gray-200 { --tw-text-opacity:1; color: rgba(217,217,227,var(--tw-text-opacity)); }

.text-gray-300 { --tw-text-opacity:1; color: rgba(197,197,210,var(--tw-text-opacity)); }

.text-gray-400 { --tw-text-opacity:1; color: rgba(172,172,190,var(--tw-text-opacity)); }

.text-gray-50 { --tw-text-opacity:1; color: rgba(247,247,248,var(--tw-text-opacity)); }

.text-gray-500 { --tw-text-opacity:1; color: rgba(142,142,160,var(--tw-text-opacity)); }

.text-gray-600 { --tw-text-opacity:1; color: rgba(86,88,105,var(--tw-text-opacity)); }

.text-gray-600\/50 { color: rgba(86, 88, 105, 0.5); }

.text-gray-700 { --tw-text-opacity:1; color: rgba(64,65,79,var(--tw-text-opacity)); }

.text-gray-800 { --tw-text-opacity:1; color: rgba(52,53,65,var(--tw-text-opacity)); }

.text-gray-900 { --tw-text-opacity:1; color: rgba(32,33,35,var(--tw-text-opacity)); }

.text-green-500 { --tw-text-opacity:1; color: rgba(25,195,125,var(--tw-text-opacity)); }

.text-green-600 { --tw-text-opacity:1; color: rgba(16,163,127,var(--tw-text-opacity)); }

.text-green-700 { --tw-text-opacity:1; color: rgba(26,127,100,var(--tw-text-opacity)); }

.text-orange-300 { --tw-text-opacity:1; color: rgba(255,160,69,var(--tw-text-opacity)); }

.text-orange-400 { --tw-text-opacity:1; color: rgba(255,128,82,var(--tw-text-opacity)); }

.text-orange-500 { --tw-text-opacity:1; color: rgba(224,108,43,var(--tw-text-opacity)); }

.text-red-400 { --tw-text-opacity:1; color: rgba(248,113,113,var(--tw-text-opacity)); }

.text-red-500 { --tw-text-opacity:1; color: rgba(239,68,68,var(--tw-text-opacity)); }

.text-red-600 { --tw-text-opacity:1; color: rgba(220,38,38,var(--tw-text-opacity)); }

.text-red-700 { --tw-text-opacity:1; color: rgba(185,28,28,var(--tw-text-opacity)); }

.text-red-800 { --tw-text-opacity:1; color: rgba(153,27,27,var(--tw-text-opacity)); }

.text-token-text-primary { color: var(--text-primary); }

.text-token-text-secondary { color: var(--text-secondary); }

.text-token-text-tertiary { color: var(--text-tertiary); }

.text-white { --tw-text-opacity:1; color: rgba(255,255,255,var(--tw-text-opacity)); }

.text-white\/25 { color: rgba(255, 255, 255, 0.25); }

.text-white\/50 { color: rgba(255, 255, 255, 0.5); }

.text-white\/80 { color: rgba(255, 255, 255, 0.8); }

.text-yellow-500 { --tw-text-opacity:1; color: rgba(234,179,8,var(--tw-text-opacity)); }

.text-yellow-700 { --tw-text-opacity:1; color: rgba(161,98,7,var(--tw-text-opacity)); }

.text-yellow-800 { --tw-text-opacity:1; color: rgba(133,77,14,var(--tw-text-opacity)); }

.text-yellow-900 { --tw-text-opacity:1; color: rgba(146,114,1,var(--tw-text-opacity)); }

.underline { text-decoration-line: underline; }

.line-through { text-decoration-line: line-through; }

.\!no-underline { text-decoration-line: none !important; }

.antialiased { -webkit-font-smoothing: antialiased; }

.placeholder-gray-500::-webkit-input-placeholder { --tw-placeholder-opacity:1; color: rgba(142,142,160,var(--tw-placeholder-opacity)); }

.placeholder-gray-500::placeholder { --tw-placeholder-opacity:1; color: rgba(142,142,160,var(--tw-placeholder-opacity)); }

.opacity-0 { opacity: 0; }

.opacity-100 { opacity: 1; }

.opacity-20 { opacity: 0.2; }

.opacity-25 { opacity: 0.25; }

.opacity-50 { opacity: 0.5; }

.opacity-70 { opacity: 0.7; }

.opacity-75 { opacity: 0.75; }

.opacity-80 { opacity: 0.8; }

.\!shadow-none { --tw-shadow:0 0 transparent !important; --tw-shadow-colored:0 0 transparent !important; box-shadow: var(--tw-ring-offset-shadow,0 0 transparent),var(--tw-ring-shadow,0 0 transparent),var(--tw-shadow) !important; }

.shadow { --tw-shadow:0 1px 3px 0 rgba(0,0,0,0.1),0 1px 2px -1px rgba(0,0,0,0.1); --tw-shadow-colored:0 1px 3px 0 var(--tw-shadow-color),0 1px 2px -1px var(--tw-shadow-color); }

.shadow, .shadow-\[0_1px_1px_0_rgba\(0\,0\,0\,0\.08\)\] { box-shadow: var(--tw-ring-offset-shadow,0 0 transparent),var(--tw-ring-shadow,0 0 transparent),var(--tw-shadow); }

.shadow-\[0_1px_1px_0_rgba\(0\,0\,0\,0\.08\)\] { --tw-shadow:0 1px 1px 0 rgba(0,0,0,0.08); --tw-shadow-colored:0 1px 1px 0 var(--tw-shadow-color); }

.shadow-\[0_1px_2px_rgba\(0\,0\,0\,0\.45\)\] { --tw-shadow:0 1px 2px rgba(0,0,0,0.45); --tw-shadow-colored:0 1px 2px var(--tw-shadow-color); }

.shadow-\[0_1px_2px_rgba\(0\,0\,0\,0\.45\)\], .shadow-\[0_1px_7px_0_rgba\(0\,0\,0\,0\.03\)\] { box-shadow: var(--tw-ring-offset-shadow,0 0 transparent),var(--tw-ring-shadow,0 0 transparent),var(--tw-shadow); }

.shadow-\[0_1px_7px_0_rgba\(0\,0\,0\,0\.03\)\] { --tw-shadow:0 1px 7px 0 rgba(0,0,0,0.03); --tw-shadow-colored:0 1px 7px 0 var(--tw-shadow-color); }

.shadow-\[0_1px_7px_0px_rgba\(0\,0\,0\,0\.06\)\] { --tw-shadow:0 1px 7px 0px rgba(0,0,0,0.06); --tw-shadow-colored:0 1px 7px 0px var(--tw-shadow-color); }

.shadow-\[0_1px_7px_0px_rgba\(0\,0\,0\,0\.06\)\], .shadow-\[0_2px_12px_0px_rgba\(0\,0\,0\,0\.08\)\] { box-shadow: var(--tw-ring-offset-shadow,0 0 transparent),var(--tw-ring-shadow,0 0 transparent),var(--tw-shadow); }

.shadow-\[0_2px_12px_0px_rgba\(0\,0\,0\,0\.08\)\] { --tw-shadow:0 2px 12px 0px rgba(0,0,0,0.08); --tw-shadow-colored:0 2px 12px 0px var(--tw-shadow-color); }

.shadow-\[0_2px_24px_rgba\(0\,0\,0\,0\.05\)\] { --tw-shadow:0 2px 24px rgba(0,0,0,0.05); --tw-shadow-colored:0 2px 24px var(--tw-shadow-color); }

.shadow-\[0_2px_24px_rgba\(0\,0\,0\,0\.05\)\], .shadow-\[0px_1px_6px_0px_rgba\(0\,0\,0\,0\.02\)\] { box-shadow: var(--tw-ring-offset-shadow,0 0 transparent),var(--tw-ring-shadow,0 0 transparent),var(--tw-shadow); }

.shadow-\[0px_1px_6px_0px_rgba\(0\,0\,0\,0\.02\)\] { --tw-shadow:0px 1px 6px 0px rgba(0,0,0,0.02); --tw-shadow-colored:0px 1px 6px 0px var(--tw-shadow-color); }

.shadow-\[0px_4px_14px_rgba\(0\,0\,0\,0\.06\)\] { --tw-shadow:0px 4px 14px rgba(0,0,0,0.06); --tw-shadow-colored:0px 4px 14px var(--tw-shadow-color); }

.shadow-\[0px_4px_14px_rgba\(0\,0\,0\,0\.06\)\], .shadow-\[inset_0_0_1px_rgba\(0\,0\,0\,0\.1\)\] { box-shadow: var(--tw-ring-offset-shadow,0 0 transparent),var(--tw-ring-shadow,0 0 transparent),var(--tw-shadow); }

.shadow-\[inset_0_0_1px_rgba\(0\,0\,0\,0\.1\)\] { --tw-shadow:inset 0 0 1px rgba(0,0,0,0.1); --tw-shadow-colored:inset 0 0 1px var(--tw-shadow-color); }

.shadow-lg { --tw-shadow:0 10px 15px -3px rgba(0,0,0,0.1),0 4px 6px -4px rgba(0,0,0,0.1); --tw-shadow-colored:0 10px 15px -3px var(--tw-shadow-color),0 4px 6px -4px var(--tw-shadow-color); }

.shadow-lg, .shadow-md { box-shadow: var(--tw-ring-offset-shadow,0 0 transparent),var(--tw-ring-shadow,0 0 transparent),var(--tw-shadow); }

.shadow-md { --tw-shadow:0 4px 6px -1px rgba(0,0,0,0.1),0 2px 4px -2px rgba(0,0,0,0.1); --tw-shadow-colored:0 4px 6px -1px var(--tw-shadow-color),0 2px 4px -2px var(--tw-shadow-color); }

.shadow-none { --tw-shadow:0 0 transparent; --tw-shadow-colored:0 0 transparent; }

.shadow-none, .shadow-sm { box-shadow: var(--tw-ring-offset-shadow,0 0 transparent),var(--tw-ring-shadow,0 0 transparent),var(--tw-shadow); }

.shadow-sm { --tw-shadow:0 1px 2px 0 rgba(0,0,0,0.05); --tw-shadow-colored:0 1px 2px 0 var(--tw-shadow-color); }

.shadow-xl { --tw-shadow:0 20px 25px -5px rgba(0,0,0,0.1),0 8px 10px -6px rgba(0,0,0,0.1); --tw-shadow-colored:0 20px 25px -5px var(--tw-shadow-color),0 8px 10px -6px var(--tw-shadow-color); }

.shadow-xl, .shadow-xs { box-shadow: var(--tw-ring-offset-shadow,0 0 transparent),var(--tw-ring-shadow,0 0 transparent),var(--tw-shadow); }

.shadow-xs { --tw-shadow:0 0 15px rgba(0,0,0,0.1); --tw-shadow-colored:0 0 15px var(--tw-shadow-color); }

.shadow-xxs { --tw-shadow:0 1px 7px 0 rgba(0,0,0,0.03); --tw-shadow-colored:0 1px 7px 0 var(--tw-shadow-color); box-shadow: var(--tw-ring-offset-shadow,0 0 transparent),var(--tw-ring-shadow,0 0 transparent),var(--tw-shadow); }

.outline-none { outline: transparent solid 2px; outline-offset: 2px; }

.outline { outline-style: solid; }

.outline-0 { outline-width: 0px; }

.outline-2 { outline-width: 2px; }

.outline-offset-2 { outline-offset: 2px; }

.ring-0 { --tw-ring-offset-shadow:var(--tw-ring-inset) 0 0 0 var(--tw-ring-offset-width) var(--tw-ring-offset-color); --tw-ring-shadow:var(--tw-ring-inset) 0 0 0 calc(var(--tw-ring-offset-width)) var(--tw-ring-color); }

.ring-0, .ring-1 { box-shadow: var(--tw-ring-offset-shadow),var(--tw-ring-shadow),var(--tw-shadow,0 0 transparent); }

.ring-1 { --tw-ring-offset-shadow:var(--tw-ring-inset) 0 0 0 var(--tw-ring-offset-width) var(--tw-ring-offset-color); --tw-ring-shadow:var(--tw-ring-inset) 0 0 0 calc(1px + var(--tw-ring-offset-width)) var(--tw-ring-color); }

.ring-2 { --tw-ring-offset-shadow:var(--tw-ring-inset) 0 0 0 var(--tw-ring-offset-width) var(--tw-ring-offset-color); --tw-ring-shadow:var(--tw-ring-inset) 0 0 0 calc(2px + var(--tw-ring-offset-width)) var(--tw-ring-color); box-shadow: var(--tw-ring-offset-shadow),var(--tw-ring-shadow),var(--tw-shadow,0 0 transparent); }

.ring-inset { --tw-ring-inset:inset; }

.ring-black { --tw-ring-opacity:1; --tw-ring-color:rgba(0,0,0,var(--tw-ring-opacity)); }

.ring-black\/10 { --tw-ring-color:rgba(0,0,0,0.1); }

.ring-white { --tw-ring-opacity:1; --tw-ring-color:rgba(255,255,255,var(--tw-ring-opacity)); }

.ring-opacity-5 { --tw-ring-opacity:0.05; }

.ring-offset-4 { --tw-ring-offset-width:4px; }

.ring-offset-black { --tw-ring-offset-color:#000; }

.blur { --tw-blur:blur(8px); }

.blur, .blur-xl { filter: var(--tw-blur) var(--tw-brightness) var(--tw-contrast) var(--tw-grayscale) var(--tw-hue-rotate) var(--tw-invert) var(--tw-saturate) var(--tw-sepia) var(--tw-drop-shadow); }

.blur-xl { --tw-blur:blur(24px); }

.filter { filter: var(--tw-blur) var(--tw-brightness) var(--tw-contrast) var(--tw-grayscale) var(--tw-hue-rotate) var(--tw-invert) var(--tw-saturate) var(--tw-sepia) var(--tw-drop-shadow); }

.backdrop-blur-sm { --tw-backdrop-blur:blur(4px); }

.backdrop-blur-sm, .backdrop-blur-xl { backdrop-filter: var(--tw-backdrop-blur) var(--tw-backdrop-brightness) var(--tw-backdrop-contrast) var(--tw-backdrop-grayscale) var(--tw-backdrop-hue-rotate) var(--tw-backdrop-invert) var(--tw-backdrop-opacity) var(--tw-backdrop-saturate) var(--tw-backdrop-sepia); }

.backdrop-blur-xl { --tw-backdrop-blur:blur(24px); }

.transition { transition-duration: 0.15s; transition-property: color, background-color, border-color, text-decoration-color, fill, stroke, opacity, box-shadow, transform, filter, backdrop-filter, -webkit-transform, -webkit-filter, -webkit-backdrop-filter; transition-timing-function: cubic-bezier(0.4, 0, 0.2, 1); }

.transition-\[flex-basis\] { transition-duration: 0.15s; transition-property: flex-basis; transition-timing-function: cubic-bezier(0.4, 0, 0.2, 1); }

.transition-\[stroke-dashoffset\] { transition-duration: 0.15s; transition-property: stroke-dashoffset; transition-timing-function: cubic-bezier(0.4, 0, 0.2, 1); }

.transition-\[transform\,opacity\] { transition-duration: 0.15s; transition-property: transform, opacity, -webkit-transform; transition-timing-function: cubic-bezier(0.4, 0, 0.2, 1); }

.transition-all { transition-duration: 0.15s; transition-property: all; transition-timing-function: cubic-bezier(0.4, 0, 0.2, 1); }

.transition-colors { transition-duration: 0.15s; transition-property: color, background-color, border-color, text-decoration-color, fill, stroke; transition-timing-function: cubic-bezier(0.4, 0, 0.2, 1); }

.transition-opacity { transition-duration: 0.15s; transition-property: opacity; transition-timing-function: cubic-bezier(0.4, 0, 0.2, 1); }

.transition-transform { transition-duration: 0.15s; transition-property: transform, -webkit-transform; transition-timing-function: cubic-bezier(0.4, 0, 0.2, 1); }

.transition-width { transition-duration: 0.15s; transition-property: width; transition-timing-function: cubic-bezier(0.4, 0, 0.2, 1); }

.delay-100 { transition-delay: 0.1s; }

.delay-\[300ms\] { transition-delay: 0.3s; }

.delay-\[500ms\] { transition-delay: 0.5s; }

.duration-100 { transition-duration: 0.1s; }

.duration-1000 { transition-duration: 1s; }

.duration-150 { transition-duration: 0.15s; }

.duration-200 { transition-duration: 0.2s; }

.duration-300 { transition-duration: 0.3s; }

.duration-500 { transition-duration: 0.5s; }

.ease-in { transition-timing-function: cubic-bezier(0.4, 0, 1, 1); }

.ease-in-out { transition-timing-function: cubic-bezier(0.4, 0, 0.2, 1); }

.ease-linear { transition-timing-function: linear; }

.ease-out { transition-timing-function: cubic-bezier(0, 0, 0.2, 1); }

.will-change-\[opacity\,transform\] { will-change: opacity, transform; }

.will-change-transform { will-change: transform; }

body, html { height: 100%; }

.dark body, .dark html { --tw-bg-opacity:1; background-color: rgba(52,53,65,var(--tw-bg-opacity)); }

#__next, #root { height: 100%; }

.markdown { max-width: none; }

.markdown h1, .markdown h2 { font-weight: 600; }

.markdown h2 { margin-bottom: 1rem; margin-top: 2rem; }

.markdown h3 { font-weight: 600; }

.markdown h3, .markdown h4 { margin-bottom: 0.5rem; margin-top: 1rem; }

.markdown h4 { font-weight: 400; }

.markdown h5 { font-weight: 600; }

.markdown blockquote { --tw-border-opacity:1; border-color: rgba(142,142,160,var(--tw-border-opacity)); border-left-width: 2px; line-height: 1.5rem; margin: 0px; padding-bottom: 0.5rem; padding-left: 1rem; padding-top: 0.5rem; }

.markdown blockquote > p { margin: 0px; }

.markdown blockquote > p::after, .markdown blockquote > p::before { display: none; }

.markdown ol { counter-reset: list-number 0; display: flex; flex-direction: column; list-style-type: none; padding-left: 0px; }

.markdown ol > li { counter-increment: list-number 1; display: block; margin-bottom: 0px; margin-top: 0px; min-height: 28px; }

.markdown ol > li::before { --tw-translate-x:-100%; --tw-numeric-spacing:tabular-nums; --tw-text-opacity:1; color: rgba(142,142,160,var(--tw-text-opacity)); content: counters(list-number, ".") "."; font-variant-numeric: var(--tw-ordinal) var(--tw-slashed-zero) var(--tw-numeric-figure) var(--tw-numeric-spacing) var(--tw-numeric-fraction); padding-right: 0.5rem; position: absolute; transform: translate(var(--tw-translate-x),var(--tw-translate-y)) rotate(var(--tw-rotate)) skewX(var(--tw-skew-x)) skewY(var(--tw-skew-y)) scaleX(var(--tw-scale-x)) scaleY(var(--tw-scale-y)); }

:is(.dark .markdown ol > li)::before { --tw-text-opacity:1; color: rgba(172,172,190,var(--tw-text-opacity)); }

.markdown ul { display: flex; flex-direction: column; padding-left: 0px; }

.markdown ul > li { display: block; margin: 0px; min-height: 28px; }

.markdown ul > li::before { --tw-translate-x:-100%; content: "â¢"; font-size: 0.875rem; line-height: 1.25rem; padding-right: 0.5rem; position: absolute; transform: translate(var(--tw-translate-x),var(--tw-translate-y)) rotate(var(--tw-rotate)) skewX(var(--tw-skew-x)) skewY(var(--tw-skew-y)) scaleX(var(--tw-scale-x)) scaleY(var(--tw-scale-y)); }

.markdown ol li > p, .markdown ol li > pre, .markdown ul li > p, .markdown ul li > pre { margin: 0px; }

.markdown ol ol, .markdown ol ul, .markdown ul ol, .markdown ul ul { margin-bottom: 0px; margin-left: 1rem; margin-top: 0px; }

.markdown table { --tw-border-spacing-x:0px; --tw-border-spacing-y:0px; border-collapse: separate; border-spacing: var(--tw-border-spacing-x) var(--tw-border-spacing-y); width: 100%; }

.markdown th { background-color: rgba(236, 236, 241, 0.2); border-bottom-width: 1px; border-left-width: 1px; border-top-width: 1px; padding: 0.25rem 0.75rem; }

.markdown th:first-child { border-top-left-radius: 0.375rem; }

.markdown th:last-child { border-right-width: 1px; border-top-right-radius: 0.375rem; }

.markdown td { border-bottom-width: 1px; border-left-width: 1px; padding: 0.25rem 0.75rem; }

.markdown td:last-child { border-right-width: 1px; }

.markdown tbody tr:last-child td:first-child { border-bottom-left-radius: 0.375rem; }

.markdown tbody tr:last-child td:last-child { border-bottom-right-radius: 0.375rem; }

.markdown a { text-decoration-line: underline; text-underline-offset: 2px; }

.conversation-item-time::before { content: attr(data-time); }

button.scroll-convo { display: none; }

@-webkit-keyframes blink { 
  100% { visibility: hidden; }
}

@keyframes blink { 
  100% { visibility: hidden; }
}

.animate-flash { animation: 2s steps(60, start) 0s 1 normal none running flash; }

@-webkit-keyframes flash { 
  0% { background-color: rgba(255, 255, 255, 0.4); }
}

@keyframes flash { 
  0% { background-color: rgba(255, 255, 255, 0.4); }
}

.result-streaming > :not(ol):not(ul):not(pre):last-child::after, .result-streaming > pre:last-child code::after { animation: 1s steps(5, start) 0s infinite normal none running blink; content: "â"; margin-left: 0.25rem; vertical-align: baseline; }

@supports (selector(:has(*))) {
  .result-streaming > ol:last-child > li:last-child > ol:last-child > li:last-child::after, .result-streaming > ol:last-child > li:last-child > ol:last-child > li:last-child > ol:last-child > li:last-child::after, .result-streaming > ol:last-child > li:last-child > ol:last-child > li:last-child > ul:last-child > li:last-child::after, .result-streaming > ol:last-child > li:last-child > ul:last-child > li:last-child::after, .result-streaming > ol:last-child > li:last-child > ul:last-child > li:last-child > ol:last-child > li:last-child::after, .result-streaming > ol:last-child > li:last-child > ul:last-child > li:last-child > ul:last-child > li:last-child::after, .result-streaming > ul:last-child > li:last-child > ol:last-child > li:last-child::after, .result-streaming > ul:last-child > li:last-child > ol:last-child > li:last-child > ol:last-child > li:last-child::after, .result-streaming > ul:last-child > li:last-child > ol:last-child > li:last-child > ul:last-child > li:last-child::after, .result-streaming > ul:last-child > li:last-child > ul:last-child > li:last-child::after, .result-streaming > ul:last-child > li:last-child > ul:last-child > li:last-child > ol:last-child > li:last-child::after, .result-streaming > ul:last-child > li:last-child > ul:last-child > li:last-child > ul:last-child > li:last-child::after, .result-streaming > ul:last-child > li:last-child[*|\:not-has\(]::after { animation: 1s steps(5, start) 0s infinite normal none running blink; content: "â"; margin-left: 0.25rem; vertical-align: baseline; }
  .result-streaming > ul:last-child > li:last-child:not(:has(* > li))::after { animation: 1s steps(5, start) 0s infinite normal none running blink; content: "â"; margin-left: 0.25rem; vertical-align: baseline; }
  .result-streaming > ol:last-child > li:last-child[*|\:not-has\(]::after { animation: 1s steps(5, start) 0s infinite normal none running blink; content: "â"; margin-left: 0.25rem; vertical-align: baseline; }
  .result-streaming > ol:last-child > li:last-child:not(:has(* > li))::after { animation: 1s steps(5, start) 0s infinite normal none running blink; content: "â"; margin-left: 0.25rem; vertical-align: baseline; }
}

@supports not (selector(:has(*))) {
  .result-streaming > ol:last-child > li:last-child::after, .result-streaming > ul:last-child > li:last-child::after { animation: 1s steps(5, start) 0s infinite normal none running blink; content: "â"; margin-left: 0.25rem; vertical-align: baseline; }
}

@-webkit-keyframes toast-open { 
  0% { opacity: 0; transform: translateY(-100%); }
  100% { transform: translateY(0px); }
}

@keyframes toast-open { 
  0% { opacity: 0; transform: translateY(-100%); }
  100% { transform: translateY(0px); }
}

@-webkit-keyframes toast-close { 
  0% { opacity: 1; }
  100% { opacity: 0; }
}

@keyframes toast-close { 
  0% { opacity: 1; }
  100% { opacity: 0; }
}

button[data-state="closed"] .toggle-item-button-closed, button[data-state="open"] .toggle-item-button-open { display: none; }

.toast-root { align-items: center; display: flex; flex-direction: column; height: 0px; transition: all 0.24s cubic-bezier(0, 0, 0.2, 1) 0s; }

.toast-root[data-state="entered"], .toast-root[data-state="entering"] { animation: 0.24s cubic-bezier(0.175, 0.885, 0.32, 1.175) 0s 1 normal both running toast-open; }

.toast-root[data-state="exiting"] { animation: 0.12s cubic-bezier(0.4, 0, 1, 1) 0s 1 normal both running toast-close; }

.toast-root .alert-root { box-shadow: rgba(67, 90, 111, 0.3) 0px 0px 1px, rgba(67, 90, 111, 0.3) 0px 5px 8px -4px; flex-shrink: 0; pointer-events: all; }

.interact-bounce { transition-duration: 0.15s; transition-property: transform, -webkit-transform; transition-timing-function: cubic-bezier(0.4, 0, 0.2, 1); }

.interact-bounce:hover { --tw-scale-x:1.05; --tw-scale-y:1.05; }

.interact-bounce:active, .interact-bounce:hover { transform: translate(var(--tw-translate-x),var(--tw-translate-y)) rotate(var(--tw-rotate)) skewX(var(--tw-skew-x)) skewY(var(--tw-skew-y)) scaleX(var(--tw-scale-x)) scaleY(var(--tw-scale-y)); }

.interact-bounce:active { --tw-scale-x:0.9; --tw-scale-y:0.9; }

.title { font-feature-settings: normal; font-family: SÃ¶hne, ui-sans-serif, system-ui, -apple-system, "Segoe UI", Roboto, Ubuntu, Cantarell, "Noto Sans", sans-serif, "Helvetica Neue", Arial, "Apple Color Emoji", "Segoe UI Emoji", "Segoe UI Symbol", "Noto Color Emoji"; font-variation-settings: normal; }

.font-circle { font-family: "SÃ¶hne Circle", system-ui, -apple-system, "Segoe UI", Roboto, Ubuntu, Cantarell, "Noto Sans", sans-serif; }

.icon-xs { stroke-width: 1.5; height: 0.75rem; width: 0.75rem; }

.icon-sm { stroke-width: 2; height: 1rem; width: 1rem; }

.icon-md { stroke-width: 1.5; height: 1.25rem; width: 1.25rem; }

.icon-lg { stroke-width: 1.5; height: 1.5rem; width: 1.5rem; }

.icon-xl { stroke-width: 1.5; height: 1.75rem; width: 1.75rem; }

.icon-2xl { stroke-width: 1.5; height: 2rem; width: 2rem; }

.dark .dark\:prose-invert { --tw-prose-body:var(--tw-prose-invert-body); --tw-prose-headings:var(--tw-prose-invert-headings); --tw-prose-lead:var(--tw-prose-invert-lead); --tw-prose-links:var(--tw-prose-invert-links); --tw-prose-bold:var(--tw-prose-invert-bold); --tw-prose-counters:var(--tw-prose-invert-counters); --tw-prose-bullets:var(--tw-prose-invert-bullets); --tw-prose-hr:var(--tw-prose-invert-hr); --tw-prose-quotes:var(--tw-prose-invert-quotes); --tw-prose-quote-borders:var(--tw-prose-invert-quote-borders); --tw-prose-captions:var(--tw-prose-invert-captions); --tw-prose-code:var(--tw-prose-invert-code); --tw-prose-pre-code:var(--tw-prose-invert-pre-code); --tw-prose-pre-bg:var(--tw-prose-invert-pre-bg); --tw-prose-th-borders:var(--tw-prose-invert-th-borders); --tw-prose-td-borders:var(--tw-prose-invert-td-borders); }

.placeholder\:text-gray-300::-webkit-input-placeholder { --tw-text-opacity:1; color: rgba(197,197,210,var(--tw-text-opacity)); }

.placeholder\:text-gray-300::placeholder { --tw-text-opacity:1; color: rgba(197,197,210,var(--tw-text-opacity)); }

.placeholder\:text-gray-500::-webkit-input-placeholder { --tw-text-opacity:1; color: rgba(142,142,160,var(--tw-text-opacity)); }

.placeholder\:text-gray-500::placeholder { --tw-text-opacity:1; color: rgba(142,142,160,var(--tw-text-opacity)); }

.placeholder\:text-gray-600::-webkit-input-placeholder { --tw-text-opacity:1; color: rgba(86,88,105,var(--tw-text-opacity)); }

.placeholder\:text-gray-600::placeholder { --tw-text-opacity:1; color: rgba(86,88,105,var(--tw-text-opacity)); }

.after\:content-\[\'\"\'\]::after, .before\:content-\[\'\"\'\]::before { --tw-content:"\""; content: var(--tw-content); }

.last\:mb-0:last-child { margin-bottom: 0px; }

.last\:mb-2:last-child { margin-bottom: 0.5rem; }

.last\:border-0:last-child { border-width: 0px; }

.last\:border-b-0:last-child { border-bottom-width: 0px; }

.last\:pr-0:last-child { padding-right: 0px; }

.last-of-type\:border-b-0:last-of-type { border-bottom-width: 0px; }

.checked\:border-blue-600:checked { --tw-border-opacity:1; border-color: rgba(37,99,235,var(--tw-border-opacity)); }

.checked\:bg-blue-600:checked { --tw-bg-opacity:1; background-color: rgba(37,99,235,var(--tw-bg-opacity)); }

.empty\:hidden:empty { display: none; }

.focus-within\:z-10[focus-within] { z-index: 10; }

.focus-within\:z-10:focus-within { z-index: 10; }

.focus-within\:border-\[rgba\(0\,0\,0\,\.1\)\][focus-within] { border-color: rgba(0, 0, 0, 0.1); }

.focus-within\:border-\[rgba\(0\,0\,0\,\.1\)\]:focus-within { border-color: rgba(0, 0, 0, 0.1); }

.focus-within\:border-indigo-600[focus-within] { --tw-border-opacity:1; border-color: rgba(79,70,229,var(--tw-border-opacity)); }

.focus-within\:border-indigo-600:focus-within { --tw-border-opacity:1; border-color: rgba(79,70,229,var(--tw-border-opacity)); }

.focus-within\:shadow-\[0_3px_3px_0_rgba\(0\,0\,0\,0\.05\)\][focus-within] { --tw-shadow:0 3px 3px 0 rgba(0,0,0,0.05); --tw-shadow-colored:0 3px 3px 0 var(--tw-shadow-color); box-shadow: var(--tw-ring-offset-shadow,0 0 transparent),var(--tw-ring-shadow,0 0 transparent),var(--tw-shadow); }

.focus-within\:shadow-\[0_3px_3px_0_rgba\(0\,0\,0\,0\.05\)\]:focus-within { --tw-shadow:0 3px 3px 0 rgba(0,0,0,0.05); --tw-shadow-colored:0 3px 3px 0 var(--tw-shadow-color); box-shadow: var(--tw-ring-offset-shadow,0 0 transparent),var(--tw-ring-shadow,0 0 transparent),var(--tw-shadow); }

.focus-within\:ring-1[focus-within] { --tw-ring-offset-shadow:var(--tw-ring-inset) 0 0 0 var(--tw-ring-offset-width) var(--tw-ring-offset-color); --tw-ring-shadow:var(--tw-ring-inset) 0 0 0 calc(1px + var(--tw-ring-offset-width)) var(--tw-ring-color); box-shadow: var(--tw-ring-offset-shadow),var(--tw-ring-shadow),var(--tw-shadow,0 0 transparent); }

.focus-within\:ring-1:focus-within { --tw-ring-offset-shadow:var(--tw-ring-inset) 0 0 0 var(--tw-ring-offset-width) var(--tw-ring-offset-color); --tw-ring-shadow:var(--tw-ring-inset) 0 0 0 calc(1px + var(--tw-ring-offset-width)) var(--tw-ring-color); box-shadow: var(--tw-ring-offset-shadow),var(--tw-ring-shadow),var(--tw-shadow,0 0 transparent); }

.focus-within\:ring-indigo-600[focus-within] { --tw-ring-opacity:1; --tw-ring-color:rgba(79,70,229,var(--tw-ring-opacity)); }

.focus-within\:ring-indigo-600:focus-within { --tw-ring-opacity:1; --tw-ring-color:rgba(79,70,229,var(--tw-ring-opacity)); }

.hover\:cursor-pointer:hover { cursor: pointer; }

.hover\:overflow-visible:hover { overflow: visible; }

.hover\:whitespace-normal:hover { white-space: normal; }

.hover\:border-\[var\(--avatar-color\)\]:hover { border-color: var(--avatar-color); }

.hover\:border-black\/10:hover { border-color: rgba(0, 0, 0, 0.1); }

.hover\:border-gray-100:hover { --tw-border-opacity:1; border-color: rgba(236,236,241,var(--tw-border-opacity)); }

.hover\:border-gray-200:hover { --tw-border-opacity:1; border-color: rgba(217,217,227,var(--tw-border-opacity)); }

.hover\:border-gray-300:hover { --tw-border-opacity:1; border-color: rgba(197,197,210,var(--tw-border-opacity)); }

.hover\:\!bg-gray-50:hover { --tw-bg-opacity:1 !important; background-color: rgba(247,247,248,var(--tw-bg-opacity)) !important; }

.hover\:bg-\[\#0000FF\]:hover { --tw-bg-opacity:1; background-color: rgba(0,0,255,var(--tw-bg-opacity)); }

.hover\:bg-\[\#0077FF\]:hover { --tw-bg-opacity:1; background-color: rgba(0,119,255,var(--tw-bg-opacity)); }

.hover\:bg-\[\#6E73FC\]:hover { --tw-bg-opacity:1; background-color: rgba(110,115,252,var(--tw-bg-opacity)); }

.hover\:bg-black:hover { --tw-bg-opacity:1; background-color: rgba(0,0,0,var(--tw-bg-opacity)); }

.hover\:bg-black\/10:hover { background-color: rgba(0, 0, 0, 0.1); }

.hover\:bg-brand-green\/30:hover { background-color: rgba(25, 195, 125, 0.3); }

.hover\:bg-brand-purple:hover { --tw-bg-opacity:1; background-color: rgba(171,104,255,var(--tw-bg-opacity)); }

.hover\:bg-gray-100:hover { --tw-bg-opacity:1; background-color: rgba(236,236,241,var(--tw-bg-opacity)); }

.hover\:bg-gray-200:hover { --tw-bg-opacity:1; background-color: rgba(217,217,227,var(--tw-bg-opacity)); }

.hover\:bg-gray-300:hover { --tw-bg-opacity:1; background-color: rgba(197,197,210,var(--tw-bg-opacity)); }

.hover\:bg-gray-50:hover { --tw-bg-opacity:1; background-color: rgba(247,247,248,var(--tw-bg-opacity)); }

.hover\:bg-gray-500\/10:hover { background-color: rgba(141, 141, 160, 0.1); }

.hover\:bg-gray-700:hover { --tw-bg-opacity:1; background-color: rgba(64,65,79,var(--tw-bg-opacity)); }

.hover\:bg-gray-800:hover { --tw-bg-opacity:1; background-color: rgba(52,53,65,var(--tw-bg-opacity)); }

.hover\:bg-green-100:hover { --tw-bg-opacity:1; background-color: rgba(210,244,211,var(--tw-bg-opacity)); }

.hover\:bg-token-surface-tertiary:hover { background-color: var(--surface-tertiary); }

.hover\:bg-transparent:hover { background-color: transparent; }

.hover\:bg-white\/40:hover { background-color: rgba(255, 255, 255, 0.4); }

.hover\:pr-14:hover { padding-right: 3.5rem; }

.hover\:pr-4:hover { padding-right: 1rem; }

.hover\:text-black:hover { --tw-text-opacity:1; color: rgba(0,0,0,var(--tw-text-opacity)); }

.hover\:text-black\/50:hover { color: rgba(0, 0, 0, 0.5); }

.hover\:text-black\/70:hover { color: rgba(0, 0, 0, 0.7); }

.hover\:text-blue-600:hover { --tw-text-opacity:1; color: rgba(37,99,235,var(--tw-text-opacity)); }

.hover\:text-gray-200:hover { --tw-text-opacity:1; color: rgba(217,217,227,var(--tw-text-opacity)); }

.hover\:text-gray-300:hover { --tw-text-opacity:1; color: rgba(197,197,210,var(--tw-text-opacity)); }

.hover\:text-gray-400:hover { --tw-text-opacity:1; color: rgba(172,172,190,var(--tw-text-opacity)); }

.hover\:text-gray-600:hover { --tw-text-opacity:1; color: rgba(86,88,105,var(--tw-text-opacity)); }

.hover\:text-gray-700:hover { --tw-text-opacity:1; color: rgba(64,65,79,var(--tw-text-opacity)); }

.hover\:text-gray-800:hover { --tw-text-opacity:1; color: rgba(52,53,65,var(--tw-text-opacity)); }

.hover\:text-gray-900:hover { --tw-text-opacity:1; color: rgba(32,33,35,var(--tw-text-opacity)); }

.hover\:text-green-700:hover { --tw-text-opacity:1; color: rgba(26,127,100,var(--tw-text-opacity)); }

.hover\:text-red-500:hover { --tw-text-opacity:1; color: rgba(239,68,68,var(--tw-text-opacity)); }

.hover\:text-red-700:hover { --tw-text-opacity:1; color: rgba(185,28,28,var(--tw-text-opacity)); }

.hover\:text-token-text-primary:hover { color: var(--text-primary); }

.hover\:text-token-text-secondary:hover { color: var(--text-secondary); }

.hover\:text-white:hover { --tw-text-opacity:1; color: rgba(255,255,255,var(--tw-text-opacity)); }

.hover\:text-white\/40:hover { color: rgba(255, 255, 255, 0.4); }

.hover\:underline:hover { text-decoration-line: underline; }

.hover\:\!opacity-100:hover { opacity: 1 !important; }

.hover\:opacity-100:hover { opacity: 1; }

.hover\:opacity-20:hover { opacity: 0.2; }

.hover\:opacity-70:hover { opacity: 0.7; }

.hover\:opacity-75:hover { opacity: 0.75; }

.hover\:opacity-80:hover { opacity: 0.8; }

.hover\:shadow-\[0_1px_7px_0px_rgba\(0\,0\,0\,0\.06\)\]:hover { --tw-shadow:0 1px 7px 0px rgba(0,0,0,0.06); --tw-shadow-colored:0 1px 7px 0px var(--tw-shadow-color); box-shadow: var(--tw-ring-offset-shadow,0 0 transparent),var(--tw-ring-shadow,0 0 transparent),var(--tw-shadow); }

.hover\:brightness-90:hover { --tw-brightness:brightness(0.9); filter: var(--tw-blur) var(--tw-brightness) var(--tw-contrast) var(--tw-grayscale) var(--tw-hue-rotate) var(--tw-invert) var(--tw-saturate) var(--tw-sepia) var(--tw-drop-shadow); }

.hover\:delay-0:hover { transition-delay: 0s; }

.focus\:border-brand-green:focus, .focus\:border-green-500:focus { --tw-border-opacity:1; border-color: rgba(25,195,125,var(--tw-border-opacity)); }

.focus\:border-green-600:focus { --tw-border-opacity:1; border-color: rgba(16,163,127,var(--tw-border-opacity)); }

.focus\:border-orange-400:focus { --tw-border-opacity:1; border-color: rgba(255,128,82,var(--tw-border-opacity)); }

.focus\:border-red-500:focus { --tw-border-opacity:1; border-color: rgba(239,68,68,var(--tw-border-opacity)); }

.focus\:bg-gray-100:focus { --tw-bg-opacity:1; background-color: rgba(236,236,241,var(--tw-bg-opacity)); }

.focus\:bg-gray-50:focus { --tw-bg-opacity:1; background-color: rgba(247,247,248,var(--tw-bg-opacity)); }

.focus\:outline-none:focus { outline: transparent solid 2px; outline-offset: 2px; }

.focus\:ring-0:focus { --tw-ring-offset-shadow:var(--tw-ring-inset) 0 0 0 var(--tw-ring-offset-width) var(--tw-ring-offset-color); --tw-ring-shadow:var(--tw-ring-inset) 0 0 0 calc(var(--tw-ring-offset-width)) var(--tw-ring-color); }

.focus\:ring-0:focus, .focus\:ring-1:focus { box-shadow: var(--tw-ring-offset-shadow),var(--tw-ring-shadow),var(--tw-shadow,0 0 transparent); }

.focus\:ring-1:focus { --tw-ring-offset-shadow:var(--tw-ring-inset) 0 0 0 var(--tw-ring-offset-width) var(--tw-ring-offset-color); --tw-ring-shadow:var(--tw-ring-inset) 0 0 0 calc(1px + var(--tw-ring-offset-width)) var(--tw-ring-color); }

.focus\:ring-2:focus { --tw-ring-offset-shadow:var(--tw-ring-inset) 0 0 0 var(--tw-ring-offset-width) var(--tw-ring-offset-color); --tw-ring-shadow:var(--tw-ring-inset) 0 0 0 calc(2px + var(--tw-ring-offset-width)) var(--tw-ring-color); box-shadow: var(--tw-ring-offset-shadow),var(--tw-ring-shadow),var(--tw-shadow,0 0 transparent); }

.focus\:ring-inset:focus { --tw-ring-inset:inset; }

.focus\:ring-blue-400:focus { --tw-ring-opacity:1; --tw-ring-color:rgba(58,136,255,var(--tw-ring-opacity)); }

.focus\:ring-gray-200:focus { --tw-ring-opacity:1; --tw-ring-color:rgba(217,217,227,var(--tw-ring-opacity)); }

.focus\:ring-green-500:focus { --tw-ring-opacity:1; --tw-ring-color:rgba(25,195,125,var(--tw-ring-opacity)); }

.focus\:ring-green-600:focus { --tw-ring-opacity:1; --tw-ring-color:rgba(16,163,127,var(--tw-ring-opacity)); }

.focus\:ring-white:focus { --tw-ring-opacity:1; --tw-ring-color:rgba(255,255,255,var(--tw-ring-opacity)); }

.focus-visible\:border-blue-500.focus-visible { --tw-border-opacity:1; border-color: rgba(69,89,164,var(--tw-border-opacity)); }

.focus-visible\:border-blue-500:focus-visible { --tw-border-opacity:1; border-color: rgba(69,89,164,var(--tw-border-opacity)); }

.focus-visible\:ring-0.focus-visible { --tw-ring-offset-shadow:var(--tw-ring-inset) 0 0 0 var(--tw-ring-offset-width) var(--tw-ring-offset-color); --tw-ring-shadow:var(--tw-ring-inset) 0 0 0 calc(var(--tw-ring-offset-width)) var(--tw-ring-color); box-shadow: var(--tw-ring-offset-shadow),var(--tw-ring-shadow),var(--tw-shadow,0 0 transparent); }

.focus-visible\:ring-0:focus-visible { --tw-ring-offset-shadow:var(--tw-ring-inset) 0 0 0 var(--tw-ring-offset-width) var(--tw-ring-offset-color); --tw-ring-shadow:var(--tw-ring-inset) 0 0 0 calc(var(--tw-ring-offset-width)) var(--tw-ring-color); box-shadow: var(--tw-ring-offset-shadow),var(--tw-ring-shadow),var(--tw-shadow,0 0 transparent); }

.enabled\:bg-brand-purple:enabled { --tw-bg-opacity:1; background-color: rgba(171,104,255,var(--tw-bg-opacity)); }

.disabled\:bottom-0:disabled { bottom: 0px; }

.disabled\:bottom-0\.5:disabled { bottom: 0.125rem; }

.disabled\:cursor-auto:disabled { cursor: auto; }

.disabled\:text-gray-300:disabled { --tw-text-opacity:1; color: rgba(197,197,210,var(--tw-text-opacity)); }

.disabled\:text-gray-400:disabled { --tw-text-opacity:1; color: rgba(172,172,190,var(--tw-text-opacity)); }

.disabled\:text-token-border-medium:disabled { color: var(--border-medium); }

.disabled\:opacity-20:disabled { opacity: 0.2; }

.disabled\:opacity-40:disabled { opacity: 0.4; }

.disabled\:hover\:bg-gray-900:hover:disabled { --tw-bg-opacity:1; background-color: rgba(32,33,35,var(--tw-bg-opacity)); }

.group:hover .group-hover\:visible, .group\/dalle-image:hover .group-hover\/dalle-image\:visible { visibility: visible; }

.group\/tool-expander-closed:hover .group-hover\/tool-expander-closed\:block, .group\/tool-expander-open:hover .group-hover\/tool-expander-open\:block { display: block; }

.group:hover .group-hover\:border-t-\[\#0077FF\] { --tw-border-opacity:1; border-top-color: rgba(0,119,255,var(--tw-border-opacity)); }

.group\/options:hover .group-hover\/options\:bg-transparent { background-color: transparent; }

.group\/toggle:hover .group-hover\/toggle\:bg-red-500 { --tw-bg-opacity:1; background-color: rgba(239,68,68,var(--tw-bg-opacity)); }

.group:hover .group-hover\:bg-gray-100 { --tw-bg-opacity:1; background-color: rgba(236,236,241,var(--tw-bg-opacity)); }

.group:hover .group-hover\:from-gray-100 { --tw-gradient-from:#ececf1 var(--tw-gradient-from-position); --tw-gradient-to:rgba(236,236,241,0) var(--tw-gradient-to-position); --tw-gradient-stops:var(--tw-gradient-from),var(--tw-gradient-to); }

.group\/button:hover .group-hover\/button\:text-blue-500 { --tw-text-opacity:1; color: rgba(69,89,164,var(--tw-text-opacity)); }

.group\/button:hover .group-hover\/button\:text-brand-green { --tw-text-opacity:1; color: rgba(25,195,125,var(--tw-text-opacity)); }

.group\/button:hover .group-hover\/button\:text-brand-purple { --tw-text-opacity:1; color: rgba(171,104,255,var(--tw-text-opacity)); }

.group\/button:hover .group-hover\/button\:text-orange-500 { --tw-text-opacity:1; color: rgba(224,108,43,var(--tw-text-opacity)); }

.group\/button:hover .group-hover\/button\:text-red-500 { --tw-text-opacity:1; color: rgba(239,68,68,var(--tw-text-opacity)); }

.group\/option:hover .group-hover\/option\:\!text-brand-green { --tw-text-opacity:1 !important; color: rgba(25,195,125,var(--tw-text-opacity)) !important; }

.group\/option:hover .group-hover\/option\:\!text-brand-purple { --tw-text-opacity:1 !important; color: rgba(171,104,255,var(--tw-text-opacity)) !important; }

.group\/option:hover .group-hover\/option\:text-gray-100 { --tw-text-opacity:1; color: rgba(236,236,241,var(--tw-text-opacity)); }

.group\/option:hover .group-hover\/option\:text-gray-800 { --tw-text-opacity:1; color: rgba(52,53,65,var(--tw-text-opacity)); }

.group\/options:hover .group-hover\/options\:text-gray-500 { --tw-text-opacity:1; color: rgba(142,142,160,var(--tw-text-opacity)); }

.group\/options:hover .group-hover\/options\:opacity-0 { opacity: 0; }

.group:hover .group-hover\:opacity-100 { opacity: 1; }

.group:hover .group-hover\:opacity-50 { opacity: 0.5; }

.group-ui-open\:bg-gray-100.group[data-headlessui-state~="open"] { --tw-bg-opacity:1; background-color: rgba(236,236,241,var(--tw-bg-opacity)); }

.group:where([data-headlessui-state~="open"]) .group-ui-open\:bg-gray-100 { --tw-bg-opacity:1; background-color: rgba(236,236,241,var(--tw-bg-opacity)); }

.gizmo .gizmo\:h-10 { height: 2.5rem; }

.gizmo .gizmo\:min-h-0 { min-height: 0px; }

.gizmo .gizmo\:gap-2 { gap: 0.5rem; }

.gizmo .gizmo\:rounded { border-radius: 0.25rem; }

.gizmo .gizmo\:rounded-lg { border-radius: 0.5rem; }

.gizmo .gizmo\:border-0 { border-width: 0px; }

.gizmo .gizmo\:border-\[rgba\(0\,0\,0\,0\.1\)\] { border-color: rgba(0, 0, 0, 0.1); }

.gizmo .gizmo\:border-token-border-light { border-color: var(--border-light); }

.gizmo .gizmo\:bg-transparent { background-color: transparent; }

.gizmo .gizmo\:bg-white { --tw-bg-opacity:1; background-color: rgba(255,255,255,var(--tw-bg-opacity)); }

.gizmo .gizmo\:from-white { --tw-gradient-from:#fff var(--tw-gradient-from-position); --tw-gradient-to:hsla(0,0%,100%,0) var(--tw-gradient-to-position); --tw-gradient-stops:var(--tw-gradient-from),var(--tw-gradient-to); }

.gizmo .gizmo\:p-3 { padding: 0.75rem; }

.gizmo .gizmo\:hover\:bg-gray-100:hover { --tw-bg-opacity:1; background-color: rgba(236,236,241,var(--tw-bg-opacity)); }

.radix-disabled\:pointer-events-none[data-disabled] { pointer-events: none; }

.radix-disabled\:cursor-auto[data-disabled] { cursor: auto; }

.radix-disabled\:bg-transparent[data-disabled] { background-color: transparent; }

.radix-disabled\:text-gray-300[data-disabled] { --tw-text-opacity:1; color: rgba(197,197,210,var(--tw-text-opacity)); }

.radix-disabled\:text-gray-500[data-disabled] { --tw-text-opacity:1; color: rgba(142,142,160,var(--tw-text-opacity)); }

.radix-highlighted\:bg-gray-50[data-highlighted] { --tw-bg-opacity:1; background-color: rgba(247,247,248,var(--tw-bg-opacity)); }

.radix-highlighted\:outline-none[data-highlighted] { outline: transparent solid 2px; outline-offset: 2px; }

.radix-state-active\:bg-gray-800[data-state="active"] { --tw-bg-opacity:1; background-color: rgba(52,53,65,var(--tw-bg-opacity)); }

.radix-state-active\:text-gray-600[data-state="active"] { --tw-text-opacity:1; color: rgba(86,88,105,var(--tw-text-opacity)); }

.radix-state-active\:text-white[data-state="active"] { --tw-text-opacity:1; color: rgba(255,255,255,var(--tw-text-opacity)); }

.group[data-state="active"] .group-radix-state-active\:fill-white { fill: rgb(255, 255, 255); }

.radix-state-checked\:translate-x-\[19px\][data-state="checked"] { --tw-translate-x:19px; transform: translate(var(--tw-translate-x),var(--tw-translate-y)) rotate(var(--tw-rotate)) skewX(var(--tw-skew-x)) skewY(var(--tw-skew-y)) scaleX(var(--tw-scale-x)) scaleY(var(--tw-scale-y)); }

.radix-state-checked\:bg-green-600[data-state="checked"] { --tw-bg-opacity:1; background-color: rgba(16,163,127,var(--tw-bg-opacity)); }

.radix-state-checked\:text-white[data-state="checked"] { --tw-text-opacity:1; color: rgba(255,255,255,var(--tw-text-opacity)); }

.radix-state-on\:bg-green-600[data-state="on"] { --tw-bg-opacity:1; background-color: rgba(16,163,127,var(--tw-bg-opacity)); }

.radix-state-on\:text-white[data-state="on"] { --tw-text-opacity:1; color: rgba(255,255,255,var(--tw-text-opacity)); }

@-webkit-keyframes contentShow { 
  0% { opacity: 0; transform: scale(0.96); }
  100% { opacity: 1; transform: scale(1); }
}

@keyframes contentShow { 
  0% { opacity: 0; transform: scale(0.96); }
  100% { opacity: 1; transform: scale(1); }
}

.radix-state-open\:animate-contentShow[data-state="open"] { animation: 0.15s cubic-bezier(0.16, 1, 0.3, 1) 0s 1 normal none running contentShow; }

@-webkit-keyframes show { 
  0% { opacity: 0; }
  100% { opacity: 1; }
}

@keyframes show { 
  0% { opacity: 0; }
  100% { opacity: 1; }
}

.radix-state-open\:animate-show[data-state="open"] { animation: 0.1s cubic-bezier(0.16, 1, 0.3, 1) 0s 1 normal none running show; }

.radix-state-open\:bg-gray-50[data-state="open"] { --tw-bg-opacity:1; background-color: rgba(247,247,248,var(--tw-bg-opacity)); }

.radix-state-open\:text-gray-600[data-state="open"] { --tw-text-opacity:1; color: rgba(86,88,105,var(--tw-text-opacity)); }

.radix-side-bottom\:animate-slideDownAndFade[data-side="bottom"] { animation: 0.4s cubic-bezier(0.16, 1, 0.3, 1) 0s 1 normal none running slideDownAndFade; }

.radix-side-bottom\:animate-slideUpAndFade[data-side="bottom"] { animation: 0.4s cubic-bezier(0.16, 1, 0.3, 1) 0s 1 normal none running slideUpAndFade; }

.radix-side-left\:animate-slideLeftAndFade[data-side="left"] { animation: 0.4s cubic-bezier(0.16, 1, 0.3, 1) 0s 1 normal none running slideLeftAndFade; }

@-webkit-keyframes slideRightAndFade { 
  0% { opacity: 0; transform: translateX(-2px); }
  100% { opacity: 1; transform: translateX(0px); }
}

.radix-side-left\:animate-slideRightAndFade[data-side="left"] { animation: 0.4s cubic-bezier(0.16, 1, 0.3, 1) 0s 1 normal none running slideRightAndFade; }

@keyframes slideLeftAndFade { 
  0% { opacity: 0; transform: translateX(2px); }
  100% { opacity: 1; transform: translateX(0px); }
}

.radix-side-right\:animate-slideLeftAndFade[data-side="right"] { animation: 0.4s cubic-bezier(0.16, 1, 0.3, 1) 0s 1 normal none running slideLeftAndFade; }

@keyframes slideRightAndFade { 
  0% { opacity: 0; transform: translateX(-2px); }
  100% { opacity: 1; transform: translateX(0px); }
}

.radix-side-right\:animate-slideRightAndFade[data-side="right"] { animation: 0.4s cubic-bezier(0.16, 1, 0.3, 1) 0s 1 normal none running slideRightAndFade; }

@keyframes slideDownAndFade { 
  0% { opacity: 0; transform: translateY(-2px); }
  100% { opacity: 1; transform: translateY(0px); }
}

.radix-side-top\:animate-slideDownAndFade[data-side="top"] { animation: 0.4s cubic-bezier(0.16, 1, 0.3, 1) 0s 1 normal none running slideDownAndFade; }

@keyframes slideUpAndFade { 
  0% { opacity: 0; transform: translateY(2px); }
  100% { opacity: 1; transform: translateY(0px); }
}

.radix-side-top\:animate-slideUpAndFade[data-side="top"] { animation: 0.4s cubic-bezier(0.16, 1, 0.3, 1) 0s 1 normal none running slideUpAndFade; }

.dark .dark\:hidden { display: none; }

:is(.dark .dark\:divide-gray-900) > :not([hidden]) ~ :not([hidden]) { --tw-divide-opacity:1; border-color: rgba(32,33,35,var(--tw-divide-opacity)); }

.dark .dark\:border { border-width: 1px; }

.dark .dark\:border-0 { border-width: 0px; }

.dark .dark\:border-\[\#4E4F60\] { --tw-border-opacity:1; border-color: rgba(78,79,96,var(--tw-border-opacity)); }

.dark .dark\:border-black { --tw-border-opacity:1; border-color: rgba(0,0,0,var(--tw-border-opacity)); }

.dark .dark\:border-black\/20 { border-color: rgba(0, 0, 0, 0.2); }

.dark .dark\:border-gray-300 { --tw-border-opacity:1; border-color: rgba(197,197,210,var(--tw-border-opacity)); }

.dark .dark\:border-gray-600 { --tw-border-opacity:1; border-color: rgba(86,88,105,var(--tw-border-opacity)); }

.dark .dark\:border-gray-700 { --tw-border-opacity:1; border-color: rgba(64,65,79,var(--tw-border-opacity)); }

.dark .dark\:border-gray-800 { --tw-border-opacity:1; border-color: rgba(52,53,65,var(--tw-border-opacity)); }

.dark .dark\:border-gray-900 { --tw-border-opacity:1; border-color: rgba(32,33,35,var(--tw-border-opacity)); }

.dark .dark\:border-gray-900\/50 { border-color: rgba(32, 33, 35, 0.5); }

.dark .dark\:border-token-border-light { border-color: var(--border-light); }

.dark .dark\:border-white { --tw-border-opacity:1; border-color: rgba(255,255,255,var(--tw-border-opacity)); }

.dark .dark\:border-white\/10 { border-color: rgba(255, 255, 255, 0.1); }

.dark .dark\:border-white\/20 { border-color: rgba(255, 255, 255, 0.2); }

.dark .dark\:border-white\/30 { border-color: rgba(255, 255, 255, 0.298); }

.dark .dark\:bg-\[\#272832\] { --tw-bg-opacity:1; background-color: rgba(39,40,50,var(--tw-bg-opacity)); }

.dark .dark\:bg-\[\#444654\] { --tw-bg-opacity:1; background-color: rgba(68,70,84,var(--tw-bg-opacity)); }

.dark .dark\:bg-\[rgba\(255\,255\,255\,0\.15\)\] { background-color: rgba(255, 255, 255, 0.15); }

.dark .dark\:bg-black { --tw-bg-opacity:1; background-color: rgba(0,0,0,var(--tw-bg-opacity)); }

.dark .dark\:bg-black\/80 { background-color: rgba(0, 0, 0, 0.8); }

.dark .dark\:bg-brand-green\/50 { background-color: rgba(25, 195, 125, 0.5); }

.dark .dark\:bg-gray-100 { --tw-bg-opacity:1; background-color: rgba(236,236,241,var(--tw-bg-opacity)); }

.dark .dark\:bg-gray-50 { --tw-bg-opacity:1; background-color: rgba(247,247,248,var(--tw-bg-opacity)); }

.dark .dark\:bg-gray-500 { --tw-bg-opacity:1; background-color: rgba(142,142,160,var(--tw-bg-opacity)); }

.dark .dark\:bg-gray-600 { --tw-bg-opacity:1; background-color: rgba(86,88,105,var(--tw-bg-opacity)); }

.dark .dark\:bg-gray-600\/70 { background-color: rgba(86, 88, 105, 0.7); }

.dark .dark\:bg-gray-700 { --tw-bg-opacity:1; background-color: rgba(64,65,79,var(--tw-bg-opacity)); }

.dark .dark\:bg-gray-800 { --tw-bg-opacity:1; background-color: rgba(52,53,65,var(--tw-bg-opacity)); }

.dark .dark\:bg-gray-800\/30 { background-color: rgba(52, 53, 65, 0.3); }

.dark .dark\:bg-gray-800\/90 { background-color: rgba(52, 53, 65, 0.9); }

.dark .dark\:bg-gray-900 { --tw-bg-opacity:1; background-color: rgba(32,33,35,var(--tw-bg-opacity)); }

.dark .dark\:bg-gray-900\/95 { background-color: rgba(32, 33, 35, 0.95); }

.dark .dark\:bg-gray-950 { --tw-bg-opacity:1; background-color: rgba(5,5,9,var(--tw-bg-opacity)); }

.dark .dark\:bg-orange-500 { --tw-bg-opacity:1; background-color: rgba(224,108,43,var(--tw-bg-opacity)); }

.dark .dark\:bg-red-200 { --tw-bg-opacity:1; background-color: rgba(254,202,202,var(--tw-bg-opacity)); }

.dark .dark\:bg-transparent { background-color: transparent; }

.dark .dark\:bg-white { --tw-bg-opacity:1; background-color: rgba(255,255,255,var(--tw-bg-opacity)); }

.dark .dark\:bg-white\/10 { background-color: rgba(255, 255, 255, 0.1); }

.dark .dark\:bg-white\/20 { background-color: rgba(255, 255, 255, 0.2); }

.dark .dark\:bg-white\/5 { background-color: rgba(255, 255, 255, 0.05); }

.dark .dark\:from-gray-700 { --tw-gradient-from:#40414f var(--tw-gradient-from-position); --tw-gradient-to:rgba(64,65,79,0) var(--tw-gradient-to-position); --tw-gradient-stops:var(--tw-gradient-from),var(--tw-gradient-to); }

.dark .dark\:from-gray-800 { --tw-gradient-from:#343541 var(--tw-gradient-from-position); --tw-gradient-to:rgba(52,53,65,0) var(--tw-gradient-to-position); --tw-gradient-stops:var(--tw-gradient-from),var(--tw-gradient-to); }

.dark .dark\:from-gray-900 { --tw-gradient-from:#202123 var(--tw-gradient-from-position); --tw-gradient-to:rgba(32,33,35,0) var(--tw-gradient-to-position); --tw-gradient-stops:var(--tw-gradient-from),var(--tw-gradient-to); }

.dark .dark\:fill-gray-500 { fill: rgb(142, 142, 160); }

.dark .dark\:text-\[\#D292FF\] { --tw-text-opacity:1; color: rgba(210,146,255,var(--tw-text-opacity)); }

.dark .dark\:text-black { --tw-text-opacity:1; color: rgba(0,0,0,var(--tw-text-opacity)); }

.dark .dark\:text-blue-200 { --tw-text-opacity:1; color: rgba(191,219,254,var(--tw-text-opacity)); }

.dark .dark\:text-blue-400 { --tw-text-opacity:1; color: rgba(58,136,255,var(--tw-text-opacity)); }

.dark .dark\:text-blue-600 { --tw-text-opacity:1; color: rgba(37,99,235,var(--tw-text-opacity)); }

.dark .dark\:text-gray-100 { --tw-text-opacity:1; color: rgba(236,236,241,var(--tw-text-opacity)); }

.dark .dark\:text-gray-200 { --tw-text-opacity:1; color: rgba(217,217,227,var(--tw-text-opacity)); }

.dark .dark\:text-gray-300 { --tw-text-opacity:1; color: rgba(197,197,210,var(--tw-text-opacity)); }

.dark .dark\:text-gray-400 { --tw-text-opacity:1; color: rgba(172,172,190,var(--tw-text-opacity)); }

.dark .dark\:text-gray-500 { --tw-text-opacity:1; color: rgba(142,142,160,var(--tw-text-opacity)); }

.dark .dark\:text-gray-600 { --tw-text-opacity:1; color: rgba(86,88,105,var(--tw-text-opacity)); }

.dark .dark\:text-gray-700 { --tw-text-opacity:1; color: rgba(64,65,79,var(--tw-text-opacity)); }

.dark .dark\:text-gray-900 { --tw-text-opacity:1; color: rgba(32,33,35,var(--tw-text-opacity)); }

.dark .dark\:text-green-100 { --tw-text-opacity:1; color: rgba(210,244,211,var(--tw-text-opacity)); }

.dark .dark\:text-red-500 { --tw-text-opacity:1; color: rgba(239,68,68,var(--tw-text-opacity)); }

.dark .dark\:text-red-800 { --tw-text-opacity:1; color: rgba(153,27,27,var(--tw-text-opacity)); }

.dark .dark\:text-white { --tw-text-opacity:1; color: rgba(255,255,255,var(--tw-text-opacity)); }

.dark .dark\:text-white\/60 { color: rgba(255, 255, 255, 0.6); }

.dark .dark\:text-white\/70 { color: rgba(255, 255, 255, 0.698); }

.dark .dark\:text-white\/90 { color: rgba(255, 255, 255, 0.9); }

.dark .dark\:opacity-100 { opacity: 1; }

.dark .dark\:shadow-xs { --tw-shadow:0 0 15px rgba(0,0,0,0.1); --tw-shadow-colored:0 0 15px var(--tw-shadow-color); box-shadow: var(--tw-ring-offset-shadow,0 0 transparent),var(--tw-ring-shadow,0 0 transparent),var(--tw-shadow); }

.dark .dark\:ring-white\/20 { --tw-ring-color:hsla(0,0%,100%,0.2); }

:is(.dark .placeholder\:dark\:text-white)::-webkit-input-placeholder { --tw-text-opacity:1; color: rgba(255,255,255,var(--tw-text-opacity)); }

:is(.dark .placeholder\:dark\:text-white)::placeholder { --tw-text-opacity:1; color: rgba(255,255,255,var(--tw-text-opacity)); }

.dark .dark\:last\:border-0:last-child { border-width: 0px; }

.dark .dark\:focus-within\:border-\[rgba\(0\,0\,0\,0\.3\)\][focus-within] { border-color: rgba(0, 0, 0, 0.3); }

.dark .dark\:focus-within\:border-\[rgba\(0\,0\,0\,0\.3\)\]:focus-within { border-color: rgba(0, 0, 0, 0.3); }

.dark .dark\:hover\:border-gray-600:hover { --tw-border-opacity:1; border-color: rgba(86,88,105,var(--tw-border-opacity)); }

.dark .dark\:hover\:border-gray-800:hover { --tw-border-opacity:1; border-color: rgba(52,53,65,var(--tw-border-opacity)); }

.dark .dark\:hover\:border-white\/10:hover { border-color: rgba(255, 255, 255, 0.1); }

.dark .dark\:hover\:\!bg-gray-700:hover { --tw-bg-opacity:1 !important; background-color: rgba(64,65,79,var(--tw-bg-opacity)) !important; }

.dark .dark\:hover\:bg-\[\#2A2B32\]:hover { --tw-bg-opacity:1; background-color: rgba(42,43,50,var(--tw-bg-opacity)); }

.dark .dark\:hover\:bg-\[\#2E2F33\]:hover { --tw-bg-opacity:1; background-color: rgba(46,47,51,var(--tw-bg-opacity)); }

.dark .dark\:hover\:bg-\[\#494A54\]:hover { --tw-bg-opacity:1; background-color: rgba(73,74,84,var(--tw-bg-opacity)); }

.dark .dark\:hover\:bg-brand-green\/40:hover { background-color: rgba(25, 195, 125, 0.4); }

.dark .dark\:hover\:bg-gray-200:hover { --tw-bg-opacity:1; background-color: rgba(217,217,227,var(--tw-bg-opacity)); }

.dark .dark\:hover\:bg-gray-500\/10:hover { background-color: rgba(141, 141, 160, 0.1); }

.dark .dark\:hover\:bg-gray-700:hover { --tw-bg-opacity:1; background-color: rgba(64,65,79,var(--tw-bg-opacity)); }

.dark .dark\:hover\:bg-gray-800:hover { --tw-bg-opacity:1; background-color: rgba(52,53,65,var(--tw-bg-opacity)); }

.dark .dark\:hover\:bg-gray-900:hover { --tw-bg-opacity:1; background-color: rgba(32,33,35,var(--tw-bg-opacity)); }

.dark .dark\:hover\:bg-white\/20:hover { background-color: rgba(255, 255, 255, 0.2); }

:is(.dark .hover\:dark\:bg-gray-700):hover { --tw-bg-opacity:1; background-color: rgba(64,65,79,var(--tw-bg-opacity)); }

:is(.dark .hover\:dark\:bg-white\/10):hover { background-color: rgba(255, 255, 255, 0.1); }

.dark .dark\:hover\:text-blue-600:hover { --tw-text-opacity:1; color: rgba(37,99,235,var(--tw-text-opacity)); }

.dark .dark\:hover\:text-gray-100:hover { --tw-text-opacity:1; color: rgba(236,236,241,var(--tw-text-opacity)); }

.dark .dark\:hover\:text-gray-200:hover { --tw-text-opacity:1; color: rgba(217,217,227,var(--tw-text-opacity)); }

.dark .dark\:hover\:text-gray-400:hover { --tw-text-opacity:1; color: rgba(172,172,190,var(--tw-text-opacity)); }

.dark .dark\:hover\:text-white:hover { --tw-text-opacity:1; color: rgba(255,255,255,var(--tw-text-opacity)); }

.dark .dark\:hover\:text-white\/50:hover { color: rgba(255, 255, 255, 0.5); }

:is(.dark .hover\:dark\:text-gray-100):hover { --tw-text-opacity:1; color: rgba(236,236,241,var(--tw-text-opacity)); }

:is(.dark .hover\:dark\:text-gray-300):hover { --tw-text-opacity:1; color: rgba(197,197,210,var(--tw-text-opacity)); }

.dark .dark\:focus\:border-white:focus { --tw-border-opacity:1; border-color: rgba(255,255,255,var(--tw-border-opacity)); }

.dark .dark\:focus\:bg-gray-700:focus { --tw-bg-opacity:1; background-color: rgba(64,65,79,var(--tw-bg-opacity)); }

:is(.dark .focus\:dark\:bg-gray-700):focus { --tw-bg-opacity:1; background-color: rgba(64,65,79,var(--tw-bg-opacity)); }

.dark .dark\:focus\:ring-gray-600:focus { --tw-ring-opacity:1; --tw-ring-color:rgba(86,88,105,var(--tw-ring-opacity)); }

.dark .dark\:focus\:ring-white:focus { --tw-ring-opacity:1; --tw-ring-color:rgba(255,255,255,var(--tw-ring-opacity)); }

.dark .dark\:focus-visible\:border-blue-500.focus-visible { --tw-border-opacity:1; border-color: rgba(69,89,164,var(--tw-border-opacity)); }

.dark .dark\:focus-visible\:border-blue-500:focus-visible { --tw-border-opacity:1; border-color: rgba(69,89,164,var(--tw-border-opacity)); }

.dark .dark\:disabled\:text-gray-400:disabled { --tw-text-opacity:1; color: rgba(172,172,190,var(--tw-text-opacity)); }

.dark .dark\:disabled\:hover\:bg-transparent:hover:disabled { background-color: transparent; }

.dark .dark\:disabled\:hover\:bg-white:hover:disabled { --tw-bg-opacity:1; background-color: rgba(255,255,255,var(--tw-bg-opacity)); }

:is(.dark .disabled\:dark\:hover\:text-gray-400:hover):disabled { --tw-text-opacity:1; color: rgba(172,172,190,var(--tw-text-opacity)); }

.dark .group:hover .dark\:group-hover\:bg-gray-700 { --tw-bg-opacity:1; background-color: rgba(64,65,79,var(--tw-bg-opacity)); }

.dark .group:hover .dark\:group-hover\:from-\[\#2A2B32\] { --tw-gradient-from:#2a2b32 var(--tw-gradient-from-position); --tw-gradient-to:rgba(42,43,50,0) var(--tw-gradient-to-position); --tw-gradient-stops:var(--tw-gradient-from),var(--tw-gradient-to); }

.dark .group\/option:hover .dark\:group-hover\/option\:text-gray-100 { --tw-text-opacity:1; color: rgba(236,236,241,var(--tw-text-opacity)); }

.dark .dark\:group-ui-open\:bg-gray-800.group[data-headlessui-state~="open"] { --tw-bg-opacity:1; background-color: rgba(52,53,65,var(--tw-bg-opacity)); }

.dark .group:where([data-headlessui-state~="open"]) .dark\:group-ui-open\:bg-gray-800 { --tw-bg-opacity:1; background-color: rgba(52,53,65,var(--tw-bg-opacity)); }

.gizmo :is(.dark .gizmo\:dark\:border-0) { border-width: 0px; }

.gizmo :is(.dark .gizmo\:dark\:bg-transparent) { background-color: transparent; }

.dark .dark\:radix-disabled\:bg-transparent[data-disabled] { background-color: transparent; }

.dark .dark\:radix-disabled\:text-gray-500[data-disabled] { --tw-text-opacity:1; color: rgba(142,142,160,var(--tw-text-opacity)); }

.dark .dark\:radix-disabled\:text-gray-600[data-disabled] { --tw-text-opacity:1; color: rgba(86,88,105,var(--tw-text-opacity)); }

.dark .dark\:radix-highlighted\:bg-\[\#2E2F33\][data-highlighted] { --tw-bg-opacity:1; background-color: rgba(46,47,51,var(--tw-bg-opacity)); }

.dark .dark\:radix-highlighted\:bg-gray-700[data-highlighted] { --tw-bg-opacity:1; background-color: rgba(64,65,79,var(--tw-bg-opacity)); }

.dark .dark\:radix-highlighted\:hover\:bg-\[\#2E2F33\][data-highlighted]:hover { --tw-bg-opacity:1; background-color: rgba(46,47,51,var(--tw-bg-opacity)); }

.dark .dark\:radix-state-active\:text-white[data-state="active"] { --tw-text-opacity:1; color: rgba(255,255,255,var(--tw-text-opacity)); }

.dark .dark\:radix-state-checked\:bg-green-600[data-state="checked"], .dark .dark\:radix-state-on\:bg-green-600[data-state="on"] { --tw-bg-opacity:1; background-color: rgba(16,163,127,var(--tw-bg-opacity)); }

.dark .dark\:radix-state-open\:bg-gray-700[data-state="open"] { --tw-bg-opacity:1; background-color: rgba(64,65,79,var(--tw-bg-opacity)); }

.dark .dark\:radix-state-open\:text-gray-400[data-state="open"] { --tw-text-opacity:1; color: rgba(172,172,190,var(--tw-text-opacity)); }

@media (min-width: 640px) {
  .sm\:absolute { position: absolute; }
  .sm\:order-1 { order: 1; }
  .sm\:order-2 { order: 2; }
  .sm\:-mx-6 { margin-left: -1.5rem; margin-right: -1.5rem; }
  .sm\:mx-1 { margin-left: 0.25rem; margin-right: 0.25rem; }
  .sm\:mb-0 { margin-bottom: 0px; }
  .sm\:mb-0\.5 { margin-bottom: 0.125rem; }
  .sm\:mb-16 { margin-bottom: 4rem; }
  .sm\:mb-2 { margin-bottom: 0.5rem; }
  .sm\:ml-0 { margin-left: 0px; }
  .sm\:mt-0 { margin-top: 0px; }
  .sm\:mt-16 { margin-top: 4rem; }
  .sm\:mt-4 { margin-top: 1rem; }
  .sm\:mt-\[20vh\] { margin-top: 20vh; }
  .sm\:mt-\[6vh\] { margin-top: 6vh; }
  .sm\:block { display: block; }
  .sm\:flex { display: flex; }
  .sm\:h-10 { height: 2.5rem; }
  .sm\:h-5 { height: 1.25rem; }
  .sm\:w-10 { width: 2.5rem; }
  .sm\:w-5 { width: 1.25rem; }
  .sm\:w-\[312px\] { width: 312px; }
  .sm\:w-\[calc\(\(100\%-12px\)\/2\)\] { width: calc(50% - 6px); }
  .sm\:w-auto { width: auto; }
  .sm\:min-w-\[148px\] { min-width: 148px; }
  .sm\:min-w-\[312px\] { min-width: 312px; }
  .sm\:min-w-\[420px\] { min-width: 420px; }
  .sm\:max-w-lg { max-width: 32rem; }
  .sm\:max-w-md { max-width: 28rem; }
  .sm\:grid-cols-2 { grid-template-columns: repeat(2, minmax(0px, 1fr)); }
  .sm\:grid-cols-3 { grid-template-columns: repeat(3, minmax(0px, 1fr)); }
  .sm\:grid-rows-2 { grid-template-rows: repeat(2, minmax(0px, 1fr)); }
  .sm\:flex-row { flex-direction: row; }
  .sm\:flex-row-reverse { flex-direction: row-reverse; }
  .sm\:items-center { align-items: center; }
  .sm\:justify-center { justify-content: center; }
  .sm\:gap-3 { gap: 0.75rem; }
  .sm\:gap-6 { gap: 1.5rem; }
  .sm\:gap-y-0 { row-gap: 0px; }
  .sm\:rounded-md { border-radius: 0.375rem; }
  .sm\:rounded-t-\[30px\] { border-top-left-radius: 30px; border-top-right-radius: 30px; }
  .sm\:border-r { border-right-width: 1px; }
  .sm\:border-t-0 { border-top-width: 0px; }
  .sm\:p-0 { padding: 0px; }
  .sm\:p-2 { padding: 0.5rem; }
  .sm\:p-3 { padding: 0.75rem; }
  .sm\:p-4 { padding: 1rem; }
  .sm\:p-6 { padding: 1.5rem; }
  .sm\:px-0 { padding-left: 0px; padding-right: 0px; }
  .sm\:px-2 { padding-left: 0.5rem; padding-right: 0.5rem; }
  .sm\:px-6 { padding-left: 1.5rem; padding-right: 1.5rem; }
  .sm\:pb-0 { padding-bottom: 0px; }
  .sm\:pb-1 { padding-bottom: 0.25rem; }
  .sm\:pb-2 { padding-bottom: 0.5rem; }
  .sm\:pb-6 { padding-bottom: 1.5rem; }
  .sm\:pl-3 { padding-left: 0.75rem; }
  .sm\:pr-2 { padding-right: 0.5rem; }
  .sm\:pt-4 { padding-top: 1rem; }
  .sm\:pt-8 { padding-top: 2rem; }
  .sm\:text-left { text-align: left; }
  .sm\:text-4xl { font-size: 2.25rem; line-height: 2.5rem; }
  .sm\:text-base { font-size: 1rem; line-height: 1.5rem; }
  .sm\:text-sm { font-size: 0.875rem; line-height: 1.25rem; }
  .sm\:duration-300 { transition-duration: 0.3s; }
  .sm\:duration-500 { transition-duration: 0.5s; }
  @supports (min-height:100dvh) {
  .sm\:supports-\[min-height\:100dvh\]\:min-h-\[100dvh\] { min-height: 100dvh; }
}
}

@media (min-width: 768px) {
  .md\:invisible { visibility: hidden; }
  .md\:static { position: static; }
  .md\:relative { position: relative; }
  .md\:-right-2 { right: -0.5rem; }
  .md\:bottom-3 { bottom: 0.75rem; }
  .md\:bottom-3\.5 { bottom: 0.875rem; }
  .md\:bottom-\[180px\] { bottom: 180px; }
  .md\:bottom-auto { bottom: auto; }
  .md\:left-4 { left: 1rem; }
  .md\:left-auto { left: auto; }
  .md\:right-3 { right: 0.75rem; }
  .md\:right-auto { right: auto; }
  .md\:top-\[22px\] { top: 22px; }
  .md\:m-0 { margin: 0px; }
  .md\:m-auto { margin: auto; }
  .md\:mx-4 { margin-left: 1rem; margin-right: 1rem; }
  .md\:mb-0 { margin-bottom: 0px; }
  .md\:mb-16 { margin-bottom: 4rem; }
  .md\:mb-3 { margin-bottom: 0.75rem; }
  .md\:mb-4 { margin-bottom: 1rem; }
  .md\:mb-auto { margin-bottom: auto; }
  .md\:block { display: block; }
  .md\:inline-block { display: inline-block; }
  .md\:flex { display: flex; }
  .md\:grid { display: grid; }
  .md\:hidden { display: none; }
  .md\:h-48 { height: 12rem; }
  .md\:h-full { height: 100%; }
  .md\:min-h-\[300px\] { min-height: 300px; }
  .md\:w-1\/2 { width: 50%; }
  .md\:w-\[100\%\] { width: 100%; }
  .md\:w-\[250px\] { width: 250px; }
  .md\:w-\[500px\] { width: 500px; }
  .md\:w-\[672px\] { width: 672px; }
  .md\:w-\[calc\(100\%-\.5rem\)\] { width: calc(100% - 0.5rem); }
  .md\:w-auto { width: auto; }
  .md\:w-full { width: 100%; }
  .md\:min-w-\[672px\] { min-width: 672px; }
  .md\:max-w-2xl { max-width: 42rem; }
  .md\:max-w-3xl { max-width: 48rem; }
  .md\:max-w-\[672px\] { max-width: 672px; }
  .md\:max-w-\[680px\] { max-width: 680px; }
  .md\:max-w-none { max-width: none; }
  .md\:max-w-xl { max-width: 36rem; }
  .md\:grid-cols-2 { grid-template-columns: repeat(2, minmax(0px, 1fr)); }
  .md\:grid-rows-\[minmax\(20px\,_1fr\)_auto_minmax\(20px\,_1fr\)\] { grid-template-rows: minmax(20px, 1fr) auto minmax(20px, 1fr); }
  .md\:flex-row { flex-direction: row; }
  .md\:flex-col { flex-direction: column; }
  .md\:items-end { align-items: flex-end; }
  .md\:justify-between { justify-content: space-between; }
  .md\:gap-2 { gap: 0.5rem; }
  .md\:gap-3 { gap: 0.75rem; }
  .md\:gap-6 { gap: 1.5rem; }
  .md\:gap-x-12 { column-gap: 3rem; }
  .md\:overflow-visible { overflow: visible; }
  .md\:whitespace-normal { white-space: normal; }
  .md\:rounded-none { border-radius: 0px; }
  .md\:border { border-width: 1px; }
  .md\:border-l { border-left-width: 1px; }
  .md\:border-t-0 { border-top-width: 0px; }
  .md\:border-gray-100 { --tw-border-opacity:1; border-color: rgba(236,236,241,var(--tw-border-opacity)); }
  .md\:border-transparent { border-color: transparent; }
  .md\:\!bg-transparent { background-color: transparent !important; }
  .md\:bg-vert-light-gradient { background-image: linear-gradient(rgba(255, 255, 255, 0) 13.94%, rgb(255, 255, 255) 54.73%); }
  .md\:p-2 { padding: 0.5rem; }
  .md\:px-0 { padding-left: 0px; padding-right: 0px; }
  .md\:px-6 { padding-left: 1.5rem; padding-right: 1.5rem; }
  .md\:px-8 { padding-left: 2rem; padding-right: 2rem; }
  .md\:px-\[60px\] { padding-left: 60px; padding-right: 60px; }
  .md\:py-10 { padding-bottom: 2.5rem; padding-top: 2.5rem; }
  .md\:py-2 { padding-bottom: 0.5rem; padding-top: 0.5rem; }
  .md\:py-2\.5 { padding-bottom: 0.625rem; padding-top: 0.625rem; }
  .md\:py-4 { padding-bottom: 1rem; padding-top: 1rem; }
  .md\:py-6 { padding-bottom: 1.5rem; padding-top: 1.5rem; }
  .md\:py-\[22px\] { padding-bottom: 22px; padding-top: 22px; }
  .md\:\!pt-0 { padding-top: 0px !important; }
  .md\:pb-5 { padding-bottom: 1.25rem; }
  .md\:pb-6 { padding-bottom: 1.5rem; }
  .md\:pb-\[8vh\] { padding-bottom: 8vh; }
  .md\:pl-0 { padding-left: 0px; }
  .md\:pl-2 { padding-left: 0.5rem; }
  .md\:pl-4 { padding-left: 1rem; }
  .md\:pl-6 { padding-left: 1.5rem; }
  .md\:pl-\[46px\] { padding-left: 46px; }
  .md\:pr-1 { padding-right: 0.25rem; }
  .md\:pr-1\.5 { padding-right: 0.375rem; }
  .md\:pr-12 { padding-right: 3rem; }
  .md\:pr-4 { padding-right: 1rem; }
  .md\:pr-8 { padding-right: 2rem; }
  .md\:pt-3 { padding-top: 0.75rem; }
  .md\:text-justify { text-align: justify; }
  .md\:text-\[32px\] { font-size: 32px; }
  .md\:text-\[40px\] { font-size: 40px; }
  .md\:text-sm { font-size: 0.875rem; line-height: 1.25rem; }
  .md\:leading-8 { line-height: 2rem; }
  .md\:opacity-0 { opacity: 0; }
  .md\:last\:mb-6:last-child { margin-bottom: 1.5rem; }
  .md\:disabled\:bottom-0:disabled { bottom: 0px; }
  .group:hover .md\:group-hover\:visible { visibility: visible; }
  .dark .md\:dark\:border-gray-700 { --tw-border-opacity:1; border-color: rgba(64,65,79,var(--tw-border-opacity)); }
  .dark .md\:dark\:border-transparent { border-color: transparent; }
  .dark .dark\:md\:bg-vert-dark-gradient { background-image: linear-gradient(rgba(53, 55, 64, 0), rgb(53, 55, 64) 58.85%); }
}

@media (min-width: 1024px) {
  .lg\:absolute { position: absolute; }
  .lg\:relative { position: relative; }
  .lg\:-left-3 { left: -0.75rem; }
  .lg\:-right-3 { right: -0.75rem; }
  .lg\:bottom-\[120px\] { bottom: 120px; }
  .lg\:left-auto { left: auto; }
  .lg\:right-0 { right: 0px; }
  .lg\:top-0 { top: 0px; }
  .lg\:top-1\/2 { top: 50%; }
  .lg\:-mx-8 { margin-left: -2rem; margin-right: -2rem; }
  .lg\:mx-auto { margin-left: auto; margin-right: auto; }
  .lg\:-ml-2 { margin-left: -0.5rem; }
  .lg\:mb-0 { margin-bottom: 0px; }
  .lg\:mt-0 { margin-top: 0px; }
  .lg\:block { display: block; }
  .lg\:hidden { display: none; }
  .lg\:w-1\/3 { width: 33.3333%; }
  .lg\:w-3\/5 { width: 60%; }
  .lg\:w-\[300px\] { width: 300px; }
  .lg\:w-\[896px\] { width: 896px; }
  .lg\:w-\[calc\(\(100\%-24px\)\/3\)\] { width: calc(33.3333% - 8px); }
  .lg\:w-\[calc\(100\%-115px\)\] { width: calc(100% - 115px); }
  .lg\:w-auto { width: auto; }
  .lg\:min-w-\[896px\] { min-width: 896px; }
  .lg\:max-w-2xl { max-width: 42rem; }
  .lg\:max-w-3xl { max-width: 48rem; }
  .lg\:max-w-\[38rem\] { max-width: 38rem; }
  .lg\:max-w-\[796px\] { max-width: 796px; }
  .lg\:max-w-\[896px\] { max-width: 896px; }
  .lg\:max-w-none { max-width: none; }
  .lg\:max-w-sm { max-width: 24rem; }
  .lg\:max-w-xl { max-width: 36rem; }
  .lg\:-translate-x-full { --tw-translate-x:-100%; }
  .lg\:-translate-x-full, .lg\:-translate-y-1\/2 { transform: translate(var(--tw-translate-x),var(--tw-translate-y)) rotate(var(--tw-rotate)) skewX(var(--tw-skew-x)) skewY(var(--tw-skew-y)) scaleX(var(--tw-scale-x)) scaleY(var(--tw-scale-y)); }
  .lg\:-translate-y-1\/2 { --tw-translate-y:-50%; }
  .lg\:translate-x-full { --tw-translate-x:100%; }
  .lg\:translate-x-full, .lg\:translate-y-0 { transform: translate(var(--tw-translate-x),var(--tw-translate-y)) rotate(var(--tw-rotate)) skewX(var(--tw-skew-x)) skewY(var(--tw-skew-y)) scaleX(var(--tw-scale-x)) scaleY(var(--tw-scale-y)); }
  .lg\:translate-y-0 { --tw-translate-y:0px; }
  .lg\:grid-cols-3 { grid-template-columns: repeat(3, minmax(0px, 1fr)); }
  .lg\:grid-cols-4 { grid-template-columns: repeat(4, minmax(0px, 1fr)); }
  .lg\:grid-cols-\[60\%_40\%\] { grid-template-columns: 60% 40%; }
  .lg\:gap-1 { gap: 0.25rem; }
  .lg\:self-center { align-self: center; }
  .lg\:rounded-lg { border-radius: 0.5rem; }
  .lg\:rounded-none { border-radius: 0px; }
  .lg\:border { border-width: 1px; }
  .lg\:border-0 { border-width: 0px; }
  .lg\:p-0 { padding: 0px; }
  .lg\:p-6 { padding: 1.5rem; }
  .lg\:px-0 { padding-left: 0px; padding-right: 0px; }
  .lg\:px-14 { padding-left: 3.5rem; padding-right: 3.5rem; }
  .lg\:px-16 { padding-left: 4rem; padding-right: 4rem; }
  .lg\:px-8 { padding-left: 2rem; padding-right: 2rem; }
  .lg\:pl-2 { padding-left: 0.5rem; }
  .lg\:pl-8 { padding-left: 2rem; }
  .lg\:pr-10 { padding-right: 2.5rem; }
  .lg\:pt-32 { padding-top: 8rem; }
  .lg\:text-left { text-align: left; }
  .lg\:text-\[22px\] { font-size: 22px; }
}

@media (min-width: 1280px) {
  .xl\:w-1\/4 { width: 25%; }
  .xl\:w-\[1024px\] { width: 1024px; }
  .xl\:w-\[350px\] { width: 350px; }
  .xl\:min-w-\[1024px\] { min-width: 1024px; }
  .xl\:max-w-3xl { max-width: 48rem; }
  .xl\:max-w-4xl { max-width: 56rem; }
  .xl\:max-w-6xl { max-width: 72rem; }
  .xl\:max-w-xl { max-width: 36rem; }
  .xl\:max-w-xs { max-width: 20rem; }
  .xl\:grid-cols-4 { grid-template-columns: repeat(4, minmax(0px, 1fr)); }
}

@media (min-width: 1536px) {
  .\32 xl\:w-\[400px\] { width: 400px; }
}

tr:last-child .\[tr\:last-child_\&\]\:border-b-0 { border-bottom-width: 0px; }

tr[data-disabled="true"] .\[tr\[data-disabled\=true\]_\&\]\:opacity-50 { opacity: 0.5; }

@font-face { font-family: KaTeX_AMS; font-style: normal; font-weight: 400; src: url("https://cdn.openai.com/common/fonts/katex/KaTeX_AMS-Regular.woff2") format("woff2"); }

@font-face { font-family: KaTeX_Caligraphic; font-style: normal; font-weight: 700; src: url("https://cdn.openai.com/common/fonts/katex/KaTeX_Caligraphic-Bold.woff2") format("woff2"); }

@font-face { font-family: KaTeX_Caligraphic; font-style: normal; font-weight: 400; src: url("https://cdn.openai.com/common/fonts/katex/KaTeX_Caligraphic-Regular.woff2") format("woff2"); }

@font-face { font-family: KaTeX_Fraktur; font-style: normal; font-weight: 700; src: url("https://cdn.openai.com/common/fonts/katex/KaTeX_Fraktur-Bold.woff2") format("woff2"); }

@font-face { font-family: KaTeX_Fraktur; font-style: normal; font-weight: 400; src: url("https://cdn.openai.com/common/fonts/katex/KaTeX_Fraktur-Regular.woff2") format("woff2"); }

@font-face { font-family: KaTeX_Main; font-style: normal; font-weight: 700; src: url("https://cdn.openai.com/common/fonts/katex/KaTeX_Main-Bold.woff2") format("woff2"); }

@font-face { font-family: KaTeX_Main; font-style: italic; font-weight: 700; src: url("https://cdn.openai.com/common/fonts/katex/KaTeX_Main-BoldItalic.woff2") format("woff2"); }

@font-face { font-family: KaTeX_Main; font-style: italic; font-weight: 400; src: url("https://cdn.openai.com/common/fonts/katex/KaTeX_Main-Italic.woff2") format("woff2"); }

@font-face { font-family: KaTeX_Main; font-style: normal; font-weight: 400; src: url("https://cdn.openai.com/common/fonts/katex/KaTeX_Main-Regular.woff2") format("woff2"); }

@font-face { font-family: KaTeX_Math; font-style: italic; font-weight: 700; src: url("https://cdn.openai.com/common/fonts/katex/KaTeX_Math-BoldItalic.woff2") format("woff2"); }

@font-face { font-family: KaTeX_Math; font-style: italic; font-weight: 400; src: url("https://cdn.openai.com/common/fonts/katex/KaTeX_Math-Italic.woff2") format("woff2"); }

@font-face { font-family: KaTeX_SansSerif; font-style: normal; font-weight: 700; src: url("https://cdn.openai.com/common/fonts/katex/KaTeX_SansSerif-Bold.woff2") format("woff2"); }

@font-face { font-family: KaTeX_SansSerif; font-style: italic; font-weight: 400; src: url("https://cdn.openai.com/common/fonts/katex/KaTeX_SansSerif-Italic.woff2") format("woff2"); }

@font-face { font-family: KaTeX_SansSerif; font-style: normal; font-weight: 400; src: url("https://cdn.openai.com/common/fonts/katex/KaTeX_SansSerif-Regular.woff2") format("woff2"); }

@font-face { font-family: KaTeX_Script; font-style: normal; font-weight: 400; src: url("https://cdn.openai.com/common/fonts/katex/KaTeX_Script-Regular.woff2") format("woff2"); }

@font-face { font-family: KaTeX_Size1; font-style: normal; font-weight: 400; src: url("https://cdn.openai.com/common/fonts/katex/KaTeX_Size1-Regular.woff2") format("woff2"); }

@font-face { font-family: KaTeX_Size2; font-style: normal; font-weight: 400; src: url("https://cdn.openai.com/common/fonts/katex/KaTeX_Size2-Regular.woff2") format("woff2"); }

@font-face { font-family: KaTeX_Size3; font-style: normal; font-weight: 400; src: url("https://cdn.openai.com/common/fonts/katex/KaTeX_Size3-Regular.woff2") format("woff2"); }

@font-face { font-family: KaTeX_Size4; font-style: normal; font-weight: 400; src: url("https://cdn.openai.com/common/fonts/katex/KaTeX_Size4-Regular.woff2") format("woff2"); }

@font-face { font-family: KaTeX_Typewriter; font-style: normal; font-weight: 400; src: url("https://cdn.openai.com/common/fonts/katex/KaTeX_Typewriter-Regular.woff2") format("woff2"); }

.katex { font: 1.21em / 1.2 KaTeX_Main, "Times New Roman", serif; text-indent: 0px; text-rendering: auto; }

.katex * { border-color: currentcolor; forced-color-adjust: none !important; }

.katex .katex-version::after { content: "0.16.0"; }

.katex .katex-mathml { clip: rect(1px, 1px, 1px, 1px); border: 0px; height: 1px; overflow: hidden; padding: 0px; position: absolute; width: 1px; }

.katex .katex-html > .newline { display: block; }

.katex .base { position: relative; white-space: nowrap; width: min-content; }

.katex .base, .katex .strut { display: inline-block; }

.katex .textbf { font-weight: 700; }

.katex .textit { font-style: italic; }

.katex .textrm { font-family: KaTeX_Main; }

.katex .textsf { font-family: KaTeX_SansSerif; }

.katex .texttt { font-family: KaTeX_Typewriter; }

.katex .mathnormal { font-family: KaTeX_Math; font-style: italic; }

.katex .mathit { font-family: KaTeX_Main; font-style: italic; }

.katex .mathrm { font-style: normal; }

.katex .mathbf { font-family: KaTeX_Main; font-weight: 700; }

.katex .boldsymbol { font-family: KaTeX_Math; font-style: italic; font-weight: 700; }

.katex .amsrm, .katex .mathbb, .katex .textbb { font-family: KaTeX_AMS; }

.katex .mathcal { font-family: KaTeX_Caligraphic; }

.katex .mathfrak, .katex .textfrak { font-family: KaTeX_Fraktur; }

.katex .mathtt { font-family: KaTeX_Typewriter; }

.katex .mathscr, .katex .textscr { font-family: KaTeX_Script; }

.katex .mathsf, .katex .textsf { font-family: KaTeX_SansSerif; }

.katex .mathboldsf, .katex .textboldsf { font-family: KaTeX_SansSerif; font-weight: 700; }

.katex .mathitsf, .katex .textitsf { font-family: KaTeX_SansSerif; font-style: italic; }

.katex .mainrm { font-family: KaTeX_Main; font-style: normal; }

.katex .vlist-t { border-collapse: collapse; display: inline-table; table-layout: fixed; }

.katex .vlist-r { display: table-row; }

.katex .vlist { display: table-cell; position: relative; vertical-align: bottom; }

.katex .vlist > span { display: block; height: 0px; position: relative; }

.katex .vlist > span > span { display: inline-block; }

.katex .vlist > span > .pstrut { overflow: hidden; width: 0px; }

.katex .vlist-t2 { margin-right: -2px; }

.katex .vlist-s { display: table-cell; font-size: 1px; min-width: 2px; vertical-align: bottom; width: 2px; }

.katex .vbox { align-items: baseline; display: inline-flex; flex-direction: column; }

.katex .hbox { width: 100%; }

.katex .hbox, .katex .thinbox { display: inline-flex; flex-direction: row; }

.katex .thinbox { max-width: 0px; width: 0px; }

.katex .msupsub { text-align: left; }

.katex .mfrac > span > span { text-align: center; }

.katex .mfrac .frac-line { border-bottom-style: solid; display: inline-block; width: 100%; }

.katex .hdashline, .katex .hline, .katex .mfrac .frac-line, .katex .overline .overline-line, .katex .rule, .katex .underline .underline-line { min-height: 1px; }

.katex .mspace { display: inline-block; }

.katex .clap, .katex .llap, .katex .rlap { position: relative; width: 0px; }

.katex .clap > .inner, .katex .llap > .inner, .katex .rlap > .inner { position: absolute; }

.katex .clap > .fix, .katex .llap > .fix, .katex .rlap > .fix { display: inline-block; }

.katex .llap > .inner { right: 0px; }

.katex .clap > .inner, .katex .rlap > .inner { left: 0px; }

.katex .clap > .inner > span { margin-left: -50%; margin-right: 50%; }

.katex .rule { border: 0px solid; display: inline-block; position: relative; }

.katex .hline, .katex .overline .overline-line, .katex .underline .underline-line { border-bottom-style: solid; display: inline-block; width: 100%; }

.katex .hdashline { border-bottom-style: dashed; display: inline-block; width: 100%; }

.katex .sqrt > .root { margin-left: 0.277778em; margin-right: -0.555556em; }

.katex .fontsize-ensurer.reset-size1.size1, .katex .sizing.reset-size1.size1 { font-size: 1em; }

.katex .fontsize-ensurer.reset-size1.size2, .katex .sizing.reset-size1.size2 { font-size: 1.2em; }

.katex .fontsize-ensurer.reset-size1.size3, .katex .sizing.reset-size1.size3 { font-size: 1.4em; }

.katex .fontsize-ensurer.reset-size1.size4, .katex .sizing.reset-size1.size4 { font-size: 1.6em; }

.katex .fontsize-ensurer.reset-size1.size5, .katex .sizing.reset-size1.size5 { font-size: 1.8em; }

.katex .fontsize-ensurer.reset-size1.size6, .katex .sizing.reset-size1.size6 { font-size: 2em; }

.katex .fontsize-ensurer.reset-size1.size7, .katex .sizing.reset-size1.size7 { font-size: 2.4em; }

.katex .fontsize-ensurer.reset-size1.size8, .katex .sizing.reset-size1.size8 { font-size: 2.88em; }

.katex .fontsize-ensurer.reset-size1.size9, .katex .sizing.reset-size1.size9 { font-size: 3.456em; }

.katex .fontsize-ensurer.reset-size1.size10, .katex .sizing.reset-size1.size10 { font-size: 4.148em; }

.katex .fontsize-ensurer.reset-size1.size11, .katex .sizing.reset-size1.size11 { font-size: 4.976em; }

.katex .fontsize-ensurer.reset-size2.size1, .katex .sizing.reset-size2.size1 { font-size: 0.833333em; }

.katex .fontsize-ensurer.reset-size2.size2, .katex .sizing.reset-size2.size2 { font-size: 1em; }

.katex .fontsize-ensurer.reset-size2.size3, .katex .sizing.reset-size2.size3 { font-size: 1.16667em; }

.katex .fontsize-ensurer.reset-size2.size4, .katex .sizing.reset-size2.size4 { font-size: 1.33333em; }

.katex .fontsize-ensurer.reset-size2.size5, .katex .sizing.reset-size2.size5 { font-size: 1.5em; }

.katex .fontsize-ensurer.reset-size2.size6, .katex .sizing.reset-size2.size6 { font-size: 1.66667em; }

.katex .fontsize-ensurer.reset-size2.size7, .katex .sizing.reset-size2.size7 { font-size: 2em; }

.katex .fontsize-ensurer.reset-size2.size8, .katex .sizing.reset-size2.size8 { font-size: 2.4em; }

.katex .fontsize-ensurer.reset-size2.size9, .katex .sizing.reset-size2.size9 { font-size: 2.88em; }

.katex .fontsize-ensurer.reset-size2.size10, .katex .sizing.reset-size2.size10 { font-size: 3.45667em; }

.katex .fontsize-ensurer.reset-size2.size11, .katex .sizing.reset-size2.size11 { font-size: 4.14667em; }

.katex .fontsize-ensurer.reset-size3.size1, .katex .sizing.reset-size3.size1 { font-size: 0.714286em; }

.katex .fontsize-ensurer.reset-size3.size2, .katex .sizing.reset-size3.size2 { font-size: 0.857143em; }

.katex .fontsize-ensurer.reset-size3.size3, .katex .sizing.reset-size3.size3 { font-size: 1em; }

.katex .fontsize-ensurer.reset-size3.size4, .katex .sizing.reset-size3.size4 { font-size: 1.14286em; }

.katex .fontsize-ensurer.reset-size3.size5, .katex .sizing.reset-size3.size5 { font-size: 1.28571em; }

.katex .fontsize-ensurer.reset-size3.size6, .katex .sizing.reset-size3.size6 { font-size: 1.42857em; }

.katex .fontsize-ensurer.reset-size3.size7, .katex .sizing.reset-size3.size7 { font-size: 1.71429em; }

.katex .fontsize-ensurer.reset-size3.size8, .katex .sizing.reset-size3.size8 { font-size: 2.05714em; }

.katex .fontsize-ensurer.reset-size3.size9, .katex .sizing.reset-size3.size9 { font-size: 2.46857em; }

.katex .fontsize-ensurer.reset-size3.size10, .katex .sizing.reset-size3.size10 { font-size: 2.96286em; }

.katex .fontsize-ensurer.reset-size3.size11, .katex .sizing.reset-size3.size11 { font-size: 3.55429em; }

.katex .fontsize-ensurer.reset-size4.size1, .katex .sizing.reset-size4.size1 { font-size: 0.625em; }

.katex .fontsize-ensurer.reset-size4.size2, .katex .sizing.reset-size4.size2 { font-size: 0.75em; }

.katex .fontsize-ensurer.reset-size4.size3, .katex .sizing.reset-size4.size3 { font-size: 0.875em; }

.katex .fontsize-ensurer.reset-size4.size4, .katex .sizing.reset-size4.size4 { font-size: 1em; }

.katex .fontsize-ensurer.reset-size4.size5, .katex .sizing.reset-size4.size5 { font-size: 1.125em; }

.katex .fontsize-ensurer.reset-size4.size6, .katex .sizing.reset-size4.size6 { font-size: 1.25em; }

.katex .fontsize-ensurer.reset-size4.size7, .katex .sizing.reset-size4.size7 { font-size: 1.5em; }

.katex .fontsize-ensurer.reset-size4.size8, .katex .sizing.reset-size4.size8 { font-size: 1.8em; }

.katex .fontsize-ensurer.reset-size4.size9, .katex .sizing.reset-size4.size9 { font-size: 2.16em; }

.katex .fontsize-ensurer.reset-size4.size10, .katex .sizing.reset-size4.size10 { font-size: 2.5925em; }

.katex .fontsize-ensurer.reset-size4.size11, .katex .sizing.reset-size4.size11 { font-size: 3.11em; }

.katex .fontsize-ensurer.reset-size5.size1, .katex .sizing.reset-size5.size1 { font-size: 0.555556em; }

.katex .fontsize-ensurer.reset-size5.size2, .katex .sizing.reset-size5.size2 { font-size: 0.666667em; }

.katex .fontsize-ensurer.reset-size5.size3, .katex .sizing.reset-size5.size3 { font-size: 0.777778em; }

.katex .fontsize-ensurer.reset-size5.size4, .katex .sizing.reset-size5.size4 { font-size: 0.888889em; }

.katex .fontsize-ensurer.reset-size5.size5, .katex .sizing.reset-size5.size5 { font-size: 1em; }

.katex .fontsize-ensurer.reset-size5.size6, .katex .sizing.reset-size5.size6 { font-size: 1.11111em; }

.katex .fontsize-ensurer.reset-size5.size7, .katex .sizing.reset-size5.size7 { font-size: 1.33333em; }

.katex .fontsize-ensurer.reset-size5.size8, .katex .sizing.reset-size5.size8 { font-size: 1.6em; }

.katex .fontsize-ensurer.reset-size5.size9, .katex .sizing.reset-size5.size9 { font-size: 1.92em; }

.katex .fontsize-ensurer.reset-size5.size10, .katex .sizing.reset-size5.size10 { font-size: 2.30444em; }

.katex .fontsize-ensurer.reset-size5.size11, .katex .sizing.reset-size5.size11 { font-size: 2.76444em; }

.katex .fontsize-ensurer.reset-size6.size1, .katex .sizing.reset-size6.size1 { font-size: 0.5em; }

.katex .fontsize-ensurer.reset-size6.size2, .katex .sizing.reset-size6.size2 { font-size: 0.6em; }

.katex .fontsize-ensurer.reset-size6.size3, .katex .sizing.reset-size6.size3 { font-size: 0.7em; }

.katex .fontsize-ensurer.reset-size6.size4, .katex .sizing.reset-size6.size4 { font-size: 0.8em; }

.katex .fontsize-ensurer.reset-size6.size5, .katex .sizing.reset-size6.size5 { font-size: 0.9em; }

.katex .fontsize-ensurer.reset-size6.size6, .katex .sizing.reset-size6.size6 { font-size: 1em; }

.katex .fontsize-ensurer.reset-size6.size7, .katex .sizing.reset-size6.size7 { font-size: 1.2em; }

.katex .fontsize-ensurer.reset-size6.size8, .katex .sizing.reset-size6.size8 { font-size: 1.44em; }

.katex .fontsize-ensurer.reset-size6.size9, .katex .sizing.reset-size6.size9 { font-size: 1.728em; }

.katex .fontsize-ensurer.reset-size6.size10, .katex .sizing.reset-size6.size10 { font-size: 2.074em; }

.katex .fontsize-ensurer.reset-size6.size11, .katex .sizing.reset-size6.size11 { font-size: 2.488em; }

.katex .fontsize-ensurer.reset-size7.size1, .katex .sizing.reset-size7.size1 { font-size: 0.416667em; }

.katex .fontsize-ensurer.reset-size7.size2, .katex .sizing.reset-size7.size2 { font-size: 0.5em; }

.katex .fontsize-ensurer.reset-size7.size3, .katex .sizing.reset-size7.size3 { font-size: 0.583333em; }

.katex .fontsize-ensurer.reset-size7.size4, .katex .sizing.reset-size7.size4 { font-size: 0.666667em; }

.katex .fontsize-ensurer.reset-size7.size5, .katex .sizing.reset-size7.size5 { font-size: 0.75em; }

.katex .fontsize-ensurer.reset-size7.size6, .katex .sizing.reset-size7.size6 { font-size: 0.833333em; }

.katex .fontsize-ensurer.reset-size7.size7, .katex .sizing.reset-size7.size7 { font-size: 1em; }

.katex .fontsize-ensurer.reset-size7.size8, .katex .sizing.reset-size7.size8 { font-size: 1.2em; }

.katex .fontsize-ensurer.reset-size7.size9, .katex .sizing.reset-size7.size9 { font-size: 1.44em; }

.katex .fontsize-ensurer.reset-size7.size10, .katex .sizing.reset-size7.size10 { font-size: 1.72833em; }

.katex .fontsize-ensurer.reset-size7.size11, .katex .sizing.reset-size7.size11 { font-size: 2.07333em; }

.katex .fontsize-ensurer.reset-size8.size1, .katex .sizing.reset-size8.size1 { font-size: 0.347222em; }

.katex .fontsize-ensurer.reset-size8.size2, .katex .sizing.reset-size8.size2 { font-size: 0.416667em; }

.katex .fontsize-ensurer.reset-size8.size3, .katex .sizing.reset-size8.size3 { font-size: 0.486111em; }

.katex .fontsize-ensurer.reset-size8.size4, .katex .sizing.reset-size8.size4 { font-size: 0.555556em; }

.katex .fontsize-ensurer.reset-size8.size5, .katex .sizing.reset-size8.size5 { font-size: 0.625em; }

.katex .fontsize-ensurer.reset-size8.size6, .katex .sizing.reset-size8.size6 { font-size: 0.694444em; }

.katex .fontsize-ensurer.reset-size8.size7, .katex .sizing.reset-size8.size7 { font-size: 0.833333em; }

.katex .fontsize-ensurer.reset-size8.size8, .katex .sizing.reset-size8.size8 { font-size: 1em; }

.katex .fontsize-ensurer.reset-size8.size9, .katex .sizing.reset-size8.size9 { font-size: 1.2em; }

.katex .fontsize-ensurer.reset-size8.size10, .katex .sizing.reset-size8.size10 { font-size: 1.44028em; }

.katex .fontsize-ensurer.reset-size8.size11, .katex .sizing.reset-size8.size11 { font-size: 1.72778em; }

.katex .fontsize-ensurer.reset-size9.size1, .katex .sizing.reset-size9.size1 { font-size: 0.289352em; }

.katex .fontsize-ensurer.reset-size9.size2, .katex .sizing.reset-size9.size2 { font-size: 0.347222em; }

.katex .fontsize-ensurer.reset-size9.size3, .katex .sizing.reset-size9.size3 { font-size: 0.405093em; }

.katex .fontsize-ensurer.reset-size9.size4, .katex .sizing.reset-size9.size4 { font-size: 0.462963em; }

.katex .fontsize-ensurer.reset-size9.size5, .katex .sizing.reset-size9.size5 { font-size: 0.520833em; }

.katex .fontsize-ensurer.reset-size9.size6, .katex .sizing.reset-size9.size6 { font-size: 0.578704em; }

.katex .fontsize-ensurer.reset-size9.size7, .katex .sizing.reset-size9.size7 { font-size: 0.694444em; }

.katex .fontsize-ensurer.reset-size9.size8, .katex .sizing.reset-size9.size8 { font-size: 0.833333em; }

.katex .fontsize-ensurer.reset-size9.size9, .katex .sizing.reset-size9.size9 { font-size: 1em; }

.katex .fontsize-ensurer.reset-size9.size10, .katex .sizing.reset-size9.size10 { font-size: 1.20023em; }

.katex .fontsize-ensurer.reset-size9.size11, .katex .sizing.reset-size9.size11 { font-size: 1.43981em; }

.katex .fontsize-ensurer.reset-size10.size1, .katex .sizing.reset-size10.size1 { font-size: 0.24108em; }

.katex .fontsize-ensurer.reset-size10.size2, .katex .sizing.reset-size10.size2 { font-size: 0.289296em; }

.katex .fontsize-ensurer.reset-size10.size3, .katex .sizing.reset-size10.size3 { font-size: 0.337512em; }

.katex .fontsize-ensurer.reset-size10.size4, .katex .sizing.reset-size10.size4 { font-size: 0.385728em; }

.katex .fontsize-ensurer.reset-size10.size5, .katex .sizing.reset-size10.size5 { font-size: 0.433944em; }

.katex .fontsize-ensurer.reset-size10.size6, .katex .sizing.reset-size10.size6 { font-size: 0.48216em; }

.katex .fontsize-ensurer.reset-size10.size7, .katex .sizing.reset-size10.size7 { font-size: 0.578592em; }

.katex .fontsize-ensurer.reset-size10.size8, .katex .sizing.reset-size10.size8 { font-size: 0.694311em; }

.katex .fontsize-ensurer.reset-size10.size9, .katex .sizing.reset-size10.size9 { font-size: 0.833173em; }

.katex .fontsize-ensurer.reset-size10.size10, .katex .sizing.reset-size10.size10 { font-size: 1em; }

.katex .fontsize-ensurer.reset-size10.size11, .katex .sizing.reset-size10.size11 { font-size: 1.19961em; }

.katex .fontsize-ensurer.reset-size11.size1, .katex .sizing.reset-size11.size1 { font-size: 0.200965em; }

.katex .fontsize-ensurer.reset-size11.size2, .katex .sizing.reset-size11.size2 { font-size: 0.241158em; }

.katex .fontsize-ensurer.reset-size11.size3, .katex .sizing.reset-size11.size3 { font-size: 0.28135em; }

.katex .fontsize-ensurer.reset-size11.size4, .katex .sizing.reset-size11.size4 { font-size: 0.321543em; }

.katex .fontsize-ensurer.reset-size11.size5, .katex .sizing.reset-size11.size5 { font-size: 0.361736em; }

.katex .fontsize-ensurer.reset-size11.size6, .katex .sizing.reset-size11.size6 { font-size: 0.401929em; }

.katex .fontsize-ensurer.reset-size11.size7, .katex .sizing.reset-size11.size7 { font-size: 0.482315em; }

.katex .fontsize-ensurer.reset-size11.size8, .katex .sizing.reset-size11.size8 { font-size: 0.578778em; }

.katex .fontsize-ensurer.reset-size11.size9, .katex .sizing.reset-size11.size9 { font-size: 0.694534em; }

.katex .fontsize-ensurer.reset-size11.size10, .katex .sizing.reset-size11.size10 { font-size: 0.833601em; }

.katex .fontsize-ensurer.reset-size11.size11, .katex .sizing.reset-size11.size11 { font-size: 1em; }

.katex .delimsizing.size1 { font-family: KaTeX_Size1; }

.katex .delimsizing.size2 { font-family: KaTeX_Size2; }

.katex .delimsizing.size3 { font-family: KaTeX_Size3; }

.katex .delimsizing.size4 { font-family: KaTeX_Size4; }

.katex .delimsizing.mult .delim-size1 > span { font-family: KaTeX_Size1; }

.katex .delimsizing.mult .delim-size4 > span { font-family: KaTeX_Size4; }

.katex .nulldelimiter { display: inline-block; width: 0.12em; }

.katex .delimcenter, .katex .op-symbol { position: relative; }

.katex .op-symbol.small-op { font-family: KaTeX_Size1; }

.katex .op-symbol.large-op { font-family: KaTeX_Size2; }

.katex .accent > .vlist-t, .katex .op-limits > .vlist-t { text-align: center; }

.katex .accent .accent-body { position: relative; }

.katex .accent .accent-body:not(.accent-full) { width: 0px; }

.katex .overlay { display: block; }

.katex .mtable .vertical-separator { display: inline-block; min-width: 1px; }

.katex .mtable .arraycolsep { display: inline-block; }

.katex .mtable .col-align-c > .vlist-t { text-align: center; }

.katex .mtable .col-align-l > .vlist-t { text-align: left; }

.katex .mtable .col-align-r > .vlist-t { text-align: right; }

.katex .svg-align { text-align: left; }

.katex svg { fill: currentcolor; stroke: currentcolor; fill-rule: nonzero; fill-opacity: 1; stroke-width: 1; stroke-linecap: butt; stroke-linejoin: miter; stroke-miterlimit: 4; stroke-dasharray: none; stroke-dashoffset: 0; stroke-opacity: 1; display: block; height: inherit; position: absolute; width: 100%; }

.katex svg path { stroke: none; }

.katex img { border-style: none; max-height: none; max-width: none; min-height: 0px; min-width: 0px; }

.katex .stretchy { display: block; overflow: hidden; position: relative; width: 100%; }

.katex .stretchy::after, .katex .stretchy::before { content: ""; }

.katex .hide-tail { overflow: hidden; position: relative; width: 100%; }

.katex .halfarrow-left { left: 0px; overflow: hidden; position: absolute; width: 50.2%; }

.katex .halfarrow-right { overflow: hidden; position: absolute; right: 0px; width: 50.2%; }

.katex .brace-left { left: 0px; overflow: hidden; position: absolute; width: 25.1%; }

.katex .brace-center { left: 25%; overflow: hidden; position: absolute; width: 50%; }

.katex .brace-right { overflow: hidden; position: absolute; right: 0px; width: 25.1%; }

.katex .x-arrow-pad { padding: 0px 0.5em; }

.katex .cd-arrow-pad { padding: 0px 0.55556em 0px 0.27778em; }

.katex .mover, .katex .munder, .katex .x-arrow { text-align: center; }

.katex .boxpad { padding: 0px 0.3em; }

.katex .fbox, .katex .fcolorbox { border: 0.04em solid; box-sizing: border-box; }

.katex .cancel-pad { padding: 0px 0.2em; }

.katex .cancel-lap { margin-left: -0.2em; margin-right: -0.2em; }

.katex .sout { border-bottom-style: solid; border-bottom-width: 0.08em; }

.katex .angl { border-right: 0.049em solid; border-top: 0.049em solid; box-sizing: border-box; margin-right: 0.03889em; }

.katex .anglpad { padding: 0px 0.03889em; }

.katex .eqn-num::before { content: "(" counter(katexEqnNo) ")"; counter-increment: katexEqnNo 1; }

.katex .mml-eqn-num::before { content: "(" counter(mmlEqnNo) ")"; counter-increment: mmlEqnNo 1; }

.katex .mtr-glue { width: 50%; }

.katex .cd-vert-arrow { display: inline-block; position: relative; }

.katex .cd-label-left { display: inline-block; position: absolute; right: calc(50% + 0.3em); text-align: left; }

.katex .cd-label-right { display: inline-block; left: calc(50% + 0.3em); position: absolute; text-align: right; }

.katex-display { display: block; margin: 1em 0px; text-align: center; }

.katex-display > .katex { display: block; text-align: center; white-space: nowrap; }

.katex-display > .katex > .katex-html { display: block; position: relative; }

.katex-display > .katex > .katex-html > .tag { position: absolute; right: 0px; }

.katex-display.leqno > .katex > .katex-html > .tag { left: 0px; right: auto; }

.katex-display.fleqn > .katex { padding-left: 2em; text-align: left; }

body { counter-reset: katexEqnNo 0 mmlEqnNo 0; }

code, pre { font-family: "SÃ¶hne Mono", Monaco, "Andale Mono", "Ubuntu Mono", monospace !important; }

code[class="language-plaintext"] { white-space: pre-line; }

code.hljs, code[class*="language-"], pre[class*="language-"] { overflow-wrap: normal; background: none; color: rgb(255, 255, 255); hyphens: none; line-height: 1.5; tab-size: 4; text-align: left; white-space: pre; word-break: normal; word-spacing: normal; }

pre[class*="language-"] { border-radius: 0.3em; overflow: auto; }

:not(pre) > code.hljs, :not(pre) > code[class*="language-"] { border-radius: 0.3em; padding: 0.1em; white-space: normal; }

.hljs-comment { color: rgba(255, 255, 255, 0.5); }

.hljs-meta { color: rgba(255, 255, 255, 0.6); }

.hljs-built_in, .hljs-class .hljs-title { color: rgb(233, 149, 12); }

.hljs-doctag, .hljs-formula, .hljs-keyword, .hljs-literal { color: rgb(46, 149, 211); }

.hljs-addition, .hljs-attribute, .hljs-meta-string, .hljs-regexp, .hljs-string { color: rgb(0, 166, 125); }

.hljs-attr, .hljs-number, .hljs-selector-attr, .hljs-selector-class, .hljs-selector-pseudo, .hljs-template-variable, .hljs-type, .hljs-variable { color: rgb(223, 48, 121); }

.hljs-bullet, .hljs-link, .hljs-selector-id, .hljs-symbol, .hljs-title { color: rgb(242, 44, 61); }

.token.cdata, .token.comment, .token.doctype, .token.prolog { color: rgb(169, 174, 193); }

.token.punctuation { color: rgb(254, 254, 254); }

.token.constant, .token.deleted, .token.property, .token.symbol, .token.tag { color: rgb(255, 160, 122); }

.token.boolean, .token.number { color: rgb(0, 224, 224); }

.token.attr-name, .token.builtin, .token.char, .token.inserted, .token.selector, .token.string { color: rgb(171, 227, 56); }

.language-css .token.string, .style .token.string, .token.entity, .token.operator, .token.url, .token.variable { color: rgb(0, 224, 224); }

.token.atrule, .token.attr-value, .token.function { color: gold; }

.token.keyword { color: rgb(0, 224, 224); }

.token.important, .token.regex { color: gold; }

.token.bold, .token.important { font-weight: 700; }

.token.italic { font-style: italic; }

.token.entity { cursor: help; }

@media screen and (-ms-high-contrast: active) {
  code[class*="language-"], pre[class*="language-"] { background: window; color: windowtext; }
  :not(pre) > code[class*="language-"], pre[class*="language-"] { background: window; }
  .token.important { background: highlight; color: window; font-weight: 400; }
  .token.atrule, .token.attr-value, .token.function, .token.keyword, .token.operator, .token.selector { font-weight: 700; }
  .token.attr-value, .token.comment, .token.doctype, .token.function, .token.keyword, .token.operator, .token.property, .token.string { color: highlight; }
  .token.attr-value, .token.url { font-weight: 400; }
}

.PopoverContent { z-index: 1000; }

.react-select-container input:focus { --tw-ring-offset-shadow:var(--tw-ring-inset) 0 0 0 var(--tw-ring-offset-width) var(--tw-ring-offset-color); --tw-ring-shadow:var(--tw-ring-inset) 0 0 0 calc(var(--tw-ring-offset-width)) var(--tw-ring-color); box-shadow: var(--tw-ring-offset-shadow),var(--tw-ring-shadow),var(--tw-shadow,0 0 transparent); }

.react-select-container .react-select__control { --tw-border-opacity:1; align-items: flex-start; border-color: rgba(236,236,241,var(--tw-border-opacity)); border-radius: 0.5rem; font-size: 0.875rem; height: 192px; line-height: 1.25rem; min-height: 192px; overflow: auto; padding: 0.5rem; }

.react-select-container .react-select__control:hover { --tw-border-opacity:1; border-color: rgba(217,217,227,var(--tw-border-opacity)); }

.dark .react-select-container .react-select__control { --tw-bg-opacity:1; background-color: rgba(32,33,35,var(--tw-bg-opacity)); border-color: rgba(255, 255, 255, 0.1); }

:is(.dark .react-select-container .react-select__control):hover { border-color: rgba(255, 255, 255, 0.2); }

.dark .react-select-container.react-select--invalid .react-select__control, .react-select-container.react-select--invalid .react-select__control { --tw-border-opacity:1; border-color: rgba(239,68,68,var(--tw-border-opacity)); }

.react-select-container .react-select__input-container, .react-select-container .react-select__placeholder { padding-left: 0.5rem; padding-right: 0.5rem; }

.dark .react-select-container .react-select__input-container { --tw-text-opacity:1; color: rgba(197,197,210,var(--tw-text-opacity)); }

.react-select-container .react-select__input { height: 2rem; }

.react-select-container .react-select__control--is-focused { --tw-ring-offset-shadow:var(--tw-ring-inset) 0 0 0 var(--tw-ring-offset-width) var(--tw-ring-offset-color); --tw-ring-shadow:var(--tw-ring-inset) 0 0 0 calc(var(--tw-ring-offset-width)) var(--tw-ring-color); box-shadow: var(--tw-ring-offset-shadow),var(--tw-ring-shadow),var(--tw-shadow,0 0 transparent); --tw-border-opacity:1 !important; border-color: rgba(217,217,227,var(--tw-border-opacity)) !important; }

.dark .react-select-container .react-select__control--is-focused { border-color: rgba(255, 255, 255, 0.2) !important; }

.react-select-container .react-select__value-container { padding: 0px; }

.react-select-container .react-select__indicators:empty { display: none; }

.react-select-container .react-select__multi-value { background-color: transparent; margin: 0px; }

.react-select-container .react-select__multi-value__label { padding: 0px; }

#intercom-container { display: none; }
------MultipartBoundary--vS53nJOjydVm2jf5zsp0U2JCPSCh5LRhvu8f55va6M----
Content-Type: image/webp
Content-Transfer-Encoding: binary
Content-Location: https://chat.openai.com/_next/image?url=https%3A%2F%2Fs.gravatar.com%2Favatar%2F6241bfe48785687ad5d68f5c836fbe6d%3Fs%3D480%26r%3Dpg%26d%3Dhttps%253A%252F%252Fcdn.auth0.com%252Favatars%252Fna.png&w=96&q=75

RIFF   WEBPVP8   Ð
 *` ` >H J¥¥#¡¦[h°	c Rºèlyæâ¿-@J±ÝÔôuÃcµX
jsrÔI¬ØzPäDà þ¸dýWÜÏT¿â½X:` ÖÍ¶ÙÀ¶*òÿÝD@øP6à^ðf%ÎÕf «ZDÔó*w}ò3ï¿æK+-8s|¾Xµº®OÃv¶9|'Î(YæÞ÷|Ø94C1&M!|ìàß¢(ûþ¹pÜE°+þÛm¥YÞëÂ2ø>³±'×a'|ÚlÚnDÜ:vÁÄ8UPÎ
¸à&w®d©X²7¼òT÷Ø6üâr þÈC<G{ÌÁ,Û&c.{Çìuüo°õPbeköÞlLöËC/ìBRoÖ¦¤f¨Ë0ð#ÂMËJÈ¨èY 'äC)T­»7¢Z »-nÆ×Øç_±(CÎÆ;æ2ÀèGØp; Ñ"ÛÂ#¶N<<\¸8%þÞÏ«DÌ WÊ1¢±£MçM¼q·©ôoÈ½ãÍÌóä Ä#<¿SäÅE2HÌ
án²Y½À     
------MultipartBoundary--vS53nJOjydVm2jf5zsp0U2JCPSCh5LRhvu8f55va6M----
Content-Type: text/html
Content-ID: 
Content-Transfer-Encoding: binary





------MultipartBoundary--vS53nJOjydVm2jf5zsp0U2JCPSCh5LRhvu8f55va6M----
Content-Type: text/html
Content-ID: 
Content-Transfer-Encoding: binary
Content-Location: https://tcr9i.chat.openai.com/v2/1.5.5/enforcement.fbfc14b0d793c6ef8359e0e4b4a91f67.html#35536E1E-65B4-4D96-9D97-6ADB7EFF8147


------MultipartBoundary--vS53nJOjydVm2jf5zsp0U2JCPSCh5LRhvu8f55va6M----
Content-Type: text/css
Content-Transfer-Encoding: binary
Content-Location: cid:css-886ba51b-a988-416d-b08d-91d9af84a91d@mhtml.blink

@charset "utf-8";

html, body { margin: 0px; padding: 0px; height: 100%; }

* { box-sizing: border-box; }

#app { height: 100%; overflow: hidden; }
------MultipartBoundary--vS53nJOjydVm2jf5zsp0U2JCPSCh5LRhvu8f55va6M----
Content-Type: text/css
Content-Transfer-Encoding: binary
Content-Location: cid:css-74cdc30c-bbf9-4958-b290-f2e09badf666@mhtml.blink

@charset "utf-8";

@keyframes spin { 
  0% { transform: rotate(0deg) translateZ(0px); }
  100% { transform: rotate(360deg) translateZ(0px); }
}

@keyframes fadeIn { 
  0% { opacity: 0; }
  100% { opacity: 1; }
}

.kXaXkDzIfSzLHRklX2r3 { position: fixed; top: 20px; right: 20px; width: 20px; height: 20px; z-index: 200; border: none; background-color: transparent; background-image: url("data:image/svg+xml;base64,PD94bWwgdmVyc2lvbj0iMS4wIiBlbmNvZGluZz0iVVRGLTgiPz4KPHN2ZyB3aWR0aD0iMzhweCIgaGVpZ2h0PSIzOHB4IiB2aWV3Qm94PSIwIDAgMzggMzgiIHZlcnNpb249IjEuMSIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIiB4bWxuczp4bGluaz0iaHR0cDovL3d3dy53My5vcmcvMTk5OS94bGluayI+CiAgICA8IS0tIEdlbmVyYXRvcjogU2tldGNoIDQ0LjEgKDQxNDU1KSAtIGh0dHA6Ly93d3cuYm9oZW1pYW5jb2RpbmcuY29tL3NrZXRjaCAtLT4KICAgIDx0aXRsZT5Hcm91cDwvdGl0bGU+CiAgICA8ZGVzYz5DcmVhdGVkIHdpdGggU2tldGNoLjwvZGVzYz4KICAgIDxkZWZzPjwvZGVmcz4KICAgIDxnIGlkPSJQYWdlLTEiIHN0cm9rZT0ibm9uZSIgc3Ryb2tlLXdpZHRoPSIxIiBmaWxsPSJub25lIiBmaWxsLXJ1bGU9ImV2ZW5vZGQiPgogICAgICAgIDxnIGlkPSJHcm91cCIgdHJhbnNmb3JtPSJ0cmFuc2xhdGUoMTkuMDAwMDAwLCAxOS4wMDAwMDApIHJvdGF0ZSg0NS4wMDAwMDApIHRyYW5zbGF0ZSgtMTkuMDAwMDAwLCAtMTkuMDAwMDAwKSB0cmFuc2xhdGUoLTYuMDAwMDAwLCAtNi4wMDAwMDApIiBmaWxsPSIjOTQ5NDk0Ij4KICAgICAgICAgICAgPHJlY3QgaWQ9IlJlY3RhbmdsZSIgeD0iMjQiIHk9IjAiIHdpZHRoPSIyIiBoZWlnaHQ9IjUwIj48L3JlY3Q+CiAgICAgICAgICAgIDxyZWN0IGlkPSJSZWN0YW5nbGUiIHRyYW5zZm9ybT0idHJhbnNsYXRlKDI1LjAwMDAwMCwgMjUuMDAwMDAwKSByb3RhdGUoOTAuMDAwMDAwKSB0cmFuc2xhdGUoLTI1LjAwMDAwMCwgLTI1LjAwMDAwMCkgIiB4PSIyNCIgeT0iMCIgd2lkdGg9IjIiIGhlaWdodD0iNTAiPjwvcmVjdD4KICAgICAgICA8L2c+CiAgICA8L2c+Cjwvc3ZnPg=="); background-repeat: no-repeat; background-position: center center; background-size: contain; cursor: pointer; pointer-events: none; transform: scale(1); transition: transform 100ms ease-in-out 0s; visibility: hidden; opacity: 0; }

.kXaXkDzIfSzLHRklX2r3:hover { transform: scale(1.3); }

.kXaXkDzIfSzLHRklX2r3.active { pointer-events: inherit; visibility: visible; opacity: 1; transition: opacity 400ms ease-in-out 0s; }

.nMUBxApsE7lSELvJiiAA { position: fixed; inset: 0px; background-color: rgba(255, 255, 255, 0.8); z-index: -1; opacity: 0; transition: opacity 300ms linear 0s; pointer-events: none; }

.nMUBxApsE7lSELvJiiAA.active { opacity: 1; pointer-events: inherit; }

.lTLYGVw1ASsTZWg0vUbC { position: absolute; top: 50%; left: 50%; margin-left: -15px; margin-top: -15px; z-index: 1010; border-radius: 50%; width: 30px; height: 30px; border-width: 3px; border-style: solid; border-color: transparent transparent transparent rgba(0, 0, 0, 0.2); transform: translateZ(0px); box-sizing: border-box; }

.lTLYGVw1ASsTZWg0vUbC { animation: 500ms linear 0s infinite normal none running spin; }

.slrEYyWESVLe_Cx3DM1k { transition: opacity 500ms ease 0s, transform 500ms ease 0s; opacity: 0; transform: scale(0.8); text-align: center; height: 100%; }

.slrEYyWESVLe_Cx3DM1k.active { opacity: 1; transform: scale(1); }

.slrEYyWESVLe_Cx3DM1k.challenge-enter { opacity: 0; transform: scale(0.8); }

.slrEYyWESVLe_Cx3DM1k.challenge-enter-active, .slrEYyWESVLe_Cx3DM1k.challenge-enter-done { transform: scale(1); opacity: 1; }

.slrEYyWESVLe_Cx3DM1k.challenge-exit { transform: scale(1); opacity: 1; }

.slrEYyWESVLe_Cx3DM1k.challenge-exit-active, .slrEYyWESVLe_Cx3DM1k.challenge-exit-done { transform: scale(0.8); opacity: 0; }

.slrEYyWESVLe_Cx3DM1k.dTcazKGGob_VbK_J853h { flex-direction: column; }

.dTcazKGGob_VbK_J853h { display: flex; align-items: center; justify-content: center; }
------MultipartBoundary--vS53nJOjydVm2jf5zsp0U2JCPSCh5LRhvu8f55va6M------
