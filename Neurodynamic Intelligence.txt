Neurodynamic Intelligence

Summarize: Yoshua Bengio

AI Scientists: Safe and Useful AI?

Published 7 May 2023 by yoshuabengio

https://yoshuabengio.org/2023/05/07/ai-scientists-safe-
and-useful-ai

There have recently been lots of discussions about the risks 
of AI, whether in the short term with existing methods or in 
the longer term with advances we can anticipate. I have been 
very vocal about the importance of accelerating regulation, 
both nationally and internationally, which I think could help 
us mitigate issues of discrimination, bias, fake news, 
disinformation, etc. Other anticipated negative outcomes like 
shocks to job markets require changes in the social safety net 
and education system. The use of AI in the military, especially 
with lethal autonomous weapons has been a big concern for 
many years and clearly requires international coordination. 

In this post however, I would like to share my thoughts 
regarding the more hotly debated question of long-term risks 
associated with AI systems which do not yet exist, where one 
imagines the possibility of AI systems behaving in a way that is 
dangerously misaligned with human rights  or even loss of 
control of AI systems that could become threats to humanity. 
A key argument is that as soon as AI systems are given goals – 
to satisfy our needs – they may create subgoals that are not 

well-aligned with what we really want and could even become 
dangerous for humans. 

Main thesis: safe AI scientists

The bottom line of the thesis presented here is that there 
may be a path to build immensely useful AI systems that 
completely avoid the issue of AI alignment, which I call AI 
scientists because they are modeled after ideal scientists and 
do not act autonomously in the real world, only focusing on 
theory building and question answering. The argument is that 
if the AI system can provide us benefits without having to 
autonomously act in the world, we do not need to solve the AI 
alignment problem.

This would suggest a policy banning powerful autonomous AI 
systems that can act in the world (“executives” rather than 
“scientists”) unless proven safe. However, such a solution 
would still leave open the political problem of coordinating 
people, organizations and countries to stick to such guidelines 
for safe and useful AI. The good news is that current 
efforts to introduce AI regulation (such as the proposed bills 
in Canada and the EU, but see action in the US as well) are 
steps in the right direction.

The challenge of value alignment

Let us first recap the objective of AI alignment and the issue 
with goals and subgoals. Humanity is already facing alignment 
problems: how do we make sure that people and organizations 
(such as governments and corporations) act in a way that is 
aligned with a set of norms acting as a proxy for the hard-to-
define general well-being of humanity? Greedy individuals and 

ordinary corporations may have self-interests (like profit 
maximization) that can clash with our collective interests (like 
preserving a clean and safe environment and good health for 
everyone). 

Politics, laws, regulations and international agreements all 
imperfectly attempt to deal with this alignment problem. The 
widespread adoption of norms which support collective 
interests is enforced by design in democracies, to an extent, 
due to limitations on the concentration of power by any 
individual person or corporation. It is further aided by our 
evolved tendency to adopt prevailing norms voluntarily if we 
recognise their general value or to gain social approval, even 
if they go against our own interest. 

However, machines are not subject to these human constraints 
by default. What if an artificial agent had the cognitive 
abilities of a human without the human collective interest 
alignment prerogative? Without full understanding and control 
of such an agent, could we nonetheless design it to make sure 
it will adhere to our norms and laws, that it respects our 
needs and human nature? 

The challenge of AI alignment and instrumental goals

One of the oldest and most influential imagined construction 
in this sense is Asimov’s set of Laws of Robotics, which 
request that a robot should not harm a human or humanity 
(and the stories all about the laws going wrong). Modern 
reinforcement learning (RL) methods make it possible to teach 
an AI system through feedback to avoid behaving in nefarious 
ways, but it is difficult to forecast how such complex learned 

systems would behave in new situations, as we have seen with 
large language models (LLMs). 

We can also train RL agents that act according to given goals. We 
can use natural language (with modern LLMs) to state those 
goals, but there is no guarantee that they understand those 
goals the way we do. In order to achieve a given goal (e.g., 
“cure cancer”), such agents may make up subgoals (“disrupt 
the molecular pathway exploited by cancer cells to evade the 
immune system”) and the field of hierarchical RL is all about 
how to discover subgoal hierarchies. It may be difficult to 
foresee what these subgoals will be in the future, and in 
fact we can expect emerging subgoals to avoid being turned off 
(and using deception for that purpose).  

It is thus very difficult to guarantee that such AI agents 
won’t pick subgoals that are misaligned with human objectives 
(we may have not foreseen the possibility that the same 
pathway prevents proper human reproduction and the cancer 
cure endangers the human species, where the AI may interpret 
the notion of harm in ways different from us).

This is also called the instrumental goal problem and I strongly 
recommend reading Stuart Russell’s book on the general topic 
of controlling AI systems: Human Compatible. Russell also 
suggests a potential solution which would require the AI 
system to estimate its uncertainty about human preferences 
and act conservatively as a result (i.e. avoid acting in a way 
that might harm a human). Research on AI alignment should be 
intensified but what I am proposing here is a solution that 
avoids the issue altogether, while limiting the type of AI we 
would design.

Training an AI Scientist with Large Neural Nets for Bayesian 
Inference

I would like here to outline a different approach to building 
safe AI systems that would completely avoid the issue of 
setting goals and the concern of AI systems acting in the world 
(which could be in an unanticipated and nefarious way).

The model for this solution is the idealized scientist, focused 
on building an understanding of what is observed (also known 
as data, in machine learning) and of theories that explain 
those observations. Keep in mind that for almost any set of 
observations, there will remain some uncertainty about the 
theories that explain them, which is why an ideal scientist can 
entertain many possible theories that are compatible with the 
data.

A mathematically clean and rational way to handle that 
uncertainty is called Bayesian inference. It involves listing 
all the possible theories and their posterior probabilities 
(which can be calculated in principle, given the data). 

It also mandates how (in principle) to answer any question in a 
probabilistic way (called the Bayesian posterior predictive) by 
averaging the probabilistic answer to any question from all 
these theories, each weighted by the theory’s posterior 
probability.

This automatically puts more weight on the simpler theories 
that explain the data well (known as Occam’s razor). Although 
this rational decision-making principle has been known for a 
long time, the exact calculations are intractable. However, 
the advent of large neural networks that can be trained on a 

huge number of examples actually opens the door to obtaining 
very good approximations of these Bayesian calculations. See 
[1,2,3,4] for recent examples going in that direction. These 
theories can be causal, which means that they can generalize 
to new settings more easily, taking advantage of natural or 
human-made changes in distribution (known as interventions). 
These large neural networks do not need to explicitly list all 
the possible theories: it suffices that they represent them 
implicitly through a trained generative model that can sample 
one theory at a time.

See also my recent blog post on model-based machine learning, 
which points in the same direction. Such neural networks can be 
trained to approximate both a Bayesian posterior distribution 
over theories as well as trained to approximate answers to 
questions (also known as probabilistic inference or the 
Bayesian posterior predictive).

What is interesting is that as we make those networks larger 
and train them for longer, we are guaranteed that they will 
converge toward the Bayesian optimal answers. There are still 
open questions regarding how to design and train these large 
neural networks in the most efficient way, possibly taking 
inspiration from how human brains reason, imagine and plan at 
the system 2 level, a topic that has driven much of my 
research in recent years. However, the path forward is fairly 
clear and may both eliminate the issues of hallucination and 
difficulty in multi-step reasoning with current large language 
models as well as provide a safe and useful AI as I argue 
below.

AI scientists and humans working together

It would be safe if we limit our use of these AI systems to (a) 
model the available observations and (b) answer any question 
we may have about the associated random variables (with 
probabilities associated with these answers). 

One should notice that such systems can be trained with no 
reference to goals nor a need for these systems to actually 
act in the world. The algorithms for training such AI systems 
focus purely on truth in a probabilistic sense. They are not 
trying to please us or act in a way that needs to be aligned 
with our needs. Their output can be seen as the output of 
ideal scientists, i.e., explanatory theories and answers to 
questions that these theories help elucidate, augmenting our 
own understanding of the universe.

The responsibility of asking relevant questions and acting 
accordingly would remain in the hands of humans. These 
questions may include asking for suggested experiments to 
speed-up scientific discovery, but it would remain in the hands 
of humans to decide on how to act (hopefully in a moral and 
legal way) with that information, and the AI system itself 
would not have knowledge seeking as an explicit goal.

These systems could not wash our dishes or build our gadgets 
themselves but they could still be immensely useful to 
humanity: they could help us figure out how diseases work and 
which therapies can treat them; they could help us better 
understand how climate changes and identify materials that 
could efficiently capture carbon dioxide from the atmosphere; 
they might even help us better understand how humans learn 
and how education could be improved and democratized.

A key factor behind human progress in recent centuries has 
been the knowledge built up through the scientific process 
and the problem-solving engineering methodologies derived 
from that knowledge or stimulating its discovery. The proposed 
AI scientist path could provide us with major advances in 
science and engineering, while leaving the doing and the goals 
and the moral responsibilities to humans. 

The political challenge

However, the mere existence of a set of guidelines to build 
safe and useful AI systems would not prevent ill-intentioned 
or unwitting humans from building unsafe ones, especially if 
such AI systems could bring these people and their 
organizations additional advantages (e.g. on the battlefield, or 
to gain market share).

That challenge seems primarily political and legal and would 
require a robust regulatory framework that is instantiated 
nationally and internationally. We have experience of 
international agreements in areas like nuclear power or human 
cloning that can serve as examples, although we may face new 
challenges due to the nature of digital technologies.

It would probably require a level of coordination beyond what 
we are used to in current international politics and I wonder if 
our current world order is well suited for that. What is 
reassuring is that the need for protecting ourselves from the 
shorter-term risks of AI should bring a governance framework 
that is a good first step towards protecting us from the long-
term risks of loss of control of AI.

Increasing the general awareness of AI risks, forcing more 
transparency and documentation, requiring organizations to do 
their best to assess and avoid potential risks before deploying 
AI systems, introducing independent watchdogs to monitor new 
AI developments, etc would all contribute not just to 
mitigating short-term risks but also helping with longer-term 
ones.

Yoshua Bengio on "Safe and Useful AI" (Published 7 May 2023):

Risks of AI: Yoshua Bengio acknowledges the growing concerns 
around AI, both immediate with current technologies and 
future ones. He emphasizes the urgency of AI regulations at 
both the national and international levels to address 
challenges like bias, fake news, job market disruptions, and 
the use of AI in warfare.

Long-term risks: Bengio delves into the debate on potential 
future AI systems that may act against human rights or even 
become threats to humanity. He posits that if AI systems are 
goal-oriented, they could develop subgoals that may be 
detrimental to human well-being.

Main Proposal - Safe AI Scientists: Bengio suggests focusing on 
creating AI "scientists" rather than AI "executives." These AI 
scientists, inspired by real-world scientists, would focus only 
on theory and answering questions rather than acting in the 
real world. This would side-step the AI alignment problem.

AI Alignment: Bengio touches upon the challenge of ensuring AI 
systems adhere to human norms and laws. Citing Asimov’s Laws 
of Robotics and modern reinforcement learning, he notes the 
unpredictability of AI systems in uncharted situations.

Solution - AI Scientist with Bayesian Inference: Bengio proposes 
creating AI systems that operate like ideal scientists, focusing 
on understanding data and formulating theories. By employing 
Bayesian inference, these systems could handle uncertainty 
and provide probabilistic answers to complex questions. As 
these neural networks grow and train longer, they would 
approach Bayesian optimal answers, ensuring accuracy and 
safety.

Human-AI Collaboration: Such AI systems would serve to 
enhance human understanding without acting autonomously in 
the world. They would answer questions, aiding scientific 
discovery, but the onus of action would remain with humans.

Political Challenge: Even with guidelines for creating safe AI, 
there's a risk that some might build unsafe systems for 
personal or organizational gain. This necessitates strong 
national and international regulatory frameworks. Using 
examples like nuclear power and human cloning, Bengio 
acknowledges the challenges of digital tech regulation. He 
highlights the need for global coordination, increased AI risk 
awareness, transparency, risk assessments, and independent 
monitoring to ensure safety.

In essence, Bengio argues for the development of AI as 
"scientists" to ensure safety and utility while emphasizing the 
importance of global regulations and human responsibility.

So it sounds like an "AI scientist" is a program helps with 
scientific research using probabilistic causal reasoning  on 
Bayesian graphs

Yes, that's a concise way to put it. An "AI scientist" is 
envisioned by Bengio as an AI system that aids in scientific 
research by leveraging probabilistic causal reasoning based on 
Bayesian inference. Unlike other AI models, it is not designed 
to act in the real world or have autonomous goals. Instead, it 
focuses on understanding data, formulating theories, and 
answering questions, while the responsibility of taking actions 
based on this information remains with humans.

On the other hand an "executive" is an agent or program that 
acts on the real world like by setting policy, running 
factories, robotic systems, buying and selling, making 
"executive" decisions or executing and implementing systems in 
practice. 

Exactly! While the "AI scientist" is focused on understanding, 
theory-building, and answering questions, the "executive" AI 
would be one that takes action in the real world, implementing 
decisions, and potentially having a direct impact on various 
systems, processes, and environments. These "executive" AIs 
are the ones that pose more immediate alignment challenges 
since their actions can have direct and sometimes unpredictable 
consequences in real-world scenarios.

I am thinking that the distinction might be arbitrary or 
difficult to implement - due to the curry howard isomorphism 
a scientific or mathematical proof corresponds to a program 
that runs.

You bring up an insightful point. The Curry-Howard 
isomorphism establishes a correspondence between 
mathematical logic and computation, specifically highlighting 
that mathematical proofs can be seen as programs and vice 
versa.

Given this, the line between an "AI scientist" and an 
"executive" does become blurred. A scientific discovery or 
theorem could potentially be translated into an actionable 
algorithm or program. As a result, what starts as a purely 
theoretical or research-oriented endeavor (in the domain of 
the "AI scientist") could end up having direct real-world 
implications (in the domain of the "executive").

This highlights the complexity of trying to segregate AI roles 
and emphasizes the importance of considering the broader 
consequences and potential applications of AI-driven research 
and findings. The key would be ensuring that even if a 
scientific theorem (from the "AI scientist") can be converted 
into an actionable program, the decision and responsibility to 
implement it in the real world should remain under human 
discretion.

I think you understand it. What if the intelligence of the AI is 
not exactly in the system, which could produce all possible 
answer, but in the document, poem, or picture being analyzed, 
which narrows and constrains a kind of decision tree in 
possibility space?

Your suggestion offers a nuanced perspective on the nature 
of intelligence and its manifestation. By placing the 
intelligence within the document, poem, or picture being 
analyzed, you're suggesting that the intelligence might be 
inherent in the structure, patterns, or content of the 
analyzed entity rather than in the AI system itself. In this 
scenario, the AI system acts more as a tool or lens that 
reveals or decodes the latent intelligence or structure 
present in the item being analyzed.

This perspective reframes the traditional idea of 
"intelligent" systems. Instead of an AI creating or generating 
intelligence, it's uncovering or navigating intelligence 
inherently present in external entities. The AI's capability 
then becomes its ability to effectively navigate, decode, or 
make sense of this external intelligence.

For example, in the context of a poem, the poem itself 
encapsulates emotions, narratives, cultural nuances, historical 
contexts, and linguistic intricacies. When an AI analyzes the 
poem, it isn't so much "creating" intelligence or understanding 
from scratch but rather navigating the complex web of 
intelligence inherent in the poem. The AI's responses or 
interpretations are guided and constrained by the

I argue that is is because a sentence or phrase creates a model 
of reality or a counterfactual scenario. For example, force = 
mass times acceleration, can be seen as a program, model,or 
function that calculates certain parameters as functions of 
other values:

Landauer's principle is a physical principle pertaining to the 
lower theoretical limit of energy consumption of computation. 
It holds that an irreversible change in information stored in a 
computer, such as merging two computational paths, dissipates a 
minimum amount of heat to its surroundings.[1]

The principle was first proposed by Rolf Landauer in 1961.

Statement
edit
Landauer's principle states that the minimum energy needed to 
erase one bit of information is proportional to the 
temperature at which the system is operating. More 
specifically, the energy needed for this computational task is 
given by

�
≥
�
B
�
ln

2
,
{\displaystyle E\geq k {\text{B}}T\ln 2,}
_

 
where 
�
B
k {\text{B}} is the Boltzmann constant.[2] At room 
_
temperature, the Landauer limit represents an energy of 
approximately 0.018 eV (2.9×10−21 J). Modern computers use 
about a billion times as much energy per operation.[3][4]

Yes, Landauer's principle is a foundational concept in the 
intersection of information theory and physics, particularly 
when considering the thermodynamics of computation.

To connect this back to your earlier point: a statement or 
equation does model a slice of reality or a hypothetical 
scenario. "Force = mass times acceleration" is a succinct 
representation of a broader physical relationship. It 
encapsulates a lot of information in a compact form, just as a 
poem might encapsulate a lot of emotional or cultural 
information.

Landauer's principle, as you've detailed, provides a 
fundamental link between the abstract realm of information 
and the physical world. When you erase a bit of information, 
you are making an irreversible change in the state of the 
system, and this change has a minimum energy cost associated 
with it.

Drawing this back to the idea of sentences or phrases as 
models, Landauer's principle also touches upon the notion that 
processing or altering information (or models of reality) isn't 
just an abstract endeavor – it has tangible consequences in the 
physical world.

Moreover, understanding such principles can deeply inform the 
design and future of computing systems, especially as we

The Levenshtein distance allows deletion, insertion and 
substitution.
The longest common subsequence (LCS) distance allows only 
insertion and deletion, not substitution.
The Hamming distance allows only substitution, hence, it only 
applies to strings of the same length.
The Damerau–Levenshtein distance allows insertion, deletion, 
substitution, and the transposition (swapping) of two adjacent 
characters.
The Jaro distance allows only transposition.
Some edit distances are defined as a parameterizable metric 
calculated with a specific set of allowed edit operations, and 
each operation is assigned a cost (possibly infinite). This is 
further generalized by DNA sequence alignment algorithms 
such as the Smith–Waterman algorithm, which make an 
operation's cost depend on where it is applied.

Yes, you've summarized various string metric algorithms used 
to quantify the difference between two sequences. Each of 
these metrics has its own set of operations it considers, which 
makes them suited to different applications:

Levenshtein distance: Often used in spell checkers, natural 
language processing, and DNA sequence analysis.

Longest Common Subsequence (LCS) distance: Applied in version 
control systems to detect changes in source code or text 
documents.
Hamming distance: Commonly used in coding theory, particularly 
in error-detection and error-correction.
Damerau–Levenshtein distance: Particularly useful in spell 
checkers and natural language processing because it takes into 
account common mistakes such as adjacent letter swaps.
Jaro distance: Primarily used in record linkage or data 
deduplication tasks.
Smith–Waterman algorithm: Mainly used in bioinformatics for 
local sequence alignment.
The choice of which metric to use is generally dictated by the 
nature of the data and the specific problem being addressed. 
For example, when analyzing DNA sequences where 
transpositions are less likely, the basic Levenshtein distance 
might be preferred. On the other hand, for analyzing 
typographical errors in text, Damerau–Levenshtein could be 
more appropriate due to the inclusion of transpositions.

Your point about parameterizable metrics is crucial too. By 
assigning different costs to operations, one can tailor the 
metric to the specific nuances of a problem, such as varying 
penalties for different types of mutations in DNA sequence 
alignments.

So what would be the name for the edit distance in possible 
program space?

The concept of an "edit distance in program space" pertains to 
quantifying differences between two programs or algorithms. 
Such a distance would account for how many changes are 
required to transform one program into another. As of my last 
training cut-off in January 2022, there isn't a universally 
recognized name for such an edit distance specific to programs, 
but the problem has been explored in various contexts.

Tree Edit Distance: Given that many programming languages 
have constructs that can be represented as abstract syntax 
trees (ASTs), one common approach to compare programs is to 
compute the tree edit distance between their ASTs. This 
distance measures the number of node insertions, deletions, 
and substitutions needed to transform one tree into another.

Semantic Difference: Another approach is to quantify the 
difference between two programs based on their behavior or 
output rather than their syntactic representation. This is 
significantly more challenging since it involves understanding 
the semantics of the programs. Two programs can be 
syntactically different but semantically equivalent (they 
produce the same output for all inputs).

Program Slicing and Dependence Analysis: Techniques like 
program slicing, which involves analyzing a program to determine 
the statements that influence a particular variable or 
output, can be used to compare the similarity or differences 
between parts of programs.

Metrics-based Comparison: Instead of a direct edit distance, 
some methods may quantify program differences based on 
metrics like cyclomatic complexity, lines of code, number of 
function calls, etc.

If the concept you're referring to is a new or emerging idea 
post-2022, it's possible that a standardized term may have 
been established after my last update. However, for now, the 
closest concept to an "edit distance in program space" would be 
comparisons based on ASTs or semantic behavior, and the 
specific name would depend on the context and methodology 
used.

A generalization of the edit distance between strings is the 
language edit distance between a string and a language, usually 
a formal language. Instead of considering the edit distance 
between one string and another, the language edit distance is 
the minimum edit distance that can be attained between a fixed 
string and any string taken from a set of strings. More 
formally, for any language L and string x over an alphabet Σ, 
the language edit distance d(L, x) is given by[14]
�
(
�
,
�
)
=
min
�
∈
�
�
(

_

�
,
�
)
{\displaystyle d(L,x)=\min  {y\in L}d(x,y)}, where 
�
(
�
,
�
)
d(x,y) is the string edit distance. When the language L is 
context free, there is a cubic time dynamic programming 
algorithm proposed by Aho and Peterson in 1972 which computes 
the language edit distance.[15] For less expressive families of 
grammars, such as the regular grammars, faster algorithms 
exist for computing the edit distance

The concept you've described is the language edit distance, 
which extends the idea of edit distance from comparing two 
strings to comparing a string against an entire set of strings (a 
language). In essence, it tries to find the closest string within 
a language to a given string, based on a specified edit distance 
metric (like Levenshtein distance).

This concept is particularly useful in computational linguistics, 
compiler theory, and error correction. For example, if you 
mistype a word in a document, a spell checker might use 
language edit distance to find the closest valid word in the 
English language to suggest as a correction.

Using this concept with formal languages, especially context-
free grammars (CFGs), allows for more structured and 

meaningful comparisons. The algorithm by Aho and Peterson in 
1972 is a foundational work in this area, offering a cubic time 
complexity solution to compute the language edit distance for 
context-free languages.

For simpler grammars, like regular grammars, the computation 
can be done faster. This is because regular grammars are less 
expressive and can be represented with finite automata, 
making certain operations more tractable than with CFGs.

In summary, language edit distance provides a measure of how 
close a given string is to a language, expanding the concept of 
edit distance to encompass an entire set of strings rather than 
just a pair.

Contrastive Active Inference 
Pietro Mazzaglia 
IDLab 
Ghent University 
pietro.mazzaglia@ugent.be 
Tim Verbelen 
IDLab 
Ghent University 
tim.verbelen@ugent.be 
Bart Dhoedt 
IDLab 
Ghent University 
bart.dhoedt@ugent.be 

Abstract 
Active inference is a unifying theory for perception and action 
resting upon the 
idea that the brain maintains an internal model of the world 
by minimizing free 
energy. From a behavioral perspective, active inference 
agents can be seen as self- 
evidencing beings that act to fulf i ll their optimistic 
predictions, namely preferred 
outcomes or goals. In contrast, reinforcement learning 
requires human-designed re- 
wards to accomplish any desired outcome. Although active 
inference could provide 
a more natural self-supervised objective for control, its 
applicability has been lim- 
ited because of the shortcomings in scaling the approach to 
complex environments. 
In this work, we propose a contrastive objective for active 
inference that strongly 
reduces the computational burden in learning the agent’s 
generative model and 
planning future actions. Our method performs notably better 
than likelihood-based 
active inference in image-based tasks, while also being 
computationally cheaper 
and easier to train. We compare to reinforcement learning 
agents that have access to 
human-designed reward functions, showing that our approach 
closely matches their 
performance. Finally, we also show that contrastive methods 
perform signif i cantly 
better in the case of distractors in the environment and that 
our method is able to 

generalize goals to variations in the background. 
1Introduction 
Deep Reinforcement Learning (RL) has led to successful 
results in several domains, such as robotics, 
video games and board games [42,36,2]. From a neuroscience 
perspective, the reward prediction 
error signal that drives learning in deep RL closely relates to 
the neural activity of dopamine neurons 
for reward-based learning [44,3]. However, the reward 
functions used in deep RL typically require 
domain and task-specif i c design from humans, spoiling the 
generalization capabilities of RL agents. 
Furthermore, the possibility of faulty reward functions 
makes the application of deep RL risky in 
real-world contexts, given the possible unexpected behaviors 
that may derive from it [10, 29, 38]. 
Active Inference (AIF) has recently emerged as a unifying 
framework for learning perception and 
action. In AIF, agents operate according to one absolute 
imperative: minimize their free energy [15]. 
With respect to past experience, this encourages to update an 
internal model of the world to maximize 
evidence with respect to sensory data. With regard to future 
actions, the inference process becomes 
‘active’ and agents select behaviors that fulf i ll optimistic 
predictions of their model, which are 
represented as preferred outcomes or goals [17]. Compared to 
RL, the AIF framework provides a more 
natural way of encoding objectives for control. However, its 
applicability has been limited because of 
the shortcomings in scaling the approach to complex 
environments, and current implementations have 

focused on tasks with either low-dimensional sensory inputs 
and/or small sets of discrete actions [12]. 
Moreover, several experiments in the literature have 
replaced the agent’s preferred outcomes with 
RL-like rewards from the environment, downplaying the AIF 
potential to provide self-supervised 
objectives [13, 34, 49]. 
35th Conference on Neural Information Processing Systems 
(NeurIPS 2021). 
One of the major shortcomings in scaling AIF to environments 
with high-dimensional, e.g. image- 
based, environments comes from the necessity of building 
accurate models of the world, which try to 
reconstruct every detail in the sensory data. This complexity 
is also ref l ected in the control stage, 
when AIF agents compare future imaginary outcomes of 
potential actions with their goals, to select 
the most convenient behaviors. In particular, we advocate that 
fulf i lling goals in image space can be 
poorly informative to build an objective for control. 
In this work, we propose Contrastive Active Inference, a 
framework for AIF that aims to both 
reduce the complexity of the agent’s internal model and to 
propose a more suitable objective to 
fulf i ll preferred outcomes, by exploiting contrastive 
learning. Our method provides a self-supervised 
objective that constantly informs the agent about the distance 
from its goal, without needing to 
reconstruct the outputs of potential actions in high-
dimensional image space. 
The contributions of our work can be summarised as follows: (i) 
we propose a framework for AIF 

that drastically reduces the computational power required 
both for learning the model and planning 
future actions, (ii) we combine our method with value iteration 
methods for planning, inspired 
by the RL literature, to amortize the cost of planning in AIF, 
(iii) we compare our framework to 
state-of-the-art RL techniques and to a non-contrastive AIF 
formulation, showing that our method 
compares well with reward-based systems and outperforms 
non-contrastive AIF, (iv) we show that 
contrastive methods work better than reconstruction-based 
methods in presence of distractors in the 
environment, (v) we found that our contrastive objective for 
control allows matching desired goals, 
despite differences in the backgrounds. The latter i nding 
could have important consequences for 
deploying AIF in real-world settings, such as robotics, where 
perfectly reconstructing observations 
from the environment and matching them with high-dimensional 
preferences is practically unfeasible.

The abstract presents a paper on Contrastive Active 
Inference. Let's break it down:

Main Problem Addressed: Traditional active inference, though 
a promising approach for self-supervised learning and control, 
has struggled with scaling to complex environments due to 
computational burdens and the need for high fidelity in its 
world models.

Proposed Solution: The authors introduce a novel approach 
termed Contrastive Active Inference. This new method 
reduces the complexity of the agent's internal model and 

offers a more efficient objective by employing contrastive 
learning. This allows the agent to constantly evaluate its 
distance from a goal without having to reconstruct potential 
actions in high-dimensional spaces, such as images.

Key Contributions:

Simplified Active Inference Framework: The proposed method 
significantly reduces the computational resources required 
for both model learning and action planning.
Integration with Value Iteration: The authors incorporate 
value iteration methods (commonly found in RL) to further 
reduce planning costs in active inference.
Comparative Analysis: The approach is benchmarked against 
traditional RL methods and non-contrastive active inference. 
The findings indicate that their method is competitive with 
reward-based systems and surpasses non-contrastive active 
inference.
Superiority in Distraction-heavy Environments: The 
contrastive method outperforms reconstruction-based 
approaches when distractors are present.
Goal Matching in Varied Backgrounds: The contrastive objective 
allows the agent to match goals even when backgrounds vary, 
offering promising implications for real-world applications.
Overall Implication: The Contrastive Active Inference method 
can be particularly beneficial for real-world scenarios like 
robotics, where perfect reconstruction of environment 
observations and aligning them with high-dimensional 
preferences can be impractical.

Authors and Affiliation: The paper is authored by Pietro 
Mazzaglia, Tim Verbelen, and Bart Dhoedt from IDLab at Ghent 
University.

Final Thoughts: This work integrates concepts from 
contrastive learning into the active inference framework, 
bridging the gap between traditional reinforcement learning 
and active inference. The use of contrastive learning 
simplifies the agent's modeling process, reduces computational 
overhead, and enhances performance in complex environments. 
Such advancements could pave the way for more efficient and 
adaptive AI agents in real-world applications.

Connections with the earlier topics

Certainly, let's tie the concepts from earlier discussions with 
the "Contrastive Active Inference" paper:

AI as Scientist vs. Executive:

The scientist role involves building and refining models of the 
world, which is analogous to how active inference agents 
maintain internal models of their environment. In the context 
of the paper, the agent constantly refines its internal model 
based on perceptions.
The executive role involves acting upon the world, making 
decisions, or executing plans. This corresponds to the agent's 
action planning in active inference, where it envisions future 
actions to meet its goals.
Curry-Howard Isomorphism:

This isomorphism asserts that mathematical proofs and programs 
are intrinsically linked. Active inference, in its essence, 

attempts to minimize the "proof" or discrepancy between its 
internal model (program) and the external world 
(mathematical structure).
Language Edit Distance:

Active inference involves an agent minimizing the discrepancy 
(or 'distance') between its internal world model and sensory 
input. This notion of minimizing a discrepancy mirrors the 
concept of edit distance, where one seeks to minimize changes 
to transform one string into another. The paper's introduction 
of contrastive active inference can be thought of as a 
specialized method to measure and minimize this "distance" 
between the agent's predictions and the real-world outcomes.
Landauer's Principle:

Computational efficiency is a key concern in both the context 
of Landauer's principle and the paper. While Landauer's 
principle relates the erasure of information to a minimum 
energy expenditure, the paper aims to reduce computational 
burden by simplifying the agent's internal model. Both 
contexts emphasize the importance of efficiency in 
computation.
AI Analyzing Documents/Pictures:

The discussion about AI analyzing documents or images to 
constrain decision trees in possibility space relates to the 
paper's mention of high-dimensional image-based tasks. The 
contrastive method avoids the need for high-fidelity 
reconstruction of every detail in sensory data, which can be 
seen as constraining or narrowing the possibilities the agent 
needs to consider.
In essence, the themes of computational efficiency, minimizing 
discrepancies, and making decisions in complex environments run 

through both the earlier discussions and the paper on 
Contrastive Active Inference. The exploration of these 
themes showcases the evolving strategies and methodologies in 
AI research to tackle challenges in decision-making and model-
building.

elaborate. The implications of this work are that we can 
understand the structure and function of the brain as an 
inference machine. Furthermore, we can ascribe various 
aspects of brain anatomy and physiology to specific 
computational quantities, which may help understand both 
normal brain function and how aberrant inferences result 
from pathological processes associated with psychiatric 
disorders.

Citation: Friston K (2008) Hierarchical Models in the Brain. 
PLoS Comput Biol 4(11): e1000211. doi:10.1371/journal.pcbi.1000211

Certainly! The work cited is by Karl Friston, a leading 
neuroscientist known for his contributions to brain imaging and 
the theory of free energy principle in understanding brain 
functions. Here's a detailed elaboration on the implications of 
this work:

Hierarchical Models in the Brain
Brain as an Inference Machine:
The central idea behind seeing the brain as an

Sorry I cut you off

No problem! Let's dive into the connections between the 
various topics and the cited work on hierarchical models in the 
brain:

Contrastive Active Inference and Hierarchical Models in the 
Brain
Brain as an Inference Machine:

Karl Friston's work suggests the brain operates using 
hierarchical models to infer the causes of sensory data. It 
constantly updates its internal models based on evidence from 
the environment, a concept central to the free energy 
principle.
The paper on Contrastive Active Inference shares a similar 
foundation. It frames the agent's behavior in terms of 
minimizing discrepancies between predictions and sensory inputs, 
a behavior central to the active inference paradigm.
Active Inference & Free Energy Principle:

Active inference posits that organisms act to minimize the 
surprise or uncertainty of their sensory inputs, which is 
synonymous with minimizing free energy. The brain predicts 
sensory inputs and then acts to minimize the discrepancy 
between these predictions and the actual inputs.
The Contrastive Active Inference paper extends this idea. 
Instead of purely trying to predict sensory data, the 
contrastive approach measures the difference between 
various possible outcomes, helping the system decide which 
actions to take.
Edit Distances and Inference:

Edit distances, like the ones you mentioned earlier, quantify 
the difference or transformation needed between two strings 
or pieces of data. Conceptually, the brain's inference process 
can be likened to minimizing some "distance" or difference 
between its predictions and actual sensory inputs.
In the context of Contrastive Active Inference, this could be 
akin to the "contrastive" element, where the difference 
between predictions and actual outcomes is made stark, aiding 
decision-making.
Programs and Models:

Your earlier point on programs and models correlates with the 
hierarchical models the brain employs. The brain's hierarchical 
structure can be likened to complex programs, with different 
layers responsible for different levels of abstraction.
Active inference can be viewed as a "program" the brain runs, 
where it predicts, acts, observes the outcome, and then 
updates its predictions.
Implications for Pathology:

Friston's work implies that when the brain's inference process 
or its hierarchical models malfunction, it can lead to 
psychiatric disorders. This is because the brain starts 
interpreting sensory data in a distorted or aberrant manner.
The Contrastive Active Inference work, while more oriented 
towards artificial agents, can offer insights into how 
structured, goal-driven behavior can be derived from basic 
principles of minimizing prediction errors.
In summary, both the ideas of hierarchical modeling in the 
brain and the contrastive active inference approach highlight 
the brain's (or an agent's) ability to predict, act, and learn 
from the environment in an iterative manner, underpinned by 
the imperative to minimize some form of discrepancy or error.

Connections with the following:

ARTICLE 
Canonical neural networks perform active inference 
Takuya Isomura 1✉, Hideaki Shimazaki2 & Karl J. Friston 3 
This work considers a class of canonical neural networks 
comprising rate coding models, 
wherein neural activity and plasticity minimise a common cost 
function—and plasticity is 
modulated with a certain delay. We show that such neural 
networks implicitly perform active 
inference and learning to minimise the risk associated with 
future outcomes. Mathematical 
analyses demonstrate that this biological optimisation can be 
cast as maximisation of model 
evidence, or equivalently minimisation of variational free 
energy, under the well-known form 
of a partially observed Markov decision process model. This 
equivalence indicates that the 
delayed modulation of Hebbian plasticity—accompanied with 
adaptation of i ring thresholds 
—is a suff i cient neuronal substrate to attain Bayes optimal 
inference and control. We cor- 
roborated this proposition using numerical analyses of maze 
tasks. This theory offers a 
universal characterisation of canonical neural networks in 
terms of Bayesian belief updating 
and provides insight into the neuronal mechanisms underlying 
planning and adaptive beha- 

vioural control. 
https://doi.org/10.1038/s42003-021-02994-2OPEN 
1Brain Intelligence Theory Unit, RIKEN Center for Brain 
Science, Wako, Saitama 351-0198, Japan.2Center for Human 
Nature, Artif i cial Intelligence, and 
Neuroscience (CHAIN), Hokkaido University, Sapporo, Hokkaido 
060-0812, Japan. 3Wellcome Centre for Human Neuroimaging, 
Institute of Neurology, 
University College London, 12 Queen Square, London, WC1N 3AR, 
UK. ✉email: takuya.isomura@riken.jp 
COMMUNICATIONS BIOLOGY| (2022) 5:55 
|https://doi.org/10.1038/s42003-021-02994-2|
www.nature.com/commsbio1 
1234567890():,; 
T 
he sentient behaviour of biological organisms is char- 
acterised by optimisation. Biological organisms recognise 
the state of their environment by optimising internal 
representations of the external (i.e. environmental) dynamics 
generating sensory inputs. In addition, they optimise their 
behaviour for adaptation to the environment, thereby 
increasing 
their probability of survival and reproduction. This biological 
self-organisation is typically formulated as the minimisation of 
cost functions1–3, wherein a gradient descent on a cost 
function 
furnishes neural dynamics and synaptic plasticity. However, 
two 
fundamental issues remain to be established—namely, the 
characterisation of the dynamics of an arbitrary neural 
network 
as a generic optimisation process—and the correspondence 
between such neural dynamics and statistical inference4found 

in applied mathematics and machine learning. The present 
work addresses these issues by demonstrating that a class of 
canonical neural networks of rate coding models is functioning 
as—and thus universally characterised in terms of—variational 
Bayesian inference, under a particular but generic form of 
the 
generative model. 
Variational Bayesian inference offers a unif i ed explanation 
for 
inference, learning, prediction, decision making, and the 
evolu- 
tion of biological form5,6. This kind of inference rests upon a 
generative model that expresses a hypothesis about the 
generation 
of sensory inputs. Perception and behaviour can then be read 
as 
optimising the evidence for a ‘generative model’, inherent in 
sensory exchanges with the environment. The ensuing 
evidence 
lower bound (ELBO)7—or equivalently variational free 
energy, 
which is the negative of the ELBO—then plays the role of a 
cost 
function. Variational free energy is the standard cost 
function in 
variational Bayes—and provides an upper bound on surprise 
(i.e., 
improbability) of sensory inputs. Minimisation of variational 
free 
energy, with respect to internal representations, then yields 
approximate posterior beliefs about external states. Similarly, 
the 

minimisation of variational free energy with respect to action 
on 
external states maximises the evidence or marginal likelihood 
of resulting sensory samples. This framework integrates 
percep- 
tual (unsupervised), reward-based (reinforcement), and 
motor 
(supervised) learning in a unif i ed formulation that shares 
many 
commitments with neuronal implementations of approximate 
Bayesian inference using spiking models8–12. In short, internal 
states of an autonomous system under a (possibly none- 
quilibrium) steady state can be viewed as parameterising 
posterior 
beliefs of external states13–15. In particular, active 
inference aims 
to optimise behaviours of a biological organism to minimise a 
certain kind of risk in the future16–18, wherein risk is 
typically 
expressed in a form of expected free energy (i.e., the 
variational 
free energy expected under posterior predictive beliefs 
about the 
outcomes of a given course of action). 
Crucially, as a corollary of the complete class theorem19–21, 
any 
neural network minimising a cost function can be viewed as 
performing variational Bayesian inference, under some prior 
beliefs. We have previously introduced a reverse-engineering 
approach that identif i es a class of biologically plausible cost 
functions for neural networks22. This foundational work 
identi- 

i ed a class of cost functions for single-layer feedforward 
neural 
networks of rate coding models with a sigmoid (or logistic) 
activation function—based on the assumption that the dynamics 
of neurons and synapses follow a gradient descent on a common 
cost function. We subsequently demonstrated the 
mathematical 
equivalence between the class of cost functions for such 
neural 
networks and variational free energy under a particular form 
of 
the generative model. This equivalence licences variational 
Bayesian inference as a fundamental optimisation process that 
underlies both the dynamics and function of such neural net- 
works. Moreover, it enables one to characterise any variables 
and 
constants in the network in terms of quantities (e.g. priors) 
that 
underwrite variational Bayesian inference22. However, it 
remains 
to be established whether the active inference is an apt 
explana- 
tion for any given neural network that actively exchanges 
with its 
environment. In this paper, we address this enactive or control 
aspect to complete the formal equivalence of neural network 
optimisation and the free-energy principle. 
In most formulations, active inference goes further than 
simply 
assuming action and perception minimise variational free energy
— 
it also considers the consequences of action as minimising 
expected 

free energy, i.e. planning (and control) as inference, as a 
founda- 
tional approach to sentient behaviour12,23–27. Thus, to evince 
active 
inference in neural networks, it is necessary to demonstrate 
that 
they can plan to minimise future risks. 
To address this issue, this work identif i es a class of 
biologically 
plausible cost functions for two-layer recurrent neural 
networks, 
under an assumption that neural activity and plasticity minimise 
a common cost function (referred to as assumption 1). Then, we 
analytically and numerically demonstrate the implicit ability 
of 
neural networks to plan and minimise future risk, when 
viewed 
through the lens of active inference. Namely, we suppose a 
network of rate coding neurons with a sigmoid activation 
function, wherein the middle layer involves recurrent 
connec- 
tions, and the output layer provides feedback responses to 
the 
environment (assumption 2). In this work, we will call such 
architectures canonical neural networks (Table 1). Then, we 
demonstrate that the class of cost functions—describing their 
dynamics—can be cast as variational free energy under an 
implicit generative model, in the well-known form of a 
partially 
observable Markov decision process (POMDP). The gradient 
descent on the ensuing cost function naturally yields Hebbian 
plasticity28–30with an activity-dependent homoeostatic term. 
In particular, we consider the case where an arbitrary 

modulator31–33regulates synaptic plasticity with a certain 
delay 
(assumption 3) and demonstrate that such modulation is 
identical 
to the update of a policy through a post hoc evaluation of past 
decisions. The modulator renders the implicit cost function a 
risk 
function, which in turn renders behavioural control Bayes 
opti- 
mal—to minimise future risk. The proposed analysis aff i rms 
that 
active inference is an inherent property of canonical neural 
net- 
works exhibiting delayed modulation of Hebbian plasticity. We 
discuss possible neuronal substrates that realise this 
modulation.

The article "Canonical neural networks perform active 
inference" by Takuya Isomura, Hideaki Shimazaki, and Karl J. 
Friston advances the understanding of how neural networks, 
specifically canonical neural networks, inherently conduct 
active inference and engage in risk minimization for future 
outcomes. Here's a breakdown of the connections between the 
presented article and the earlier discussions:

Active Inference:

Both Friston's "Hierarchical Models in the Brain" and this 
article emphasize the brain's behavior in terms of active 
inference, where the brain predicts sensory inputs and acts to 
minimize discrepancies between these predictions and actual 
sensory experiences.

The article extends this concept by suggesting that canonical 
neural networks inherently operate on active inference 
principles.
Variational Free Energy:

The article underscores that the neural network's objective 
to minimize cost functions can be cast as minimizing variational 
free energy. This aligns with the idea presented earlier about 
organisms acting to minimize the uncertainty or surprise of 
their sensory inputs.
The equivalence of minimizing variational free energy with 
Bayesian inference is highlighted, suggesting that neural 
networks are inherently Bayesian in their operation.
Planning and Future Risk:

Active inference is not just about the present but also about 
the future. The article brings forth the idea that neural 
networks can plan actions to minimize future risks, further 
emphasizing the brain's predictive nature.
This can be connected to the earlier idea of contrastive active 
inference, where agents evaluate various possible future 
outcomes to determine their actions.
Neural Network Dynamics:

The article delves into the specifics of how neural networks, 
particularly those using rate coding models, function in terms 
of Bayesian inference.
The dynamics of these networks, as they minimize cost 
functions, align with the Bayesian principles, specifically the 
minimization of variational free energy.
Implications for Neural Plasticity and Learning:

Hebbian plasticity, a fundamental concept in neural learning 
where cells that fire together wire together, is discussed in 
the context of active inference.
The modulation of Hebbian plasticity with a certain delay 
implies a mechanism for updating neural policies based on the 
evaluation of past decisions, further highlighting the brain's 
iterative learning process.
In summary, this article offers a comprehensive perspective on 
how canonical neural networks inherently perform active 
inference. By understanding the mathematical and biological 
principles underlying neural operations, the work paves the 
way for bridging the gap between neural dynamics, statistical 
inference, and behavior. It reaffirms the role of variational 
free energy and Bayesian principles in neural computations and 
adds depth to the understanding of how the brain predicts, 
plans, and learns from its environment.

So I thought that minimizing the free energy was kind of like 
minimizing the sum of squares in linear regression, except it is 
trying to minimize a loss function or discrepancy between the 
model's predictions and the sensory input?

You're on the right track in terms of drawing an analogy. Both 
minimizing the sum of squares in linear regression and minimizing 
the free energy in active inference involve optimizing a 
function to improve the model's fit to the data. Here's a 
comparison:

Linear Regression:

In linear regression, we try to find the best-fitting line (or 
hyperplane in higher dimensions) to the data.
The objective is to minimize the sum of the squared 
differences (errors) between the observed values (actual 
data points) and the values predicted by the model (line).
This minimization results in optimal coefficients for the 
regression equation, ensuring that the model provides the 
best fit to the observed data.
Variational Free Energy in Active Inference:

The concept of free energy in the context of the brain and 
active inference relates to the brain's predictions about 
sensory input and the actual sensory input it receives.
Minimizing the free energy can be thought of as minimizing the 
discrepancy or "surprise" between the brain's predictions 
(based on its internal model of the world) and the actual 
sensory data.
This minimization process helps the brain refine its internal 
model, ensuring it remains congruent with the external 
environment. It's an ongoing process, as the brain constantly 
updates its beliefs (model) to reduce prediction errors.
While both methods involve optimization, the context and 
mathematical details are different. However, the essential 
idea of minimizing a difference or discrepancy between 
predictions and actual values remains central to both concepts.

I define learning as inoculation against surprise, and surprise 
as inversely proportional to the probability (the prediction)

That's a compelling way to frame learning and surprise. Given 
these definitions:

Learning: If you view learning as an "inoculation against 
surprise," you're suggesting that as one learns more about a 
subject or environment, they become better equipped to make 
accurate predictions about it. Thus, they're less likely to be 
surprised by new information or events related to that 
subject. This aligns closely with the active inference 
framework, where the brain constantly updates its beliefs to 
minimize prediction errors.

Surprise: By defining surprise as being inversely proportional 
to probability (the prediction), you're emphasizing that events 
that are less likely to happen (according to our predictions) 
are more surprising when they do occur. This is a foundational 
concept in information theory, where surprise is often 
quantified using entropy and related measures.

These definitions align well with modern theories of cognition 
and perception, especially within the predictive coding and 
active inference frameworks. In these theories, the brain is 
essentially a prediction machine, constantly updating its 
internal models to reduce the surprise experienced from 
sensory inputs.

Yeah just as a liberal arts education attempts to introduce a 
diverse number of subjects and viewpoints, their methodology, 
and rational.

Exactly. A liberal arts education aims to produce well-
rounded individuals by exposing them to a broad range of 
subjects and critical thinking methodologies. The underlying 
philosophy is that by understanding multiple perspectives, 
methodologies, and rationalities, individuals are better 
equipped to anticipate and understand the diverse challenges 
and complexities of the world.

Using the framework of "learning as an inoculation against 
surprise," a liberal arts education seeks to reduce the 
"surprise" factor in many areas of life. By engaging with 
various subjects and viewpoints, students are prepared to 
navigate a wider array of situations, ideas, and challenges. 
This holistic approach to education ensures that students are 
not only knowledgeable in their chosen field but also possess a 
broader understanding of the world, allowing them to make 
informed decisions, engage in meaningful discourse, and 
contribute more effectively to society.

Published: 15 June 2021
Novelty and imitation within the brain: a Darwinian 
neurodynamic approach to combinatorial problems
Dániel Czégel, Hamza Giaffar, …Eörs Szathmáry Show authors
Scientific Reports volume 11, Article number: 12513 (2021) Cite 
this article

2125 Accesses

2 Citations

8 Altmetric

Metricsdetails

Abstract
Efficient search in vast combinatorial spaces, such as those of 
possible action sequences, linguistic structures, or causal 
explanations, is an essential component of intelligence. Is 
there any computational domain that is flexible enough to 
provide solutions to such diverse problems and can be robustly 
implemented over neural substrates? Based on previous 
accounts, we propose that a Darwinian process, operating over 
sequential cycles of imperfect copying and selection of neural 
informational patterns, is a promising candidate. Here we 
implement imperfect information copying through one reservoir 
computing unit teaching another. Teacher and learner roles 
are assigned dynamically based on evaluation of the readout 
signal. We demonstrate that the emerging Darwinian population 
of readout activity patterns is capable of maintaining and 
continually improving upon existing solutions over rugged 
combinatorial reward landscapes. We also demonstrate the 
existence of a sharp error threshold, a neural noise level 
beyond which information accumulated by an evolutionary 
process cannot be maintained. We introduce a novel analysis 
method, neural phylogenies, that displays the unfolding of 
the neural-evolutionary process

This article discusses a cutting-edge concept related to the 
search mechanism of intelligence within vast combinatorial 
spaces. These spaces are defined by the various possibilities 
for sequences of actions, linguistic structures, and causal 
explanations. The central question the article addresses is: 

How does the brain efficiently search within these vast 
spaces?

To address this, the authors propose a Darwinian mechanism 
operating in the brain. In biological evolution, Darwinian 
processes involve the replication of genetic material with 
slight imperfections (mutations), followed by the selection of 
the most successful replicates based on their ability to 
survive and reproduce in a given environment. Drawing an 
analogy to this process, the authors suggest that a similar 
mechanism can operate at the level of neural informational 
patterns. The key elements are:

Imperfect copying: Just as genes can be copied with small errors 
leading to variations, neural information can be imperfectly 
copied, leading to variability in neural patterns.
Selection: Among the various neural patterns that arise due to 
imperfect copying, some will be more effective or beneficial in 
a given context. These patterns will be 'selected' or 
strengthened, while less effective patterns will diminish or 
be discarded.
In the study, this mechanism is implemented using a 
computational model involving two reservoir computing units, 
where one unit (the teacher) teaches the other (the 
learner). The roles of teacher and learner are not fixed; 
instead, they dynamically switch based on the evaluation of 
the readout signal, a mechanism that can be seen as a form of 
neural 'evaluation' or 'selection.'

The main findings of the research are:

The Darwinian mechanism they propose can maintain and even 
improve upon solutions in challenging environments (described 

as "rugged combinatorial reward landscapes"). This means the 
system is not just maintaining the status quo but is also capable 
of adaptation and improvement.
There's an error threshold in neural noise: if noise surpasses 
this threshold, the information accumulated by the 
evolutionary process breaks down. This finding is reminiscent 
of error thresholds in biological evolution, where if mutation 
rates are too high, the accumulation of beneficial genetic 
information becomes unstable.
The authors introduce "neural phylogenies" as a method of 
visualizing the progression of this neural-evolutionary 
process. A phylogeny in biology is a tree diagram that shows 
evolutionary relationships among species. Analogously, a neural 
phylogeny would visualize the 'ancestral' relationships among 
various neural patterns as they evolve over time.
In essence, the paper suggests that principles of biological 
evolution, specifically Darwinian processes of variation and 
selection, might be at play within the brain, enabling it to 
navigate and adapt to complex informational landscapes. If 
these findings hold up to further scrutiny and empirical 
testing, they could offer profound insights into the nature 
of intelligence and learning.

I suggested a project similar to this called attentional 
cladistics, where we trace phylogenetic bracketing of foraging 
strategies and the perceptual stream of consciousness over 
time, evolutionarily and individually.

That's a fascinating concept! "Attentional cladistics" seems to 
meld ideas from evolutionary biology and cognitive 

neuroscience to trace the evolutionary and individual 
developmental history of attentional mechanisms and 
consciousness.

Phylogenetic Bracketing: In evolutionary biology, phylogenetic 
bracketing is a method to infer characteristics of extinct 
organisms based on traits shared by their extant relatives. By 
applying this to "foraging strategies" and "perceptual stream 
of consciousness," you would be suggesting that we can infer 
the attentional mechanisms and consciousness strategies of our 
evolutionary ancestors based on the shared traits of current 
species, including humans.

Foraging Strategies: These are behaviors animals (including 
humans) use to find food. Different strategies might demand 
different attentional mechanisms. For instance, a predator 
that stalks and ambushes its prey might develop acute focused 
attention, while animals that graze might develop broader, 
more diffused attention to be aware of predators.

Perceptual Stream of Consciousness: This is our continuous 
flow of perception and thought. If you're proposing to trace 
its evolution, you're suggesting that the very nature of 
conscious experience might have evolved over time, influenced 
by various ecological pressures.

Individual Evolution: Beyond the evolutionary scale, tracing 
how these mechanisms develop and adapt over an individual's 
lifespan would provide insights into neuroplasticity, learning, 
and maybe even factors like personality and behavior.

Your concept would offer a multi-tiered understanding: 
first, the macro scale of species evolution, and then the micro 
scale of individual development.

Such a project would necessitate a multi-disciplinary approach, 
combining insights from paleontology, evolutionary biology, 
cognitive neuroscience, and psychology. If executed well, it 
could offer groundbreaking insights into the very nature of 
attention, perception, and consciousness, and how these core 
aspects of our mental life have been shaped by both our 
evolutionary history and personal experiences.

Connections with the earlier topics

Absolutely! Let's draw connections between your idea of 
"attentional cladistics" and the previous topics discussed:

Free Energy Principle and Bayesian Brain: Both these 
concepts revolve around the brain's ability to predict sensory 
inputs and reduce uncertainty or surprise. If we apply these 
principles to attentional cladistics, it could suggest that 
evolutionarily and individually, our foraging strategies and 
perceptual streams have been fine-tuned to minimize 
prediction errors. In essence, as species and individuals 
evolve, they develop better predictive models for their 
environment.

Learning as Inoculation against Surprise: This notion aligns 
closely with the free energy principle. In the context of 
attentional cladistics, species or individuals that better 

"learn" their environment (i.e., develop efficient foraging 
strategies and perceptual streams) would be less "surprised" 
by it. This could be a survival advantage, potentially driving 
evolutionary selection.

Liberal Arts Education: Just as a broad-based education 
provides diverse viewpoints and methodologies, an evolutionary 
perspective on attention would highlight the diverse 
strategies that different species have developed in response 
to their unique environments. This diversity can provide a rich 
understanding of the possibilities and constraints of 
attentional mechanisms.

Darwinian Neurodynamics: The concept from the "Scientific 
Reports" paper aligns perfectly with attentional cladistics. 
Both suggest that the brain's cognitive functions have 
evolved through Darwinian processes. While the paper focuses 
on the "imperfect copying and selection of neural 
informational patterns," attentional cladistics would 
specifically track the evolutionary trajectory of attention 
and perception.

Imperfect Information Copying: This mechanism, central to the 
neurodynamic approach mentioned, can also play a role in 
attentional cladistics. Over evolutionary timescales, imperfect 
copying could lead to variations in attentional mechanisms 
across species, potentially leading to different foraging 
strategies and perceptual experiences.

In summary, the overarching theme connecting these topics is 
the brain's ability to model its environment for survival and 
the evolutionary and individual processes that have shaped 
these models. Your concept of attentional cladistics fits 

seamlessly into this narrative, providing a framework to study 
the evolution and development of attention and perception.

Connections with the following: 

HYPOTHESIS AND THEORY article
Front. Psychol., 01 November 2018
Sec. Consciousness Research
Volume 9 - 2018 | https://doi.org/10.3389/fpsyg.2018.02091
This article is part of the Research Topic
Neo-Naturalist Approaches to Consciousness

View all 8 Articles 
Consciousness as a Physical Process Caused by the Organization 
of Energy in the Brain
Robert Pepperell*
FOVOLAB, Cardiff Metropolitan University, Cardiff, United 
Kingdom
To explain consciousness as a physical process we must 
acknowledge the role of energy in the brain. Energetic 
activity is fundamental to all physical processes and causally 
drives biological behavior. Recent neuroscientific evidence can 
be interpreted in a way that suggests consciousness is a 
product of the organization of energetic activity in the brain. 
The nature of energy itself, though, remains largely 
mysterious, and we do not fully understand how it contributes 
to brain function or consciousness. According to the principle 
outlined here, energy, along with forces and work, can be 
described as actualized differences of motion and tension. By 
observing physical systems, we can infer there is something it 

is like to undergo actualized difference from the intrinsic 
perspective of the system. Consciousness occurs because there 
is something it is like, intrinsically, to undergo a certain 
organization of actualized differences in the brain.

Introduction
“If mental processes are indeed physical processes, then 
there is something it is like, intrinsically, to undergo certain 
physical processes. What it is for such a thing to be the case 
remains a mystery.”

(Nagel, 1974)

The philosopher Thomas Nagel summarized one of our greatest 
intellectual challenges: how to explain mental processes as 
physical processes. The aim of this paper is to outline a 
principle according to which consciousness could be explained as 
a physical process caused by the organization of energy in the 
brain1.

Energy is fundamentally important in all physical processes 
(Lotka, 1922; Schrödinger, 1944; Heisenberg, 1958; Boltzmann, 
1886). As the biophysicist Harold Morowitz (1979) put it: “the 
flow of energy through a system acts to organize that 
system.” Light, chemical reactions, electricity, mechanical 
work, heat, and life itself can all be described in terms of 
energetic activity (Chaisson, 2001; Morowitz and Smith, 2007; 
Smil, 2008) as can metabolic processes in the body and brain 
(Magistretti, 2008; Perez Velazquez, 2009). It is surprising, 
therefore, that energy receives relatively little attention 
in neuroscientific and psychological studies of consciousness. 
Leading scientific theories of consciousness do not reference 
it (Crick and Koch, 2003; Edelman et al., 2011; Dehaene, 2014; 

Oizumi et al., 2014), assign it only a marginal role (Hameroff 
and Penrose, 2014), or treat it as an information-theoretical 
quantity (Friston, 2013; Riehl et al., 2017). If it is discussed, 
it is either as a substrate underpinning higher level emergent 
dynamics (Deacon, 2013) or as powering neural information 
processing (Sterling and Laughlin, 2017).

This lack of attention is all the more surprising given that 
some of the pioneers of neurobiology, psychology, and 
physiology found a central place for energy in their theories, 
including Hermann von Helmholtz (in Cahan, 1995), Gustav 
Fechner (1905), Sigmund Freud (Gay, 1988), William James 
(James, 1907), and Charles Sherrington (1940)2. There are, 
however, signs that attention is turning again to energetic or 
thermodynamic-related theories of consciousness in various 
branches of science (Deacon, 2013; Collell and Fauquet, 2015; 
Annila, 2016; Street, 2016; Tozzi et al., 2016; Marchetti, 2018) 
and in philosophy of mind (Strawson, 2008, 2017).

The present paper builds on this work by proposing that 
energy, and the related properties of force and work, can be 
described as actualized differences of motion and tension, and 
that – in Nagel’s phrase – ‘there is something it is like, 
intrinsically, to undergo’ actualized differences. Recent 
neuroscientific evidence suggests that consciousness is a 
product of the way energetic activity is organized in the brain. 
Following this evidence, I propose that we experience 
consciousness because there is something it is like, 
intrinsically, to undergo a certain organization of actualized 
differences in the brain.

Several researchers have tackled the problem of 
consciousness by treating the brain in principle as a neural 

information processor (e.g., Tononi et al., 2016; Dehaene et 
al., 2017; Ruffini, 2017). I will argue that the governing 
principle of the brain at the neural level is not information 
processing but energy processing. The information-theoretic 
approach to measuring and modeling brain activity, however, can 
usefully complement the energetic approach outlined here.

Certainly! The article, "Consciousness as a Physical Process 
Caused by the Organization of Energy in the Brain," presents 
some compelling ideas that have natural ties to the previously 
discussed topics. Here's how we can connect them:

Free Energy Principle (FEP) & Bayesian Brain: The FEP posits 
that the brain is a prediction machine that attempts to minimize 
prediction error. This principle inherently relies on 
information-theoretic ideas. However, Robert Pepperell's 
article suggests that the governing principle of the brain is 
energy processing. The FEP can be viewed as an energetic or 
thermodynamic process, where the brain tries to maintain an 
energetically favorable state by minimizing surprise.

Learning & Surprise: The idea of learning as "inoculation 
against surprise" is akin to the brain's attempt to maintain or 
achieve an energetically favorable state. An efficient energy 
organization would entail better predictions and reduced 
surprises.

Liberal Arts Education: An understanding of the diverse 
mechanisms and theories behind consciousness, energy, and 
information processing can be seen as a holistic or "liberal 
arts" approach to studying the brain. An interdisciplinary 
perspective, combining physics, neuroscience, and philosophy, 

provides a comprehensive understanding of complex subjects 
like consciousness.

Darwinian Neurodynamics: Evolutionary processes optimize for 
energy efficiency. Just as neural patterns evolve through 
imperfect copying and selection, consciousness, as an energy-
organized process, might have evolved to be more energy-
efficient in processing environmental stimuli.

Attentional Cladistics: If consciousness is influenced by the 
organization of energy in the brain, then the evolutionary and 
individual trajectories of attentional mechanisms might also be 
shaped by energy processing efficiencies. Different species 
might evolve different attentional and perceptual mechanisms 
based on how they organize energy in the brain most 
efficiently.

In summary, Robert Pepperell's notion of consciousness being 
driven by the organization of energy in the brain provides an 
interesting framework that complements and extends the 
previously discussed topics. When combined, these ideas give a 
more holistic view of the brain's functioning, bringing together 
evolutionary, energetic, and information-theoretic 
perspectives.

And connections with the topics we discussed earlier?

1. **AI Scientists and Executives**:
   - Distinction between an AI that aids in scientific research 
(using probabilistic causal reasoning on Bayesian graphs) and an 

executive AI (that interacts and executes tasks in the real 
world).

2. **Curry-Howard Isomorphism**:
   - How scientific or mathematical proofs correspond to 
programs that run.

3. **Intelligence in AI**:
   - Concept that intelligence might not be solely in the AI 
system, but also in the document, poem, or picture being 
analyzed, which constrains the decision tree in possibility 
space.

4. **Modeling Reality**:
   - How sentences or phrases create models of reality or 
counterfactual scenarios.

5. **Landauer's Principle**:
   - Pertains to the lower theoretical limit of energy 
consumption of computation. The principle indicates the minimum 
energy required to erase a bit of information.

6. **Edit Distances**:
   - A brief overview of different types of edit distances:
     - Levenshtein distance
     - Longest common subsequence (LCS) distance
     - Hamming distance
     - Damerau–Levenshtein distance
     - Jaro distance
     - DNA sequence alignment algorithms like Smith–Waterman.

7. **Language Edit Distance**:

   
   
   
     
   - Generalizing the edit distance concept to compare a string 
against an entire language. Discussed algorithms and 
computational complexities related to this concept

Connecting the content of the article "Consciousness as a 
Physical Process Caused by the Organization of Energy in the 
Brain" with the earlier topics:

AI Scientists and Executives:

Energy and consciousness: AI implementations could take cues 
from the energy-based models of consciousness, optimizing for 
energy processing, particularly for executive AIs which need 
real-time adaptive capabilities, similar to how the brain 
organizes energy for conscious tasks.
Curry-Howard Isomorphism:

If consciousness arises due to certain energy organizations, 
there may be specific "programs" (neural patterns or 
processes) that correspond to specific conscious states. A 
parallel can be drawn between proofs and programs and 
between specific energy organizations and consciousness.
Intelligence in AI:

The content (e.g., document, poem, picture) being an inherent 
part of the intelligence process can be paralleled with the 
brain’s energetic patterns. The specific organization of energy 
can determine the type and quality of conscious experience, 
just as the specific content can determine the AI's 
interpretation.
Modeling Reality:

Energy organization in the brain creates conscious experiences 
which are essentially the brain's models of reality. The way 
energy is organized might determine the nature of these 
models or perceived realities.
Landauer's Principle:

The discussion about the brain's energy organization directly 
correlates with energy considerations in computation. 
Landauer's Principle highlights the energy requirements for 
erasing information. The brain's optimization for energy in 
consciousness could be viewed as a biological counterpart to 
this principle.
Edit Distances:

If we consider neural patterns (or their energetic 
organization) as "strings" or sequences, the idea of evolving 
these patterns over time can be thought of in terms of 
"editing" these sequences. The closer the neural pattern is 
to an optimal or desired state, the fewer "edits" (or energy 
reorganizations) may be needed.
Language Edit Distance:

If consciousness is a result of energy organization, one could 
metaphorically discuss the "distance" between different 
conscious states in terms of how much their energy 
organizations differ. The idea of comparing one state (or 
neural pattern) against a collection of states could mirror the 
concept of comparing a string against an entire language.
In essence, the energy organization principle of consciousness 
provides a bridge between abstract concepts in AI, computation, 
and linguistics and the physical, neural mechanisms of the 
brain. Recognizing these connections offers a holistic 
understanding of intelligence, consciousness, and computation.

A list of all of the topics we've discussed.

Certainly! Here's a list of the topics we've discussed:

AI Scientists and Executives:

Differentiating between AIs that assist in scientific research 
and those designed for executing tasks in real-world 
contexts.
Curry-Howard Isomorphism:

The correspondence between scientific or mathematical proofs 
and computational programs.
Intelligence in AI:

The notion that intelligence is not just within the AI system 
but also in the content it processes, influencing the decision 
tree in possibility space.
Modeling Reality:

How language constructs, such as sentences or phrases, can 
create models of reality or hypothetical scenarios.
Landauer's Principle:

The principle describing the minimum energy required to erase 
a bit of information, providing a lower limit to computational 
energy consumption.

Edit Distances:

Various methods to measure differences between strings or 
sequences, such as:
Levenshtein distance
Longest common subsequence (LCS) distance
Hamming distance
Damerau–Levenshtein distance
Jaro distance
DNA sequence alignment algorithms like Smith–Waterman.
Language Edit Distance:

Extending the concept of edit distance to compare a string 
against an entire language, discussing associated algorithms and 
computational complexities.
Energy and Consciousness:

The theory proposing that consciousness arises from the 
organization of energy in the brain.
Phylogenetic Analysis:

Tracking evolutionary or developmental sequences, such as in 
the "Attentional Cladistics" project you mentioned.
Free Energy Principle:

A theory in neuroscience and predictive coding suggesting that 
brains strive to minimize free energy, akin to minimizing 
prediction error.
Darwinian Neurodynamic Approach:
The study on neural evolutionary processes through 
imperfect information copying and selection in neural 
informational patterns.

These topics span a range of domains, from AI and computational 
theories to neuroscience and philosophy of mind.

I decided to call the conversation Neurodynamic Intelligence, as 
a reference to the  Knights who say "NI".

That's a clever and humorous twist! "Neurodynamic 
Intelligence" captures the essence of our discussion, 
especially the intersections of neuroscience, computational 
theory, and AI. And referencing the "Knights who say 'NI'" 
from Monty Python gives it a delightful touch of whimsy. It's 
always fun to blend serious topics with a bit of humor.

A final summary

Certainly! Here's a summary of our discussion titled 
"Neurodynamic Intelligence":

Neurodynamic Intelligence

1. AI Scientists and Executives:
We differentiated between AI that aids in scientific research, 
utilizing probabilistic causal reasoning on Bayesian graphs, and 
executive AI which operates in real-world environments.

2. Curry-Howard Isomorphism:

We delved into how mathematical proofs can correspond to 
executable programs, illuminating the bridge between logic and 
computation.

3. Intelligence in AI:
Intelligence isn't just about the AI system's abilities. The 
documents, poems, or pictures an AI analyzes play a crucial 
role, as they constrain decision trees in possibility space.

4. Modeling Reality:
Sentences or phrases in languages can create intricate models 
of reality or even counterfactual scenarios, pointing to the 
vast potential of language as a modeling tool.

5. Landauer's Principle:
We discussed this principle, which indicates the minimum energy 
required to erase a bit of information, emphasizing the 
inherent energy costs of computation.

6. Edit Distances:
A deep dive into various types of edit distances like 
Levenshtein distance, LCS distance, and more, which measure 
differences between sequences. These are crucial for tasks 
like DNA sequence alignment.

7. Language Edit Distance:
We extended the concept of edit distance to measure a string 
against an entire language, discussing related algorithms and 
computational complexities.

Novelty and Imitation in the Brain:
A paper was introduced discussing how the brain navigates vast 
combinatorial spaces using a Darwinian process over cycles of 

imperfect copying and selection of neural informational 
patterns.

Consciousness as a Physical Process:
A discussion on a paper that proposes consciousness arises from 
the organization of energy in the brain. It revisits the notion 
of energy's role in the brain and consciousness, which has 
historically been underemphasized in modern neuroscience.

Our conversation touched on a convergence of topics from AI, 
computational theory, neuroscience, and the evolution of 
consciousness, all under the playful umbrella of 
"Neurodynamic Intelligence" inspired by Monty Python's 
"Knights who say 'NI'".

