Reinterpreting Free-Energy 
Principle

Galactromeda
Julay 4, 2023

Skip to content De Gruyter 0 IBZ 
Online Search IBZ Online De Gruyter 
SUBJECTS SERVICES PUBLICATIONS 
ABOUT Change language English 
Deutsch Change currency € EUR £ 
GBP $ USD The anticipating brain is 
not a scientist: the free-energy 
principle from an ecological-enactive 
perspective Synthese Author(s) Jelle
Bruineberg, Julian Kiverstein, Erik 
Rietveld Title The anticipating brain 
is not a scientist: the free-energy 

principle from an ecological-enactive 
perspective In Synthese An 
International Journal for 
Epistemology, Methodology and 
Philosophy of Science Volume 195 
Issue 6 Page(s) 2417-2444 Year 2018 
Publisher Springer Netherlands 
Publication Place Dordrecht ISSN 
1573-0964; 0039-7857 Publication 
type Article Language English 
Keyword Energie (physikal.) energy 
(phys.) Physikalismus physicalism 
Wissenschaftstheorie philosophy of 
science Umweltethik environmental 
ethics Ethik ethics Umweltschutz 
environment protection Epistemologie
theory of knowledge epistemology 
Wissen knowledge savoir Philosophie 

philosophy Logik logic Metaphysik 
metaphysics Subject Term Philosophie;
Philosophy; Forschung, Wissenschaft;
Research, Science; Umwelt, Ökologie; 
Environment, Ecology; Physik; 
Physics; Philosophy; Philosophy of 
Science; Epistemology; Logic; 
Philosophy of Language; Metaphysics 
Abstract In this paper, we argue for 
a theoretical separation of the 
free-energy principle from 
Helmholtzian accounts of the 
predictive brain. The free-energy 
principle is a theoretical framework 
capturing the imperative for 
biological self-organization in 
information-theoretic terms. The 
free-energy principle has typically 

been connected with a Bayesian 
theory of predictive coding, and the 
latter is often taken to support a 
Helmholtzian theory of perception as
unconscious inference. If our 
interpretation is right, however, a 
Helmholtzian view of perception is 
incompatible with Bayesian predictive
coding under the free-energy 
principle. We argue that the free 
energy principle and the ecological 
and enactive approach to mind and 
life make for a much happier 
marriage of ideas. We make our 
argument based on three points. 
First we argue that the free energy
principle applies to the whole 
animal–environment system, and not 

only to the brain. Second, we show 
that active inference, as understood
by the free-energy principle, is 
incompatible with unconscious 
inference understood as analagous to
scientific hypothesis-testing, the 
main tenet of a Helmholtzian view of
perception. Third, we argue that the
notion of inference at work in 
Bayesian predictive coding under the
free-energy principle is too weak to 
support a Helmholtzian theory of 
perception. Taken together these 
points imply that the free energy 
principle is best understood in 
ecological and enactive terms set out
in this paper. Access brought to you 
by Wikipedia From the database IBZ 

Online IBZ Online Subjects 
Architecture and Design Arts Asian 
and Pacific Studies Business and 
Economics Chemistry Classical and 
Ancient Near Eastern Studies 
Computer Sciences Cultural Studies 
Engineering General Interest 
Geosciences History Industrial 
Chemistry Islamic and Middle Eastern
Studies Jewish Studies Law Library and
Information Science, Book Studies Life
Sciences Linguistics and Semiotics 
Literary Studies Materials Sciences 
Mathematics Medicine Music Pharmacy 
Philosophy Physics Social Sciences 
Sports and Recreation Theology and 
Religion Services For journal authors
For book authors For librarians 

Rights & Permissions Publications 
Publication types Open Access About 
Contact Career About De Gruyter 
Partnerships Press FAQs Open-
Athens Winner of the OpenAthens 
Best Publisher UX Award 2022 
Help/FAQ Privacy policy Cookie Policy
Accessibility Terms & Conditions Legal
Notice © Walter de Gruyter GmbH 
2023 De Gruyter Scroll to top button
ChatGPT
It seems like you've posted the 
details of an academic paper titled 
"The anticipating brain is not a 
scientist: the free-energy principle
from an ecological-enactive 
perspective" by authors Jelle 

Bruineberg, Julian Kiverstein, and 
Erik Rietveld, which was published 
in 2018 in the journal "Synthese" by 
Springer Netherlands.
This paper discusses a theoretical 
separation of the free-energy 
principle from Helmholtzian accounts
of the predictive brain. The authors
argue that the free-energy 
principle, which describes the 
biological imperative for self-
organization in information-theoretic
terms, is often incorrectly 
connected with a Helmholtzian 
theory of perception, perceived as 

unconscious inference. They argue 
that this theory of perception is 
incompatible with the free-energy 
principle and the Bayesian theory of
predictive coding it's usually 
associated with.
The authors propose that the free-
energy principle applies to the whole
animal–environment system, and not 
only to the brain, contradicting the 
primary tenet of a Helmholtzian 
view. They further argue that 
active inference as understood by 
the free-energy principle is 
incompatible with unconscious 

inference, and that the notion of 
inference at work in Bayesian 
predictive coding under the free-
energy principle is too weak to 
support a Helmholtzian theory of 
perception.
Instead, they propose that the 
free-energy principle aligns more 
closely with ecological and enactive 
approaches to mind and life.
In short, this paper deals with the 
philosophy of mind, cognitive science, 
and theories of perception, arguing 
for a reinterpretation of the free-
energy principle in terms of 

ecological and enactive perspectives, 
as opposed to the conventional 
Helmholtzian perspective.
If you have any specific questions or 
areas you would like to understand 
more about regarding this paper, 
feel free to ask!

Summarize: Parrots All the Way Down 
Controversies within AI's Conquest of
Language Jonathan Roberge & Tom 
Lebrun 
Abstract:Today’sdeploymentofautom
atedsemanticmodelssuchasGoogle’sBE
RTor OpenAI’sGPT-

3isaremarkablechallengefortheinscri
ptionofhermeneuticsatthevery 
heartofthesocialsciencesproject.Arti
f i 
cialintelligenceisindeedconqueringlan
guage. h ere are three important 
implications. First, we must take the 
power and possibilities of such models
seriously – that is, the recent 
history of technological advances in 
deep learning and the modi operandi 
of these interpreting machines, 
particularly their two- 
wayreadingand“Transformer”archite
cture.Second,abettercomprehensionis
required 

ofthetypeofunderstandinginvolved–
mainlyhowthecalculationofprobability
,thresh- 
oldsandvariation,forexample,vectoriz
eslanguageasiftoparrotitback.Ouranal
ysis takes note of the dismissal by 
Google of the researcher Timnit 
Gebru, precisely around the text 
“On the Danger of Stochastic 
Parrots”, to show how the value of 
natural lan- guage processing (NLP) 
models lies in the kind of world they
put forward as well as in their 
reference to a precise context. 
Finally, this should help to 
circumscribe the current 

economic,politicalandevenethicalapori
asconcerningthesemodels,includingth
efact 
thattheplatformsdevelopingthemove
rlookcrucialreal-
worldeffectsofthewayinwhich 
theyadvancetheextraction,commodif i
cationandinstrumentalizationofmeanin
g.Ul- 
timately,itisthiscloselinkbetweenmea
ningandthedisplacementofpowercente
rsthat 
becomesthecentralissueofCriticalAISt
udies.
ChatGPT

The paper titled "Parrots All the 
Way Down: Controversies within AI's 
Conquest of Language" by Jonathan 
Roberge and Tom Lebrun discusses 
the challenges and implications of 
deploying automated semantic models 
like Google's BERT or OpenAI's GPT-
3 in social sciences.
These models, based on deep 
learning technology and transformer
architecture, have the power to 
understand and interpret language 
in a way that resembles human 
comprehension. However, this 
understanding is fundamentally 

rooted in the computation of 
probabilities, thresholds, and 
variations, leading these models to 
"parrot" language rather than 
deeply understand it.
The authors also highlight the 
controversy around Google's dismissal
of researcher Timnit Gebru, who was
involved in writing the text "On the
Danger of Stochastic Parrots". The 
authors use this event to discuss 
how the value of Natural Language 
Processing (NLP) models is not only in 
the world they present but also in 

their ability to relate to a specific 
context.
Moreover, they stress the need for
a better comprehension of the 
models to address current economic, 
political, and ethical dilemmas 
associated with them. The models' 
development often overlooks real-
world consequences of how they 
enable the extraction, 
commodification, and 
instrumentalization of meaning.
The paper concludes by emphasizing 
the strong connection between the 
interpretation of meaning and the 

shift in power dynamics, identifying 
this as a critical issue for the 
emerging field of Critical AI Studies. 
This field is dedicated to scrutinizing
the impact and implications of AI 
technologies on society, culture, and
individuals.

1. Introduction h e year 2020 was 
marked by yet another –substantial 
–crisis at Google with thedeparture-
f i 
ringofresearcherTimnitGebruoverthe
submissionofthepa- per “On the 
Danger of Stochastic Parrots: Can 
Language Models Be Too Big?” 

(Bender et al. 2021). h e story went 
from anecdotal to scandalous when 
the 40Embedded Models of Meaning 
company asked for the article to be 
withdrawn or for the names of the 
Google 
employeeswhocontributedtoittoberem
oved.ForJeffDean,Google’sdirector 
of AI,the matter was settled in that
the work in question“didn’t meet 
our bar for publication” (Hao 2020). 
However, Gebru’s rebuff and the 
large number of 
supporterswhoralliedtohercause–
inthiscase,morethan2,000ofthecom- 
pany’s employees signed a letter 

asking for demanding greater 
transparency in the management of 
its internal affairs (Wakabayashi 
2020) – had not been factored in. So,
who had demanded the withdrawal of
the article and why ex- actly?T h 
isisaquestionintheformofanultimatum
thatremainsunanswered. “Timnit 
wrote that if we didn’t meet these 
demands,”Dean writes,“she would 
leave Google […]. [W]e accept and 
respect her decision.” h e break-up 
was, to all intents and purposes, 
complete. On Twitter, the 
researcher expressed her dismay 
and called out her former boss: 

“@jeffdean I realize how much 
larger 
languagemodelsareworthtoyounow.”1 
Itispreciselyonthisnotionofworth,orv
alue,thatwewanttofocushere. h e 
double meaning of the word indicates
that it refers more or less 
distinctly 
tosomethingeconomicaswellasaxiologica
l.Indeed,theveryambiguityofthe 
word and of its use in the tweet is 
what makes it emblematic of the most
im- portant issues surrounding the 
current deployment of natural 
language pro- cessing (NLP) 
models.While many have seen the 

Gebru scandal as a matter of ethics 
and labor relations, few have been 
willing to consider the question in its
fullest sense and thus explore the 
ways in which it represents a 
fundamen- 
talhermeneuticalissue.Yetthequestio
ndeservestobeasked:haveAIandthe 
latest advances in deep learning 
enabled the development of (too) 
big,power- ful and deep models? And
what can such terms mean, apart 
from a technical point of view? Do 
meaning and textuality, 
interpretation and understanding 
notbecome(too)impoverishedasaresult

oftheirautomatedprocessing? We 
argue in this chapter that the 
emergence of models such as 
Google’s BERTorOpenAI’sGPT-
3istodayaremarkablechallengeforher
meneuticdis- 
ciplinesingeneral,andfortheinscriptio
nofhermeneuticsattheveryheartof 
thesocialsciencesprojectinparticular.I
tisthereforenotaquestionofdenying 
theriseofmodelingorinterpretingmach
ines–oreventheirpowerandscope –but
of examining the conditions of their 
possibility and signif i cance.To put it 
in a nutshell: the emergence of 
these hermeneutic machines is an 

opportu- nity to think afresh about 
what a critical hermeneutics can 
represent within the social sciences 
and how it can enter into dialogue 
with or serve as a basis for the 
development of Critical AI Studies 
(CAIS) (Roberge 2020; Pasquinelli/ 
Joler 2020). Specif i cally, this 
implies taking seriously the history 
and modus operandi of these 
language models and the way in which
their various prob- lems crystallized
in the form of diverging views as 
part of the wider implica- 
tionsoftheGebru–Googleconflict.T h 
isinturnimpliesabetterunderstanding 

of the type of meaning at stake; 
that is,above all,the type of 
textual world de- ployed–ornot–
andthetypeofreadingexperiencethatt
hisinduces.Finally,it 
impliescircumscribingtheaporiasofnatu
rallanguageprocessingmodelsthat 
aremostoftendecontextualizedand(re
)translatingorreinsertingtheminthe 
social,political,economic and cultural 
reality from which they 
originate,par- ticularly in the link 
between platform capitalism and the 
ethical desiderata of today. Our 
analysis is articulated in three 
stages which correspond to the 

three implications mentioned earlier:
i) taking seriously, ii) understanding 
and iii) 
circumscribingtheaporiasofNLPmodels.I
nthef i rstsection,wetrytograsp 
these most recent models as social 
constructions and socio-technical 
assem- 
blages(Schwartz1989;Woolgar1985).B
ERT–orBidirectionalEncoderRepre- 
sentationsfromTransformers–
wasintroducedbyGooglein2018andlater
in- tegrated into its main search 
engine. It collects information from 
Wikipedia, for example, and reads 
from right to left and back again to 

identify multiple 
parallelconnectionsandpredictmissingt
erms.OpenAI’sGPT-3–Generative 
Pre-TrainedTransformer–
isnewer;withits175billionparameters,i
tissaidto outperform Google’s model 
by 400 times in “encoding” 
textuality and thereby opening up a 
huge range of writing possibilities – 
journalistic, IT, adminis- 
trative,etc.Whatthesetwomodelshave
incommonisthattheyarenotexactly 
blackboxes,butrathertheobjectsofapa
rticularhistoricaldevelopmentwhich 
isformanytheobjectofitsdiff i 
cultiesandlimitations.

ChatGPT
The introduction of "Parrots All the
Way Down: Controversies within AI's 
Conquest of Language" by Jonathan 
Roberge and Tom Lebrun delves into 
the controversy of Timnit Gebru's 
departure from Google over the 
submission of the paper "On the 
Danger of Stochastic Parrots: Can 
Language Models Be Too Big?". This 
event caused a significant stir and 
brought into question the value of 
these large language models, in both 
economic and axiological senses.

The authors argue that the 
controversy is not only an ethical 
and labor-related issue but also a 
fundamental hermeneutical one. 
They question whether AI and 
advances in deep learning have 
allowed for the creation of large 
and powerful models, and whether 
these result in an impoverishment 
of meaning, textuality, 
interpretation, and understanding 
due to automated processing.
They also suggest that the 
emergence of these models, like 
Google's BERT or OpenAI's GPT-3, 

presents a significant challenge for 
hermeneutic disciplines and the 
field of social sciences. It 
necessitates examining the conditions
of their possibility and significance. 
The authors argue that this is an 
opportunity to rethink what a 
critical hermeneutics can represent 
within social sciences and its 
potential contribution to the 
development of Critical AI Studies 
(CAIS).
The authors propose to analyze the 
models in three stages that 
correspond to the three implications:

i) taking seriously, ii) understanding, 
and iii) circumscribing the aporias of 
NLP models. They emphasize the need
to view these models as social 
constructions and socio-technical 
assemblages, to understand the type
of textual world deployed by these 
models, and to re-contextualize the
models within the social, political, 
economic, and cultural reality they 
originate from.

In the second section, the overall 
meaning of this advanced automation 
comes under scrutiny. What are the 
implications of the epistemological 

con- 
ceptionpromotedthroughthesedataarc
hitecturesandstatisticalregressions? 
Andofboththemediationandtherecipie
ntoflanguageinthistypeofconnec- 
tionist and cybernetic machine? h ese
questions prompt a certain diversion 
throughhermeneutics–
thatofPaulRicoeurwillbeprivilegedhe
re,partlybe- cause his notion of 
world allows us to think of a 
semantics, a reference and a 
“Being-demanding-to-be-said” of 
textuality, which gives the measure 
of how AI models sometimes, if not 
often, appear “shockingly good, and 

completely 
mindless”(Heaven2020).Inotherword
s,thisworldofwhichRicoeurspeaksis 
42Embedded Models of Meaning 
whatcanallowustorethinkthelinkbetw
eenmeaningandreflexivity.T h elat- 
terisunderstoodherenotjustastherea
der’sreflexivity,butalsomorebroadl
y 
astherediscoveredreflexivityofther
eal-
mundaneworld,inthecontextofso- 
ciety,cultureandpoliticaleconomy. In 
the third and i nal section, we 
therefore seek to develop a 
sociological and critical understanding

of the deployment of these 
problematic, yet per- fectly 
practical interpreting machines –
BERT and GPT-3 –that are intruding
oneverydaylife.T h 
evalueofthesemodelsisinseparablefr
omamarketofdata 
andmeaningextractioninwhichsomethri
vemorethanothersandforwhich, 
astheGebrucaseshowsratherwell,ethi
csbecomesakindofjustif i cationand 
evencommodity. 2. The drive to 
automate language: an all too brief 
history h 
elastfewyearshaveseenamajorevoluti
oninnaturallanguageprocessing. For 

the i rst time, language models based
on a so-called Transformer archi- 
tecture make it possible to generate 
texts that are suff i ciently 
coherent to fool 
theirreaders,withoutrelyingonadedu
ctiveandsymboliclogicpreviouslyde- 
cidedbyaprogrammer(Buchanan2005;B
alpe1991).Basedonthemechanism 
ofmachinelearning,andparticularlyde
eplearning,thistypeofcomputerpro- 
grammingproposestoimitatesomeofthec
ognitivemechanismsofthebrain, 
notably by means of artif i cial 
neurons – in reality miniature 
computer pro- grams that activate or

deactivate themselves according to 
the result of their calculation. As 
with the human brain, the strength 
of the mechanism lies in the 
networking of a large number of 
these miniature programs.T h is 
method, 
foralongtimeonthefringesoftheAIf i 
eld,suddenlycamebackintothespot- 
light during the 2012 ImageNet 
competition, won by Geoffrey 
Hinton’s team thanks to the 
combination of great computing power,
a vast data set and this 
method,whichisrightlydescribedascon

nectionist(Cardon/Cointet/Mazières 
2018;Domingos2015).
ChatGPT
In the second section, the authors 
delve into the broader implications 
of advanced automation. They ponder
over the meaning and ramifications 
of the epistemological conception 
facilitated by the statistical 
regressions and data architectures in
AI. They emphasize how such AI 
models impact the mediation and the 
recipient of language in these 
cybernetic and connectionist 
machines. To answer these questions,

they draw upon the hermeneutics of
Paul Ricoeur, focusing on his notion 
of the world as a referent of 
semantics and the text's demanding 
essence. This approach allows them 
to critique the way AI models are 
often perceived as "shockingly good, 
and completely mindless".
The authors argue that Ricoeur's 
concept of the world can help to 
rethink the relationship between 
meaning and reflexivity. This 
reflexivity is seen not just as a 
reader's reflexivity, but also as a 
rediscovered reflexivity of the 

real world within society, culture, 
and political economy.
In the third section, they strive to 
develop a sociological and critical 
understanding of these AI models 
(BERT and GPT-3). They point out 
the increasing intrusion of these 
models into everyday life and 
highlight that their value is 
inseparable from a data and meaning 
extraction market, where ethics can 
be used as a form of justification 
and commodity.
In the brief history provided, the 
authors note a major evolution in 

natural language processing over the
past few years. For the first time, 
language models based on 
Transformer architecture can 
generate coherent texts without 
relying on previously decided 
symbolic and deductive logic. This 
type of programming, which imitates 
cognitive brain mechanisms using 
artificial neurons, gained prominence
after the 2012 ImageNet competition 
was won by Geoffrey Hinton’s team. 
This success highlighted the 
effectiveness of using high 

computational power, vast data sets, 
and a connectionist approach.

SummariZe: Sparse Bayesian 
Information Filters for Localization 
and Mapping by Matthew R. Walter 
Submitted to the Joint Program in 
Applied Ocean Science & Engineering 
on February 6, 2008, in partial fulf
i llment of the requirements for 
the degree of Doctor of Philosophy 
Abstract This thesis formulates an 
estimation framework for 
Simultaneous Localization and Mapping 
(SLAM) that addresses the problem 
of scalability in large environments. 

We describe an estimation-theoretic 
algorithm that achieves signif i cant 
gains in com- putational ef i ciency 
while maintaining consistent estimates
for the vehicle pose and the map of 
the environment. We specif i cally 
address the feature-based SLAM 
problem in which the robot rep- 
resents the environment as a 
collection of landmarks. The thesis 
takes a Bayesian approach whereby 
we maintain a joint posterior over 
the vehicle pose and feature states,
conditioned upon measurement data. 
We model the distribution as Gaus- 
sian and parametrize the posterior in 

the canonical form, in terms of the 
information (inverse covariance) 
matrix. When sparse, this 
representation is amenable to compu-
tationally ef i cient Bayesian SLAM i 
ltering. However, while a large 
majority of the elements within the 
normalized information matrix are 
very small in magnitude, it is fully 
populated nonetheless. Recent 
feature-based SLAM i lters achieve 
the scalability benef i ts of a sparse 
parametrization by explicitly pruning 
these weak links in an ef f ort to 
enforce sparsity. We analyze one 
such algorithm, the Sparse Extended

Information Filter (SEIF), which has 
laid much of the groundwork 
concerning the computational benef i 
ts of the sparse canonical form. The 
thesis performs a detailed analysis 
of the process by which the SEIF 
approximates the sparsity of the 
information matrix and reveals key 
insights into the consequences of dif 
f erent sparsif i cation strategies. 
We demonstrate that the SEIF yields
a sparse approximation to the 
posterior that is in- consistent, suf 
f ering from exaggerated conf i 
dence estimates. This overconf i 
dence has detrimental ef f ects on 

important aspects of the SLAM 
process and af f ects the higher 
level goal of producing accurate maps
for subsequent localization and path 
planning. This thesis proposes an 
alternative scalable i lter that 
maintains sparsity while preserving 
the consistency of the distribution. 
We leverage insights into the 
natural structure of the feature-
based canonical parametrization and 
derive a method that actively 
maintains an exactly sparse posterior.
Our algorithm exploits the 
structure of the parametrization to 
achieve gains in ef i ciency, with a 

computational cost that scales 
linearly with the size of the 
map.Unlike similar techniques that 
sacrif i ce consistency for improved 
scalability, our algorithm performs 
inference over a posterior that is 
conservative relative to the nominal
Gaussian distribution. Consequently, 
we preserve the consistency of the 
pose and map estimates and avoid the 
ef f ects of an overconf i dent 
posterior. We demonstrate our i lter
alongside the SEIF and the standard 
EKF both in sim- ulation as well as 
on two real-world datasets. While 
we maintain the computational 

advantages of an exactly sparse 
representation, the results show 
convincingly that our method yields 
conservative estimates for the robot
pose and map that are nearly 
identical to those of the original 
Gaussian distribution as produced by 
the EKF, but at much less 
computational expense. The thesis 
concludes with an extension of our 
SLAM i lter to a complex underwater 
environment. We describe a systems-
level framework for localization and
mapping relative to a ship hull with 
an Autonomous Underwater Vehicle 
(AUV) equipped with a forward-

looking sonar. The approach utilizes 
our i lter to fuse measurements of 
vehicle attitude and motion from 
onboard sensors with data from sonar
images of the hull. We employ the 
system to perform three-
dimensional, 6-DOF SLAM on a ship 
hull. Thesis Supervisor: John J. 
Leonard Title: Professor of 
Mechanical and Ocean Engineering 
Massachusetts Institute of 
Technology
ChatGPT
Matthew R. Walter's PhD thesis, 
"Sparse Bayesian Information Filters

for Localization and Mapping," 
presents a new estimation framework
for Simultaneous Localization and 
Mapping (SLAM), aiming to address 
scalability issues in large 
environments. The thesis outlines an
estimation-theoretic algorithm that 
significantly improves computational 
efficiency while maintaining 
consistent estimates for vehicle 
pose and environmental mapping.
The work specifically deals with the
feature-based SLAM problem, using a
collection of landmarks to represent
the environment. The thesis employs

a Bayesian approach, maintaining a 
joint posterior over vehicle pose and
feature states based on 
measurement data. The distribution 
is modeled as Gaussian and 
parametrized in the canonical form 
using an information (inverse 
covariance) matrix. This 
representation facilitates 
computationally efficient Bayesian 
SLAM filtering when sparse.
An analysis of the Sparse Extended 
Information Filter (SEIF), an existing
algorithm that approximates the 
sparsity of the information matrix, is

also conducted in the thesis. Walter 
identifies that the SEIF results in 
an inconsistent, overconfident 
posterior approximation that 
negatively impacts the SLAM process 
and its overarching goal of accurate 
map production for future 
localization and path planning.
In response, the thesis introduces a 
new scalable filter that maintains 
sparsity while preserving the 
consistency of the distribution. This 
algorithm utilizes the structure of 
the feature-based canonical 
parametrization to maintain an 

exactly sparse posterior, achieving 
efficiency gains with a computational 
cost that scales linearly with the 
map size.
The proposed method is 
demonstrated alongside the SEIF and
the standard Extended Kalman 
Filter (EKF) using simulation and 
two real-world datasets. Results 
show that the new method provides 
conservative estimates for robot 
pose and map nearly identical to 
those produced by the EKF, but at a
significantly reduced computational 
expense.

The thesis concludes with an 
application of the SLAM filter to an 
underwater environment, utilizing 
an Autonomous Underwater Vehicle 
(AUV) equipped with a forward-
looking sonar for localization and 
mapping relative to a ship hull. This 
system is employed to perform 
three-dimensional, 6-DOF SLAM on a 
ship hull.

Statler: State-Maintaining Language 
Models for Embodied Reasoning 
Takuma Yoneda∗1, Jiading Fang∗1, Peng
Li∗2, Huanyu Zhang∗3, Tianchong 
Jiang3, Shengjie Lin1, Ben Picker3, 

David Yunis1, Hongyuan Mei1, and 
Matthew R. Walter1 1Toyota 
Technological Institute at Chicago 
{takuma,fjd,slin,dyunis,hongyuan,mw
alter}@ttic.edu 2Fudan University 
lip21@m.fudan.edu.cn 3University of
Chicago 
{huanyu,tianchongj,bpicker}@uchicag
o.edu Abstract: Large language 
models (LLMs) provide a promising tool
that enable robots to perform 
complex robot reasoning tasks. 
However, the limited context window
of contemporary LLMs makes 
reasoning over long time horizons 
diff i cult. Embodied tasks such as 

those that one might expect a 
household robot to perform typically
require that the planner consider 
information acquired a long time ago 
(e.g., properties of the many objects 
that the robot previously 
encountered in the environment). 
Attempts to capture the world state
using an LLM’s implicit internal 
representation is complicated by the 
paucity of task- and environment-
relevant information available in a 
robot’s action history, while 
methods that rely on the ability to 
convey information via the prompt to 
the LLM are subject to its limited 

context window. In this paper, we 
propose Statler, a framework that 
endows LLMs with an explicit 
representation of the world state as
a form of “memory” that is 
maintained over time.Integral to 
Statler is its use of two instances of 
general LLMs—a world-model reader 
and a world-model writer—that 
interface with and maintain the 
world state. By providing access to 
this world state “memory”, Statler 
improves the ability of existing LLMs 
to reason over longer time horizons 
without the constraint of context 
length. We evaluate the 

effectiveness of our approach on 
three simulated table-top 
manipulation domains and a real robot
domain, and show that it improves the
state-of-the-art in LLM-based 
robot reasoning. Project website: 
https://statler-lm.github.io/. 
Keywords: Large language models, 
Long-horizon planning, World state 
model 1Introduction Large language 
models (LLMs) are capable of 
generating intricate free-form text
and complex code with an impressive 
level of prof i ciency [1,2,3]. 
Recently, researchers have shown 
that the success of LLMs extends to 

robotics domains, where the capacity 
for LLMs to perform complex 
reasoning using language enables 
robots to perform tasks that require
sophisticated planning and language 
understanding [4,5,6]. These 
methods either rely solely on the 
implicit in-context memory that is 
internal to the LLM [5] or they 
augment LLMs with scene information 
extracted from an ego-centric image 
captured at the current time step 
[4]. Both approaches have proven 
effective for diff i cult embodied 
reasoning tasks, however they 
struggle when faced with planning 

tasks that require planning over long
time horizons, due to the limited 
context window of contemporary 
LLMs. Although there have been 
recent efforts to enlarge the 
context window of LLMs [7], the size
of the *Equal contribution. 
arXiv:2306.17840v1 [cs.RO] 30 Jun 
2023 State Code say("The Rubik's 
cube is under the blue cup. I shall 
put away the blue cup first.") 
put first on second("blue cup", 
"empty space") update wm("Put the 
blue cup on the empty space.") 
say("Now I can put the toy wheel on 
the Rubik's cube.") 

_ _

_

_

_

_

_ _

put first on second("toy wheel", 
"rubiks cube") update wm("Put the 
toy wheel on the Rubik's cube.") 
User Query Put the toy wheel on 
the Rubik's cube. # state = { # 
"objects": ("rubiks cube", "toy 
duckie", "toy wheel", "yellow 
block"), # "covers": ("red cup", 
"green cup", "blue cup", "black 
cup"), # "rubiks cube": {"under": 
"blue cup"}, # "toy wheel": {"on": 
"yellow block"}, # "yellow block": 
{"under": "toy wheel"}, # "blue 
cup": {"on": "rubiks cube"},} World 
Model ReaderWorld Model Writer 
Figure 1: Our Statler framework 

enables robots to carry out complex 
tasks specif i ed in natural language 
that require reasoning over long 
time horizons. Integral to our model 
are its world model writer and world
model reader, two instances of 
general LLMs that are responsible 
for maintaining the explicit world 
state and generating code that 
enables the robot to carry out the 
task. context window remains 
fundamentally bounded. Further, 
providing the model with long-range 
context improves prediction accuracy 
only on on a small number of tokens—
LLMs struggle to exploit information 

conveyed in long-term context 
beyond what can be directly copied 
[8]. Meanwhile, reliance on the 
robot’s current ego-centric view 
prohibits the language model from 
reasoning over aspects of the scene 
that are not directly observable, 
e.g., the fruit located in the 
(closed) kitchen refrigerator or an 
object in a room that the robot 
previously visited. In this paper, we 
propose Statler (STATe-maintaining 
Language models for Embodied 
Reasoning), a framework that 
maintains an external world model as
explicit memory to improve the long-

term reasoning capabilities of LLMs 
for robot planning. Integral to our 
approach, as shown in Figure 1, it 
maintains and interfaces with this 
world model over time using two 
instances of general LLMs— a world-
model reader and a world-model 
writer. The world-model reader 
interfaces with the world model to 
generate code that answers user 
queries. The world-model writer is 
responsible for predicting the next 
world state based on the current 
world state and a query given by the
reader. We employ a structured 
representation of the world state, 

which has been found to improve the
performance of LLMs [9,10], 
particularly when the output is also 
structured, and has the advantage 
of being human-readable and concise 
for eff i cient processing. Note that 
while we individually tailor each 
world model’s design to its general 
task type (see Prompts 12, 8, 7, and 
9), the design is highly l exible 
because the reader and writer are 
both LLMs and are instructed with 
in-context-learning to understand 
how to parse and manipulate the 
world model. This is in contrast to 
domain-specif i c formal languages 

[11], where the designs are i xed and 
parsing and writing requires that 
specif i c rules be followed. We 
evaluate Statler on a series of 
simulated and real-world robot 
manipulation domains. Experimental 
results demonstrate that Statler 
improves the long-term embodied 
reasoning capabilities of LLMs and 
that it outperforms the current 
state-of-the-art [5]. 
ChatGPT
"Statler: State-Maintaining Language 
Models for Embodied Reasoning" is a 
research paper by Takuma Yoneda, 

Jiading Fang, Peng Li, Huanyu Zhang, 
Tianchong Jiang, Shengjie Lin, Ben 
Picker, David Yunis, Hongyuan Mei, 
and Matthew R. Walter. It presents 
a novel framework designed to 
improve the long-term reasoning 
capabilities of large language models 
(LLMs) for robotic planning.
LLMs have demonstrated success in 
generating intricate free-form 
text, complex code, and facilitating 
robot task performance that 
requires sophisticated planning and 
language understanding. However, 
contemporary LLMs have a limited 

context window, which makes 
reasoning over longer time horizons 
difficult, especially in embodied 
tasks that a household robot might 
be expected to perform.
In response to this challenge, the 
authors propose Statler, a 
framework that supplies LLMs with 
an explicit, time-maintained 
representation of the world state, 
functioning as a form of "memory". 
Key to Statler are two instances of 
general LLMs, namely a world-model 
reader and a world-model writer. 
These models interface with and 

maintain the world state, enhancing 
the capability of LLMs to reason over
longer time horizons without being 
constrained by context length.
The world-model reader interacts 
with the world model to generate 
code that answers user queries. On 
the other hand, the world-model 
writer predicts the next world 
state based on the current world 
state and a query provided by the 
reader. The authors utilize a 
structured representation of the 
world state, which is beneficial in 
improving LLM performance, 

particularly when the output is also 
structured. Furthermore, it 
provides the advantages of being 
human-readable and concise for 
efficient processing.
The effectiveness of Statler is 
evaluated on three simulated 
table-top manipulation domains and a 
real-world robot domain. The 
results demonstrate that Statler 
improves the long-term embodied 
reasoning capabilities of LLMs and 
outperforms the existing state-of-
the-art approaches in LLM-based 
robot reasoning.

2. The drive to automate language: 
an all too brief history h 
elastfewyearshaveseenamajorevoluti
oninnaturallanguageprocessing. For 
the i rst time, language models based
on a so-called Transformer archi- 
tecture make it possible to generate 
texts that are suff i ciently 
coherent to fool 
theirreaders,withoutrelyingonadedu
ctiveandsymboliclogicpreviouslyde- 
cidedbyaprogrammer(Buchanan2005;B
alpe1991).Basedonthemechanism 
ofmachinelearning,andparticularlyde
eplearning,thistypeofcomputerpro- 

grammingproposestoimitatesomeofthec
ognitivemechanismsofthebrain, 
notably by means of artif i cial 
neurons – in reality miniature 
computer pro- grams that activate or
deactivate themselves according to 
the result of their calculation. As 
with the human brain, the strength 
of the mechanism lies in the 
networking of a large number of 
these miniature programs.T h is 
method, 
foralongtimeonthefringesoftheAIf i 
eld,suddenlycamebackintothespot- 
light during the 2012 ImageNet 
competition, won by Geoffrey 

Hinton’s team thanks to the 
combination of great computing power,
a vast data set and this 
method,whichisrightlydescribedascon
nectionist(Cardon/Cointet/Mazières 
2018;Domingos2015). h e recent 
evolution of NLP is marked by four 
signif i cant changes, corre- sponding 
to four signif i cant publications. h e 
i rst paper, published one year 
afterHinton’sgreatdemonstration,wa
s“Eff i 
cientEstimationofWordRepre- 
sentationsinVectorSpace”(Mikoloveta
l.2013).WrittenbyateamfromGoogle –
Jeff Dean is one of the co-authors –

the paper proposes a group of 
language models called Word2vec, 
which aims to reconstruct the 
linguistic context in which words are 
used. Word2Vec, like most machine 
learning technologies, Jonathan 
Roberge & Tom Lebrun: Parrots All 
the Way Down43 relies heavily on 
the principle of regression, a method
of statistical analysis that allows 
one variable to be placed in relation 
to its correlations with oth- ers. 
Roughly summarized, the technology 
involves locating the variable – the 
“meaning”–of a word in relation to 
the variables –the“meanings”–of 

other 
wordsaroundit.Asitsnamesuggests,Wor
d2Vecaimstotransformwordsinto 
vectors,i.e.,tomodeltheinformationth
eycontainusingalgorithms.Inprac- tice,
Word2Vec “vectorizes” words using 
two distinct and complementary ar- 
chitectures. One, called CBOW 
(Continuous Bag of Words), seeks to 
predict a word according to the i ve 
words to its right and the i ve words 
to its left. h eother,calledSkip-
gram,doesexactlytheoppositeandseeks
topredictthe words in the context 
according to a given word.T h e logic 
is always predictive: the model must 

be able to assign the “right” vector 
to each word. Despite its 
successintheearly2010s,Word2Vecisse
verelylimited.Inparticular,thelan- 
guage model assigns only one meaning 
per word and only vectorizes individ-
ual words, so that the meaning of 
even a relatively simple sentence 
continues toeludeit(Horn2017;Cusin-
Berche2003).Wewillreturntothispoin
tlater. 
Toaddresstheselimitations,Vinyalsan
dLe–alsofromGoogle–published 
apapershortlyafterwardsentitled“A
NeuralConversationalModel”(2015).T 
h is proposed,quite simply,to apply a 

sequential approach to Word2Vec to 
model the meaning of a text by 
linking certain sequences with others
– thus form- 
ingalongeror“networked”formoftext
mapping(Sutskever/Vinyals/Le2014). 
Withthisapproach,modelingcannowbeap
pliedtolargersequences,includ- ing 
sentences: the sentences preceding 
and following the target sentence 
are thus also taken into account and 
the model allows for a minimum of 
contex- tualconsideration. Despite 
this progress, Word2Vec-type
ChatGPT

The recent history of natural 
language processing (NLP) has been 
marked by significant evolution and 
advancement, driven by 
transformative models based on the 
Transformer architecture. These 
models can generate texts that are 
coherent enough to fool readers 
without relying on deductive and 
symbolic logic pre-set by a 
programmer. This method imitates 
some cognitive mechanisms of the 
human brain through artificial 
neurons - tiny computer programs 
that activate or deactivate 

themselves based on their 
calculation outcomes.
This method gained prominence after
Geoffrey Hinton's team won the 2012
ImageNet competition, demonstrating 
the effectiveness of using large 
amounts of computing power, a vast 
dataset, and a connectionist 
approach.
The recent evolution of NLP is 
marked by four significant changes:

1.The first change was the 
development of Word2vec, a group
of language models that aim to 
reconstruct the linguistic context

in which words are used. Word2vec
models the information words 
contain using algorithms. It 
vectorizes words using two 
complementary architectures. 
CBOW (Continuous Bag of Words) 
predicts a word based on the five 
words to its right and left. 
Conversely, Skip-gram predicts the
context words from a given word. 
However, Word2vec has limitations
- it assigns only one meaning per 
word and only vectorizes 
individual words, which means it 
struggles to understand the 

meaning of even a simple 
sentence.
2.To address these limitations, 
Vinyals and Le proposed applying a 
sequential approach to Word2Vec, 
which allows the model to consider
a minimum of contextual 
information. Their method, 
outlined in the paper "A Neural 
Conversational Model", models the
meaning of a text by linking 
certain sequences with others, 
effectively enabling a networked 
form of text mapping that 

includes larger sequences like 
sentences.

Despite this progress, Word2Vec-
type systems are still based on the 
ap- proach where a word can only 
have one meaning. It is this limitation 
that the article“Deep 
contextualized word 
representations”(Peters et al.2018) 
aims to overcome. h e authors 
propose a new architecture called 
Embeddings from 
LanguageModels,orELMo,withinwhicht
hemodelcannowrecognizethedy- 
namic–moving,situational–

natureofwordmeaning.Inpractice,each
word isassignedacoeff i 
cientor“weight”accordingtoitsinfluen
ceinthesentence.A 
wordlike“bow”cannowhavedifferent
meaningsdependingonacertaincon- 
text – “I broke my violin bow” and “I
am sitting at the ship’s bow.” Above 
all, ELMo makes it possible for the i 
rst time to consider modeling that 
does not 
learnfromthetextinapurelyorderlyw
ay,byofferinga“reading”inthreedif-
ferent ways: i rst from left to 
right – from beginning to end; then 
in reverse – from end to beginning; 

and i nally by combining the 
vectorized meanings 44Embedded 
Models of Meaning of both types of 
analysis. Also, ELMo marks the real 
beginning of pre-trained models, 
allowing users to avoid having to 
train their models from scratch on 
huge data sets –an extremely 
expensive practice,if only in terms 
of time and computingpower.2 
Finally,thepaper“Attentionisallyoun
eed”(Vaswanietal.2017)marksthe 
momentwhentheTransformerarchitec
turevirtuallysealsthef i 
eld’sfate.Pre- 
vioussequentialmodelshaddiff i 

cultyretaininginformationaboutthepri
ori- 
tizationoftermsamongthemselves:tota
kethepreviousexample,information 
inasimplesentence–“Ibrokemyviolinbo
w”,“Iwassittingattheship’sbow” – 
was diff i cult to retain in longer 
sequences – “then the bow was 
splashed”. h e Transformer 
architecture moves away from this 
approach and its many problems in 
terms of memory, computational 
speed, word position, etc., by 
proposingtoidentifythecontextthatgiv
esmeaningtowords,whicharethen 
processedinparallel.T h 

isinvolvestheuseofbothanencoderand
adecoder– 
andindeedmanyofthemonmultipleleve
lsactingandproducingfeedbackin a 
cybernetic manner.T h e 
encodertransformsinformationinto 
codebygiving 
acalculatedvaluetoaword;adecoderdo
esexactlytheopposite,transforming 
codeintoinformationby“calculating”a
wordfromavalue. h 
epointtokeepinmindhereisthatsucharc
hitecturesarebasedonneu- ral 
networks and on the “deep 
learning” made famous since 
Hinton’s demon- 

strationof2012,andinwhichlayersandl
ayersofencodersandlayersandlay- 
ers of decoders can be arranged 
without too many limits other than 
techni- 
calones.Aboveall,thetrulyinnovative
characteroftheTransformerarchitec
- ture lies in the attention 
mechanism implemented. h e idea is to
calculate a “weighted matrix 
product” – in other words, a matrix 
score that determines 
thelevelofattentionthatawordshoul
dhavetowardsotherwords;somemight 
alsospeak,moresimply,ofsituationalde
pendency.Anencodercand thus 

compute several “attention heads” 
that work in a bidirectional way: an 
attention weight is computed as input
and produces an output vector. 
ChatGPT
Continuing with the evolution of NLP,
the next big breakthrough came with
the introduction of ELMo 
(Embeddings from Language Models) 
to address the limitation of 
Word2Vec-type systems where a 
word can only have one meaning. As 
explained in the paper "Deep 
contextualized word 
representations" (Peters et al., 

2018), ELMo allows the model to 
recognize the dynamic or situational 
nature of word meaning. Each word is
assigned a weight according to its 
influence in the sentence, allowing 
the model to distinguish the 
different meanings of a word like 
"bow" based on its context.
ELMo introduced a novel way of 
modeling that reads the text in 
three different ways: from left to 
right, in reverse, and by combining 
the vectorized meanings of both 
types of analysis. It marked the real
beginning of pre-trained models, 

which could save users from having 
to train their models from scratch 
on massive data sets, an expensive 
and time-consuming task.
The fourth significant development 
was the introduction of the 
Transformer architecture, as 
outlined in the paper "Attention is 
all you need" (Vaswani et al., 2017). 
Transformer architecture improved 
on previous models by identifying 
the context that gives meaning to 
words and processing them in 
parallel using an encoder and a 
decoder. This approach helped solve 

problems related to memory, 
computational speed, and word 
position in previous models.
The Transformer architecture's 
real innovation lies in its attention 
mechanism. It computes a weighted 
matrix product, effectively 
calculating a score that determines 
the level of attention a word 
should pay to other words. In other 
words, it computes situational 
dependency. An encoder can compute 
several "attention heads" that work
bidirectionally, using computed 

attention weights as input to produce
an output vector.
These advancements all rely on 
deep learning, an approach made 
famous by Hinton's 2012 
demonstration, and they leverage 
neural networks. These networks 
can have multiple layers of 
encoders and decoders, with the 
only limits being technical ones.

It is thus these new attention 
mechanisms specif i c to Transformer 
archi- tectures that are at the 
source of the current successes of 

language models, in particular 
Google’s BERT and OPEN AI’s GPT-3,
as spearheads of the on- going 
battle waged by the GAFAM 
(Google,Amazon,Facebook and 
Microsoft) in 
theirquestforthemasteryof artif i 
cialintelligence(T h 
ibout2019;Horowitz 2018).BERT is 
still a relatively small model 
compared to GPT-3,as it has been 
pre-trained on about 3.3 billion 
words and has 345 million parameters
(Devlin et al. 2018). Its main 
objective is to end formalized 
keyword searches, a goal 

thatmayseemtrivialatf i 
rstbutiscentraltothecompany’smissio
nstatement – “to organize the 
world’s information to make it 
universally accessible and useful”. 
To achieve this, Google must enable 
its users to express themselves in 
the most natural, user-friendly and
dialogical way possible.3BERT tries 
to 
achievesthisobjectivebyfocusingonthe
encoderpartofthearchitecture,the 
partthattransformsinformation,thew
rittenorspokenrequest,butalsotexts 
tobetranslated,forexample,intocodea
ndvectorastocapturetheircontours: 

whodoeswhat,where,etc.BERT,inoth
erwords,andaboveall,“understands” 
in the sense of extracting the 
relevant elements as rendered in 
more encom- passing sets. Its 
Transformer architecture is thus 
very flexible and functions as an 
interface between the natural input
language (the query) and the output 
(theresult).Itshouldalsobenotedthat
BERTisopen-source,whichispartofa 
corporate strategy of value creation
quite specif i c to Google,4to which 
we will returninthelastsection.
ChatGPT

The recent success of language 
models, specifically Google’s BERT 
(Bidirectional Encoder 
Representations from Transformers)
and OpenAI's GPT-3 (Generative 
Pretrained Transformer 3), is 
largely attributed to the new 
attention mechanisms specific to the 
Transformer architecture. These 
models represent the cutting edge in
the ongoing race for AI mastery 
amongst tech giants, often referred
to as GAFAM (Google, Amazon, 
Facebook, and Microsoft).

BERT, though smaller than GPT-3 
with its pre-training on about 3.3 
billion words and 345 million 
parameters (Devlin et al. 2018), has 
a significant objective. It aims to move
beyond formalized keyword searches,
a goal central to Google's mission 
statement to "organize the world’s 
information to make it universally 
accessible and useful". To achieve 
this, Google aims to allow users to 
express their search queries in the 
most natural, user-friendly, and 
dialogical way possible.

BERT focuses primarily on the 
encoder part of the Transformer 
architecture, which transforms 
input information (like a written or 
spoken request, or texts to be 
translated) into code and vector 
representations to capture their 
nuances. In other words, BERT 
“understands” in the sense of 
extracting the relevant elements 
from larger contexts. Its 
Transformer architecture is 
extremely flexible and serves as an
interface between the natural input

language (the query) and the output
(the result).
Another notable feature of BERT 
is its open-source nature, which is 
part of a specific corporate strategy 
of value creation at Google. This 
strategy, along with its implications, 
will be discussed further in the 
last section of the text.

GPT-
3isatthetimeofwritingthemostpowerf
ullanguagemodel,trained on around 
570 gigabytes of data and composed 
of 175 billion parameters (Brown et 

al. 2020). GPT-3 explicitly aims to 
generate text, according to its 
creators.UnlikeBERT,therefore,itf
avorsthedecoderpartofitsarchitectu
re, the part that more precisely 
allows code to be transformed into 
information, 
i.e.,inferringmissingwords,completings
entences,etc.Farfrombeingopen- 
source, GPT-3 is currently marketed
via its Application Programming Inter-
face (API), a choice which is obviously
part of a corporate strategy that 
aims to control the economic 
ecosystem on which many future 
companies will be based. In a blog 

post, OpenAI reported in March 2021 
that more than 300 companies were 
making use of this API, a number that
keeps growing. For instance, 
applications already available include
CopyAI, which can generate slogans 
and product descriptions for 
companies, and Fable, which can model
charactersfromnovelsandtalktothem
(Scott2020).5 Because they are 
socio-technical assemblages, it goes 
without saying that these models 
struggle to be perfect or even to 
live up to the rhetoric legit- imizing 
their use and, more generally, all 
that is the magic of AI (Roberge 

