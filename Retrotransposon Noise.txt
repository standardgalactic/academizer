Retrotransposon Noise
Open sidebar
Retrotransposon Noise
DALL��E 3
Retrotransposons in the brain according to Fred Gage
ChatGPT
Fred Gage is a neuroscientist known for his work on neurogenesis, the process by which new neurons are generated in the adult brain. One of the most intriguing areas of his research involves the role of retrotransposons in the brain.
Retrotransposons are mobile genetic elements that can copy and insert themselves into new locations within the genome. Originally considered "junk DNA", these elements have been found to play active roles in shaping the genome and influencing gene expression.
Gage's research has suggested the following key points regarding retrotransposons in the brain:
Activity in the Brain
: Unlike other cells in the body where retrotransposon activity is suppressed, in the brain, particularly in neurons, there is evidence of active retrotransposon elements. This means that the genomes of individual neurons can differ due to the insertion of these elements.
Contribution to Diversity
: This activity might contribute to the diversity of neuronal populations, making each neuron genetically unique in some ways. This genomic mosaicism could play a role in how neurons form connections and function.
Potential Role in Evolution
: The activity of retrotransposons in the brain might have evolutionary implications. It might provide a mechanism for rapid adaptation by creating genetic diversity within neuronal populations.
Neurological Disorders
: There is evidence to suggest that dysregulation of retrotransposon activity might be linked to certain neurological disorders. However, the exact mechanisms and implications are still under investigation.
Response to Experience
: Some studies suggest that retrotransposon activity in the brain might be influenced by external factors such as stress or environmental stimuli. This hints at a dynamic genome that can change in response to experiences.
Potential Therapeutic Applications
: Understanding the role of retrotransposons in the brain might pave the way for therapeutic applications, especially in disorders where their activity is dysregulated.
It's important to note that while the role of retrotransposons in the brain is a fascinating area of study, many aspects of this research are still in the early stages, and much remains to be discovered.
Aren't there some error-correcting code where noise might increase accuracy, or be used to generate input for various functions and filters, as in stable diffusion algorithms?
ChatGPT
Yes, there are concepts in both error-correcting codes and algorithms where noise or perturbations can be beneficial. Here's a brief overview:
Stochastic Resonance
: This is a phenomenon where the presence of a certain amount of noise can increase the sensitivity of a system to weak signals. In other words, adding noise can make a weak signal more detectable than it would be in a noise-free environment. This concept is found in various physical and biological systems and has been applied in signal processing.
Dithering
: In digital signal processing and image processing, dithering is the intentional addition of noise to reduce quantization error. This can make quantized signals and images look or sound more natural.
Simulated Annealing
: This is an optimization algorithm that occasionally accepts worse solutions to escape local minima. It introduces a controlled amount of "noise" or randomness to the search process. The probability of accepting worse solutions decreases over time, mimicking the annealing process in metallurgy.
Error-Correcting Codes
: While noise is generally detrimental to data transmission and storage, error-correcting codes are designed to handle a certain amount of noise or errors. By adding redundancy to data, these codes can detect and correct errors. In some advanced coding theories, noise can be used to understand the limits of error correction and to design better codes.
Stable Diffusion Algorithms
: In computational methods, noise can be introduced to ensure stability in certain diffusion processes. For instance, in some machine learning models, noise is added to the input data to make the model more robust and prevent overfitting.
Noise-based Logic
: This is a more experimental area where computations are performed using noise. In certain logic gates, the introduction of noise can lead to more accurate results under specific conditions.
Randomized Algorithms
: Some algorithms, especially in computer science, use randomization (which can be seen as a form of noise) to achieve better average-case performance or to simplify their design.
In all these cases, noise is not just an unwanted disturbance; it plays an integral role in the functioning or optimization of the system. However, the amount and type of noise that is beneficial is highly specific to the particular system or algorithm in question.
So I mean that these retrotransposons insert a kind of "noise" into neurogenesis and the reading of the genetic code, since there isn't just a program, but these so called junk DNA can create arbitrary functions not specified in the "blueprint" allowing for more variation which allows genetic algorthim that rely on selection to occur.
ChatGPT
Yes, your analogy is apt. Retrotransposons, by introducing "noise" or variability into the genomic landscape, can be thought of as introducing randomness or perturbations in the genetic code during neurogenesis. This randomness can potentially lead to greater diversity in neuronal populations.
Drawing a parallel to algorithms and systems:
Genetic Algorithms (GAs)
: In GAs, randomness is introduced during mutation and crossover operations to explore the solution space. This randomness ensures that the algorithm doesn't get stuck in local optima and can explore a wider range of potential solutions.
Similarly, the insertion of retrotransposons can be likened to the mutation operation in GAs. By introducing variability, retrotransposons can potentially lead to novel genetic configurations. Some of these configurations might be advantageous and provide a selective advantage, while others might be neutral or detrimental.
Evolutionary Advantage
: Just as randomness in algorithms can lead to better solutions or more robust systems, the genomic variability introduced by retrotransposons might confer certain evolutionary advantages. It could lead to a more adaptable neuronal population capable of better responding to environmental changes or challenges.
Selection
: In both GAs and natural evolution, selection plays a crucial role. Only those configurations (or organisms) that have a selective advantage survive and reproduce. In the context of neurogenesis, only those neuronal configurations that are functional and beneficial would likely persist, while detrimental configurations might be pruned or lead to non-functional neurons.
In summary, the introduction of "noise" or variability, whether in algorithms or biological systems, can lead to a richer exploration of solution spaces or genetic landscapes. When coupled with selection mechanisms, this variability can drive optimization and adaptation. Retrotransposons, by introducing variability into the genome, especially during neurogenesis, can potentially play a role in the evolutionary adaptability and diversity of neuronal populations.
I was looking for connections with this theory:

TS2 SPACE
Search our catalog
search
person
Sign in
shopping_cart
0
chevron_right
Posts
chevron_right
Aging: A New Theory of Selective Destruction
AGING: A NEW THEORY OF SELECTIVE DESTRUCTION
Aging: A New Theory of Selective Destruction
Researchers from Newcastle University, James Wordsworth and Daryl Shanley, have proposed a groundbreaking theory of aging called selective destruction theory (SDT). This theory challenges the prevailing belief that aging is solely a result of accumulated damage in organisms. Instead, the researchers argue that aging is a consequence of a process involving selective destruction of cells in multicellular organisms.

In the field of gerontology, the dominant theory has been the disposable soma theory, which suggests that organisms allocate fewer resources to somatic maintenance as they prioritize reproduction. As a result, somatic damage accumulates, leading to the aging process. However, recent discoveries have made it difficult to align all aspects of aging with the accumulation of damage theory.

The researchers offer SDT as an alternative explanation for aging that is not dependent on the build-up of damage. Their theory proposes that aging can occur through the selective destruction of cells, regardless of energetic costs. To demonstrate this process, agent-based modeling was used.

According to their model, slower cells induce epigenetic changes in faster cells, slowing down their metabolism without killing them. This not only reduces unnecessary cell death, but also prevents the spread of fast cells, thus decreasing the likelihood of overactivity disorders.
ChatGPT
The theory of selective destruction (SDT) you mentioned posits a different mechanism for aging than the commonly accepted disposable soma theory. The connection between retrotransposons and this theory could be conceptualized in the following way:
Cellular Variation and Noise
: Just as retrotransposons introduce genetic variability, the selective destruction theory suggests variability in cell metabolism rates. Some cells are "faster" while others are "slower". This inherent variability might be key to understanding how selective processes work.
Selective Pressure
: In the context of SDT, slower cells exert a kind of selective pressure on faster cells, inducing epigenetic changes to slow them down. Similarly, retrotransposons can be seen as exerting selective pressures on the genome. When they insert themselves into certain genes, they can alter gene function or expression, leading to selective pressures on those cells or individuals carrying those genes.
Regulation and Homeostasis
: The SDT model suggests that slowing down faster cells is beneficial for preventing overactivity disorders and reducing unnecessary cell death. In a way, this is a regulatory mechanism to maintain homeostasis. Similarly, retrotransposons, though often considered disruptive, might play roles in regulating gene expression or introducing variability that could be beneficial under certain conditions.
Beyond Accumulation of Damage
: Both SDT and the role of retrotransposons in neurogenesis suggest mechanisms of aging and cellular differentiation that go beyond mere accumulation of damage. While the disposable soma theory focuses on damage accumulation, both SDT and the potential role of retrotransposons in neurogenesis highlight that there are active and selective processes at play that contribute to aging and cellular diversity, respectively.
Modeling and Prediction
: The use of agent-based modeling in SDT to understand the interactions between cells and the effects of selective pressures could also be applied to study the effects of retrotransposons on neurogenesis. Modeling could provide insights into how retrotransposons influence neuronal differentiation and function.
In essence, while SDT focuses on aging and the retrotransposon theory on neurogenesis, both theories highlight the importance of variability, selective pressures, and active regulatory mechanisms in shaping biological processes. Drawing parallels between these theories can offer a more holistic understanding of cellular dynamics and the factors that influence them.
I also wanted to mention the importance of the interstitium in maintaining bodily integrity
ChatGPT
The interstitium is a recently recognized organ in the human body, and its understanding has evolved significantly in recent years. Comprising a fluid-filled space within and between tissues, the interstitium plays a crucial role in various physiological processes and in maintaining bodily integrity. Here's how:
Fluid and Nutrient Transport
: The interstitium acts as a conduit for the transport of fluids, nutrients, and waste products between cells and the bloodstream. This ensures that cells receive the nutrients they need and that waste products are efficiently removed.
Mechanical Support
: The interstitium provides structural support to tissues and organs. Its fluid-filled spaces act as shock absorbers, helping to protect tissues from damage due to mechanical stress or impact.
Immune Function
: The interstitium is involved in the body's immune response. It contains immune cells that can detect and respond to pathogens, ensuring rapid immune activation when needed.
Regulation of Fluid Balance
: The interstitium plays a role in maintaining the body's fluid balance. It helps regulate the movement of fluid between the bloodstream and cells, ensuring that tissues are adequately hydrated.
Role in Disease
: Changes in the interstitium can be indicative of various diseases. For instance, fibrosis or thickening of the interstitial spaces can occur in certain conditions. Additionally, the interstitium might play a role in the spread of cancer cells, making it a potential target for therapeutic interventions.
Thermal Regulation
: The fluid in the interstitium can help distribute heat, playing a role in maintaining the body's temperature.
Endocrine Function
: There's evidence to suggest that the interstitium may have endocrine functions, releasing signaling molecules that can affect distant organs.
Sensory Role
: The interstitium might play a role in sensing mechanical changes in the body, like stretching or pressure, and transmitting this information.
Recognizing the importance of the interstitium in maintaining bodily integrity has opened up new avenues for medical research. Understanding its functions and its role in health and disease can lead to better diagnostic tools and treatments for various conditions. The interstitium, with its multifunctional role, underscores the complexity and interconnectedness of the human body.
I think that really the brain is doing geometric reasoning on graphs, no matter what scale the computation or the mechanism
ChatGPT
The idea that the brain engages in geometric reasoning on graphs, regardless of the scale or mechanism, is a fascinating perspective that aligns with some of the latest theories in neuroscience and computational cognitive science. Here's a breakdown of this concept:
Graph Theory and Brain Networks
: The brain can be conceptualized as a complex network of interconnected nodes (neurons or brain regions) and edges (synaptic connections or pathways). Graph theory, a branch of mathematics, provides tools to analyze these networks, and it has been applied to study the brain's structural and functional connectivity.
Geometric Representations
: Geometric representations can be thought of as spatial maps in the brain, where information is organized in a structured manner. For instance, place cells in the hippocampus provide a geometric representation of an animal's environment. Similarly, grid cells offer a hexagonal grid representation of space.
Multi-Scale Computation
: The brain operates at multiple scales, from individual neurons to neural ensembles to entire brain regions. Geometric reasoning could be applied at each of these scales. For example, at a microscale, neurons might be involved in local computations, while at a macroscale, entire brain regions might be engaged in holistic spatial reasoning.
Topological Data Analysis
: This is an emerging field that applies topology (a branch of mathematics concerned with properties of space) to study complex datasets. In neuroscience, topological techniques can be used to study the brain's high-dimensional geometric structures and their functions.
Cognitive Processes
: Many cognitive processes can be understood in terms of geometric reasoning. For instance, navigation involves constructing and using a mental map of the environment. Memory, too, might involve organizing information in a structured, spatial manner. The idea is that our thoughts and memories are organized in a non-linear, interconnected manner, much like nodes and edges on a graph.
Learning and Plasticity
: As we learn and experience the world, the graphs or networks in our brains dynamically rewire. This plasticity can be seen as the brain updating its internal geometric representations in response to new information.
Neural Geometry
: Neurons and their dendritic trees have intricate geometric shapes. The spatial arrangement and geometry of these structures can influence signal processing and integration.
In essence, the brain's ability to process information, reason, and learn could very well be grounded in its ability to engage in geometric reasoning on complex graphs. Whether we're considering the intricate dance of molecules at synapses or the coordinated activity of entire brain regions during decision-making, the underlying principles might be deeply geometric and topological in nature.
2 / 2
Connections with the following:

Open AccessPerspective
There���s Plenty of Room Right Here: Biological Systems as Evolved, Overloaded, Multi-Scale Machines
by Joshua Bongard 1,���,���ORCID andMichael Levin 2,*,���,���ORCID
1
Department of Computer Science, University of Vermont, Burlington, VT 05405, USA
2
Allen Discovery Center at Tufts University, 200 Boston Ave., Suite 4600, Medford, MA 02155, USA
*
Author to whom correspondence should be addressed.
���
These authors contributed equally to this work.
���
Institute for Computer Designed Organisms.
Biomimetics 2023, 8(1), 110; https://doi.org/10.3390/biomimetics8010110
Received: 20 December 2022 / Revised: 24 February 2023 / Accepted: 1 March 2023 / Published: 8 March 2023

Abstract
The applicability of computational models to the biological world is an active topic of debate. We argue that a useful path forward results from abandoning hard boundaries between categories and adopting an observer-dependent, pragmatic view. Such a view dissolves the contingent dichotomies driven by human cognitive biases (e.g., a tendency to oversimplify) and prior technological limitations in favor of a more continuous view, necessitated by the study of evolution, developmental biology, and intelligent machines. Form and function are tightly entwined in nature, and in some cases, in robotics as well. Thus, efforts to re-shape living systems for biomedical or bioengineering purposes require prediction and control of their function at multiple scales. This is challenging for many reasons, one of which is that living systems perform multiple functions in the same place at the same time. We refer to this as ���polycomputing������the ability of the same substrate to simultaneously compute different things, and make those computational results available to different observers. This ability is an important way in which living things are a kind of computer, but not the familiar, linear, deterministic kind; rather, living things are computers in the broad sense of their computational materials, as reported in the rapidly growing physical computing literature. We argue that an observer-centered framework for the computations performed by evolved and designed systems will improve the understanding of mesoscale events, as it has already done at quantum and relativistic scales. To develop our understanding of how life performs polycomputing, and how it can be convinced to alter one or more of those functions, we can first create technologies that polycompute and learn how to alter their functions. Here, we review examples of biological and technological polycomputing, and develop the idea that the overloading of different functions on the same hardware is an important design principle that helps to understand and build both evolved and designed systems. Learning to hack existing polycomputing substrates, as well as to evolve and design new ones, will have massive impacts on regenerative medicine, robotics, and computer engineering.
Keywords: biology; computer science; robot; artificial life; artificial intelligence; machine learning; evolution
1. Introduction
In Feynman���s famous lecture titled ���There���s Plenty of Room at the Bottom��� [1], he argued that vast technological progress could be achieved by learning to manipulate matter and energy at ever-smaller scales. Such potential could presumably be exploited by natural selection as well. How does biology expand the adaptive function of an existing system? It cannot go down, since there is already something there, exhibiting functional competencies at every level [2]. Instead, it squeezes more action from each level by overloading mechanisms with multiple functions���which we term as polycomputing. We argue that the most effective lens for a wide range of natural and engineered systems must enable a multiple-observers view where the same set of events can be interpreted as different computations (Figure 1 illustrates how artists have recognized this feature). Indeed, depending on their definition of computation, some human observers may conclude that the observed system is not computing at all.
Biomimetics 08 00110 g001 550Figure 1. Polycomputing concepts in art. (A) Sculpture by Shigeo Fukuda, ���Lunch with a helmet on���, 1987���appears as a random pile of knives and forks but when observed in just the right way, light moving through the sculpture reveals another pattern (a motorcycle) present at the same time in the same structure. (B) A well-known bistable (ambiguous) image, ���My Wife and my Mother-in-Law��� by British cartoonist William Ely Hill in 1915, reveals how our nervous system is not suited to taking in multiple meanings���it prefers to squash down to a single interpretation, even if it then has to vacillate back and forth.
Herein, we review remarkable examples of biological polycomputing, such as spider webs that serve as auditory sensors and prey capture devices [3], and holographic memory storage in the brain [4,5]. We will also review emerging examples in computer and materials engineering [6]. We provisionally define polycomputing as the ability of a material to provide the results of more than one computation in the same place at the same time. To distinguish this from complex materials that necessarily produce complex results in the same place at the same time, such as the multiple peaks in the frequency spectrum of a vibrating material, polycomputing must be embodied in a material that has been evolved, or can be designed to produce particular results���such as the results of particular mathematical transformations like digital logic���and must be readable by other parts of the material or other devices. That is, the computation, to be considered a computation, must be useful to one or more observers (which, in biology, can exist on multiple scales, with multiple subsystems from the molecular to the whole organism, or swarm levels being able to reap the diverse evolutionary benefits of a single process if they interpret it as processing information that provides an adaptive advantage). These ideas, which describe new ways of understanding and exploiting polycomputing in biology, may suggest ways to improve synthetic polycomputing systems, which, in turn, will shed light on the nature of computation, evolution, and control. Biological systems that polycompute also contribute to an ongoing conceptual debate within interdisciplinary science���the applicability of computer frameworks and metaphors to living systems [7]���in three ways. First: if polycomputing changes our understanding of what computation is, that might change whether we consider a living system to be a computer (Section 1.1). Second: a living system (or inorganic material) may be considered to be polycomputing, depending on one���s point of view, suggesting that observer dependence is unavoidable when considering whether or what a living or engineered system computes (Section 1.2). Third: increasingly intricate admixtures of technological and biological components that compute are forcing a redefinition of life itself (Section 1.3).
1.1. What Constitutes a Computer?
The notion of a ���computer��� needs to be expanded: it no longer only refers to the sequential, deterministic, silicon-embodied, human-programmed, von Neumann/Turing architectures with which biologists are familiar. Those are indeed dissimilar to living systems. There is now a widening array of computational substrates and robots that are often massively parallel (such as GPUs and computational metamaterials [8]), stochastic (hard to predict) [9], able to exploit non-obvious (and potentially not-yet-understood) properties of the exotic substrates they are built from [10], emergent, produced by evolutionary techniques [11], and built by other machines [12] or programmed by other algorithms [13,14,15]. The benefit of considering biological systems as members of this broader class is that it avails powerful conceptual frameworks from computer science to be deployed in biology in a deep way, and therefore to understand life far beyond its current limited use in computational biology. Moreover, exploring this powerful invariant between natural and synthetic systems can enrich intervention techniques within biology and improve the capabilities of engineered devices, revealing gaps in our understanding and the capabilities of both computer science and biology. Polycomputing is a powerful but, as of yet, under-appreciated example of the many ways in which the wider class of computer devices can help to revolutionize the life sciences. In the same way that organic and inorganic materials acting as computers increasingly challenges the claim that living materials are not computers, we have argued elsewhere [16] that the widening array of materials that can now be viewed or engineered with as machines is corroding the classic claim that living systems are not machines, and forcing an improved definition of ���machine��� that escapes the narrow definitions of past decades, which are no longer appropriate [17,18,19].
1.2. Observer Dependency
In the statement ���living things are (or are not) computers���, ���are��� implies the existence of an objective, privileged view of both computers and biology that allows an unambiguous, universal decision as to whether they are related. This binary view is untenable and gives rise to numerous pseudo-problems. We argue instead for an observer-dependent view, in which computational formalisms are just metaphors; of course, all scientific concepts are just metaphors, with varying degrees of utility (which is not binary). Once we come to grips with the fact that ���all models are wrong but some are useful��� [20], it is possible to adopt a pragmatic approach [21] in which anything is a computer in a given context, to the degree to which it enables an observer to predict and control that thing better than any competing metaphors allow us to do. In this view, whether something is computing is not a philosophical question, but one to be settled experimentally by specifying a computational framework and showing empirically what new levels of capability, experiments, and research are enabled by adopting that framework. The only thing left is to enable system subcomponents, not just human scientists, to act as observers [22,23,24,25]. From that perspective, the quality of a computational metaphor in science is evidenced by its degree of productivity in new experimental capabilities, while the quality of a computational stance adopted by a biological subsystem is cashed out by the adaptive advantage that is evinced by it. Of course, it is expected that future progress will uncover even better frameworks, so the answer is never final, but always provisional and relative to a specific perspective. This view is akin both to the intentional stance in the philosophy of the mind [26], and in the driving of the development of frameworks and tools from cognitive science that can be broadly deployed across biology and the biomedical sciences [2,27,28].
1.3. What Things Are Alive?
Finally, the question of what constitutes a ���living thing��� is itself undergoing a renaissance due to the new chimeric, synthetic, and bioengineering techniques being developed [29]. Active matter, synthetic biology, and biohybrids [30,31,32,33,34,35,36] are blurring the line between evolved and designed systems, and dissolving the distinctions between ���life��� and ���machine��� [16,18,37], which were easy to maintain when our capabilities did not permit the construction and analysis of the full option space of agents [38,39]. At this point, the life sciences have expanded well beyond the N = 1 example of phylogenetic history here on Earth, to a clear mandate to understand life as it can be via synthetic and exobiological explorations [40,41,42,43,44,45,46].
1.4. From a Philosophy to a Science of How Life (Poly)Computes
We propose that the way to side-step philosophical debates about whether biological systems ���are��� computers is to adopt an observer-centered, scale-free view of the computational formalisms in biology. Polycomputing is an ideal example of a linking concept that will enrich both fields, which enables a number of fascinating questions with many fundamental and practical implications to be asked. What are the key functional and control properties of polycomputing systems? How does evolution create systems where multiple functions reside in the same hardware, and what does this design principle mean for evolvability? How can we derive intervention policies that make rational changes in existing polycomputing systems, and what are the efficient paths to the design of novel polycomputing materials, control algorithms, and device architectures?
Regardless of whether or not a living system is distally observed, it still polycomputes, because life itself adopts the same operator-dependent approach. In other words, a biological mechanism polycomputes because its functionality and signaling are interpreted in different ways by other components of that same living system. Each level and component of a living system are simultaneously observers and hackers, interpreting and taking advantage of different aspects of the mechanisms in their microenvironments, in parallel. Life polycomputes because it is a set of overlapping, competing, cooperating nested dolls, each of which is doing the best it can to predict and exploit its microenvironment [47,48,49,50,51,52,53].
1 / 2
ChatGPT
The article you've shared by Joshua Bongard and Michael Levin delves into the concept of biological systems as evolved, overloaded, multi-scale machines that engage in "polycomputing." Here's how the concepts presented in the article connect with the topics we've discussed previously:
Polycomputing and Retrotransposons
: The idea of polycomputing suggests that biological substrates can perform multiple computations simultaneously in the same location. This mirrors the behavior of retrotransposons, which introduce variability into the genome. This variability can be thought of as adding multiple layers of computation (or information processing) to the genetic code, allowing for diverse outcomes in neurogenesis and other processes.
Observer-Centered View and Geometric Reasoning
: The article highlights the importance of an observer-centered view of computation in biology. Similarly, the idea that the brain engages in geometric reasoning on graphs can be seen as a specific instance of this observer-centered view. Different parts of the brain, or different observers, might interpret the same neural activity or network topology in various ways, leading to diverse computational outcomes.
Biological Systems as Overloaded Machines
: The concept of biological systems as overloaded machines ties in with the idea of the interstitium maintaining bodily integrity. Both ideas suggest that biological systems are multifunctional and operate at multiple scales. The interstitium, for instance, performs various functions from fluid balance to immune response, akin to the concept of polycomputing where multiple computations occur simultaneously.
Mesoscale Events
: The article emphasizes the importance of understanding mesoscale events. This aligns with the idea that the brain's operations, whether through geometric reasoning or other processes, occur at multiple scales���from individual neurons to entire neural networks.
Biomimetics and Engineering
: The article touches upon the importance of understanding biological systems to improve synthetic systems and vice versa. This bi-directional relationship between biology and engineering can be seen in the study of the brain as well. Insights from computational models can inform our understanding of neural processes, and conversely, the brain's operations can inspire novel computational techniques.
Evolution and Design
: The article discusses the blurring line between evolved and designed systems. Similarly, when looking at the brain's operations, it's evident that both evolutionary processes (like the action of retrotransposons) and design principles (like geometric reasoning on graphs) play roles in shaping its functionality.
Computational Frameworks in Biology
: The article's emphasis on applying computational frameworks to biology resonates with the idea that the brain, at its core, is a computational machine. Whether through geometric reasoning, neural network processing, or other mechanisms, the brain's operations can be understood using computational metaphors and models.
In summary, the concepts presented in the article by Bongard and Levin provide a broader framework for understanding the multifunctionality and multi-scale operations of biological systems, including the brain. The ideas of polycomputing, observer-centered views, and the interplay between evolution and design offer fresh perspectives on how we conceptualize and study complex biological processes.
1.5. Why ���Life as Computation��� Matters

The transfer of knowledge between the disciplines of biology and computation forms a positive feedback loop for increasing the insight within both. Biological examples help to widen the range of implementations for computing devices and provide novel ideas for architectures [54,55,56,57,58]; unconventional computing platforms include fungal networks, ant colonies, and DNA. In complement, computer science and its idea of functionalist substrate independence (multiple realizability) helps biologists to focus on essential, rather than contingent, design principles, expanding biology beyond zoology and botany. This has been most prevalent in neuroscience [59,60,61], but more recently has been extended far beyond it, in recognition of the fact that neural dynamics are only an extension of far older biological problem solving architectures [28,62,63,64].

A key conceptual insight from computer science that informs biology concerns the nature of computation. For example, the field of physical reservoir computing [65], in which a neural network is trained to map the dynamics occurring within an inorganic, biological, or technological system (the ���reservoir���) into an output desired by a human observer, helps us to see the observer-dependent aspect of biology. This offers ways to think about biology as nested societies of elements which are exploiting the information-processing capacities [66] of their living environment. Cells, parasites, conspecifics, commensal organisms, and evolution itself are all hackers in the sense of using their parts and neighbors as affordances in whatever way they can, rather than in some single, unique, privileged, and objective way that reflects ���true��� functionality.

The concepts of superposition in quantum mechanics and the primacy of observer frames in relativity have transformed the understanding of this phenomena on very small and very large scales, respectively. Polycomputing challenges us to apply the same concepts to computation and life at mesoscales. Here, we overview the concepts of superposition and observer frames as they are applied to mesoscales and argue that the polycomputing lens, like the agential matter lens [67,68], helps us to understand, predict, and control new classes of evolved and designed materials, with numerous applications ranging from regenerative medicine to engineering.

2. Current Debates: Dissolving Dichotomous Thinking

Whenever technological progress within a particular domain begins to slow, researchers often look to nature for fresh inspiration. Examples of this include the use of photosynthesis for new energy capture devices [69] and flapping wings for new drone designs [70]. Following this tradition, the increasing difficulty of packing more computing ability into microchips [71] challenges us to seek new paths forward by considering how computation is embedded within living systems. Comparing how organisms and machines compute requires one to view an organism as a kind of machine; otherwise, no comparison is possible. The debate about how or whether organisms are machines has a long history, and has become more intense in recent years [16,17,18,37,62,72,73], as various disciplines not only compare life to machines, but attempt to merge the two (reviewed in [38]).

Our usage of the term ���machine��� in what follows will denote a subset of machines that are capable of computation. Such machines include robots and physical computers but exclude simple mechanical devices such as combustion engines and flywheels, for which no way to stimulate or exploit them to produce computation has yet been invented (if such interventions are discovered, these technologies can then be considered as belonging more to the class of computational machines). In the spirit of our thesis, we acknowledge that there is no clear dividing line between these two ���types��� of machines, as circuitry-free machines such as mechanical computers, physical reservoir computers, [65] and computational metamaterials [8] can still compute. As always, there is a continuum: in this case, it is across machines capable of more or less computation. A possible exception may exist for machines that compute by exploiting quantum effects, although even there the notion of an observer plays a key role in deriving binary outcomes from a fundamentally indeterminate reality. The usage of the term ���machine��� rather than ���computer��� in what follows is meant to remind the reader that we are considering organisms vis-a-vis human-made things that compute, rather than just comparing them to traditional computers
ChatGPT
The extended excerpt you've shared delves deeper into the concept of "Life as Computation" and its implications for understanding biological systems and advancing technological development. Here's a summary and how it connects to previous discussions:
Life as Computation - Mutual Benefits
: The symbiotic relationship between biology and computation leads to advancements in both fields. Biological systems offer inspiration for unconventional computational models and architectures, while computational principles provide biologists with tools and frameworks to understand biological processes more deeply.
Nature of Computation
: Physical reservoir computing is highlighted as an example of how computational models can be applied to various systems, emphasizing the observer-dependent nature of biology. This viewpoint aligns with the idea that biological entities, from cells to entire ecosystems, exploit their environments' information-processing capacities. They interact with and interpret their surroundings based on their specific needs and perspectives, much like "hackers."
Superposition and Observer Frames
: The article draws parallels between the principles of superposition in quantum mechanics and observer frames in relativity, suggesting that these concepts can be applied to understanding life and computation at mesoscales. The idea is that biological systems can be in multiple states or perform multiple functions simultaneously, akin to the superposition principle in quantum mechanics.
Dissolving Dichotomous Thinking
: The article emphasizes the need to move away from binary or dichotomous thinking. Just as the boundary between organisms and machines is blurring, so is the distinction between different types of machines. The term "machine" is used in a broad sense to include any entity capable of computation, whether it's a traditional computer, a robot, or a biological system.
Challenge of Technological Progress
: As advancements in traditional computing (like microchip development) slow down, there's a push to look towards nature for inspiration. Biological systems, with their inherent complexity and multi-functionality, offer a rich source of ideas for new computational models and techniques.
Organisms as Machines
: The debate around viewing organisms as machines is highlighted. While there's a long-standing discussion about this comparison, the increasing convergence of biology and technology (e.g., biohybrids) has intensified this debate. The article posits that organisms can be viewed as a subset of machines capable of computation.
The overarching theme is that life, in all its complexity, can be understood through the lens of computation. This perspective not only offers insights into the inner workings of biological systems but also paves the way for innovations in technology, robotics, and medicine. The mutual benefits of integrating biology and computation underscore the potential of interdisciplinary research in shaping the future of both fields.
2.1. Structure Function Mapping and Polycomputing

An obvious starting point for the comparison between organisms and computers, or organisms and machines, is to assume a 1-to-1 mapping between the structure and function. A comparison can then be attempted between the organism���s and machine���s structures, and then between their functions. Finally, one can compare the structure-to-function mappings of the organisms and machines. However, teasing apart the structure and function for such comparisons is difficult. Genetics [74] and neuroscience [75] can both provide historical examples of how 1-to-1 structure/function mappings were rapidly replaced by models with increasingly dense and non-intuitive interactions between their structural and functional units. Even harder than making predictions based on this nontrivial structure-to-function mapping is inferring which interventions to make for rational changes at the system level, as is needed in the example of regenerative medicine���replacing complex organs such as hands and eyes [27,28]. Advances in other areas where biology and computer science meet are similarly demolishing these long held dichotomies (Table 1).

Indeed, an understanding of this wide range of implementations (materials, including organic components) and origin stories (e.g., evolutionary design techniques [76]) for machines makes it clear that, in many cases, a modern machine lens for life facilitates progress. The machine metaphor is a functional approach that seeks to develop possible efficient ways to predict, control, communicate with, and relate to a system and its reliable behavior modes. However, one aspect has lagged in both engineering and biology. It is relatively easy to see that technological or living components can support different functions at the same time but at different spatial scales: myosin, for example, supports muscle fiber contraction and legged locomotion simultaneously. It is also easy to see how components can support different functions on the same spatial scale but at different times: myosin can support legged locomotion and then tree climbing. However, it can be difficult to see how a component can provide multiple uses for multiple beneficiaries (or compute different functions from the vantage point of different observers) on the same spatial scale and at the same time. Investigating this last phenomenon���polycomputing���enables not only a new set of questions for biology, but also a quest for engineers to understand how to pack more functionality into the same machine.

Table 1. Some common assumed distinctions in biology and technology, and recent advances that serve as counterexamples, suggesting a spectrum of complementarity.

���

2.2. Dichotomous Thinking in the Life Sciences

Biology does not really support dichotomous categories. While it is sometimes convenient for biologists to adopt discrete criteria for specific characters, evolution and developmental biology both exhibit remarkable examples of scale invariance and gradual change. Neither process supports any kind of clean bright line that separates the cognitive human being from the ���just physics��� of a quiescent oocyte or the ���true grounded knowledge��� from the statistically driven speech behavior of babies and some AIs, etc. (Table 1). All of these, like the process of slowly changing a being from a caterpillar to a butterfly [47], show that familiar categories in fact represent the poles of a spectrum of highly diverse mixed properties. The interoperability of life [47,83,84,85] enables chimeras at all levels of an organization, which provide a continuum of every possible combination of features from supposedly distinct categories (Table 1), making it impossible to objectively classify either natural or artificial chimeras [29,38]. It is becoming increasingly apparent that the departmental, funding, and publication distinctions between disciplines (e.g., neuroscience and cell biology, are much more of a practical consequence of our cognitive and logistical limitations than the reflection of a deep underlying distinction. In fact, these divisions obscure important invariants: the symmetries across categories that enable unifications, such as the use of cognitive neuroscience techniques to understand the collective intelligence of cells during morphogenesis [27,28,86,87], or indeed of physics itself [24,88].

2.3. Dichotomous Thinking in Computer Science

Advances in the computational sciences also increasingly disrespect human-devised categorical boundaries. One such boundary under attack is that between the body and brain. One set of technologies that is eating away at this distinction is physical computing; a conceptual advance doing similarly caustic work is that of morphological computation. In mechanical computing, computation is performed without recourse to electronics and instead relies on optical [89], mechanical [90], or quantum [91] phenomena. Recent advances in mechanical computing show how inert bulk materials can be induced to perform non-trivial calculations, including error backpropagation, the algorithmic cornerstone of modern AI [77]. A recent demonstration by one of the authors (Bongard), showing that propagation of acoustic waves through granular metamaterials can be exploited to perform multiple Boolean operations in the same place at the same time [8], can be considered to be the first example of mechanical polycomputing. Mechanical computing, and now, mechanical polycomputing, challenge the assumption that, in organisms, there needs to be one subsystem that computes and controls (such as the nervous system) and another that accepts that control (the body).

Morphological computation, a concept originating in the robotics literature, upholds that the body of an animal or robot can indeed compute, and, moreover, it can ���take over��� some of the computation performed by a nervous system or robot control policy [92,93]. Although mechanical computing and morphological computing are similar in spirit, in mechanical computing, the bulk material passively accepts whatever computation is forced upon it. In contrast, in morphological computation, the animal or robot may adaptively perform computation either neurally or mechanically, depending on the current demands of its environment. This flow of computation back and forth between the body and brain (or between the digital circuitry and bulk materials) suggests that the two human-devised categories of the ���body��� and the ���brain��� should not be as distinct as once thought.

2.4. Polycomputing in Bodies and Brains

If polycomputing is to be considered a kind of computation, one can then ask whether polycomputation can be shuttled back and forth between biological bodies and brains, or if it can be made to do so between machine bodies and brains. For this to work, polycomputation must be implementable in different kinds of substrates. Traditional computation is assumed to be substrate agnostic: if it is configured appropriately, any physical material can compute. In contrast, only vibrational materials have been shown capable of polycomputing to date, as polycomputation requires the storage of the results of multiple computations at the same place and at the same time, but at different peaks in the frequency spectrum (non-vibrational materials may also be capable of polycomputing: materials with vectorial modes could store multiple results in different vector elements, or multimodal systems could store results in different modes). This focus on vibration would seem to preclude some materials, such as digital circuitry and biological nervous systems, from performing polycomputation, since digital circuitry traffics in electrons, and nervous systems traffic in chemicals and ions, while neither seem to traffic in vibration. At first glance, this seems poised to rescue the brain/body distinction via the surprising route of suggesting that bodies and brains are different things, because bodies polycompute, but brains do not.

However, this odd-seeming distinction may be short-lived. It has been shown that neurons may communicate mechanically [94] in addition to electrically and chemically. If so, such mechanical neural communication may contain vibrational components, suggesting that nervous systems may be polycomputing as well. If this turns out to be the case, it, in turn, opens up the possibility that nervous tissues may have evolved incremental enrichments of the non-neural cells��� already-proven ability to polycompute. This would once again frustrate our attempts to cleave the body from the brain, in this case by the claim that one polycomputes, while the other does not.

Mechanical computing and morphological computation are closely related to another way in which computer science provides useful viewpoints for biology. In computer science, the view that an algorithm drives (functionally determines) outcomes, even though it is implemented by the microphysics of the electron flows through a CPU, is accepted and indeed essential to performing the useful and powerful activity of programming. This is in stark contrast to the debates within biology and neuroscience about whether higher levels of description are merely epiphenomenal [95,96,97,98,99], supervening on biochemical microstates (reductionism). Computer science clearly shows how taking a causal stance at higher levels enables progress. Indeed, the recent advances in information theories around quantifying causal emergence [99,100] show how the same Boolean network can compute different functions simultaneously, depending on the level of analysis chosen by an observer [99]. This has interesting biological implications, since such networks are a popular model for understanding the functional relationships between genes [101,102,103].

Biological nervous systems���the human brain in particular���have attracted increasingly computational metaphors throughout the industrial revolution and information age. The application of computational concepts to brains has had unintended consequences, most of all being the implicit assumption that tissues, cells, and other biological systems that are not brains do not compute. However, the brain���body dichotomy is being increasingly dismantled by studies of basal cognition (i.e., intelligence in unfamiliar embodiments) in plants [104,105], single somatic cells [106,107,108], microbes [109,110,111,112], and at the tissue level in organisms [113,114,115,116]. Indeed, the bioelectric and neurotransmitter dynamics that implement predictive processing and other computations in brains are speed-optimized versions of the extremely ancient bioelectrical computations that navigated spaces (such as anatomical morphospace and physiological space, etc.) long before brains and muscles appeared [64,117,118]. Indeed, the tools of neuroscience���from conceptual approaches such as active inference [28,87] to molecular tools such as optogenetics [119,120,121,122]���do not distinguish between neurons and non-neural contexts, being broadly applicable across biology.

The benefit of dissolving these arbitrary distinctions is that commonalities and fundamental design principles across substrates are beginning to emerge across evolved and designed constructs on all scales [38,107,123]. Frameworks that are to survive the next decades, in which technological advancement will further enmesh biology and technology, must facilitate experimental progress at the expense of philosophical preconceptions. More than that, they must provide a unifying insight by identifying the symmetry and deep order across the fields to combat the ever-growing problems of big data and the interpretability crisis [124,125]. Here, we delve into one emerging principle: polycomputing, which places front and center the fascinating issues of form, function, control, interpretation, and the role of the observer.

3. Learning from Superposed Systems in Engineering

The ability to compute and store multiple results in the same locality at the same time is of obvious engineering interest, as it could greatly increase computational density. Various technologies are now being built that roll back the assumption that such superposition is impossible. These technologies, reviewed below, suggest ways of looking for similar phenomena in natural systems.

Quantum computing has made it clear that multiple computations can be performed simultaneously. However, practical and general-purpose quantum computing remain a distant prospect. Recently, one of the authors (Bongard) showed that quantum effects are not necessary for polycomputing [8]: even relatively simple materials that are composed of only 30 parts are capable of computing two Boolean logical functions (AND and XOR) in the same place at the same time. This non-quantum form of computational superposition suggests not only that more computation may be packed into smaller spaces, but also that the fundamental way in which computation arises in technological and biological materials may need to be rethought.

Holographic data storage (HDS; [126]) is another set of related technologies that do not assume that only one datum or computational result is stored locally. HDS stores and reads data that have been dispersed across the storage medium. It does so by etching a distributed representation of a datum across the storage medium, for example with laser light, from a particular direction. That datum can then be retrieved by capturing the reflection of light cast from the same direction. By storing data in this way, from multiple directions, parts of multiple pieces of data are stored in the same place, but accessed at different times. Exactly how this can be achieved in hardware, such that it affords appreciable increases in the storage density more than current traditional approaches, has yet to be resolved.

A third technology relaxing the assumption of the data/compute locality is physical reservoir computing (PRC). PRC, inspired by HDS, attempts to retrieve the results of the desired computations by exciting inert bulk materials, such as metal plates or photonic crystals, and capturing the resulting mechanical vibrations or refracted light, respectively. Different computations can be extracted from the same material by exciting it in different ways. An attempt to ���program��� PRCs, thus easing the ability to extract the desired computation from them, has also been reported [127]. Notably, this method has been used to create ���deep physical neural networks��� [77]: the input, and the parameters describing an artificial neural network, are combined into forces that are supplied to the material. The forces captured back from the material are interpreted as if the input had been passed through a neural network with those parameters. The errors in the output can then be used to modulate the input, and the process repeats until a set of input forces has been found that produces the desired output. Importantly, the internal structure of the bulk material is not changed during this training process. This means that the same material can embody different computations. Just how distributed or localized these computations are within these materials remains to be seen.

Other materials are not only changed by the forces acting on them, but retain an imprint of those forces even after they cease: they are capable of memory. Efforts are also underway to design such materials to maximize the number of overlapping memories that they can store [128,129]. The ability of the designed materials to absorb the forces, compute with them, and output the transformed forces that encode the results of those computations, holds great promise for robotics. If future robots can be built from such materials, the force propagation within them could simultaneously produce external behavior and internal cogitation, without requiring distinct behavior-generating components (the body) and computation-generating components (the brain). Indeed, soft robots are already demonstrating how exotic materials enable the traditionally distinct functions of sensation, actuation, computation, power storage, and power generation to be performed simultaneously by the same parts of the robot���s body [130].

4. Biology Is Massively Overloaded: Polycomputing

Analyzing natural systems to determine whether or how they perform polycomputation is particularly challenging, as most analytic approaches are reductionist: they ���reduce��� to characterizing one phenomenon that arises in one place, at one time, under one set of circumstances. Synthesis is also difficult: polycomputable technologies seem, to date, resistant to the traditional engineering design principles such as hierarchy and modularity. A fundamental problem is that typical human designs are highly constrained, such that any changes made to optimize one function often interfere with another. Although humans struggle to manually design polycomputing technologies, it turns out that AI methods can do so, at least in one domain. We have recently applied an evolutionary algorithm���a type of AI search method���to automatically design a granular metamaterial that polycomputes. It does so by combining vibrations at different frequencies at its inputs, and providing different computations in the same place, at the same time, at different frequencies. Figure 2 illustrates this process.

���

Figure 2. Engineering polycomputing materials. (A) A granular metamaterial can be assembled by embedding stiff (dark gray) and soft (light gray) particles into a sheet and wrapping it into a tube. If a particle collides with soft particles, it only slightly affects their motion. (B) If it hits rigid particles, their motion is affected more. (C) An evolutionary algorithm (EA) can be created that evolves populations of metamaterials, where each one has a unique combination of stiff and soft particles. The EA can then delete those metamaterials that perform poorly at some desired task, such as performing a computation, and make randomly modified copies of those that do a better job. (D) This can result in the evolution of a material that acts as an AND gate, a building block of computers: some designated ���output��� particle (red) should only vibrate if two other ���input��� particles are vibrated from outside the system (green and blue). (E) An evolutionary algorithm can be asked to evolve a metamaterial that acts as an AND at one frequency, but also to act as another computational building block, an XOR gate, at a higher frequency: the output particle should only vibrate if one of the input particles is vibrated. (F) This process results in the evolution of a polycomputing material: if inputs are supplied at two different frequencies, the evolved material acts as an AND and XOR gate simultaneously: it provides the results of these two computations at the same place at the same time (the output particle), but at different frequencies. Details can be found in [8].; panels (D���F) used with permission from ACM.

Many biological functions have been usefully analyzed as computations [62] (Table 2). These include molecular pathways [128,131], individual protein molecules [63], cytoskeletal elements [129,132,133], calcium signaling [134], and many others. Although differing from the most familiar, traditional algorithms, the massively parallel, stochastic (indeterministic), evolutionarily shaped information processing of life is well within the broad umbrella of the computations familiar to workers within the information sciences. Indeed, information science tools have been used to understand cell- and tissue-level decision making, including estimations of uncertainty [108,135,136,137,138,139,140,141,142,143,144,145], analog/digital dynamics [146], and distributed computations [104]. Bioelectric networks within non-neural tissues, just like their neural counterparts, have shown properties that are very amenable to polycomputation, including the ability to store diverse pattern memories that help to execute morphogenesis on multiple scales simultaneously [28,30,116,147,148,149,150,151,152,153], and enable the same genome to produce multiple diverse outcomes [154].

A key aspect to recognizing unconventional computing in biology is that the notion of ���what is this system really computing��� has to be dropped (because of the multiple observers issue described above; see also [155] for a discussion of the role of the observer in unconventional computing). Once we do this, biology is rife with polycomputing on all scales. An example of this includes the storage of a (very large) number of memories in the same neuronal real-estate of the brain [156,157], and many others are summarized in Table 2. We do not yet know whether the prevalence of polycomputing is because of its efficiency, robustness, or other gains that override the evolutionary difficulty of finding such solutions. Or, perhaps we overestimate this difficulty, and evolution has no problem in identifying such solutions���they may indeed be the default. If so, it may be because of the generative, problem solving nature of developmental physiology that is the layer between the genotype and the phenotype [38,158]. For these reasons, polycomputing may join degeneracy and redundancy [159], as well as the minimization of stress and frustration [2,160,161,162,163], as one of the organizing principles that underlies the open-ended, robust nature of living systems.

Table 2. Examples of biological polycomputing at diverse scales.
ChatGPT
The provided text delves deeper into the concept of polycomputing in both biological and technological contexts. Here's a summary:
2.1. Structure Function Mapping and Polycomputing
While it's tempting to map the structure and function of organisms and machines on a 1-to-1 basis, this approach has been shown to be limiting.
Traditional models that aimed for a simple structure-function mapping have been replaced with more complex models, as observed in fields like genetics and neuroscience.
Polycomputing, the simultaneous execution of multiple functions by a single component, is a promising avenue for exploration in both biology and engineering.
2.2. Dichotomous Thinking in the Life Sciences
Biological evolution and development don't conform to strict dichotomies. Instead, there are spectrums and continuums of properties.
The divisions we see in scientific disciplines might be more a result of human cognitive and logistical limitations rather than genuine distinctions in the natural world.
2.3. Dichotomous Thinking in Computer Science
Advances in computer science blur the lines between conventional categories, such as body and brain.
Technologies like mechanical computing and concepts like morphological computation challenge traditional ideas about where and how computation takes place.
The idea that certain components of a system are solely for computation while others merely execute commands is becoming outdated.
2.4. Polycomputing in Bodies and Brains
Polycomputing must be implementable in various substrates. While it's known that vibrations can lead to polycomputing, it's yet to be seen if biological nervous systems, which primarily rely on chemicals and ions, can also engage in polycomputing.
Computer science offers a perspective where higher-level algorithms determine outcomes, a concept that could be beneficial for biology.
Many biological functions can be understood as computations, and recognizing the polycomputational nature of these systems can lead to new insights.
3. Learning from Superposed Systems in Engineering
Technologies like quantum computing, holographic data storage, and physical reservoir computing are challenging traditional ideas about computation and data storage.
Such technologies hold potential not just for increasing computational density but also for rethinking the fundamental nature of computation.
4. Biology Is Massively Overloaded: Polycomputing
Many biological systems can be understood as performing computations. From molecular pathways to bioelectric networks, these systems operate in ways that can be analyzed using computational models.
Polycomputing might emerge as a key organizing principle in understanding biological systems, much like other concepts such as redundancy or the minimization of stress.
The overarching theme is that traditional boundaries and dichotomies, whether in biology or computer science, are being challenged by the emerging understanding of polycomputing. This concept, which describes the ability of a system or component to perform multiple computations simultaneously, has profound implications for how we understand and design biological and technological systems. The integration of these ideas could lead to revolutionary advancements in both fields.
4.1. Evolutionary Pivots: Origins of Polycomputing?

Evolution is remarkably good at finding new uses for existing hardware due to its fundamental ability to generate novelty, in order to exploit new niches while being conservative in terms of building upon what already exists. This ability to simultaneously innovate and conserve plays out across structural, regulatory, and computational domains. Moreover, the re-use of the same conserved mechanisms in situ has enabled evolution to pivot successful algorithms (policies) from solving problems in metabolic spaces to solving them in physiological, transcriptional, and anatomical (morphospace) spaces, and finally, once muscles and nerves arrived on the scene, to 3D behavioral spaces [22,28,64,207]. For example, [64], the same ion channels that are used for the physiological control of cell homeostasis and metabolism are used simultaneously in large-scale bioelectric circuits that compute the direction of the adaptive changes in growth, and form in embryogenesis, metamorphosis, regeneration, and cancer suppression [64,112,117,208,209,210,211,212]. Indeed, in some animals such as planaria and axolotl, this all happens at the same time, as these exact same mechanisms in neural cells are guiding behavior [47]. Biology extensively uses polycomputing because it uses a multi-scale competency architecture, where every level of the organization is competent in solving certain problems within its own space. It is doing so at the same time via the same physical medium that is interpreted by observers on different scales, which exploits the results of those computations [2,22] (Figure 3).

���

Figure 3. Polycomputing architectures in biology. (A) 3D computer rendering of a eukaryotic cell modeled using X-ray, nuclear magnetic resonance (NMR), and cryo-electron microscopy datasets for all its molecular actors. The image was created by Evan Ingersoll and Ga��l McGill (Digizyme Inc.) and illustrates the pressure on biology to have each component perform multiple duties (there is not much room to add additional components); this image shows a dilute cytoplasm relative to a real cell). Used with permission. (B) Multi-scale competency architecture of life consists of molecular networks which make up cells, which make up tissues, which make up organs, which make up organisms within swarms. Each layer is performing specific functions simultaneously; for example, the tissue layer is attempting to compute the correct attractor for the collective morphogenetic behavior of planarian fragment cells, which can build one of several head shapes). Each layer deforms the action landscape for the layer below it, providing incentives and shaping geodesics that force the lower-level components to use their behaviors in service of the higher level���s goals. Taken with permission from [22]. Images in panel B by Jeremy Guay of Peregrine Creative Inc. and Alexis Pietak.

Polycomputing is seen even at the lowest scale of molecular biological information. It has long been known that genomes are massively overloaded, providing polycomputing not only because of the multiple reading frames (overlapping genes) for some loci [181,182], but also because the question of ���what is this gene for?��� may have a clear answer at the molecular scale of a protein, but often has no unique answer at the phenotypic scale, because complex traits are implemented by many genes, and many (or most [74]) genes contribute to multiple mesoscale capabilities. Moreover, epigenetics enables the same genomic information to facilitate the embodied computation that results in multiple different anatomical, physiological, and behavioral forms [213,214].

4.2. Polycomputing and the Range of Phenotypic Possibility

How much actionable information on how to assemble an adaptive, behaving organism can be packed into the same genomic and physiological information medium? A recent example of massive phenotypic plasticity are Xenobots���proto-organisms that result from a rebooting of multicellularity with frog skin cells [79,215]. The Xenobots self-assemble as spheroids that are self-motile, exhibiting a range of autonomous behaviors, including highly novel ones such as kinematic self-replication: the ability, of which von Neumann famously dreamed, to assemble copies of themselves from material found in their environment [78]. In the case of Xenobots, this material is the dissociated cells that are introduced into their surroundings. A key point is that Xenobots are not genetically modified, and their novel functionality is implemented by perfectly standard frog cells. So, what did evolution learn [216,217,218,219,220] in crafting the Xenopus laevis genome and the frog eggs��� cytoplasmic complements? It was not just how to make a frog, it was how to make a system in which cells allow themselves to be coerced (by other cells) into a boring, two-dimensional life on the animal���s outer skin surface, or, when on their own, to assemble into a basal three-dimensional creature that autonomously explores its environment and has many other capabilities (Figure 4).

This capacity to do things that were not specifically selected for [221] and do not exist elsewhere in their (or others���) phylogenetic history reveals that evolution can not only create seeds for cellular machines that do multiple things, but for ones that can do novel things. This is because genomic information is overloaded by physiological interpretation machinery (internal observer modules): the exact same DNA sequence can be used by cells to build a tadpole or a Xenobot (and the same planarian genome can build the heads of several different species [222,223]). Thus, evolution teaches us about powerful polycomputing strategies because it does not make solutions for specific problems���it creates generic problem solving machines, in which the competition and cooperation of overlapping, nested computational agents at all levels exploit the ability of existing hardware to carry out numerous functions simultaneously. This is closely tied to recent advances at the interface of the fields of developmental biology and primitive cognition, with the generation of models in which larger-scale Selves (in psychological and anatomical spaces, etc.) arise as composite systems made of smaller Selves, all of which are pursuing diverse agendas [47,48,224].

As surprising as these examples are, we should have already seen this coming���biology has to work like that, and it could not work otherwise. First, the ���sim-to-real gap��� (the difference an agent experiences when it is trained in virtual environments, built as a robot, and deployed into a real environment [225,226]) is as real for biology as it is for robotics: prior evolutionary experience in a past environment is not a reliable guide to the novel challenges that each generation experiences in new environments. Thus, evolution does not overtrain on prior examples, but generalizes, producing substrates that can compute different functions for different needs (Figure 5). Second, the evolutionary process is not working with a blank slate���the canvas of embryogenesis is made up of cells which used to be independent organisms. Evolution exploits the competencies of cells in numerous problem spaces as a toolkit of affordances to be exploited. Development is akin to behavior-shaping, where evolution finds signals that cells can send to other cells to push them into specific actions. This is a strong start for polycomputing as a design principle���working with an agential material [67] requires strategies that do not establish a single, privileged, new way of doing things, but instead drive adaptive outcomes by incentivizing subsystems to manage and exploit the many things that the material is already doing. This perspective is congruent with a ���process metaphysics��� [227,228]. Third, evolution simply would not work well with an architecture that did not support polycomputing, because each new evolutionary experiment would wreck the prior gains, even if it itself was an advance. Developing a new perspective on a set of events which provides a useful computational function enables subsystems to derive an adaptive advantage without having to change the events in question (thus not risking breaking something that other subsystems depend on). For example, we have shown that useful computational functions, such as associative memory, can be derived from a gene-regulatory network without changing the structure of the network (and thus without any possibility of adversely affecting any dependents), simply by a mechanism that interprets its outcomes in a particular way (by mapping specific nodes to the functional elements in an associative conditioning paradigm) [169,171]. This is readily evolved, and provides a way for evolution to squeeze additional benefits from the existing components without needing to change them in any way���all the work is done on the observer���s end, who also reaps the benefits without any negative consequences for the other internal observers (subsystems).

���

Figure 4. What does the Xenopus laevis genome specify? (A) The standard Xenopus laevis genome (in a frog egg) typically causes the construction of a set of standard embryonic stages (Path A) which results in tadpoles with specific behaviors. However, in a different context, the skin cells can autonomously create a Xenobot (Path B)���a spherical construct with autonomous motion, a different morphogenetic sequence, and behaviors such as kinematic self-replication. The same genomic information contains simultaneously seeds of emergent tadpoles or Xenobots. (B) Similar to the iconic cover image of the classic book Godel, Escher Bach [229] this image illustrates how the same hardware (the standard frog egg in the middle) can be used to generate diverse forms of living constructs. Different environments, external signals, and physiological events can coax diverse morphogenetic outcomes out of a constant biological information string (DNA). Images in (A) courtesy of Xenbase and Douglas Blackiston, Levin lab. Image in (B) by Jeremy Guay of Peregrine Creative.

���

Figure 5. Morphogenesis plays the hand it is dealt. The essence of biological morphogenesis is that it does not assume much about current circumstances and attempts to create a viable organism with whatever is at hand [159]. Thus, frog embryos in which eye primordia cells are moved from the head to the tail still make good eyes (A), try to connect to the spinal cord (A���, red stain), and enable the organism to exhibit behavioral vision (A���), despite a completely novel visual system���brain architecture which had no evolutionary prep time to get used to the new arrangement���nothing needed to be changed (at the DNA level) to make this new configuration workable. Similarly, tadpoles (B) which must rearrange their face to turn into a frog (B���) can still do so even if everything is artificially placed in a scrambled configuration (B���), because each organ is able to move as needed to get its job done (reach a specific region of morphospace). Finally, the cross-level nature of this overloading of basic mechanisms is seen in newt kidney tubules schematized here in cross-section. While they normally consist of 8���10 cells that communicate to make a tubule (C), the cells can be made experimentally very large���in that case, fewer cells will work together to make the same size tubule (C���). In the case of enormous cells, a completely different mechanism (cytoskeletal bending) will be used by a single cell to create a lumen (C���)���showing how the same machine (genome and cell) can enable high-level anatomical goals to trigger diverse low-level molecular mechanisms, as needed for a global goal (such as reaching a specific region of anatomical morphospace). Panels (A,A���,A���,B) courtesy of Douglas Blackiston, used with permission after [230]. Panel (B���) courtesy of Erin Switzer, (B���) taken with permission from [231]. Panels (C,C���,C���) by Jeremy Guay of Peregrine Creative. * p < 0.05 (A���).

4.3. Evolving Polycomputing

The rate of evolution would be much slower without this multi-scale competency architecture���the ability of the parts to get their job accomplished even if circumstances change [159] In one remarkable example, the tadpole eye, placed in the wrong position on the head, or even on the tail [230], still provides vision, because the eye primordia cells can make an eye in aberrant locations, move it if possible [232], and if not, connect it to the spinal cord (rather than directly to the brain), providing visual signals that way [233,234]. This competency of the substrate in regulative development and remodeling [22,38] can neutralize the lethal side effects of many mutations, enabling the exploration of other possibly beneficial effects. For example, consider a mutation that causes the displacement of the mouth and also another effect, E, elsewhere in the body. The potential benefits of E might never be explored in a monocomputational developmental architecture, because the mouth defect would prevent the animal from eating and drive the fitness to 0. The exploration of the effect of E would have to wait for another mutation to appear that produces the same effect without untoward side effects elsewhere���a very long wait, and often altogether impossible. In contrast, in a polycomputing architecture, structures solve morphological and physiological problems simultaneously: the mouth will move to the right location on its own [232], in parallel to all of the other developmental events, enabling evolution to explore the consequences of E. Thus, the overloaded competencies of the cells and tissues allow for evolution to simultaneously explore the other effects of those mutations on a phenotype (of which pleiotropy is one example).

In this case, these competencies create the ���hidden layer��� of developmental physiology that sits between genomic inputs and phenotypic outputs, and provides a problem solving capacity: getting an adaptive task completed, despite changes in the microenvironment or in their own parts [2,22]. This occurs simultaneously at all scales of the organization (Figure 3), and thus, each level computes specific functions not only in its own problem space, but also participates in the higher level���s space (as a component) and has an influence that deforms the action space of its lower levels��� components [22]. By using behavior-shaping competent subunits as agential materials [67], evolution produces modules that make use of each other���s outputs in parallel, virtually guaranteeing that the same processes are exploited as different ���functions��� by the other components of the cell, the body, and the swarm.

The evolutionary pressure to make existing materials perform multiple duties is immense. However, much remains to be learned about how such pressure brings about polycomputing, and how some materials can be overloaded with new functions without negatively impacting the existing ones.

In parallel to such biological investigations, within the computer science domain, much work remains to be done to devise the optimization pressures that create polycomputing substrates, and then create new programming strategies that are suitable for polycomputing. For example, no programming language has yet been devised that truly takes advantage of the polycomputational metamaterials described above. Despite our ignorance about how evolutionary or optimization pressures can create polycomputational systems, what is clear is that evolution would not work at all if living things were not machines���predictable, tractable systems. The key aspects of machines are that they harness the laws of physics and computation, etc., in a reliable, rational manner to produce specific, useful outcomes. The evolutionary process exploits the fact that life is a machine by making changes to the material, the control algorithm, and indirectly, to the environment, in a way that gives rise to predictable, adaptive outcomes. Cells could not influence each other during development to reliably achieve the needed target morphologies if they could not be efficiently controlled. Life shows us the true power of the ���machine���: a powerful, multi-scale polycomputing architecture, in which machines control and comprise other machines, all working at the same time in the same locality, but in different modalities and virtual problem spaces, producing massive amounts of plasticity, robustness, and novelty.

4.4. A New Approach to Identifying and Harnessing Computational Capabilities In Vivo and In Silico

One way to exploit this property is to use protocols that examine a particular mechanism for the novel things it can do, and for the best way to induce it to execute some of its capabilities. At the molecular level, an example is gene regulatory networks (GRNs), a formalism whereby a set of genes up- and down-regulate each other���s functions [235,236]. While GRNs and protein pathways are normally studied for ways to explain a particular aspect of biology (e.g., neural crest tissue formation or axial patterning in development [237,238]), we asked whether existing neural network models could have novel computational functions, specifically learning functions. Our algorithm took biological GRN models and, for each one, examined each possible choice of the triplets of nodes as the candidates for conditioned and unconditioned stimuli and response, as per Pavlovian classical associative learning [239]. We found numerous examples of learning capacity in biological networks and many fewer in control random networks, suggesting that evolution is enriching for this property [170]. Most strikingly, the same networks offered multiple different types of memory and computations, depending on which of the network���s nodes the observer took as their control knobs and salient readout in the training paradigm. This approach is an example of searching not for ways to rewire the causal architecture of the system for a desired function, but searching instead for a functional perspective from which an unmodified system already embodies novel functions.

This illustrates an important principle of biological polycomputing: evolution can prepare a computational affordance (the GRN) with multiple interfaces (different gene targets) through which engineers, neighboring cells, or parasites can manipulate the system to benefit from its computational capabilities. We suggest that this kind of approach may be an important way of understanding biological evolution: as a search for ways in which the body���s components can adaptively exploit other its other components as features of their environment���a search for optimal perspectives and ways to use existing interfaces. At the organism level, an excellent example is the brain, in which an immense number of functions are occurring simultaneously. Interestingly, it has been suggested that the ability to store multiple memories in the same neuronal real-estate is implemented by phase [5].

The results of our probing neural networks for novel functions also suggest that, alongside tools for predicting ways to rewire living systems [240,241,242], we should be developing tools to identify the optimal perspectives with which to view and exploit existing polycomputing capacities.

5. Conceptual Transitions

To develop such tools, we will need to overcome human cognitive bias and resist the temptation to cleave the phenomena apart in ways that feel comfortable. One approach is to look for particularly non-intuitive phenomena that defy our attempted categories. Better yet is to seek gradients, along which we can move from the ���obvious��� approximations of phenomena to increasingly ���non-obvious���, but more accurate, reflections of reality.

5.1. Directions of Conceptual Travel

One such gradient is the one that leads from serial to parallel to superposed processes. The industrial revolution demonstrated the advantage of performing tasks in parallel rather than serially; the computer age similarly demonstrated the power of parallel over serial computation. One reason for these slow transitions may be cognitive limitations: despite the massive parallelism in the human brain, human thinking seems to proceed mostly, or perhaps completely [243], in a serial fashion. ���Traditional��� parallelism, as it is usually understood, assumes that multiple processes are coincident in time but not in space. An even more difficult of a concept to grasp is that of superposition: the performance of multiple functions in the same place at the same time.

Another conceptual direction that leads from obvious into non-obvious territories is that which leads from modular processes into non-modular ones. In general, the cardinal rule in engineering, and software engineering in particular, is modular design. However, this is a concession to human cognitive limits, not necessarily ���the best way to do things���: many natural phenomena are continua. Taking another step, if we consider biological or technological polycomputing systems, we might ask whether they are modular. However, if a system polycomputes, different observers may see different subsets of functions and some may be more modular than others. In that case, the question of whether a given polycomputing biological system (or bioinspired technology) is more or less modular becomes ill-defined. We argue that, to facilitate future research, these classical distinctions must now be abandoned (at least in their original forms).

5.2. Practical Implications for AI/Robotics

Learning how biological systems polycompute, and building that learning into technology, is worth doing for several practical reasons. First, creating more computationally dense AI technologies or robots may enable them to act intelligently and thus, do useful, complex work, using fewer physical materials and thus creating less waste. Second, the technological components that polycompute may be more compatible with naturally polycomputing biological components, facilitating the creation of biohybrids. Third, creating machines that perform multiple computations in the same place at the same time may lead to the creation of machines that perform different functions in different domains���sensing, acting, computing, storing energy, and releasing energy���in the same place at the same time, leading to new kinds of robots. Fourth, polycomputing may provide a new solution to catastrophic interference, a ubiquitous problem in AI and robotics, in which an agent can only learn something new at the cost of forgetting something it has already learned. A polycomputing agent might learn and store a new behavior at an underutilized place on the frequency spectrum of its metamaterial ���brain��� better than a polycomputing-incapable agent that must learn and incorporate the same behavior into its already-trained neural network controller. Such an ability would be the neural network analogue of cognitive radio technologies, which constantly seek underutilized frequency bands from which to broadcast [244].

6. Gradual Computing in Biology: When Does the (Digital) Soul Enter the (Analog) Body?

The importance of continuous models (and the futility of some binary categories) is readily apparent when tracking the slow process of the emergence of specific features that we normally identify in their completed state, and when considering a spectrum of hybrid cases that are readily produced via evolution or bioengineering. Examples include pseudo-problems like ���when does a human baby become sentient during embryogenesis���, ���when does a cyborg become a machine vs. organism?���, and ���when does a machine become a robot?���; all of these questions force arbitrary lines to be chosen that are not backed up by discrete transitions. Developmental biology and evolution both force us to consider gradual, slow changes to be essential to the nature of the important aspects of the structure and function. This biological gradualism has strong parallels in computer science. An unfertilized human oocyte, mostly amenable to the ���chemistry and physics��� lens, eventually transforms into a complex being for whom behavioral and cognitive (and psychotherapeutic) lenses are required. What does the boot-up of a biologically embodied intelligence consist of? What are the first thoughts of a slowly developing nervous system? One key aspect of this transition process is that it involves polycomputing, as structural and physiological functions become progressively harnessed toward new, additional tasks for navigating behavioral spaces, in addition to their prior roles in metabolic, physiological, and other spaces [22,245,246,247,248]. These ideas also have implication for niche construction and the extended phenotype, in blurring the distinctions between internal and external affordances [249].

Similarly, one can zoom into the boot-up process when a dynamical system consisting of electrical components becomes a computer. During the first few microseconds, when the power is first turned on, the system becomes increasingly more amenable to computational formalisms, in addition to the electrodynamics lens. The maturation of the process consists of a dynamical mode which can profitably be modeled as ���following an algorithm��� (taking instructions off a stack and executing them). Similarly, one could observe externally supplied vibrations spreading through a metamaterial and consider when it makes sense to interpret the material���s response as a computation or the running of an algorithm. In essence, the transition from an analog device to a computer is really just a shift in the relative payoffs for two different formalisms from the perspective of the observer. These are readily missed, and an observer that failed to catch the ripening of the computational lens during this process would be a poor coder indeed, relegated to interacting with the machine via Maxwell���s laws that guide electron motion and atomic force microscopy, not by exploiting the incredibly rich set of higher-level interfaces that computers afford.

6.1. Agency and Persuadability: Implication for Polycomputing

One of the most important next steps, beyond recognizing the degree to which certain dynamical systems or physical materials can be profitably seen as computational systems, is to observe and exploit the right degree of agency. Systems vary widely along a spectrum of persuadability [2], which can be described as the range of techniques that are suitable for interacting with these systems, including physical rewiring, setpoint modification, training, and language-based reasoning. Animals are often good at detecting agency in their environment, and for humans, the theory of mind is an essential aspect of individual behavior and social culture. Consistent with the obvious utility of recognizing the agency in potential interaction partners, evolution has primed our cognitive systems to attribute the intentional stance quite readily [250,251]. Crucially, making mistakes by overestimating this agency (anthropomorphizing) is no worse than underestimating agency���both reduce the effectiveness of the adaptive interactions with the agent���s world.

6.2. The Impact of Observer Frames

So far, we have considered a single human observer of a biological or technological system, how much agency they detect in the system from their perspective, and how they use that knowledge to choose how to persuade it to do something. However, a biological system may have many observers (neighboring cells, tissues, conspecifics, and parasites) trying to ���persuade��� it to do different things, all at the same time (scare quotes here remind us that we must, in turn, decide to adopt the intentional stance for each of the observers). A polycomputing system may be capable of acceding to all of these requests simultaneously. As a simple example, an organism may provide a computational result to one observer while also providing the waste heat produced by that computation to a cold parasite. Traditional computers are not capable of this, or at least are not designed to do so, but future polycomputational machines might be.
3 / 3
ChatGPT
The text provided is a comprehensive examination of the concept of polycomputing in biology and its implications for evolutionary processes, biological systems, and technological advancements. The article delves deep into the intricacies of how biological systems evolve and utilize polycomputing, emphasizing the importance of multi-scale competency and the ability of organisms to adapt and respond to different challenges. It further examines the concept of polycomputing from an evolutionary, biological, and technological perspective.
Key takeaways from the text:
Evolutionary Pivots
: Evolution has the ability to innovate and conserve simultaneously. This capability allows it to repurpose existing mechanisms for new functions. Biological systems exhibit polycomputing, where multiple computations occur simultaneously, optimizing the use of existing hardware for diverse functions.
Polycomputing in Phenotypic Possibility
: The article highlights the Xenobots as an example of phenotypic plasticity. These are self-assembling organisms made from frog skin cells that exhibit unique behaviors, underscoring the immense adaptability and potential of biological systems.
Evolving Polycomputing
: Evolution benefits from a multi-scale competency architecture, which allows biological systems to adapt and evolve more rapidly. The adaptability of systems, like the eye of a tadpole being functional even when misplaced, highlights the robustness of biological systems.
A New Approach to Computational Capabilities
: The article suggests that to harness the full potential of polycomputing, there's a need to develop new tools and perspectives. This involves identifying and utilizing the inherent computational capabilities of systems.
Conceptual Transitions
: The text emphasizes the need to shift from traditional binary thinking to a more gradient-based perspective. It highlights the progression from serial to parallel to superposed processes and challenges the traditional modular design, suggesting that a more integrated approach may be beneficial.
Practical Implications for AI/Robotics
: Integrating the principles of biological polycomputing into AI and robotics can lead to more efficient and adaptable systems. This includes addressing issues like catastrophic interference and maximizing computational density.
Gradual Computing in Biology
: The article explores the transition from analog to digital in biological systems, emphasizing the gradual nature of these changes. It suggests that the line between analog and digital is not always clear-cut and that the transition is more of a spectrum.
Agency and Persuadability
: Recognizing and leveraging the inherent agency in systems is crucial. Systems have varying degrees of persuadability, and understanding this can optimize interactions with them.
Impact of Observer Frames
: The perspective of the observer plays a crucial role in understanding and interacting with polycomputing systems. Multiple observers might interact with a system simultaneously, and a polycomputing system might cater to all these interactions at once.
Overall, the text provides a thorough exploration of the concept of polycomputing, emphasizing its significance in understanding evolutionary processes, biological adaptability, and the potential for technological advancements.
6.3. Becoming a Computer

For many outside the computational sciences, ���computer��� denotes the typical physical machines in our everyday lives, such as laptops and smartphones. Turing, however, provided a formal definition for computers that is device-independent: in summary, a system is a computer if it has an internal state, if it can read information from the environment (i.e., its tape) in some way, update its behavior based on what it has read and its current state, and (optionally) write information back out to the tape. This theoretical construct has become known as a Turing machine; any physical system that embodies it, including organisms, is formally referred to as a computer. This broad definition admits a wide range of actors that do not seem like computers, including consortia of crabs [252], slime molds [253], fluids [254], and even algorithms running inside other computers [255]. For all of these unconventional computers, as well as for the novel mechanical computing substrates discussed above, it is difficult to tell at which point they transition from ���just physical materials��� into computers. With continuous dynamical systems such as these, observers may choose different views from which the system appears to be acting more or less like a physical instantiation of a Turing machine.

Even if an observed system seems to be behaving as if it is a Turing machine, identifying the components of that machine, such as the tape or the read/write head, can be difficult. This is a common reason why it is often claimed that organisms are not machines/computers [19,73,256]. Consider an example from the authors��� own recent work [78]: we found that motile multicellular assemblies can ���build��� other motile assemblies from loose cells. This looks very much like von Neumann machines: theoretical machines that can construct copies of themselves from the materials in their environment. Von Neumann initially proved the possibility of such machines by mathematically constructing Turing machines that built copies of themselves by referring to and altering an internal tape. However, in the biological Turing machines that we observed, there seems to be no tape. If there is one, it is likely not localized in space and time.

This difficulty in identifying whether something is a computer, or at what point it becomes one, is further frustrated by the fact that biological and non-biological systems change over time: even if one view is held constant, the system, as it changes, may seem to act more or less like a computer. Finally, a polycomputing system, because it can provide different computational results to different observers simultaneously, may at the same time present as different computers���better or worse ones, more general or more specialized ones���to those observers. Such behavior would not only foil the question ���Is that a computer?���, but would even foil any attempts to determine the time at which a system becomes a computer, or begins to act more like a computer. Zooming out, it seems that as more advanced technology is created, and as our understanding of biological systems progresses, attempts to attribute any singular cognitive self to a given system will become increasingly untenable. Instead, we will be forced, by our own engineering and science, to admit that many systems of interest house multiple selves, with more or less computational and agential potential, not just at different size scales, but also superimposed upon one another within any localized part of the system.

As Hoel points out about the ad hoc status of claiming one single privileged perspective within a system, according to the integrated information theory (IIT) account of consciousness [257]: ������There are so many viable scales of description, including computations, and all have some degree of integrated information. So, the exclusion postulate is necessary to get a definite singular consciousness. This ends up being the most controversial postulate within IIT, however.��� [99].

We maintain that, as our understanding of polycomputing biological and technological systems increases, it will eventually exclude the exclusion postulate from any attempt to explain human consciousness as a mental module operating within the brain.

7. Conclusions

Prior skeptical debates about whether biological systems are computers reflect both an outdated view of computation and a mistaken belief that there is a single, objective answer. Instead, we suggest a view in which computational interpretations are not simply lenses through which diverse observers can all understand a given system in the same way, but indeed that several diverse interpretations of the information being processed by a set of events can simultaneously be useful to different subsystems (observers) at the same time. It is now seen that there is no one-to-one mapping between biological form and function: the high conservation of the biological form and function across evolutionary instances implements a kind of multiple realizability. At the same time, biological components are massively overloaded with regard to polycomputing. Indeed, their competency, plasticity, and autonomy [2,22,258,259] may enable a kind of second-order polycomputing, where various body components attempt to model each other���s computational behavior (in effect serving as observers) and act based on their expected reward, from their perspective. Thus, modern computer engineering offers metaphors much more suited to understand and predict life than prior (linear and absolute) computational frameworks. Not only are biological systems a kind of computer (an extremely powerful one), but they are amazing polycomputing devices, of a depth which has not yet been achieved by technology. In this sense, biological systems are indeed different than today���s computers, although there is no reason why the future efforts to build deep, multi-scale, highly plastic synthetic devices cannot take advantage of the principles of biologic polycomputing. A key implication of our view is that that blanket pronouncements about what living or non-living machines can do are worthless: we are guaranteed to be surprised by outcomes that can only be achieved by formulating and testing hypotheses. It is already clear that synthetic, evolved, and hybrid systems far outstrip our ability to predict the limits of their adaptive behavior; abandoning the absolutist categories and objective views of computation is a first step towards expanding our predictive capabilities.

At stake are numerous practical outcomes, in addition to fundamental questions. For example, to make transformative advances in our ability to improve the health in biomedical settings [260], we must be able to control multiple scales of biological organizations which are heavily polycomputing���from cellular pathways to patient psychological states. It is essential to begin to develop computational frameworks to facilitate that kind of control. The ability to construct and model a kind of computational superposition, in which diverse observers (scientists, users, the agent itself, and its various components) have their own model of the dynamic environment, and optimize their behavior accordingly, will also dovetail with and advance the efforts of synthetic bioengineering, biorobotics, smart materials, and AI.
ChatGPT
This is a detailed and insightful exploration of the concept of computation and its implications for understanding biological systems. The text delves into the challenges of defining what constitutes a "computer" and highlights the evolving nature of our understanding of computation, especially in the context of biological systems.
Key takeaways from the provided text:
Becoming a Computer
:
The general perception of a "computer" often refers to physical machines like laptops and smartphones. However, Alan Turing provided a broader, device-independent definition. A system is considered a computer if it possesses an internal state, can read information from its environment, adjust its behavior based on this input, and write back to the environment. This definition leads to the concept of a Turing machine.
Various unconventional entities, like crabs, slime molds, and fluids, can be seen as computers from this broad perspective.
Identifying components of a Turing machine in observed systems can be challenging. An example is given of motile multicellular assemblies that seem to function like von Neumann machines but don't appear to have a clear "tape" component.
The dynamic nature of biological and non-biological systems further complicates the identification of whether they function as computers.
Perspectives on Computation
:
Our understanding of systems can vary based on the perspective of the observer. A system might seem to function more or less like a computer depending on who or what is observing it.
The notion of polycomputing suggests that a system can present itself differently to various observers. It challenges the idea of attributing a singular cognitive self to any system.
Conclusions
:
The debate on whether biological systems are computers is rooted in outdated computational concepts. The authors propose that computational interpretations should be understood as diverse lenses through which various observers perceive a system.
Biological systems are complex, multi-layered, and capable of polycomputing, making them incredibly powerful computational devices. Their complexity surpasses the capabilities of current technological systems.
The authors emphasize the importance of moving away from rigid computational categories to enhance our understanding and predictive capabilities. This shift is crucial for advancements in fields like biomedicine, biorobotics, and AI.
Overall, the text underscores the idea that as our understanding of computation evolves, so too must our interpretations of what constitutes a computer, especially when examining complex systems like biological organisms.
How does this article relate to the topics we were discussing before?
ChatGPT
The article you provided delves deep into the concept of computation, especially in relation to biological systems, and challenges traditional notions of what constitutes a "computer." This ties directly to our previous discussion on unconventional computing, where we explored different mediums and methods used for computation outside of traditional silicon-based computers.
Here's how the article relates to our previous topics:
Unconventional Computing Mediums
:
The article mentions unconventional entities such as crabs, slime molds, and fluids as potential computers. This aligns with our discussion on using unconventional materials and systems for computation.
Defining a Computer
:
Our previous conversation revolved around how unconventional computing methods challenge our traditional understanding of computation. The article further expands on this by emphasizing Turing's broader definition of a computer and how various systems, including biological ones, can fit this definition.
Complexity of Biological Systems
:
Both the article and our previous discussion highlight the intricacy of biological systems and their potential computational capabilities. The article's emphasis on polycomputing and the multifaceted nature of biological entities resonates with the idea of tapping into the vast computational potential of nature.
Challenges in Understanding Computation
:
In our earlier conversation, we touched upon the challenges and limitations of current computational models. The article delves deeper into this, discussing the difficulties in identifying when a system functions as a computer and the varying perspectives from which a system can be observed.
The Future of Computation
:
Our prior discussions highlighted the future possibilities in unconventional computing. The article also suggests that as technology advances and our understanding of biological systems grows, our perspectives on computation will need to evolve, hinting at a future where biological systems and technology might merge in novel ways.
In summary, the article deepens and complements our earlier exploration of unconventional computing by providing a rigorous exploration of how biological systems challenge and expand our traditional notions of computation.
And before that, the topics were: 

1. Fred Gage's research on retrotransposons in the brain.
2. Error-correcting codes and algorithms where noise is beneficial.
3. The concept of noise in systems and algorithms, including stochastic resonance, dithering, simulated annealing, error-correcting codes, stable diffusion algorithms, noise-based logic, and randomized algorithms.
4. The analogy between retrotransposons introducing variability in neurogenesis and the role of noise in algorithms.
5. The theory of selective destruction (SDT) for aging and its comparison to the disposable soma theory.
6. The importance and functions of the interstitium in maintaining bodily integrity.
7. The concept of the brain performing geometric reasoning on graphs at various scales.
ChatGPT
Considering the topics you listed and the article you provided, here's a comprehensive relation:
Retrotransposons in the Brain
:
Both Fred Gage's research and the article emphasize the complexity and adaptability of biological systems. The activity of retrotransposons introduces variability in brain cells, which could be likened to unconventional computing systems that adapt and process information in unique ways.
Error-correcting Codes and Beneficial Noise
:
The article's discussion on polycomputing and the unpredictability of biological systems relates to the concept of error-correcting codes. Just as biological systems can process information in multifaceted ways, error-correcting codes manage noise and errors in computational systems to ensure accurate data transmission.
Noise in Systems and Algorithms
:
The article touches on the idea that biological systems can be seen from various perspectives, possibly acting more or less like a computer. This aligns with our earlier discussion on how noise, such as stochastic resonance or dithering, can be beneficial in certain algorithms, adding a level of adaptability or improving signal quality.
Analogy of Retrotransposons and Noise in Algorithms
:
The article's exploration of unconventional computing systems, including biological entities, reinforces the analogy between retrotransposons introducing variability and the role of noise in algorithms. Both phenomena introduce elements of unpredictability that can lead to beneficial outcomes.
Theories on Aging
:
The article emphasizes the complexity and adaptability of biological systems, which can also be related to the theories of aging. The way biological systems process information and adapt over time can offer insights into how aging mechanisms, like the theory of selective destruction or the disposable soma theory, operate within the body.
Interstitium and Bodily Integrity
:
The article's focus on the intricate nature of biological systems and their computational potential can be linked to the importance of the interstitium in maintaining bodily integrity. Both highlight the interconnectedness and multifunctionality of biological structures.
Brain's Geometric Reasoning
:
The article suggests that biological systems, including the brain, operate as polycomputing devices. This can be related to our earlier discussion on how the brain performs geometric reasoning on graphs, indicating the vast computational capabilities of biological systems.
In essence, the article provides a framework to understand biological systems as multifaceted computational entities. This perspective aligns with our previous discussions, emphasizing the complexity, adaptability, and unique computational capabilities of biological entities, from the cellular level to entire organisms.
I decided to call the conversation "Retrotransposon Noise".

	Reichmann J., Crichton J.H., Madej M.J., Taggart M., Gautier P., Garcia-Perez J.L., Meehan R.R., Adams I.R. Microarray analysis of LTR retrotransposon noise identifies Hdac1 for a regulator of retrotransposon expression in mouse seed stem cells. PLoS Comput. Biologic. 2012; 8:e1002486.
ChatGPT
"Retrotransposon Noise" is an apt title that encapsulates the main themes of our discussion. The title suggests the role of retrotransposons in introducing variability or "noise" into biological systems, and how this noise can influence the system's function and adaptability.
The citation you provided references a study on the regulation of retrotransposon expression in mouse embryonic stem cells. Specifically, it highlights the role of the Hdac1 gene in controlling the expression of these retrotransposons. This is interesting because it suggests that while retrotransposons introduce variability, there are also mechanisms in place to regulate this variability, much like how noise in algorithms can be beneficial but is also managed to ensure optimal function.
In the context of our conversation, this study further reinforces the idea that biological systems, like computational systems, are intricate and multifaceted, with various components interacting in complex ways to achieve balance and functionality. The regulation of retrotransposon activity is just one example of how biological systems manage the "noise" they encounter.
Connections with the previous topics.
ChatGPT
Certainly! Let's draw connections between the "Retrotransposon Noise" and the previously discussed topics:
Fred Gage's Research on Retrotransposons in the Brain
:
Both deal directly with retrotransposons. Gage's research emphasized the role of retrotransposons in introducing genetic variability in the brain, which can be likened to "noise". The study on Hdac1 provides insight into the mechanisms that control this noise, ensuring it doesn't become detrimental.
Error-Correcting Codes and Beneficial Noise Algorithms
:
Just as error-correcting codes manage noise in computational systems to maintain integrity, the Hdac1 gene regulates retrotransposon activity, ensuring that the introduced variability doesn't compromise the integrity of the genetic code.
Noise in Systems and Algorithms
:
Retrotransposons introduce genetic "noise" or variability. The discussed algorithms and systems, like stochastic resonance or simulated annealing, leverage noise for better performance. Similarly, retrotransposon activity might offer evolutionary advantages by increasing genetic diversity.
Analogy between Retrotransposons and Noise in Algorithms
:
Retrotransposons in neurogenesis introduce variability, much like noise in algorithms, which can lead to new solutions or paths. The regulation of this noise ensures that it's beneficial rather than harmful.
Selective Destruction Theory (SDT) for Aging
:
Both SDT and the regulation of retrotransposons deal with managing detrimental effects in biological systems. While SDT focuses on removing damaged cells to maintain organism health, Hdac1 regulates retrotransposon activity to prevent potential genetic damage.
Interstitium in Maintaining Bodily Integrity
:
Just as the interstitium plays a role in maintaining bodily integrity, the regulation of retrotransposons ensures genetic integrity. Both mechanisms work to maintain the balance and health of the organism.
Brain Performing Geometric Reasoning on Graphs
:
This topic is more abstract compared to the others. However, one could argue that just as the brain processes information and makes connections at different scales, the genetic "noise" introduced by retrotransposons might influence how genes are expressed and connected, leading to diverse outcomes in neural structures and functions.
In summary, the concept of "Retrotransposon Noise" ties together the idea that biological systems, like computational ones, introduce variability (or noise) for potential benefits but also have mechanisms in place to regulate and manage this variability to ensure optimal functionality and integrity.
Ok thank you. Could I get a final summary of the whole conversation?
ChatGPT
Of course! Here's a concise summary of our extensive discussion:
Title
:
Retrotransposon Noise
Fred Gage's Research
: We discussed Gage's findings on how retrotransposons introduce genetic variability in the brain, potentially playing a role in cognitive diversity and adaptability.
Beneficial Noise in Algorithms
: We explored how noise can be beneficial in various computational algorithms, including stochastic resonance, dithering, simulated annealing, and error-correcting codes. These algorithms leverage noise to improve performance or reach optimal solutions.
Retrotransposons and Noise Analogy
: Drawing a parallel between retrotransposons in neurogenesis and noise in algorithms, we contemplated how both introduce variability that can lead to evolutionary advantages or novel solutions.
Selective Destruction Theory (SDT)
: We touched on the SDT for aging, which postulates that selectively destroying damaged cells can combat aging. This was compared to the disposable soma theory.
Interstitium's Role
: The importance of the interstitium in maintaining bodily integrity was discussed. This tissue plays a crucial role in fluid balance and might have implications in health and disease.
Brain's Geometric Reasoning
: We delved into the concept that the brain might perform geometric reasoning on graphs at multiple scales, reflecting its complex information processing capabilities.
Retrotransposon Noise Article
: The study on Hdac1 was highlighted, which identified this gene as a regulator of retrotransposon expression in mouse embryonic stem cells, emphasizing the importance of controlling genetic "noise" for cellular integrity.
Linking Topics
: We connected the discussed topics, emphasizing how biological systems, much like computational ones, introduce variability for potential benefits but need mechanisms to regulate this variability to ensure functionality and integrity.
Throughout the conversation, the recurring theme was the intricate interplay between noise (or variability) and regulation in both biological and computational systems, and how this balance can lead to evolutionary advantages, novel solutions, and system integrity.
ChatGPT may produce inaccurate information about people, places, or facts.
ChatGPT September 25 Version
?
ChatGPT



