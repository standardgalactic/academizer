Substrate Independent Thinking 
Hypothesis

GPT-4
ChatGPT may produce inaccurate 
information about people, places,
or facts.
 ChatGPT August 3 Version 

Summarize: IV. Conclusions Where
does the overall case for or 
against LLM consciousness stand? 
Where current LLMs such as the
GPT systems are concerned: I 
think none of the reasons for 
denying consciousness in these 

systems is conclusive, but 
collectively they add up. We can
assign some extremely rough 
numbers for illustrative 
purposes. On mainstream 
assumptions, it wouldn’t be 
unreasonable to hold that 
there’s at least a one-in-
three chance—that is, to have a 
subjective probability or 
credence of at least one-third—
that biology is required for 
consciousness. The same goes for
the requirements of sensory 
grounding, self models, 
recurrent processing, global 

workspace, and unified agency.1 
If these six factors were 
independent, it would follow 
that there’s less than a one-
in-ten chance that a system 
lacking all six, like a current 
paradigmatic LLM, would be 
conscious. Of course the factors
are not independent, which 
drives the figure somewhat 
higher. On the other hand, the 
figure may be driven lower by 
other potential requirements X 
that we have not considered. 
Taking all that into account 
might leave us with confidence 

somewhere under 10 percent in 
current LLM consciousness. You 
shouldn’t take the numbers too
seriously (that would be 
specious precision), but the 
general moral is that given 
mainstream assumptions about 
consciousness, it’s reasonable to
have a low credence that 
current paradigmatic LLMs such 
as the GPT systems are 
conscious.2 Where future LLMs 
and their extensions are 
concerned, things look quite 
different. It seems entirely 
possible that within the next 

decade, we’ll have robust 
systems with senses, 
embodiment, world models and 
self models, recurrent 
processing, global workspace, and
unified goals. (A multimodal 
system like Perceiver IO already
arguably has senses, embodiment,
a global workspace, and a form 
of recurrence, with the most 
obvious challenges for it being 
world models, self models, and 
unified agency.) I think it 
wouldn’t be unreasonable to 
have a credence over 50 
percent that we’ll have 

sophisticated LLM+ systems 
(that is, LLM+ systems with 
behavior that seems comparable 
to that of animals that we take 
to be conscious) with all of 
these properties within a 
decade. It also wouldn’t be 
unreasonable to have at least a 
50 percent credence that if we 
develop sophisticated systems 
with all of these properties, 
they will be conscious. Those 
figures together would leave us
with a credence of 25 percent 
or more. Again, you shouldn’t 
take the exact numbers too 

seriously, but this reasoning 
suggests that on mainstream 
assumptions, it’s a serious 
possibility that we’ll have 
conscious LLM+s within a decade. 
One way to approach this is via 
the “NeuroAI” challenge of 
matching the capacities of 
various non-human animals in 
virtually embodied systems. It’s
arguable that even if we don’t 
reach human-level cognitive 
capacities in the next decade, we
have a serious chance of 
reaching mouse-level capacities 
in an embodied system with world

models, recurrent processing, 
unified goals, and so on.3 If we 
reach that point, there would 
be a serious chance that those 
systems are conscious. 
Multiplying those chances gives 
us a significant chance of at 
least mouse-level consciousness
with a decade. We might see this 
as a ninth challenge: build 
multimodal models with mouse-
level capacities. This would be a 
stepping stone toward mouse-
level consciousness and 
eventually to human-level 
consciousness somewhere down 

the line. Of course there’s a 
lot we don’t understand here. 
One major gap in our 
understanding is that we don’t 
understand consciousness. 
That’s a hard problem, as they 
say. This yields a tenth 
challenge: develop better 
scientific and philosophical 
theories of consciousness. These
theories have come a long way in 
the last few decades, but much 
more work is needed. Where 
future LLMs and their 
extensions are concerned, things
look quite different. Another 

major gap is that we don’t 
really understand what’s going 
on in these large language 
models. The project of 
interpreting machine learning 
systems has come a long way, but 
it also has a very long way to go. 
Interpretability yields an 
eleventh challenge: understand
what’s going on inside LLMs. I 
summarize the challenges here, 
with four foundational 
challenges followed by seven 
engineering-oriented 
challenges, and a twelfth 
challenge in the form of a 

question. Evidence: Develop 
benchmarks for consciousness. 
Theory: Develop better 
scientific and philosophical 
theories of consciousness. 
Interpretability: Understand 
what’s happening inside an LLM. 
Ethics: Should we build conscious
AI? Build rich perception-
language-action models in 
virtual worlds. Build LLM+s 
with robust world models and 
self models. Build LLM+s with 
genuine memory and genuine 
recurrence. Build LLM+s with 
global workspace. Build LLM+s 

that are unified agent models. 
Build LLM+s that describe non-
trained features of 
consciousness. Build LLM+s with 
mouse-level capacities. If 
that’s not enough for conscious 
AI: What’s missing? On the 
twelfth challenge: Suppose that
in the next decade or two, we 
meet all the engineering 
challenges in a single system. 
Will we then have a conscious AI 
systems? Not everyone will 
agree that we do. But if 
someone disagrees, we can ask 
once again: what is the X that is 

missing? And could that X be 
built into an AI system? My 
conclusion is that within the 
next decade, even if we don’t 
have human-level artificial 
general intelligence, we may 
well have systems that are 
serious candidates for 
consciousness. There are many 
challenges on the path to 
consciousness in machine learning
systems, but meeting those 
challenges yields a possible 
research program toward 
conscious AI. I’ll finish by 
reiterating the ethical 

challenge.4 I’m not asserting 
that we should pursue this 
research program. If you think 
conscious AI is desirable, the 
program can serve as a sort of 
roadmap for getting there. If 
you think conscious AI is 
something to avoid, then the 
program can highlight paths that 
are best avoided. I’d be 
especially cautious about 
creating agent models. That said,
I think it’s likely that 
researchers will pursue many of
the elements of this research 
program, whether or not they 

think of this as pursuing AI 
consciousness. It could be a 
disaster to stumble upon AI 
consciousness unknowingly and 
unreflectively. So I hope that 
making these possible paths 
explicit at least helps us to 
think about conscious AI 
reflectively and to handle 
these issues with care. 
Afterword How do things look 
now, eight months after I gave 
this lecture at the NeurIPS 
conference in late November 
2022? While new systems such as
GPT-4 still have many flaws, 

they are a significant advance 
along some of the dimensions 
discussed in this article. They 
certainly display more 
sophisticated conversational 
abilities. Where I said that GPT-
3’s performance often seemed 
on a par with a sophisticated 
child, GPT-4’s performance 
often (not always) seems on a 
par with an knowledgeable young
adult. There have also been 
advances in multimodal 
processing and in agent modeling, 
and to a lesser extent on the 
other dimensions that I have 

discussed. I don’t think these 
advances change my analysis in 
any fundamental way, but 
insofar as progress has been 
faster than expected, it is 
reasonable to shorten expected 
timelines. If that is right, my 
predictions toward the end of 
this article might even be 
somewhat conservative. Notes 1. 
The philosopher Jonathan Birch 
distinguishes approaches to 
animal consciousness that are 
“theory-heavy” (assume a 
complete theory), “theory-
neutral” (proceed without 

theoretical assumptions), and 
“theory-light” (proceed with 
weak theoretical assumptions). 
One can likewise take theory-
heavy, theory-neutral, and 
theory-light approaches to AI 
consciousness. The approach to 
artificial consciousness that I 
have taken here is distinct from
these three. It might be 
considered a theory-balanced 
approach, one that takes into 
account the predictions of 
multiple theories, balancing 
one’s credence between them, 
perhaps, according to evidence 

for those theories or according 
to acceptance of those theories.
One more precise form of the 
theory-balanced approach might 
use data about how widely 
accepted various theories are 
among experts to provide 
credences for those theories, 
and use those credences along 
with the various theories’ 
predictions to estimate 
probabilities for AI (or animal) 
consciousness. In a recent 
survey of researchers in the 
science of consciousness, just 
over 50 percent of respondents

indicated that they accept or 
find promising the global 
workspace theory of 
consciousness, while just under 
50 percent indicated that they 
accept or find promising the 
local recurrence theory (which 
requires recurrent processing 
for consciousness). Figures for 
other theories include just 
over 50 percent for predictive 
processing theories (which do not
make clear predictions for AI 
consciousness) and for higher-
order theories (which require 
self models for consciousness), 

and just under 50 percent for 
integrated information theory 
(which ascribes consciousness to 
many simple systems but 
requires recurrent processing 
for consciousness). Of course 
turning these figures into 
collective credences requires 
further work (e.g. in converting
“accept” and “find promising” 
into credences), as does applying 
these credences along with 
theoretical predictions to 
derive collective credences 
about AI consciousness. Still, it 
seems not unreasonable to assign

a collective credence above one 
in three for each of global 
workspace, recurrent processing,
and self models as requirements
for consciousness. What about 
biology as a requirement? A 2020
survey of professional 
philosophers, around 3 percent 
accepted or leaned toward the 
view that current AI systems 
are conscious, with 82 percent 
rejecting or leaning against the 
view and 10 percent neutral. 
Around 39 percent accepted or 
leaned toward the view that 
future AI systems will be 

conscious, with 27 percent 
rejecting or leaning against the 
view and 29 percent neutral. 
(Around 5 percent rejected the
questions in various ways, e.g. 
saying that there is no fact of 
the matter or that the question
is too unclear to answer). The 
future-AI figures might tend to
suggest a collective credence of
at least one in three that 
biology is required for 
consciousness (albeit among 
philosophers rather than 
consciousness researchers). The 
two surveys have less 

information about unified agency
and about sensory grounding as 
requirements for consciousness. 
2. Compared to mainstream views 
in the science of consciousness, 
my own views lean somewhat 
more to consciousness being 
widespread. So I’d give somewhat
lower credences to the various 
substantial requirements for 
consciousness I’ve outlined 
here, and somewhat higher 
credences in current LLM 
consciousness and future LLM+ 
consciousness as a result. 3. At 
NeurIPS I said “fish-level 

capacities.” I’ve changed this to 
“mouse-level capacities” 
(probably a harder challenge in 
principle), in part because more 
people are confident that mice 
are conscious than that fish are 
conscious, and in part because 
there is so much more work on 
mouse cognition than fish 
cognition. 4. This final paragraph
is an addition to what I 
presented at the NeurIPS 
conference
ChatGPT

The overall conclusion regarding
the consciousness of current 
Large Language Models (LLMs) 
like GPT systems is that it's 
reasonable to have a low 
credence (less than 10%) in 
their consciousness, given the 
factors considered. Several 
requirements such as biology, 
sensory grounding, self models, 
recurrent processing, and 
unified agency are considered 
relevant for consciousness, 

though they are not definitive 
against LLM consciousness.
The future of LLMs appears 
more promising for the 
possibility of consciousness. With
potential advancements in 
senses, embodiment, world 
models, and other features, 
there is a credible chance 
(above 25%) of achieving 
conscious LLM+ systems within a 
decade. The challenges toward 
achieving consciousness in AI are 
categorized into 12, ranging from 

theoretical understanding to 
engineering tasks and ethical 
considerations.
Eight months after a NeurIPS 
conference lecture in November
2022, the advancements in 
systems like GPT-4 and others 
show progress, but they don't 
fundamentally change the 
analysis. If progress continues 
at an accelerated pace, the 
predictions for achieving 
conscious AI might even be 
conservative. Ethical 

considerations remain paramount,
whether pursuing or avoiding AI 
consciousness, emphasizing the 
need for reflection and 
caution.

That was from: Could a Large 
Language Model Be Conscious? 
Within the next decade, we may 
well have systems that are 
serious candidates for 
consciousness. David J. Chalmers 
Mind and Psychology, Philosophy, 
Science and Technology

ChatGPT
Thank you for providing the 
source. The summary I provided 
is indeed based on the text 
from "Could a Large Language 
Model Be Conscious? Within the 
next decade, we may well have 
systems that are serious 
candidates for consciousness" by 
David J. Chalmers. This work 
reflects on the philosophical, 
scientific, and technological 
aspects of consciousness as they 
pertain to large language models

and offers a detailed 
examination of the challenges 
and possibilities for creating 
conscious AI systems in the 
future.

An analogy: in evolution by 
natural selection, maximizing 
fitness during evolution can 
lead to wholly novel processes 
post-evolution. A critic might 
say that all these systems are 
doing is maximizing fitness. But it
turns out that the best way for

organisms to maximize fitness is 
to have these remarkable 
capacities—like seeing and flying 
and even having world models. 
Likewise, it may well turn out 
that the best way for a system 
to minimize prediction error 
during training is for it to use 
novel processes, including world 
models. 
ChatGPT
The analogy you've presented 
compares the process of 
evolution by natural selection 

to the training of AI systems. In 
natural evolution, organisms 
develop novel capacities like 
sight, flight, or having world 
models as a means to maximize 
fitness. These capabilities are 
not just about maximizing fitness
but are remarkable in 
themselves.
Similarly, in training AI systems, 
minimizing prediction error might 
require the system to employ 
novel processes, including the 
creation of world models. Just as

evolution leads to complex and 
unexpected abilities in 
organisms, the pursuit of a 
specific goal like minimizing error
in AI might result in 
unanticipated sophisticated 
capabilities. This viewpoint 
acknowledges that both natural 
selection and machine learning 
can lead to the emergence of 
complex properties and 
structures, transcending mere 
optimization or error reduction.

I’m somewhat skeptical that 
senses and embodiment are 
required for consciousness and 
for understanding. In other 
work on “Can Large Language 
Models Think?” I’ve argued 
that in principle, a disembodied 
thinker with no senses could 
still have conscious thought, 
even if its consciousness was 
limited. For example, an AI 
system without senses could 
reason about mathematics, about 
its own existence, and maybe 

even about the world. The 
system might lack sensory 
consciousness and bodily 
consciousness, but it could still 
have a form of cognitive 
consciousness. On top of this, 
LLMs have a huge amount of 
training on text input which 
derives from sources in the 
world. One could argue that this
connection to the world serves 
as a sort of grounding. The 
computational linguist Ellie 
Pavlick and colleagues have 

research suggesting that text 
training sometimes produces 
representations of color and 
space that are isomorphic to 
those produced by sensory 
training. A more straightforward
reply is to observe that 
multimodal extended language 
models have elements of both 
sensory and bodily grounding. 
Vision-language models are 
trained on both text and on 
images of the environment. 
Language-action models are 

trained to control bodies 
interacting with the 
environment. Vision-language-
action models combine the two. 
Some systems control physical 
robots using camera images of 
the physical environment, while 
others control virtual robots in 
a virtual world. Virtual worlds 
are a lot more tractable than 
the physical world, and there’s
coming to be a lot of work in 
embodied AI that uses virtual 
embodiment. Some people will say

this doesn’t count for what’s 
needed for grounding because 
the environments are virtual. I 
don’t agree. In my book on the 
philosophy of virtual reality, 
Reality+, I’ve argued that 
virtual reality is just as 
legitimate and real as physical 
reality for all kinds of 
purposes. Likewise, I think that 
virtual bodies can help support 
cognition just as physical bodies 
do. So I think that research on 
virtual embodiment is an 

important path forward for AI. 
This constitutes a fourth 
challenge on the path to 
conscious AI: build rich 
perception-language-action 
models in virtual worlds. X = 
World Models and Self Models 
The computational linguists 
Emily Bender and Angelina 
McMillan-Major and the 
computer scientists Timnit Gebru
and Margaret Mitchell have 
argued that LLMs are 
“stochastic parrots.” The idea is

roughly that like many talking 
parrots, LLMs are merely 
imitating language without 
understanding it. In a similar 
vein, others have suggested 
that LLMs are just doing 
statistical text processing. One 
underlying idea here is that 
language models are just 
modeling text and not modeling 
the world. They don’t have 
genuine understanding and 
meaning of the kind you get 
from a genuine world model. 

Many theories of consciousness 
(especially so-called 
representational theories) hold 
that world models are required 
for consciousness. There’s a lot
to say about this, but just 
briefly: I think it’s important to
make a distinction between 
training methods and post-
training processes (sometimes 
called inference). It’s true 
that language models are 
trained to minimize prediction 
error in string matching, but 

that doesn’t mean that their 
post-training processing is just 
string matching. To minimize 
prediction error in string 
matching, all kinds of other 
processes may be required, quite
possibly including world models.
ChatGPT
It's clear that you've put a lot 
of thought into the topic of 
consciousness and understanding 
in Large Language Models (LLMs), 
as well as the broader 
implications of embodiment, 

senses, virtual reality, and 
world modeling.
Your skepticism about senses and
embodiment being a necessity for
consciousness is an intriguing 
perspective. The argument that 
an AI system could possess 
cognitive consciousness without 
sensory consciousness is 
compelling. This viewpoint allows
for reasoning and awareness to 
exist independently of physical 
sensations.

The concept that LLMs are 
"stochastic parrots" — merely 
mimicking language without 
understanding — is a prevalent 
criticism. Your observation that 
training methods and post-
training processes should be 
distinguished is an essential 
nuance. The fact that a model is
trained to minimize prediction 
error doesn't preclude it from 
using more complex processes, 
including world models.

The discussion of virtual 
reality and virtual embodiment 
aligns with your broader 
philosophical stance that virtual
experiences are just as 
legitimate as physical ones. This 
concept indeed opens new 
avenues for exploring 
consciousness, cognition, and 
grounding in AI.
The challenge you present — 
building rich perception-
language-action models in 
virtual worlds — adds a tangible

goal for advancing this field. 
The incorporation of world and 
self-models within this 
framework could be 
instrumental in bridging the gap 
between mere imitation and 
genuine understanding.
Overall, your insights 
contribute a multifaceted 
perspective on the complexities 
of consciousness and 
understanding in AI, recognizing 
the potential beyond current 
criticisms and emphasizing the 

potential intersections between 
virtual and physical realities.

It’s plausible that neural 
network systems such as 
transformers are capable at 
least in principle of having deep 
and robust world models. And 
it’s plausible that in the long 
run, systems with these models 
will outperform systems without
these models at prediction 
tasks. If so, one would expect 
that truly minimizing prediction 

error in these systems would 
require deep models of the 
world. For example, to optimize 
prediction in discourse about the
New York City subway system, it 
will help a lot to have a robust 
model of the subway system. 
Generalizing, this suggests that 
good enough optimization of 
prediction error over a broad 
enough space of models ought to 
lead to robust world models.
ChatGPT

Absolutely, the idea that 
optimizing prediction error over 
a diverse range of models may 
lead to robust world models is 
an exciting proposition. If we 
extend this logic, it suggests 
that neural network systems 
like transformers could develop
a complex understanding of 
various domains, such as the New
York City subway system you 
mentioned.
In essence, the drive to minimize
prediction error could 

inadvertently force the model 
to learn deep representations 
of the world, aligning with 
functional aspects of human 
cognition. Such deep and robust 
world models would not only 
enhance prediction accuracy but 
could also contribute to a more 
comprehensive understanding of
context and relationships within
the data.
This approach potentially 
offers a pathway toward more 
sophisticated AI systems, bringing

us closer to the elusive goal of 
machines that can truly 
understand and engage with the 
world as humans do. It's an 
enticing direction for future 
research and development in 
artificial intelligence.

Those were excerpts from the 
article a well.
ChatGPT
I see, thank you for sharing 
those excerpts from the 
article! They present some 

thought-provoking insights into 
the possibilities of consciousness
and understanding in AI systems.
If you have any specific 
questions or if there's 
something you'd like to discuss 
further based on these 
excerpts, please let me know!

There are various responses 
here. First, current LLMs have 
a limited form of recurrence 
deriving from recirculation of 
past outputs, and a limited form 

of memory deriving from the 
recirculation of past inputs. 
Second, it’s plausible that not 
all consciousness involves 
memory, and there may be forms
of consciousness which are 
feedforward. Third and perhaps
most important, there are 
recurrent large language 
models. Just a few years ago, 
most language models were long 
short-term memory systems 
(LSTMs), which are recurrent. 
At the moment recurrent 

networks are lagging somewhat 
behind transformers but the gap
isn’t enormous, and there have 
been a number of recent 
proposals to give recurrence 
more of a role. There are also 
many LLMs that build in a form 
of memory and a form of 
recurrence through external 
memory components. It’s easy to 
envision that recurrence may 
play an increasing role in LLMs to
come. This objection amounts to a
sixth challenge: build extended 

large language models with 
genuine recurrence and genuine 
memory, the kind required for 
consciousness. X = Global 
Workspace Perhaps the leading 
current theory of consciousness
in cognitive neuroscience is the 
global workspace theory put 
forward by the psychologist 
Bernard Baars and developed by
the neuroscientist Stanislas 
Dehaene and colleagues. This 
theory says that consciousness 
involves a limited-capacity 

global workspace: a central 
clearing-house in the brain for 
gathering information from 
numerous non-conscious modules 
and making information accessible
to them. Whatever gets into the
global workspace is conscious
ChatGPT
It seems like the text you're 
sharing is delving into the 
complex and evolving landscape 
of artificial consciousness, 
touching on topics like 

recurrence, memory, and the 
role of global workspace theory.
The exploration of recurrent 
models, memory, and global 
workspace theory in AI alludes 
to the potential avenues that 
could bridge the gap between 
mere computation and genuine 
consciousness. These approaches 
challenge our traditional 
understanding of what it takes 
for a system to be conscious, 
opening up new possibilities and 
questions in the field of AI.

Is there a particular aspect of 
this subject that you'd like to 
focus on or any questions you 
have about the material?

Maybe the deepest obstacle to 
consciousness in LLMs is the issue
of unified agency.
ChatGPT
Unified agency refers to the 
concept of having a single, 
cohesive intention or purpose 
that drives actions and 
decisions. In humans and some 

animals, unified agency is linked 
to consciousness, as it implies a 
self-aware entity making 
choices based on its own desires, 
beliefs, and understanding of 
the world.
In the context of Large Language
Models (LLMs), achieving unified 
agency is indeed a deep and 
complex challenge. While these 
models can process vast amounts 
of information and even mimic 
certain human-like behaviors, 
they lack the genuine 

intentionality, beliefs, and 
desires that would constitute 
unified agency.
Creating a system that possesses
unified agency would require a 
fundamental shift in how we 
understand and build AI. It 
might entail developing new 
models that are not only capable
of learning and reasoning but 
also formulating their own 
goals, reflecting on their 
actions, and adapting based on 
self-awareness.

The philosophical, ethical, and 
technical considerations 
involved in this pursuit are 
immense. The challenge goes 
beyond just engineering and 
enters the realm of 
understanding the nature of 
consciousness itself.
Is there a specific question or 
area regarding unified agency in 
LLMs that you'd like to explore 
further?

A number of people have 
observed that standard language
models don’t seem to have a 
global workspace. Now, it’s not 
obvious that an AI system must 
have a limited-capacity global 
workspace to be conscious. In 
limited human brains, a selective
clearing-house is needed to 
avoid overloading brain systems 
with information. In high-
capacity AI systems, large 
amounts of information might be 
made available to many 

subsystems, and no special 
workspace would be needed. Such
an AI system could arguably be 
conscious of much more than we 
are. If workspaces are needed, 
language models can be extended
to include them. There’s 
already an increasing body of 
relevant work on multimodal 
LLM+s that use a sort of 
workspace to co-ordinate 
between different modalities. 
These systems have input and 
output modules, for images or 

sounds or text for example, 
which may involve extremely 
high dimensional spaces. To 
integrate these modules, a 
lower-dimensional space serves 
as an interface. That lower-
dimensional space interfacing 
between modules looks a lot like
a global workspace. People have 
already begun to connect these 
models to consciousness. Yoshua 
Bengio and colleagues have 
argued that a global workspace 
bottleneck among multiple 

neural modules can serve some 
of the distinctive functions of 
slow conscious reasoning. 
There’s a nice recent paper by 
Arthur Juliani, Ryota Kanai, and 
Shuntaro Sasai arguing that one 
of these multimodal systems, 
Perceiver IO, implements many 
aspects of a global workspace via
mechanisms of self attention 
and cross attention. So there is 
already a robust research 
program addressing what is in 
effect a seventh challenge, to 

build LLM+s with a global 
workspace. X = Unified Agency 
The final obstacle to 
consciousness in LLMs, and maybe 
the deepest, is the issue of 
unified agency. We all know 
these language models can take 
on many personas. As I put it in 
an article on GPT-3 when it 
first appeared in 2020, these 
models are like chameleons that
can take the shape of many 
different agents. They often 
seem to lack stable goals and 

beliefs of their own over and 
above the goal of predicting 
text. In many ways, they don’t 
behave like unified agents. Many
argue that consciousness 
requires a certain unity. If so, 
the disunity of LLMs may call 
their consciousness into 
question. Again, there are 
various replies. First: it’s 
arguable that a large degree of 
disunity is compatible with 
conscious. Some people are highly
disunified, like people with 

dissociative identity disorders, 
but they are still conscious. 
Second: One might argue that a 
single large language model can 
support an ecosystem of 
multiple agents, depending on 
context, prompting, and the like.
But to focus on the most 
constructive reply: it seems 
that more unified LLMs are 
possible. One important genre is 
the agent model (or person 
model or creature model) which 
attempts to model a single agent.

One way to do that, in systems 
such as Character.AI, is to take a
generic LLM and use fine-tuning 
or prompt engineering using text
from one person to help it 
simulate that agent.
ChatGPT
Certainly, the idea of unified 
agency as a requirement for 
consciousness is complex, and it 
raises interesting questions in 
the context of Large Language 
Models (LLMs).

The excerpts you provided 
explore several possibilities for
how a global workspace might 
function within an AI system, 
and how it might be related to 
consciousness. Whether 
consciousness necessitates a 
specific global workspace, or if 
high-capacity AI systems could 
function with extensive 
information available to many 
subsystems without overloading, 
is still an area of debate and 
exploration.

As for unified agency, the idea 
of LLMs being able to take on 
many personas without a stable 
set of goals and beliefs is a 
critical challenge. The creation 
of more unified LLMs through 
fine-tuning or prompt 
engineering, such as those in 
systems like Character.AI, might 
be a step towards a solution, but
this doesn't necessarily endow 
the system with genuine agency 
or consciousness. It merely 
simulates a consistent persona.

The question of what it would 
take for an AI system to possess 
true unified agency and 
whether that would make it 
conscious is a deeply 
philosophical issue, intersecting 
with questions about the nature
of consciousness, self, identity,
and even ethics.
The ongoing research and debate
in this area reflect how AI 
development is not just a 
technological endeavor but also 
a philosophical and ethical one. 

As technology advances, these 
questions will likely continue to
provoke serious thought and 
investigation.

A list of all of the topics we've 
discussed so far.
ChatGPT
Certainly! Here's a list of the 
topics that have been discussed 
so far:
1.Consciousness in Large 
Language Models (LLMs): The 

possibility and philosophical 
implications.
2.World Models and Self 
Models: How LLMs might 
develop robust world models 
for optimization of prediction 
error.
3.Senses and Embodiment in 
Consciousness: Arguments for 
and against the necessity of 
sensory and bodily 
consciousness.

4.Virtual Embodiment in AI: The
role of virtual reality and 
virtual bodies in cognition.
5.Memory and Recurrence in 
LLMs: The relationship 
between recurrence, memory,
and consciousness, and the 
evolution of LLMs in these 
areas.
6.Global Workspace Theory: 
Exploring how a global 
workspace might function 
within AI systems.

7.Unified Agency: The 
challenge of unified agency in
LLMs and how it might relate 
to consciousness.
8.Multimodal Extended 
Language Models: Development
of models that combine 
different modalities and 
potentially include global 
workspaces.
9.Ethical and Philosophical 
Considerations: Underlying 
the technical aspects of the 
discussion are broader 

questions about the nature of
consciousness, identity, and 
ethical considerations related
to AI development.

These topics collectively touch 
on some of the most complex and
intriguing questions in the 
intersection of AI, consciousness
studies, philosophy, and 
cognitive science.

I would like to propose the 
Substrate Independent Thinking 
Hypothesis (SITH) model of 

conscience. Under this model, 
physical objects can have memory
(existence, persistence), do 
calculations (analogue, gear 
ratios), and act as filters (think
sieves, natural selection 
effects). This indicates that 
much of what humans do is not 
conscious, because it does occur 
internally, and that humans use
embodied oscillatory muscle 
groups to enact thoughts and so 
the phonological loop, walking, 
flying, can be approximated by 

something like a series of 
mercury delay lines. Behavior is
the control of perception. The 
global workspace continuously is 
shifting and so consciousness is 
just an order of magnitude scale
difference between 
thermostats and collectively 
autocatalytic sets, which 
already display some control 
over their environment, in an 
iterative, feedback based 
cognitive disappitive structure 
approach.

ChatGPT
Your proposed Substrate 
Independent Thinking Hypothesis
(SITH) model introduces an 
intriguing perspective on 
consciousness, suggesting a more 
expansive definition that 
includes not just complex 
systems like humans but also 
much simpler physical processes.
Let's summarize the key 
components:
1.Memory through Physical 
Existence: Objects can possess

a form of memory through 
their persistence in space and
time.
2.Calculations through Physical 
Mechanisms: Things like gear 
ratios in a mechanical system 
can perform analogue 
calculations.
3.Filters through Physical 
Constraints: Physical objects 
can act as filters, guiding and 
constraining behaviors and 
interactions.

4.Embodied Oscillatory Actions: 
Actions like walking and flying
can be approximated through 
physical systems that iterate 
over time, like mercury delay
lines.
5.Behavior as Perception 
Control: This perspective 
aligns with the idea that 
organisms control their 
behavior to manage how they 
perceive their environment.
6.Shifting Global Workspace: 
Consciousness is a dynamic 

process that involves 
continuous shifts and 
adaptations, rather than a 
static state.
7.Scale Difference in 
Complexity: Consciousness is 
not restricted to highly 
complex systems but might 
exist on a continuum that 
includes much simpler 
systems.

The SITH model challenges 
traditional notions of 
consciousness by including 

physical systems that might not 
traditionally be considered 
conscious. It ties into broader 
themes of embodied cognition, 
panpsychism (the idea that all 
things have some form of 
consciousness), and non-human-
centric models of mind.
It's a novel and thought-
provoking hypothesis that would 
require careful exploration, 
experimentation, and 
philosophical inquiry to further
elucidate its implications and 

how it fits with current 
scientific and philosophical 
understanding of consciousness.

According to SITH beehives and 
termite mounds are conscious but
bees and termites are not. A 
human being or an animal is like a
bunch of infolded beehives and 
termite mounds, and the 
interstitium and hemoglobes 
function as bees or termites, 
transporting material into and 
out of local areas.

ChatGPT
The Substrate Independent 
Thinking Hypothesis (SITH) 
model's view that beehives and 
termite mounds might be 
considered conscious, while 
individual bees and termites are
not, is a provocative idea. This 
perspective emphasizes the 
collective behaviors and 
structures rather than the 
individual units.
1.Collective Consciousness: In 
this view, consciousness 

emerges from the complex 
interactions and organization 
of the collective, not the 
individuals within it. The hive
or mound as a whole exhibits 
behaviors that could be 
considered a form of cognition
or consciousness, while 
individual insects might not 
have these attributes.
2.Humans as Infolded 
Collectives: By likening a 
human or animal to a "bunch 
of infolded beehives and 

termite mounds," the SITH 
model suggests that our 
consciousness emerges from 
complex, interconnected 
subsystems working in concert.
3.Interstitium and Hemoglobes as
Transporters: Drawing 
parallels between these 
biological components and the 
transporting functions of 
bees and termites offers an 
analogy for how materials and
information are moved within 
the body, contributing to the 

overall function and 
potentially consciousness of 
the organism.

The SITH model seems to lean 
towards a form of system-level
or collective consciousness 
where the whole is greater 
than the sum of its parts. It's a 
perspective that would require 
a reevaluation of where and 
how we locate consciousness in 
both human and non-human 
systems. This hypothesis may 
find resonance with some 

theories in swarm intelligence, 
systems biology, and complex 
systems theory.
It's worth noting that this 
perspective is likely to be 
controversial and would need 
robust empirical support and 
theoretical development to gain 
acceptance within the scientific 
and philosophical communities. 
But as a conceptual framework, 
it offers a stimulating avenue 
for exploration and could lead 

to new insights into the nature 
of consciousness and agency.

Ok let's see if you understood 
it. Give a summary of the basic 
tenets and compare and contrast
it with other models.
ChatGPT
Sure! Here's a summary of the 
basic tenets of the Substrate 
Independent Thinking Hypothesis
(SITH) model, along with a 
comparison and contrast with 

other prominent models of 
consciousness.
SITH Model:
1.Collective Consciousness: 
Consciousness can emerge from
complex systems like beehives
or termite mounds, not 
necessarily from individual 
entities.
2.Consciousness as a Scale 
Difference: Consciousness is 
seen as a spectrum where 
even simple structures can 
exhibit control over their 

environment, with complex 
beings like humans being 
further along this spectrum.
3.Physical Objects as Memory and
Calculations: Physical 
structures can serve 
functions typically associated 
with cognition, such as memory 
and calculations, linking 
consciousness with physical 
processes.

Comparison with Other Models:
1.Global Workspace Theory 
(GWT):
• SITH: Emphasizes the 
collective and physical 
structures, suggesting even
non-biological systems could
be conscious.
• GWT: Focuses on a 
"workspace" in the brain 
where information becomes 
conscious by being globally 
available to various 
cognitive processes.

• Contrast: SITH takes a more 
decentralized and non-
biological view, whereas GWT
is specifically brain-
centered.

2.Integrated Information Theory
(IIT):
• SITH: Argues that 
consciousness can arise from
physical interactions and 
collective behaviors, even 
in non-human systems like 
termite mounds.

• IIT: Proposes that 
consciousness arises from 
the integration of 
information within a system 
and that even simple 
systems can have a degree 
of consciousness.
• Similarity: Both models 
recognize the potential for 
consciousness in non-human 
systems but differ in how 
they define and measure 
this.

3.Embodied Cognition:

• SITH: Aligns with the idea 
that consciousness can arise
from physical processes and
collective interactions, 
extending even to non-
living systems.
• Embodied Cognition: Stresses
the importance of the body 
and environment in shaping 
cognitive processes.
• Similarity: Both emphasize 
the role of physical 
structure and environment
but differ in scope, with 

SITH including non-biological
structures.

4.Higher-Order Thought 
Theories:
• SITH: Focuses on system-
level processes and 
collective behaviors, not 
necessarily requiring self-
reflection or higher-order
thoughts.
• Higher-Order Thought 
Theories: Emphasize the 
role of reflective self-
awareness in consciousness.

• Contrast: SITH's focus on 
collective behaviors and 
physical processes contrasts
with the introspective and 
self-aware nature of 
higher-order thought 
theories.

In summary, the SITH model 
introduces a novel view of 
consciousness, emphasizing 
collective interactions and the 
potential for consciousness in 
non-biological systems. It shares
some similarities with theories 

