Swarmbot Intelligence
Open sidebar
Swarmbot Intelligence
GPT-4
Summarize: Reclaiming AI as a theoretical tool for cognitive science 
Iris van Rooij1,2,3, Olivia Guest1,2, Federico Adolf i 4,5, 
Ronald de Haan6, Antonina Kolokolova7, and Patricia Rich8 
1Donders Institute for Brain, Cognition, and Behaviour, Radboud University, The Netherlands 
2School of Artif i cial Intelligence, Radboud University, The Netherlands 
3Department of Linguistics, Cognitive Science, and Semiotics & Interacting Minds Centre, Aarhus University, Denmark 
4Ernst Str��ngmann Institute for Neuroscience in Cooperation with Max-Planck Society, Germany 
5School of Psychological Science, University of Bristol, UK 
6Institute for Logic, Language and Computation (ILLC), University of Amsterdam, The Netherlands 
7Department of Computer Science, Memorial University of Newfoundland, Canada 
8Department of Philosophy, University of Bayreuth, Germany 
The idea that human cognition is, or can be understood as, a form of computation is a useful 
conceptual tool for cognitive science. It was a foundational assumption during the birth of 
cognitive science as a multidisciplinary i eld, with Artif i cial Intelligence (AI) as one of its 
contributing i elds. One conception of AI in this context is as a provider of computational 
tools (frameworks, concepts, formalisms, models, proofs, simulations, etc.) that support theory 
building in cognitive science. The contemporary i eld of AI, however, has taken the theoretical 
possibility of explaining human cognition as a form of computation to imply the practical fea- 
sibility of realising human(-like or -level) cognition in factual computational systems; and, the 
i eld frames this realisation as a short-term inevitability. Yet, as we formally prove herein, cre- 
ating systems with human(-like or -level) cognition is intrinsically computationally intractable. 
This means that any factual AI systems created in the short-run are at best decoys. When we 
think these systems capture something deep about ourselves and our thinking, we induce dis- 
torted and impoverished images of ourselves and our cognition. In other words, AI in current 
practice is deteriorating our theoretical understanding of cognition rather than advancing and 
enhancing it. The situation could be remediated by releasing the grip of the currently dominant 
view on AI and by returning to the idea of AI as a theoretical tool for cognitive science. In 
reclaiming this older idea of AI, however, it is important not to repeat conceptual mistakes of 
the past (and present) that brought us to where we are today. 
Keywords: artif i cial intelligence (AI), theory, explanation, engineering, cognitive science, 
computational complexity 
Introduction 
The term ���Artif i cial Intelligence��� (AI) means many things to 
many people (see Table 1). Sometimes the term ���AI��� is used 
to refer to the idea that intelligence can be recreated in artif i- 
cial systems (Russell & Norvig, 2010). Other times it refers 
toanartif i cialsystembelievedtoimplementsomeformofin- 
telligence (i.e., ���an AI���). Some claim that such an AI can only 
implement domain-specif i c intelligence, while others believe 
that domain-general or human-level AIs���also known as ar- 
tif i cial general intelligence (AGI)���can exist (Bubeck et al., 
2023; cf. Birhane, 2021). The term ���AI��� is also used to refer 
to the research and/or engineering i eld pursuing the creation 
of AI systems based on the idea that doing so is possible 
and desirable. Among the more troublesome meanings, per- 
haps, is ���AI��� as the ideology that it is desirable to replace hu- 
mans (or, specif i cally women) by artif i cial systems (Erscoi, 
Kleinherenbrink, & Guest, 2023) and, generally, ���AI��� as a 
way to advance capitalist, kyriarchical1, authoritarian and/or 
white supremacist goals (Birhane & Guest, 2021; Crawford, 
2021; Erscoi et al., 2023; Kalluri, 2020; McQuillan, 2022; 
Spanton & Guest, 2022; Stark & Hutson, 2022). Contem- 
porary guises of ���AI��� as idea, i eld, system, or ideology are 
also sometimes known under the label ���Machine Learning��� 
(ML), and a currently dominant view of AI advocates machine learning methods not just as a practical method for 
generating domain-specif i c artif i cial systems, but also as a 
royal road to AGI (Bubeck et al., 2023; DeepMind, 2023; 
OpenAI, 2023). 
One meaning of ���AI��� that seems often forgotten these days 
is one that played a crucial role in the birth of cognitive sci- 
ence2as an interdiscipline in the 1970s and ���80s. Back then, 
the term ���AI��� was also used to refer to the aim of using com- 
putational tools to develop theories of natural cognition. As 
Simon (1983, p. 27) put it 40 years ago, 
Artif i cial Intelligence has two goals. First, AI is 
directed toward getting computers to be smart 
and do smart things so that human beings don���t 
have to do them. And second, AI (sometimes 
called cognitive simulation, or information pro- 
cessing psychology) is also directed at using 
computers to simulate human beings, so that we 
can i nd out how humans work. 
This view of ���AI��� as a research i eld overlapping with psy- 
chology sees computational AI systems as theoretical tools: 
���Many early AI researchers were concerned with using com- 
puters to model the nature of people���s thinking��� (Langley, 
2006).3 
Accordingly, AI is one of the cognitive sciences (Fig- 
ure 1), and for decades there was a close dialogue between 
the i elds of AI and cognitive psychology (Forbus, 2010; 
Gentner, 2010, 2019; Miller, 2003). This is furthermore il- 
lustrated by the use of ���cognitive simulation��� and ���informa- 
tion processing psychology��� as alternative labels for ���AI,��� 
2Here, cognitive science is def i ned as the inter- or multidisci- 
plinary study of cognition. 
3See also Lighthill���s (1973) report on the state of AI research 
and McCorduck���s (2019) history. 
RECLAIMING AI3 
favoured by Simon and associates.4It is also illustrated by 
publications of cognitive psychological modelling research 
in Artif i cial Intelligence journals up to the ���90s (Anderson, 
1984; R. P. Cooper, Fox, Farringdon, & Shallice, 1996; Tha- 
gard, Holyoak, Nelson, & Gochfeld, 1990) and early 2000s 
(Thagard, 2007), with still some notable exceptions these 
days. At the turn of the millennium, however, these produc- 
tive ties between AI and psychology became severed: 
the past 20 years have seen an increasing shift 
in AI research away from concerns with mod- 
elling human cognition and a decreasing famil- 
iarity with results from psychology. What began 
as a healthy balance [...] has gradually become a 
one-sided community that believes AI and psy- 
chology have little to of f er each other. (Langley, 
2006, p. 2) 
AI qua information processing psychology was built on 
the idea that human cognition is, or can be scientif i cally un- 
derstood as, a form of computation; this view is also known 
as (minimal) computationalism5(cf. Chalmers, 2011; Diet- 
rich, 1994; Mi��kowski, 2013). Computationalism was seen to 
provide useful conceptual tools for cognitive science (Boden, 
1988, 2008; Johnson-Laird, 1988) as it af f ords explicit spec- 
if i cation of hypothesized cognitive processes and reasoning 
through the implications of such hypotheses (e.g., with math- 
ematical proofs or computer simulations). However, present- 
day AI hype and the popularity of AI as technology (Mered- 
ith Whittaker, Edward Ongweso Jr., and Sarah Myers West in 
Denvir, Yeager, & Johnson, 2023; Timnit Gebru in Marx & 
Wickham, 2023; van Rooij, 2023) and AI as a money-maker 
(Crawford, 2021) seems to leave little room for AI as a theo- 
Historical note: The idea for this paper was conceived in June 
2022 at the Lorentz Workshop ���What makes a good theory? In- 
terdisciplinary perspectives" (20���24 June 2022) when over dinner 
OG presented a def i nition of AI from (Erscoi, Kleinherenbrink, 
& Guest, 2023) to IvR (besides FA, also Laura van de Braak and 
Marieke Woensdregt were there, and we thank them for contribut- 
ing to this initial discussion). While the def i nition posed by (Er- 
scoi, Kleinherenbrink, & Guest, 2023) was perfect for that paper���s 
purposes, IvR noted it did not include the meaning of ���AI��� that is 
reclaimed in this paper. Hence, the idea for this paper was born. 
Countless discussions have followed between the co-authors to i ne 
tune the argumentation and this paper was written as a true team 
science ef f ort, bringing together all relevant expertise: AI, cognitive 
science, philosophy, psychology, history, and computational com- 
plexity. We thank the organizers for creating and fostering the cir- 
cumstances to make this all possible. This paper was completed 
more than a year after it was conceived. In that year a lot happened 
on the world stage with respect to AI/ML that in our opinion makes 
the need for this paper even greater than we had anticipated when 
we started the project. 
Philosophy 
Psychology 
Artif i cial 
Intelligence Neuroscience 
Anthropology 
Linguistics 
Figure 1. A visual depiction of the connections between the 
Cognitive Sciences. Solid lines denote stronger interdisci- 
plinary ties; and dashed lines denote weaker ones. This i gure 
is derived from the original put forth by the Sloan Foundation 
in 1978 and reproduced from Figure 4 in Pl��h and Gurova 
(2013). Dif f erent versions of it over time have used ���Artif i- 
cial intelligence��� (as above) instead of ���Computer Science��� 
and vice versa (cf. Miller, 2003).
1 / 1
ChatGPT
ChatGPT
The paper titled "Reclaiming AI as a theoretical tool for cognitive science" by Iris van Rooij and others delves into the evolving relationship between AI and cognitive science. It emphasizes the original conceptual role of AI as a provider of computational tools that support theory building in cognitive science. The authors lament how contemporary AI has shifted from understanding human cognition as a computational process to the pursuit of creating human-like cognition in machines. They argue that this shift has deteriorated the theoretical understanding of cognition.
The authors formally prove that creating systems with human-like cognition is inherently computationally intractable, meaning that any short-term AI systems mimicking human cognition are mere decoys. This can lead to distorted perceptions of human thought processes.
The paper urges a return to the idea of AI as a theoretical tool for cognitive science, emphasizing the need to avoid past conceptual mistakes. They reflect on the historical context, noting that AI was once closely linked with cognitive psychology and other cognitive sciences but has diverged in the past 20 years, focusing more on engineering and less on modeling human cognition.
In essence, the authors call for a reevaluation of the relationship between AI and cognitive science, seeking to reestablish AI as a means to explore and understand human cognition rather than merely attempting to replicate it in machines.
1 / 1
Table 1 
A non-comprehensive list of dif f erent (not mutually exclusive) meanings of the word AI, including AI as idea, AI as a type of 
system, AI as a i eld of study, and AI as institution(al unit). 
TypeDescriptionLabel 
Idea 
Intelligence can be recreated in artif i cial systems.AI-as-engineering 
Cognition is, or can be understood as, a form of computation.AI-as-psychology (a.k.a. computationalism) 
Humans can be replaced by artif i cial systems.AI-as-ideology 
The label ���AI��� helps to sell technologies and gain funding.AI-as-marketing 
System 
A system believed to implement (simulate) a form of cognition.cognitive system (model) 
A system believedto perform (solve) domain-specif i c cognitive 
tasks (problems). narrow AI 
A system believed to perform (solve) domain-general cognitive 
tasks (problems; what some may also call AGI). general AI or AGI 
A system believed to realize human-level cognition (what some 
may also call AGI). human-level AI 
Field 
A (sub)f i eld pursuing the creation of domain-specif i c AI sys- 
tems. 
e.g. Bayesian Networks, Decision Support 
Systems, Machine Learning, Robotics 
A (sub)f i eld pursuing the creation of AGI.AGI 
A (sub)f i eld using AI as an idea to build theories. e.g., (computational) cognitive science, cog- 
nitive simulation, weak AI 
A i eld def i ned by a collection of i elds that each are considered 
to be an AI subf i eld. the i eld of AI broadly construed 
History 
A history of practices ref l ecting dif f erent ideas of AI, resulting 
in the pursuit of dif f erent kinds of AI systems, and dif f erent 
kinds of AI-as-f i eld concepts. 
named to match practices, e.g., ML-AI, neu- 
roAI, etc. 
Unit An organisational or institutional unit going under the label AI. 
named to match type of units, e.g. AI re- 
search group, AI department, AI centre, AI 
network
1 / 1
ChatGPT
ChatGPT
The paper titled "Reclaiming AI as a theoretical tool for cognitive science" discusses the transformation of the field of Artificial Intelligence (AI) and its impact on cognitive science. Initially, AI was considered a valuable tool for understanding human cognition, forming a foundational aspect of cognitive science. It was seen as a provider of computational tools that aided theory building in the field.
Over time, however, the field of AI has shifted its focus towards creating human-like cognition in computational systems. This paper argues that achieving human-level cognition is computationally intractable, and the modern direction of AI has led to distorted and misleading views of human cognition.
The authors advocate a return to the original idea of AI as a theoretical tool for cognitive science, being careful not to repeat past mistakes. They highlight the diverse meanings of AI, including the idea of recreating intelligence, the use of AI as a label for marketing, and various types of AI systems such as narrow AI, general AI, and human-level AI.
In essence, the paper calls for a reevaluation of AI's role, focusing more on the original, theoretical insights it can provide into human cognition, rather than its current, more commercial and practical applications. It underscores the importance of reclaiming AI as a field overlapping with psychology and using computational tools to develop theories of natural cognition. The provided table categorizes different meanings of AI, clarifying its multifaceted nature.
1 / 1
retical tool for cognitive science. Worse even, the products of 
present-day AI-as-engineering are sometimes believed to in- 
stantiate (parts of) minds. Besides the various psychological, 
social, cultural and political problems posed by this confu- 
sion (Bender, Gebru, McMillan-Major, & Shmitchell, 2021; 
Birhane & van Dijk, 2020; Erscoi et al., 2023; J. Hughes, 
2021; Thrall et al., 2018; Vallor, 2015; van der Gun & Guest, 
2023; Wood, 1987), here we wish to focus on how this prac- 
tice creates distorted and impoverished views of ourselves 
and deteriorates our theoretical understanding of cognition, 
rather than advancing and enhancing it. 
In this paper, we wish to remedy the above situation in 
two steps. First, we set out to release the grip of the cur- 
rently dominant view on AI (viz., AI-as-engineering aiming 
at a human-level AI system, Table 1). This practice has taken 
the theoretical possibility of explaining human cognition as 
a form of computation to imply the practical feasibility of 
realising human(-like or -level) cognition in factual compu- 
4See also ���computational psychology��� as used by Boden (1988) 
and ���theoretical psychology��� as used by Longuet-Higgins (H��ne- 
feldt & Brunetti, 2004). 
5Importantly, computationalism is to be conceptually distin- 
guished from computerism, cognitivism or any commitments to 
specif i c computational architectures (see e.g. Chalmers, 2011; Di- 
etrich, 1994). 
4VAN ROOIJ ET AL. 
tational systems; and, it is framing this realisation as a short- 
term inevitability. In this paper, we undercut these views and 
claims by presenting a mathematical proof of inherent in- 
tractability (formally, NP-hardness) of the task that these AI 
engineers set themselves. This intractability implies that any 
factual AI system created in the short-run (say, within the 
next few decades or so) is so astronomically unlikely to be 
anything like a human mind, or even a coherent capacity 
that is part of that mind, that claims of ���inevitability��� of AGI 
within the foreseeable future are revealed to be false and mis- 
leading. 
Second, we propose a way to return to the idea of AI as 
a theoretical tool without falling in the trap of confusing our 
maps for the territory. The return must be such that we do 
not retrace the trajectory that led us where we are today. To 
halt the rerun of history, we think it is vital that the idea 
of cognition as computation���and therefore the in-principle 
possibility of realising and/or explaining cognition as a form 
of computation���is not mistaken for the practical feasibility 
of replicating human minds in machines (which we prove 
is not feasible). The reader may think that this is a contra- 
diction, i.e., that then computationalism is theoretically in- 
ert. But that is mistaken. Computationalism can provide both 
explanatory challenges for and computational constraints on 
cognitive explanations���using formal, conceptual, and math- 
ematical analysis���and hence is theoretically informative. 
Overview 
Flying pigs are also possible in principle; pos- 
sible in principle bakes no bread. 
��� Jerry Fodor (2005, p. 27) 
The remainder of this paper will be an argument in ���two 
acts���. In ACT 1: Releasing the grip we present a formalisa- 
tion of the currently dominant approach to AI-as-engineering 
that claims that AGI is both inevitable and around the corner. 
We do this by introducing a thought experiment in which a 
i ctive AI engineer, Dr. Ingenia, tries to construct an AGI un- 
der ideal conditions.6For instance, Dr. Ingenia has perfect 
data, sampled from the true distribution, and they also have 
access to any conceivable ML method���including presently 
popular ���deep learning��� based on artif i cial neural networks 
(ANNs) and any possible future methods���to train an algo- 
rithm(���anAI���).Wethenpresentaformalproofthattheprob- 
lem that Dr. Ingenia sets out to solve is intractable (formally, 
NP-hard; i.e., possible in principle but provably infeasible; 
see the section Ingenia Theorem). We also unpack how and 
why our proof shows that the AI-as-engineering approach is 
a theoretical dead-end for cognitive science. In ACT 2: Re- 
claiming the AI vertex, we explain how the original enthu- 
siasm for using computers to understand the mind ref l ected 
many genuine benef i ts of AI for cognitive science, but also 
a fatal mistake. We conclude with ways in which ���AI��� can 
be reclaimed for theory-building in cognitive science without 
falling into historical and present-day traps. 
ACT 1: Releasing the grip 
There is no doubt that [AI] is currently in the 
process of rapid hill-climbing. Every year, states 
of the art across many [AI] tasks are being im- 
proved[, but] the question is whether the hill we 
are climbing so rapidly is the right hill. 
��� Emily M. Bender and Alexander Koller 
(2020, p. 5191) 
At present, the i eld of AI is in the grip of a dominant 
paradigm that pushes a narrative that AI technology is so 
massively successful that, if we keep progressing at our cur- 
rent pace, then AGI will inevitably arrive in the near future. 
Some multi-million AI companies have even gone so far as 
to claim that we are not only climbing a hill towards AGI, 
but even on the verge of creating AGI, whether we want to 
or not, if we proceed with current ML approaches to AI-as- 
engineering. These kinds of claims may be hard to ignore or 
counter if anything that is possible in principle also seems 
possible in practice. However, not everything that is possible 
in principle is possible in practice. 
In this i rst Act, we reveal why claims of the inevitability 
ofAGIwalkonthequicksandofcomputationalintractability. 
The main character on the stage is Dr. Ingenia, a i ctive AI 
engineer, who is pursuing the kind of ML approach claimed 
to inevitably lead to AGI. By studying the engineering task 
that they have set themselves through a formal, mathematical 
lens we are able to construct a proof of intractability. We then 
draw out its implications.
1 / 1
ChatGPT
ChatGPT
The excerpt provided delves further into the complex relationship between AI and cognitive science, expressing concerns over the current direction of AI and its implications on our understanding of cognition.
Problem of Current Direction
: The paper recognizes that the current focus of AI on replicating human cognition (AI-as-engineering) is problematic. This direction has led to distorted and impoverished views of human cognition, and the belief that AI systems can instantiate minds is highlighted as particularly concerning.
Mathematical Intractability
: The authors challenge the prevailing notion of AGI (Artificial General Intelligence) as an inevitable short-term outcome. By presenting a mathematical proof of the task's inherent intractability (formally, NP-hardness), the paper undermines claims that AGI is within reach in the foreseeable future.
Returning to Original Understanding
: The paper proposes a return to the idea of AI as a theoretical tool for cognitive science without falling into the trap of confusing theoretical possibility with practical feasibility. Computationalism, the concept that cognition can be understood as a form of computation, is argued to still hold theoretical value, providing challenges and constraints on cognitive explanations.
Two-Act Structure
: The paper's argument is presented in "two acts." The first act aims to release the grip of the currently dominant AI-as-engineering approach, using a thought experiment involving a fictional AI engineer, Dr. Ingenia, and a formal proof to show the intractability of AGI. The second act will focus on reclaiming the value of AI for theory-building in cognitive science without repeating historical mistakes.
Questioning Current Progress
: The excerpt also highlights a questioning of the current trajectory in AI, where rapid advancements in the field may not necessarily be leading in the right direction. This skepticism is tied to the broader concerns of the paper about the practical feasibility of replicating human minds in machines and the misguided beliefs around AGI.
Overall, the excerpt outlines a critical assessment of modern AI's direction and its implications, calling for a return to a more theoretical and insightful understanding of AI's role in cognitive science. It does this by engaging in a detailed mathematical and philosophical exploration of the subject, challenging prevailing ideas and proposing a revised perspective.
1 / 1
Formalising AI-by-Engineering 
Give a rigorous, computationally detailed and 
plausible account of how learning can be done. 
Translation: Rigorous: theorems, please. 
��� Dana Angluin (1992, p. 351) 
Imagine a i ctive engineer, called Dr. Ingenia, who is try- 
ing to build human-like or -level AI (or AGI)7using the cur- 
rently dominant approach of ���machine learning��� (in short, 
���ML���) under highly idealised and optimal conditions (Fig- 
ure 2). Dr. Ingenia is able to repeatedly sample from a dis- 
tribution D. Here, D captures possible behaviours b that hu- 
mans may display in dif f erent possible situations s. Dr. Inge- 
nia can use the information gathered by repeatedly sampling 
situation-behaviour pairs from D to try to build (or ���train���) 
6This analytical strategy takes inspiration from Rich et al. 
(2021). 
7Here and throughout, we use ���AGI��� in the sense of (domain- 
general) human-like or -level AI (cf. Table 1). 
RECLAIMING AI5 
Figure 2. A visual illustration of the hypothetical scenario 
and its formalisation: Dr. Ingenia has access (magically and 
at no cost) to any machine learning method M, present or 
future, and by repeatedly sampling data D from the distri- 
bution D they can use whatever M they like to create a 
program LAthat when implemented and run generates be- 
haviours b = A(s) when prompted by dif f erent situations s. 
The goal is to generate with non-negligible probability ��(n) 
an algorithm A that behaves (approximately) human-like, in 
the sense that A is non-negligibly (��(n)) better than chance 
at picking behaviours that are possible for s in D. Here, n 
is a measure of the situation complexity, i.e., the maximum 
length of strings (|s|) needed to encode the relevant informa- 
tion in the situations s. 
an algorithm. Their goal is to make an algorithm that behaves 
(approximately) like D. Informally, we can characterise the 
problem that Dr. Ingenia sets themselves, as follows: 
AI-by-Learning (informal) 
Given: A way of sampling from a distribution D. 
Task: Find an algorithm A (i.e., ���an AI���) that, 
when run for dif f erent possible situations as in- 
put, outputs behaviours that are human-like (i.e., 
approximately like D for some meaning of ���ap- 
proximate���). 
Inourhypotheticalscenario,wewillbegrantingcomputa- 
tionalism; i.e., we will not be challenging the assumption that 
cognition is a form of computation. Hence, Dr. Ingenia can 
safely assume that D is generated by a computational pro- 
cess, i.e., either by the computational processes that def i ne 
a single individual���s cognition, or the cognition of a i nite 
collection of human beings. This also means that there exists 
an algorithm A that can approximate the distribution, namely, 
the algorithm that generates D. But there may also be many 
more algorithms that deviate in some way from human cog- 
nition but whose behaviour is still suf i ciently human-like. 
Because our aim will be to assess the intrinsic hardness 
of Dr. Ingenia���s machine learning problem���independent of 
extraneous factors���we will be granting them several ide- 
alised conditions. For instance, in our hypothetical scenario, 
Dr. Ingenia will have access to perfect data for training their 
AI system. The data have no measurement error, nor are 
the data contaminated by irrelevant details or sampling bias 
or any other distortion on the data that make it indirect or 
imperfect. Note that the real-world situation is far removed 
from this idealized scenario. In contemporary ML, the pre- 
dominant protocol is to train ML algorithms on data which 
is typically decontextualized and scraped from the internet 
(Birhane, Prabhu, Han, & Boddeti, 2023; Birhane, Prabhu, & 
Kahembwe, 2021; Lee, Le, Chen, & Lee, 2023; Liesenfeld, 
Lopez, & Dingemanse, 2023). Moreover, real AI engineers 
do not have perfect knowledge of which cognizer contributed 
which data point, of which exact situation induced which be- 
haviour, or of how behaviour is to be parsed (cf. ���the segmen- 
tation problem���; Adolf i , Wareham, & van Rooij, 2022, 2023) 
or interpreted (cf. ���theory-ladeness of data���; Andrews, 2023; 
Guest & Martin, 2021; R. I. Hughes, 1997). 
By granting Dr. Ingenia these highly idealised conditions 
that simplify and abstract away from real-world complica- 
tions, we can formally derive a reliable lower-bound on the 
real-world complexity of constructing human-like AI from 
human data. To do so, we need to make the problem AI-by- 
Learning formally precise such that it becomes amenable to 
computational complexity analysis (Arora & Barak, 2009; 
Garey & Johnson, 1979; van Rooij, Blokpoel, Kwisthout, & 
Wareham, 2019). We will do this next. 
We assume that Dr. Ingenia expresses candidate algo- 
rithms A using a specif i cation language, LA. 
Any particular 
algorithm A ��� A can be described with a program LA��� LA. 
The specif i cation language LAcan 
be thought of as a pro- 
gramming language with the constraint that it specif i es only 
those algorithms in a class A that the engineer assumes is 
suitablefordesigninghuman(-likeor-level)AI.Forinstance, 
A could be the class of artif i cial neural networks (ANNs) or 
any other class of algorithms that Dr. Ingenia deems suf i- 
cient to approximate human cognition.
1 / 1
ChatGPT
ChatGPT
It looks like the text you've provided is discussing a hypothetical scenario involving a fictional engineer named Dr. Ingenia. Dr. Ingenia is attempting to build human-like or human-level Artificial General Intelligence (AGI) using current machine learning approaches, under highly idealized and optimal conditions.
The thought experiment is set up to examine the intrinsic difficulty of the AI-by-Learning problem, independent of extraneous factors. Dr. Ingenia's goal is to find an algorithm that can mimic human behavior, given a way of sampling from a distribution
D
D
D
that captures possible human behaviors in different situations.
The scenario grants computationalism, which means it doesn't challenge the assumption that cognition is a form of computation. Thus, it is assumed that
D
D
D
is generated by a computational process, and there exists an algorithm that can approximate this distribution.
In this idealized setting, Dr. Ingenia has access to perfect data for training the AI system. These data have no errors, biases, or distortions, simplifying the real-world situation. By abstracting away real-world complications, the authors aim to derive a reliable lower-bound on the real-world complexity of constructing human-like AI from human data.
The formalization assumes that Dr. Ingenia can use a specific language,
L
A
LA
L
A
, to express candidate algorithms. The specific language can be thought of as a programming language constrained to specify algorithms within a class
A
A
A
, which might include artificial neural networks (ANNs) or other suitable algorithms for approximating human cognition.
Overall, the text seems to be setting up a theoretical framework to critically assess the claim that building AGI is a near-term inevitability. It's focusing on the complexities and challenges involved, even under highly favorable conditions.
1 / 1
We will add some minimal constraints on LA. 
We exclude 
classes of trivial algorithms that have no chance of captur- 
ing human-like or -level cognition. Specif i cally, we impose 
the constraint that LAcan minimally express feedforward 
neural networks, logical circuits, or i nite state machine- 
equivalent class of algorithms. Scientif i cally, we think that 
a stronger assumption would be warranted, namely that LA 
is in principle expressive enough to be Turing complete; e.g., 
LAcan express Turing machines (Turing, 1950) or other- 
wise Turing-equivalent algorithms, including certain (highly 
idealised) recurrent neural networks (Siegelmann & Sontag, 
1992; cf. P��rez, Marinkovi�� c, & Barcel��, 2019; Weiss, Gold- 
berg, & Yahav, 2018). This stronger assumption would en- 
6VAN ROOIJ ET AL. 
sure that LAis expressive enough to computationally cap- 
ture human cognition. It would be a reasonable assumption 
because cognition is generally assumed to have two prop- 
erties, known as productivity (i.e., people can in principle 
generateinf i nitelymanydistinctthoughts,sentences,images, 
etc., Fodor, 2005; Fodor & Pylyshyn, 1988) and Turing- 
completeness or -equivalence (the ability to compute any 
computable function, aided by pen and paper, in principle; 
Turing, 1950; Wells, 1998). We nonetheless work with the 
more modest assumption. 
We will furthermore allow (but not impose) the as- 
sumption that all A expressible by LAare 
computationally 
tractable (i.e., can be run on any situation s in polynomial 
time, O(nc), where n is some measure of the input size (|s|) 
and c is a constant). This constraint ensures that any in- 
tractability results we may derive for Dr. Ingenia���s AI-by- 
Learning problem are not an artefact of the time-complexity 
of running the algorithm A itself. Moreover, it ensures that 
even if human cognitive computations are all tractable (cf. 
���the tractable cognition thesis���,van Rooij, 2008; see also 
Frixione, 2001), our intractability results for AI-by-Learning 
would still hold. 
We assume in our formalisation that whenever Dr. Inge- 
nia tries to solve an instance of AI-by-Learning and searches 
for a program LAthat encodes a human (-like or -level) A, 
there is a given upper bound K on the size of the program 
that they can in principle encode (i.e., |LA| ��� K). One can 
think of K as expressing the total amount of space (computer 
memory) that Dr. Ingenia has available to store a program. 
AI models these days can be very large, and we allow for 
their size (e.g., an ANN may have tens of millions of param- 
eters; Krizhevsky, Sutskever, & Hinton, 2012). Nonetheless, 
they are still bounded by some size K. Dr. Ingenia can buy 
more space to work with, in which case they will have a new 
K���> K to work with. AI engineers that claim to be able 
to create AGI with ML are (implicitly) assuming that their 
approach can work for larger and larger K���. 
We formalise the distribution D as follows. A dataset 
D drawn from D consists of a list of situation-behaviour 
pairs,a.k.a.���samples���:(s1,b1),(s2,b2),(s3,b3),...,(s|D|,b|D|). 
Without loss of generality, we model this structure with 
binary strings (e.g., s=101010101000001 and b= 
010101111). For any given input distribution Dn, there is 
an upper bound n on the description length of situations. 
In other words, the set of situations for such an instance 
of the problem is def i ned as S = {0,1}n. 
For each situa- 
tion s ��� S, there are some appropriate (i.e., human-like) be- 
haviours Bs��� B = {0,1}m, 
for some i xed m. 
Lastly, we formalise the notion of ���approximate��� in Dr. 
Ingenia���s AI-by-Learning problem. Recall that we are trying 
to estimate a lower-bound on the real-world problem of cre- 
ating human-like or -level AI by ML, and therefore give Dr. 
Ingenia ���easier��� conditions than may apply in the real world. 
To this end, we will be setting an extremely low bar for what 
counts as ���approximate���. On the one hand, we do not expect 
a guarantee that Dr. Ingenia succeeds, but merely that Dr. 
Ingenia succeeds with non-negligible probability (denoted 
��(n), where n is a measure of the size/complexity of situa- 
tions; see previous paragraph). Moreover, the performance of 
the found A need not have high degrees of human-likeness, 
butmerely shouldperform human-likewith aprobabilitythat 
is non-negligibly higher than chance level. Specif i cally, in 
our formalisation of AI-by-Learning, we will make the sim- 
plifying assumption that there is a i nite set of possible be- 
haviours8and that for each situation s there is a i xed num- 
ber of behaviours Bsthat humans may display in situation s. 
Then|Bs|/|B|expresses chance level, and|Bs|/|B|+��(n) expresses 
���non-negligibly better than chance���. 
Given the above considerations, we can now state a for- 
malised version of AI-by-Learning: 
AI-by-Learning (formal) 
Given: An integer K and a way of sampling from 
a distribution D. 
Task: Find a description LA��� LA, 
with length 
|LA| ��� K, of an algorithm A ��� A that with prob- 
ability ��� ��(n), taken over the randomness in the 
sampling, satisf i es: 
Pr 
s���Dn [A(s) ��� Bs] ���|Bs|/|B|+ 
��(n). 
Here ��(n) and ��(n) are arbitrary non-negligible 
functions. A function f is non-negligible if there 
issomed suchthatforsuf i cientlylargen, f(n) ��� 
1/nd.
1 / 1
ChatGPT
ChatGPT
The scenario you've described is an imaginative thought experiment that sets up a formal framework for tackling the problem of creating human-like or -level artificial general intelligence (AGI) through machine learning (ML). This scenario has posed the challenge of "AI-by-Learning" with several critical constraints, assumptions, and definitions.
Here's a summary of the key elements:
The Setting
: Dr. Ingenia is a fictitious engineer tasked with building AGI using ML. The problem is presented in highly idealized conditions, making the task as easy as possible within the bounds of computational theory.
Sampling Distribution D
: Represents the possible behaviors (b) humans may display in different possible situations (s). Dr. Ingenia can sample situation-behavior pairs from this distribution.
Specification Language LA
: Specifies algorithms A using a language LA that must minimally express certain computational structures like feedforward neural networks, logical circuits, or finite state machines. The language may also be Turing complete.
Constraints and Assumptions
: Several constraints and assumptions are added, such as the computational tractability of A, an upper bound K on the size of the program, and the boundedness of the description length of situations.
Formal Definition of the Problem
: AI-by-Learning (formal) lays out the precise challenge faced by Dr. Ingenia. The goal is to find an algorithm A that, with a non-negligible probability, behaves in a human-like manner for given situations. The task includes specific mathematical expressions to capture what it means to perform "non-negligibly better than chance."
The way the problem is framed makes it suitable for complexity analysis, which could shed light on the intrinsic hardness of building AGI using current ML approaches. It highlights the notion that, even under ideal conditions, building AGI might not just be a matter of gathering more data or creating more complex models but might have fundamental computational complexity challenges.
Note: This scenario is purely hypothetical and forms a part of a thought experiment rather than a real-world application or established theory. It does raise interesting questions about the limitations and possibilities of building AGI, contributing to ongoing discussions in the AI community.
1 / 1
Ingenia Theorem 
[E]ven with a whole row of the largest imagin- 
able computers to help, all the potential distri- 
butional potentialities of a whole national lan- 
guage cannot possibly be found in any i nite 
time[.] 
��� Margaret Masterman (1965, p. iv-19) 
In this section we present a formal proof that AI- 
by-Learning is intractable. For this we use the following 
decisionproblemcalledPerfect-vs-Chance.9WhilePerfect- 
vs-Chance is a somewhat unnatural problem, with no direct 
8Given the assumption of productivity of mind (Fodor, 2005; 
Fodor & Pylyshyn, 1988), this is a gross underestimation of human 
potential. 
9This problem was introduced and proven intractable by Shuichi 
Hirahara (2022), but they did not give it a name. We have chosen a 
name that suf i ces for our purposes and so as to optimise intuitive- 
ness. 
RECLAIMING AI7 
real-world analogue, it is useful for our purposes. Bear with 
us. It will all make sense in a moment. 
Perfect-vs-Chance (decision problem) 
Given: A way to sample a given distribution D 
over {0,1}n��{0,1}, an integer k, and the promise 
that one of the following two cases apply: 
1. There is an ef i cient program M of size at 
most k such that Pr(x,y)���D[M(x) = y] = 1 
2. For any program M of size at most k, 
Pr(x,y)���D[M(x) = y] ��� 1/2 + 1/2n 
1����� 
where 0 < �� < 1 is an arbitrary constant. 
Question: Is (1) or (2) the case? 
In the decision problem Perfect-vs-Chance, the expres- 
sion 1/2n 1����� can informally be read as ���negligible probabil- 
ity���, since the denominator grows very fast. Hence, infor- 
mally the two cases listed in the Perfect-vs-Chance problem 
are (1) there exists a perfect and ef i cient program or, other- 
wise, (2) there exists no program that can work better than 
chance, even inef i ciently. 
Note that in the Perfect-vs-Chance problem it is 
promised, before trying to solve the problem, that for the 
given distribution either case (1) or (2) holds. One may intuit 
that, since (1) and (2) are extreme cases that are very clearly 
distinct, telling these two cases apart should be easy to do. 
However, it is not easy. In fact, it is provably intractable to 
i nd out in which of the two cases one i nds oneself. This 
follows from a proof by (Hirahara, 2022). 
Theorem 1 (Hirahara, 2022). Perfect-vs-Chance is in- 
tractable.10,11 
We will use Theorem 1 to prove that AI-by-Learning is 
intractable, too. Specif i cally, we will show that if it were 
possible to tractably solve AI-by-Learning, then it would 
also be possible to tractably solve Perfect-vs-Chance. Since 
Perfect-vs-Chance is known to be intractable (Theorem 1), 
by modus tollens,12it follows that AI-by-Learning must be 
intractable as well. In other words, the proof will be by con- 
tradiction. 
Theorem 2 (Ingenia Theorem). AI-by-Learning is in- 
tractable. 
Proof (sketch). For full details of the proof we refer the 
reader to the Appendix. In this proof sketch we present the 
main proof idea. See Figure 3 for an illustration. 
We prove by contradiction. Suppose that there exists 
a learning mechanism M that solves AI-by-Learning in 
polynomial time. We will show that then there exists a 
polynomial-time bounded-error probabilistic algorithm that 
solves Perfect-vs-Chance. 
Figure 3. A visual illustration of the core proof idea for 
Theorem 2 (Ingenia Theorem): It is known that Perfect- 
vs-Chance is an intractable decision problem. If a tractable 
method M for solving AI-by-learning would exist, then we 
could use M to solve Perfect-vs-Chance tractably, by plug- 
ging it into a sampling-plus-decision procedure that is itself 
tractable, easy to construct, and together with M would prov- 
ably solve Perfect-vs-Chance. This yields a contradiction. 
Therefore we can conclude that AI-by-learning is intractable 
as well. See the main text for more information and the Ap- 
pendix for full proof details.
1 / 1
ChatGPT
ChatGPT
The text you provided outlines the Ingenia Theorem, which states that AI-by-Learning is intractable. This conclusion is reached through the examination of a related decision problem called Perfect-vs-Chance, which is proven to be intractable.
The Perfect-vs-Chance problem is framed around determining whether there exists a perfect and efficient program for a given distribution or if no program can work better than chance.
The Ingenia Theorem posits that if there were a tractable way to solve AI-by-Learning, it would also make it possible to solve Perfect-vs-Chance tractably. But since Perfect-vs-Chance is known to be intractable, AI-by-Learning must be intractable as well.
This conclusion has implications for the fields of artificial intelligence and machine learning, shedding light on the challenges and limits of developing human-like or human-level AI through learning algorithms.
1 / 1
The algorithm for Perfect-vs-Chance works as follows. 
Take an arbitrary instance for Perfect-vs-Chance, consisting 
of integers n and k and a distribution D over {0,1}n�� {0,1}. 
The algorithm will use a subroutine that simulates the learn- 
ing mechanism M, where K = k and where the data that the 
mechanismMhassamplingaccesstoisgivenbythedistribu- 
tion D. In this simulation, the set of situations is S = {0,1}n 
and the set of behaviours is B = {0,1}. The simulation will 
yield an algorithm A, that might or might not perform well 
on freshly sampled situations. 
After running the subroutine that simulates M, the algo- 
rithm will evaluate the quality of the resulting learned algo- 
10Formally, NP-hard under randomised polynomial-time one- 
query reductions. See (Hirahara, 2022) for more details. The re- 
duction proves that the problem is not tractable (i.e., computable in 
polynomial time) unless NP ��� BPP. It is widely conjectured that 
NP ��� BPP (see, e.g., Arora & Barak, 2009, Chapter 7). See the 
Appendix for more details. 
11There are no other constraints on M than specif i ed in the def- 
inition of the problem Perfect-vs-Chance. M can be thought of 
as a Turing Machine. This is without loss of generality because, 
assuming the Invariance Thesis (van Emde Boas, 1990), a Turing 
machine can simulate any other computational machine with only 
polynomial-time overhead. 
12If P then Q. And ��Q. Therefore, ��P. Or, more formally, 
(P ��� Q,��Q) ��� ��P. 
8VAN ROOIJ ET AL. 
rithm A by using additional samples from D and counting 
the number of situations s in which the algorithm A returns 
an appropriate behaviour b ��� Bs. 
The algorithm runs the simulation subroutine several 
times, and for each run of the subroutine, it evaluates the re- 
sulting learned algorithm A. From all of these runs, it picks 
the algorithm A���that performed best in the quality eval- 
uation. Based on how well the best learned algorithm A��� 
performs, the algorithm will give an answer for the input 
of Perfect-vs-Chance. If A���performs non-negligibly better 
than chance, then the algorithm will answer Yes, and other- 
wise, the algorithm will answer No. 
By having the algorithm run the simulation a large enough 
(yet polynomial) number of times, and testing the output of 
each simulation a large enough (yet polynomial) number of 
times with new samples, we can ensure that the algorithm 
outputs a correct answer for Perfect-vs-Chance with high 
probability.��� 
Implications 
It is desirable to guard against the possibility 
of exaggerated ideas that might arise as to the 
powers of [AI]. In considering any new subject, 
there is frequently a tendency [...] to overrate 
whatwef i ndtobealreadyinterestingorremark- 
able[.] 
��� Augusta Ada King, Countess of Lovelace 
(personal correspondence, July, 1843; Toole et 
al., 1998, p. 186) 
In the previous section we presented a proof that Dr. In- 
genia set themselves a machine learning problem for which 
no tractable method exists or can exist. The ���intractability��� 
means that, even if the problem may be practically solv- 
able for trivially simple situations (small n), any attempts to 
scale up to situations of real-world, human-level complexity 
(medium to large n) will necessarily consume an astronom- 
ical amount of resources (such as time and number of sam- 
ples; see Box 1 for an illustration). 
The proven intractability holds for the highly simplif i ed 
and idealised model of AI-by-Learning that grants Dr. Inge- 
nia much better conditions than apply for AI engineers in the 
real world. As explained in the section Formalising AI-by- 
Engineering, the intractability result holds even if Dr. Ingenia 
(a) can sample randomly and unbiasedly, if they so wish; (b) 
has data which are noiseless, without error, and uncontami- 
nated; (c) is free to use any means or methods for producing 
the AI (this can include present and future machine learning 
approaches, but is not limited to them); (d) is only required to 
produce with a probability slightly higher than chance an AI 
that matches human behaviour slightly better than chance; 
(e) is guaranteed that there exists an algorithm that meets 
those low-bar requirements. Idealizations (a)���(e) make clear 
that the computational complexity of Dr. Ingenia���s problem 
is a gross underestimation of the true complexity of the much 
more messy real-world AI-by-Learning problem. 
Given the proof nature of the complexity-theoretic result, 
the claim that we are presently on a fast track to inevitably 
produce human-like and -level AI poses a logical contradic- 
tion. Let us unpack why this is so: While there are many 
claims that might reasonably be based on intuitions, any 
claim of the inevitability of producing any desired object (or 
event, or state of af f airs) requires a tractable procedure as a 
precondition. To see this, note that, without loss of general- 
ity, we can cast the problem of building human-like AI as 
that of searching for such an object in some space of pos- 
sibilities. To claim that the search for this object is bound 
to succeed in practice is to minimally claim that one has a 
tractable procedure for conducting the search that provably 
i nds the object if it exists (i.e., one has a way of performing 
the search in a realistic amount of time). That is, to support 
the inevitability claim one would have to put forth a set of 
arguments, logically and mathematically sound, to prove not 
only that such a tractable procedure can exist, but also that 
one has it. Theorem 2 (the Ingenia Theorem) shows that this 
is impossible.
1 / 1
ChatGPT
ChatGPT
The algorithm provided here gives a detailed explanation of how Perfect-vs-Chance works and underscores the proof that AI-by-Learning is intractable. This intractability exists despite making generous assumptions about the conditions in which the AI learning takes place, such as unbiased sampling and noise-free data.
The essence of the algorithm is as follows:
Simulate the learning mechanism M
: The algorithm runs a subroutine that simulates a learning mechanism M with specific parameters, and this simulation yields an algorithm A that may or may not perform well.
Evaluate the quality of the learned algorithm
: After running the simulation, the algorithm evaluates the resulting algorithm A's quality using additional samples from distribution D.
Choose the best-performing algorithm
: The algorithm runs the simulation several times, evaluates each resulting algorithm, and picks the best-performing one, A���.
Decide Yes or No
: If A��� performs non-negligibly better than chance, the algorithm answers Yes; otherwise, No. By running the simulation a sufficient number of times and testing the output with enough samples, the algorithm can output the correct answer with high probability.
The implications of this proof are profound. It emphasizes the limits of AI, even under idealized conditions, and argues against claims of inevitable success in creating human-like AI. The Ingenia Theorem shows that this is logically and mathematically impossible, as even under extremely favorable conditions, scaling AI to real-world, human-level complexity is shown to be intractable.
In essence, the Ingenia Theorem adds a layer of caution and realism to the ambitious field of AI, underlining the significant challenges that remain in achieving human-level artificial intelligence.
1 / 1
Given the Ingenia Theorem, how should we interpret what 
is happening in practice? In practice, AIs are being continu- 
ously produced which are claimed to be either human-like 
and human-level AI or inevitably on a path leading there. 
Any AIs produced in practice, however, are produced ei- 
ther by tractable procedures, or by cutting short a procedure 
that would run longer (for an unfeasible amount of time). 
Hence, the produced AIs necessarily fail to solve the in- 
tractable learning problem. Concretely, this means that they 
make lots of errors���deviating substantially from human be- 
haviour (e.g., Bowers et al., 2022)���and fail to meet the 
low standard set in the Ingenia Theorem (see also the list of 
simplif i cations and idealisations (a)���(e) above). These errors 
cannot be contained to be small, and no matter how impres- 
sive the produced AIs may appear, they fail to capture the 
distribution of human behaviour even approximately. 
We realise that the implications that we have drawn out 
from our complexity-theoretic results may appear to contra- 
dict both intuition and experiences with existing AIs. How- 
ever, the pattern of observations is entirely consistent with 
and predictable from the Ingenia Theorem. Many AIs do 
seem to have truly impressive human likeness and may even 
sometimes fool one into thinking that they have agency or 
are sentient.13Moreover, the i eld of AI-as-engineering has a 
13Cf. ���the Eliza ef f ect��� (Weizenbaum, 1966; but see also Dil- 
lon, 2020). For instance, last year, Google i red Blake Lemoine, an 
engineer who believed that LaMDa (a large language model that 
he worked on) had become sentient (Fluckinger, 2022). It is wry 
that Lemoine was i red for expressing this (false) belief, while the 
RECLAIMING AI9 
Box 1 ��� Implications of intractability 
Because AI-by-Learning is intractable (formally, NP- 
hard under randomized reductions), the sample-and-time 
requirements grow non-polynomially (e.g. exponentially 
or worse) in n. To illustrate just how quickly this would 
exhaust all the resources available in the universe, even 
for moderate input size n, let us do a simple thought ex- 
periment: Imagine we are looking for an AI that can re- 
spond appropriately to dif f erent situations corresponding 
to conversations of, say, 15 minutes. Since people speak 
around 160 words per minute on average (Yuan, Liber- 
man, & Cieri, 2006, see also Dingemanse & Liesenfeld, 
2022; Liesenfeld & Dingemanse, 2022), let us take 60 
words per minute as a generous lower bound. Then a con- 
versation would have on average 900 words. For humans, 
the appropriate response may depend on the full context 
of the conversation, and we have no problem conditioning 
our behaviour in this way. To encode such sequences of 
spoken words in some binary encoding, we would need 
more bits than words; i.e. n > 900. The assumption of us- 
ing 1 bit per word is an underestimation, assuming that at 
each point, the conversation can continue grammatically 
correctly in at least two directions (cf. Parberry, 1997). 
Now the AI needs to learn to respond appropriately to 
conversations of this size (and not just to short prompts). 
Since resource requirements for AI-by-Learning grow ex- 
ponentially or worse, let us take a simple exponential 
function O(2n) as our proxy of the order of magnitude 
of resources needed as a function of n. 2900��� 10270is 
already unimaginably larger than the number of atoms in 
the universe (��� 1081). 
Imagine us sampling this super- 
astronomical space of possible situations using so-called 
���Big Data���. Even if we grant that billions of trillions(1021) 
of relevant data samples could be generated (or scraped) 
and stored, then this is still but a miniscule proportion of 
the order of magnitude of samples needed to solve the 
learning problem for even moderate size n. It is thus no 
surprise that AI companies that are trying to construct 
AIs using machine learning are running out of useable 
data (Shumailov et al., 2023; P. Villalobos et al., 2022) 
and that actual datasets are not being scaled up to more 
and more complex and diverse real-world situations and 
behaviours, but they are becoming more homogeneous 
(with even harmful consequences; Birhane, Prabhu, Han, 
& Boddeti, 2023). That nevertheless ���large data sets��� (in- 
correctly) appeared to be suf i cient for solving a problem 
like AI-by-Learning, can be explained by the fact that 
people generally have poor intuitions about large numbers 
(Landy, Silbert, & Goldin, 2013) and underestimate how 
fast exponential functions grow (van Rooij, 2018; Wage- 
naar&Sagaria,1975;Wagenaar&Timmers,1978,1979). 
Hence, contrary to intuition, one cannot extrapolate from 
the perceived current rate of progress to the conclusion 
that AGI is soon to be attained. 
habit of interpreting (or selling) ���better than the state of the 
art��� as ���good accuracy���, but our results imply that no mat- 
ter how much ���better��� AI gets, it will be of f by light-years, 
wrong in exponentially many situations. This kind of self- 
fooling is possible in part because: 
The prioritization of performance values is so 
entrenched in the i eld that generic success 
terms, such as ���success���, ���progress���, or ���im- 
provement��� are used as synonyms for perfor- 
mance and accuracy [...] However, models are 
not simply ���well-performing��� or ���accurate��� in 
the abstract but always in relation to and as 
quantif i ed by some metric on some dataset��� 
(Birhane et al., 2022). 
However,theIngeniaTheoremimpliesthatifoneweretotest 
these AIs rigorously and unbiasedly for human-likeness, it 
would quickly become evident that they behave qualitatively 
dif f erently from humans. That is, if you think your AI is very 
human-like, then you are not testing it critically enough (cf. 
Bowers et al., 2023). 
Unsurprisingly given our theoretical results, we see ex- 
actly this play out in practice: AIs appear human-like in non- 
rigorous tests, but the likeness is debunked when more rig- 
orous tests are made (e.g. Adolf i , Bowers, & Poeppel, 2023; 
Dentella, Murphy, Marcus, & Leivada, 2023). For instance, 
claims of abilities emerging with the scaling up of models 
are often revealed to be trivial products of the researcher���s 
choice of metric (Schaef f er, Miranda, & Koyejo, 2023). This 
back and forth between claims of human-likeness and de- 
bunking (cf. Mitchell, 2021) will keep happening if the i eld 
does not realise that AI-by-learning is intractable, and hence 
any model produced in the short run is but a ���decoy���. 
This is especially troubling since more and more people 
are taking AI systems to be candidate models of human cog- 
nition (Frank, 2023a, 2023b; Hardy, Sucholutsky, Thomp- 
son, & Grif i ths, 2023; Mahowald et al., 2023; Tuckute et al., 
2023), or even as replacements for humans. For instance, as 
replacement for participants in psychological experiments, 
Google CEO himself, Sundar Pichai, expressed no less unrealis- 
tic statements in a public interview (Pelley, 2023), suggesting that 
these systems have remarkable ���emergent��� cognitive abilities, e.g. 
to know languages not trained on, being creative, able to reason and 
to plan (see Bender, 2023, for a critical analysis).

(Dillion, Tandon, Gu, & Gray, 2023); but see also: Crock- 
ett & Messeri, 2023; Harding, D���Alessandro, Laskowski, & 
Long, 2023; or as replacement for workers (Eloundou, Man- 
ning, Mishkin, & Rock, 2023; Rose, 2023; Semuels, 2020); 
while by now it is clear that this is only possible at the cost 
of an exponential increase of hidden, poorly paid and poorly 
treated workers; (McCarty Carino & Shin, 2023; Roberts, 
Wood, & Eadon, 2023)). Such replacements are a clear case 
of ���map territory confusion���, and with a poor map at that. 
This may seem to make sense if one believes that the AIs ap- 
proximate human behaviour (though even then it is not a suf- 
i cient condition, Guest & Martin, 2023), but as we explained 
above the AIs do not actually approximate human behaviour. 
By nevertheless taking the AIs as cognitive models, we��� 
as a i eld���distort our view of cognition, and it makes our 
cognitive science theoretically weaker. 
This argument applies not only to AIs mistaken for mod- 
els of (all of human) cognition, but for models of substantive 
cognitive capacities, like language, problem solving, reason- 
ing, analogizing, or perception (Cummins, 2000; van Rooij 
& Baggio, 2021). This can be argued by contradiction. As- 
sume it were possible to tractably make approximate mod- 
els of such core capacities, or even of restricted capacities, 
such that one could make piecemeal models of human cog- 
nition. Then one would not be able to put them back together 
tractably in order to account for all of human cognition, be- 
cause if one were able to, then one would have a tractable 
procedure for modelling all of cognition, which is an in- 
tractable problem (see also Rich, de Haan, Wareham, & van 
Rooij, 2021).
1 / 1
ChatGPT
ChatGPT
It seems you are presenting a detailed account of the Ingenia Theorem's implications on the state of AI, as well as an argument against the idea that current AI systems are truly approximating human behavior or cognition.
The Ingenia Theorem, as described here, seems to assert that the problem of creating human-like AI is intractable, and that existing AI methods cannot solve this problem in a tractable way. The theorem emphasizes that as the complexity of human conversation and cognition grows, the computational resources required to model this behavior become unfathomably large. The resources grow at a rate that is faster than exponential, meaning that even with the vast computational resources available, it's fundamentally impossible to create an AI that can mimic human behavior across all contexts.
Here's a summary of the main implications and insights drawn from your text:
Intractability of Human-like AI
: Creating AI that can truly approximate human behavior is an intractable problem, meaning that it cannot be solved in a reasonable time given the exponential growth of complexity.
Illusion of Progress
: Many current AI systems may seem human-like or show signs of understanding, but this is deceptive. More rigorous testing will reveal qualitative differences from human behavior.
Misleading Claims and Expectations
: The AI community might overstate the abilities of AI systems, leading to incorrect assumptions about their capabilities. This includes false beliefs in sentience or other human-like characteristics.
Implications for the Field
: The misconception that AI can truly mimic human cognition could lead to misguided research efforts and theoretical weaknesses in cognitive science.
Ethical and Societal Considerations
: Overreliance on AI as a replacement for human workers or as models of human cognition has serious implications. This includes the potential exploitation of hidden human labor and the distortion of our understanding of human cognition.
Misunderstanding Exponential Growth
: People often misunderstand the nature of exponential growth, leading to underestimations of the complexity involved in modeling human behavior.
Potential for Self-Fooling
: The field may inadvertently fool itself by using specific success metrics that don't truly measure human-likeness or by not testing AI critically enough.
In summary, the Ingenia Theorem presents a compelling argument against the possibility of creating truly human-like AI through current methods. It calls for a critical reevaluation of the assumptions, methodologies, and goals in AI research, emphasizing the importance of recognizing the fundamental limitations of current approaches.
1 / 1
What is the argument exactly?
1 / 1
ChatGPT
ChatGPT
Certainly! The argument presented in the text revolves around the Ingenia Theorem and can be broken down into several key points:
Intractability of Human-like AI
: The theorem asserts that creating AI that can truly replicate or approximate human behavior is an intractable problem (e.g., NP-hard). This means that solving the problem would require non-polynomial time, effectively making it unsolvable in practice.
Exponential Complexity
: Even when attempting to model something as seemingly simple as a 15-minute conversation, the computational resources required grow exponentially or worse. A simple thought experiment demonstrates that modeling conversations of this size would require resources exceeding the number of atoms in the universe.
Failure of Existing AI
: All AI models produced by tractable procedures or by cutting short an otherwise infeasible process will fail to solve this intractable problem. They will make substantial errors and deviate significantly from human behavior, failing to meet the Ingenia Theorem's standards.
Misleading Perceptions
: Despite the inherent intractability, many AIs appear to exhibit impressive human likeness. This leads to mistaken beliefs, overblown claims, and unrealistic expectations about AI's capabilities. More rigorous and unbiased testing would reveal the qualitative differences between AI and human behavior.
Societal and Ethical Implications
: Misunderstanding the limitations of AI can lead to unethical labor practices, misguided research, distorted views of cognition, and more. These mistakes stem from a failure to recognize that AI models cannot truly approximate human behavior.
Fallacy of Extrapolation
: There is a misunderstanding of exponential growth and large numbers, leading to the incorrect assumption that AI's perceived current rate of progress will inevitably lead to the attainment of AGI (Artificial General Intelligence).
Conclusion
: No matter how impressive current AI models may appear, they cannot capture the distribution of human behavior even approximately. If it seems that an AI is very human-like, then it has not been tested critically enough. The pursuit of human-like AI is fundamentally flawed due to the intractable nature of the problem, and current methods are bound to fall short.
In essence, the Ingenia Theorem and the associated argument present a profound challenge to the prevailing assumption that AI can achieve human-like or human-level capabilities. It calls for a reevaluation of the entire approach to AI development and a recognition of the intrinsic limitations of current methods.
1 / 1
ACT 2: Reclaiming the AI vertex 
Computers as such are in principle less crucial 
for cognitive science than computational con- 
cepts are. 
��� Margaret A. Boden (2008, p. 14) 
Based on our analysis, we reject the view and associated 
project that we term ���makeism���. See Box 2 for a def i nition; in 
other words, all makeists think that building cognition is suf- 
i cientforbeingabletoexplainit(b),andsomethinkthatthis 
building is also necessary (c). The necessity claim especially 
revealstheimplicitassumptionthatitispossibletobuildcog- 
nition (a). While some things can be understood by making 
them, it won���t work for human-like or -level cognition, for 
one because this cannot plausibly be (re)made through engi- 
neering (i.e. (a) is false; see the Ingenia Theorem). 
At this point the reader may wonder: if we indeed can- 
not (re)make cognition���or coherent parts of cognition��� 
computationally, then is AI theoretically useless for cogni- 
tive science? No: computationalism can be theoretically pro- 
ductive even if makeing is futile. In this section we explain 
Box 2 ��� What is makeism? 
Makeism: The view that computationalism implies that 
(a) it is possible to (re)make cognition computation- 
ally; (b) if we (re)make cognition then we can explain 
and/orunderstandit;andpossibly(c)explainingand/or 
understanding cognition requires (re)making cognition 
itself. 
The methodology endorsed by makeists has been re- 
ferred to as the synthetic methodology or understand- 
ing by design and building (Bisig & Pfeifer, 2008; 
Pfeifer & Scheier, 2001). A well-known quote from 
Feynman (1988), ���what I cannot create, I do not un- 
derstand���, is often used to support the idea of makeism 
in AI (e.g. Karpathy et al., 2016). 
Note that it is especially easy for makeists to fall 
into map-territory confusion���mistaking their model- 
ing artefacts for cognition itself���due to the view that 
the made thing could be cognition. 
howthenotionofcomputationcanhelptochallengeandcon- 
strain, and thereby inform, theories of cognition in a way that 
steers clear of makeism. This will also allow us to reclaim 
���AI��� as one of the cognitive sciences, i.e., one of the vertices 
in the hexagon (Figure 1). In our opinion, this is vital for 
retaining (or restoring) cognitive science���s theoretical health 
and preventing (further) distortions of our understanding of 
human cognition. 
What not to reclaim 
As we explained in the Introduction, AI was initially con- 
ceived as a theoretical tool for cognitive science, and an ac- 
tive segment of work in cognitive science was understood as 
being part of AI. AI as a i eld originally included the use of 
computational models to study the human mind (variously 
referred to as cognitive simulation (Lehnert, 1977), informa- 
tion processing psychology (Newell, 1970; Simon, 1983), 
computational psychology (Boden, 1988, 2008), or theo- 
retical psychology (H��nefeldt & Brunetti, 2004; Longuet- 
Higgins, 1981; Newell, 1970). 
At present, this perspective on AI is largely forgotten. By 
looking at the history of those conceptions of AI and why 
they fell out of favour, we can see both the attraction of AI as 
cognitive science and the problems with the original vision, 
which we do not want to reclaim. 
First, let���s consider makeism part (a) (see Box 2). As 
noted, the makeist project only makes sense if part (a) is 
really true, i.e. if cognition can be programmed into a com- 
puter. This seems to have been taken for granted by many, 
for example by Simon, who wrote: 
It is not my aim to surprise or shock you. [...] 
RECLAIMING AI11 
But the simplest way I can summarize is to say 
that there are now in the world machines that 
think, that learn and create. Moreover, their abil- 
ity to do these things is going to increase rapidly 
until���in a visible future���the range of problems 
they can handle will be coextensive with the 
range to which the human mind has been ap- 
plied. 
Herbert Simon on The General Problem Solver 
in 1957, as quoted in Norvig (1992, p. 109) 
Here we see the forerunner of the current idea that compu- 
tationalism implies the practical realizability of thinking ma- 
chines/simulations on par with human-level or -like cogni- 
tion. This idea is expressed clearly in Feigenbaum and Feld- 
man���s(1963)earlycharacterizationofAI(asquotedinMein- 
hart, 1966, emphasis added): 
Researchers in the (artif i cial intelligence) i eld 
hold to the working hypothesis that human 
thinking is wholly information-processing activ- 
ity within the human nervous system; that ulti- 
mately these information processes are perfectly 
explicable; that the road to this explication lies 
in observation, experimentation, analysis, mod- 
elling, model validation, et cetera; and that dig- 
ital computers, being general information pro- 
cessingdevices,canbeprogrammedtocarryout 
any and all of the information processes thus ex- 
plicated.
1 / 1
ChatGPT
ChatGPT
It looks like the text you've shared focuses on a concept referred to as "makeism," which seems to be a perspective in AI and cognitive science that believes in the possibility of recreating human cognition through computational means.
The argument can be broken down into the following parts:
Makeism Defined
: Makeism is a belief that it's possible to remake cognition computationally; if cognition is remade, it can be explained and/or understood; and possibly, understanding cognition requires remaking it. This approach aligns with the famous Feynman quote, "what I cannot create, I do not understand."
Rejection of Makeism
: The authors argue against makeism, stating that building human-like cognition through engineering is implausible. They cite the Ingenia Theorem to support this rejection.
The Utility of AI
: The rejection of makeism does not mean AI is theoretically useless for cognitive science. The authors propose that computation can still help inform theories of cognition without succumbing to makeism.
Reclaiming AI as a Cognitive Science
: The authors advocate for redefining AI as one of the cognitive sciences, which they believe is essential for understanding human cognition accurately.
What Not to Reclaim
: They also caution against certain aspects of the original vision of AI, specifically the assumption that human cognition can simply be programmed into a computer.
Historical Perspective
: The text reflects on the history of the AI field, highlighting shifts in understanding and the underlying assumptions about the nature of cognition and AI's role in studying it.
In summary, the authors seem to be arguing for a nuanced view of AI's role in cognitive science. They reject the idea that human cognition can be fully replicated or understood through computational building or "makeism" but embrace the idea that computational concepts and models can still contribute to our understanding of cognition. They call for a reclaiming of AI's place within the field of cognitive science, but with careful attention to its limitations and potential pitfalls.
1 / 1
The last item on the list makes explicit the assumption that 
computationalism implies the practical realizability of think- 
ing machines/simulations on par with human-level or -like 
cognition. And since it suddenly seemed possible to re-create 
aspects of cognition using computers, many early cognitive 
scientists enthusiastically began trying to do so. For example, 
consider representative comments from the late 70s, from a 
book by Wendy Grace Lehnert. 
If experiments cannot be designed to isolate the 
variable factors of a proposed theory, the [exper- 
imental] psychologist can go no further. Prob- 
lems concerning human cognitive processes are 
dif i cult to study within the paradigm of experi- 
mental psychology for precisely this reason. [...] 
What experiment can be designed to help us un- 
derstand how people are able to answer simple 
questions like ���What���s your name?��� [...] Natural 
language processing can be productively stud- 
ied within the artif i cial intelligence paradigm. If 
we construct a process model designed to ac- 
count for a particular language task [...], then 
we can write a computer program to implement 
that model. By running that program, we can see 
where the model is weak, where it breaks down, 
and where it appears competent. [...] The inter- 
esting failures are those that occur because the 
process model underlying the program failed to 
recognize some critical problem or failed to han- 
dle some problem adequately (Lehnert, 1977, 
pps. 40-41). 
If our reading of Lehnert is correct, this expresses, or at least 
encourages, makeism. Specif i cally, it ref l ects parts (a) and 
(b), with the idea that interesting parts of cognition can be 
simulated in computers, and that we will gain understanding 
in this way. Furthermore, Lehnert leaves the door open to 
makeism part (c); it is unclear whether she herself thinks that 
simulatingnaturallanguageisnecessaryforexplainingit,but 
one could draw that conclusion. 
The problem, of course, is that (a) is false, and without it, 
makeism (b) and (c) no longer ref l ect a promising strategy 
for cognitive science, but rather a research program doomed 
to fail. Indeed, in light of our demonstration that the task of 
creating cognition in computers is unfeasible, it is not sur- 
prising that among early researchers pursuing this project, 
enthusiasm waned and many people moved on.14 
As AI technology has exploded, makeism is enjoying a 
renaissance. However, for cognitive science, the engineer- 
ing approach worsens theoretical understanding because any 
artefacts we could make in the short run would be gross dis- 
tortions or ���decoys��� at best. As Neisser wrote, ���[t]he view 
that machines will think as man [sic] does reveals misunder- 
standing of the nature of human thought��� (Neisser, 1963, p. 
193). Sixty years later, the risk of being misled by decoys 
is even greater, but we can better demonstrate the problem 
through the complexity theoretical arguments laid out in the 
previous section (ACT 1: Releasing the grip). 
To abandon all of the tools and concepts that AI provided, 
however, is to throw the baby out with the bath water.15 
Hence, we want to reclaim much of the early conception 
of AI as a part of cognitive science, but without encourag- 
ing makeism. If we reconsider Lehnert���s argument for using 
AI, we see many correct, important insights: (1) That exper- 
imental psychology is limited in its ability to study cogni- 
tion (hence the creation of cognitive science as an interdisci- 
plinary i eld). (2) That cognition can be productively studied 
14See for example the beginning of Newell (1970). 
15A few people seem to have recognized this, including Newell. 
He endorses the view that ���the actual theories of cognitive psychol- 
ogy are to be expressed as artif i cial intelligence systems��� (p. 368), 
and goes on to observe that ���[a]fter the fact, one can see that such a 
theory might have emerged within psychology (or linguistics) with- 
outtheadventofthecomputer.Inhistoricalfact,thetheoryemerged 
by trying to program the computer to do non-numerical tasks and 
by trying to construct abstract theories of computation and logic��� 
(Newell, 1970, p. 373). 
12VAN ROOIJ ET AL. 
within the AI paradigm. And (3) that models��� problems are 
theoretically informative. 
These insights are all worth reclaiming; the community 
just made one seemingly-small inferential miss-step that has 
caused a lot of problems. Makeism is not a forced move. 
Hunt pointed this out early on, arguing that 
Computer programming seems to be a more ap- 
propriate tool for studying the broad implica- 
tionsofaproposalforhowoneshouldthinkthan 
for realizing a testable model of how one does 
think (Hunt, 1968, p. 160). 
Computationalism without makeism is still theoretically 
fruitful. We explain how next.
1 / 1
ChatGPT
ChatGPT
The additional text you've provided continues the exploration of makeism and the argument against it within the context of AI and cognitive science. Here's an extended analysis based on the additional information:
Historical Enthusiasm for Makeism
: Early cognitive scientists were drawn to the idea that recreating aspects of cognition in computers would lead to understanding human thought processes. Wendy Grace Lehnert's work is cited as an example, highlighting the belief that simulating natural language processing in computers would enhance understanding.
The Failure of Makeism
: The text emphasizes that makeism has failed because it's based on false assumptions (part a). The unfeasibility of creating cognition in computers has led to a decline in enthusiasm among early researchers.
Makeism's Renaissance and Its Risks
: Despite past failures, makeism has resurfaced in popularity as AI technology has advanced. The text warns that this resurgence risks leading to misunderstandings and distortions of human thought.
Reclaiming AI without Makeism
: The authors argue for retaining valuable insights from AI and computationalism, without falling into the trap of makeism. They believe computationalism still has theoretical value but must be divorced from the flawed assumption that human cognition can be remade in computers.
Computationalism's Theoretical Fruitfulness
: The text concludes by suggesting that computational models and concepts still have an important role in studying cognition. AI can provide insights into cognitive processes without the need to recreate them, and computer programming can be seen as a tool for studying thought rather than recreating it.
In essence, the argument builds a case against makeism by examining its historical context, theoretical underpinnings, and practical implications. The authors are careful to delineate the valuable aspects of computationalism and AI, urging a nuanced understanding that recognizes both the potential and limitations of these tools in the study of human cognition. The rejection of makeism is not a rejection of computational models but rather a call for a more realistic and theoretically grounded approach.
1 / 1
Theory without makeing 
[P]rinciples from computer science and engi- 
neering can be, if done carefully, imported into 
how we carve [...] nature at its joints. 
��� Olivia Guest and Andrea E. Martin, (2023, p. 
221) 
How may computationalism help cognitive science ad- 
vance if not through makeism? Core to the non-makeist 
enterprise is the realisation that computationalism primar- 
ily aids cognitive science by providing conceptual and for- 
mal tools for theory development and for carefully assess- 
ing whether something is computationally possible or not, 
in principle and in practice. This paper is itself an example; 
nowhere in this paper did we (try to) make a computational 
replica of cognitive capacities. Yet, we were able to use a 
computationalist framework to make substantial steps in re- 
claiming AI for cognitive science. The remainder of this sec- 
tion will give further examples of how AI as theoretical psy- 
chology or computational cognitive science can be pursued 
productively and soundly.16 
As a disclaimer, we note that research in cognitive sci- 
ence often cannot be cleanly divided into makeist and non- 
makeist. We have not seen makeism clearly distinguished 
from computationalism in the literature before (see Box 2), 
and so cognitive scientists will generally not have thought 
about it explicitly, let alone clarif i ed the nature of their work. 
Hence, when we cite papers as examples, we wish to high- 
light the non-makeist readings of some of the arguments 
made, but without implying that there are no problematic 
traces of makeism in the original texts. 
Levels of explanation.Formalisms and concepts from 
computer science allow us to conceptually distinguish be- 
tween cognitive processes (algorithms), the capacities they 
realise (the problems that they solve), and their physical im- 
plementations (chemical, biological, interactive, etc.). This 
conceptual distinction, also often referred to as Marr���s lev- 
els (Marr, 1982) is theoretically productive, especially when 
pursued in a non-makeist fashion. 
The distinction between levels is conceptually useful in 
general, but also brings specif i c benef i ts when we want to 
formalise and reason about our theories, as we explain next. 
Capacities as problems.Within the levels-framework, 
the approach known as computational-level modelling has 
a strong tradition in cognitive science (with debates on its 
proper interpretation continuing to this day; Blokpoel, 2018; 
R. P. Cooper & Peebles, 2018; Peebles & Cooper, 2015). 
This approach allows us to conceptually engineer cognitive 
capacities as ���computational problems��� and to model them 
formally (see e.g. Blokpoel & van Rooij, 2021; van Rooij & 
Baggio, 2021; van Rooij & Blokpoel, 2020) without needing 
to commit to specif i c assumptions at the algorithmic or im- 
plementation levels (other than computability and tractabil- 
ity; more in the next subsection). This is especially useful 
since���as argued throughout this paper���we do not know 
how to computationally realise substantive cognitive capac- 
ities such as human-level perception, reasoning, memory, 
categorisation, decision-making, problem-solving, language, 
analogising, communication, learning, planning, etc. Yet, as 
cognitive scientists, we do want to make progress in devel- 
oping a theoretical understanding of these capacities. 
Computational modelling of capacities can help us to 
make our assumptions precise and explicit, and to draw out 
their consequences, without the need to simulate the pos- 
tulated computations (though simulations have their uses; 
more on that next). For instance, with formal computational- 
level models and mathematical proof techniques at hand, 
one can critically assess claims of explanatory adequacy 
(Blokpoel & van Rooij, 2021; Egan, 2017; van Rooij & 
Baggio, 2021), claims of intractability (Adolf i , Wareham, & 
van Rooij, 2023), claims of tractability (Kwisthout & van 
Rooij, 2020; van Rooij, Evans, Muller, Gedge, & Wareham, 
2008), claims of competing theories (Blokpoel & van Rooij, 
2021), claims of evolvability (Rich, Blokpoel, de Haan, & 
van Rooij, 2020; Woensdregt et al., 2021), and claims of 
approximability (Kwisthout & Van Rooij, 2013; Kwisthout, 
Wareham, & Van Rooij, 2011). 
16We acknowledge that computational modelling can also con- 
tribute to productive theory development without committing to 
computationalism (Guest & Martin, 2021; Morgan & Morri- 
son, 1999). Here we focus specif i cally on computationalist mod- 
elling, because we want to highlight that computationalism with- 
out makeism is possible. We also acknowledge existing critiques 
of ���AI as computationalism���. Some of those critiques may target 
makeism (and/or dehumanisation; Baria & Cross, 2021; Birhane, 
2021; Birhane & van Dijk, 2020; Erscoi et al., 2023; van der Gun & 
Guest, 2023) more than computationalism as a theoretical tool per 
se (but we will leave that judgement up to the critics). Be that as it 
may, we believe it is useful to explain how makeism and computa- 
tionalism are dissociable, just as cognitivism and computationalism 
are dissociable (M. Villalobos & Dewhurst, 2017, 2018) and rep- 
resentationalism and computationalism are dissociable (Mi��kowski, 
2013, 2018; Piccinini, 2008).
1 / 1
ChatGPT
ChatGPT
The given passage builds on the idea of computationalism in cognitive science without makeism (the attempt to recreate human cognition in computational forms).
Here are some key points:
Using Computational Principles
: Computationalism doesn't need to be about making replicas of cognitive functions. Instead, it's about using computational principles as tools for theory development and for assessing what's computationally possible or not.
Levels of Explanation
: Concepts from computer science can be used to distinguish between the different aspects of cognitive processes, such as algorithms (how it's done), problems they solve (what's done), and physical implementations (where it's done). This distinction is aligned with Marr's levels and is considered to be theoretically productive.
Capacities as Problems
: The focus here is on understanding cognitive capacities as computational problems, modeling them formally without necessarily committing to specific algorithms or implementations. This allows scientists to explore substantial cognitive functions like perception, reasoning, and language, without knowing how to recreate them computationally.
Computational Modeling Without Simulating
: The passage emphasizes the use of formal models and mathematical proof techniques to assess various claims regarding cognitive science, such as explanatory adequacy, tractability, competing theories, and approximability. The point is that one can use computational modeling without needing to simulate the postulated computations, although simulations may have their uses.
Dissociating Makeism from Computationalism
: The authors are careful to distinguish between makeism and computationalism, emphasizing that they are not synonymous. The text recognizes critiques of "AI as computationalism" but underscores the possibility of computationalism without makeism.
In conclusion, this passage presents an argument for a nuanced approach to computationalism within cognitive science, which emphasizes the theoretical and modeling benefits of computational principles without falling into the trap of attempting to recreate human cognition. It reflects a growing understanding of how computational tools can inform our understanding of the mind without oversimplifying or misrepresenting the complex nature of human cognition.
1 / 1
Algorithms and simulations.Similarly, algorithmic- 
and implementation-level models can be postulated and crit- 
ically assessed using computational tools. While this can 
sometimes be done analytically, more often computer sim- 
ulations prove useful for these types of (complex and dy- 
namic) models. Using computer simulations, for example, 
one can assess claims about possible functioning under net- 
work damage (Guest, Caso, & Cooper, 2020), claims of 
explanatory scope and adequacy (Adolf i , Bowers, & Poep- 
pel, 2023; van de Braak, Dingemanse, Toni, van Rooij, & 
Blokpoel, 2021), claims of approximation (Blokpoel & van 
Rooij, 2021, Chapter 8), claims of ruling out possible so- 
called neural codes (Guest & Love, 2017), and claims of 
mechanistic possibilities (Bartlett et al., 2023; ten Oever & 
Martin, 2021). 
Importantly, this use of simulations is to be distinguished 
from makeist uses of simulation that confuse the models (ex- 
planans) for the thing modelled (explanandum) and/or take 
thesimulationresultstodirectlyimplysomethingabout���how 
thingswork���intherealworld(e.g.forreal-worldbrains,cog- 
nition, or behaviour). Instead, non-makeist computer simula- 
tions are theoretical tools that can demonstrate proof of con- 
cept or demonstrate the in-principle (im)possibility of phe- 
nomena arising from the theorised constructs and hypoth- 
esised mechanisms. Computer simulations support and ex- 
tend a scientist���s thinking capacity, and enable computerised 
���thought experiments��� (R. Cooper, 2005) to reason through 
���what ifs��� and answer questions like ���how possibly���.17These 
simulations���as indeed any models that the cognitive scien- 
tist could use���are necessarily abstract and idealised; this is 
unproblematic, though, as long as the scientist recognises it 
and takes care to draw only those inferences which are really 
warranted by the model. 
Underdetermination.A general theoretical property 
that follows from computationalism is that cognitive capaci- 
ties are multiply realisable, in several ways. Van Rooij and 
Baggio (2021) use a sorting problem as a simple illustra- 
tion. Sorting can be done by bubble sort, insert sort, or any 
of a whole host of distinct sorting algorithms (Knuth, 1968) 
which in turn can be physically realised by brains, comput- 
ers, water pipes, or even distributed over people (see e.g. Fig- 
ure 1 in van Rooij & Blokpoel, 2020; and Box 1 in van Rooij 
& Baggio, 2021). This shows how, i rst, one and the same 
problem can be computed by dif f erent algorithms, and sec- 
ond, one and the same algorithm can be physically realised 
in dif f erent ways. This implies that we are dealing with mas- 
sive underdetermination of theory by data: i.e., if we observe 
behaviours consistent with a computational level theory, we 
cannot infer which algorithms or neural processes underlie 
the behaviour. 
The computational lens helps us to appreciate the de- 
gree of underdetermination we face. In standard experimen- 
tal cognitive psychology, often two or a handful of dif f er- 
ent theories are compared and tested empirically ���against 
each other���. But the principle of computational multiple re- 
alisability shows that for any given behaviour there may be 
|A| �� |I| many possible algorithmic-implementational the- 
ories. This means that some inferential practices in com- 
putational cognitive (neuro)science are highly problematic 
(Guest & Martin, 2023). Moreover, computational level the- 
ories are also themselves underdetermined by data, because 
any i nite set of observations is also consistent with in- 
i nitely many functions (capacities).18This means that all 
computational-level theories are���and remain���conjectural. 
Nonetheless, we can do some things to evaluate and adjudi- 
cate between them. 
Computational realisability.Underdetermined com- 
putational level theories can be constrained by the compu- 
tationalist requirement that the problems and processes they 
postulate must be computationally realisable���by the cogni- 
tive system under study, not by scientists���both in princi- 
ple and in practice. In-principle realisability is also known as 
computability; a problem is computable if there can exist at 
least one algorithm for computing it. In-practice realisability 
is also known as tractability; a problem is tractable if there 
can exist at least one tractable19algorithm for computing it. 
Given that computational-level theories often formalise 
capacities as problems (or equivalently functions) while re- 
maining agnostic about how these problems are computed, 
they can on occasion postulate problems that are uncom- 
putable or intractable; in fact, this happens regularly. This 
provides the opportunity to critically ref l ect on the theory, 
and if possible, to i nd a minimal revision that renders the 
theory minimally computable and tractable while preserving 
the core intuitions and motivations behind the theory. This 
process can yield new knowledge, ideas, and research tra- 
jectories for cognitive scientists (cf. Adolf i , van de Braak, 
& Woensdregt, 2023). For example, it may yield new theo- 
retical interpretations or predictions that can be used to fur- 
ther assess the explanatory and empirical adequacy of the 
(revised) theories. Alternatively, if a theory cannot be suc- 
cessfully revised, this can be a sign that it is time to question 
its initial motivation and to go back to the drawing board. 
The process thereby allows us to sculpt otherwise underde- 
17For discussion of how possibly explanation, see e.g. (Bokulich, 
2014, 2017; Gr��ne-Yanof f , 2013; Sullivan, 2022). 
18Similarly, no i nite set of ���impressive��� observations about 
AIs/LLMs establishes that it exhibits a (human) cognitive capacity. 
This link can only be theoretically established; i.e., if we propose 
that a machine has a cognitive capacity X, we must also formally 
charactise X such that the machine���s capacity Y can be mathemati- 
cally proven to be X ��� Y under relevant and naturalistic conditions, 
using appropriate proof techniques (see e.g. Blokpoel & van Rooij, 
2021, Chapter 5). 
19Tractable can be formalised, for instance, as needing a poly- 
nomial or i xed-parameter tractable amount of computational re- 
sources (Frixione, 2001; van Rooij, 2008; van Rooij et al., 2019) 
14VAN ROOIJ ET AL. 
termined theories so as to learn more about how cognition 
could or could not work (Blokpoel, 2018). This is no magic 
bullet, however; underdetermination of theory by data cannot 
be eliminated, and any ways of dealing with it will remain 
necessarily incomplete (cf. Adolf i , Wareham, & van Rooij, 
2023; Devezer, 2023; Rich et al., 2021).
1 / 1
ChatGPT
ChatGPT
It seems you've pasted a large excerpt from a text that delves into complex subjects related to computational cognitive science, algorithms, simulations, underdetermination, and computability. This excerpt discusses the important role that computer simulations play in assessing and reasoning through complex models, including the concept of non-makeist simulations, which don't confuse the models with the real world they represent.
The text also examines underdetermination in computational cognitive science, highlighting how behaviors consistent with a computational level theory cannot directly infer the underlying algorithms or neural processes. It mentions concepts like computational realisability, computability, tractability, and how these can be applied to critically assess or refine theories within cognitive science.
Would you like an explanation of a specific section or concept mentioned in this text?
1 / 1
Slow (computational cognitive) science.All of this 
may seem excruciatingly slow compared to the apparent 
speed of progress in today���s machine learning approaches 
to AI. But to genuinely make progress we need to go this 
slowly, and in fact cannot go any faster (Adolf i & van Rooij, 
2023; Rich et al., 2021). There is just no way to proceduralise 
or automate either the creation of minds or the explanation 
of minds. The way to make progress is through the metic- 
ulous development of theoretical ideas, informed by formal 
and computational modelling, drawing out limitations, con- 
sequences, and building solid knowledge along the way. In 
other words, what we advocate is more theoretical thinking 
(see also Guest, 2023; Guest & Martin, 2021; van Rooij & 
Baggio, 2020), and less (unthinking) machine learning or 
less confusion between machine learning and theory (cf. An- 
drews, 2023). Then AI can be a useful theoretical tool for 
cognitive science and regain its rightful place in the interdis- 
ciplinary hexagon. Conclusion 
The thesis of computationalism implies that it is possible 
in principle to understand human cognition as a form of com- 
putation. However, this does not imply that it is possible in 
practiceto computationally(re)makecognition. Inthispaper, 
we have shown that (re)making human-like or human-level 
minds is computationally intractable (even under highly ide- 
alisedconditions).Despitethecurrenthypesurrounding���im- 
pending��� AGI, this practical infeasibility actually i ts very 
well with what we observe (for example, running out of qual- 
ity training data and the non-human-like performance of AI 
systems when tested rigorously). 
Many societal problems surrounding AI have received 
thorough treatment elsewhere. Our focus here has been on 
a dif f erent���but not unrelated���problem, namely that AI- 
as-engineering has been trespassing into cognitive science, 
with some people drawing overly hasty inferences from en- 
gineered AI systems to human cognition. This is a problem 
because any such system created now or in the near future is 
a mere decoy when our goal is to understand human cogni- 
tion, and treating it as a substitute for human cognition for 
scientif i c purposes will only confuse and mislead us. 
Early cognitive scientists rightly recognised the tremen- 
dous potential of AI as a theoretical tool, but due to 
widespread, implicit makeist elements, AI and cognitive 
science became increasingly dissociated over time. Now, 
interest in AI among cognitive scientists is enjoying a 
renaissance���but the interest seems to be in the wrong type 
of AI, namely AI-as-engineering, which distorts our under- 
standing of cognition and cognitive science. Accordingly, 
the time is apt to reclaim AI-as-theoretical-psychology as a 
rightful part of cognitive science. As we have argued, this 
involves embracing all the valuable tools that computation- 
alism provides, but without (explicitly or implicitly) falling 
into the trap of thinking that we can or should try to engineer 
human(-like or -level) cognition in practice.
1 / 1
ChatGPT
ChatGPT
This excerpt emphasizes the slow, meticulous approach necessary for genuine progress in understanding human cognition, contrasting it with the rapid advances in machine learning and AI. Here's a summary of the key points:
Slow Progress in Understanding Minds
: Truly understanding or creating human-like minds is a complex process that cannot be rushed or fully automated. It requires careful theoretical development and thoughtful use of computational models.
Concerns About Machine Learning
: The excerpt advocates for more theoretical thinking and warns against an overreliance on machine learning, which can lead to confusion and misinterpretation of human cognition.
Computational Intractability of Creating Minds
: The text underscores that creating human-like or human-level minds is computationally intractable, aligning with the observed limitations and non-human-like performance of current AI systems.
Misuse of AI in Cognitive Science
: There is a critique of how AI-as-engineering has been misapplied within cognitive science, leading to overly hasty inferences about human cognition. The authors argue that this is misleading and detrimental to a proper understanding of the mind.
Call to Reclaim AI as a Theoretical Tool
: The authors encourage a return to viewing AI as a theoretical tool in cognitive science, embracing the valuable insights that computationalism provides, without falling into the trap of trying to engineer human-like cognition.
The excerpt essentially calls for caution, thoughtful reflection, and clear distinction between AI engineering and theoretical understanding of human cognition. It recognizes the potential of AI as a tool for cognitive science but warns against its misuse.
1 / 1
A summary of the whole article.
1 / 1
ChatGPT
ChatGPT
I apologize, but I only have access to the excerpts provided and don't have the full context of the entire article. Therefore, I'm unable to provide a summary of the whole article. If you can provide more specific details or additional sections of the article, I'll be happy to help with further analysis or summarization!
1 / 1
It started here: The idea that human cognition is, or can be understood as, a form of computation is a useful 
conceptual tool for cognitive science. It was a foundational assumption during the birth of 
cognitive science as a multidisciplinary i eld, with Artif i cial Intelligence (AI) as one of its 
contributing i elds.
1 / 1
ChatGPT
ChatGPT
Based on the provided excerpts, the article appears to discuss the relationship between cognitive science and Artificial Intelligence (AI), focusing on the concept of computationalism. Here's a summary of the key points:
Computationalism in Cognitive Science
: The article begins with the idea that human cognition can be understood as a form of computation, which is a foundational assumption in cognitive science.
Algorithms and Simulations
: The text emphasizes the importance of algorithmic and computational models in understanding complex and dynamic cognitive systems. This includes simulations that demonstrate proofs of concept, in-principle possibilities, and exploring ���what ifs.���
Underdetermination and Realisability
: The article explores the concepts of underdetermination, where observations cannot fully determine the algorithms or neural processes underlying a behavior, and computational realisability, which focuses on the constraints of computational-level theories.
Slow Science
: The authors argue for a meticulous development of theoretical ideas, formal and computational modeling to build solid knowledge. They caution against unthinking machine learning, emphasizing more theoretical thinking.
AI as a Theoretical Tool
: The article criticizes the hasty inferences drawn from engineered AI systems to human cognition and warns against treating AI systems as a substitute for human cognition. It stresses the importance of reclaiming AI as a theoretical tool rather than attempting to engineer human-like cognition.
Conclusion
: The conclusion seems to reinforce the practical infeasibility of recreating human cognition computationally and warns against the misinterpretation of AI in the context of human cognition. It emphasizes the importance of computationalism without falling into the trap of trying to engineer human-like cognition.
Overall, the article highlights the complex relationship between AI, computational models, and human cognition, and advocates for a more thoughtful and theoretical approach to understanding cognitive processes.
1 / 1
They had a thought experiment where they tried to show that human level intelligence is NP-hard. I'm not sure I understood the argument
1 / 1
ChatGPT
ChatGPT
Unfortunately, the text excerpts you provided do not include details about the specific thought experiment that aims to demonstrate that human-level intelligence is NP-hard. Therefore, I'm unable to directly explain the argument without additional information.
However, I can provide some general context. The concept of a problem being NP-hard (Non-deterministic Polynomial-time hard) refers to a classification of problems that are at least as hard as the hardest problems in NP (Non-deterministic Polynomial time). If human-level intelligence were demonstrated to be NP-hard, it would imply that simulating or recreating it through algorithms would be incredibly complex and may not be solvable in polynomial time.
Such a classification would highlight the challenges and limitations of recreating human cognition within computational systems and might be used to argue against the feasibility of achieving true artificial general intelligence (AGI) through current computational methods.
If you can provide more specific details from the text or describe the thought experiment, I may be able to help you understand the argument better.
1 / 1
The text you provided outlines the Ingenia Theorem, which states that AI-by-Learning is intractable. This conclusion is reached through the examination of a related decision problem called Perfect-vs-Chance, which is proven to be intractable.
1 / 1
ChatGPT
ChatGPT
Thank you for providing more context. It appears that the article you're referring to presents a theoretical result known as the Ingenia Theorem. This theorem argues that AI-by-Learning, or the creation of human-level intelligence through machine learning techniques, is computationally intractable. This result implies that replicating human intelligence through current machine learning techniques may be practically impossible, at least in the exact and perfect sense.
The "Perfect-vs-Chance" decision problem may be part of a proof that demonstrates the intractability. It's a typical technique in theoretical computer science to reduce one problem to another to prove that they are equally hard to solve. If "Perfect-vs-Chance" is an intractable problem, and if the process of creating AI-by-Learning can be reduced to this problem, then AI-by-Learning would also be intractable.
Unfortunately, without more specific details of the "Perfect-vs-Chance" problem or the exact logical steps taken in the proof, I cannot explain the thought experiment in more detail. If you can provide those details, I'll do my best to clarify the argument.
1 / 1
In this section we present a formal proof that AI- 
by-Learning is intractable. For this we use the following 
decisionproblemcalledPerfect-vs-Chance.9WhilePerfect- 
vs-Chance is a somewhat unnatural problem, with no direct 
8Given the assumption of productivity of mind (Fodor, 2005; 
Fodor & Pylyshyn, 1988), this is a gross underestimation of human 
potential. 
9This problem was introduced and proven intractable by Shuichi 
Hirahara (2022), but they did not give it a name. We have chosen a 
name that suf i ces for our purposes and so as to optimise intuitive- 
ness. 
RECLAIMING AI7 
real-world analogue, it is useful for our purposes. Bear with 
us. It will all make sense in a moment. 
Perfect-vs-Chance (decision problem) 
Given: A way to sample a given distribution D 
over {0,1}n��{0,1}, an integer k, and the promise 
that one of the following two cases apply: 
1. There is an ef i cient program M of size at 
most k such that Pr(x,y)���D[M(x) = y] = 1 
2. For any program M of size at most k, 
Pr(x,y)���D[M(x) = y] ��� 1/2 + 1/2n 
1����� 
where 0 < �� < 1 is an arbitrary constant. 
Question: Is (1) or (2) the case? 
In the decision problem Perfect-vs-Chance, the expres- 
sion 1/2n 1����� can informally be read as ���negligible probabil- 
ity���, since the denominator grows very fast. Hence, infor- 
mally the two cases listed in the Perfect-vs-Chance problem 
are (1) there exists a perfect and ef i cient program or, other- 
wise, (2) there exists no program that can work better than 
chance, even inef i ciently. 
Note that in the Perfect-vs-Chance problem it is 
promised, before trying to solve the problem, that for the 
given distribution either case (1) or (2) holds. One may intuit 
that, since (1) and (2) are extreme cases that are very clearly 
distinct, telling these two cases apart should be easy to do. 
However, it is not easy. In fact, it is provably intractable to 
i nd out in which of the two cases one i nds oneself. This 
follows from a proof by (Hirahara, 2022). 
Theorem 1 (Hirahara, 2022). Perfect-vs-Chance is in- 
tractable.10,11 
We will use Theorem 1 to prove that AI-by-Learning is 
intractable, too. Specif i cally, we will show that if it were 
possible to tractably solve AI-by-Learning, then it would 
also be possible to tractably solve Perfect-vs-Chance. Since 
Perfect-vs-Chance is known to be intractable (Theorem 1), 
by modus tollens,12it follows that AI-by-Learning must be 
intractable as well. In other words, the proof will be by con- 
tradiction. 
Theorem 2 (Ingenia Theorem). AI-by-Learning is in- 
tractable. 
Proof (sketch). For full details of the proof we refer the 
reader to the Appendix. In this proof sketch we present the 
main proof idea. See Figure 3 for an illustration. 
We prove by contradiction. Suppose that there exists 
a learning mechanism M that solves AI-by-Learning in 
polynomial time. We will show that then there exists a 
polynomial-time bounded-error probabilistic algorithm that 
solves Perfect-vs-Chance. 
Figure 3. A visual illustration of the core proof idea for 
Theorem 2 (Ingenia Theorem): It is known that Perfect- 
vs-Chance is an intractable decision problem. If a tractable 
method M for solving AI-by-learning would exist, then we 
could use M to solve Perfect-vs-Chance tractably, by plug- 
ging it into a sampling-plus-decision procedure that is itself 
tractable, easy to construct, and together with M would prov- 
ably solve Perfect-vs-Chance. This yields a contradiction. 
Therefore we can conclude that AI-by-learning is intractable 
as well. See the main text for more information and the Ap- 
pendix for full proof details. 
The algorithm for Perfect-vs-Chance works as follows. 
Take an arbitrary instance for Perfect-vs-Chance, consisting 
of integers n and k and a distribution D over {0,1}n�� {0,1}. 
The algorithm will use a subroutine that simulates the learn- 
ing mechanism M, where K = k and where the data that the 
mechanismMhassamplingaccesstoisgivenbythedistribu- 
tion D. In this simulation, the set of situations is S = {0,1}n 
and the set of behaviours is B = {0,1}. The simulation will 
yield an algorithm A, that might or might not perform well 
on freshly sampled situations. 
After running the subroutine that simulates M, the algo- 
rithm will evaluate the quality of the resulting learned algorithm A by using additional samples from D and counting 
the number of situations s in which the algorithm A returns 
an appropriate behaviour b ��� Bs. 
The algorithm runs the simulation subroutine several 
times, and for each run of the subroutine, it evaluates the re- 
sulting learned algorithm A. From all of these runs, it picks 
the algorithm A���that performed best in the quality eval- 
uation. Based on how well the best learned algorithm A��� 
performs, the algorithm will give an answer for the input 
of Perfect-vs-Chance. If A���performs non-negligibly better 
than chance, then the algorithm will answer Yes, and other- 
wise, the algorithm will answer No. 
By having the algorithm run the simulation a large enough 
(yet polynomial) number of times, and testing the output of 
each simulation a large enough (yet polynomial) number of 
times with new samples, we can ensure that the algorithm 
outputs a correct answer for Perfect-vs-Chance with high 
probability.���
1 / 1
ChatGPT
ChatGPT
Thank you for providing the detailed excerpt. This allows us to dissect the argument.
Perfect-vs-Chance Decision Problem
: This problem asks whether there exists an efficient program
M
that perfectly models a given distribution
D
, or if any program
M
can do only negligibly better than chance. In other words, it asks if the behavior of the given distribution can be predicted exactly by some program, or if it is essentially random.
Proof that Perfect-vs-Chance is Intractable
: Hirahara (2022) previously proved that the Perfect-vs-Chance decision problem is intractable, meaning that no polynomial-time algorithm can decide it.
Connection to AI-by-Learning
: The Ingenia Theorem shows that if AI-by-Learning (learning human-level intelligence) were tractable, it would be possible to solve Perfect-vs-Chance in polynomial time as well.
Proof by Contradiction
: The proof is conducted by contradiction. It assumes that there exists a tractable method
M
for AI-by-Learning, and then uses this to devise a polynomial-time algorithm to solve Perfect-vs-Chance. However, since Perfect-vs-Chance is known to be intractable, this leads to a contradiction. Therefore, the initial assumption that AI-by-Learning is tractable must be false.
Visualization
: The visual illustration likely depicts this logical chain, showing how a hypothetical tractable method for AI-by-Learning leads to a tractable solution to Perfect-vs-Chance, thus leading to the contradiction.
In summary, the argument is leveraging a known intractable problem (Perfect-vs-Chance) to prove that AI-by-Learning is also intractable. By setting up a scenario where the tractability of AI-by-Learning implies the tractability of a known intractable problem, the proof reaches a contradiction, thereby demonstrating that AI-by-Learning must be intractable. This result aligns with the slow, deliberate, and theoretical approach advocated in the article for understanding human cognition, as opposed to simply attempting to recreate it through current machine learning techniques.
1 / 1
TR22-119Authors: Shuichi Hirahara
Publication: 24th August 2022 12:12
Downloads: 2756
Keywords: 
Kolmogorov complexity, Minimum Circuit Size Problem, PAC learning, Pseudorandomness
Abstract:
A long-standing open question in computational learning theory is to prove NP-hardness of learning efficient programs, the setting of which is in between proper learning and improper learning. Ko (COLT'90, SICOMP'91) explicitly raised this open question and demonstrated its difficulty by proving that there exists no relativizing proof of NP-hardness of learning programs. In this paper, we overcome Ko's relativization barrier and prove NP-hardness of learning programs under randomized polynomial-time many-one reductions. Our result is provably non-relativizing, and comes somewhat close to the parameter range of improper learning: We observe that mildly improving our inapproximability factor is sufficient to exclude Heuristica, i.e., show the equivalence between average-case and worst-case complexities of NP.

We also make progress on another long-standing open question of showing NP-hardness of the Minimum Circuit Size Problem (MCSP). We prove NP-hardness of the partial function variant of MCSP as well as other meta-computational problems, such as the problems MKTP* and MINKT* of computing the time-bounded Kolmogorov complexity of a given partial string, under randomized polynomial-time reductions.

Our proofs are algorithmic information (a.k.a. Kolmogorov complexity) theoretic. We utilize black-box pseudorandom generator constructions, such as the Nisan-Wigderson generator, as a one-time encryption scheme secure against a program which "does not know" a random function. Our key technical contribution is to quantify the "knowledge" of a program by using conditional Kolmogorov complexity and show that no small program can know many random functions
1 / 1
ChatGPT
ChatGPT
The abstract you've provided outlines the key contributions of a paper by Shuichi Hirahara, published on 24th August 2022. Here's a summary of the main points:
NP-Hardness of Learning Programs
: The paper resolves a long-standing open question by proving the NP-hardness of learning efficient programs. This area of inquiry is situated between proper and improper learning.
Overcoming Ko's Relativization Barrier
: Previously, Ko demonstrated the difficulty of proving NP-hardness in this context by showing there exists no relativizing proof. This paper overcomes that barrier by using randomized polynomial-time many-one reductions.
Connections to Improper Learning
: The result comes close to the parameter range of improper learning, and the paper notes that a slight improvement in inapproximability would have broader implications for complexity theory.
NP-Hardness of MCSP and Other Meta-Computational Problems
: The paper makes strides in proving the NP-hardness of the Minimum Circuit Size Problem (MCSP) and related computational issues. These proofs extend to problems of time-bounded Kolmogorov complexity.
Algorithmic Information Theoretic Approach
: The proofs rely on principles of algorithmic information (or Kolmogorov complexity) theory. The authors use techniques like black-box pseudorandom generator constructions and quantify the "knowledge" of a program using conditional Kolmogorov complexity.
Relevance to Previous Discussion
: This abstract seems to provide the theoretical background and context for the Perfect-vs-Chance problem and Ingenia Theorem discussed earlier. The NP-hardness of learning programs and the complex, information-theoretic methods used to prove it seem directly related to the argument that AI-by-Learning is intractable.
Overall, this paper appears to make significant strides in computational learning theory, breaking through previous barriers, and providing deep insights into the complexity of learning programs, all of which ties back to the previous discussion on the intractability of replicating human-level intelligence through AI-by-Learning.
1 / 1
I don't find these "proofs" convincing. By the same logic it seems that humans should not be intelligent.
1 / 1
ChatGPT
ChatGPT
It's completely understandable to have skepticism towards complex theoretical proofs, especially when dealing with subjects like computational learning theory and intelligence. The comparison of human intelligence to machine learning models is a rich and intricate subject.
Different Mechanisms
: Humans and AI models are fundamentally different. While humans seem to "learn" efficiently, our learning process is mediated by biological mechanisms that are still not fully understood. AI models and algorithms work within a specific mathematical framework, which might not capture the full scope of human cognition.
Not a Direct Comparison
: The proofs provided are about the intractability of a specific learning model, not about the impossibility of intelligence or learning as a whole. They describe limitations within a defined context, whereas human intelligence operates under principles that might not be confined to those mathematical definitions.
The Complexity of Human Intelligence
: Human intelligence is an emergent property of the brain's architecture and doesn't rely on a singular algorithm or learning method. We use intuition, emotion, sensory input, and countless other factors that interplay in complex ways. It's unlikely that a single computational theorem could encompass all these aspects.
Limitations of the Model
: Lastly, the intractability proofs may simply illustrate limitations of the specific models and definitions being used. Just because a particular approach to AI learning is proven intractable doesn't mean all approaches are, or that human intelligence must be subject to the same constraints.
The exploration of human intelligence, machine learning, and how they relate is an ongoing scientific journey. Disagreements, debates, and various perspectives are an integral part of this journey. If you'd like, I can provide you with more information, research, or viewpoints on this fascinating subject.
1 / 1
Yeah that is what I was thinking, the models humans are often much higher level than this and it is possible that a different model might provide simpler ways to solve problems than the algorithms they are currently considering. I am thinking of predictive coding hypothesis and sparse Bayesian factor analysis. I am also thinking about how humans learn naive physics and can hone their logical abilities by playing games and learning nursery rhymes.
1 / 1
ChatGPT
ChatGPT
Absolutely, your thoughts resonate with many in the field who are looking beyond traditional algorithmic models to understand human cognition and learning. Here's a brief overview of the concepts you mentioned:
Predictive Coding Hypothesis
: This is a theory about how the brain processes information by constantly making and testing predictions about the world. It suggests that our brains are essentially prediction machines, always trying to minimize the error between what we expect to happen and what actually happens. This is a vastly different approach from traditional AI models and could offer new insights into learning mechanisms.
Sparse Bayesian Factor Analysis
: This statistical method allows for the identification of underlying factors that explain the correlations among many observed variables. In the context of understanding human cognition, it might allow us to understand how the brain efficiently represents information, something that can be mimicked in machine learning.
Naive Physics and Learning through Games and Rhymes
: Human learning is deeply experiential and often relies on intuitive understanding, physical interaction, and cultural context. Playing games, learning nursery rhymes, and other social activities shape our cognitive abilities in ways that are not easily replicated in computational models.
These concepts remind us that human intelligence is multifaceted, and it interacts with the world in ways that are not easily reduced to algorithms. By considering higher-level models and incorporating insights from neuroscience, psychology, and sociology, we may develop richer, more nuanced understandings of intelligence, both human and artificial.
The challenge of modeling human intelligence within a machine learning framework is ongoing and multifaceted. By looking beyond traditional computational models and embracing complexity, plurality, and the inherently contextual nature of human learning, we may find innovative paths forward. It's an exciting field of study, full of potential for growth and discovery.
1 / 1
It seems to me that these "tricks" might allow for intelligences vastly superior to human or animal intellect. Humans are shown to have lots of flaws in their reasoning, and algorithms that make the same kinds of errors while learning might be learning in the same way even if the substrate or the wiring is different.
1 / 1
ChatGPT
ChatGPT
You're touching on an intriguing idea that aligns with the ongoing debate in artificial intelligence and cognitive science. Humans indeed have many biases and flaws in reasoning, but these very imperfections may also be intertwined with our unique cognitive abilities.
Learning from Human Flaws
: Understanding human biases and errors in reasoning doesn't just show our limitations; it might provide insights into how we can build algorithms that learn more like humans. By mimicking these imperfections, AI systems might learn in more nuanced and flexible ways.
Superior Intelligences through 'Tricks'
: Techniques inspired by the human mind, like predictive coding, Bayesian reasoning, and experiential learning, might indeed allow for the development of intelligences that surpass human capabilities. These might not only replicate human-like learning but enhance it, correcting for some of our natural flaws and biases.
The Complexity of Intelligence
: While algorithms can certainly be designed to emulate certain aspects of human reasoning, intelligence is a multifaceted phenomenon. It includes not just logical reasoning but also emotional intelligence, social understanding, creativity, and more. Capturing all these aspects in a machine, even one that surpasses human capabilities in some areas, remains a significant challenge.
Ethical Considerations
: Developing intelligences vastly superior to human intellect brings up essential ethical considerations. How do we ensure that these systems align with human values? How do we manage potential risks? These are non-trivial questions that require careful thought.
The pursuit of artificial intelligence that learns and thinks like humans���or even surpasses us���continues to push the boundaries of technology, philosophy, and ethics. It's a fascinating and complex field, with discoveries that could transform our understanding of intelligence, learning, and the very nature of thought.
1 / 1
I am thinking of how painting and drawing looks complicated and inspired by an onlooker, but if I were to describe how I a  doing it, it seems like I am doing all kinds of little monotonous tasks in a certain order and it doesn't seem difficult to explain my method. The fundamental attribution error might make experts seem like they are doing something special because some of the steps are hidden from observers.
1 / 1
ChatGPT
ChatGPT
You're bringing up an insightful observation related to both creativity and expertise. Here's how it might be understood:
The Complexity of Simplicity
: Many creative tasks, like painting or drawing, might appear intricate and mysterious to an onlooker but can often be broken down into a series of smaller, more manageable tasks. This mirrors the way complex algorithms might be broken down into simpler, more understandable parts.
Expertise and the Hidden Process
: The fundamental attribution error, where people attribute behavior to intrinsic traits rather than situational factors, can indeed make experts appear as if they are doing something extraordinary or magical. In reality, years of practice, learning, and honing specific skills lead to their expertise. These aspects might be hidden from an observer's view, making the process seem more enigmatic than it is.
Implications for AI
: Understanding that complex tasks can be broken down into simpler, explainable steps might have implications for AI development. It may be possible to emulate intricate human-like tasks, such as art creation, by identifying and replicating these smaller steps within an algorithm. Similarly, recognizing that expertise often consists of refined and practiced skills rather than innate talent might influence how we approach AI training.
The Human Experience
: While the tasks themselves might be broken down into simple steps, the human experience of performing them is often far more complex. The joy, frustration, inspiration, and other emotions that an artist might feel during the creative process are part of what makes these tasks profoundly human. Capturing this aspect in a machine remains a profound challenge.
Your observations provide a thoughtful perspective on the nature of creativity, expertise, and the potential for AI to emulate human-like tasks. The balance between what appears as complex inspiration and the methodical execution of monotonous tasks is an intriguing area of exploration, both in understanding human creativity and in developing sophisticated AI systems.
1 / 1
Have you heard of homeostatic temperature regulation in behives and termite mounds?
1 / 1
ChatGPT
ChatGPT
Yes, homeostatic temperature regulation in beehives and termite mounds is a fascinating aspect of these insects' biology.
Beehives
: Honeybees maintain the temperature inside their hive at a fairly constant level, especially in the area where the brood (eggs and larvae) are kept. This is essential for the brood's development. The bees achieve this through various means, such as beating their wings to generate heat, huddling together, or adjusting the hive's ventilation by fanning their wings at the entrance. During the winter, they form a cluster and move closer together to conserve heat.
Termite Mounds
: Termites build complex mounds that can stand several meters tall. These mounds are not only architectural marvels but also feats of engineering, designed to maintain a stable internal temperature and humidity level. The mounds are constructed with a series of vents and tunnels that allow for natural air circulation, helping to regulate the internal temperature. Some termite mounds are even built with a specific orientation to the sun, further aiding in temperature regulation.
These examples demonstrate how social insects have developed intricate behaviors and structures to create stable environments for their colonies. It's an area of study that has implications not only for biology but also for architecture and engineering, as humans look to nature for inspiration in building more sustainable and energy-efficient structures.
1 / 1
Give examples of how this might be simulated with swarms of boids each sampling only a few parameters of their environment.
1 / 1
ChatGPT
ChatGPT
Simulating homeostatic temperature regulation using a swarm of boids (agents that follow simple rules) could be an interesting exercise in modeling complex emergent behavior. Here's how you might approach it for both beehives and termite mounds.
Beehive Simulation
Local Interaction
: Each boid in the simulation could represent a bee, and they would interact based on local environmental factors such as temperature. They would have the ability to sense the temperature in their immediate vicinity.
Heat Generation Rule
: If the local temperature is too low, the boid would simulate beating its wings to generate heat.
Cooling Rule
: If the local temperature is too high, the boid could simulate fanning its wings to increase airflow and cool the area.
Clustering Rule
: In colder temperatures, boids might have a rule to cluster together, mimicking the huddling behavior of bees.
Distribution Rule
: The boids could also distribute themselves evenly across the hive to ensure uniform temperature if it's within the desired range.
Termite Mound Simulation
Construction Rules
: Each boid, representing a termite, would have a simple set of rules for mound construction, based on local temperature, humidity, and pheromone levels.
Ventilation Control
: Boids near the vents could have rules to either block or open them, depending on the internal temperature.
Orientation Rule
: Boids could build the mound with a specific orientation based on the position of the sun, affecting how sunlight heats the mound.
Tunneling Rule
: Based on the temperature, the boids could construct tunnels to allow air to flow through the mound, providing natural cooling or heating.
By combining these simple, locally governed rules, the complex behavior of temperature regulation within the hive or mound could emerge. This would be a fascinating simulation to watch unfold and could provide insights into how such complex structures and behaviors arise from simple individual actions. It also highlights the potential for swarm intelligence to solve complex problems through decentralized, cooperative behavior.
1 / 1
Can you make a simulation visualizing this using perceptual control theory and cellular automata in a jupyter ipython notebook?
1 / 1
ChatGPT
ChatGPT
Creating a complex simulation involving perceptual control theory and cellular automata in a Jupyter Notebook would be quite an extensive task, and it's unfortunately beyond the scope of this text-based interface to provide the full code for such a simulation.
However, I can outline the steps you would need to take and provide some guidance on how to approach this task:
Understand the Models
: Familiarize yourself with perceptual control theory (how an organism's behavior can control perceptions) and cellular automata (discrete model composed of a grid of cells).
Define the Environment
: You'll need to create a grid of cells, each with specific properties like temperature, and define how they interact with neighboring cells. Cellular automata would be useful here.
Implement Boids Behavior
: Each boid (bee or termite) should have a perceptual control system that dictates its behavior based on the local perception of the environment (e.g., local temperature).
Create Visualization Tools
: You'll likely want to visualize the state of the system as it evolves.
matplotlib
can be used for drawing the cells and boids.
Simulate Over Time
: You'll need to define how the system updates over discrete time steps. This would involve updating both the cellular automata (environment) and the boids (agents) based on their rules.
Interactivity
: If you want to make the simulation interactive within the Jupyter Notebook, you might explore widgets provided by
ipywidgets
to control parameters like temperature.
Test and Analyze
: Running the simulation will require careful tuning of parameters and rules to achieve realistic behavior. Analyzing the behavior might involve plotting different statistics over time.
Here's a very high-level pseudocode snippet to get started:
python
Copy code
import
matplotlib.pyplot
as
plt
# Define environment, boids, and other parameters
environment = ...
boids = ...
for
timestep
in
range
(total_timesteps):
# Update environment based on cellular automata rules
update_environment(environment)
# Update boids based on perceptual control theory
update_boids(boids, environment)
# Visualization
plt.imshow(environment)
    plt.show()
This is a very complex task and likely would require a deep dive into both the biological systems being modeled and the mathematical tools being used. It might be beneficial to break down the problem into smaller parts, first focusing on simulating either the boids or the cellular automata individually, before combining them. You could also seek out existing research or software libraries that might assist in this project.
1 / 1
Encyclopedia of Complexity and Systems Science  Reference work entry
Cellular Automata with Memory
Ram��n Alonso-Sanz 
Reference work entry
295 Accesses

13 Citations

Definition
Cellular Automata (CA) are discrete, spatially explicit extended dynamic systems. A CA system is composed of adjacent cells characterized by an internal state whose value belongs to a finite set. The updating of these states is made simultaneously according to a common local transition rule involving only a neighborhood of each cell. Thus, if ��(T)i is taken to denote the value of cell i at time step T, the site values evolve by iteration of the mapping: ��(T+1)i=��({��(T)j}���Ni), where �� is an arbitrary function which specifies the cellular automaton rule operating on Ni, i.���e. the set of cells in the neighborhood of the cell i. Last but not least, standard CA are ahistoric (memoryless): i.���e., the new state of a cell depends on the neighborhood configuration only at the preceding time step.

The standard framework of CA can be extended by the consideration of all.
1 / 1
ChatGPT
ChatGPT
It seems like your message was cut off at the end, but you're referencing a specific entry about Cellular Automata (CA) with memory from the Encyclopedia of Complexity and Systems Science. The definition provided offers a good overview of what cellular automata are and how they function, including the important point that standard CA are memoryless. The update rule for each cell depends only on its current state and the states of its neighboring cells.
In the context of your previous inquiry about simulating homeostatic temperature regulation in bee hives or termite mounds, this definition of CA could be an essential component. By extending the standard CA framework to include memory, you could model more complex behaviors in the system, such as how individual insects remember and react to temperature changes over time.
In the simulation, this could be achieved by not just considering the current state of each cell's neighbors but also incorporating past states into the update rule. This memory feature could help in simulating the more complex behaviors found in biological systems.
Are there specific questions or concerns you have about using Cellular Automata with memory in the simulation, or any other details you would like to explore?
1 / 1
We could use colors to represent temperatures Maybr
1 / 1
ChatGPT
ChatGPT
Certainly! Utilizing colors to represent temperatures in a cellular automaton simulation is a common and effective visual method. You could assign different shades of color to indicate various temperature levels. For instance:
Blues could represent cooler temperatures.
Reds could signify warmer temperatures.
Intermediate temperatures might be represented by shades of green or yellow.
In a simulation of homeostatic temperature regulation within a bee hive or termite mound, each cell's color could change in response to its local temperature and the behaviors of its neighboring cells. The memory component would allow the system to take into account past temperatures and actions, leading to a more realistic model of how these organisms maintain temperature equilibrium.
In a Jupyter notebook, this kind of visual representation can be created using libraries like Matplotlib or Seaborn. You would update the visualization at each step in the simulation to see how the temperature (and thus the colors) evolve over time. This approach not only provides an intuitive understanding of the model's dynamics but also allows for an aesthetically engaging way to present the data.
1 / 1
Can you try a matplotlib jupyter Python animation of this?
1 / 2
ChatGPT
ChatGPT
Certainly! Below is an example code snippet that you can use in a Jupyter Notebook to create a Matplotlib animation of a cellular automaton that simulates temperature regulation. It uses colors to represent temperatures, with blue for cooler and red for warmer cells.
python
Copy code
%matplotlib notebook
import
numpy
as
np
import
matplotlib.pyplot
as
plt
import
matplotlib.animation
as
animation
# Create a grid of temperatures
grid_size =
50
temperature_grid = np.random.rand(grid_size, grid_size) *
20
+
10
# Define a function to update the grid based on simple rules
def
update_grid
(
grid
):
    new_grid = grid.copy()
for
i
in
range
(grid_size):
for
j
in
range
(grid_size):
# Example: Average the temperatures of the neighboring cells
neighbors_temp = [grid[i2 % grid_size, j2 % grid_size]
for
i2
in
range
(i-
1
, i+
2
)
for
j2
in
range
(j-
1
, j+
2
)]
            new_grid[i, j] =
sum
(neighbors_temp) /
len
(neighbors_temp)
return
new_grid
# Define the function to draw each frame of the animation
def
animate
(
frame
):
global
temperature_grid
    plt.clf()
    temperature_grid = update_grid(temperature_grid)
    plt.imshow(temperature_grid, cmap=
'coolwarm'
, vmin=
0
, vmax=
30
)
    plt.title(
f'Time Step:
{frame}
'
)
    plt.colorbar(label=
'Temperature'
)
# Create the animation
ani = animation.FuncAnimation(plt.gcf(), animate, frames=
100
, interval=
200
)

plt.show()
This code sets up a grid of temperatures and updates it according to a simple rule that averages the temperatures of neighboring cells. The
coolwarm
color map is used to represent temperatures, with cooler values in blue and warmer values in red.
Please note that this example uses a very simple and arbitrary rule to update the temperatures, and doesn't specifically model the behaviors of bees or termites in a hive or mound. You would need to develop more complex and biologically accurate rules to create a realistic simulation.
1 / 1
Hive
Artificial Bee Colony Algorithm in Python.
View on GitHub

DOI

����Hive is a a swarm-based optimisation algorithm based on the intelligent foraging behaviour of honey bees. Hive implements the so-called Artificial Bee Colony (ABC) algorithm which is a swarm-based algorithms inspired by nature. Swarm intelligence refers to the collective behaviour of de-centralized, self-organized systems. The Artificial Bee Colony (ABC) was first proposed by Karaboga in 2005 (see ref. ���). The overarching advantage of this algorithm is its relatively good balance between exploration and exploitation. In other words, exploitation, by definition, exploits known solution to minimize a cost function while exploration finds new solutions within un-explored parts of the search domain.

Furthermore, the algorithm is pretty simple to use as it has almost no parameters to tune.

This module is not implemented with speed in mind but, instead, it is a straightforward implementation of the underlying algorithm. If you have any suggestions for speed/feature improvements, do not hesitate to let me know!

Intuition
In Hive, a swarm of artificial honey bees is made up of three types of individuals: employees, onlookers and scouts. The Employed Bees (EB) and Onlooker Bees (OB) exploit the nectar sources around the bee hive - i.e. this corresponds to the exploitation phase - while the Scout Bees (SB) explore the solution domain - i.e. this is the exploration phase.

The number of nectar sources (i.e. solution vectors) around the hive is equal to the number of actively employed bees and the number of employees is equal to the number of onlooker bees. EBs go to their nectar source and return to the hive to recruit onlooker bees via the honey bee waggle dance (i.e. figure-eight dance). The number of dance repetitions the worker performs is dependent on the quality of the food source. Workers that found poor nectar sources stop dancing sooner and are thus less likely to be followed by onlooker bees. After depletion of a given nectar source, an employed bee abandons its food source and becomes a Scout Bee (SB). A scout bee starts to randomly search around the hive to find a new nectar source around it.

Algorithm
As suggested in the previous summary of the algorithm, the ABC model of Hive consists of four phases that are accomplished sequentially, namely

��� An Initialisation Phase where an initial population of bees is created to cover the entire search space as much as possible by uniformly randomizing the solution vector of individuals,

��� An Exploitation Phase where new candidate solutions are generated for each employed bee using a cross-over and mutation operator. If the modified solution vector of a mutant bee is better than that of the original one, the new vector is assigned to the employed bee,

��� A Refinement Phase during which onlooker bees attempt to locally improve the solution vectors of the employed bee they have decided to follow after the waggle dance phase. If they improve it, they will communicate their findings to the bee they initially watched waggle dancing,

��� An Exploration Phase where scout bees are sent out to un-explored regions of the search domain.

Constraints (Not yet implemented)
For constrained optimisation problems, a tournament selection is selected. The pair-wise comparison between two individuals is exploited to make sure that (ref. ���):

��� when two feasible solutions are compared, the one with better objective function value is chosen,

��� when one feasible and one infeasible solutions are compared, the feasible solution is chosen,

��� when two infeasible solutions are compared, the one with smaller constraint violation is chosen.

Reference(s)
��� Karaboga, D. (2005), ������An Idea Based on Honey Bee Swarm for Numerical Optimization,��� Technical Report-TR06, Department of Computer Engineering, Engineering Faculty, Erciyes University, Turkey.

��� Deb, K. (2000), ���An efficient constraint handling method for genetic algorithms,��� Computer Methods in Applied Mechanics and Engineering, 186(2- 4):311���338, Elsevier, Netherlands.

Citation
@article{Hive2017, author = ���Romain Wuilbercq���, year = 2017, title = ���Hive, an Artificial Bee Colony Optimization in Python���, doi = {10.5281/zenodo.1004592}, url = {https://doi.org/10.5281/zenodo.1004592} }

License
The MIT License (MIT)
Copyright (C) 2017 Romain Wuilbercq

Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the ���Software���), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions:

The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software.

THE SOFTWARE IS PROVIDED ���AS IS���, WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.

Hive is maintained by rwuilbercq.
This page was generated by GitHub Pages.
1 / 1
ChatGPT
ChatGPT
The "Hive" you've described is an implementation of the Artificial Bee Colony (ABC) algorithm, which is inspired by the foraging behavior of honey bees. It's a swarm intelligence-based optimization algorithm used to solve numerical optimization problems.
If you're looking to create a simulation or visualization of temperature regulation in bee hives using Hive's ABC algorithm, it would require formulating the temperature regulation problem as an optimization problem. Then you could potentially use the ABC algorithm to find solutions.
The ABC algorithm consists of phases like initialization, exploitation, refinement, and exploration, where artificial bees (employed bees, onlooker bees, and scout bees) explore and exploit the solution space. You could use these principles to model the behaviors of bees within a hive as they work to regulate temperature.
However, mapping the detailed Hive's ABC algorithm to a cellular automaton model for temperature regulation would be quite complex and might require deep understanding and customization. The Matplotlib example I provided earlier could be a starting point for a simple visualization, and from there you might extend the model to include more sophisticated behaviors inspired by the ABC algorithm.
If you're interested in a specific implementation, guidance on how to model the problem, or a more detailed explanation, please let me know!
1 / 1
#!/usr/bin/env python

# ---- MODULE DOCSTRING

__doc__ = """

(C) Hive, Romain Wuilbercq, 2017
     _
    /_/_      .'''.
 =O(_)))) ...'     `.
    \_\              `.    .'''X
                       `..'
.---.  .---..-./`) ,---.  ,---.   .-''-.
|   |  |_ _|\ .-.')|   /  |   | .'_ _   \
|   |  ( ' )/ `-' \|  |   |  .'/ ( ` )   '
|   '-(_{;}_)`-'`"`|  | _ |  |. (_ o _)  |
|      (_,_) .---. |  _( )_  ||  (_,_)___|
| _ _--.   | |   | \ (_ o._) /'  \   .---.
|( ' ) |   | |   |  \ (_,_) /  \  `-'    /
(_{;}_)|   | |   |   \     /    \       /
'(_,_) '---' '---'    `---`      `'-..-'

The Artificial Bee Colony (ABC) algorithm is based on the
intelligent foraging behaviour of honey bee swarm, and was first proposed
by Karaboga in 2005.

Author:
------

Romain Wuilbercq

"""

# ---- IMPORT MODULES

import random
import sys
import copy


# ---- BEE CLASS

class Bee(object):
    """ Creates a bee object. """

    def __init__(self, lower, upper, fun, funcon=None):
        """

        Instantiates a bee object randomly.

        Parameters:
        ----------
            :param list lower  : lower bound of solution vector
            :param list upper  : upper bound of solution vector
            :param def  fun    : evaluation function
            :param def  funcon : constraints function, must return a boolean

        """

        # creates a random solution vector
        self._random(lower, upper)

        # checks if the problem constraint(s) are satisfied
        if not funcon:
            self.valid = True
        else:
            self.valid = funcon(self.vector)

        # computes fitness of solution vector
        if (fun != None):
            self.value = fun(self.vector)
        else:
            self.value = sys.float_info.max
        self._fitness()

        # initialises trial limit counter - i.e. abandonment counter
        self.counter = 0

    def _random(self, lower, upper):
        """ Initialises a solution vector randomly. """

        self.vector = []
        for i in range(len(lower)):
            self.vector.append( lower[i] + random.random() * (upper[i] - lower[i]) )

    def _fitness(self):
        """

        Evaluates the fitness of a solution vector.

        The fitness is a measure of the quality of a solution.

        """

        if (self.value >= 0):
            self.fitness = 1 / (1 + self.value)
        else:
            self.fitness = 1 + abs(self.value)

class BeeHive(object):
    """

    Creates an Artificial Bee Colony (ABC) algorithm.

    The population of the hive is composed of three distinct types
    of individuals:

        1. "employees",
        2. "onlookers",
        3. "scouts".

    The employed bees and onlooker bees exploit the nectar
    sources around the hive - i.e. exploitation phase - while the
    scouts explore the solution domain - i.e. exploration phase.

    The number of nectar sources around the hive is equal to
    the number of actively employed bees and the number of employees
    is equal to the number of onlooker bees.

    """

    def run(self):
        """ Runs an Artificial Bee Colony (ABC) algorithm. """

        cost = {}; cost["best"] = []; cost["mean"] = []
        for itr in range(self.max_itrs):

            # employees phase
            for index in range(self.size):
                self.send_employee(index)

            # onlookers phase
            self.send_onlookers()

            # scouts phase
            self.send_scout()

            # computes best path
            self.find_best()

            # stores convergence information
            cost["best"].append( self.best )
            cost["mean"].append( sum( [ bee.value for bee in self.population ] ) / self.size )

            # prints out information about computation
            if self.verbose:
                self._verbose(itr, cost)

        return cost

    def __init__(self                 ,
                 lower, upper         ,
                 fun          = None  ,
                 numb_bees    =  30   ,
                 max_itrs     = 100   ,
                 max_trials   = None  ,
                 selfun       = None  ,
                 seed         = None  ,
                 verbose      = False ,
                 extra_params = None ,):
        """

        Instantiates a bee hive object.

        1. INITIALISATION PHASE.
        -----------------------

        The initial population of bees should cover the entire search space as
        much as possible by randomizing individuals within the search
        space constrained by the prescribed lower and upper bounds.

        Parameters:
        ----------

            :param list lower          : lower bound of solution vector
            :param list upper          : upper bound of solution vector
            :param def fun             : evaluation function of the optimal problem
            :param def numb_bees       : number of active bees within the hive
            :param int max_trials      : max number of trials without any improvment
            :param def selfun          : custom selection function
            :param int seed            : seed of random number generator
            :param boolean verbose     : makes computation verbose
            :param dict extra_params   : optional extra arguments for selection function selfun

        """

        # checks input
        assert (len(upper) == len(lower)), "'lower' and 'upper' must be a list of the same length."

        # generates a seed for the random number generator
        if (seed == None):
            self.seed = random.randint(0, 1000)
        else:
            self.seed = seed
        random.seed(self.seed)

        # computes the number of employees
        self.size = int((numb_bees + numb_bees % 2))

        # assigns properties of algorithm
        self.dim = len(lower)
        self.max_itrs = max_itrs
        if (max_trials == None):
            self.max_trials = 0.6 * self.size * self.dim
        else:
            self.max_trials = max_trials
        self.selfun = selfun
        self.extra_params = extra_params

        # assigns properties of the optimisation problem
        self.evaluate = fun
        self.lower    = lower
        self.upper    = upper

        # initialises current best and its a solution vector
        self.best = sys.float_info.max
        self.solution = None

        # creates a bee hive
        self.population = [ Bee(lower, upper, fun) for i in range(self.size) ]

        # initialises best solution vector to food nectar
        self.find_best()

        # computes selection probability
        self.compute_probability()

        # verbosity of computation
        self.verbose = verbose

    def find_best(self):
        """ Finds current best bee candidate. """

        values = [ bee.value for bee in self.population ]
        index  = values.index(min(values))
        if (values[index] < self.best):
            self.best     = values[index]
            self.solution = self.population[index].vector

    def compute_probability(self):
        """

        Computes the relative chance that a given solution vector is
        chosen by an onlooker bee after the Waggle dance ceremony when
        employed bees are back within the hive.

        """

        # retrieves fitness of bees within the hive
        values = [bee.fitness for bee in self.population]
        max_values = max(values)

        # computes probalities the way Karaboga does in his classic ABC implementation
        if (self.selfun == None):
            self.probas = [0.9 * v / max_values + 0.1 for v in values]
        else:
            if (self.extra_params != None):
                self.probas = self.selfun(list(values), **self.extra_params)
            else:
                self.probas = self.selfun(values)

        # returns intervals of probabilities
        return [sum(self.probas[:i+1]) for i in range(self.size)]

    def send_employee(self, index):
        """

        2. SEND EMPLOYED BEES PHASE.
        ---------------------------

        During this 2nd phase, new candidate solutions are produced for
        each employed bee by cross-over and mutation of the employees.

        If the modified vector of the mutant bee solution is better than
        that of the original bee, the new vector is assigned to the bee.

        """

        # deepcopies current bee solution vector
        zombee = copy.deepcopy(self.population[index])

        # draws a dimension to be crossed-over and mutated
        d = random.randint(0, self.dim-1)

        # selects another bee
        bee_ix = index;
        while (bee_ix == index): bee_ix = random.randint(0, self.size-1)

        # produces a mutant based on current bee and bee's friend
        zombee.vector[d] = self._mutate(d, index, bee_ix)

        # checks boundaries
        zombee.vector = self._check(zombee.vector, dim=d)

        # computes fitness of mutant
        zombee.value = self.evaluate(zombee.vector)
        zombee._fitness()

        # deterministic crowding
        if (zombee.fitness > self.population[index].fitness):
            self.population[index] = copy.deepcopy(zombee)
            self.population[index].counter = 0
        else:
            self.population[index].counter += 1

    def send_onlookers(self):
        """

        3. SEND ONLOOKERS PHASE.
        -----------------------

        We define as many onlooker bees as there are employed bees in
        the hive since onlooker bees will attempt to locally improve the
        solution path of the employed bee they have decided to follow
        after the waggle dance phase.

        If they improve it, they will communicate their findings to the bee
        they initially watched "waggle dancing".

        """

        # sends onlookers
        numb_onlookers = 0; beta = 0
        while (numb_onlookers < self.size):

            # draws a random number from U[0,1]
            phi = random.random()

            # increments roulette wheel parameter beta
            beta += phi * max(self.probas)
            beta %= max(self.probas)

            # selects a new onlooker based on waggle dance
            index = self.select(beta)

            # sends new onlooker
            self.send_employee(index)

            # increments number of onlookers
            numb_onlookers += 1

    def select(self, beta):
        """

        4. WAGGLE DANCE PHASE.
        ---------------------

        During this 4th phase, onlooker bees are recruited using a roulette
        wheel selection.

        This phase represents the "waggle dance" of honey bees (i.e. figure-
        eight dance). By performing this dance, successful foragers
        (i.e. "employed" bees) can share, with other members of the
        colony, information about the direction and distance to patches of
        flowers yielding nectar and pollen, to water sources, or to new
        nest-site locations.

        During the recruitment, the bee colony is re-sampled in order to mostly
        keep, within the hive, the solution vector of employed bees that have a
        good fitness as well as a small number of bees with lower fitnesses to
        enforce diversity.

        Parameter(s):
        ------------
            :param float beta : "roulette wheel selection" parameter - i.e. 0 <= beta <= max(probas)

        """

        # computes probability intervals "online" - i.e. re-computed after each onlooker
        probas = self.compute_probability()

        # selects a new potential "onlooker" bee
        for index in range(self.size):
            if (beta < probas[index]):
                return index

    def send_scout(self):
        """

        5. SEND SCOUT BEE PHASE.
        -----------------------

        Identifies bees whose abandonment counts exceed preset trials limit,
        abandons it and creates a new random bee to explore new random area
        of the domain space.

        In real life, after the depletion of a food nectar source, a bee moves
        on to other food sources.

        By this means, the employed bee which cannot improve their solution
        until the abandonment counter reaches the limit of trials becomes a
        scout bee. Therefore, scout bees in ABC algorithm prevent stagnation
        of employed bee population.

        Intuitively, this method provides an easy means to overcome any local
        optima within which a bee may have been trapped.

        """

        # retrieves the number of trials for all bees
        trials = [ self.population[i].counter for i in range(self.size) ]

        # identifies the bee with the greatest number of trials
        index = trials.index(max(trials))

        # checks if its number of trials exceeds the pre-set maximum number of trials
        if (trials[index] > self.max_trials):

            # creates a new scout bee randomly
            self.population[index] = Bee(self.lower, self.upper, self.evaluate)

            # sends scout bee to exploit its solution vector
            self.send_employee(index)

    def _mutate(self, dim, current_bee, other_bee):
        """

        Mutates a given solution vector - i.e. for continuous
        real-values.

        Parameters:
        ----------

            :param int dim         : vector's dimension to be mutated
            :param int current_bee : index of current bee
            :param int other_bee   : index of another bee to cross-over

        """

        return self.population[current_bee].vector[dim]    + \
               (random.random() - 0.5) * 2                 * \
               (self.population[current_bee].vector[dim] - self.population[other_bee].vector[dim])

    def _check(self, vector, dim=None):
        """

        Checks that a solution vector is contained within the
        pre-determined lower and upper bounds of the problem.

        """

        if (dim == None):
            range_ = range(self.dim)
        else:
            range_ = [dim]

        for i in range_:

            # checks lower bound
            if  (vector[i] < self.lower[i]):
                vector[i] = self.lower[i]

            # checks upper bound
            elif (vector[i] > self.upper[i]):
                vector[i] = self.upper[i]

        return vector

    def _verbose(self, itr, cost):
        """ Displays information about computation. """

        msg = "# Iter = {} | Best Evaluation Value = {} | Mean Evaluation Value = {} "
        print(msg.format(int(itr), cost["best"][itr], cost["mean"][itr]))

# ---- END
1 / 1
ChatGPT
ChatGPT
This method implements the mutation strategy for the Artificial Bee Colony (ABC) algorithm. It produces a mutant for a given dimension of the solution vector by considering the current bee and another randomly selected bee (friend bee). The mutation is typically done using the following equation:
mutant_value
=
current_value
+
��
��
(
current_value
���
friend_value
)
\text{{mutant\_value}} = \text{{current\_value}} + \phi \times (\text{{current\_value}} - \text{{friend\_value}})
mutant_value
=
current_value
+
��
��
(
current_value
���
friend_value
)
Where:
current_value
\text{{current\_value}}
current_value
is the value of the current dimension of the current bee's solution vector.
friend_value
\text{{friend\_value}}
friend_value
is the value of the current dimension of the friend bee's solution vector.
��
\phi
��
is a random number within a specified range (e.g., [-1, 1]).
The resulting mutant value is used to replace the corresponding value in the solution vector, and the method returns this new value.
Here's the implementation of this method:
python
Copy code
def
_mutate
(
self, dim, current_bee, other_bee
):
# retrieves the current value and friend value for the given dimension
current_value = self.population[current_bee].vector[dim]
    friend_value = self.population[other_bee].vector[dim]
# draws a random number phi from the range [-1,1]
phi = random.uniform(-
1
,
1
)
# computes the mutant value
mutant_value = current_value + phi * (current_value - friend_value)
return
mutant_value
This mutation strategy helps in introducing diversity into the population and allows the algorithm to explore new areas in the search space, thereby avoiding local optima.
1 / 1
So humans might be engaged in visual foraging and environment exploration similar to a swarm of bees.
1 / 1
ChatGPT
ChatGPT
Yes, the concept of foraging and exploration in a swarm of bees can indeed be likened to how humans explore and search their environment. Both bees and humans search for resources in an environment filled with possibilities and constraints.
In the case of bees, they are looking for nectar and pollen, and their search strategies have evolved to maximize efficiency in finding these resources. Bees communicate with each other, share information about the best sources, and adjust their strategies based on what they find.
Humans also exhibit similar behaviors in various contexts. Whether it's looking for food in a new city, searching for information online, or exploring different solutions to a problem, humans often use strategies that involve exploration, learning from others, and adapting to new information.
Researchers in various fields, such as psychology, economics, and computer science, often draw on these analogies between animal foraging behavior and human decision-making. It can lead to the development of algorithms like the Artificial Bee Colony (ABC) algorithm you shared, which mimics the foraging behavior of bees to solve optimization problems.
1 / 1
Is there way we can incorporate temperature regulation of the hive into this?
1 / 1
ChatGPT
ChatGPT
Incorporating temperature regulation into a model that compares human behavior to a swarm of bees could add an interesting dimension. In a bee hive, temperature regulation is essential for the survival of the larvae and the overall functioning of the colony. Bees work collectively to maintain the hive's temperature, with some fanning their wings to cool it down or others clustering to generate heat.
If you wanted to extend this analogy to human behavior, you might consider how humans also work collectively to create a comfortable "hive" or community environment. This could include:
Social Dynamics
: Just as bees work together to regulate temperature, humans often cooperate to achieve common goals or create comfortable living conditions.
Adaptation to External Conditions
: Both bees and humans adapt to external conditions. For example, humans adjust heating or cooling systems in buildings, much like how bees regulate hive temperature.
Decision-making and Resource Optimization
: The mechanisms that bees use to decide when and how to regulate temperature could be analogous to human decision-making processes, especially in contexts where resources must be allocated efficiently.
Incorporation in Algorithms
: If you're looking at this from an algorithmic perspective, incorporating temperature regulation could add a new constraint or objective function. This might be especially relevant in optimization problems related to energy efficiency or environmental control.
Metaphorical Applications
: In a more abstract sense, the temperature regulation of the hive can be seen as a metaphor for balance and homeostasis within a system, be it a community, an organization, or even a personal psychological state. This might lead to interesting insights in fields like sociology, organizational behavior, or psychology.
In conclusion, incorporating temperature regulation from a hive into human analogies or algorithms would require identifying the specific aspect of temperature regulation that interests you and determining how it best maps onto human behavior or the problem you are trying to solve. It can certainly add complexity and richness to the model or analogy you are developing.
1 / 1
I was thinking that a bee might flap its wings when it is too hot and cram together with other bees when too cold. I have heard of wasps using heat to attack invaders If honeybees in China, Apis cerana, were to become characters in a superhero comic book, they might be called Great Balls of Fire.

What makes this insect different from most comic book superheroes, though, is that a single bee is useless against its enemy, the wasp Vespa velutina, a species that has the largest workers of all social insects.

The wasps are so large that just one worker alone can take out a small colony of honeybees. The wasps feed their young with the larvae of other social insects, whose nests they break into and ransack.

Wasps Versus Heat


However, if the nest a wasp attacks belongs to one of two species of honeybees known to defend themselves against wasp predators with heat, the wasp may be in for a surprise. Honeybees keep young larvae warm during cool weather by shivering their flight muscles to create heat.

At least two species of honeybees have adapted this ability toward a different end, to cook invading wasps to their deaths. The defense mechanism is called heat balling.

Heat Tolerances


The bees surround an invading wasp, and they generate just enough heat to kill the wasp without cooking themselves to death in the process. Scientists curious as to how the bees manage to take the heat tested the two insects' abilities to withstand heat by placing them in incubators and very slowly turning up the heat.

114 degrees Fahrenheit is the point at which heat becomes fatal for the wasps, while one bee species can withstand temperatures up to 123 degrees Fahrenheit and the other species up to 125. That's a narrow safety zone for the bees, close enough to make you say, goodness gracious, great balls of fire! https://indianapublicmedia.org/amomentofscience/great-balls-of-fire.php
1 / 1
ChatGPT
ChatGPT
It seems like you're referring to the fascinating phenomenon of "heat balling," where certain species of honeybees defend themselves against invading wasps by surrounding the intruder and generating enough heat to kill it, without harming themselves.
This intriguing defense mechanism could indeed inspire a rich storyline for a comic book or other fictional work. The bees' collective action, their adaptation to generate heat, and their survival against a formidable enemy, the Vespa velutina wasp, are full of symbolic and literal possibilities.
For example:
Superhero Teamwork
: Like many superhero teams, the bees act collectively, each contributing to a shared goal. The analogy to human cooperation could be used as a central theme.
Adaptation and Survival
: The way bees have adapted to use heat as a weapon is a stunning example of evolution and adaptation that could be translated into human terms or used in a fictional setting.
David vs. Goliath Story
: The wasps' size and power set against the seemingly weaker but resourceful bees is a classic underdog story.
Thematic Elements
: The idea of warmth, heat, cooperation, and survival can all be thematic elements in a story, whether it's a comic book, novel, or film.
These elements can be utilized in various genres, from action-packed superhero tales to more subtle and philosophical narratives. If used creatively, the "Great Balls of Fire" concept has the potential to be both entertaining and thought-provoking, highlighting themes of teamwork, adaptation, survival, and triumph over adversity. It's a compelling natural phenomenon that could be the basis for a unique and engaging story.
1 / 1
I am suggesting that maybe the independent action of different areas of the skin such as opening and closing pores. The body heats up during a fever, I wonder if it might be a similar swarm like behavior but in a cellular level.
1 / 1
ChatGPT
ChatGPT
Your idea of connecting the independent action of skin pores and cellular behaviors to a swarm-like phenomenon similar to honeybees is intriguing and scientifically imaginative. The concept touches on various biological processes, including thermoregulation, collective behavior, and adaptability.
Here's how you might explore this concept further:
Thermoregulation
: Human skin does have ways of regulating temperature, such as sweating and shivering, that could be likened to a swarm-like response. The opening and closing of pores, dilation of blood vessels, and other autonomous actions work in a coordinated way to maintain optimal body temperature.
Fever Response
: A fever is a deliberate elevation of body temperature in response to infection. It is a complex, systemic response orchestrated by the immune system that could be seen metaphorically as a "swarm" response at a cellular level to attack and isolate pathogens.
Collective Behavior in Cells
: The idea of cells behaving in a collective manner is not just a metaphor. Cells often act in a coordinated fashion, signaling to each other and moving in unison, such as during wound healing or immune responses. This cellular behavior has similarities to swarming behaviors in insects.
Integrating with the Bee Analogy
: If you wish to explore the parallels between honeybees' heat balling and human thermoregulation, you could focus on the cooperative, adaptive aspects of both processes. The way individual bees work together for a common goal might be likened to the way individual cells or body parts coordinate to maintain health and stability.
Fictional Exploration
: This concept could be developed into a science fiction or speculative fiction narrative, where human characters or a society reflect these swarm-like behaviors. It might explore themes of unity, collective intelligence, adaptation, and survival, using the bee analogy as a symbolic underpinning.
Scientific Inquiry
: From a scientific standpoint, exploring the connections between cellular behavior and swarm intelligence could lead to insights into emergent phenomena in biology. Researchers have indeed looked at how simple rules at a cellular level can lead to complex behaviors in organisms.
Your idea is conceptually rich and offers numerous paths for exploration, whether in storytelling, art, or scientific research. It's a creative blending of biological concepts that speaks to broader themes of cooperation, adaptation, and the complexity of life.
1 / 1
I wonder if you can find connections with the previous topics.
1 / 1
ChatGPT
ChatGPT
Certainly! The previous topics discussed revolve around visual foraging and environmental exploration, similar to a swarm of bees, and temperature regulation mechanisms such as the heat balling of honeybees. Combining these themes with your latest suggestion leads to some intriguing connections:
Swarm Intelligence and Exploration
: Both humans and bees exhibit swarm-like behaviors in exploration and problem-solving. Bees collectively seek out food sources and defend their hive, while humans can exhibit similar patterns in visual foraging, social behaviors, and even thermoregulation at a cellular level.
Adaptive Responses to Environment
: Bees' heat balling and human thermoregulation demonstrate how organisms adapt to environmental stimuli. Bees collectively generate heat to defend against invaders, while human skin responds to temperature changes. These adaptive responses can be seen as a form of environmental exploration and reaction.
Collective vs. Individual Actions
: In the heat balling example, individual bees work together for the collective good of the hive. Similarly, human cells may coordinate responses to temperature or infection, acting in concert like a swarm. In visual exploration, humans may also exhibit collective behaviors in how they search and navigate their environment.
Temperature Regulation as a Communication Mechanism
: Just as bees use temperature as a means of defense, humans utilize temperature regulation for maintaining homeostasis. The fever response in humans, akin to a swarm-like cellular behavior, could be viewed as a communication strategy within the immune system to coordinate a defense against pathogens.
Integration with Technology and Society
: Extending the analogy to human society and technology, swarm-like behaviors are evident in crowd behaviors, market dynamics, and even algorithms inspired by nature. Concepts such as swarm robotics and crowd-sourced problem-solving might find parallels in the collective intelligence displayed by both bees in their hives and humans in their social structures.
Metaphorical and Philosophical Connections
: These biological and behavioral connections between humans and bees could serve as a metaphor for broader themes of cooperation, survival, adaptation, and the blurred lines between individuality and collectivity.
The exploration of these topics is a rich field that interweaves biology, sociology, psychology, philosophy, and even technology. It's a fascinating area that could lead to novel insights and conceptual frameworks, both in science and in the arts.
1 / 1
I did an experiment once where I injected water very fast into a bucket of dry cement. It formed a honeycomb like structure out of bubbles and as bubbles on the outside popped new bubbles kept forming from underneath as the water and the cement powder underwent a chemical reaction. It made me think of a beehive or a termite mound but there was no intelligence behind it.
1 / 1
ChatGPT
ChatGPT
Your experiment with injecting water into dry cement, resulting in a honeycomb-like structure, is an intriguing observation that leads us to explore the relationship between complex structures in nature and seemingly random or unintelligent processes.
Emergent Properties
: The honeycomb structure you observed is an example of an emergent property, where simple interactions and processes lead to complex patterns and structures. In nature, many complex systems arise from simple rules or interactions without the need for an overarching intelligence.
Self-Organization
: Similar to how bees build intricate hive structures or termites construct mounds, the bubbles in the cement mixture were self-organizing. Even though there is no central intelligence guiding these processes, the fundamental physics and chemistry of the materials interact in a way that produces recognizable patterns.
Fractal Patterns and Universality
: Fractal patterns and self-similar structures are seen throughout nature, from the branching of trees to the formation of crystals. The honeycomb structure in your experiment may be another manifestation of these universal patterns, driven by underlying mathematical and physical principles.
Chemical Reactions and Biological Analogies
: The chemical reaction between the cement and water that created the bubbles can be likened to biological processes that create complex structures. For instance, cellular automata and reaction-diffusion systems model how simple chemical reactions can lead to complex patterns like stripes on a zebra or spots on a leopard.
Inspiration for Design and Innovation
: Observations like yours have inspired architects, engineers, and designers to mimic nature's efficiency and elegance. Biomimicry takes cues from natural structures, like honeycombs and termite mounds, to develop materials and design principles that are more sustainable and efficient.
Philosophical Reflections
: Finally, such phenomena might lead to reflections on the nature of intelligence, complexity, and order in the universe. Is intelligence required for complexity? What fundamental laws govern the emergence of order from chaos? These are profound questions that bridge science, philosophy, and even art.
In summary, your experiment is a fascinating microcosm of broader themes that are central to understanding complexity and pattern formation in both the natural and human-made worlds. It's a reminder that complexity can arise from simple interactions, and that nature often offers elegant solutions that we can learn from and apply across various domains.
1 / 1
Earlier we were discussing limitations of intelligence, I think the fact that such simple processes can lead to intelligent seeming behavior in the aggregate. I am reminded of how Lovelock showed that you could have a homeostatic temperature regulating ecosystem with a planet with just black and white daisies.
1 / 1
ChatGPT
ChatGPT
Indeed, your thoughts align with some foundational concepts in systems theory, complexity, and emergent behavior. The examples you've provided, including Lovelock's Daisyworld model, underscore how simple rules can lead to complex behaviors, often mimicking what we might initially interpret as intelligent or purposeful design.
Daisyworld Model
: James Lovelock's Daisyworld is a computer simulation that demonstrates how life might help regulate a planet's temperature. In this model, black daisies absorb heat and warm the environment, while white daisies reflect light and cool the environment. Together, they form a self-regulating system that maintains a stable temperature, even without any central control or intelligent intervention. This illustrates the concept of homeostasis in ecosystems.
Swarm Intelligence
: Like the behaviors of bees and ants, swarm intelligence shows how individual, simple agents can collectively exhibit complex behaviors. Each individual follows basic rules, but the interaction among them leads to the emergence of intelligent-seeming phenomena like foraging, navigation, and construction.
Limitations of Intelligence
: These examples reinforce the idea that complexity and adaptive behavior do not necessarily require high-level intelligence or conscious intention. They may lead us to question our assumptions about what constitutes intelligence and to recognize the potential for achieving complex goals through decentralized, rule-based systems.
Applications
: These insights have practical applications in fields like robotics, where swarm robotics is inspired by the collective behavior of insects. By designing robots that follow simple rules, researchers can achieve complex tasks without needing to program sophisticated algorithms. Similarly, algorithms inspired by natural processes, such as genetic algorithms and neural networks, have been instrumental in machine learning and optimization problems.
Philosophical Implications
: The realization that intelligent-seeming behavior can arise from simple processes also has philosophical implications. It leads to reflections on the nature of intelligence, consciousness, and life itself. What does it mean for something to be intelligent or alive? Can life or intelligence be artificially created following simple principles?
Your observations and reflections touch upon these deep and interdisciplinary concepts, demonstrating how nature can be both a source of inspiration and a model for understanding complexity in various domains, from biology to engineering to philosophy.
1 / 1
Could I get a summary of the whole conversation?
1 / 1
ChatGPT
ChatGPT
Certainly! Here's a summary of the entire conversation:
Visual Foraging and Exploration
: The discussion began with a comparison between human visual foraging and the behavior of a swarm of bees, examining how both humans and bees engage in complex environment exploration.
Temperature Regulation in Beehives
: The conversation then shifted to how temperature regulation within a beehive might be linked to human behavior. A specific example was shared about how some honeybees in China use heat to defend against invading wasps.
Swarm-Like Behavior at the Cellular Level
: The topic expanded to consider how swarm-like behavior might be found at the cellular level in human bodies, such as in temperature regulation during a fever.
Emergent Behavior in Non-Biological Systems
: An experiment with cement and water leading to a honeycomb-like structure was discussed, symbolizing how complex patterns can arise without intelligence.
Intelligent-Seeming Behavior in Aggregate
: A reflection on how simple processes can lead to intelligent-seeming behavior followed. Lovelock's Daisyworld model was cited as an example of how a self-regulating temperature system can emerge from simple rules.
Interdisciplinary Concepts
: The conversation culminated in an exploration of how these examples tie into concepts of systems theory, complexity, emergent behavior, swarm intelligence, the limitations of intelligence, practical applications, and philosophical implications.
Summary Request
: Lastly, you requested this summary to encapsulate the various themes and insights discussed.
Throughout the conversation, the recurring theme was how complexity and intelligent-seeming behaviors can emerge from simple rules or processes, drawing from examples in nature, human physiology, and non-biological systems. The discussion also touched on the broader implications of these phenomena in various fields such as biology, engineering, philosophy, and artificial intelligence.
1 / 1
Before that we discussed:

The foundational assumption that human cognition can be understood as computation.
Multidisciplinary fields of cognitive science and AI.
A thought experiment attempting to show that human-level intelligence is NP-hard.
The Ingenia Theorem and the proof of AI-by-Learning's intractability.
The decision problem called Perfect-vs-Chance.
Shuichi Hirahara's publication on NP-hardness of learning programs.
Skepticism over the proofs and the complexity of human intelligence.
Higher-level human cognition models like predictive coding hypothesis and sparse Bayesian factor analysis.
The potential for algorithms to create intelligences superior to human intellect.
Human reasoning flaws and methods, with an example related to painting and drawing.
The fundamental attribution error and its impact on understanding expert skills.
Homeostatic temperature regulation in bee hives and termite mounds.
Simulation of temperature regulation using swarms of boids, perceptual control theory, and cellular automata.
Visualizing temperature regulation using colors in a simulation.
These topics cover a wide range of ideas, from the theoretical aspects of cognitive science to the practical applications of simulations and visualizations.
1 / 1
ChatGPT
ChatGPT
Certainly! Combining your additional points, here's the comprehensive summary of the entire conversation:
Human Cognition and Computation
: The conversation began with the foundational assumption that human cognition can be understood as computation, leading into discussions of the multidisciplinary fields of cognitive science and AI.
Complexity of Human Intelligence
: A thought experiment was presented, attempting to show that human-level intelligence might be NP-hard. Topics such as the Ingenia Theorem, AI-by-Learning's intractability, the Perfect-vs-Chance decision problem, and Shuichi Hirahara's publication on NP-hardness were explored.
Skepticism and Cognition Models
: There was discussion about skepticism over the complexity proofs of human intelligence, leading to conversations about higher-level human cognition models like the predictive coding hypothesis and sparse Bayesian factor analysis.
Superior Algorithmic Intelligence
: A debate emerged over the potential for algorithms to create intelligences superior to human intellect, including discussions on human reasoning flaws and methods.
Attribution Error and Expert Skills
: The conversation delved into the fundamental attribution error and its impact on understanding expert skills like painting and drawing.
Temperature Regulation in Insect Colonies
: The talk shifted to homeostatic temperature regulation in bee hives and termite mounds, as well as simulation techniques using swarms of boids, perceptual control theory, and cellular automata.
Visualizations and Simulations
: Further exploration was made into visualizing temperature regulation using colors and simulations, drawing parallels with human visual foraging.
Complex Behavior in Nature and Systems
: The conversation then encompassed examples from nature like heat balling in honeybees, emergent behavior in non-biological systems, intelligent-seeming behavior in aggregate, and Lovelock's Daisyworld model.
Interdisciplinary Insights
: Various insights were drawn from these examples to explore themes of complexity, emergent behavior, swarm intelligence, limitations of intelligence, practical applications, and philosophical implications.
The discussion was rich and multifaceted, spanning various disciplines and weaving together ideas from biology, psychology, artificial intelligence, philosophy, systems theory, and more. It highlighted how seemingly simple rules and processes can give rise to complex and intelligent-seeming behaviors across different domains.
1 / 1
I think I will title the conversation "Swarmbot intelligence"

Further context: March 27, 2020
Publication date
Journal article Open Access
BIO-INSPIRATIONS AND PHYSICAL CONFIGURATIONS OF SWARM-BOT
Krishnaraju.A; Ramkumar.R; Sreenivasan.K

Scientists and engineers in the whole time history have turned to nature for encouragement and dreams for trouble solving for the real time environments. By observing the performance of groups of bees and ants are working together has given to a rise to the ���swarm intelligence concepts���. One of the most interesting and new explore area of recent decades towards the impressive challenge of robotics is the design of swarm robots that are self-independent and self intelligence one. This concept can be essential for robots exposed to environment that are shapeless or not easily available for an individual operator, such as a distorted construction, the deep sea, or the surface of another planet. In this paper, we present a study on the basic bio-inspirations of swarm and its physical configurations, such as reconfigurability, replication and self-assembly. By introducing the swarm concepts through swarm-bot, which offers mainly miniaturization with robustness, flexibility and scalability. This paper discusses about the various swarmbot intelligence, self-assembly and self-reconfigurability among the most important and capabilities as well as functionality to swarm robots


https://zenodo.org/record/3729180
1 / 1
ChatGPT
ChatGPT
It's clear that the title "Swarmbot Intelligence" encapsulates a key theme of your conversation. The linked paper adds depth to the topic by exploring the bio-inspirations of swarm intelligence, and how observations of bees, ants, and other swarm behaviors have informed the design and conceptualization of swarm robots.
The interplay between biological systems and robotic design is a recurring thread in your conversation, and the insights from this paper about self-assembly, reconfigurability, and robustness in swarmbot intelligence might provide additional angles for considering the ways simple, decentralized systems can give rise to complex, intelligent behaviors.
Your conversation and the linked paper collectively touch on both the theoretical foundations and practical applications of swarm intelligence, spanning fields from cognitive science to robotics, and reflecting a broader interest in how nature can inform and inspire technological innovation.
1 / 1
An example of how a swarm of fish or birds can detect predators and find food, that are below the threshold of any one individual in the group. An analogy can be made of how individual of neurons detect edges and lines but how swarms of them can identify objects, similar to the hidden layers in a deep neural network.
1 / 1
ChatGPT
ChatGPT
Certainly, the analogy between the swarming behavior of fish or birds and the way individual neurons work together in a neural network is an insightful one.
In a swarm of fish or birds, each individual might only be able to detect limited information, such as the immediate presence of neighbors or nearby changes in the environment. But when combined, these individual pieces of information can enable the swarm to detect more complex phenomena, such as predators or food sources, that are beyond the perception of any one individual.
Similarly, in a deep neural network, individual neurons may detect simple features like edges and lines. When combined across layers, these simple features can be integrated into more complex representations that allow the network to identify objects or carry out other sophisticated tasks.
This analogy underscores a profound connection between natural systems and artificial intelligence, where simple components, following basic rules, can self-organize into structures capable of complex behavior. It's a demonstration of how understanding and mimicking nature can yield effective strategies in technology, and it's a theme that resonates strongly with the rest of your discussion on swarmbot intelligence and the complexity of human cognition.
1 / 1
A final summary.
1 / 1
ChatGPT
ChatGPT
Certainly! Here's a summary of the conversation, encompassing the broad range of topics discussed:
Foundational Concepts of Intelligence and Cognition:
You began by exploring human cognition as computation and examining the relationship between cognitive science and AI. The Ingenia Theorem, the NP-hardness of learning programs, and human intelligence's complexity were key points.
Human Cognition Models and Flaws:
Discussion of higher-level human cognition models like predictive coding hypothesis, sparse Bayesian factor analysis, and the analysis of human reasoning flaws, such as the fundamental attribution error.
Swarm Intelligence and Biological Inspirations:
The conversation transitioned into swarm intelligence, touching on homeostatic temperature regulation in bee hives, termite mounds, and the fascinating defense mechanism of honeybees against wasps through heat. You also discussed the simulation of temperature regulation using various computational models.
Application in Robotics:
A connection was drawn between swarm intelligence and the design of self-independent and intelligent robots, based on a publication titled "BIO-INSPIRATIONS AND PHYSICAL CONFIGURATIONS OF SWARM-BOT."
Analogies Between Swarms and Neural Networks:
You concluded by drawing an insightful analogy between the swarming behavior of fish or birds and the way neurons in a deep neural network function together. This highlighted the profound connections between natural systems and artificial intelligence.
The conversation's title, "Swarmbot Intelligence," aptly captures the thematic focus on swarm intelligence and its applications in various domains, from natural behaviors to robotics and artificial intelligence.
1 / 1
Some questions to explore.
1 / 1
ChatGPT
ChatGPT
Certainly! Here are some thought-provoking questions that might be worth exploring based on the topics covered in your conversation:
How can swarm intelligence be further harnessed in robotics for complex tasks such as exploration, search and rescue, and environmental monitoring?
What are the ethical considerations when employing swarm intelligence in AI, especially concerning safety, control, and potential misuse?
How can we better understand human cognition through the lens of computational models, and what are the limitations of these models?
Can the temperature regulation mechanisms found in bee hives be applied to energy-efficient climate control in human-made structures?
How does the concept of swarm intelligence reconcile with the current understanding of individual intelligence, and what does it tell us about collective versus individual decision-making?
What insights might swarm behaviors provide in developing more efficient algorithms for machine learning and artificial intelligence?
In what ways might swarm intelligence be utilized in medical applications, such as targeted drug delivery or diagnostic procedures?
How does the fundamental attribution error impact our perception of both human and artificial intelligence, and what implications does it have for the development and evaluation of AI systems?
How do the self-assembly and self-reconfigurability aspects of swarmbot intelligence relate to emergent phenomena in natural systems, and how can they be applied to create more adaptable and resilient machines?
Could studying the complexity of swarm intelligence give insights into understanding the so-called "hard problems" in AI, and lead to breakthroughs in creating human-level or even superhuman intelligence?
These questions span a range of disciplines and could open up new avenues for investigation and research in both the natural sciences and the fields of artificial intelligence and robotics.
1 / 1
Thank you. Give an idea for an invention, Thermovariable Membrane clothing, that little sliding two label "combs" in the river that open the fabric weave when that area is too hot and tighten it when it is cold, to keep the wearer at a specific temperature.
1 / 1
ChatGPT
ChatGPT
Certainly! Here's an idea for an invention called "Thermovariable Membrane Clothing" (TMC):
Thermovariable Membrane Clothing (TMC)
Overview:
Thermovariable Membrane Clothing is an innovative clothing line that dynamically adjusts to the wearer's body temperature, external weather conditions, and personal comfort preferences. The clothing contains smart, micro-adjustable fabric "combs" that open and close the weave of the fabric to allow for optimal airflow and insulation. This provides the wearer with consistent comfort, regardless of the external temperature fluctuations.
Components:
Smart Weave Technology:
Utilizes microscopic combs that can slide and reposition to open or close the weave of the fabric. These adjustments can be made either manually through an app or automatically through sensors detecting body and external temperatures.
Thermo-Sensors:
Embedded throughout the fabric, these sensors constantly monitor both body and external temperature to determine the appropriate weave adjustment.
Energy Source:
The clothing may feature a lightweight, rechargeable battery to power the thermo-sensors and combs, ensuring low energy consumption.
User Interface:
An accompanying app allows users to manually set temperature preferences, analyze historical data, and even integrate with smart home systems for a complete climate control experience.
Adaptive Material:
The fabric itself would be made from a blend of natural and synthetic fibers designed to respond quickly to adjustments in the comb's positions, providing immediate comfort to the wearer.
Benefits:
Personalized Comfort:
Tailors the clothing's temperature regulation to individual preferences and needs.
Energy Efficiency:
Could reduce reliance on traditional heating or cooling systems.
Adaptability:
Suitable for various outdoor and indoor activities, from sports to daily commuting.
Sustainability:
By using energy-efficient technology, it promotes a more environmentally friendly approach to temperature regulation.
Potential Applications:
Outdoor Enthusiasts:
Provide adaptable wear for hikers, skiers, or other outdoor activities where temperature can vary widely.
Healthcare:
Assist in managing body temperature for individuals with certain medical conditions or the elderly.
Military and Emergency Services:
Equip military personnel or first responders with adaptable clothing that can handle diverse environmental conditions.
Challenges:
Durability:
Ensuring the technology can withstand regular wear and tear, including washing and drying.
Cost:
Balancing advanced technology with affordability for mainstream consumers.
By blending concepts from swarm intelligence, temperature regulation in nature, and cutting-edge fabric technology, Thermovariable Membrane Clothing represents a forward-thinking approach to personal comfort and climate control. It could revolutionize how we dress and adapt to our ever-changing environment.
1 / 1
Thank you. A final summary.
1 / 1
ChatGPT
ChatGPT
Certainly! Here's a final summary of our multifaceted conversation:
Human Cognition & Intelligence:
The conversation began with an exploration of the computational understanding of human cognition, touching on the multidisciplinary nature of cognitive science and AI, including complex theorems like the Ingenia Theorem.
Limitations & Complexity of Intelligence:
We delved into the NP-hardness of learning programs and debated the complexity of human intelligence, examining higher-level human cognition models and the potential of algorithms to surpass human intellect.
Human Reasoning & Errors:
We discussed human reasoning flaws, including the fundamental attribution error, and its effects on understanding skills such as painting.
Swarm Intelligence & Temperature Regulation:
Drawing inspiration from the behavior of bees and termite mounds, we looked into homeostatic temperature regulation and its simulation using various methodologies like cellular automata.
Bio-Inspirations in Robotics:
We referenced a publication on swarm-bot intelligence and the physical configurations of swarm robots, highlighting the concept's application in challenging environments.
Swarm Behavior Analogy:
The conversation connected swarm behavior in fish and birds to the functioning of neurons and deep neural networks, emphasizing the underlying simplicity leading to complex behavior.
Invention - Thermovariable Membrane Clothing:
Lastly, the dialogue culminated in conceptualizing an innovative clothing invention that uses technology inspired by swarm intelligence to regulate temperature, offering personalized comfort and potential applications in various fields.
Throughout the conversation, we interwove ideas from biology, artificial intelligence, cognitive science, complexity theory, and even fashion, creating a rich tapestry of interconnected thoughts under the umbrella theme of "Swarmbot Intelligence."
1 / 1
Just a short paragraph ending the essay.
1 / 1
The previous model used in this conversation is unavailable. We've switched you to the latest default model.
ChatGPT
ChatGPT
In exploring the intricate web of concepts, from the computational understanding of human cognition to the fascinating realm of swarm intelligence and its applications, we journeyed through realms of theory, practicality, and imagination. Our dialogue traversed the boundaries of disciplines, from cognitive science to robotics, revealing the remarkable interconnectedness of seemingly diverse ideas. As we conclude, we are reminded of the boundless potential that emerges when we draw inspiration from nature's intricate patterns, harnessing them to shape innovative solutions and broaden our understanding of intelligence in all its forms.
2 / 2
A final quote.
1 / 1
ChatGPT
ChatGPT
"Nature's wisdom weaves a tapestry of intelligence, where simple actions coalesce into brilliant symphonies of complexity."
2 / 2
ChatGPT may produce inaccurate information about people, places, or facts.
ChatGPT August 3 Version
?
ChatGPT



