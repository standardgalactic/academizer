Tom welcome. Thank you. Hello. Yes, welcome to me. Yes. Awesome. So we're going to talk
about the recent, the Slate Star Codex blog. There was a, I mean, it's a backstory, we're
going to have to fill in a lot of gaps because this goes back at least sort of six to eight months.
But just a few days ago, the New York Times brought out a long awaited piece about the
blog Slate Star Codex. And this is a very high profile blog, especially in the rationalist
community. And we'll kind of explain why that's significant, who the rationalists are. And it's
probably fair to say it's one of the most influential blogs in the world. Would that be fair?
Yeah, I mean, yeah, depending on how you define blog, I suppose. But yes, yes, I think that is
probably true. Yeah. Yeah. And thank you for joining me. You are literally the man who wrote
the book on the rationalists. Yes, yes, I did. Yes. Just to summarise maybe about why this is
significant. For me, it taps into many different issues. It's about sort of the decline of the
traditional media or the tensions of the traditional media versus kind of the upstart
alternative media. It's a very influential, important community, the rationalists, which
we'll talk about in a bit. And as you pointed out in a really good piece you wrote for Unheard,
it's also about a kind of way of dealing with ideas we don't like, whether we engage with them
or whether we kind of, whether we don't, and the art of persuasion, which you argue is something
that is quite rare in that piece. Do we tell the backstory first just to fill in the gaps?
Last June, well, it was very obvious that this rationalist community, in Scott Alexander in
particular, but the rationalist community more widely, had done a lot better than most people
at predicting or sort of preparing for the COVID pandemic. They were ahead of everyone on, look,
masks are a good idea. This is probably aerosol transmitted rather than
fomite transmitted. They just made shutting borders might be a good idea. They had the
sort of numerical thinking and the sort of quantified thinking that made them able to do
these judgments much better. And that attracted interest. And this guy from New York Times,
Cade Metz, is a tech reporter there. He got interested in that. People have said to him,
look, this blog, Slate Star Codex, has got these interesting people. It's been out ahead of all
the COVID stuff and it's worth paying attention to that. And so he went and he started speaking
to a rationalist. He came and spoke to me. He tried to speak to Scott. Scott did exchange emails with
him. I don't think he spoke in person or spoke on the phone, but he exchanged email.
And this was, as far as I was concerned, it was looking like it was going to be quite a
positive piece. But then something happened, which was New York Times said, if you remember,
Scott Alexander is a pseudonym. It's a sort of thin pseudonym. It's just his
first and middle names. And if you put it in a bit of effort, it's quite easy to work out
his real name. But New York Times said, well, we're going to reveal your real name.
And Scott Alexander said, well, that's a terrible idea because I am a psychiatrist and I need to
maintain a relationship with my patients. And if my name is splashed all over the internet,
that becomes harder, which has proved to be the case for other psychiatrists in the past and
struck me as a pretty reasonable point of view. But New York Times said, no, we must name you.
It's just our policy. There was some back and forth over whether that is actually
their policy, because I don't seem to name everyone, but anyway, by the by.
And so Scott, I think it's fair to say panicked or certainly got spooked and took down his entire
blog, five, what, seven years of work and put up a post instead saying, they're going to dox me
and it's threatening my safety. And I am basically, I'm scared. So I'm, I'm, I'm, I'm
taking it all off the internet until, until this goes away. And then I think in a really
basic tactical era, I think I'm really, I'm too sure he is disappointed. I think he regrets this,
he said in a piece that he regrets it anyway. And he then asked his readers to email
cavemets and cavemets is editor, tech editor at the New York Times to say, please don't
dox him. And of course, that means you get some poor people get 5,000 emails and they're in box the
next day. And it's to them. And anyway, from there, if suddenly you've got an antagonistic
relationship, there's also an issue that there are quite a lot of people on the internet really
don't like Slate Stark Odex and the blog, the Slate Stark Odex and, and Scott Alexander and they,
and they were sort of whispering, they would, they would then became contacts with cavemets and
saying, look, he's actually, you know, suggesting that he's actually linked to all bunch of scientific
racists. And he's actually men's rights activists and neoreactionaries and all these things, which
I would either say false or true in a misleading way, but you know, that's, that's an argument
for further down the line. And, and then this whole massive rail broke out. This was last June.
Other media organizations, including, you know, got involved there, New York, New Yorker wrote
a really interesting piece about the whole thing. And then everything went quiet for seven or eight
months. Scott Alexander quit his job out of all this thing. He decided what the sensible thing
to do was to quit his job as a psychiatrist, set up his own practice, run it differently and move
his blogging thing to substack so that he could get money and not worry about making money from
things. So totally reordered his life. So he was no longer afraid to put his real name out there,
put his real name out there about three weeks ago or something like that, or a month ago.
And then, and restarted on, on substack still blogging. And then that'll done. Then the New
York Times finally published its piece saying, this is the guy's name. And he shut off his blog.
And it was all a bit, when the New York Times piece came out, it was all a bit of an anti-climax.
And I wasn't actually that much there, except a few somewhat, to my mind, guilt by association
smears, you know, like this guy once mentioned Charles Murray on his blog, that sort of thing.
And it felt, felt like a bit of a damp squib in the end. But yeah, so that's what I think. I think
that's where we are now. I think in your piece, you said that you had actually spoken to,
to Scott and recommended that he speak to, to cavemets at the time that you, you thought it,
you thought it was going to be a positive piece.
I did. Yeah. I am, I don't know whether I misjudge, I, I, I, I emailed Scott, or had an exchange of
emails with Scott, and had a, and posted some, posted some things on the SlateStart Codex subreddit
and things and just sort of discussed it. Because I, I don't know, I, I, I, maybe that I'm a total
rube, right? And then I had Scott absolutely taken like a sucker. But I did not get, I got the
impression from speaking to Mets that he was really interested in how the rationalists had been
sort of so far out, out ahead on COVID and things. And that was, you know, I think that's
a really interesting thing. Because these are, it just, they're, these were not,
it's not a community of epidemiologists or virologists or anything. It's a community of
smart generalists who are comfortable with numbers and who could look, who did some, you know, really
basic things like look at an exponential trend and extend, extend that exponential trend upwards,
you know, and say, well, if this carries on, we should be really worried about it when other
people would, you know, and it's really, I think that is a really interesting, there's a really
interesting discussion there about knowledge and expertise and the difference between expertise
and forecasting ability. I think that's a fascinating topic, which I would love
someone to really go into in great depth. There was also, I mean, because the New Yorker wrote
this piece, this really sort of well-balanced and interesting piece about six months ago,
which was neither forging to the rationalists nor, you know, nor uncharitable. It was,
and I feel like the New York Times, when its piece came out, it couldn't just write the
same piece New Yorker had six months ago and say it was, you know, that's what we're doing now.
It had to come with some new angle. So I think that's why the, it went down on the
guilt by association stuff. But also, most of the piece seemed to be about,
about how the blog was deleted and how, you know, then, and now it's come back and
some people sent emails to cavemats. And I don't know, it felt like very,
it felt like they didn't have very much to say, but they felt they had to say it. Anyway,
I don't think he went in there with the intention of a hatchet job. I think he went in there
trying to write about an interesting group of people who have had this interesting
sort of been out there ahead of the game on COVID. And I think that
for reasons which I both, which I understand, and to some, yeah, to some extent sympathize with
insofar as it was this sort of mutual antagonism thing and a lot of burning bridges on both sides.
And a lot of people who weren't Scott Alexander throwing around a lot of, you know, the journalists
are all evil and we're all doing hatchet jobs. It became this sort of mutually antagonistic
thing. And then it was always going to be a somewhat antagonistic, antagonistic piece after
that. I mean, I'm interested in this tension between the, and we're talking about the links
to the tech companies here as well. And this also feeds into the, the growing tension between
the tech companies and especially the New York Times, but tech journalists more generally,
where, and I think at the root of this is that journalists believe that they should be the
gatekeepers about what people should, what ideas should be part of the conversation and what
shouldn't be. And that was kind of the subtext of that piece was, oh, he's talked to these people,
he's engaged in these ideas. And, and that's very, very anti both the rationalists and also the sort
of the, the dominant sort of strain of kind of maybe ideological libertarianism in Silicon Valley
as well. And I feel like that way of that sort of gatekeeping role of the media that is increasing
being lost is sort of, for me, that seems like a defunct strategy because we're never going to
get the genie back in the box. The nature of kind of alternative media or decentralized media means
we now have a much wider marketplace of ideas. And the whole, the whole way of saying you mustn't
talk to this person, you mustn't think about this, or you mustn't engage with this, it just
isn't going to work anymore. So I think, I think in a broader strategy, we have to engage with
bad ideas, or we have to engage with kind of ideas that are beyond the mainstream, if only to say
why they're not, why they're disreputable, or why they won't work, or why they, they're, they're
wrong. Whereas the, the, the media, I think still has this idea that we can kind of circumscribe the,
the bounds of thought and the journalists are the right ones to do that.
As you say, there is, there are lots of other ways the information is put around.
And I think there is an element of, in the media of thinking that, well, this is somehow bad. This
is, you know, we, we were before filtering the, the acceptable versions of these, of this information
to you. And now any, any idiot with a WordPress account or a Twitter account.
But I think, I don't know, in so far as journalists do believe that, they overestimate
their own ability to filter things well, because like we are no cleverer or better on the whole than
any reasonably smart group people, you know, and I think, and I think that is, we're, we're
misjudging our own abilities if we think that we, we are the only safe route from which,
through which these ideas can be filtered. And I think that actually
things like the rationalist approach are, I think it's really useful when, when, when they go and
sort of look, look, investigate these things in, in, in depth and in a way that mainstream
journalism isn't set up to do. You couldn't have a 30,000 word New York Times piece about the ideas
of the reactionaries. It just, it wouldn't, it wouldn't fit like that. You couldn't make that
go anywhere in, and, and the, the idea, and, and there's also, I mean, we have, in journalism,
we have sort of traditions which are hard to get out of, but are also not always easy to justify
when you're coming to refresh. Like you can never write, I'm not sure about this very easily in a,
in a news article or a, or certainly a comment piece, you have to sort of present this godlike
status of omniscience. Like I, I come to you with my tablets from the mount and,
and I, one of the great things about the rationalist community is they will write blog post saying,
I'm, I'm only 60% confidence in this. I'm not sure if I've understood it correctly. You know,
and, and there's much more, and I, and they, they, they go and take it from a totally different
point of view that is not bred by three, you know, two centuries of journalistic tradition,
and it's just some people making it afresh. And then they make, I'm sure they make a lot of the
same mistakes that we would make without, you know, it looks a lot of the reasons these true
journalistic traditions have sprung up for a reason, but they also bring in new fresh ways
of doing it, which I think are really useful. And I, so yes, this is a very long way of saying,
yes, I absolutely agree. There is a gatekeeper aspect to the media. Partly that is a, a, a matter
of just how the media was before. There wasn't any other way for information to get around.
But there is a tendency to think that it is not that it is more than just an accident of history
and that we are somehow meant to be doing it. And we are the best ones to do it. And I'm not
sure that's true at all. The piece, as we said, was called Silicon Valley safe space and kind of
gave the idea that it was, if not an ideological monoculture, then at least specifically hostile
to social justice ideas. Do you think that was a fair summary of the rationalist perspective or
that blog in particular? No, no, I definitely don't. I think, I think if you do,
because they're nerds, right, they do surveys. The less wrong blog and Slate Star Codex did
annual surveys of their readers for years. I don't, I couldn't really, I couldn't work out,
the find out 2020 data very easily. I had to look at it recently, but 2019 data had
liberals and self-described social Democrats and liberals, you know, the
so left, what Americans were called liberals and British were called left, I think, you know,
Democrats outnumbered conservatives, self-described by eight to one. There was about half as many
libertarians as there were liberals. So they were a non-trivial number, but they were still
a small minority. The rule is you can discuss almost anything. They'll ban you for certain
topics. But in Slate Star Codex thing, you can, as long as you are polite and not trying to start
a fight, then you are broadly allowed to talk about anything. And that leads to a very rare
situation where you get people who disagree with each other and sort of talking in the same space.
And then if you are someone who's just used to never seeing anyone saying, well, I think
feminism is a, you know, online feminism has gone wrong or is a bad thing or whatever, you know,
then even seeing one will leap out at you. And that will be like, that's what I remember from
the 500 comments, the three that said that, you know, that's got that on it. And then you think,
well, this, you know, just by the workings of the availability heuristic, you know, just the
workings of your mind, that'll be what you remember. And that'll be what you think of as the whole,
there's the whole sort of tenor of the debate. And there's the other side as well, which is that
if you are not allowed to say certain, discuss certain topics in a lot of the areas, but you are
allowed to, as long as you're polite in this area, then you will, then people who aren't allowed to
discuss ideas elsewhere will tend to come there. Yeah. And I think I share a little bit of like,
I'm not a free speech absolutist. I see a lot of that, especially on YouTube.
And I think that the tech companies do need to take more responsibility, for example, with
some more, more sort of journalistic and curatorial moderation responsibility. I do share some of the
concerns that are being expressed by the sort of more journalistic side. But I also think that
it's a losing battle if you're trying to circumstribe certain topics as off the table,
because that horse has bolted. And I think we need to kind of act, knowing that that horse has bolted.
There was a really interesting tweet that I saw, where someone talked about the off ramp.
So there's a lot of talk about the on ramp to bad ideas. And I think this is sort of,
this was the tone of that piece was the rationalists are an on ramp to bad ideas because they engage
with these other communities that have more reprehensible ideas. But this point, the point
that was made is we don't talk a lot about the off ramp. And yet the off ramp has to exist as
well. And I remember there were even some studies that were done into the effect of, I think this
was specifically Jordan Peterson's ideas. They tracked kind of comments and basically concluded
that people had gone from watching Jordan Peterson's content to watching more extreme content.
Then if you look to the methodology, and that may be true, that may not be true. We're not sure.
But they didn't check the other way. So they didn't do, so it basically was only looking at one,
at one side of that, of that equation. And a lot of people who follow Jordan Peterson's content
would say that he has de radicalized people as well. And there's no, but the study only looked
at the, at the on ramp effectively. And this is really interesting, I think, because this is what
you're pointing to in your piece is you have to engage with these ideas, you have to kind of
potentially persuade people away from, from them. Otherwise, what do you make of that?
Kind of like the off ramp as an idea, just I think it's really important discourse.
I think, yeah, we're talking about what we're talking about persuasionally,
where the idea of the, the, the one of the things I love about Slate Star Codex is writing is that
it will try and approach people who disagree with the point it's making and say, look, I, you know,
it's not just blue out group. It's not saying, look, how, you know, aren't you smart for already
agreeing with me? There's, look, here is something which you probably don't agree with. I will try
to change your mind. So this ties back to what you're saying about the extremism thing, or at
least the, you know, the people see it as a, people see it, and it's always the left that
sees this as this sort of gateway drug slippery slope. Everyone will, will read an article saying,
perhaps, um, Jezebel behaved badly at some point. And therefore they'll slide from there into
men's rights activism, whatever, you know, I'm being flippant, I shouldn't be, but you know,
that sort of thing. It's always going to be the slide from left to right into, into right,
rightist extremism. But the point of Slate Star Codex was it is trying to persuade everyone to
agree with Slate Star Codex, which is obviously, you know, as we all are, we're all trying, you
know, DM, we all want people to agree with. And so it was, it was reaching out to people on the
rights and the libertarians saying, you know, actually, you know, libertarian, you know, you
can be too libertarian, you get these, there's, you know, there are flaws with these, you Trump
voting, Trump reaching out to Trump voters reaching out. And it is, it is pulling people towards this,
towards the positions that Slate Star Codex holds, which are not extremist right wing. And
therefore the, it ends up, it ends up neither sort of, yes, it moves people who are left of Slate
Star Codex right, if that is a meaningful thing, but also move people right of Slate Star Codex
left. And it brings you, so yes, it has this anti extremism effect, or certainly
it seems likely to me, and Scott Alexander could point to those people, the comments on his
anti Trump blog post about it. So yes, I think I think the, I think the off ramp is a real and
I think important phenomenon. And I would love someone to do some sort of quantification, you
know, of it, because I, this is just me sharing anecdotes. But yes, I think it is a real thing.
And just before we close, is there anything more that you wanted to say? Is there anything more
that people should know about the rationalists that you think they probably don't?
I think the, their, their ability, their predictions of COVID, their sort of
tendency to be out ahead of the game has been really noteworthy, as this is the piece that
Cade Mets, I wish he had written. And I think that is very much to do with this tendency to not
go with a herd, you know, back in January last year, to January 2020, the media were all writing
the same pieces of, you know, don't worry about COVID, worry about the flu. Don't, you know, this
is the, you know, this is not, and saying, you know, if everyone worrying about this, the over
panicking about COVID as being sort of anti Chinese racist and everything like that. And I think
this was because the media, and I'm as guilty as this event as anyone, right, of, you don't want
to look like the idiot who panics. You don't want to look like the guy who goes, ah, it's all
terrible. And then nothing happens. You're safe to do, to stay with your herd and not, not look
like you're straying outside of it, even if an exponential, even if it's a simple,
simply following the extra dots on the exponential curve, but I told you that actually this is going
to be around the world in six weeks, you know, that, that would have been,
that would have been, should not have been beyond the wit of someone who can do GCST maths, right?
Then, and I think the, the ability to follow these arguments, these, you know, in that case,
barely an argument, but the maths, but it does involve also sort of understanding of sort of
some of the theory as well beyond to logical conclusions and not be swayed away from them by
group thinkers, a dismissive way of putting it, but, you know, the sort of, the social pressures
to say, to not look like you're panicking, to not look like you're scaremongering. I felt that,
that is a really interesting topic. And it is, it is the same mechanism that you see
with their fears about AI, I think. And this is what I wrote in the end of my book is about,
but they, when you follow their arguments about AI, it is very hard to see where they are wrong,
right? It's very hard to go, okay, look, what you, yeah, the AI does go wrong in these ways.
It is rapidly getting more powerful and put, and the AI researchers, if you interview them,
do think you'll probably have superhuman AI in the next 80 years and, you know,
my children's lifetime and that it will probably, and there is a decent chance of
really, really terrible outcomes, while also a decent chance of very, very positive outcomes, you
know. But there's just as with, just as with COVID, there's this really, I found it, I noticed it,
I wrote about it myself, this is just weird, right? This is weird. This is not,
this is not the sort of thing that I think is real. This is not the sort of thing that
happens in the world, but they all, they've done is doing the same thing, pretty much, extend a
trend line out on a graph and take some basic theory points and some fairly unconvincing and
say, well, if all these things are true, which you agree with, then you extend it out and you
end up with a disastrous outcome in 50, 80, 100 years time, and we need to do something about that.
And I think that, and they also, what it's worth, have been banging on for years about a
bioengineered pandemic being that and, well, being the real other great risks of human extinction,
that and AI, and then also generally the risk of global pandemics. This is stuff they have been
talking about. And so I think it is a, the COVID stuff is a really good reminder that actually
this sort of viewpoint that is not as we, as I, as the media, as humanity has a tendency to do a
sort of steering away from ideas that seem uncomfortable because they're weird or because
they're not, you know, they're not what sort of thing we talk about normally, and don't want to
look scared, don't want to look panicky, don't want to look like we're socially, you know, outside the
herd. Sometimes it's really useful having people around who don't think like that and who do just
take the trend line and extend it out and say, well, no, but hang on, this means that we're all
going to be locked up in our houses in three weeks time. Maybe we should be thinking about that.
And I think that should be a bit of a reminder to at least not dismiss their fears about stuff
like bioengineered pandemics or AI and the human extinction stuff. Because I think it,
the same mechanisms are in play in both places. That would be my, that would be what I would say.
Since the beginning of Rebel Wisdom, we've been thinking about how to create spaces for people to
have new kinds of conversations around big ideas, which is why we've just launched our digital
campfire, which is a central place for people to gather to find the others and to make sense together.
It's a place to practice the skills we talk about on the channel. So we have regular sessions that
help us improve our sense making and also tap into our collective intelligence. And it's all
hosted on a discussion platform called Circle, where you can have conversations around our films
and articles or on any other topic you're interested in. We've designed it all to be
participatory. So you can set up real time conversations by creating a crew to dive deeper
into different topics or practices. So we've got three different levels of membership.
Wise Rebel, Explorer and Sense Maker. All three levels have access to the digital campfire on Circle.
And the explorers also have access to the following official Rebel Wisdom run sessions.
So on Mondays, we have live sense making, which is a session where we come together
to discuss a hot cultural topic. And then on Tuesdays, we have our Academy sessions,
where we have some of the best facilitators in the world teaching various skills.
So for example, collective intelligence practice, facilitation training.
Then on Thursday, we have our connection gym. And the sense makers are also invited to our
Wednesday sessions, which alternates between Q&A and the Wisdom Gym. The Q&A is with one of the
stars of our films and will often go up on the channel itself. And the Wisdom Gym is where we
bring in some of the biggest names in transformation and growth to share their practices with us.
Within Circle, we've also included a number of resources that we found useful. So sense making
tools, meditations, authentic relating games and guides for how to host your own session.
So the most important thing to remember is that this is an experimental space
and is designed to be participatory. So it's really your space to come in and make your own.
So we'd love to see you there.
