A modern jet plane, the epitome of speed and power.
Some people think today's sophisticated computers are also the epitome of speed and power, but
today's personal computer may be more like a DC3 compared to a new kind of machine called
a reduced instruction set computer.
What is that?
You'll find out today on this edition of the Computer Chronicles.
Computer Chronicles is made possible by Leading Edge, makers of IBM-compatible computer systems,
including Lotus Look-alike spreadsheet, word processing with spelling correction, communication
software, and Hayes-compatible 1200-baud modem.
Leading Edge with over 1,000 service centers nationwide.
Additional funding is provided by McGraw-Hill, publishers of Byte.
Byte's detailed technical articles on new hardware, software, and languages cover developments
in computer technology worldwide.
Welcome to the Computer Chronicles, I'm Stuart Schiffet and this is Gary Kildall.
Gary and I are playing this game of RISC here, R-I-S-K, and as you know Gary, there's a very
risky game out there called RISC, standing for Reduced Instruction Set Computing, some
major players in that game, IBM and Eulet Packard for example.
Some people are saying RISC technology is the wave of the future, other people are saying
RISC is already a passe technology.
What does this expert think?
Well Stuart, the current IBM PC line, the PC, the XT, the AT, and so forth, they're
based upon a processor called the 8086, the 8086 was really the next generation following
the 8-bit processors, the 400 was called the 8080.
And so we've got right now, we try to expand into a larger memory space, go from the 640
K restriction to let's say 8 megabytes and things like that, it's really very, very difficult
to do that, it's kind of a messy architecture.
So RISC gives this as a new starting point, we can say okay here's a simpler instruction
set, faster, less code space, it makes it easier for a compiler writer to write compilers,
and for a software writer we can stay with high level languages and really don't care.
Well we're going to take a look at IBM's RISC machine today, the RTPC, we'll meet the
man who coined the term RISC, and we'll talk to some other RISC experts both pro and con.
As I mentioned Eulet Packard is one of the major players in RISC right now, and we're
going to start out by going out to HP's RISC lab.
In the search for faster and more powerful computers, improved hardware is often taken
center stage while changes in architecture trailed behind.
The new architectural design used in RISC, or reduced instruction set computers, uses
simpler instructions requiring fewer machine cycles.
In the early 1970's, IBM examined the kinds of operations most commonly carried out by
a complex instruction set machine, and found that only about 20% of the simplest instructions
represented about 80% of the processing time.
RISC architecture relies on these simple computational elements to execute most of its operations
in a single cycle.
While RISC has made few inroads in the market, at least one major company, Eulet Packard,
demonstrated its confidence in RISC by introducing a whole line of RISC-based machines as the
next generation of its high-end computers.
All computations except load and store take place in the data path.
Inline subroutines can be called up for complex functions, and assist processors operate when
required for floating point arithmetic and graphics.
The special nature of RISC design makes it well suited to engineering and scientific
applications, where instructions of four bytes or less can be executed in one cycle.
But while such applications demonstrate the advantages of RISC, some critics contend that
the speed gained by reduced instructions could be wiped out by the longer data paths, and
by the need to accommodate complex functions.
Joining us now in the studio is Dr. Joel Birrenbaum, Vice President and General Manager for the
Information Technology Group at Eulet Packard, and next to Joel is Dr. David Patterson, Professor
of Computer Science at UC Berkeley.
David, just for background, where did RISC come from?
What's the origin and what really makes it different from traditional kind of machine
architecture?
Well, the idea is going towards simplicity, going back to kind of a small, beautiful philosophy.
There's lots of origins to it.
One of the main ones was done at IBM during the middle 70s in some research groups out
here in California.
The philosophy is by trying to make simpler computers to make them faster and more cost-effective.
Now, if we compare, let's say, a traditional, what we would call a traditional machine,
like a 286 IBM PCAT, how does a RISC machine compare in terms of speed?
The belief is by those researchers in the field is that using the same resources, you
can get something like a factor of two to four improvement in cost performance compared
to the traditional design.
So given the same transistors as Intel uses on the 286, finally the AT, we should be able
to build another machine twice as fast or four times as fast.
Joel, I had heard a number which makes it perhaps easy for some people to understand
about this 80% and 20% business, which somebody discovered maybe it was you back at IBM in
the 70s or so.
Could you explain that?
Well, I think the notion is that computers spend most of their time, 80% of their time
is a symbolic number, doing a few simple things and don't do the complex things very often.
And so the notion is that if you optimize the machine to do the simplest things as well
as possible and to do the complex things as infrequently as possible, then you might
come out with a machine which for certain types of jobs would be more effective than
those which pay the penalty of complexity across all of the things that it does each
of the times that it does them.
Joel, if you talk about RISC, is that a philosophy, a way you design a machine architecture,
or is it a specific set of instructions?
It certainly is a philosophy or a style, and in fact it's a style which differs depending
on what you're trying to do.
So the main idea is not to add any complexity to the machine unless it pays for itself by
how frequently you would use it.
And so for example, a machine which was being used in a very heavily scientific way where
floating point instructions were important might make a different set of trade-offs than
another machine where that wasn't important.
Only one in which compatibility with other machines was important or in which certain
types of networking was important would include different features, but in each case they
ought to be done as the result of measurements of relative frequency of use and the penalty
that you would pay for the inclusion or non-inclusion of a particular feature.
So where do you see RISC architecture being important to say in computing right now?
We're going to see it moving to the scientific areas, is there going to be a replacement
for the current desktop computers for RISC?
I don't think you're going to like my answer because I don't think there is such a thing
as RISC architecture.
I think what there is is good engineering design of a machine called a computer.
And in my view, all such machines ought to be designed by analyzing exactly what they
do and what the objectives of the person building the machine are.
And then building that analysis into the specific design.
It's turned out that a smaller number of instructions, those which are most frequently
used, can in fact address a very wide range of applications and types of applications,
sizes and types.
And so that's come to be called the reduced instruction set machines because in fact they
do wind up being a smaller number of hardwired instructions.
But to make a successful product includes many, many other things.
At HP for example, we spent probably two thirds of our time on portions of our new computer
family that have nothing to do with the instruction set itself.
If we introduce a new machine into society, there's a problem obviously of trying to get
applications onto that machine.
What's the strategy in terms of, let's say, healed packages?
How are you going to get people to move into a new architecture like that?
Well in our case, we're trying to get our customers and if we're lucky maybe a few other
people's customers to move on to the new machines from at least three previous families, each
with their own instruction sets, compiler families and so forth.
So we've had to provide smooth migration paths for previous customers.
When the people have programmed in a high-level language, we can often accommodate that simply
by recompiling, which gets us most of the features but not all of the native mode architecture.
When people don't have that capability, we run in a compatibility mode and we have a
wide range of tools and accelerators, sometimes in hardware and sometimes in software, which
essentially take the old instructions and transliterate them or translate them into
the new ones and that can be done in a variety of ways.
David, you helped create this notion of risk.
How do you think the evolution has come?
I mean, you hear Joel saying, well, it's not really such a simple thing as just a risk
machine.
Well, I think it's been interesting is how viable the philosophy has been in many different
fields.
The original work that was done ignored what's called symbolic computing for artificial intelligence
and the folding point computations.
The experiments that several groups have done since then have shown that risk philosophy
of an experimental-based design works extremely well for symbolic processing, for languages
like common list and for the floating point computations, number crunching applications.
So I think it's a surprisingly viable approach.
Okay, gentlemen, thank you.
Now we've been talking about a risk machine and just a minute we'll take a look at one,
the IBM RTPC in just a minute to stay with us.
Joining us now in the studio is Dr. W. Frank King, General Manager for Advanced Engineering
Systems at IBM, and next to Frank is you, Martin, Vice President of Development at Ridge
Computers.
Gary?
The risk concept is really apparently the central processing unit instruction set philosophy.
Now what about related concepts like parallelism, IO concepts, fast IO processing, have you
worked on those things at all?
Sure.
At Ridge what we've done is we've concentrated first on the central processor but the entire
machine has to be designed to be efficient and so we spend a lot of time initially on
the IO processor section to make sure that the performance of the IO matches the central
processing unit and now that we've got this simplified processor we're examining parallel
architectures which can take advantage in the same way of this simplified architecture.
Now VAX computers, the 11, 780s and 750s and so forth, are used a lot because they're
good IO processors.
Do you think that the risk architecture with, say, fast IO is going to be a competitor
for VAX?
Well, I think that whether or not you have a risk computer you need a fast IO system.
When we designed our first machine we made sure that we had an extremely fast IO system
to go along with this fast computer and so as we move into marketplaces where there are
lots of users and disks connected to the machine we find that we do just as well as
a VAX.
Okay, good.
Frank, this is a real example of a risk machine.
It's the RT.
This apparently, you can give a pretty good comparison on the speed of this because isn't
there a co-processor that runs in this machine that will run the standard 286 applications?
There is, Gary.
This machine runs 1.6 to 2.1 million instructions per second and it has in it a co-processor
board with a 286 on it that runs about a third of that speed.
Frank, what market is this machine aimed at?
You've got this out on the market now for some period of months now.
Who's buying the risk machine?
We packaged a tabletop version of the machine primarily for those people that need speed
and capacity above what the IBM personal computer and the personal computer type systems can
support.
So it's really for people who need more speed and more capacity.
Well, who are they?
Engineers is an easy example.
Architects doing building design.
People in the insurance industry that are examining actuarial tables, those kinds of
people.
So you see business applications in addition to science, engineering, operations, or
rooms.
Yes, high-function business applications.
Now, Frank, just for a reference point, what's the cost of this machine?
This machine, as it sits here on the table, is about $10,000.
And that comes with how much memory?
This has up to four megabytes of memory in it and a big 70 megabyte file in it.
Okay, Frank, you mentioned design and Gary, I'm going to put you on the spot now.
I want to see how easy it is to use this RTPC here.
You're going to be our operator and you're going to help show off Frank's machine.
What can you show us on this, Gary?
Well, I'm an expert now.
I've had a total of 10 or 15 seconds of instruction.
I'm going to go in here and get a technical illustration.
So I select that and click on it and it opens the window up.
And here's a document of some sort with a vector drawing, I believe.
And I'm going to do is go in and select this.
If I point at this little segment properly, I've got to get a hold of it.
There it goes. It starts flashing.
And now I can get a little menu down here and I'm going to take this thing and rotate it.
And as I move the mouse around, you can see that the object itself is moving.
And then I can just leave it in that position.
Okay, in that real-time rotation, I take it as an example of something that's requiring
a lot of fast processing to handle that.
That's right.
Or I can pick up a piece right here, for example, get a hold of it.
And my hand-eye coordination isn't that good.
Okay. Well, there it goes again.
Okay, now what I'm going to do at this point is I'm going to take the thing and move it around.
We're going to take it and slide it down to this portion of the screen.
So you can see the action there is pretty nice.
And then just leave it.
Frank, one of the concerns I've heard about the RISC machines is the problem of adapting software and so on.
Where do you stand on that?
Do you have to write new software just for this?
Not really. Most of the software that runs on this machine
is written in a high-level language.
And so the compilers have to be there to support it.
And you do have to have compilers that know how to work with RISC architecture.
But the compiler really masks moving the current software to this kind of machine.
This is basically a Unix operating environment?
Yes, this is basically a Unix operating environment in support C and Fortran
and those kinds of standard languages.
What's your strategy for moving applications over?
We talked to Hield Packard about that. What's your...
Well, first of all, we define a set of compilers and those are the normal compilers.
And then the price performance of the machine has to justify people wanting to spend the money to move their software to it.
You have a small company, Rich Computer, isn't it?
I think you've sold four or five hundred of these RISC machines.
Who are you selling them to and how are you carving out a niche among the HPs and the IBMs?
Well, our niche is primarily price performance in a technical marketplace.
A primary example of a customer you may be familiar with, if you've seen the Super Bowl or the Olympics,
all the animation that precedes those programs were done by a local company, Pacific Data Images.
They have 12 ridges doing nothing but cranking frame after frame.
So, once again, the heavy duty graphics requiring the speed and the power that RISC can give them.
Frank, where do you see going next?
We hear rumors of an enhanced RTPC coming out and some changes in it. What's... is that true?
Well, certainly IBM never talks about its futures, so I have to start out by saying we're not going to talk about our futures.
But what we see going on in this industry, the desktop micro-computer industry,
is really a doubling of speed and capacity about every 12 to 14 months.
So, clearly, if we're going to participate in this marketplace and we intend to,
that kind of improvement over time has to be sustained.
Now, we're seeing some pretty substantial improvements in speed on the 2A6 and 3A6 side.
Isn't that pretty competitive with the RISC architecture? Let's say a 24 MHz 386.
Isn't that pretty close to the speed of this machine?
The 3A6s that are available today, of course, are 16 MHz.
But as the Intel architecture becomes faster, in order for this kind of a machine to justify the price and the price performance,
it's going to have to stay faster, and that is our intention.
Gentlemen, thank you very much. Now, we've seen a RISC computer. We've been talking about applications.
Let's find out about how one customer is actually using a RISC machine.
Wendy Woods went to ESL in Sunnyvale, California to find out.
ESL in Sunnyvale, California is designing the world's fastest systolic processor,
a chip that could be used in such diverse technology as sophisticated listening devices,
or in image processing of pictures from outer space.
The task requires tremendous computing power, for which engineers depend on a RISC-based, pyramid technology's mini-computer.
It enables us to have a machine that runs much faster, and that's very important to us because our programs take so long.
We also have a number of users on these machines, so it's very efficient in handling that.
And the particular kind of instruction mix that we get from our integrated circuit design,
which is fixed point instructions or integer instructions as opposed to floating point numbers that have exponents,
those run particularly fast on this RISC architecture.
The integrated circuit that ESL is designing has 62,000 transistors,
and the RISC computer is employed to perform a thorough simulation and verification of that design
before it is committed to silicon to make sure that it actually works.
You'd think that after two years, people here would be sold on RISC technology, but not so.
Parallel processing, they say, offers similar advantages.
But the bottom line is that these kinds of projects never have enough processing power,
and engineers are not ready to marry any one technology in their attempts to get a job done.
For The Computer Chronicles, I'm Wendy Woods.
With us now in the studio is Jan Lewis, President of the Palo Alto Research Group,
and next to Jan is George Morrow, whom we all know is our commentator here on Computer Chronicles,
and George is also currently Vice President for Engineering at Intelligent Access.
Gary.
Jan, the computer community has been pretty reluctant to accept new architectures in the past.
Is there going to be a problem with the RISC architecture?
I think this will be a problem for a couple of reasons.
First is the concept of accepting a newer architecture.
But the other thing is that there's a lot about RISC that I'm very skeptical about.
And in fact, the commercial applications have yet to actually prove that RISC is going to give us true performance increases.
The machines that are out there, both the HP that's shipping in small quantities and the RT,
really are watered down versions of RISC.
They're not true RISC machines, although they do use some of the RISC principles.
And some of the performance improvements that we see may in fact not be because of RISC,
but because of other factors such as on chip registers being a lot more in quantity.
George, what do you think about RISC technology?
Well, looking at it from a point of view of silicon, the richer register sets and the simplified instruction sets favor the use of silicon.
So that's a positive thing.
On the negative side, we just seem to keep making new instruction sets.
And that's like inventing a new typeface every time you want to say something again.
So it's a real balancing act.
What I'd really like to see happen is for some of these guys to figure out how to make a computer that would run anybody's binary
without recompiling or translations of any of this stuff.
That's what I'd really like.
Jen, you said the machines on the market are watered down and they don't really reflect the theory.
What's the problem? What's the technology problem that prohibits that?
Well, there's a few things. First of all, the concept is that you have very few number of instructions.
I say 50 or less. George says eight or less.
But you have a few number of instructions, each one using basically very few clock cycles.
One, theoretically, and that you get maximum performance as a result.
But in fact, the machines that are out there do need extensions.
They do need other things.
And in fact, the RT, for instance, actually averages about one instruction per 2.7 cycles as opposed to the theoretical one.
So they're not purely risk.
We've sort of evolved from risk in the pure sense to risk like architectures to architectures using risk principles
to simply simplified instruction sets on Chef registers.
What are the first applications going to be for risk architecture?
Where are we going to see it? Is it going to be in the scientific CAD-CAM community?
I think it will be largely in the scientific community.
For one thing, it doesn't require quite the same amount of I.O., which still is a problem.
And the other thing is, in terms of accepting new architectures,
I think if there's one community that is less standards conscious,
it probably is the engineering and scientific community.
George and Jen, you're both saying there are quite a few problems here with risk.
We've heard the phrase that Yulet Packard is gambling its whole computer division on risk technology.
I mean, is that a real danger?
Well, I don't think that's quite fair because I think part of what HP has done
is an investment to transport customers from their standard environments
over into something that's perhaps a little bit more flexible.
So I don't think they're gambling the whole company on it.
What Jen says is right.
But on the positive side, compiler writers, with lots and lots of registers,
compiler writers are going to be able to write better compilers.
And maybe we'll finally get to the point where we've truly got portable, high-level languages
with this kind of background.
That's probably the biggest hope.
Of course, the scientific community will jump on them quicker.
It does rotate images faster, that sort of thing.
What about the competition from the new processors from Motorola, 68020, the new 386?
They're talking about 24 megahertz now.
Is that going to knock them out?
I was on a forum called the 32-bit shootout where Motorola introduced the 68030.
And those are very, very high-performance machines.
So it's possible the traditional approach will continue to get better.
But eventually, we're going to get down to the point where silicon is important.
And their risk machines win on silicon better than the complex instruction set.
George, Jen, Gary, we're out of time.
Thank you very much.
And we'll be back in just a minute with this week's Computer News.
In the Random Access file this week,
both IBM and Ulet Packard have just made announcements affecting their risk machines.
HP says software problems will cause a delay in the planned shipment of its Spectrum mini-computer.
Delivery is now expected in mid-1987.
And IBM has announced upgrades and improvements in its desktop-risk computer, the RTPC.
The new RT Model 15 will feature improved networking capability, memory, and storage.
IBM also said it's cutting the price on the RTPC.
IBM is also beefing up its sales force,
announcing the transfer of more than 2,000 employees into sales
with the goal of increasing its overall sales force by 5,000 people.
Apple has announced a deal with Northern Telecom that will enable the Macintosh to network over regular phone lines.
The new hardware and software can speed up Mac communications by a factor of 18.
Lotus is showing off its new HAL program for 123 users at this week's Info86 show in New York.
HAL is an AI-based add-on that simplifies interaction with Lotus 123.
And Lotus has also announced its first CD-ROM product called One Source.
It's a compact disk containing 20 years' worth of financial data.
The One Source CD-ROM can communicate with 123.
At the annual Software Publishers Association Convention in Washington,
a spokesman for Education Systems Technology said computers will reverse the trend of smaller classroom sizes.
The report said computers will enable teachers to handle classes of 90 students
with more individual attention than is now possible in smaller classes.
Time for this week's Software Review, and here's Paul Schindler.
You know, computer software writers are constantly on the lookout for ways to improve manual systems.
Now, if you, like me, use this kind of system for keeping track of things on your desk,
I think you'll like Tornado Notes.
Tornado Notes is a memory-resonant, multi-purpose note-taker and finder.
It's like a combination of yellow stick-on slips, paper, and a database management system.
Tornado Notes takes this common idea a step further by creating a more desk-like environment.
Not only can it search your whole pile of notes for a specific reference,
it can arrange those notes as you'd arrange a pile of paper with some notes easy to access on top
and some notes buried on the bottom.
Tornado Notes can insert dates and times, important export files,
and offers a reasonable word processor plus online help.
Unlike some older memory-resonant programs, Tornado Notes seems to coexist well.
Perhaps a sign that memory-resonant program writers have finally signed some peace treaties.
Tornado Notes is $50 from MicroLogic in Hackensack, New Jersey.
For the Computer Chronicles, I'm Paul Schindler.
A company named Gamalink has come up with a new product that lets you send copy from a PC directly to a fax machine.
The add-on board costs about $1,000.
By the way, Federal Express announced this week that it is eliminating its own fax service, ZAP mail, due to heavy losses.
An organization called the Overseas Security Advisory Council is starting up a bulletin board on international terrorism.
The council says it hopes the BBS will become a source of important information for American businessmen traveling abroad.
A blind businessman in Indiana has developed a talking spreadsheet program for blind users.
It's called RAPSheet and it offers six rows by 22 columns and most spreadsheet functions.
The developer says he'll give it away for free to other sightless people.
Finally, researchers at Carnegie Mellon University in Pittsburgh say they are making progress on synthesizing the protein molecules
that may one day be the CPUs of molecular computers.
One CMU researcher says molecular computers will make possible the replacement of defective human neurons in the brain
and will allow for tiny tumor patrol computers in the body that would spot abnormal cell growth and automatically repair the cells.
That's it for this week's Chronicles. We'll see you next time.
Additional funding is provided by McGraw Hill, publishers of Byte.
Byte's detailed technical articles on new hardware, software and languages cover developments in computer technology worldwide.
Thank you for watching.
