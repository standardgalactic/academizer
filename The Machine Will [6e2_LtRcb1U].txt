So, this talk is based on our book.
This is the way the book will look in August when, as they have planned, it will appear.
The subtitle is Artificial Intelligence Without Fear.
Now, this book was written in part in order to attack the idea that AI systems would become
ever more intelligent and that they will one day become more intelligent than human beings
and then begin to replicate themselves and become progressively more and more intelligent
until they take over the universe.
AI systems will never have goals of their own, so they will never have a goal of taking
over the whole of civilization.
So this doesn't mean that there won't be evil people with powerful weapons, but there will
never be evil AI.
That's a reformulated version of the thesis of the book.
So AI works for various kinds of devices.
It works in games.
It works in highly structured areas, and we'll talk about some of those highly structured
areas later, or it can work also in areas that we can highly structure, and I'll explain
what that means also later.
Where it doesn't work is in predicting the behaviors of complex systems like the oil
market, or the marriage market, or weather, or climate change, or the brain.
The two main chapters in the book are a very carefully formulated mathematical argument
why that is the case.
Now, I'm going to talk today about a topic which is also discussed in the book, but is
not a central topic of the book, which is will or drivenness, and so there are two kinds
of systems.
There are those systems which are driven, which means that they don't move towards an equilibrium
state, but rather they move from state to state on the basis of their own energy.
We all have plans for tomorrow.
We're driven.
And because we're humans, we have excess drive, and the fact that we're in this room
means that we have a rather high degree of excess drive, as contrast to the people who
already left, who have a slightly lower degree.
Drivenness is based on energy.
We take energy from the environment, and we store it in molecules in all our cells, and
then we use that stored energy in order to give lectures to partially hostile audiences.
I hope only partially.
So the force may be continuous, as in the case of the sun's heat.
We take energy from the sun, or as in the case of your laptop when it's switched on.
So being driven in the case of a laptop just means being switched on.
As long as we are alive, we are driven.
Now as I said, there are some natural systems which are driven, like the weather and the
climate system, which are driven primarily by taking heat from the sun, and then there
is the artificial drivenness of devices when they are switched on, as in the case of your
laptop.
The device drivenness always depends on human drivenness, so we can just switch off the laptop.
The laptop is then no longer driven, but the laptop will in any case wear out over time.
There will come a time when the laptop, if it's not maintained by human beings, will
cease to be functional.
So there are two ways in which artificial drivenness is dependent upon human drivenness.
The first way is that we provide the fuel, and the second way is that we provide the
maintenance.
Then they will look like this.
So these are the types of drivenness.
We have animate drivenness.
The weather is an example of inanimate, but still natural drivenness, and then everything
else is artificial drivenness.
And it's always a matter of energy, so we move from state to state.
The New York Stock Exchange is an example of mixed animate artificial drivenness.
So there are all those machines, all those servers warring away day and night.
There are all the people who are programming those servers, who are typing messages into
those servers, and the stock exchange is changing its state all the time.
And that's why AI will never make you rich on the stock exchange.
You can't predict what the New York Stock Exchange will do because it's a complex system
which is driven.
All right, now Arnold Galen, a German philosopher and anthropologist, developed the idea of
human excess drive.
So animals are driven, but their drivenness is always along certain fixed channels, which
are a product of evolution.
So they can't change their environment.
They can't create better environments.
They're guided by their instincts so that they can survive in the environment that they've
been selected for.
They have a limited repertoire of actions.
Humans, because they needed to survive in harsher environments, have to some degree
abandoned the realm of instincts, and instead they have curiosity and willingness to try
out new kinds of environments.
And so they create new sorts of environments.
This is an igloo hotel, and they create new sorts of cultural niches to which they can
go in specific times, like this, for instance.
So rabbits don't do anything like this, and we create new sorts of scientific niches.
We create even new sorts of abstract niches.
So the sciences, mathematics are new kinds of environments in which we can collaborate
and plan and make decisions and so forth.
Now there is a new book being published next week, I believe.
This seems to be the most up-to-date attempt to grasp the idea of human curiosity, human
knowledge seeking that drives for knowledge.
So I was attracted, as you can imagine by the title, but also studying these phenomena
from the point of view of how an AI system could emulate human drivenness.
So this is very relevant to my thesis.
My thesis is that machines have no will, and so they have no curiosity, they have no autonomy.
In chapter one, they discuss various ways in which AI scientists have claimed to have
emulated the will, autonomy, curiosity in the machine.
This is a typical sentence, research and machine learning has shown that intrinsically generating
sense making rewards, rewards is a very important word in the book also, help to foster robust
learning by encouraging structured exploration.
The question is how do rewards work in machine learning?
And if we look at AlphaGo, we have a very simple example of how rewards work.
AlphaGo was built by training a neural net on the basis of the rewards gained by the
computer playing the game of Go billions of times.
And here we have a metric, the rules of Go allow you to assign a score to any given move.
So these are rewards which can be defined computationally.
And it may sound trivial, but it's actually very important.
Reinforcement learning, which is what we're talking about here, like any other kind of
machine learning, only works when you can compute the rewards.
Now for most human activities, you cannot compute rewards.
The next time you have a conversation with somebody, try and work out a reward system
so that you can give rewards to the person who's winning the conversation at any given
state.
You will fail.
For most of the activities that humans engage in, you can't compute a reward system.
All right, now this reward idea is the basis of the most famous definition of intelligence
within the AI world.
It's a definition of intelligence which is supposed to be universal.
So it applies in principle to bacteria and animals as well as to humans and computers.
But in fact, it only works for computers and it only works for a reward system like the
one I just described.
So in other words, it's not universal at all.
I would add also that it has the problem not merely that the rewards are confined to a
certain kind of computational reward, but the environments are restricted to those kinds
of environments which can be captured by binary strings.
And most environments that human beings are in are not such that you can capture them
by means of binary strings.
You have to remember that this is changing constantly and it's changing constantly in
such a way that what the hunter down here does is affecting what the birds out here
do and vice versa.
That kind of dependence of rewards on rewards is way beyond anything that the computer can
emulate.
All right, now the greatest of all the AI researcher, great man, is Schmitt Huber who
is discussed in the Drive to Knowledge book that I mentioned.
So they mention that Schmitt Huber has 1990 already.
He's worked on the problem of curiosity.
How can we implement in a machine the way in which scientists search for new questions,
new good questions?
That's curiosity.
And he wrote this program called Power Play in 2011 which continually searches for novel
well-defined computational problems whose solution and so forth.
So he's trying to emulate curiosity by taking the realm of computational problems and drawing
conclusions having to do with having recreated the power of curiosity inside the computer.
But this only works for curiosity, as we might call it, in relation to certain kinds of computer
problems which can be generated automatically just by combinatorial means.
But he draws a conclusion that the rewards that he's built into this Power Play algorithm
instill what he calls a desire on the part of the system to improve its understanding
of the world to model its own ignorance, thus showing a rudimentary form of self-introspective
behavior.
It doesn't do any of those things.
It's a machine.
It's computing.
It's a Turing machine.
It doesn't have self-introspective behavior or understanding of the world or desires.
Now there's one other case mentioned in the same book which talks about the benefits of
an intrinsic curiosity model which can speed up learning.
And they really do show that their model can speed up learning when it's conjoined with
other ML algorithms.
But where do they show this?
They show it in playing video games.
Now video games are just another example of a computational problem.
It's a static mathematical environment and you can solve a game just by reversing the
operator that defines the environment.
So you're not doing anything which has to do with curiosity.
And so the question arises, are there any places outside of games and outside the realm
of computational problems such as dealt with by Schmidhooper where we have success of AI
in the modern stochastic sense?
And I'm going to talk about two.
First is Google Translate.
The book, the final chapter, is a statement of optimism about AI.
Both Yorbs and I think that AI has a great future.
It will create many great things for human beings in the future.
But that most of the claims made on its behalf currently are ungrounded.
Google Translate, we think, is a fantastic tool.
It has its weaknesses, but it's a fantastic tool.
What does it do?
It takes sentences in German and it turns, for instance, and it turns them into sentences
in English using an algorithm with 213 million parameters.
You can imagine it as a large polynomial functor which applied to an input, gives you an output.
The input is an English sentence expressed as a binary vector and the output is a German
sentence or vice versa expressed as a binary vector.
Google Translate works pretty well with text.
You'll find it doesn't work so well with articles in physics journals, but it works
pretty well with text in ordinary English or Urdu or any other language on the list
of languages which is growing.
And the reason it works with text is that we can take a lot of text from the English
language and a lot of text from the German language and we can convert that finite corpus
of text that we've created into a simple system, what we call a logic system, a system that
can be computed over.
And that means that we can compute from English into German because we can compute with logic
systems to create outputs which are other logic systems.
This works where we have a sufficient amount of data, but it works not just because we
have a large amount of data, it's because the data is representative.
So people think that all you need in order to do machine learning is a huge amount of
data, but if the data is not a sample which has the same variance as the entirety of the
data from which that sample is drawn, the machine learning won't work.
We can do that with text, it's static, we can find a representative sample.
It won't be static forever, so German and English are both changing and so if they don't
change Google Translate at regular intervals, it will gradually lose its value.
It changes slowly enough, the languages, that Google Translate is very useful.
Dialects change more quickly, but you can't do any of this with human conversations.
Human conversations change continuously and it's very hard to collect data about conversation
and they often involve a lot of our own movements and gestures.
There are very few videos of real conversation, it's going to be hard to get enough data to
use stochastic algorithms in order to pass the Turing test, but the more significant
problem is that every conversation is different.
So people can use the same sentences over and over again, but every conversation is
going to involve a different context, different concerns, different goals on the part of the
context, participants and so forth, and so it's going to be impossible to find a representative
sample.
And the result for Google Translate is that you have a really good translation engine,
but it's shallow, for a certain kinds of complexity it fails.
And it doesn't work in professional context, simultaneous translation and so forth.
Could we make it better by having larger and larger training sets?
Well yes, but only up to a point, language is changing, they have a volunteer system,
they say whereby people can substitute corrections, people speaking minority languages particularly,
but it seems that people don't submit corrections as far as you can tell and that Google is
not encouraging such volunteers to come forward as they once were.
There are also computational limits to getting around the shallowness problem, and as I say
languages and dialects are continuously changing.
Now the second example is AlphaFold, and this is DeepMind which is owned by Google, it's
a fantastic algorithm for solving the problem of protein folding, which has been a problem
for 50 years, and which is a very important problem, principle solving this problem could
provide valuable inputs for finding new drugs and so forth.
So it's a really important and interesting problem, the fact that they've solved it in
the way that I will describe in a minute is an impressive achievement.
There are several limitations which your documents in this talk here, which will soon be published
as a paper, it only works if the protein who's folding or whose structure you're trying
to predict belongs to a family of homologous proteins, at least one of whose members has
already been determined structurally by means of old fashioned experiments.
One protein takes five to ten years, only for those proteins which are in a family for
which we have at least one analysed structure, can be analysed by AlphaFold to predict its
structure.
So, as I say, we are really impressed, but we're impressed in part because this is an
achievement of humanity, it involved many many people going back generations, it involved
many many sources, different kinds of experimental achievement, different kinds of data, different
kinds of mathematics in fact.
Humanity could create this because many people wanted to create it, they had the will, they
wanted to create it collaboratively, so they had the will to share their efforts with other
people who also wanted to create it, and this is an achievement of the human will.
Alright, now the human neurocognitive system is a very complex system on many levels and
the system of systems which is formed by groups of humans is even more complicated because
it has the complexity of the human brain but multiplied by the number of people in the
collaborating organisation.
There are some relations between minds and brains which are specific to individuals.
Individuals are changed by their specific environments, by their evolution, by their
socialisation and by random factors, so that the idea that we could have a theory which
describes the activities of the human mind on the basis of what we know about neurons
and so on is a mistake.
We would have to carry out that investigation for every single individual human that we're
interested in, we can't just do it once and then generalise to all human beings and that
is important again for understanding the ways in which human conversation differ so greatly
from one instance of conversation to the next.
So we'd have to understand the molecular configurations of all neurons in order to understand their
dynamic interactions which create those things which we know subjectively as molecular, as
mental processes.
And as I said, this will vary, the way we go from studying neurons to studying mental
processes will vary from person to person.
As humans have a will, they can build things like this.
It took 632 years to build this.
It's Cologne Cathedral and AlphaFold looks like this.
So I'll just tell you, present to you four diagrams which show you the successive steps
in AlphaFold and it didn't take 632 years to put these together but it took a long time
and a lot of testing and rejecting and testing and rejecting and thinking, finding new sources,
working out how to input those new sources and those new sources themselves may have
rested upon chains of human.
There's a lot of prior knowledge about biology that is actually encoded mathematically and
this requires a huge amount of design.
So basically the whole multiple sequence alignment but also the non-protein structures
of the norm represents those alignments are here encoded into martyred CZ, then feed
into the model.
If you remove that, the model has zero predictive value.
So that's very interesting to realise how the human knowledge is basically leveraged
in a much more complicated way than the GLM model that we showed from China.
This is the most high sophisticated neural network that is currently available and the
way that it's used is very elegant, I just wanted to highlight that.
The hypothesis that I'm defending now is that there can be no machine will.
Machines cannot want to do anything.
Even if they could want to do it, they couldn't do it because it involves too many complicated
collaborations of the sort which only humans are capable of.
But machines can't want to do it.
There can be no machine will.
And that means that machines cannot emulate human or animal volition.
Animals have the problem that they have to find food and they consume the food and that
means that their food is all gone.
So they have to go looking for more food.
And this is true whether they're foraging or whether they're hunting.
They need energy.
They have to find food.
They use up all the food.
They have to go and find new energy.
That's why perception evolved because they needed to see the new food.
They needed to find it.
They needed to have curiosity and they needed to have a will, not just sit and die.
They want to go and live on and so they go to find new food.
And they gradually, of course, they will face new kinds of dangers, predators and so forth.
And so they will develop communication skills signaling in order to signal that there is
a predator nearby.
So there is a constant need to adapt to new food sources.
But this is always within a single ancestral environment and it's all of it based on instincts.
But animals have what we call primal intelligence, which is basically the kind of intelligence
which is expressed when you're reacting to a biological need, eating, fighting and so.
And humans have this primal intelligence also, but it's always mixed in humans with another
kind of intelligence, which I'll talk about in a minute.
This primal intelligence is not something that animals learn.
It's instinct based.
The idea is that to be a case of intelligence, it has to be a spontaneous response to something
called the novel.
The features of primal intelligence are that it belongs as part of the abilities to adapt
to new environments.
It happens suddenly, it's not trained and it relates to something which is novel.
They've never seen it before.
But non-human animals can't have long term plans.
That's something which requires another kind of intelligence, which we'll see in a minute.
Other animals can have more sophisticated forms of collaboration.
They can develop signal mechanisms.
But humans, they can plan and they can collaborate.
They can use not just signaling mechanisms, but they can use propositional language in
order to come to agreement about complex tasks, like invading Normandy, or again, building
Cologne Cathedral.
And this is what we call objectifying or propositional intelligence.
It's the capacity to conceive of a new kind of environment or a new kind of achievement
or a new kind of device and plan and build it.
A new kind of life involving new kinds of risk or involving new kinds of energy source.
This leads to our modern techno sphere reflects the fact that humans, unlike animals, can
shape the world.
They can create new environments and live in those environments because they're not relying
on instincts.
They're relying on human excess drive, on curiosity.
Now how do we come to have this channeled drivenness?
Well roughly speaking, we're drilled by our parents to have certain capabilities and this
drilling by our parents to have certain capabilities gives us, when we move out of the family environment,
the capability to drill ourselves, to force ourselves to keep pressing forward with some
difficult tasks, even when it's inconvenient or uncomfortable.
This again is something that we're not going to be able to simulate or emulate within an
AI.
So Schmitthuber again, the AI people do not recognize this problem.
Schmitthuber doesn't recognize it.
They talk about it.
They talk about goals.
The AI having goals, but they never give an account of how an AI could have goals.
So Schmitthuber talks about goals in terms of reward, but we've seen that there are
problems with the way in which computational AI understands rewards.
They don't explain how the reference to expected cumulative future reward, which is the equation
I showed earlier, is connected to a machine agent having intentions with specific contents,
which is what human goal seeking is all about.
So the fact that you can be involved in a mathematical environment which has a computational
reward system doesn't explain how you can have goals with specific contents.
And what happens, of course, is, as we saw in the case of AlphaFold, and as we see in
every case of AI, it's humans who supply the goals.
We have to set the rewards for AlphaFold, AlphaGo, and for every other AI system.
Now Nick Bostrom makes a similar mistake.
So this is a book about how robots will arise who will want to take over the universe unless
we help Bostrom try to prevent it.
He wants all philosophers to give up and pure mathematicians to give up what they're doing
and join him in working to defeat the singularity.
But he doesn't reflect on where a will could come into existence, so if the robots are going
to take over the world, they're going to have to want to take over the world.
But he doesn't say what wanting would be on the side of a robot.
He just postulates that they would want, that they would have a will, and that some of them
would have an evil will.
If you can't have a will, you can't have an evil will.
But he does cite Yudkowski, who is a very interesting chap, who I guess is Bostrom's
minister for volition.
And he cites Yudkowski and apologizes when he cites him.
But this is the best he could find.
Yudkowski formulates the idea of a goal system for an AI which contains only decisions, supergoals,
and beliefs, with all sub-goal content being identical with beliefs about which events are
predicted to lead to other events and to desirability being identical with leads to supergoalness.
Now, he's trying to do two things at once, he's trying to explain how an AI could have
goals, but he's also trying to explain how what we want is an AI with supergoal, which
are better than all other goals, and put all good goals together to create a supergoal.
If our wish, if we knew more, ThoughtFyster, were more the people we wished we were, had
grown up further together, where the extrapolation converges rather than diverges, where our wishes
cohere rather than interfere, extrapolated as we wish that extrapolated, interpreted
as we wish that interpreted, then we would have a supergoal.
The volition, the coherent extrapolated volition, which is superintelligence we'll have, which
is extrapolated from our rather insipid human goals, which are not supergoals.
This is nonsense, it's, it's, it's, anyway, nonsense.
Now, Hutter has a slightly different way of pulling the rabbit out of the hat.
So he says, it seems hard if not impossible to define rationality without the notion of
a goal.
We have rationality, therefore, we have a goal, that's, that's it.
He has rationality, he thinks, because his computers are doing logic, but therefore he
must have a goal.
And then he speculates.
But what with the goals of an AIB?
He doesn't explain how the AI could have goals or will or any of these other things.
So why is a machine will so important?
Without a will, you can't have autonomy.
Without autonomy, you can't act either morally or immorally.
You can't act.
And therefore, you can't be subject to ethics.
Now, some people talk about AI ethics.
We think they're making a mistake, unless somebody can show us how an AI system could
be autonomous, and that means having a will.
The question is, well, what is it for a human being to have a will?
And the best account that we have found is in this German philosopher who looks like
the sort of person who has a really strong will, Max Schäler.
And his writings on the will are interesting, not least because they were used by Wojtyła
in his book on Person and Act.
It's based very much on Schäler.
And Jorbs and I have a principle that every talk we give has to mention at least one saint.
Schäler's view is that there are two kinds of ethics.
There's formal ethics, which is just principles or axioms, as we find in Kant.
And then there is value ethics or material ethics, which is based on what he calls value
feelings.
So we can feel that something is good or bad, and it's those feelings which are the basis
for our understanding of ethically relevant or morally relevant phenomena.
And they precede any rational evaluation.
And there are some people who just don't have a gland for feeling values.
So they are pathological people.
He describes his account of the will in terms of a rescue scenario.
So somebody sees a drowning child and realizes he has to jump into the river to save the
child.
And so he wills himself to save the child and jumps in.
But the same scenario could be applied to anywhere where a human being acts voluntarily.
And in many cases, it involves unconscious steps.
But Schäler unpacks the succeeding steps, which he thinks are involved in the rescue
scenario in order to understand what the will is.
So the first set is the acting person sees the object that brings forth his action in
a given situation.
The child has fallen into the river.
And then there is the acting person realizes the need to jump in the river, the content
to be realized, and has the salient value for your feelings.
So he feels that it would be good to jump in, it would be bad not to jump in, and so
on.
All of this is compressed together in one single momentary mental awareness.
And then he decides to jump in.
And this deciding has three components.
He forms an intention to act, he deliberate, this is almost simultaneous, he deliberate
how to act, and he resolves in the sense that he actually jumps.
So resolving is temporarily identical to, it's the other side of the coin with actual
molecular signaling which triggers corresponding bodily movements.
Schäler was very much of the opinion that there is a mind-body continuum.
There isn't the mind and then a body, but rather there's a continuum.
And this continuum means that there are parts of what we think of as mental processes, which
are physical.
And vice versa.
So the resolving and the molecular signaling which trigger bodily movement, they're the
same thing, just different sides of the same coin.
All right, so that's Schäler's account.
I will conclude just with this question.
How could we build an artificial general intelligence that could or would do all of this?
So it's not enough to just program a goal system, whatever that would mean.
And some people say, well, if we have enough computers attached to the internet, they will
spontaneously create some kind of super mind which will have goals, or if we have a sufficiently
fast quantum computer, then it will be so powerful that it will develop goals.
All of that is nonsense.
A quantum computer is just a Turing machine, it just moves very quickly.
There is no way in which we can conceive that a machine could have all of this is involved
in those four steps.
Because in order to have those four steps, we need a very, very intricate complex of
mental and physical dispositions, which took millions of years to evolve.
And we can't simulate evolution in the computer either because evolution is much more difficult
to understand than intelligence.
And so I think that will be the end.
Okay.
Thank you.
Thank you.
Thank you.
Thank you.
Thank you.
Thank you.
Thank you.
Thank you.
Thank you.
Thank you.
Thank you.
Thank you.
Thank you.
Thank you.
Thank you.
Thank you.
Thank you.
Thank you.
Thank you.
Thank you.
Thank you.
Thank you.
Thank you.
Thank you.
Thank you.
Thank you.
Thank you.
Thank you.
Thank you.
