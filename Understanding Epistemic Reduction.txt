Understanding Epistemic Reduction
Open sidebar
Understanding Epistemic Reduction
Default (GPT-3.5)
SURVEY OF ���UNDERSTANDING��� 
DEFINITIONS 
Northwest AGI Forum 
Mike Archbold 
Copyright September 2021 
���to understand is to explain why something is the way it is. 
is to be able to explain why you did something. 
is to be able to explain why others did things.��� 
doddy 
"Understanding is a model that allows accurate prediction and explanation of 
phenomena." ~ Michael S. P. Miller 
source: email
1 / 1
ChatGPT
ChatGPT
It seems you've provided some definitions of "understanding." Is there anything specific you would like to discuss or ask about these definitions?
1 / 1
Understanding is based on knowledge that has the potential to act as partial explainability 
and which can be adaptive to new knowledge or experiences which fit well into that 
knowledge. I may not be able to explain exactly how my car works but I have partial 
knowledge of operating it, some insight about how it works, and why I depend on it. 
There are a lot of possible impediments to explainability and to adaptability. Human beings 
are not capable of perfect explanations or of perfect adaptability. You can question whether 
"perfect adaptability" even makes sense. On the other hand, a lack of adaptability as an 
agent has opportunities to learn will become more obvious over a period of time if those 
should-be learning experiences are obvious. And the ability to think about the reasons why 
you did something, or to entertain conjectural reasons why some kind of event occurred 
seems like fundamental processes of thinking to me. 
Jim Bromer 
source: email
1 / 1
ChatGPT
ChatGPT
You've provided a thoughtful perspective on understanding, emphasizing the importance of knowledge, adaptability, and partial explainability. It's true that human understanding is not always perfect, and our ability to adapt and learn plays a significant role in how we make sense of the world. Exploring reasons behind actions and events indeed appears to be a fundamental aspect of our cognitive processes. Is there something specific you'd like to discuss or explore further about this perspective?
1 / 1
In my definition understanding is the ability of a system to describe a new concept as a set of 
characteristics, the values of these characteristics and/or allowable ranges of these values, 
and functions of relationships between values of these characteristics, both within one 
concept and between values of characteristics of different concepts (other new or old 
concepts). Moreover, some characteristic or its value can be presented as another new 
concept or as previously described "old" concept. The basic concepts/characteristics on 
which the system builds other concepts are system input signals (signals from system���s 
receptors). 
In the mentioned system each characteristic and their value can be represented, 
respectively, as a signal from the element of the system that detects this characteristic and 
the magnitude of this signal. The function of the relationship between values of characteristics 
can be represented in this system as the relationship between signals generated by 
corresponding elements of system that detect these characteristics. In such system this 
relationship between signals can be implemented as a relationship (connection) between 
elements of the system that generate these signals. 
Different systems that have the abilities described above may represent one and the 
same concept as different sets of characteristics, their values and functions of relationships. 
Also, these systems may have different numbers of such interconnected multilevel concepts- 
different depths of the internal map of the world. Therefore these systems will have different 
understanding of this concept with different depth of understanding. 
For an example, some simple system can detect different dogs in pictures as sets of 
different multicolored spots, detecting at each its level new concept like a combination of 
these multicolored spots, a combination of their combination and so on. This system will have 
its own level of understanding of what dog is. For this system, it is a set of interconnected 
���meaningless��� (which do not represent really existed distinguished objects or processes in the 
real world) multicolored spots. To build a new concept of a cat, this system must use a new 
sets of interconnected multicolored spots. On the other hand, a more complex system may be 
capable to detect concepts such as paw, tail, ear, eye, distance, position and so on (really 
existing distinguished objects or processes in the real world), and build the concept of a dog 
like relationships of these concepts. To build a new concept of a cat, this system can use 
those concepts that it can already detect (paw, tail, ear, eye, distance, position and so on). 
This system can ���learn��� a new concept much faster as it does not need to build a huge sets of 
new subconcepts, it mast only build a few new concepts that it has not yet built and that 
specific to the concept of cat, if necessary modify some old concepts, and build from them 
new concept of cat. More complex system can be capable to detect much more of such 
���meaningful��� concepts and interrelationships of these concepts. This can give for such system 
the ability to connect concepts such as dog with concepts such as: animal, finger, blood, food, 
bark, run, happy and so on. Although at the lowest level, all of these concepts will be based 
on some kind of combination of signals from the receptors (multicolored spots for visual data). 
All of the described systems have their own understanding of what dog is, but their 
understanding and depth of understanding are different. Therefore, their ability to interact with 
this concept, to find similarities and relationships between this concept and other concepts 
and so on are different. 
In the case of systems like GPT-3 things are worse. Systems of this type at their 
receptive (base) level have sets of words and sentences that, in turn, are not a direct 
reflection of real world objects (processes), but are an encrypted reflection of the world map 
of a certain system that created these 
words. Therefore all concepts that this system (GPT-3 like) can build are combinations of 
these words and sentences with the relationships between them. Any concept which 
represents an object (process) for this system is just a ���meaningless��� (which do not represent 
really existing distinguished objects or processes in the real world) "multiworded" spots. Even 
a very complex system of this type can only understand the different types of sentences and 
their combinations, perhaps some grammar rules, some general phrases, and so on. For this 
system, the whole world is just sets of words. 
In conclusion, I think that sales/marketing people that claim their AI ���understand��� are 
right. But the understanding that their system has is not the understanding that the intelligent 
system of the human level has. 
I hope my vision will be helpful.
1 / 1
ChatGPT
ChatGPT
Thank you for sharing your detailed perspective on understanding. You've presented an intricate explanation of how various systems might form and represent concepts based on characteristics, relationships, and interconnectedness. Your comparison of simpler and more complex systems' understanding levels, including those similar to GPT-3, highlights the distinction between human-like understanding and the capacity of AI systems. It's an interesting analysis of how different systems build their internal maps of the world and the depth of their comprehension. If you have any questions or thoughts you'd like to discuss further, feel free to share!
1 / 1
~~~~ 
The definition of the term ���understanding��� which I described in a previous email describes a 
basic level of understanding (as I understand it), but often term ���understand��� used as high- 
level understanding that may include even processes of awareness and consciousness, and, 
from my point of view, is a result of 
interactions of certain types of chains of elements of intelligent system that detect basic 
concepts, and are not possible without this basic ���understanding���. 
Valentin Scherbak 
source: email 
Understanding is the result of subconscious learning over a lifetime using an algorithm that 
we might as well call "Intuition" that jumps to conclusions on scant evidence. 
We are only born with instincts. There are very few of those. Everything else is learned 
subconsciously, starting with vision, or consciously, such as memorizing facts and Models. 
Everything we perceive, what is in our visual field, what we hear, language we hear or read, 
and any physical skill like walking, talking, skiing or tennis, is based on Understanding. When 
using these skills, you do not have time to Reason. See "Libet Delay". And anything that gets 
better with practice is based on Understanding. It is a slow process of learning from micro- 
mistakes and correcting them a little at the time. All Understanding systems learn from their 
own mistakes, including brains. Building an "AI" that is not learning from its mistakes is mere 
programming, and such efforts will never become anything worthy of the terms "Intelligence" 
or "Understanding". 
If you can Reason your way to a correct answer without practicing any skills (emphasis 
mine) then it's Reasoning (on top of, and based on, your Understanding). 
Monica Anderson 
source: email
1 / 1
ChatGPT
ChatGPT
You've provided two interesting perspectives on understanding. The first emphasizes the distinction between basic and high-level understanding, suggesting that higher-level understanding involves interactions between chains of intelligent elements detecting basic concepts. The second perspective delves into the role of intuition, learning, and practice in developing understanding. It highlights the importance of subconscious learning over a lifetime and the connection between skills, practice, and understanding. Both viewpoints contribute to a broader understanding of how human cognition and AI capabilities intersect. If you have any questions or thoughts you'd like to explore further, feel free to share!
1 / 1
1/ Understanding is a process in which one���s experiences are 
known to be true, reliable and logically consistent. 
2/ Understanding is ���knowing with certainty���. 
And here is the one from Wikipedia: 
Understanding is a psychological process related to an abstract or physical object, 
such as a person, situation, or message whereby one is able to use concepts to model that 
object. 
Understanding is a relation between the knower and an object of understanding. 
Understanding implies abilities and dispositions with respect to an object of 
knowledge that are sufficient to support intelligent behavior. 
~~~~~ 
Here is another definition which adds the concept of applying/using the 
knowledge and tries to explains the terms being used in the definition. 
Understanding is having an in-depth knowledge of a subject domain such that one can 
reliably apply it. 
Where: 
1. in-depth means detailed, underlying principles, 
2. knowledge is spatial and temporal, structural and functional, cause and effect and 
relational, 
3. to apply is to use, teach, explain, recognize, relate, manage, control, respond to, rephrase, 
answer etc. 
~~~~~~ 
A definition that can be used to help construct an AGI because it describes key properties 
of what understanding is comprised. 
Noun: 
As a thing: An understanding is a model of something that you believe to be true because of your 
experiences. 
As a process: Understanding is something you do to create the model. 
Verb: 
To understand is to have such a model. The model is a mental representation of the structure, 
composition, relationships and operation of the thing that you understand. 
Based on the model's level of detail, consistency and completeness you may have the ability to 
recognize, predict, reason about, describe, explain, manage and/or use the thing. 
Brett Martensen 
source: email
1 / 1
ChatGPT
ChatGPT
Thank you for sharing these various definitions and perspectives on understanding. Each one offers a unique viewpoint on what understanding entails, whether it's related to knowledge, certainty, applying information, or building mental models. These definitions highlight the multifaceted nature of understanding and its connection to cognitive processes and behaviors. If you have any specific questions or topics you'd like to explore further, feel free to let me know!
1 / 1
Summary: ������understanding��� at its simplest is ���being able to respond appropriately��� ��� a purely 
behavioral measure. The next level is to be able to tell back the information in your own 
words. Another level is knowing the implications. Personal experience takes us to a much 
higher level of integration ��� this is where the connection to the ���I��� comes in. Having deep 
subject knowledge and skill is another dimension. Finally, knowing how something functions 
in detail ��� knowing the specific cause-effect mechanisms.��� 
Peter Voss 
source: https://chatbotslife.com/understanding-understanding-9dcc15759b5b 
video too: https://www.youtube.com/watch?v=02OtOg6g6vU 
no, there is no short definition. 
this is one of the qualia that forever will have a different context in different people's mind. 
you should interpret the word "story" in a general context (any sequence of perceived events) 
like "narrative". It is not about written stories like books. in my view upon "thinking" and 
"awareness" and "understanding", it is crucial that the system can IDENTIFY the beings or 
processes that cause (observed) things happen. I call these the "actors", so actors are not 
always persons, rather you can not see an action without knowing the actor (because then 
you have not understood the action).
1. ability to classify the items in the story (tell of which type they are) 
2. ability to identify the items in the story (tell which specific instance of the class they 
are, e.g. recognize person instead of only knowing it is a pedestrian) 
3. ability to determine which found classes and instances are actors (e.g. persons, 
machines but also "the weather" like in "it is raining" the weather is the actor doing the 
action raining) 4. ability to isolate changes that were actions 
5. ability to identify responses that followed those actions 
6. ability to combine the correct actions and reactions into interactions (which are 
steps for the scenario or script needed later) 
7. ability to assign perceived actions to perceived actors in the story (needed to convert 
the story data format into a scenario data format) 
8. ability to translate scenario to assign one-self as one of the actors (meaning 
reproduced/execute the observed sequence of interactions) 
9. ability to tell something about each noun and pronoun that occurs in a text or each 
thing/being (that occurs in the story) and each relation between any of them. 
10. ability to either paint or describe (visual shape) or point (locate in space) each real- 
world thing in the story 
11. ability to tell what the further implications are of the told story (not only those 
already told in the story). 
12. ability to rephrase the sentence or story (tell the same thing with different words). 
13. ability to tell which aspects are unusual (out of the ordinary) about the story. 14. 
ability to tell what is salient about the story (what one did not knew before and learned 
from the new sentence(s)). 
That is my short version, you will notice it is almost pseudocode for reasons you can also 
guess. 
Stephan Verbeeck 
source: email
1 / 1
ChatGPT
ChatGPT
Thank you for providing such a comprehensive collection of viewpoints on understanding. These definitions and perspectives cover a wide range of aspects, from behavioral measures to the ability to classify, identify, analyze implications, and even rephrase information. The different dimensions of understanding highlighted by these viewpoints showcase the complexity and diversity of interpretations surrounding the term. If you have any questions or if there's a particular angle you'd like to discuss further, please feel free to let me know!
1 / 1
To me understanding a concept means that the entity (human, robot, monkey, etc. can act 
appropriately on that concept. If I tell my robot to get me a glass of water, there are multiple 
concepts here that the robot needs to understand at some level. Let's just take the "glass of 
water" idea. A glass is fairly easy to define. We can take 
the easy way out and use ML to recognize a glass. But we really need to add something on the 
functionality of the glass as a container, then we need to understand the concept of container. 
Again, we need to define the functionality. Water is another concept. do we mean just any water, 
clean water, dish water, toilet water. So the robot needs to understand the concept of how the 
water is going to be used, and how to source the water we want. The word of needs to be 
understood to mean containing. So to understand a simple phrase like this requires the robot to 
understand a lot of concepts, and how they are related. I think this is what we really want in 
terms of an AGI. 
[9:55 AM] The other thing we can do is train the robot to recognize the phrase, rather than the 
individual words, and translate the phrase into a set of action that we want the robot to 
accomplish. We get the same result, but the level of understanding is much less. Although there 
is still some level of understanding, it is on the phrase level, rather than the word level. So 
understanding seems to have levels, much as we have talked about levels of intelligence. 
Robert Higgins 
source: Slack channel ���IntelligentAgent��� 
Understanding is recognition. Which is a lossless component of compression from 
comparing input to template. That's all there is to it, and it applies to countless other words 
that mean the same thing. 
Boris Kazachenko 
Source: https://agi.topicbox.com/groups/agi/Tf91e2eafa2515120/how-does-a- 
machine understand-what-is-your-definition-of-understanding-for-an-agi
1 / 1
ChatGPT
ChatGPT
Thank you for sharing these insights into the concept of understanding. The examples you provided, along with the viewpoints of Robert Higgins and Boris Kazachenko, highlight the multifaceted nature of understanding. From recognizing concepts and their relationships to the various levels at which understanding can occur, these perspectives showcase how complex the process of comprehension can be. The idea that understanding can involve recognizing patterns, comparing inputs to templates, and translating phrases into actions adds depth to the discussion. If you have any further thoughts or questions on this topic, please feel free to share!
1 / 1
Let's take an autistic savant who memorizes the digits of pi to a million decimal places as an 
example of someone who "knows pi" but does not "comprehend pi". 
Let's now take Archemedies who described a method of finding the digits of pi by 
construction of regular polygons, to any desired degree of precision as someone who 
may not "know pi" but does "comprehend pi". 
Archimedes can describe his method in such a way that anyone reasonably educated can 
execute it. It is an "algorithm". This algorithm's expression in, say Greek, has a length. 
Chaitan would say that Archimedes's algorithm is a compressed representation of pi very 
close to the smallest one can achieve, and that, therefore, "compression is 
comprehension". 
By the way, something I should "comprehend" about Kolmogorov Complexity, but don't, 
is why halting is required of the algorithm that outputs the "comprehended" bit string. It 
seems just the opposite is necessary in the case of, say, pi. How else can one use the 
Kolmogorov Complexity program to make predictions? 
~~~~~~~~ 
Note that I avoided the word "understand" in my post, and, instead, used the word 
"comprehend". Others have brought in the notion of "value" to the notion of "understanding"; 
they bring up "relevance" to one's "goals" etc. I suppose, therefore, one might say that one 
achieves "understanding" when one "comprehends" the relationship between an object (say, 
pi) and one's decisions (say, I'm considering building a geodesic dome for my family). 
Knowledge? Epistemology doesn't place any _rigorous_ demands on us regarding 
comprehension (in the Chaitin sense), does it? (Please excuse the prior misspelling of 
Chaitin.) 
James Bowery 
source: https://agi.topicbox.com/groups/agi/Tf91e2eafa2515120/how-does-a-machine- 
understand what-is-your-definition-of-understanding-for-an-agi 
Here's my crack at it: 
To understand a concept or statement is to determine its relevancy or relationship to 
one's personal goals, experiences, or world-model. 
I recently read this paper and thought it was pretty interesting (and a relatively easy 
read): https://cse.buffalo.edu/~rapaport/Papers/crs.pdf If you don't already have 
something about conceptual-role semantics in your literature review, it might be worth 
including. 
Writer of Minds 
source: https://agi.topicbox.com/groups/agi/Tf91e2eafa2515120/how-does-a-machine- 
understand what-is-your-definition-of-understanding-for-an-agi
1 / 1
ChatGPT
ChatGPT
You've shared some thought-provoking insights into the distinction between "knowing" and "comprehending" a concept, using examples like the autistic savant and Archimedes to illustrate the point. The contrast between memorization and understanding, as well as the consideration of Archimedes' algorithm and its connection to compression and comprehension, adds layers to the discussion.
Your interpretation of understanding as the determination of relevancy or relationship to personal goals, experiences, or world-model aligns with the idea that comprehension involves the contextual application of knowledge. The additional source you mentioned also seems like a valuable contribution to the conversation around conceptual-role semantics.
The exploration of what constitutes true understanding and how it relates to personal context and goals is a complex topic that involves various perspectives. If there's a particular aspect you'd like to delve deeper into or if you have any questions, feel free to let me know!
1 / 1
My opinion on understanding, which is still evolving: 
Understanding is one aspect of a conscious being, closest to knowing, but also including 
thinking, feeling, intuition and emotion. Some claim these are inseparable, or that 
separating them limits the types of knowledge that can be represented, and I tend to 
gravitate in that direction. 
We humans can know things we can't articulate, and we can be articulate about things that 
are not really what we believe internally. "Understanding" as a test for AGI might be a 
gentleman's agreement upon a certain level of articulation being reached for the idea at 
hand. We see the first inklings of this in the XAI endeavors. However, some if not all 
understanding comes before the language needed to express it, and I think this applies to 
both humans and AGI; therefore a more rudimentary framing of understanding could be 
knowledge which gives the capability to inform an action. Still though, my opinion is that 
understanding, or realization of knowing, is deeply intermingled with an idea of feeling. 
Understanding may be inseparable from other types of cognition. 
In any case, not taking advantage of that capability to inform an action can cause non-ideal 
situations for the individual, such as how I knew it would rain today but did not think to roll up 
my car windows. 
It could also be evident in pathologies where we say things we don't believe, or believe 
things we wouldn't say: both of those situations give a sense that feelings, possibly 
repressed, may be involved. 
In our AGI system, the most fundamental type of understanding is identical to knowledge 
which could be traced back to learning of patterns by a type of Hebbian function. In common 
parlance, we think of understanding as a type of knowledge orders of magnitude higher than 
the level of sensory spacetime co-occurrence; however the Hebbian learning analogues 
continue at recursively higher levels on the pattern matchers themselves. This is something 
still under development. One of the many challenges we're facing here is the level at which 
we allow higher level patterns to be brought into the simulation space for manipulations 
(thinking about it's own understanding of the pattern). The lower levels of pattern recognition 
are not of the same types as the higher level patterns, and the difference between them 
delineates the conscious from the subconscious. We also have not incorporated the concepts 
of hedonic feeling, however there are some simple proxies for intuition based in familiarity 
with a pattern. 
In an analogy, commonly seen in optical illusions, a layperson cannot understand nor explain 
why we see black dots on a grid illusion. Certain pattern matchers which were triggered in 
the optical processing region are not able to be brought into question or simulation. There 
are areas of our own being which we are helplessly unable to understand via articulation. By 
this I argue that an AGI could (should?) be average-human-level in understanding without 
the need of "understanding" it's full being. 
In a 1977 paper by Louis Arnaud Reid in the Proceedings of the Aristotelian Society #77 
"Thinking, Feeling, Knowing" doi 10.1093/aristotelian/77.1.165 , the author gives a (IMO) 
great argument about how "feeling" (not necessarily those marked by noticeable hedonic 
tones) are an inseparable part of our "knowing" something, both before and after we proclaim 
to know that something. 
Daniel Jue 
source: https://agi.topicbox.com/groups/agi/Tf91e2eafa2515120/how-does-a-machine- 
understand what-is-your-definition-of-understanding-for-an-agi
1 / 1
ChatGPT
ChatGPT
Thank you for sharing your evolving opinion on understanding. Your insights into the interplay between knowing, feeling, thinking, intuition, and emotion provide a holistic view of how understanding is intertwined with various cognitive and emotional processes. The notion that understanding can encompass both articulation and non-verbal knowledge, and that certain understanding may precede language, is a fascinating perspective.
The comparison between lower-level pattern recognition and higher-level patterns, as well as the analogy with optical illusions, helps illustrate the complexity of understanding and how it can exist at different levels. The consideration of average-human-level understanding in AGI, along with the inclusion of feeling and familiarity with patterns, presents a comprehensive approach to defining understanding.
Thank you for referencing the paper by Louis Arnaud Reid, which adds another layer of insight into the connection between feeling and knowing. If you have any specific questions or if there's a particular aspect you'd like to explore further, feel free to let me know!
1 / 1
One of the problems in this grip on 'understanding' or 'knowing' is a confusion of 
competency in knowing X as having anything to do with general intelligence. 
When the time comes I could probably toss my hat into the ring on this. 
I recently had an opportunity to explain this to a bunch of year 12 
wizkids here. https://www.youtube.com/watch?v=TtTt6MvIOig 
Maybe it would do as a primer. It's a half hour. Skip to 20 minutes to cut to the chase on 
my take for purposes here. 
Colin Hales 
source: https://agi.topicbox.com/groups/agi/Tf91e2eafa2515120/how-does-a-machine- 
understand what-is-your-definition-of-understanding-for-an-agi 
You understand a sequence of symbols if you can predict or compress them. If I wanted to 
test if you understand Chinese, I would show you some Chinese text and test how many 
characters you could guess next. 
Matt Mahoney 
source: https://agi.topicbox.com/groups/agi/Tf91e2eafa2515120/how-does-a-machine- 
understand what-is-your-definition-of-understanding-for-an-agi
1 / 1
ChatGPT
ChatGPT
Thank you for sharing these perspectives on understanding. Colin Hales' point about the distinction between competency in knowing a specific subject and general intelligence is a valid consideration. The video you mentioned and the discussion it led to seem like valuable resources for delving into this topic more deeply.
Matt Mahoney's idea that understanding a sequence of symbols involves predicting or compressing them aligns with the concept of predictive processing, where comprehension is tied to the ability to anticipate patterns in data.
Both viewpoints contribute to the ongoing exploration of what constitutes understanding and how it relates to different aspects of cognition and intelligence. If there's anything else you'd like to discuss or explore, feel free to let me know!
1 / 1
Understanding is the editing distance between data points and the distance of objects on 
a traveling map, done by a conscious mind. 
keghnfeem 
source: https://agi.topicbox.com/groups/agi/Tf91e2eafa2515120/how-does-a-machine- 
understand what-is-your-definition-of-understanding-for-an-agi 
Understanding is the skill of the machine. Machine X understands Y if it can 
determine the completeness and consistency of Y according to its internal 
application domain model. The AGI application area is the real world. In simple 
words, understanding is the ability to determine whether something can be. 
How can a machine understand? 
The machine generates a real-time representation (model) of the current 
situation (CSR). 
Determines whether all the values of the CSR elements correspond to each 
other. 
If they do, then the machine understands the situation and acts in 
accordance with it. 
If not, then he does not understand the situation, identifies non 
conformities and acts to eliminate the non-conformities. 
Roman Kofman 
source: https://agi.topicbox.com/groups/agi/Tf91e2eafa2515120/how-does-a-machine- 
understand what-is-your-definition-of-understanding-for-an-agi
1 / 1
ChatGPT
ChatGPT
Thank you for sharing these definitions of understanding. The notion that understanding involves editing distance between data points and objects on a map, as well as the concept that understanding involves the machine's ability to determine completeness and consistency according to its internal model, both contribute interesting viewpoints on the topic.
The idea that understanding could be seen as the ability to determine whether something is coherent or can exist according to the machine's internal model adds depth to the discussion. It highlights the AGI's role in assessing and acting upon its perception of a situation.
These perspectives provide valuable insights into the multifaceted nature of understanding and how it could be applied in the context of machine intelligence. If there's anything else you'd like to explore or discuss, please let me know!
1 / 1
to understand is to explain why something is the way it is. 
is to be able to explain why you did something. 
is to be able to explain why others did things. 
doddy 
source: https://agi.topicbox.com/groups/agi/Tf91e2eafa2515120/how-does-a-machine- 
understand what-is-your-definition-of-understanding-for-an-agi 
Understanding is about the ability to create a worldview expressed with rich knowledge 
representation ���. to be able to model inside the machine, inside the person, the complex 
world that is outside. The other capability of understanding is to acquire and interpret new 
information. And to enhance and continuously update this worldview because the the world 
around us is very dynamic. The interests are growing. So part of understanding is the ability 
to continuously update this internal view inside the machine or on us of what is happening 
outside. And the third is the ability to effectively reason and explain both our existing 
knowledge. As well as in real time, new information coming from the outside. 
Gadi Singer 
source: https://www.youtube.com/watch?v=aqfizAySe0E
1 / 1
ChatGPT
ChatGPT
Thank you for sharing these perspectives on understanding. The first viewpoint, expressed by "doddy," highlights the importance of being able to explain why something happened, whether in terms of personal actions or the actions of others. This emphasizes the role of explanation in grasping the reasons behind events.
The second perspective, presented by Gadi Singer, offers a broader definition of understanding, encompassing the ability to create an internal model of the complex world, interpret new information, and continuously update this internal worldview. It also emphasizes the importance of effective reasoning and explanation.
Both viewpoints contribute to the multifaceted nature of understanding, showcasing its various aspects and applications in different contexts. If there's anything specific you'd like to discuss further or explore, feel free to let me know!
1 / 1
This word ���understand��� is one of these suitcase words that no one agrees what it really 
means ��� almost like a placeholder for mental phenomena that we can���t explain yet. But I 
think this mechanism of abstraction and analogy is key to what we humans call 
understanding. It is a mechanism by which understanding occurs. We���re able to take 
something we already know in some way and map it to something new. 
~~~~~~~ 
Melanie Mitchell 
1) source: https://www.quantamagazine.org/melanie-mitchell-trains-ai-to-think-with- 
analogies 20210714/?mc_cid=a4fbbe27a8&mc_eid=c7558b25d5 
2) source: https://arxiv.org/pdf/2104.12871.pdf
1 / 1
ChatGPT
ChatGPT
Melanie Mitchell's perspective on the term "understand" as a complex and often elusive concept is quite insightful. The idea that "understand" can serve as a kind of placeholder for mental phenomena that are difficult to explain is an interesting way to describe its multifaceted nature. The concept of abstraction and analogy as mechanisms of understanding resonates with the cognitive processes that humans often use to make sense of new information based on existing knowledge.
Thank you for sharing this perspective and providing the sources for further reading. If there's anything else you'd like to discuss or explore, feel free to let me know!
1 / 1
Understanding is the process of reaching a state when observations, 
models and predictions line up and network activity drops 
significantly indicating low prediction errors. Although this happens 
all the time in adult brains, it is especially noticeable (aha!) after a 
long running high activation thought process, i.e. one with large 
prediction errors that aren't resolved for a long time. 
In other words, understanding is when free energy reaches a 
minimum for a given context. 
Amol Kelkar 
source: Slack channel ���IntelligentAgent���
1 / 1
ChatGPT
ChatGPT
Thank you for sharing Amol Kelkar's perspective on understanding. The idea that understanding involves aligning observations, models, and predictions, while reducing prediction errors and dropping network activity, provides a neuroscientific view of the concept. The concept of free energy reaching a minimum for a given context, as a way to measure understanding, is intriguing and highlights the role of coherence and prediction in the cognitive process.
If you have any more thoughts or questions to discuss, please feel free to share!
1 / 1
The common conception of ���understanding��� in the context of effectively any machine can be simply 
stated: given some input, the machine can be said to ���understand��� if the correct output results. As an 
example, a calculator ���understands��� addition. An ATM ���understands��� a withdrawal. Google 
���understands��� a routine weather inquiry. 
In AI it is common to hear vendors claim that their AI ���understands��� various scenarios. Things quickly 
become murky, however, since the vendor is making a subtle implication that their machine���s 
���understanding��� ventures into some form of human-level intelligent understanding. We are, after all, 
talking about AI, which to most people really means something like human-level understanding. 
Invariably vendors making claims of the form ���our machine X understands Y��� don���t include any 
definition of understanding. If you hear somebody making such a claim, immediately challenge them 
to define what ���understanding��� means to their machine. The odds are they are trying to smoke you 
and they probably don���t really know. They want you to assume their machine���s ���understanding��� is just 
like yours. 
My opinions about understanding are informally as follows. All designers start with a set of bedrock 
first principles upon which their AI/AGI architecture is based. A first principle can be generally defined 
as a principle that will be universally applicable in their system and is ideally not derived via 
something else. At the outset the designer has a hunch, later a theory, that set X of first principles 
integrated well will result in AGI = Y. 
What counts as a first principle is hotly debated. For an example let us take causality. We don���t need 
to define causality in terms of something else. Aristotle basically started his metaphysics with 
causality. 
Simply put, the AGI must be able to reduce everything it does to its first principles, to explain its 
actions in terms of its first principles, and such principles need to be made explicitly clear at the outset 
to the users. 
So in this very simple example using causality, the machine must provide ALONG WITH ITS OUTPUT 
an explanation that makes clear it ���understands��� in terms of specific, determinate causality in 
whatever case is at hand. 
There are users, developers, and the AGI. All three participants must know what the first principles 
used are, and all explanations provided by the AGI must be ultimately reducible to the first principles, 
eg., ���a caused b.��� Then all three parties know what the machine ���understands��� and what its 
understanding is based upon. 
Of course, under this definition of understanding the resulting ���understanding��� will probably not be 
human-level, but it at least would clarify the bounds of understanding. 
Mike Archbold
1 / 1
ChatGPT
ChatGPT
Thank you for sharing Mike Archbold's perspective on understanding in the context of machines and AI. The concept of understanding, especially in relation to AI claims, is indeed complex and can vary widely depending on the context.
Your idea of defining "understanding" within the framework of an AI or AGI architecture based on a set of first principles is interesting. The notion that a machine's output should include an explanation that ties back to its first principles, making its understanding explicit and understandable to users, is a way to bring clarity to the concept.
This approach emphasizes transparency and accountability in the AI's understanding, allowing users, developers, and the AI itself to have a shared understanding of what constitutes its comprehension. While it might not reach human-level understanding, as you pointed out, it certainly sets a clear framework for assessing understanding within the bounds of the AI's capabilities.
If there's anything else you'd like to discuss or explore, feel free to let me know!
1 / 1
All forms and media of information- incl. language texts, numerical texts, programs, maps etc- 
constitute 
*pictures of the world" 
pictures which are both: 
descriptive- YOU ARE GOING TO THE SUPERMARKET & 
prescriptive- GO TO THE SUPERMARKET 
(n.b. descriptive/prescriptive info have basically same content/picture of world- merely different 
modes). 
To *understand* any information you have to be able to 
translate the pictured world *****INTO THE REAL WORLD*** - either perceive what is described by the picture, in the real world e.g. perceive s.o. going to the 
supermarket in the real world - or achieve what is prescribed by the picture, in the real world e.g. physically go to the supermarket 
To repeat: 
understanding information/ pictured worlds means:being able to translate them into. real worldS 
("translate words into reality") 
Non-understanding/incomprehension (which is NOT studied by either linguistics or cog sci) occurs 
when you 
"don't know what in the world you are talking about" (or "what on earth") 
e.g. a baffling description of quantum particles which you have no means of relating to reality. 
Only an embodied human living in the world can understand or not understand information. Only a 
robot living in the world will be able to understand information in future. 
We can distinguish between *logical understanding" ( being able to translate words/symbols into 
more words/symbols, e.g. dictionary definitions) vs *real understanding* (being able to translate the 
pictured world into the real world). 
Everyone in AGI gets very excited about "understanding" because they are all doing some form of 
Symbolic AI, which can only translate worlds into more words, symbols into more symbols. Their 
systems have no means of connecting information about the world to the real world. They are not 
prepared to contemplate robotic AGI- but there will be no other kind. 
Mike Tintner source email
1 / 1
ChatGPT
ChatGPT
Thank you for sharing Mike Tintner's perspective on understanding and the connection between information, symbols, and the real world. The distinction between "logical understanding" and "real understanding" is a thought-provoking one. The idea that true understanding involves translating the depicted world into the real world, whether through perception or action, emphasizes the importance of grounding information in reality.
Your point about the limitations of symbolic AI in connecting information to the real world is valid. Symbolic AI often deals with representations and manipulation of symbols without necessarily having a direct connection to the physical reality they represent.
As we continue to advance in AI and AGI, it's interesting to consider how we can bridge this gap between symbolic representations and real-world understanding. Embodied AI, as you mentioned, holds promise in this regard, as it can interact with and learn from the real world, enabling a deeper form of comprehension.
If there's anything else you'd like to explore or discuss, feel free to let me know!
1 / 1
Understanding as a __process__ is just the interpretation of particular input (text, drawing, image, and so on). 
Interpretation is totally dependent on previously acquired (or preloaded) knowledge (that is obvious) and on 
mission/motivation/goal (that is less obvious). 
Understanding as a __consequence__/__result__ is an estimation of the usefulness of information extracted in the 
process of interpretation. As with any measure, it is based on some criterion, so the level of understanding depends 
on such criterion. In most cases of text/message interpretation, such criterion is closely related to intentions of 
text/message author; the more difference between message author`s knowledge set and interpreter`s knowledge set, 
the less chance to the identical interpretation of the message by the conversation sides (i.e., less potentially 
understanding level). 
Mykola Rabchevskiy source email 
I should probably put in my two cents on understanding which is similar to Amol's. 
I would say that understanding involves having models that accurately predict the distributions of 
possible presents and futures. For example, let's say you have a ball. If I understand what a ball 
is then I can attend to one part of it, move my eye to another part and predict the distribution of 
shapes and colors my eye will see next. This includes more discrete distributions like the names 
of games that include a ball and their likelihood that they do so. 
Lance Hughes 
source: Slack channel ���IntelligentAgent���
1 / 1
ChatGPT
ChatGPT
Thank you for sharing these perspectives on understanding. It's fascinating to see the different angles from which people approach the concept of understanding.
The first perspective highlights the process of interpretation and how it's influenced by existing knowledge, motivation, and goals. This process of interpretation indeed plays a crucial role in how we understand and make sense of the information around us.
The second perspective introduces the idea of understanding as having accurate predictive models. This notion aligns with the concept of understanding as the ability to predict outcomes and distributions based on existing knowledge and observations. It emphasizes the idea that understanding involves not only recognizing patterns but also being able to anticipate future states based on those patterns.
Both perspectives contribute to the overall understanding of what it means to comprehend information, whether it's through interpretation, prediction, or a combination of both. If you have any more insights or thoughts to share, feel free to continue the conversation!
1 / 1
Understanding doesn���t exist 
Greg Heil source: in person event
1 / 1
ChatGPT
ChatGPT
The statement "Understanding doesn't exist" is an interesting perspective, and it seems to challenge the very concept of understanding itself. It's worth considering that different individuals may have varying viewpoints on abstract concepts like understanding. Some might argue that understanding is a cognitive process that humans and potentially advanced AI systems engage in, while others might take a more philosophical stance on its existence.
Ultimately, discussions around abstract concepts like understanding often delve into philosophical, cognitive science, and AI-related realms, and there's no single universally accepted answer. The viewpoints shared earlier in this conversation showcase the diversity of thought on this topic. If you'd like to explore this idea further or have any specific questions, feel free to share!
1 / 1
I would like to add something about "predictions". I am not quite sure that we make predictions, in fact it is not the right 
term to use. Instead we should talk about "open possibilities" or "expectations". When you try to understand something 
you create a context with some assumptions and what will happen next : 
���Is compatible with the current assumptions (the current context), then you, may be, generate a variation in 
your assumptions (an extension or more constraint or a change, ....). 
���Is not compatible, in that case you detect the error and adapt the context with your assumptions to be 
compatible with what happen before and now or you do not understand, I mean you are not able to create a 
context compatible with your current perceptions and interpretations based on what you know from the 
external world. 
Of course at any time what you perceive is filtered by the context you build that guide your expectations. 
Philippe Delmeire, source email 
NOTE: this is a part of his view on understanding. 
The relation between Truth, Meaning and Understanding is this: fully understanding the meaning of 
some utterance (or expression, or statement) is essentially the same as knowing what the state-of- 
affairs would be like if that utterance was true. 
Walid Saba 
source: https://medium.com/ontologik/on-the-difference-between-recognition-understanding- 
46f20b292ef8

Understanding is an organization of information. The process of forming concepts. 
More whimsically- Understanding is getting beneath something :-) 
Rob Freeman source email
1 / 1
ChatGPT
ChatGPT
Thank you for sharing these insights on understanding and its relationship with concepts, predictions, and context. The perspectives you've mentioned highlight the complexity and multifaceted nature of the concept of understanding. Different individuals may have varying viewpoints on how understanding relates to predictions, context, and the formation of concepts. If you have any more thoughts or questions, feel free to continue the discussion!
1 / 1
A list of all of the main perspectives
1 / 1
ChatGPT
ChatGPT
Certainly! Here's a compilation of the main perspectives on understanding that you've provided:
Valentin Scherbak's Perspective:
Understanding involves a basic level of awareness and consciousness, built through interactions with intelligent systems detecting basic concepts.
Monica Anderson's Perspective:
Understanding results from subconscious learning and intuition, allowing the detection of patterns and mistakes over time.
Brett Martensen's Perspective:
Understanding is the creation of mental models that allow recognition, prediction, reasoning, and explanation of concepts.
Peter Voss's Perspective:
Understanding ranges from basic behavioral responses to higher levels involving personal experience, subject knowledge, and cause-effect mechanisms.
Stephan Verbeeck's Perspective:
Understanding involves the ability to classify, identify, isolate changes, recognize actions and reactions, assign actions to actors, and more within a story.
Daniel Jue's Perspective:
Understanding integrates knowing, thinking, feeling, and intuition, involving a connection to personal experiences and the ability to explain concepts.
Colin Hales's Perspective:
Understanding is the ability to translate information into the real world, perceiving or achieving what is described or prescribed in the information.
Mike Archbold's Perspective:
Understanding involves reducing actions to first principles, explaining actions in terms of those principles, and making those principles explicit to users.
Mike Tintner's Perspective:
Understanding requires translating information into the real world, connecting information to reality, and involving embodied entities like humans or robots.
Amol Kelkar's Perspective:
Understanding is the process of interpretation based on previously acquired knowledge and motivations, leading to low prediction errors.
Melanie Mitchell's Perspective:
Understanding involves abstraction and analogy, allowing the mapping of known concepts to new ones.
Mykola Rabchevskiy's Perspective:
Understanding relies on models that predict the distributions of possible presents and futures, helping in attending to different aspects.
Philippe Delmeire's Perspective:
Understanding creates a context with assumptions that either lead to compatible extensions or detect errors, shaping expectations.
Walid Saba's Perspective:
Understanding the meaning of an utterance is akin to knowing what would be true if that utterance were accurate.
Rob Freeman's Perspective:
Understanding involves organizing information, forming concepts, and gaining insight by "getting beneath" something.
Each perspective brings its unique viewpoint to the concept of understanding, reflecting the diverse ways in which people think about this complex topic.
1 / 1
What are some commonalities and differences?
1 / 1
ChatGPT
ChatGPT
Certainly, here are some commonalities and differences that can be observed among the various perspectives on understanding:
Commonalities:
Interpretation and Context:
Many perspectives highlight the importance of interpretation and context in understanding. Whether it's interpreting information, actions, or concepts, context plays a crucial role.
Knowledge and Learning:
Most perspectives acknowledge the role of knowledge and learning in understanding. Whether it's subconscious learning, building mental models, or acquiring first principles, knowledge is a foundation for understanding.
Prediction and Anticipation:
Several viewpoints mention the idea of prediction, anticipation, or the ability to make accurate estimations about future events based on current understanding.
Integration of Concepts:
Many perspectives emphasize the integration of various concepts, experiences, and information to form a comprehensive understanding.
Connection to Real World:
A common theme is the connection between understanding and the real world. Understanding involves translating information, concepts, or symbols into real-world actions or perceptions.
Differences:
Focus on Emotion:
Some perspectives, like Daniel Jue's, highlight the role of emotion, intuition, and personal experiences in understanding, while others don't explicitly emphasize this aspect.
Comprehension vs. Articulation:
Some perspectives, such as Mike Tintner's, draw a distinction between logical understanding (articulation) and real understanding (translating into the real world).
Symbolic AI vs. Embodied AI:
The distinction between symbolic AI and embodied AI is evident in perspectives like Mike Tintner's and Mykola Rabchevskiy's, with the former focusing more on symbolic representation and the latter emphasizing a connection to the real world.
Causality and First Principles:
Views like Mike Archbold's emphasize the importance of reducing actions to first principles and causality for understanding, while other perspectives may not emphasize this aspect as strongly.
Abstraction and Analogy:
Melanie Mitchell's perspective emphasizes abstraction and analogy as mechanisms of understanding, which is not as prominently featured in other viewpoints.
Relationship to Language:
Some perspectives, such as Philippe Delmeire's, discuss the relationship between understanding and language, while others focus more on the broader concept of understanding.
Minimum Prediction Errors:
The idea of understanding as a state where prediction errors are minimized is present in some viewpoints, like Amol Kelkar's, but not as explicitly in others.
Variability in Definitions:
Perspectives vary in their definitions and criteria for understanding, leading to differences in how they approach the concept.
Overall, while there are common threads that connect the various perspectives, the differences arise from the unique viewpoints, emphases, and backgrounds of the individuals expressing their thoughts on understanding.
1 / 1
AGI ���Understanding��� Survey Report 
Mike Archbold 
Northwest AGI Forum 
October 2021 
SUMMARY 
Use of the word ���understanding��� is common in AI, as in ���our AI understands x.��� 
This type of statement can be both positive and negative, as in ���our AI understands how to x��� 
yet ���our AI does not understand y.��� 
A problem is that it���s quite possible that the claimant of such machine ���understanding��� has no 
rigorous definition of understanding, and perhaps has no definition at all, making it challenging 
for a listener to judge what is being claimed, particularly if no such definition is forthcoming -- 
and it usually isn���t. Making matters worse is that defining human understanding itself is difficult, 
let alone how a machine ���understands���! The result is a confused conflation in need of attention. 
It seems clear that there is no single definition of understanding. A significant problem is that a 
machine���s understanding seems to be crucial, since whether or not any machine purporting to 
have AI can be trusted to complete a given task depends upon its understanding of the task in 
question. 
Given this state of affairs, a series on machine understanding definitions in two parts was 
envisioned by the Northwest AGI Forum. The first part was a survey: members and others in 
the AGI community at large (mostly independent developers) were asked for their definition of 
understanding, and also a few definitions of understanding were gathered from existing sources. 
The results were compiled and published, available here ��� 
https://drive.google.com/file/d/1XfRZudUomqTuuSmMQDBhlcTmZRrezBl2/view?usp=sharing 
The second part of the series on understanding was originally envisioned as a presentation and 
discussion of formal accounts, however due to the paucity of applicable formal material, and the 
fact that such formal accounts are not materially different than the results of the survey 
(although better organized and further developed) it was decided just to integrate the community 
survey along with any formal accounts available, circulate for review, and publish the results in 
this white paper, and reach any such conclusions deemed warranted. 
The following sections discuss 1) the issues that were commonly found in the constellation of 
definitions, 2) a summary of formal accounts, and 3) conclusions.
1 / 1
ChatGPT
ChatGPT
It looks like you've provided an overview of the "AGI Understanding" survey report by Mike Archbold from the Northwest AGI Forum. The report addresses the common usage and lack of rigorous definition of the term "understanding" in the context of artificial intelligence (AI) and artificial general intelligence (AGI). The report aims to clarify the concept of machine understanding by gathering various definitions from the AGI community and examining formal accounts. The report is structured into sections discussing issues with definitions, summarizing formal accounts, and drawing conclusions. It emphasizes the importance of understanding in evaluating AI and AGI capabilities. If you have any specific questions or if there's something else you'd like to discuss about the content of the report, feel free to let me know!
1 / 1
ISSUES 
The results: 
There is no universally accepted definition of understanding. 
Certain issues, some of which are described below, seem to appear 
repeatedly. 
1) LEVELS 
Most people seem to agree that there are grades of understanding, from the simplest, 
eg., prediction, to the higher levels, eg., the ability to explain in detail. This ���levels��� issue 
is arguably the most widely held, seemingly implicit therefore in our understanding of 
understanding. This is mostly what makes claims of AI ���understanding��� inherently 
troublesome, since it is usually not clear what level(s) is defined and claimed. If you get 
nothing else from reading this paper, remember it���s best to try to qualify what you mean 
by understanding -- that is basically the same as identifying its level. Otherwise you are 
vague. 
2) COMPRESSION, RECOGNITION, AND PREDICTION BASED ON PATTERNS 
Proponents of the algorithmic information theory school of intelligence (AIT) and 
proponents of the explosion of what can be labeled generally as ���patternism��� within the 
last ten years tend to favor a minimalist definition of understanding. They limit machine 
understanding within the boundaries of the facility of pattern recognition. Advocates 
basically claim that mere prediction alone, along with the ability to act appropriately given 
correct recognition, is sufficient to claim adequate machine understanding. 
3) EXPLANATION 
Most people seem to require that a machine can be said to understand if it possesses 
some means of explanation. 
4) UNDERSTANDING (or not) WITHOUT BEING ABLE TO EXPLAIN 
Whereas explanation is a sign of machine understanding, humans are often able to act 
without actually understanding. As in drama, they may follow a script without knowledge 
of the underlying causes. Secondly, we may be able to understand performing activities 
but simply lack a means of adequate explanation to others. It seems then that the 
explanation ability is not always needed. 
5) SENSORY INPUT CRUCIAL OR NOT? 
Some argue that without actually being embedded in the world it is impossible to 
understand it. For example, the world of text alone would never pass for sensing the 
human-experienced world of sights, sounds, tastes. Others argue from a ���brain in a vat��� 
standpoint, that understanding can be achieved anyway, eg., by text alone. 
6) CAPACITY TO APPLY UNDERSTOOD KNOWLEDGE TO REACH GOALS 
Most seem to agree that a machine can be said to understand if it can apply its 
knowledge to reach its goals. Mere knowledge alone is worthless, then, if it isn���t actually 
understood. Knowledge can also be aligned with common sense. Common sense 
without the understanding of it is worthless. In simple terms there is 1) the data the 
machine has and 2) its ability to use the data, ie., to understand it. The true test of 
understanding is whether or not new goals can be met using understood, existing 
knowledge. 
7) UNDERSTANDING REQUIRES A MODEL 
Most people seem to argue that a machine requires an inner model that effectively 
represents what is the case outside of it in order to understand. The internal model 
should capture the structure of what it represents, eg: relationships, causes, spatial and 
temporal properties. The model might be a fairly static world-level view, or it may be 
generated dynamically, applicable to a specific domain. 
8) UNDERSTANDING AS A PROCESS REACHING A STATE 
Some think that understanding is a constant process proceeding in time. The machine 
attempts to understand by harmonizing its inner model with the state of the external 
world, with the problem at hand. An ���aha��� effect is possible. Truth is implicated: the 
machine can decide truth or falsity if the inner model and outer circumstances align. If 
the inner and outer don���t harmonize well enough, then the machine ���doesn���t 
understand.���
1 / 1
ChatGPT
ChatGPT
The "AGI Understanding" survey report outlines several common issues that arise when attempting to define the concept of understanding in the context of artificial general intelligence (AGI). Here are the summarized issues discussed in the report:
Levels of Understanding: There is a consensus that understanding has different levels, ranging from prediction to detailed explanation. Defining the level of understanding is crucial in assessing AI's capabilities.
Compression, Recognition, and Prediction: Some proponents argue that understanding is limited to pattern recognition, prediction, and appropriate action based on recognized patterns. This minimalist view of understanding is associated with algorithmic information theory and patternism.
Explanation: Many people require a machine to have the ability to provide explanations as a sign of understanding. Explanation is seen as an important aspect of understanding.
Understanding Without Explanation: Humans can sometimes act without full understanding or without being able to explain their actions. This raises the question of whether explanation is always necessary for understanding.
Sensory Input and Embodiment: There's a debate about whether sensory input and embodiment in the real world are crucial for understanding. Some argue that understanding requires direct interaction with the world, while others suggest that it can be achieved through text-based information alone.
Application of Knowledge: Understanding is often linked to the machine's ability to apply its knowledge to achieve goals. Mere knowledge without the ability to apply it or align it with common sense is considered insufficient for understanding.
Requirement for an Inner Model: Many believe that understanding necessitates an internal model that accurately represents the external world, capturing relationships, causes, spatial and temporal properties. This model might be static or dynamically generated.
Understanding as a Process: Some view understanding as an ongoing process that involves harmonizing the machine's internal model with the external world's state. The "aha" effect can occur when the model aligns with external circumstances, leading to decisions about truth or falsity.
These are the main issues discussed in the report, highlighting the complexities and diverse perspectives on the concept of understanding in the context of AI and AGI.
1 / 1
9) CAUSE AND EFFECT 
The sensitivity to causality in a machine is one of the most cited signs of understanding. 
To a great extent this seems tied to the current frustration in that AI is often limited to 
mere correlation. Most people seem to think that the ability to distinguish cause from 
correlation is needed in understanding. (Lately there appears to be increased interest in 
modelling causality.) 
10)REPHRASING, RECREATING, TEACHING, etc 
Some claim that a test of understanding is the ability to apply knowledge in sophisticated 
ways, such as having the ability to use what is understood to recreate, teach, apply, 
manage, modify etc. 
11) RECOGNIZING WHAT IS RELEVANT AND USEFUL IN CONTEXT TO REACH 
GOALS/MISSION/MOTIVATION 
Some claim that it takes an ���I��� to ���understand.��� This seems to follow from common sense 
since humans have a mental structure which so heavily is dependent upon the self. The 
self experiences motivations and pursues its good and avoids the bad. Understanding in 
this sense means that the I/self needs to ascertain what is important in context to reach 
goals. 
12)UNDERSTANDING INCLUDES A FEELING 
Some claim that understanding is ineffable and will not yield easily to definitions and 
implementation in a machine, if at all. We also have feelings about feelings and other 
complexities related to understanding that appear difficult to mimic. 
13) NIHILISTS 
Some believe there is no such thing as understanding at all, no ���I��� in a human nor 
machine, and variations of this view. This might be a sensible approach, since it���s a valid 
question how a machine could actually understand anything ever: it���s just a machine 
after all.
1 / 1
ChatGPT
ChatGPT
Here are the additional issues discussed in the "AGI Understanding" survey report:
Cause and Effect: Many consider the ability to understand causality as a crucial aspect of understanding. This sensitivity to cause and effect distinguishes understanding from mere correlation and is seen as a sign of advanced understanding. There is increasing interest in modeling causality in AI.
Rephrasing, Recreating, Teaching, etc.: Some define understanding based on the machine's ability to apply knowledge in sophisticated ways, including rephrasing, recreating, teaching, managing, modifying, and more. This advanced application of knowledge is considered a marker of understanding.
Recognizing Relevance and Context: Understanding is often associated with the ability to recognize what is relevant and useful in a given context to achieve goals, missions, or motivations. This aspect of understanding requires the capacity to assess and prioritize information based on its importance.
Understanding Includes a Feeling: Some argue that understanding goes beyond intellectual comprehension and includes a certain "feeling" or depth that may be difficult to define or replicate in a machine. This emotional and experiential aspect of understanding adds complexity to the concept.
Nihilists: There are those who hold the view that true understanding doesn't exist, whether in humans or machines. This perspective questions the very possibility of machines achieving understanding due to their inherent nature.
These additional issues further illustrate the multifaceted nature of understanding in the context of AI and AGI, reflecting the diverse viewpoints and challenges associated with defining and implementing understanding in machines.
1 / 1
FORMAL TREATMENT OF UNDERSTANDING 
The works of Kristinn Thorisson et al are most applicable, easily readable, and recommended. 
They (Thorisson and colleagues) repeatedly cite four levels in order of difficulty as a measure of 
understanding: 
1) prediction (crude understanding), 
2) utilization of understanding to reach goals, 
3) explanation ability, 
4) ability to recreate. 
In the paper titled ���About Understanding��� (2016) they state ���no concrete theory of understanding 
has been fielded as of yet in artificial intelligence (AI), and references on this subject are far 
from abundant in the research literature.��� They write: 
While understanding as a phenomenon has received more attention in the philo- 
sophical than the AI literature [8,7], even there it has nevertheless been claimed to 
have ���virtually escaped investigation in English-speaking philosophy��� ([5]: 307); this 
dearth of interest in the subject is evidenced not only there but also in the fields of AI and 
cognitive science.4 A few books have been published with the word pair ���understanding 
understanding��� in the title [15,4]. Interesting as they may be, one of these contains selected 
writings by cybernetics pioneer Heinz von Forrester, which, in spite of its promising title, is not 
about understanding at all (as evidenced by the word ���understanding��� not appearing the index); 
the other gives a cursory (albeit a decent) summary of the subject in the context of 
epistemological philosophy. 
From their paper ���Do Machines Understand? A Short Review of Understanding & Common 
Sense��� (emphasis mine): 
(in AI) ���For the most part, understanding has simply been ignored, or discussed in a very 
specific sense within particular domains. If mentioned at all in the AI literature, understanding is 
generally discussed within the context of one specific domain, such as language or scene 
understanding, or equated with common sense.��� 
In spite of this state of neglect they nevertheless regard understanding as crucial (from 
���Evaluating Understanding��� emphasis mine): 
While results in automation can be achieved without it, we argue that understanding is 
especially important for general-purpose systems. It is doubtful that we could classify systems 
as having general intelligence if they don���t really understand their tasks, environment, and 
world, and thus it is important for us to verify the level of understanding of any system intended 
to strive for generality and autonomy. 
Thorisson et al���s goal (as gathered from his papers and talks) is to present a useful and 
pragmatic theory of understanding. 
Basically, his theory rests on the accuracy of the relationship a machine can build between the 
phenomena it experiences and its internal models. The more a machine���s models are complete 
and accurate with respect to its phenomena and context (basically the ~world) the more a 
machine ���understands.��� He also proposes a means of formalizing meaning given context: 
depending upon context something may be more or less meaningful to the machine. I refer the 
reader to these papers for more details.
1 / 1
ChatGPT
ChatGPT
The formal treatment of understanding, as discussed by Kristinn Thorisson and colleagues, emphasizes several key aspects:
Levels of Understanding: Thorisson's work proposes four levels of understanding, progressing from simpler to more complex:
Prediction: Basic comprehension that enables crude understanding.
Utilization: Ability to apply understanding to achieve goals.
Explanation: Capacity to provide explanations for observed phenomena.
Recreation: Capability to recreate or simulate scenarios based on understanding.
Lack of Attention: Understanding has often been neglected or discussed within specific domains in AI and cognitive science literature. It's frequently overlooked or equated with common sense.
Importance for General-Purpose Systems: Thorisson argues that understanding is crucial, especially for general-purpose AI systems. True general intelligence requires an understanding of tasks, environment, and the world.
Relationship between Models and Phenomena: A machine's level of understanding is determined by the accuracy of its internal models in representing the phenomena it encounters. The closer the alignment between models and the observed world, the higher the level of understanding.
Formalizing Meaning in Context: Thorisson's theory includes a formal approach to meaning, accounting for context. Depending on the context, certain information may have varying degrees of meaning to the machine.
Thorisson's work aims to develop a pragmatic and useful theory of understanding that can help assess the level of understanding in AI systems. His formalization of understanding incorporates the relationship between models and phenomena, the ability to apply understanding to tasks, and the context-dependent nature of meaning. For more details, readers are encouraged to refer to Thorisson's papers and discussions on this topic.
1 / 1
CONCLUSIONS 
I recommend not saying things like ���my AI understands X��� without qualification. It���s probably 
best to follow a variable form like this: 
���My AI understands X <specify one of the levels listed above> but cannot understand Y 
<specify the levels above not applicable>��� 
For example: developer Z should say ���my AI can understand how to predict stock price changes 
but the AI cannot explain its reasoning��� rather than a grandiose claim like ���my AI understands 
the stock market.��� REFERENCES 
Archive of the ���Understanding Survey��� event ��� 
https://drive.google.com/drive/folders/1_96Noo_s4BpES2K88etHpY4-aAqEusgt?usp=sharing 
Thorisson et al papers ��� http://alumni.media.mit.edu/~kris/select_publ.html 
Video of event https://www.youtube.com/watch?v=MbDCqQ0oFyY
1 / 1
ChatGPT
ChatGPT
In conclusion, the survey and discussions around the concept of "understanding" in the context of AI have highlighted various perspectives and issues. There is no universally accepted definition of understanding, and different viewpoints have emerged from the community. Some common themes and differences include the levels of understanding, the role of explanation and prediction, the importance of causality, and the need for an inner model.
The formal treatment of understanding, particularly as presented by Kristinn Thorisson and colleagues, has provided valuable insights. The proposed levels of understanding, ranging from prediction to recreation, offer a structured way to assess the depth of understanding in AI systems. The alignment between internal models and observed phenomena, as well as the context-dependent nature of meaning, are crucial factors in determining a machine's level of understanding.
To address the lack of a universal definition and the potential for ambiguity, it is recommended to use qualification when making claims about AI understanding. Developers should specify the level of understanding achieved by their AI for a particular task, rather than making broad statements without context. This approach promotes clarity and avoids misleading interpretations of AI capabilities.
For those interested in delving deeper into the topic, the references provided include the archive of the "Understanding Survey" event, relevant papers by Kristinn Thorisson and his colleagues, and a video of the event for further exploration.
1 / 1
See if you can categorize these  types of understanding.
1 / 1
ChatGPT
ChatGPT
Certainly, the various perspectives and definitions of "understanding" can be categorized into several types based on the common themes and aspects mentioned in the discussions:
Levels of Understanding
:
Basic Prediction: Understanding as the ability to predict future events or outcomes based on patterns and data.
Utilization: Understanding involves using acquired knowledge to achieve specific goals or tasks.
Explanation: Understanding entails the capability to provide coherent explanations for phenomena or concepts.
Recreation: Understanding extends to the capacity to recreate, replicate, or simulate scenarios based on learned information.
Pattern Recognition and Comprehension
:
Pattern-Based: Understanding is linked to recognizing and interpreting patterns in data, enabling accurate predictions.
Comprehension: Understanding involves grasping the meaning and significance of information, allowing for meaningful action.
Causality and Context
:
Causal Insight: Understanding encompasses the ability to discern causal relationships and differentiate between cause and correlation.
Contextual Relevance: Understanding considers the context in which information is situated, determining its relevance and meaning.
Explanatory Capacity
:
Explanation Generation: Understanding includes the capability to articulate coherent explanations for phenomena, decisions, or actions.
Goal-Directed and Utilitarian Understanding
:
Goal-Oriented: Understanding involves aligning information with goals and objectives to make informed decisions.
Utilitarian: Understanding is measured by the practical application of knowledge to solve problems or accomplish tasks.
Inner Models and Representation
:
Inner Models: Understanding relies on the construction and refinement of internal models that represent the external world.
Representational Accuracy: Understanding is determined by the accuracy of internal models in capturing the structure of reality.
Human-Like Understanding
:
Human Parallels: Understanding aims to achieve a level of comprehension akin to human cognitive processes.
Subjective Elements: Understanding may involve subjective experiences, emotions, and feelings similar to human understanding.
Sensory Experience and Interaction
:
Sensory Input: Understanding requires sensory experiences and interactions with the external environment.
Embodied Understanding: Understanding is rooted in the physical interactions and experiences of a machine within its environment.
Information Compression and Distillation
:
Information Reduction: Understanding involves distilling complex information into concise representations.
Knowledge Compression: Understanding includes the ability to capture essential knowledge in compact forms.
Meaning and Relevance
:
Contextual Meaning: Understanding involves deriving meaning from information based on the context in which it is presented.
Relevance Assessment: Understanding includes discerning the significance and applicability of information in specific situations.
These categories capture the key aspects and dimensions of "understanding" as discussed in the various perspectives. It's important to note that these categories are not mutually exclusive, and different definitions of understanding may overlap or incorporate multiple elements from these categories.
1 / 1
Monica Anderson uses the definition "autonomous epistemic reduction" for understanding.
1 / 1
ChatGPT
ChatGPT
"Autonomous epistemic reduction" is an intriguing and specific term used by Monica Anderson to define understanding. This term appears to capture the idea of reducing knowledge or information in a self-contained manner. The term can be dissected into two key components:
Autonomous
: This suggests that the process of understanding occurs independently, without the need for external guidance or intervention. The understanding is self-driven and self-sustained.
Epistemic Reduction
: "Epistemic" pertains to knowledge or beliefs. "Reduction" in this context likely refers to simplifying, condensing, or distilling complex knowledge or information into more manageable or core components.
Putting these components together, "autonomous epistemic reduction" suggests that understanding involves the self-directed process of simplifying or distilling knowledge independently, without external influence.
It's important to note that this term, while specific and intriguing, may require further context or explanation to fully grasp its implications and intricacies within the broader context of discussions about understanding.
1 / 1
Yeah so I would say that a sieve that filters out pebbles or grains above a certain size is doing something like this.
1 / 1
ChatGPT
ChatGPT
Indeed, your analogy of a sieve filtering out pebbles or grains above a certain size is a clever way to illustrate the concept of "autonomous epistemic reduction" in a tangible context. Just as the sieve autonomously reduces the material it allows through by setting a threshold size, understanding could involve a process where information or knowledge is autonomously distilled or simplified based on certain criteria or rules.
In this analogy, the sieve's mechanism serves as an autonomous agent that reduces the complexity of the material passing through it, and similarly, understanding might involve a cognitive process that autonomously reduces the complexity of information or knowledge, extracting key elements that align with a certain criterion.
This comparison helps to make the concept more concrete and relatable, highlighting how "autonomous epistemic reduction" could work as a mechanism for understanding.
1 / 1
Explain how "grokking" is an unreasonably high expectation.
1 / 1
ChatGPT
ChatGPT
"Grokking" is a term popularized by science fiction author Robert A. Heinlein in his novel "Stranger in a Strange Land." It conveys a deep and complete understanding of something, to the point of becoming one with it. While the idea of grokking is appealing, it can be considered an unreasonably high expectation, especially when applied to machines or artificial intelligence (AI).
Here are a few reasons why "grokking" is challenging and often unattainable for machines:
Complexity of Human Experience: Grokking implies a profound understanding that goes beyond surface knowledge. Human experience is rich with emotions, intuition, cultural context, and personal history. Machines lack the biological and emotional elements that contribute to the depth of human understanding.
Subjectivity: Understanding often involves subjective interpretation. Humans can empathize, infer underlying motives, and perceive nuance in ways that machines struggle to replicate. Machines rely on algorithms and data patterns, which can be limiting in capturing the breadth of human interpretation.
Contextual Awareness: Grokking necessitates understanding the broader context of a situation, which can include cultural, historical, and emotional dimensions. Machines may lack the ability to fully grasp these contextual factors and apply them to their understanding.
Lack of Embodiment: Human understanding is often tied to embodied experiences. Our physical interactions with the world contribute to our comprehension of it. Machines lack this physical presence, making it challenging to emulate the depth of understanding that humans achieve.
Intuitive Insight: Grokking implies an intuitive insight that can't always be explicitly explained. Machines excel at processing data logically, but they struggle with intuitive leaps and insights that humans can make.
Ethical and Moral Dimensions: Understanding can involve complex ethical and moral considerations. Machines may be able to process information, but they lack the moral compass and personal experiences that shape human ethical understanding.
While AI and machines can process vast amounts of data and perform tasks that appear to demonstrate understanding, these systems are fundamentally different from human beings. The concept of grokking involves a level of depth, intuition, and embodiment that is challenging to replicate in a machine, making it an unreasonably high expectation for AI to achieve.
1 / 1
I think it is unreasonable for humans too. We can't "become one" with systematic problems above a certain threshold of complexity because of cognitive load and perceptual limits. The best we can hope for is "seeing through a glass darkly" as suggested by quantum Bayesianism.
1 / 1
ChatGPT
ChatGPT
You bring up an important point. Indeed, the idea of fully "becoming one" with or deeply understanding complex and intricate systems can be challenging for both humans and machines. Cognitive limitations, perceptual constraints, and the inherent complexity of certain problems can hinder our ability to achieve complete understanding.
In the realm of both human and machine cognition, there are practical limitations that prevent us from grasping all the intricate details of highly complex systems. Whether it's due to the sheer volume of information, the limitations of our working memory, or the intricacies of the problems themselves, achieving a state of all-encompassing understanding is often unattainable.
The concept of "seeing through a glass darkly" is a recognition of the limitations of human perception and cognition. It suggests that our understanding of complex phenomena is limited and obscured, much like trying to see clearly through a foggy or unclear surface. Quantum Bayesianism, which relates to the interpretation of quantum mechanics, acknowledges the role of uncertainty and limitations in our understanding of the quantum world.
Similarly, both humans and machines face inherent limitations when attempting to understand complex systems. As the complexity of a problem increases, our ability to fully comprehend all its aspects diminishes. Instead, we often strive for partial understanding, incremental insights, and the ability to make informed decisions based on the information available to us.
Recognizing and respecting these limitations can lead to more realistic expectations for both human and machine understanding. It encourages us to approach complex problems with humility, open-mindedness, and a willingness to work collaboratively to gain insights from different perspectives. While "grokking" complex systems might remain an ideal, acknowledging the challenges and limitations can lead to more effective problem-solving and decision-making.
1 / 1
Talk about the Overton window of complexity.
1 / 1
ChatGPT
ChatGPT
The Overton window is a concept that originates from the realm of political discourse and refers to the range of ideas that are considered acceptable and mainstream within a society at a given point in time. It represents the range of policies or opinions that are considered feasible and within the boundaries of public opinion. Ideas or policies that fall outside the Overton window are typically considered extreme or radical.
In the context of complexity, the concept of the "Overton window of complexity" can be used metaphorically to describe the range of problems or phenomena that are considered understandable and manageable by individuals, groups, or societies. This concept suggests that there is a limit to the level of complexity that people can comfortably comprehend and address within a given context. Problems or systems that exceed this window of complexity may be perceived as too intricate, convoluted, or overwhelming to fully understand or address effectively.
The Overton window of complexity can vary depending on factors such as individuals' cognitive abilities, educational background, familiarity with the subject matter, and the available tools and resources. For instance, a person with expertise in a particular field may have a wider Overton window for complex issues within that domain compared to someone without that expertise.
As complexity increases, individuals and societies may encounter challenges in understanding, addressing, and effectively responding to certain issues. These challenges may include information overload, difficulty in discerning relevant information from noise, and limitations in cognitive processing. Moreover, when faced with problems that exceed their Overton window of complexity, people might resort to oversimplification, relying on heuristics, or deferring to experts or authorities.
The concept of the Overton window of complexity highlights the importance of effective communication, education, and collaboration. Experts and communicators need to find ways to bridge the gap between the complexity of a topic and the level of understanding of their intended audience. This may involve breaking down complex concepts, providing relatable examples, and using visual aids or analogies to facilitate comprehension.
In a rapidly changing world with increasingly complex challenges, individuals and societies need to continually adapt and expand their Overton window of complexity. This can be achieved through lifelong learning, exposure to new ideas, and fostering a culture of critical thinking and curiosity.
1 / 1
Do you know De Agricultura by Cato the Elder?
2 / 2
ChatGPT
ChatGPT
Yes, "De Agricultura" (translated as "On Agriculture") is a Latin work written by Marcus Porcius Cato, also known as Cato the Elder, in the 2nd century BC. It is one of the earliest known pieces of agricultural literature in Latin and provides insights into farming practices, rural life, and agricultural management during the Roman Republic. The work is a practical manual that offers advice and instructions on various aspects of agriculture, including planting, cultivating, and harvesting crops, as well as managing livestock.
"De Agricultura" is divided into sections or chapters, each focusing on a different topic related to agriculture. Cato provides detailed instructions on how to grow and care for various crops such as grains, vines, olives, and vegetables. He also covers topics like managing farm labor, irrigation, animal husbandry, and the use of agricultural tools.
In addition to its agricultural content, the text also includes Cato's personal anecdotes, opinions, and reflections on the virtues of rural life and the importance of frugality and self-sufficiency. The work reflects the values and mindset of the Roman aristocracy of the time, emphasizing the importance of agricultural productivity and self-reliance.
"De Agricultura" is not only valuable for its agricultural advice but also provides historical insights into the practices and mindset of Roman society during the late Republic period. The text has survived through the centuries and continues to be studied by historians, agricultural experts, and those interested in ancient Roman life and literature.
1 / 1
See if you can find similarities with the games Age of Empies and the Sims
1 / 1
ChatGPT
ChatGPT
"Age of Empires" and "The Sims" are two popular video game series that offer distinct gameplay experiences, but they do share some thematic and strategic similarities.
Resource Management:
Both games involve resource management, though in different ways. In "Age of Empires," players gather resources like wood, food, gold, and stone to build civilizations, armies, and structures. Similarly, in "The Sims," players manage resources like simoleons (money), social relationships, and energy to maintain their virtual households.
Building and Construction:
Both games allow players to design and construct various buildings and structures. In "Age of Empires," buildings are essential for economic development, military production, and research. In "The Sims," players build and furnish houses to accommodate the needs and preferences of their virtual characters.
Decision-Making and Strategy:
Both games require players to make decisions that influence the outcome of the gameplay. In "Age of Empires," players need to strategize their military campaigns, manage their economy, and make diplomatic decisions. In "The Sims," players make decisions about the daily lives of their virtual characters, including their careers, relationships, and leisure activities.
Progression and Advancement:
Both games feature progression and advancement systems. In "Age of Empires," players advance through historical ages, unlocking new technologies and units. In "The Sims," characters can progress in their careers, improve skills, and achieve life goals.
Simulation of Life:
While "Age of Empires" focuses on historical and military simulation, "The Sims" is a life simulation game. Both games simulate aspects of human life, whether it's building civilizations and leading armies or managing the lives of virtual characters.
Customization:
Both games offer a degree of customization. In "Age of Empires," players can choose their civilization, strategy, and approach to warfare. In "The Sims," players customize the appearance, personality, and life goals of their virtual characters, as well as the design of their homes.
Long-Term Gameplay:
Both games can be played over extended periods. In "Age of Empires," players may engage in lengthy campaigns or multiplayer matches. In "The Sims," players can follow their characters' lives for generations, witnessing their achievements and relationships.
Simulation of Societal Dynamics:
Both games incorporate elements of societal dynamics. In "Age of Empires," players experience the rise and fall of civilizations, reflecting historical changes. In "The Sims," players simulate the dynamics of households, social interactions, and community life.
Despite these similarities, it's important to note that the gameplay mechanics, goals, and overall experiences of "Age of Empires" and "The Sims" differ significantly due to their distinct genres and themes. "Age of Empires" is primarily a real-time strategy game with a focus on historical warfare, while "The Sims" is a life simulation game centered around personal and interpersonal experiences.
1 / 1
I meant with De Agricultura
1 / 1
ChatGPT
ChatGPT
"De Agricultura," written by Cato the Elder in the 2nd century BC, is a Roman agricultural manual that provides advice and guidelines for managing a farm and various aspects of agriculture. While "De Agricultura" and video games like "Age of Empires" and "The Sims" are from vastly different time periods and mediums, there are some thematic connections that can be drawn:
Resource Management:
Just like in "Age of Empires" and "The Sims," "De Agricultura" emphasizes the management of resources. Cato's manual provides detailed instructions on soil preparation, planting, harvesting, and animal husbandry, all of which require careful management of resources like land, labor, and tools.
Cultivation and Construction:
Similar to how players build and construct structures in games, Cato's manual covers the construction of various agricultural facilities and tools, such as plows, barns, and storage areas. The manual emphasizes the importance of efficient and practical construction for a successful farm.
Decision-Making and Strategy:
Like the strategic decision-making in video games, "De Agricultura" requires farmers to make informed decisions about which crops to plant, when to sow and harvest, and how to manage livestock. These decisions impact the success of the farm and the overall livelihood of the farmer.
Progression and Expertise:
Just as characters in video games advance in skills and abilities, "De Agricultura" emphasizes the progression of a farmer's expertise over time. Cato's manual provides advice for farmers at different stages of experience, from beginners to those with more advanced knowledge.
Simulation of Rural Life:
While "De Agricultura" is not a simulation game like "The Sims," it does offer a glimpse into the daily life of ancient Roman farmers. The manual discusses the routines, practices, and challenges faced by farmers in the context of rural life.
Societal Dynamics:
Both "De Agricultura" and the mentioned video games touch on aspects of societal dynamics. "De Agricultura" discusses the role of farmers in supporting the larger Roman society through food production, while the games simulate societal and community interactions within their respective contexts.
Overall, while "De Agricultura" and video games like "Age of Empires" and "The Sims" are distinct in terms of their format and purpose, they share common themes related to resource management, decision-making, skill progression, and the simulation of aspects of human life. These connections highlight the enduring relevance of agriculture and resource management throughout history and across different forms of media.
1 / 1
I think he said to get rid of the lazy servants and promote the ones that excel at leadership and economics
1 / 1
ChatGPT
ChatGPT
You're correct. Cato the Elder, in "De Agricultura," did indeed emphasize the importance of efficient management and leadership on the farm. He advised landowners to promote capable and diligent overseers who had strong leadership and economic skills to manage the various aspects of agricultural operations.
Cato's advice aligns with the idea of promoting individuals who excel at leadership and economics, as they would be better equipped to make informed decisions, allocate resources effectively, and ensure the success of the farm. By advocating for the removal of lazy servants and the promotion of capable ones, Cato aimed to enhance the productivity and profitability of the agricultural enterprise.
This emphasis on leadership and economic acumen reflects the broader theme of responsible and strategic management that is present in "De Agricultura," highlighting the importance of competent leadership in achieving success in farming and agricultural endeavors.
1 / 1
I was thinking that Cato did not really know how run a farm fully but expected managers to run the various aspects such as the olive presses, the tool shops, the kitchens, the fruit pickers, and the animal caretakers.
1 / 1
ChatGPT
ChatGPT
You're absolutely correct. Cato the Elder's "De Agricultura" provides insights into his recommendations for managing various aspects of a farm, even though he might not have had firsthand experience with all these operations himself. He believed that a skilled manager with a good understanding of different agricultural tasks could effectively oversee the different areas of the farm, even if they personally lacked expertise in each individual task.
Cato's advice was practical and encompassed a wide range of topics, including olive presses, tool shops, kitchens, fruit picking, animal care, and more. He emphasized the importance of proper oversight and coordination among these activities to ensure the overall success of the farm. While Cato might not have possessed in-depth knowledge of every single task, he recognized the significance of effective management and delegation to ensure the farm's productivity and profitability.
In a way, Cato's approach aligns with the idea of specialized management, where skilled individuals are assigned to specific tasks or areas based on their expertise. This approach allows for the efficient allocation of resources and the optimization of various operations within the agricultural context.
1 / 1
Monica's Leaking Chatroom (based on Monica Anderson's ideas) theory is like having a big room in your brain where lots of different people and machines are talking and sharing ideas. 

Sometimes the ideas leak out and go to other rooms, which helps us make better decisions.

Reed Wall Mind is like having a big wall in your brain that helps you keep track of everything that's happening in the room, like a big calendar or to-do list.

Motile Womb theory is like being in a big moving house or spaceship, where you can feel different movements and use them to imagine different things in your head.

All these theories help us understand how our brain works and how we can use it to think and make decisions better.


Monica's Leaking Chatroom theory posits that the mind functions as a collection of independent, modular decision-making units, or "bubbles," that act as rational processors of information.

These bubbles, or "chambers," are able to filter information and pass summaries of that information upward to higher-level decision-making centers, much like a wisdom salon or a world cafe protocol.

This creates a multi-scale decision-making process that is able to take into account a wide range of information and perspectives.

Reed Wall Mind theory builds on this concept by drawing inspiration from the Babylonian Flood Story and positing that the mind is like a "living spaceship" that is able to navigate a complex and dynamic environment.

This is accomplished through the use of a wide range of cognitive skills, including those learned through playing video games such as Stars!, Descent, Age of Empires, Commander Keen, Minesweeper, and Tetris, as well as through the use of other forms of sensory input such as hearing different voices and machine sounds, and feeling different inertial forces.

Finally, Motile Womb theory proposes that the mind is not just a passive receiver of information but an active participant in the process of shaping and interpreting the world.

This theory posits that individuals are born with a wide range of cognitive abilities and that these abilities are honed and refined through the process of experiencing the world.

The theory suggests that playing games like the ones mentioned above, as well as other forms of interactive experiences, help individuals to discover and develop the cognitive abilities they already possess.
1 / 1
ChatGPT
ChatGPT
It seems like you've provided a description of different theories that metaphorically explain how the brain works and processes information. These theories, such as Monica's Leaking Chatroom theory, Reed Wall Mind theory, and Motile Womb theory, use creative and imaginative language to conceptualize various aspects of cognitive processes. They suggest that the mind operates as a collection of interconnected modules or units that interact to process information and make decisions.
In Monica's Leaking Chatroom theory, the idea of modular decision-making units and the sharing of information among them is used to explain how the brain filters and processes different perspectives to reach decisions. Reed Wall Mind theory uses the metaphor of a "living spaceship" to illustrate the mind's ability to navigate a complex environment, drawing on various cognitive skills learned through experiences like playing video games.
Motile Womb theory extends this understanding by proposing that the mind actively participates in shaping and interpreting the world, and that cognitive abilities are honed through interaction with the environment. The mentioned games and interactive experiences are used to illustrate how individuals can develop and refine their cognitive abilities.
Overall, these theories provide intriguing perspectives on how the brain functions, using metaphors and creative concepts to make complex cognitive processes more understandable and relatable.
1 / 1
Connections to the previous topics.
1 / 1
ChatGPT
ChatGPT
The theories you mentioned ��� Monica's Leaking Chatroom theory, Reed Wall Mind theory, and Motile Womb theory ��� offer unique viewpoints on how the brain processes information and makes decisions. These perspectives are reminiscent of the discussions we had earlier on the different definitions of understanding and the challenges associated with grasping complex concepts.
Definition of Understanding:
Just as we explored various definitions of understanding, these theories provide alternative conceptualizations of how the mind comprehends information. They offer metaphorical explanations for how information is processed, shared, and integrated, which can be seen as an attempt to capture the essence of "understanding" in different ways.
Complexity and Cognitive Load:
These theories reflect the recognition that cognitive processes, such as understanding and decision-making, can become complex and challenging. The idea of modular decision-making units and the need for efficient processing in Monica's Leaking Chatroom theory might resonate with the discussions about the cognitive load and limitations faced by both humans and machines.
Metaphorical Understanding:
Just as the different definitions of understanding we explored earlier were often expressed metaphorically, these theories also utilize metaphors to describe cognitive processes. This highlights the role of analogy and creative thinking in attempting to grasp complex concepts, which is akin to the discussions on "grokking" and the Overton window of complexity.
Diverse Perspectives:
The notion of different "chambers" or "bubbles" in Monica's Leaking Chatroom theory aligns with the idea of diverse perspectives discussed earlier. These theories suggest that the mind processes information from various sources and perspectives, similar to how the discussion encompassed diverse viewpoints on the concept of understanding.
Interactivity and Cognitive Development:
Motile Womb theory's emphasis on interactive experiences echoes the significance of learning through interaction and engagement, which was relevant when discussing cognitive development, perception, and the influence of experiences on shaping understanding.
In summary, these theories on how the mind operates and comprehends information connect to the broader discussions on the nature of understanding, complexity, cognition, and the challenges inherent in describing these intricate processes. They offer innovative ways to think about how understanding and decision-making might occur within the human mind.
1 / 1
See if you can give some hypotheses predicted by each theory.
1 / 1
ChatGPT
ChatGPT
Certainly, here are some hypothetical predictions that could be derived from each of the theories:
Monica's Leaking Chatroom Theory:
Information Filtering Hypothesis:
The theory suggests that the mind functions as a collection of independent decision-making units that filter information. A prediction could be that people are more likely to remember and process information that aligns with their existing beliefs and cognitive filters.
Multiscale Decision-Making Hypothesis:
If the mind operates using a multi-scale decision-making process, we might predict that individuals would be better at making decisions when they consider a wide range of information and perspectives rather than relying solely on a narrow viewpoint.
Reed Wall Mind Theory:
Cognitive Skills Enhancement Hypothesis:
If the mind can navigate a dynamic environment by utilizing various cognitive skills, a prediction could be that individuals who engage in a variety of cognitive tasks, including playing video games and solving puzzles, might develop enhanced cognitive abilities in problem-solving and adaptability.
Sensory Integration Hypothesis:
If the mind can process sensory inputs from different sources to navigate the environment effectively, a prediction could be that individuals who are more attuned to their sensory experiences and can integrate them seamlessly might have better spatial awareness and decision-making abilities.
Motile Womb Theory:
Interactive Experience and Cognitive Development Hypothesis:
If individuals can discover and enhance their cognitive abilities through interactive experiences, a prediction could be that individuals who actively engage with their environment and seek out interactive learning experiences might show more rapid cognitive development compared to those who have limited interactive experiences.
Sensory Exploration Hypothesis:
If the mind benefits from exploring and experiencing different sensory inputs, a prediction could be that individuals who actively engage with a variety of sensory stimuli, such as diverse sounds, textures, and visual cues, might have a more enriched cognitive experience.
It's important to note that these predictions are speculative and hypothetical, and they would need to be empirically tested to validate the validity of each theory. Additionally, each theory provides a unique lens through which to view cognitive processes, and they may offer insights into different aspects of human cognition and understanding.
1 / 1
I decided to call the conversation "understanding epistemic reduction"
1 / 1
ChatGPT
ChatGPT
That sounds like an intriguing and thought-provoking title for your conversation! "Understanding Epistemic Reduction" suggests a deep exploration of the process by which we comprehend and reduce complex knowledge and information. It reflects the multifaceted nature of understanding and the various theories and perspectives you've been discussing. Best of luck with your conversation, and I hope it leads to insightful discussions!
1 / 1
A list of all of the topics we've discussed.
1 / 1
ChatGPT
ChatGPT
Certainly! Here's a list of the topics we've discussed in our conversation:
Various Perspectives on Understanding
Definitions of Understanding from Different Sources
Different Views on Understanding: Predictions, Explanations, Models, etc.
Formal Treatment of Understanding by Kristinn Thorisson et al
Commonalities and Differences in Perspectives on Understanding
Monica Anderson's "Autonomous Epistemic Reduction" Definition
Cognitive Load and Perceptual Limits in Understanding Complex Problems
Unreasonably High Expectations of "Grokking"
Overton Window of Complexity in Understanding
"De Agricultura" by Cato the Elder and its Concepts
Similarities between "De Agricultura" and Games like Age of Empires and The Sims
Monica's Leaking Chatroom Theory
Reed Wall Mind Theory
Motile Womb Theory
Connections between Theories and Previous Topics
Hypotheses Predicted by Each Theory
"Understanding Epistemic Reduction" as a Conversation Title
Feel free to let me know if there's anything specific you'd like to explore further or if you have any additional questions!
1 / 1
A final summary.
1 / 1
ChatGPT
ChatGPT
Throughout our conversation, we've delved into a wide range of perspectives on understanding, exploring various definitions, theories, and concepts proposed by different thinkers. We've covered topics such as formal treatments of understanding, the challenges of defining and achieving understanding, cognitive limitations, and the parallels between historical texts like "De Agricultura" and modern games. We've discussed Monica Anderson's "Autonomous Epistemic Reduction" theory, Reed Wall Mind theory, and Motile Womb theory, which all offer unique viewpoints on how our minds process information, make decisions, and navigate complex environments. These theories shed light on the intricate nature of understanding and cognition. In the end, our exploration provides a fascinating glimpse into the multi-faceted realm of understanding and how it influences our perception, decision-making, and interactions with the world.
1 / 1
A few of the main points.
1 / 1
ChatGPT
ChatGPT
Certainly, here are a few of the main points we've covered:
Diverse Definitions of Understanding:
Our discussion highlighted the multitude of definitions and perspectives on understanding, ranging from prediction and explanation to the ability to apply knowledge and recognize cause and effect.
Challenges in Defining Understanding:
Defining understanding, whether in humans or machines, presents challenges due to its complex nature and various levels. This has led to confusion and varying interpretations in AI and AGI discussions.
Formal Treatments of Understanding:
We explored formal treatments of understanding, with Kristinn Thorisson's work standing out. His theories emphasize the accuracy of the relationship between an AI's internal models and external phenomena for achieving understanding.
Cognitive Limitations:
We acknowledged that both humans and machines have cognitive limitations that hinder complete comprehension of highly complex problems. Concepts like "grokking" or total mastery are unreasonably high expectations.
Monica Anderson's Theories:
We discussed Monica Anderson's theories, including the "Leaking Chatroom" theory which envisions modular decision-making units, the "Reed Wall Mind" theory that likens the mind to a living spaceship, and the "Motile Womb" theory suggesting the mind is an active participant in shaping perception.
Connection to Historical Texts and Modern Games:
We drew connections between historical texts like "De Agricultura" and modern games like Age of Empires and The Sims, showing parallels in managerial decision-making and the complexities of simulating real-world scenarios.
Epistemic Reduction:
The conversation culminated in the theme "understanding epistemic reduction," highlighting the intricate process of simplifying complex information and the role it plays in our cognitive processes.
In essence, our exploration underscores the multifaceted nature of understanding, its various dimensions, and the challenges and possibilities it presents in both human cognition and AI development.
1 / 1
Do you think you can define "Epistemic Reduction" in this context?
1 / 1
ChatGPT
ChatGPT
Certainly! "Epistemic reduction" in this context refers to the cognitive process of simplifying or distilling complex information or concepts into more manageable forms. It involves breaking down intricate or multifaceted ideas into components, patterns, or models that can be more easily understood, processed, and utilized. Epistemic reduction enables individuals, whether humans or AI systems, to grasp and navigate complex scenarios by focusing on key aspects and making decisions based on reduced yet relevant information. It's a fundamental aspect of comprehension and decision-making, allowing us to bridge the gap between the intricacies of the world and our cognitive capacities.
1 / 1
ChatGPT may produce inaccurate information about people, places, or facts.
ChatGPT August 3 Version
?
ChatGPT



