So, my talk is going to be not an introduction to category theory, but actually an answer
to this question.
So, why would I, as a functional programmer, ever care about category theory?
What did it do for me?
What is the advantage that I get out of it?
Is there any advantage?
Or is it just how I talk about it, you know?
I actually write code, and then after I write my code somehow, I'm talking about it in this
language, but nobody really cares, because I already written my code.
Maybe it's like this, then I don't care, then I don't want to ever study this.
If there are actual advantages to category theory, and if I will be a better programmer,
write better code, then I want it.
So, that's what I mean by an extreme, pragmatic, and unacademic approach, all right?
Just a few words about myself.
I have a degree in physics, and I am a previous, my previous career in academic.
I know very well, I can hear it right away when something is purely academic, with no
applications, just to make you feel better.
I can see that.
I don't want that.
I want actual answers to this actual question.
And to begin, I would say, well, what do functional programmers actually do most often?
What kind of things do they benefit from?
And there are these things.
So there are actually five things that they do, which makes functional programming a beneficial
thing to do for most programmers.
First, work with collections using methods such as map, filter, fold, flat map.
Second, use what I call disjunctive types.
So the types that describe a disjunctive set of values or situations.
So these are called case classes or algebraic data types, although there's not a lot of
algebraic stuff about them.
But so these are already hugely beneficial if you use them in your code.
Third, using type constructors with type parameters and the flexibility that this gives you.
Here I gave an example of an Akastreams declaration, where you have a flow of Kafka
message of my data, Kafka message of my result, and not you.
So all these things are type parameters, and some of them have further type parameters.
So the flexibility that this gives you is an extremely useful benefit.
So you can describe a lot just by this type, prevent lots of errors just by having this
type.
Fourth, people use what I call functor blocks.
So these are the four yield constructions or called sometimes four comprehensions.
That name doesn't describe anything really.
So functor blocks are used like this, and they make code much easier to read, much easier
to refactor.
So they have concrete benefits, although it's a little bit of a more advanced feature for
beginners, but this is a very useful thing.
And finally, carried functions, so functions that return functions.
So powerful feature, which is used less often maybe in ordinary code, but it is used still
nevertheless.
For example, in Akastreams, something like this, it has a carried function right there.
So far, we need no category theory.
A new programmer can learn these features of Scala and become proficient, and they don't
need to learn any category theory.
And this is what I believe is true.
You can know how to use, say, future in the four yield construction and so on.
And you don't need to even hear the word monad to do that.
So where is it that you actually get any benefit from category theory?
So first use of category theory is that when you start working with type classes that have
laws, now type constructors with methods such as map, filter, fold, and flat map are not
just accidentally having a certain type signature and so on.
They actually must satisfy certain laws in order to be useful.
And these laws are, for example, for the map function, identity and composition.
For the filter function, there are four laws.
For the fold or traverse functions, there are three laws.
For flat map functions, there are three laws, actually four, there are two naturality laws.
Now, where did these laws come from?
And why are they important?
These laws can be motivated purely from programmer's perspective by requirements about what code
should one write and how this code should behave.
So these, for example, the map law of composition is something that tells you that when you
map with one function and you map with another function, it's the same as when you map with
the composition of the two functions.
If that were not so, you would write code that is wrong, but it would be very hard to
see why.
And I give you an example of an incorrect implementation of these type classes.
So the case class BAD has implementations written right here for map, flat map, and filter.
This compiles.
This code compiles.
And once I have this, I can write code that I'm showing below with four yield.
It will compile.
But this code will not give you a result to expect.
For example, in the code that I'm just showing, I expect that the result is the same as the
value B, because I'm just going over the values in B, and y is equal to x, so that it should
be any different from what it was before.
So this I expect to be just equal to B, but it's not.
As you see, if you call some method, it has a different result.
Now looking at this code, I don't see where this has happened.
Where did I replace the value incorrectly?
So it will be very hard for me to find the bug.
This is, of course, a very artificial example.
But imagine that there is a custom data type for which you implemented map incorrectly.
That does not satisfy laws.
It will be very hard for you to find your bugs, because it will mostly work.
Sometimes it will not work.
It will be not easy to see why.
And the reason is that you violated laws.
So these methods that you implemented are not lawful.
Unless you know that, unless you know how to check these laws, you will not be able to
implement your type classes correctly for some custom data types.
So this is important for library authors, but it is also important for application programmers
who would like to implement these methods for their custom data.
So your custom data could be dictated by business logic.
Your product manager dictates it essentially, not you.
You need to implement the type class correctly.
So that's where you start looking at these laws.
No category theory so far, but you start looking at something that's a little more abstract
now.
What are these laws?
In fact, it's not easy to reason about these laws.
They're all different.
But with some work, one can transform and simplify these laws into a common pattern,
which I call lifting.
So you need to derive mathematically equivalent formulations of these laws.
So this is some work that, let's say, takes you two months of calculations.
So every day you do calculations for two months.
And two months later, you end up with this table.
And not joking, by the way.
You end up with this table.
So this table formulates the type classes on the left with certain methods, which is
not this method maybe you thought about, but it is equivalent.
And its type signature is shown on the right in this table.
So for example, for the functor type class, instead of map, you use fmap.
You need to swap arguments in map so that the signature looks like this.
For the concha functor, you need to do cmap, which is, again, you need to swap arguments
and it should be like this.
For the filter, you should not look at the filter function.
You should define a function called liftopt with this type signature.
But this function is actually equivalent to the filter function, which requires you to
derive all that and prove.
For the monad, you use a flat map.
For the applicative functor, you use app.
And for comonad, you use a coflat map.
So once you do that, you start noticing a lot of similarity between all of these functions.
So the type signatures are very similar.
You have some kind of function, which is not necessarily a to b, some kind of twisted
type.
a to f of b, a to option b, or b to a, or some kind of other thing.
And that function is mapped into a function f of a to f of b, which is, again, kind of
a little twisted.
But it's always this mapping.
So first you start looking at this and you're thinking, why is that?
Why are all these things kind of the same, except for these little twistings?
But it's not the end, because actually you see the laws also follow the same pattern.
There are two laws, identity and composition, always just two laws.
If you look at this table and you see I'm using, as I said, specific functions, f map,
c map, lift, opt, you need to use them.
If you use them, their laws are just two laws, identity and composition.
In each case, there is some kind of composition of this twisted function that again gives
you a twisted function.
And there is some kind of identity function, which is also twisted, which is then lifted
into an identity function.
Now I'm not going through these derivations, because that would take two months.
But I'm giving you the results.
The result is that you have this heuristic picture of lifting.
All these laws and all these functions look like liftings.
You have some kind of types A, B that are lifted into f of A, f of B and so on.
And you have functions that have twisted function type, and they're lifted into this f of A
to f of B.
So if you look at the type signatures, you see how that works.
And if you lift identity, you get identity.
If you lift composition, you get composition.
But the composition needs to be defined specially, because you see there's no way to compose
these two functions directly, because the types don't match.
So there's a twisted composition operation that you need to use.
However, it's always, once you define that, it's always going to be the same thing.
So now you're saying, I've been writing the same law all over again in disguise.
What is that law?
What am I doing?
And that's what category theory says you're doing.
So there is an abstract picture of this lifting.
And this lifting in category theory is a function between two categories.
And there's an abstract picture of the twisted function type, which is called a morphism.
So morphisms between A and B are functions with some twisted type.
For example, B to option of A, or f of A to B, or something.
But these are always similar in very important ways.
In fact, the category has laws that there must be an identity morphism, and there must
be a composition operation, and there must be identity and associativity laws for these
operations.
And this is true for every one of those examples, the same laws hold for them.
And then the functor is a description of how to map types and functions or morphisms from
one category to another.
And you see, functors have two laws, the law of identity and composition.
And so that's what we've been doing here all this time.
So every one of these type signatures is a lifting corresponding to a certain functor
in the category theory language.
Which means that once I learn those definitions, I need to learn only once, this definition
of category definition of functor, I can just say, you know what, I don't really remember
the laws for applicative functor, but I remember this is a lifting from a functor from this
category to this category.
So that's much easier to remember.
Everything else will just follow automatically.
The laws are the same.
Once I know what twisting I apply on the left and what twisting I apply on the right, that's
it, I'm done.
So this is a great economy of thinking.
So that's what mathematics is very strong at.
You abstract away details and you find common patterns and you understand where all this
comes from, so to speak.
So here are examples of categories.
So the plain category is just plain types A, B, C, and so on and functions A to B. The
F-lifted category, where the types are wrapped into F and morphisms are also wrapped, so
F of A to F of B.
Then there is an F-clisely category where morphisms have this signature, A to F of B.
So I'll just remind you, we have two of them here, this one and this one of this kind,
where the twisting is of the type A to something of B.
So then let's call this F-clisely.
The identity morphism is the pure method and the composition is this operation that I
denote by the diamond, which is implemented using flat map.
So the clisely composition has this type signature, A to F of B, composed with B to F of C, gives
you A to F of C.
And if you write down the laws for this composition, which means that identity law on the left,
identity law on the right, and associativity law.
These three laws are equivalent to the laws of moonettes.
Again, this is a derivation that should occupy you for some hours.
But at least it's something that you can do.
The F-applicative category, the morphisms, so the types are just, again, A, B, C, but
morphisms are wrapped in the functor like this.
So not like this, as in the clisely, but like this.
And so here, the composition is defined differently using the map tool operation of an applicative
functor, and the pure is defined differently.
Other than that, everything is the same.
So the difference is in the twisting, but the laws are the same.
So you have this twisted function, which is in the category theory language called morphism,
and you have this twisted composition.
Other than that, everything is the same for all these examples.
They're examples of the same construction.
Contra functors, you just reverse the order of the arguments of the function, and that's
it.
Exactly the same laws, exactly the same results.
Now, the big advantage comes when you start to ask, well, what can I do to existing monads
to get a new monad, or what do I do to existing functors to get a new functor?
And category theory tells you there are general constructions about that, and you can just
copy from book, and you have assurance that it will work in your code, that your laws
will be correct, because this is proved once and for all in this abstract language of morphisms
and objects.
It will equally apply to functors, monads, applicatives, and so on.
Filterable, contra-functor, contra-filterable, whatever you want.
As long as you can find this formulation, you know all these results automatically apply.
So this is a great advantage if what you want is to talk about properties of all type classes
at once.
Now, how often do you, in your programming work, need to do that?
To talk about properties of all type classes at once, or you have a function that's parametrized
by an unknown type class.
I don't think I've seen an example of that.
But if you wanted to do that, that's what category theory is for.
An example with filterable functors.
Now, filterable is the type class that is not often explained.
It's just the filter function everybody knows, but there's no type class for it usually,
and there are no laws that people talk about usually, but actually it's an omission.
There is a very well-defined categorical formulation, or a functor from an option classly category
to the F-lifted category, and then you just check everything works.
It's exactly the same thing, and because the laws of the filterable functor are not well
known, you might be unsure that you have guessed them correctly, because first, as I said,
you take your programmer's experience and you translate that into your expectations
as to what the laws must be.
I have a filter.filter.filter.
What should it be?
Well, I expect as a programmer to have this, to simplify this code like that.
For example, filter with a predicate that's identically true.
I expect that to be an identity function, because it's a filter that never filters out
anything.
I don't change my container that I'm filtering, so that's one of the laws.
But you don't know that you guessed all these laws correctly.
Category theory assures that your laws are consistent, useful, very general.
Once you make contact with that, you have great assurance that your library is correct,
your laws are correct, and so on.
Category theory also suggests generalizations, what you can do more than this.
So for example, here in this construction, you see the option plays a prominent role,
but actually, this can be replaced by any other monad.
Then I have something that I call mfilterable functor.
Maybe it's useful, maybe not, but it's a suggestion that you can explore.
Another suggestion, why don't I use the opposite category here instead of a to option b, I
get b to option a in this function, or here instead of f of a to f of b, f of b to f of
a.
Why don't I do that?
And I explore what happens, and I get again something possibly useful.
So category theory gives you these suggestions, it doesn't show which one of them will come
out to be useful for you, and it doesn't show how the driver proves anything.
That's your work, that's going to be not easy.
So the second major situation where you want to use category theory is if you make a library
that you could call a type constructor library, a library that's parametrized by type constructors.
An example is various libraries that give you a free functor, free applicative, free monad.
You can have a free of any of these type constructors, type classes that I listed over there.
Free filterable, free contra functor, you can have all of that free.
And that is a function in the library that's parametrized by a type constructor.
Now once you are working with that kind of thing, you better understand what your laws
are, you better understand how to implement this correctly.
It's not obvious at all.
These constructions are not easy to just wing.
You have to understand that you derive the laws correctly for your implementation and
so on.
That's where category theory helps, because all of these things are general constructions
that follow from some generalized functors in some generalized category.
So now at least you know where to look.
The second example is the church encoding of a free monad, which is currently known
under the name tagless final.
Now this name means nothing.
There's nothing useful about that name, so I would prefer to call it a church encoding
of a free monad.
Again, church encoding is a general construction that you can look at in arbitrary category
and prove its properties and apply as necessary, free monad as already said.
So if you're writing a library for this, you better know which laws you need.
You better know what categories need to be checked for consistency.
Another example is the church encoding of recursive type constructor, which will make
it non-recursive.
Now this is parametrized by a type constructor and also by recursion scheme.
So there might be different recursion schemes and you might have different implementations.
So I'm not going to go into details here, but basically these are examples where I would
suggest going into category theory.
Yes.
What is the recursive type constructor?
List for example.
It's a type constructor whose definition uses itself.
Okay.
So list is a nil or a cons with a list inside or tree, a tree has a tree inside.
So any kind of constructor of that kind is a recursive type constructor.
Now if you're writing a library that works with these type constructors in general, parametrized
by arbitrary type constructor with arbitrary properties or for all monads or for all applicatives
or something like that, that's the first sign that you might need to look a little bit at
the definitions of functor, category, try to see if your laws are correct, if you chose
them correctly.
That's kind of the first sign.
Unless you work with this, category theory isn't going to help you much.
Now even if you work with it, category theory will not help you derive any of these laws
actually for your code.
Category theory doesn't know about your code.
It's too general.
Category theory talks about properties that are in common between monads, functors, applicatives,
filterables and contra-functors.
What kind of properties can this be?
Certainly not something that's in your code.
Your code is extremely specific to all these things.
So category theory cannot possibly know how to talk about your code.
And so it will not help you deriving or proving these laws.
You need special techniques if you want to derive or prove that your implementation is
correct.
So in order to teach people how to do this, I'm writing a new book which I call the Signs
of Functional Programming.
And this book will explain with all kinds of examples and exercises and diagrams and
calculations how to do computations, how to show this, how to derive these.
So I talk about category theory a little bit in this book, but it's by no means a book
about category theory because like I said, unless you're going into such territory that
you're working with type constructors that are arbitrary and that have type class laws
or something, you don't need category theory at all.
So let me summarize.
Signal programming needs to be learned as such, not as an application of category theory.
That's my main topic.
That's my main statement.
Category theory helps understand where the type class laws come from, does not help you
derive these laws, prove these laws, or write code correctly.
Category theory helps you to understand how to write these general type constructor libraries.
Now this is not often that something like this needs to be done in your work.
But if you're doing this, yes, you need to understand.
So yet even then a library author gets inspired by category theory, but needs to do symbolic
reasoning by hand in order to see that the code is correct.
Category theory won't help there.
And learning how to do that takes a lot of time.
So this is part of why I'm writing this book.
This knowledge is never explained.
You will not find a tutorial, a book, an article that tells you why the state monad satisfies
its laws.
I challenge you.
Show me.
Who writes out this proof?
Nobody.
I don't even know how to write it so that you don't fall asleep because it's very difficult
to use color syntax or Haskell syntax or something like this.
You need special techniques.
So that's part of what my book is about.
So right now the entire book is going to be free.
It's available on this GitHub.
Chapters 1 to 8 are ready.
And I'm going to continue working on it.
But so I hope I explained what category theory gives us and what it doesn't give us and where
you might want to look into it and where you might want to stay away from.
Thank you.
Questions, please.
Thank you that category theory helps a lot and allows us to give very interesting things.
But I see them as patterns, right?
So the same way you fold a ball like object oriented for them, you ask patterns.
And I feel like sometimes people use too much patterns, right?
So you have something that could be just like a straight forward cold and then you have
cold monads and crazy things.
How do you see the limits of how much you should use on the pattern and how much you
should just have during presentation?
So the question is, how much is it reasonable to use these patterns of functional programming,
monad, cold monad, and so on?
How much advantage does it give?
So yeah, this is a very good question.
So I would say it is harder to change code that has been written with a pattern that
you don't really need.
So it will bite you eventually because I would say that right now the community has found
certain cases where it is certainly beneficial to use, say, a four yield, which is not necessarily
very complicated, but it's something that once you start using, you can't stop using.
So I think there are certain situations where it is beneficial to use it and people gradually
understand that this is good for certain things.
But if your four yield is one line, maybe it's not so good to use it, right?
Maybe if you have a co-monad, just because you feel like having a co-monad, you should
try to write the code without it and see how much shorter it becomes.
If it becomes shorter, you're not gaining anything.
If your code without the co-monad is easy but with the co-monad becomes harder, then okay,
what are you really gaining, right?
So there are certain situations where right now the functional programming community does
not yet know when to use these things.
So right now, for instance, some people say you should always use cat's effect, IO, and so on,
and other people don't do that.
They just use ordinary, effective, non-pure code with a bit of monad thrown in, right?
I don't think that it is right now very obvious which patterns to use in which case,
but in some cases it is clear.
So some cases, very clear, for instance, if you have a lot of code with error handling,
or you want to make sure that your code writes a transaction to the database of the right places,
you should have it in the monad, use a free monad,
and you guarantee that it will never make a mistake of that kind.
So I think it is clear by now that we have these figured out,
but there are many other places where it's still experimental.
So you should not think that somehow this is a silver bullet,
whenever something comes from category theory, should be used always in your code,
because that's still not clear, and it will take maybe another decade
until we are completely comfortable making that call.
Questions?
You talked a bit about how this final and free monad,
what would you say the differences are between those and what are the use cases for one
and what are the use cases for the other?
So the question is, tagless final as opposed to free monad,
when should we use free monad and when should we use tagless final?
So tagless final is equivalent to free monad, first of all.
So you are not gaining anything in functionality.
You might be gaining in architecture of your code.
So in other words, you can rewrite one into the other and back,
but the code will change, it will become maybe less maintainable.
Now, if you are just using one DSL,
and there's nothing else you'll ever use, so one free monad,
and you'll never use another one, and there's only one interpreter for it.
You only do one thing, then you don't need to use tagless final.
The church encoding of a free monad, first of all, it is slower, it has more code,
but it gives you a lot of flexibility in how you use the program.
So if you have a program that is only used once in your course,
you want this free monad, make a program in the free monad, and you run it.
And that's only once in your course.
There's no advantage in making a flexible encoding that is parameterized
by arbitrary output monads, which is what tagless final does.
So I don't think that would be advantage.
But if you have a lot of little pieces that could be interpreted in different monads,
immediate representations, and so on, a lot of flexibility that you need,
yes, then you might do it.
So in that case, there's no clear-cut recipe here.
You need to look at your specific code, and sometimes you refactor the code,
and you see that it becomes simpler, even though you're not using this fancy thing,
maybe it's okay.
So I would say, most importantly, they're equivalent.
You need to understand that this is just the same thing from the point of view of power
of what you're doing.
It's just that your code is going to be organized differently
with a little more flexibility in one place and a little less flexibility in another place.
So that's what you need to look at.
There is no recipe.
You're generalizing, filtering, a lift-up, and that's taking a function,
from A to option of being sort of able to hold it.
Yeah.
Can you explain more why that, how that helps me pragmatically?
Sorry, how did it help you?
What does that bias by?
So the question is, what does it...
Okay, here's this thing.
The question is, what do we gain by rewriting filter as lift-opt with this other type signature?
So for programming, you should never use this function, lift-opt.
You should always use filter.
It is pretty much useless to coding with this lift-opt.
Very inconvenient around about...
It's equivalent though.
It's completely equivalent, but it's inconvenient for actual coding.
The gain that you get out of lift-opt is about understanding the laws.
Only in that you have gain.
So if you ask what are the laws of the filter function,
I'm pretty sure nobody will tell you because nobody knows.
And I can tell you, well, it's not an easy thing to derive them,
but if you understand that it's equivalent to lift-opt,
lift-opt has this very suggestive type signature from which it kind of begs to say,
oh, on the left I have A to option B.
This is a clisly of option.
On the right I have F of A to F of B.
This is just lifted to F.
I know what the laws must be.
Oscar?
I wouldn't be so harsh on a library that we call this option map.
And indeed, it's very often not optionality.
And then you transform it.
Let me get the thing because it might be none.
And now let me transform that.
And that might be none, blah, blah, blah.
So actually you use option map quite a lot.
And so you actually might have lots of great uses for doing that.
Thank you.
So I will just repeat that in Oscar's experience they have use of this function
when they validate or check presence or absence of data.
So it's kind of a combination of map and filter in a single function.
Right.
Well, true.
So in my view, most of the time you don't need this.
Certainly I might not have that experience.
What I've actually found the use for this is when you filter,
you lose the value that you're filtering on.
Right?
You've now gone on some empty monad or bad state or exceptions or whatever it is.
But you can't do anything with the value that you filtered out
such as to recover by using that value or even just as a side effect
to say this is why we fit.
And so in some cases I've wanted something that would allow me to both filter
and see the value, continue to see the value that I filtered out along.
So the question is, is there some other type class that allows you to filter
but continue to see or to have access to the values that you rejected?
Because filter drops these values and you cannot have any access to them.
Yeah, I don't know.
So there are functions like partition where you partition by a predicate
and you have two collections as a result.
Right?
So you have all the ones you rejected if you want them.
So I think you need a monad that gives you pass-fail with an error information
and then you can put all that into the error information
or you can try to do something else.
But I don't know of hand what that would be.
To motivate the example, I'm looking at locales and I've said a supported locales.
I don't support this locale and filter it out.
I want to hang on to this locale though because I think I can find a close match to it.
Right?
So I can't use this one but I want to use it to get to one I can't use.
Yeah.
Transform into an either and then have some kind of recovery
that sometimes recovers an error for either into a non-error.
And so you don't use filter.
You transform into, because filter will drop, right?
So you want something else.
So transform into an either and then sometimes you can recover that.
So it's like a pass-fail or you sometimes recover the failure.
Right.
And recover the failure in the context of the thing that I threw out.
Right.
Well, yeah, because you can have that value in there in the left part of either.
You can have still whatever you need.
So maybe something like this.
All right.
Well, thank you.
I think we're out of time.
Thank you very much.
