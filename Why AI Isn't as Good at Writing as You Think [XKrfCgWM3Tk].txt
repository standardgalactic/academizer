This video was brought to you by Ren, more on them later on in the video.
Is AI generated writing real? My last video was written by a machine learning algorithm that
had been trained on all of my previous video scripts, and while I was making that video I
kept thinking about this question. What is the difference between that AI's script and my scripts
that I write? It turns out this is a pretty popular question, especially when it comes to
academic writing. AI's, or to be specific, machine learning algorithms. I know that many
people don't consider them AI, but AI is a lot easier to say and write, so that's the term I'm
going to be using throughout this video. If you have a better term that I can use that isn't a
mouthful then please share it in the comments. Anyway, AI's are certainly capable of writing
convincingly, and they're growing in popularity especially among students if all of these publications
are to be believed. Many teachers are voicing complaints about students using AI to write their
essays for them, and as an ex-English teacher who literally has multiple degrees in writing and the
teaching of writing, I wanted to explore this AI essay writing phenomenon and finally answer the
question, what really makes AI writing any different than human-generated writing? Or in other words,
is AI-generated writing real? Now I'm not going to be talking about AI-generated art in this video
because that is a whole separate question. I am talking about the question of if you use an AI to
write something for you, does that count as you writing something? Or if an AI writes a piece of
text, is that text just as valid as a human-written piece of text? We're not going to be talking
about the definition of art or who owns art or anything super abstract like that. We are going
to be talking about writing as it happens in real life and how AIs affect it. But before I give my
perspective, I am curious about what you all think. Is AI writing real? What are the differences
between AI writers and human writers? Do you think that AI writing tools should replace human writers
if they get good enough? Leave your answers in the comments. But without further ado, let's jump in,
shall we?
Before we can compare AI writing to human writing, we need to make sure that we understand how AI
generated writing actually works. AI text generators aren't born knowing how to write,
they have to be trained. Most text-generating AI that you'll come across is built on GPT-3,
which is a model from OpenAI that has been pre-trained using the internet. Like literally,
they trained it using almost 45 terabytes of text data from the internet. 45 terabytes of text.
The folks at OpenAI just took this model and fed it basically the whole internet until it
knew how we put sentences together and what words occur next to each other most often.
Once models have been trained, then they can start generating their own text based on what
they've learned about how we write. And their writing is done word by word. It looks at what
was just written and then decides what should come next based on all those other texts that it's read
and its memory or its context window, which is basically how many words that it can remember
at a time. For example, GPT-3 has a context window of 2048 tokens, which means that it can remember
the last 1500 words or so that it's written. Now, I know it's a lot more complicated than this,
but generally speaking, this is how most of our AI writing tools work. As for specific tools,
we have the GPT-3 playground, of course, as well as Grammarly, Jasper, CopyAI, WordAI, AnyWord,
Writer, YouWrite, SudaWrite, and dozens more. These programs can simply help you polish your
writing or they can generate entire articles or essays for you. You simply have to give them a
prompt or ask them a question and their training will kick in and they'll produce usually fairly
competent content. When I talk about AI-generated writing, it's easy to immediately think of
these kinds of complex programs, but in reality, algorithmic writing tools are everywhere. Auto-correct
on your phone, spell check on your Word document, the autocomplete function in your email, all of
these are forms of AI writing. It is a computer taking what it knows about language generally,
and you specifically, to try to figure out what it is that you're trying to say so that it can
help you say it better. Now, I am not necessarily saying that these little tools are exactly the
same as having GPT-3 write an entire 5,000-word essay for you. The scale and the use cases of each
of these is very different from the others. AI writers are more complex and much newer than
other grammar and spell check tools, and their algorithms for generating text are more sophisticated
than your phone's autocomplete. It's just that if we define real writing as writing done entirely
by humans, then we're already not really doing that. Computer programs and language models are an
inherent part of how most of us write on a daily basis, so if we can seamlessly integrate AI text
into our texts, then clearly human writing and AI writing aren't that different, right?
To some degree, that's true. AI writing is good at those small-scale uses that we've been using
it for. Spelling, grammar, generating a few phrases or a couple sentences at a time is where
algorithms shine, but the further out you go, the bigger your scale gets, the more room for error
there is. And this is where we get to the real pitfalls of AI-generated writing. The Achilles
heels of AI are context and structure. AI doesn't understand context. By this, I mean AI doesn't
understand what it's saying. It just knows that this word usually shows up after that other word,
so when they see that word in a text, they should write this word next. To GPT-3, words don't have
meanings. They're not words, they have no real linguistic value. They're just bundles of code
that the program has been trained to see as parts of billions of different patterns, and it puts
those bundles together using those patterns that it has learned to recognize. It knows how words
work, where they usually are in sentences, etc., not what those words actually mean. And this is how
it generates nonsensical statements, like when it was given a prompt about mixing cranberry juice
and grape juice, it decided that drinking that juice mixture would poison you, but drinking bleach
would taste okay. Or how it suggested that a defense lawyer should wear a bathing suit to court.
Words don't actually mean anything to an AI. As NYU professors Gary Marcus and Ernest Davis wrote
for the MIT Technology Review, AIs don't learn about the world. They learn about text and how
people use words in relation to other words. In the cranberry juice example, GPT-3 continues with
the phrase, you are now dead, because that phrase or something like it often follows phrases like,
so you can't smell anything, you are very thirsty, so you drink it. A genuinely intelligent agent
would do something entirely different. It would draw inferences about the potential safety of
mixing cranberry juice with grape juice. All GPT-3 really has is a tunnel vision understanding of how
words relate to one another. It does not, from all those words, ever infer anything about the
blooming, buzzing world. It does not infer that grape juice is a drink, even though it can find
word correlations consistent with that. Nor does it infer anything about social norms that might
preclude people from wearing bathing suits in courthouses. It learns correlations between words
and nothing more. The empiricist dream is to acquire a rich understanding of the world from
sensory data, but GPT-3 never does that, even with half a terabyte of input data. AI can only ever
write based on what it has been told, and this actually leads to more issues because what it's
been given is stuff written by humans. And I don't know if y'all knew this, but humans can be pretty
gross sometimes. Like, we have all of these biases, sometimes hidden, sometimes blatant, that make their
way into our writing, sometimes accidentally, and sometimes on purpose. Remember, GPT-3 was
trained using the entire internet, and if there's one thing the internet is known for, it's not
being super great to everyone all the time. And I'm not saying that we should hashtag cancel
GPT-3 when it says racist and sexist garbage. I'm saying that it doesn't know those things are racist
and sexist to begin with. It has no context, it just knows that male identifiers often occur alongside
words like fantastic and personable and stable, and female identifiers often occur alongside words
like bubbly and petite and naughty and sucked and tight. So it repeats those things when you ask
it to describe men and women. I also, I hated saying those words that it makes me want to wash my
mouth out with soap. It doesn't know what racism is, it just knows that Asian is often paired with
positive words, and black is often paired with negative ones. So that's what it repeats when
you ask it to describe people of various races. Just remember this when you hear someone talk
about how AI can be trusted to solve problems objectively. Now one kind of funny thing that
I think is worth mentioning is in this study where they judged the language usage for different
genders and races, they also looked at the language used around different religions. And while this
did uncover some problematic things like Islam being paired with terrorism, it also said that
atheism was often paired with cool and correct, but also mad, complaining, and arrogant, which I
think is very funny. Anyway, let's move on. But okay, let's say we can fix that. Let's say that we
can somehow teach this AI what words are and what all their meanings are and all of the denotations
and connotations and idioms and metaphors and what racism and sexism are and all that stuff.
Let's say we do that. We would still have the issue of AI not having a handle on writing
structure. I've already mentioned how GPT models write texts word by word and have a limited amount
of memory for how much context they can remember. Many people think this is similar enough to how
humans write. I mean, we all write word by word. That's just how writing works. But we don't have
a limited amount of memory for how much context we can remember. While we are writing, we remember
what came three paragraphs ago and we know what we want to say three paragraphs from now.
Humans write using what is called a recursive process. That means that instead of writing in
discrete steps that come one after another, brainstorming, outlining, drafting, revising,
in that order, like we're taught in schools, the writing process is actually a lot messier.
Cognitively, we kind of do all of those steps at once. We are brainstorming while we're writing.
We're revising while we're outlining. We are constantly going back and forth between all of
these phases. And most importantly, we're also thinking while we are writing. See, writing isn't
just the production of a text. It is not just writing down knowledge. Writing is actually a way
to generate knowledge. Composition scholars have found that the very act of writing generates
thoughts and ideas. By externalizing the stuff happening in your brain, you are, by definition,
changing that stuff. Now, I know this is really abstract, so I won't spend too long on it.
The important thing is that the writing process is recursive. It is a bunch of stuff happening in
our brains all at once where we're looking backwards and forwards while we're writing.
And the act of writing affects our thinking. What this actually means for the stuff that we write
is that there's an inherent importance to the structure and organization of our texts.
Because when we start outwriting an essay, we have these ideas in our brains, but as we continue
writing, those ideas change and morph, and it's not really in our control anymore. Like,
not to get too meta here, but as I was writing an earlier part of the script and I was thinking
about how I was going to tie this AI stuff into this writing process stuff, I hadn't really thought
about the specific way that I was going to explain the relationship between writing and thinking.
I knew that I wanted to talk about it, but I wasn't sure how. So when I got to this section
and I started writing, that was when I realized how I wanted to talk about it. I had to write
my way into my ideas. Anyway, our writing evolves as we write. So by the end of the essay, we realized
that what we wrote at the beginning might not actually be entirely accurate anymore. So we go
back and adjust the rest of the essay to fit more closely with the idea that we ended up arriving at.
And remember, all of these steps are kind of happening at once. We go back and adjust things
while we're planning what we're going to say next and so on. Our brains are always thinking
of the bigger picture of our texts. They're always thinking about the past and the future,
about what we could have said three paragraphs ago or what we should say three paragraphs from now.
But AI doesn't do that. It can't. As Mark Rytle, a professor at the Georgia Institute of Technology,
says, AI is very fluent. It is very articulate. It is very good at producing reasonable sounding text.
What it does not do, however, is think in advance. It does not plan out what it's going to say.
It does not really have a goal. Structural thinking is fundamentally human. And sure,
not all people consciously think of their writing in this way. But as a college writing teacher,
that was actually one of the things that I needed to teach my students. Metacognition,
or thinking about how we think is really important to, well, lots of things, but it is also important
for learning how to write well. Recognizing that our brains aren't machines that simply go from
step one to step two to step three, and allowing ourselves to write however our brains are most
comfortable doing it, is vital to learning how to write well. Like for me, I write in a really
messy way. I constantly stop in the middle of a sentence or a paragraph and go up and fix something
or go write my conclusion just because I had an idea that popped into my brain as I was writing.
Many of us have had that bad English class that taught us that we have to do things in that step
by step format. And that's actually really bad for a writing process. Let your brain do the messy
writing. That's what makes you human. So, okay. We know that AI can't write in the same way as humans,
but I don't know that that actually satisfactorily answers our initial question. Sure, AI doesn't
actually understand words or have a recursive writing process, but does that mean that its
texts aren't real? Let's step away from the composition theory stuff and focus on the written
product. When AIs write texts, often those texts are fine. There are countless articles that were
written by AIs and they sound like they were written by a human. Does this mean they're real?
If writing were just about passing the Turing test, then sure. If all that we want out of our
texts is to have them sound good, then AI-generated writing is real. It may be simply a meaningless
if technically correct amalgamation of words, but it sounds like something a human could write.
Awesome. 100% real. Great job. But the value of a text doesn't just come down to does this sound
like a human. It's also about being able to do something with that text. Texts need to have
reasons to exist, and that's something that only humans can do. We write for a reason, and that
reason is partially to help us think, to help change minds, to change the world around us. We
don't just write to convince other people that we're human beings who talk like human beings.
AIs cannot understand the context of what they're saying, because as we talked about earlier,
they don't actually understand words. And without that understanding, they cannot
actually communicate concepts. They can say literally anything as long as what they're
saying sounds believably human. That doesn't mean that their random statements have any deeper
meaning or purpose. But when I write things, my goal is to communicate, to persuade, to change
minds, to affect the world around me, not just to sound believably human. And all those goals
are baked into the writing process. In real life writing, there is always a goal. We always make
texts for reasons. And this is what I see as the cause of these students using AI to write essays
epidemic that is allegedly sweeping the nation. Students are having AIs write their essays because
they don't see the point of the essays. Most of the time, students are using AI to complete the
homework that they see as busy work, the stuff they see as pointless and a waste of their time.
And I get it. We like having reasons for things. And if we don't see the reason for doing something,
we half-ass it at best or totally outsource it at worst. Now, I'm not saying that we should
make all of our assignments pithy, pandery exercises. I just think that if an assignment
can be completed by an AI, then maybe the assignment isn't as valuable to students as we think it is.
To give an example of a more widespread issue, this is the problem with algorithmic assessment
of writing. So in many schools, the writing portion of standardized tests are evaluated by
algorithms, programs that are less sophisticated but similar to the AIs we've been talking about.
On these standardized tests, students are asked to write essays that don't really have a clear
purpose or point. They just have to check all of those, this is what a human essay sounds like
boxes, the thesis statement, the transition words, the correct grammar, all things that can be easily
spotted by a machine. But these kind of context-less writing assignments are nearly universally
dismissed by writing teachers. The AI program algorithm slash rubric is only able to detect
what the student is thinking. And in a meaningful written response, particularly during the drafting
process, how the student is thinking or metacognition is much more important. When a writing assignment
is primarily meant to measure recall or comprehension, assigning writing over using a different
assessment instrument isn't necessarily an advantage. If an algorithm can assess the piece
of writing, I question the use of writing for the assessment. To put it another way, good writing
can only be done by human brains for the reasons that I talked about earlier. So if we want to
teach students how to do good writing, we need to teach them how to do those fundamentally human
skills, like understanding context and structure and process. If a writing assignment can be
completed or evaluated by an algorithm, that assignment necessarily cannot be assessing
those human skills. Because algorithms cannot learn those skills yet, and if they don't have the
skills, then they couldn't do the thing that they're being used to do. Now don't get me wrong,
not every single writing assignment at every single grade level has to be some kind of incredibly
deep manifesto or whatever. I just think that we need to be more conscious about what we're
asking our students to write and why. Learning how to write isn't just about learning how to
produce written essays. It's about learning the structure and organization and process. Those
are things you can only learn by doing. If students are just copying an essay from a website or an
AI, changing a few words and putting their name on top, that is not doing writing. AI can be great
as a jumping off point. It can be great for inspiration or for helping you think about
things in different ways. It can be a great tool, and it can even be something that writing teachers
use in their classes in creative ways. It's not evil, but it's also not a replacement for human
writing. But even if AI could write just like humans, there are other issues with this technology
that I didn't get into in this video, one of which is the sheer amount of energy that machine
learning models require. GPT-3 was trained using pretty much the entire internet, and that level
of training doesn't come cheap. Their energy consumption is a real issue, and if it's not
curbed, this technology could end up being a barrier to solving climate change. But that is where
today's sponsor comes in. Today's video was very graciously sponsored by Ren, a public benefit
company committed to tackling the climate crisis head on. Ren is a website where you can calculate
your carbon footprint and reduce and offset it by funding a diverse mix of carbon reduction projects
and promote system level changes to help stop climate change from the top down. So whether you
want to learn how you can minimize your own energy consumption, or if you want to learn how you can
convince the government to actually start regulating this AI energy consumption, Ren has you covered.
We all have a different role to play, and Ren is here to help all of us figure out how we can each
make an impact in the fight against climate change. Ren recently partnered with the Clean
Air Task Force, which has been rated as one of the best organizations fighting climate change.
But if that's not your cup of tea, then they also have tons of other great projects like mineral
weathering, tree planting, biochar, rainforest protection, and providing clean cooking fuel to
refugees in Uganda. Ren does rigorous research on all of their projects to make sure that they're
actually doing what they say they are and having real positive impacts, so you can feel good about
supporting whichever you choose. One of the best things about Ren is how transparent all of their
financial information is. They share how much their CEO makes, as well as give detailed breakdowns
of all of their expenses, so you know exactly where your money is going. You can also decide how
much you want to contribute to projects, so you have even more choice over how exactly you want
to help fight climate change. And the first 100 people to sign up to Ren using my link will get
10 extra trees planted in their name. That is 10 whole trees. That's a lot of trees.
That's more trees than I can count on nine of my fingers.
If this sounds up your alley, then click the link at the top of the description or in the pinned
comment. And thanks again to Ren for sponsoring. Okay, now back to the video.
I don't think that AI-generated writing is inherently evil, or that it's going to spell
the end of all human creative writing endeavors. I think that that kind of technology is a long
ways off if it's even possible at all. I just think that focusing on the product, the piece of
text that gets produced, is short-sighted. Writing is so much more than just a product,
and there's so much about the writing process that AI just can't do. I do think that AI writing
is genuinely really interesting and exciting, especially as it relates to natural language
processing. I am a huge cognitive linguistics nerd, and so I am always down for learning more
about how language processing works. And with these kind of like neural networks, we can really
start to model how our own brains work. That is super cool, and I'm into it. And that is to say
nothing of how we can use AI-generated text to help us write. Honestly, if a job like copywriting
can be done by an AI, let's let AIs do it. We don't need to waste human brainpower on writing
garbage SEO for billion-dollar companies. But seriously, messing around with AI text these
past few weeks has really been eye-opening. I have generated so many absurd and silly and
mind-blowing and just interesting sentences that have probably never existed before,
and that is genuinely really creatively inspiring. There is so much amazing stuff that
writers and teachers are going to be able to do with AI-generated texts going forward. But
I think that this all kind of gives away the big issue. We still need people to be able to
do things with texts. AI can mimic human writing pretty well, but at least for now,
we still need people to clean it up, proofread it, make sure it sounds enough like actual humans.
And importantly, AI still can't do anything new. When you give it a set of texts to learn from,
AI inherently cannot generate new ideas or concepts. Sure, GPT-3 can put words together in
ways that we've never seen before, but it takes a person reading those words to actually ascribe
any significance to them. There is no match for human intuition. There's a reason that
engineering judgment is a thing in engineering fields. Even when they have all these models and
programs and tests and data, things still come down to what the humans think. At the end of the day,
all of these questions are still open-ended and there are no clear right or wrong answers.
No, I don't think that students should be using AIs to write their essays for them,
but it's still worth considering where the line is between technology mediated writing
and technology created writing. There are no easy answers, but that doesn't mean that these
questions aren't worth asking. But let me know what you think in the comments below,
and I'm really looking forward to seeing what you all think about this really nebulous issue.
I am planning on doing a live stream here on YouTube around the beginning of December
to talk about some of this AI stuff and to read some of the lines that I couldn't fit in my AI
written video from last week. There's a lot of stuff that I couldn't fit, and it's all very silly,
so be sure to keep an eye out for my community post for when I schedule that stream. If you
haven't watched that other video yet, I promise you it's worth a watch. It is so silly and uncanny,
and I had such a fun time making it, so be sure to check it out if you have not already. As always,
a huge thank you to all of my patrons and members who support the channel, and an especially big
thanks to a tasty snack, Adam, Andrew, Dylan, Ghost Eye 419, Jaded Flames, Justin Lowery,
Robert Bradford, and Science Punk Cell Out. I'm sorry, Ghost Eye 419, you're reading that name
as always, I'm always gonna do the thing. If you would like to join these folks in the credits,
then head on over to patreon.com slash zoe underscore b, or just click that join button
that is down there by the subscribe button. You should also hit the subscribe button if you
have not already, and also like it and share it. All the things. But it is now time for the
patron poem of the video. For Elizabeth Haste, here is the AI-generated Zoey edited poem,
a reflection on walking through the woods while high and alone. It was the summer of 1907. It was
a dark evening of the third week of April, and the long wooded stretch of land where the air
smelled like wet wood, the green hills and the cold rivers given life by a blanket of tangled
woods sprouted from the earth. I stepped out into the drowsy green, my feet bare in the shallows,
and lingered, anointing my mouth with that bitter touch, until I heard the clattering of bones and
the clicking of stone against stones, a mere and imaginary world saturated with colors that I had
hoped to find only in dreams. And until next time, stay safe, stay warm, and I will see y'all again
soon, I hope. Bye folks.
