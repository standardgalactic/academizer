Last week, we got an overview of machine learning, where we found out that, broadly speaking,
machine learning approaches are traditionally divided into three categories, which are known
as supervised learning, unsupervised learning, and reinforcement learning.
Today, we'll get an introduction to supervised learning.
More specifically, we'll get an introduction to regression, which is a self-field of supervised
learning.
But wait, didn't I cover regression in episode 205 when I covered how to plot a linear regression line?
Was that machine learning?
Or was that something else?
What exactly is regression, and why is it considered part of machine learning?
Well, let's find out.
Welcome to Julia for Talented Amateurs, where I make wholesome Julia tutorials, or Talented
Amateurs everywhere.
I am your host, the dabbling doggo.
I dabble.
As a reminder, supervised learning is when the user provides both the inputs and the outputs,
and then the computer tries to estimate the function that determines the relationship between
the inputs and the outputs.
Supervised learning may be split further between regression and classification.
In today's tutorial, we'll learn about regression.
Regression is used to understand the relationship between some independent variable x in order
to predict the value of a dependent variable y.
There's a non-machine learning approach to solving these problems.
And then there's a machine learning approach.
Today, you'll see both approaches so that you can compare and contrast the differences.
The outline for this tutorial series is loosely based on Andrew Ng's machine learning course
from Stanford University.
I watched all of the lectures so that you wouldn't have to, and I'm happy to report
that I understood maybe 10% of that course.
I'm not kidding.
It's an incredibly difficult course, but if you think you're up for the challenge,
I've provided a link to it in the description below.
There's a lot of math involved with machine learning, but for my tutorial series, there
are no special math prerequisites since I'll be covering any math that you need to know.
But I am assuming that you know the basic syntax and semantics of Julia and that you
know how to use VS Code along with the Julia extension for VS Code.
While conducting research for my tutorial series, I realized that different instructors
use different naming conventions when describing the same machine learning concepts.
For the sake of consistency, I will try my best to use the naming conventions used by
Andrew Ng.
But just be aware that you may encounter different names used by different instructors
to describe the same things in machine learning.
With that said, let's jump right into VS Code and start coding.
In your VS Code Explorer panel, create a new folder for this tutorial.
In the tutorial folder, create a new file called SL underscore regression.jl.
I'm using SL to stand for supervised learning.
Launch the Julia REPL by using ALT J then ALT O.
Maximize the terminal panel.
Change the present working directory to your tutorial directory.
Enter the package REPL by hitting the closing square bracket.
Activate your tutorial directory.
Add the following packages, CSV, GLM, plots, and type tables.
I have covered all of these packages in previous non-machine learning tutorials.
You'll be happy to know that you do not need any special packages in order to get started
with machine learning in Julia.
Type in status to confirm the version numbers.
Just as in FYI, I'm using Julia version 1.7.1.
Exit the package REPL by hitting Backspace.
Minimize the terminal panel.
I'm going to go through this section fairly quickly since this is not machine learning
and since I've already covered it in episode 205.
Watching episode 205 is not required for this tutorial.
For this tutorial, you will need a dataset that you can download from my GitHub repository.
There's a link to it in the description below.
You can save it by right clicking on it.
When you download the file, save it to your tutorial directory.
It's a CSV file called HousingData, and it contains a collection of input-output pairs
showing the relationship between the size of the houses in square feet and the price
of the houses.
I'm not sure what year this data is from nor can it confirm its accuracy, but it's
supposed to reflect housing prices in the Portland, Oregon area.
We will be using this dataset as our motivating example of using regression to predict the
price of a house based on the size of the house in square feet.
Okay, now that we have our data, let's start coding.
Start by calling the packages.
Use the CSV package to import the data from the CSV file that you just downloaded.
You can then use the type tables package to convert that data into a Julia table.
To make the numbers easier to read, let's divide the housing prices by $1,000.
The GLM package requires data to be in the form of a table.
Okay, now that we have our data set up, let's plot it and take a look at it.
Subjectively speaking, it looks like there is a linear relationship between the size
of the house and the price of the house.
Let's use the GLM package to find the best fit linear regression line using the ordinary
least squares method.
If you haven't used the GLM package before, GLM stands for generalized linear models.
The ordinary least squares method takes the square of the vertical distance between the
predictive value versus the actual value along the y-axis and then adds them up.
By using this code, the GLM package will return the values of the y-intercept and the slope
of the line that minimizes the value of the sum of the squares.
In order to see the output, you might need to maximize the REPL or scroll up.
There's a lot of information in the output, but we're only interested in the coefficients
listed in the first column of numbers.
The intercept is the value of the y-intercept, and the coefficient value listed on the second
row is the slope of the line.
Make a note of those numbers.
In a few minutes, we're going to set up a machine learning algorithm to see if our computer
can teach itself how to estimate those values.
Now that we have an ordinary least squares model, we can add it to our plot.
The predict function comes included with the GLM package, and it returns the y-values along
the linear regression line.
And that's it.
Again, subjectively speaking, it looks like that linear regression line does a pretty
good job of estimating the relationship between the size of the house and the price
of the house.
Let's use this model to predict the price of a house given a new value for the size
of a house.
So this model is estimating that a house that is 1,250 square feet will have a price around
$240,000.
So that was pretty easy, right?
If it's this easy to use linear regression without machine learning, then why do we need
machine learning?
Well, it turns out that this non-machine learning approach works great for simple datasets.
In this example, we're using the size of the house as the only independent variable to
predict the price of the house.
But the real world is, of course, more complicated than that.
What if we wanted to use more independent variables, like the number of bedrooms, the
number of floors, the age of the house, and so on?
In order to make more sophisticated predictions, we need a more sophisticated approach in order
to use all of these inputs.
That's where machine learning comes in.
Rather than sitting through a boring lecture about abstract machine learning concepts,
let's dive right into the code and learn by doing.
I'll provide the explanations along the way.
Set up a variable called epochs, or epics, either pronunciation is acceptable.
I'll explain what this is in a few minutes.
Next, let's replot our data.
You don't need to retype all of this in.
You can just rerun the code from above, but for the sake of completeness, I'm going to
copy and paste the plotting code from above.
The only difference is that I added some text to the title to include the epochs.
Next, we need to initialize the parameters.
So recall from math class that the formula for any line is y equals mx plus b, where
m is the rise over run slope of the line, and b is the y-intercept of the line.
In machine learning, both m and b are sometimes called parameters.
Other times, machine learning folks will refer to the slope m as a weight, and the y-intercept
b as a bias.
This is what I meant earlier when I mentioned that different instructors use different terminologies
when they're actually referring to the same thing.
In this tutorial, I'll be referring to both the slope m and the y-intercept b as parameters.
By changing the values of these parameters, you can change the characteristic of the line.
As you increase the value of m, then the line becomes steeper.
If you increase the value of b, then the line moves higher along the y-axis.
By changing just those two values, you can define every line in a Cartesian coordinate
plane.
In machine learning, the convention is to use theta 0 to refer to the y-intercept and
theta 1 to refer to the slope of the line.
The other convention is to initialize both theta 0 and theta 1 to the value of 0.
By setting both values to 0, that means that our initial line will be a horizontal line
along the x-axis.
Next, we use the parameters to define our linear regression model, which is just a formula
for a straight line.
A letter h refers to hypothesis, and this is the function that we'll be using to make
predictions.
I know that this formula looks a little weird with theta 0 and theta 1, but it's the same
formula for a line y equals mx plus b.
The reason for this naming convention is because, in theory, you can have more than one feature,
so it's possible to have a theta 2, a theta 3, and so on.
But for today, we're going to stick with our basic line.
Let's add this regression line to our plot.
I don't know if you can see this on the screen, but there's a blue line sitting along the
x-axis.
That's the starting point for our linear regression estimates.
As you can see, it's way off, but we're going to see if we can train our computer to figure
out a better estimate.
So how exactly are we going to train our computer?
Well, we need some way to measure how good or how bad the estimate is.
For that, we're going to need something called a cost function.
There are different cost functions that are used in machine learning, but I'm going to
use the cost function from Andrew Ng's course.
For every value of x, this cost function will calculate the vertical distance between the
predictive value of y versus the actual value of y, and then it will square that distance.
The function will then calculate the average distance between the predictive values of y
compared to the actual values of y, and then divide that by 2.
Those of you with a math background may recognize this formula as the mean squared error.
The only difference is that our cost function is divided by 2, which will make it slightly
easier to deal with partial derivatives later.
Our computer will try to improve its estimate by changing the values of theta0 and theta1.
Your computer will know when it's found a good solution to the problem when it's able
to minimize the value of this cost function.
Confused?
This will make more sense once you see it in action and can see some real numbers.
For now, let's add the code for this cost function.
Here the variable m refers to the number of samples.
It does not refer to the slope of the line.
Other instructors may use the uppercase letter n for the number of samples, but I'm using
m in order to stay consistent with Andrew Ng's notation.
The variable y hat is sometimes used for the predictive value of y, which is how it's
being used here.
This is the actual cost function that I described earlier.
The cost function is sometimes referred to as the loss function.
The uppercase letter j is sometimes used to refer to the cost function.
It's not clear to me why the uppercase j is used, but it is a convention used elsewhere
in math.
This value is the cost when the parameters theta 0 and theta 1 are both equal to 0 for
this data set.
The actual value is not important, but the change in value over time is important, so
let's keep track of the history of this value.
This is the value that we want to minimize.
So here's the million dollar question.
How is your computer supposed to figure out the minimum value of the cost function?
Well, we know that we can define any line simply by adjusting the values of the y-intercept
and the slope.
So we need to set up a process where our computer can make a slight adjustment to the values
of both the y-intercept and the slope, and then check to see if that may think better
or worse.
Based on the result, the computer can make another slight adjustment to the values of
both the y-intercept and the slope, and then check again to see if that made things better
or worse.
By following this process, your computer can, quote unquote, learn how to improve the estimate
through its experience.
Your computer can repeat this process as many times as necessary until it finds an optimal
solution.
The process that I just described is known as the gradient descent algorithm.
We'll be using a version called the batch gradient descent algorithm.
The math involved in deriving the gradient descent formulae is a bit complicated, so
I'm going to skip over it and simply present the formulae to use.
These formulae are the result of taking partial derivatives of the cost function.
You do not need to know what a partial derivative is in order to use these formulae.
This is the function that will determine how to adjust the values of the y-intercept.
And this is the function that will determine how to adjust the value of the slope.
Now we need to set the value for something called the learning rate, which is sometimes
referred to by the Greek letter alpha.
In machine learning, the learning rate is known as a hyperparameter to distinguish it
from other parameters like theta 0 and theta 1.
That's an 8 with 7 zeros in front of it.
The learning rate is used to prevent your computer from making large adjustments to the values
of either the y-intercept or the slope.
You can use any values you want for the learning rate.
I have to admit that I cheated here.
By convention, the default learning rate is set to 0.01 for all parameters, but I found
that it was taking too long to find a solution, which isn't great since I was trying to make
a tutorial out of this.
Using these values will allow your computer to find an optimal solution faster, just for
this example.
One of the issues here is that the values along the x-axis are much larger compared to the
values along the y-axis.
In future tutorials, we'll discuss methods to adjust for this, but for today, we'll leave
the values as they are.
For now, let's write the code to make this gradient descent algorithm come alive.
The use of temp variables here will allow us to make simultaneous updates to the values
of both the y-intercept as well as the slope.
So if you look at the values, you can begin to see what's going on.
The value of theta 0, or the y-intercept, has changed from 0 to 31.
Which means the new line has moved up slightly along the y-axis.
Also, the value of theta 1, or the slope, has changed from 0 to 0.06, which means that
the line has gone from being horizontal to sloping up slightly.
In theory, these changes should improve our estimate.
Now let's go back up to theta 0 temp and theta 1 temp.
The actual values are not important here, but note that both the values are negative
and note the magnitude of the values.
So what are these numbers?
Without getting into the math, here's how you can think about them.
Your computer is asking itself the question, in order to improve my estimate, in what direction,
and by what magnitude, do I need to change the values of my parameters?
Does the change need to be positive or negative?
Does the change need to be big or small?
These values provide the answers to those questions.
Let's recalculate the cost to see if this theory is true.
You see, the value of the cost dropped from around 65,000 to around 20,000.
Again, the actual value is not important, but the direction and the magnitude of the
drop is significant.
Making those slight adjustments to the values of the y-intercept and the slope made a huge
improvement in the cost.
Let's keep track of this value and then add it to our plot.
You should see a new blue line on your plot showing the latest estimate using the new
values of theta 0 and theta 1.
It's a big improvement from the initial horizontal line along the x-axis, but it's still pretty
far away from the actual data points.
This is a good time to point out that this is an iterative process, so your computer
will need to repeat these steps in order to continue to improve.
The variable named epochs is keeping track of the number of iterations.
Epoch is another example of a hyperparameter.
The term epoch is very specific in machine learning.
Technically, epoch means the number of times your machine learning algorithm has seen all
of the data.
The term iteration is used slightly differently in other machine learning scenarios, but in
this case both epoch and iteration mean the same thing.
You could write a for loop to automate this iterative process, but since we're learning
fundamentals, let's go through this manually to see how the numbers change as we repeat
this process.
Both of the value of the cost dropped again.
You should see another blue line added to your plot.
This time, it's much closer to the data points, but still appears to be off.
Let's repeat this process a few more times.
I'll fast forward through this part.
So the improvements appear to be slowing down, and that last estimate looks pretty close
to what we calculated using the GLM package.
Just to be sure, let's add the linear regression line that we calculated before.
That looks pretty close, but it's not exactly the same.
In theory, if you kept repeating this process, the machine learning model would eventually
converge with the model that we calculated using the GLM package.
If you scroll up, you can see that the last estimate for the y-intercept, or Theta 0,
is around 68, and the estimate for the slope, or Theta 1, is around 0.14.
If you maximize the ripple and scroll up, you should be able to find the coefficients
calculated by the GLM package.
GLM calculated a y-intercept of around 71 and a slope of around 0.13.
So overall, our machine learning algorithm got pretty close, which is amazing since it
figured out those values on its own.
Let's take a look at a so-called learning curve to see the progress over time.
The actual values are not important, but the overall shape of the curve is important.
The plot shows some dramatic improvements initially, but it converges near a minimum
value over time.
As a final sanity check, let's use our machine learning generated model to make a price prediction
based on a new value for the square footage of a house.
The value is displayed in the ripple.
So our machine learning model is predicting a price around $237,000.
Let's compare that with a value estimated by the GLM model.
The GLM model is predicting a price around $239,000, which is not the same, but it's
pretty close, which means that we have successfully implemented our very first machine learning
algorithm.
I know that I covered a lot today, so let's take a few minutes to recap what we just learned.
Today, we got an introduction to supervised learning.
Supervised learning is when the user provides both the inputs and the outputs, and then
the computer tries to estimate the function that determines the relationship between the
inputs and the outputs.
Supervised learning may be further split between regression and classification.
Today, we specifically learned about supervised learning using a regression algorithm.
By regression, we mean that we're trying to understand the relationship between some
independent variable x in order to predict the value of a dependent variable y.
As a motivating example, we examined a data set showing the relationship between the size
of the houses in square feet and the price of the houses.
After a quick review of how to calculate a linear regression using a traditional non-machine
learning approach, we dove right in to create a supervised machine learning algorithm that
allowed our computer to learn on its own how to derive an estimate for the linear regression line.
Some of the key elements of the machine learning workflow include defining a hypothesis model
in order to make predictions, defining a cost function in order to measure performance,
using a gradient descent algorithm to incrementally improve performance, iterating until the value
of the cost function converged on a minimum value.
For this simple example, using a machine learning algorithm may have been excessive, but working
through this motivating example allowed us to get an introduction to many of the fundamental
concepts in machine learning.
I know that I glossed over some of the explanations, but at this point, I think that it's more
important to get a feel for the terminology, the code, and the overall workflow using a
concrete example rather than getting lost in some abstract exposition.
You'll be seeing these machine learning concepts over and over throughout this series, so we'll
have an opportunity to examine them in more detail as time goes on.
For now, you can be proud that you have officially moved from theory into practice, so you are
well on your way on this exciting machine learning adventure.
Well, that's all for today.
If you made it this far, congratulations!
If you enjoyed this video and you feel like you learned something new, please give it
a thumbs up.
For more wholesome Julia tutorials, please be sure to subscribe and hit that bell.
If you like what I do, then please consider joining and becoming a channel member.
New tutorials are posted on Sundays slash Mondays.
Thanks for watching, and I'll see you in the next video.
Wow!
