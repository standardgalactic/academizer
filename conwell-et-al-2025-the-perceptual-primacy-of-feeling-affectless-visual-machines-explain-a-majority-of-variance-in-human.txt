RESEARCH ARTICLE
PSYCHOLOGICAL AND COGNITIVE SCIENCES
COMPUTER SCIENCES
OPEN ACCESS
The perceptual primacy of feeling: Affectless visual machines
explain a majority of variance in human visually evoked affect
Colin Conwella,1 ID , Daniel Grahamb ID , Chelsea Boccagnoc,d, and Edward A. Vessele ID
Edited by Terrence Sejnowski, Salk Institute for Biological Studies, La Jolla, CA; received April 14, 2023; accepted August 27, 2024
Looking at the world often involves not just seeing things, but feeling things. Modern
feedforward machine vision systems that learn to perceive the world in the absence
of active physiology, deliberative thought, or any form of feedback that resembles
human affective experience offer tools to demystify the relationship between seeing
and feeling, and to assess how much of visually evoked affective experiences may be a
straightforward function of representation learning over natural image statistics. In this
work, we deploy a diverse sample of 180 state-of-the-art deep neural network models
trained only on canonical computer vision tasks to predict human ratings of arousal,
valence, and beauty for images from multiple categories (objects, faces, landscapes, art)
across two datasets. Importantly, we use the features of these models without additional
learning, linearly decoding human affective responses from network activity in much
the same way neuroscientists decode information from neural recordings. Aggregate
analysis across our survey, demonstrates that predictions from purely perceptual models
explain a majority of the explainable variance in average ratings of arousal, valence,
and beauty alike. Finer-grained analysis within our survey (e.g. comparisons between
shallower and deeper layers, or between randomly initialized, category-supervised,
and self-supervised models) point to rich, preconceptual abstraction (learned from
diversity of visual experience) as a key driver of these predictions. Taken together, these
results provide further computational evidence for an information-processing account
of visually evoked affect linked directly to efﬁcient representation learning over natural
image statistics, and hint at a computational locus of affective and aesthetic valuation
immediately proximate to perception.
aﬀective science | cognitive neuroscience | visually evoked aﬀect | aesthetics | machine vision
For sentient biological agents, looking at the world almost always means feeling the
world. Though often studied in isolation, perception and affect are linked intimately in
everyday phenomenological experience (1), at both conscious and subconscious levels
(2, 3). Our exposure to a beautiful, moving, inviting, or aversive stimulus perhaps self-
evidently evokes processes beyond what the contemporary psychological community
typically refers to as vision, but where exactly “seeing” stops and “feeling” begins is a
question the holistic nature of human experience makes difﬁcult to answer with brain or
behavioral experiments alone.
Here, we attempt to bypass this barrier by using a large survey of visual machines—
which only see and cannot feel—to predict human affective responses to a diverse set of
natural images. In doing so, our ultimate goal is to better isolate the unique contributions
of (visual) perception to (visually evoked) affect. And because deﬁnitions are critical to
the logic of meeting this goal, we begin with this initial glossary: (visual) perception,
which we deﬁne as processes that map (visual) sensation (i.e. light reﬂected on the retina)
into mental representations whose primary referents are external objects (e.g. physical
matter); affect, which we deﬁne both as “core affect” (i.e. arousal and valence, exclusively)
and affect more broadly construed [i.e. “anything emotional” (4)]; visually evoked affect,
which we deﬁne as affect that arises in response to an external (visual) stimulus.
While other deﬁnitions abound, many theoretical frameworks of (visually evoked)
affect would typically describe “seeing with feeling” (c.f. 2) as the product of 3 interactive
processes: perception of the immediate sensory environment (e.g. seeing a bear in a
forest), physiological or bodily state change (e.g. increased heartbeat), and cognitive
interpretation of the physiological or bodily state change with respect to one’s percepts
(e.g. interpreting one’s increased heartbeat to mean “fear of the bear”) (4–6). That some
combination of perception, physiology, and cognition undergirds our feelings is a given,
but the relative contributions of each to any particular instance of affect remains a matter
of deep debate (7–9). Even well-controlled studies that scrutinize the behavioral outputs
of affect (and the intermediate brain states that produce them) suffer from the practical
Signiﬁcance
Human visual experience is
defined not only by the light
reflecting on our eyes (sensation),
but by the feelings (aﬀect) we feel
concurrently. Psychological
theories about where these
feelings come from often focus
mostly on the role of changes in
our bodily states (physiology) or
on our conscious thoughts about
the things we are seeing
(cognition). Far less frequently do
these theories focus on the role
of seeing itself (perception). In
this research, we show that
machine vision systems—which
have neither bodily states nor
conscious thoughts—can predict
with remarkable accuracy how
humans will feel about the things
they look at. This suggests that
perceptual processes (built on
rich sensory experiences) may
shape what we feel about the
world around us far more than
many psychological theories
suggest.
Author aﬀiliations: aDepartment of Psychology, Harvard
University,
Cambridge,
MA
02139;
bDepartment
of
Psychological
Science,
Hobart
and
William
Smith
Colleges;
cDepartment of Psychiatry, Massachusetts
General Hospital, Boston, MA 02114; dDepartment of
Epidemiology, Harvard T.H. Chan School of Public Health;
and
eDepartment of Psychology, City College, City
University of New York, New York, NY 10031
Author contributions: C.C., D.G., and E.A.V. designed
research; C.C. performed research; C.C. analyzed data;
and C.C., D.G., C.B., and E.A.V. wrote the paper.
The authors declare no competing interest.
This article is a PNAS Direct Submission.
Copyright © 2025 the Author(s). Published by PNAS.
This open access article is distributed under Creative
Commons Attribution License 4.0 (CC BY).
1To whom correspondence may be addressed. Email:
conwell@g.harvard.edu.
This article contains supporting information online
at https://www.pnas.org/lookup/suppl/doi:10.1073/pnas.
2306025121/-/DCSupplemental.
Published January 23, 2025.
PNAS
2025
Vol. 121
No. 4
e2306025121
https://doi.org/10.1073/pnas.2306025121
1 of 12
Downloaded from https://www.pnas.org by 159.2.192.168 on February 19, 2025 from IP address 159.2.192.168.
impossibility of fully isolating one type of process from another,
especially when the subject of study is human.
Visual machines, on the other hand, give us the empirical
control to do precisely this: that is, to disentangle perceptual
computations from the inﬂuence of physiology and cognitive
manipulation and isolate those computations we can veridically
consider “void of affect.” Again, key to understanding this control
is a proper delineation of what these machines are, and what
they are not. What these machines generally tend to be are
highly specialized, feedforward representation learners designed
to transform digital inputs (e.g. the pixels of an image) into
one of multiple numerical vectors (e.g. the one-hot embedding
of a category label). What these models by deﬁnition are not,
however, is affective—with no states that correspond under any
deﬁnition to those of a physiological body, nor the capacity for
the abstract symbol manipulations we typically associate with
deliberative thought or cognition.
The power of these machines as empirical tools for studying
“seeing with feeling,” then, is in their ability to “see” without
“feeling” anything at all. Once trained, their outputs serve
effectively as a coordinate system for triangulating the location
of a stimulus in a representational space circumscribed entirely
by four constraints: input, architecture, task, and learning rule.
By using these coordinates to predict what humans will feel
in response to visual stimuli, we can begin to approximate not
only how much of visually evoked affect may be supported by
perception alone but also how purely perceptual processes could
support affective experience in the ﬁrst place.
Candidate answers to these questions have long been pro-
posed in the literature of several ﬁelds. Decades before the
building of a “seeing” machine became a tangible reality, the
“affective computing” movement (10, 11) made great strides
in conceptualizing the ways an (artiﬁcially) intelligent system
might experience affect as a function of perceptual sense-making.
Computational aesthetics research has been another signiﬁcant
contributor to the idea [hearkening back even to Kant’s “universal
subjective” (12)] that what we feel may be modeled directly as
a function of what we see: In recent years, for example, such
research has shown that image-computable statistics and features
extracted from visual machines can successfully distinguish art
from nonart (13–18) and are directly able to predict a certain
degree of aesthetic liking and beauty (19–28). In affective science
more broadly, we see models such as “Emo-Net” (29), a modiﬁed
machine vision system that uses similar image-computable feature
extraction pipelines, predicting a large variety of other affective
and emotional responses (e.g. fear, surprise).
Other precedents may be found in applied machine learning,
where the simultaneous advent of broadly available computing
power and large-scale datasets of human image ratings [e.g. the
AVA dataset (30)] has now allowed for the engineering of models
trained directly to predict human affective and aesthetic responses
to images (31–40). These models have also been employed to
optimize graphic design or aid with visual composition (41, 42)—
a trend that has become particularly relevant with the recent
resurgence of algorithmic text-to-image pipelines and other
“generative AI” systems (e.g. OpenAI’s Dall.E3 and MidJourney)
(43, 44).
These many diverse research efforts have given us solid
methodological groundwork for exploring the intersection of
perceptual computation and affect, but none in and of themselves
answer the questions of just how far we can go in predicting
affect with perceptual computations alone, and why. Our work
attempts to ﬁll this gap by employing a comprehensive survey of
180 diverse, state-of-the-art machine vision systems to determine
the statistical upper limit on affective prediction from perceptual
computations alone, and use the variation across these models
(i.e. in terms of architecture, task, and input) to develop
a deeper understanding of how such prediction is possible
(45–47). This “model zoology” approach—applied similarly in
recent neuroscientiﬁc works (48, 49)—allows us to unify the
seminal groundwork of affective (neuro)science, computational
aesthetics, and machine learning in a single uniﬁed pipeline, and
affords us deep statistical conﬁdence in three key claims:
• The purely perceptual computations of “affect-less” visual
machines are sufﬁcient for explaining the majority of explain-
able variance in human affective responses to images.
• These affectless machines predict the majority of explainable
variance not only in arousal and valence ratings (often
considered more “physiological” in nature) but also in beauty
ratings [often considered more “cognitive” in nature (50)].
• The ability of these machines to predict these responses is
not solely a function of their ability to efﬁciently encode
the statistical properties of individual images but is instead a
function of the representations they learn from experience over
many images—in other words, their hierarchically structured
knowledge of the visual world.
Taken together, our results suggest that perceptual learning over
natural image statistics may occupy a more central place in the
ontology of visually evoked affect than previous theories have
suggested.
1. Results
Our general approach (Fig. 1) is to ﬁt linear decoding models to
the features extracted from every layer of 180 distinct deep neural
network models to assess the extent to which affect-predictive
information is inherent to these features, despite these features
never having been shaped to predict affective experiences per se.
We do this for two datasets: the open affective standardized
image set (OASIS) images [900 images with ratings of arousal,
valence, and beauty for each; (51, 52)] and the Vessel dataset [562
images with ratings of beauty only; (53)]. We contextualize the
predictive accuracies of these linear decoders by comparing their
performance to two measures of interrater variability: the mean-
minus-one correlation (i.e. the correlation of each respondent’s
ratings to the group-average ratings minus that respondent,
henceforth rMM1) and the Spearman–Brown split-half reliability
(henceforth rsplit). rMM1 we interpret (with some caveats) as a
“ceiling of shared taste” (c.f. refs. 53 and 54); rsplit we interpret
as a more traditional “noise ceiling.” Both of these thresholds
allow us to quantify how accurate our models’ predictions are,
given how accurate they could in practice be based on the level of
agreement between human respondents. Intuitively, if all human
respondents agree, both of these ceilings would be 1; if all human
respondents disagree, this ceiling would be 0. (Details on all
aspects of our approach may be found in Materials and Methods;
details on the collection of the behavioral ratings data may be
found in SI Appendix, section 1).
To “score” how accurately we can predict affect from a
candidate feature space, we use two metrics: The ﬁrst is the
Pearson correlation between the predicted ratings decoded from
the model, and the actual affect ratings provided by the human
respondents (henceforth (r(y,ˆy)). The second, which we refer to as
“explainable variance explained” (henceforth r2
EVE) is the squared
Pearson correlation between predicted and actual ratings, divided
by the squared Spearman–Brown split-half reliability:
2 of 12
https://doi.org/10.1073/pnas.2306025121
pnas.org
Downloaded from https://www.pnas.org by 159.2.192.168 on February 19, 2025 from IP address 159.2.192.168.
A
B
Fig. 1.
Overview of our approach. (A) shows a schematic of our the target human behavioral data. Respondents in the human behavioral paradigm were
asked (via one of 3 prompts, corresponding to the 3 aﬀect categories) to provide ratings for a series of images on a scale of 0 to 7. The average of these
ratings (per image) is the target of our decoding. (B) shows a schematic of the decoding pipeline. To decode the group-average aﬀect for a given image, we
extract the features for that image at each distinct computational submodule (layer) of a candidate deep neural network model, producing a series of layerwise
feature maps, the dimensionality of which we reduce with sparse random projection. We then use these dimensionality-reduced features as the predictors in a
leave-one-out cross-validated, regularized regression wherein the output is a prediction of the group-average aﬀect rating per image. We then correlate these
predictions with the actual ratings across images to obtain a single accuracy value per feature map (model layer).
r2
EVE =
r2
(y,ˆy)
r2
split
[1]
This second metric, a variant of the “noise-normalized” or
“noise-corrected” scores commonly used in the “neural bench-
marking” literature (e.g., refs. 55–57; see also ref. 58) converts
our ﬁrst-order accuracy measure (r(y,ˆy)) into a proportion of the
variance in the data that is not otherwise attributable to noise (or
in this case, to human disagreement not otherwise attributable
to the stimuli). (For more discussion of this choice of metrics, as
well as results provided in a more standard “explained variance
score,” see SI Appendix, section 2).
Unless otherwise noted, we use the following convention in the
reporting of means: arithmetic mean [lower 95% bootstrapped
CI, upper 95% bootstrapped CI]. A glossary with the expanded
forms of metric notation, dataset, and model acronyms may be
found in SI Appendix, section 1.
1.1. How Accurately Can We Decode Aﬀect from Purely
Perceptual Models? Here, we address the central question of this
work, reporting both the average and highest affect-predictive
accuracies
of
the
(ImageNet-trained)
object
recognition
networks, which in this analysis we treat as the canonical case of
a purely perceptual model.*
1.1.1. The average perceptual model predicts group-level aﬀect
far above the ceiling of “shared taste,” and over halfway to the
overall noise ceiling. Object recognition models perform well
in the prediction of group-level affect ratings across all 3 affect
categories (arousal, valence, and beauty) and across both datasets
surveyed. For the OASIS dataset, mean predictive accuracies
(r(y,ˆy)) across the N = 72 object recognition models are 0.688
[0.679, 0.697] for arousal; 0.663 [0.652, 0.675] for valence; and
0.746 [0.736, 0.755] for beauty. For the Vessel dataset (beauty
alone), mean predictive accuracy is 0.671 [0.665, 0.678].
To better contextualize these accuracies, we consider two forms
of variability in the human respondent ratings: the mean-minus-
one correlations (rMM1) and the Spearman–Brown split-half
reliability (rsplit). The ﬁrst of these, rMM1, gives us a sense of
the overall agreement among human respondents for a given set
of affect ratings. In the OASIS dataset, the mean rMM1 across
*Note that our justification in treating “category-supervised” object recognition models
as purely perceptual models is somewhat historical, and a more modern theoretical
understanding might instead favor “self-supervised models” as a more justifiable choice
for the label of “purely perceptual.” For further discussion of this point, see Section 1.2.
respondents is 0.481 [0.458, 0.504] for arousal, 0.643 [0.632,
0.654] for valence; 0.752 [0.739, 0.765] for beauty. In the Vessel
dataset (beauty only), the mean rMM1 across respondents is 0.461
[0.408, 0.513]. These values are useful reference points in this
case because they allow us to answer a number of clarifying
questions about how accurate the models are, given how accurate
the models could be with respect to the agreement among human
respondents.
The ﬁrst question we can answer with our rMM1 values is
whether higher average agreement for a given affective rating
entails higher levels of predictive accuracy. Our results plainly
suggest that it does not: rMM1 values for arousal are far lower
than for valence, but average predictive accuracies for arousal are
slightly higher than for valence.
The second question we can answer with our rMM1 values
is how “representative” of the group-average a given model’s
predicted ratings are, relative to the ratings of individual
human respondents. We deﬁne “representativeness” in this case
as the similarity of a reference (the group) to a comparand
(an individual model or human respondent). Here, we quantify
the representativeness of the perceptual models as the proportion
of respondents in the human respondent pool whose rMM1
values are lower than the value of mean predictive accuracy r(y,ˆy)
across models. Using this metric, we ﬁnd in the OASIS dataset
that model predictions are more representative of the group-
average ratings than 78%, 17%, and 74% of individual human
respondents for arousal, valence, and beauty respectively. In the
Vessel dataset, model predictions are more representative of the
group-average ratings than 100% of human respondents (i.e. no
single human respondent is as “representative” of the group as the
average perceptual model). This suggests that the average object
recognition model substantially exceeds the “ceiling of shared
taste” (mean rMM1) for every rating apart from valence, and yields
roughly the same predictive accuracies as if we were to construct
a model from the responses of the 32.5% most representative
human respondents across all our affective ratings combined.
Whereas mean-minus-one correlations quantify how well
an individual respondent’s ratings “predict” the group-average
ratings data, the Spearman–Brown split-half reliability (rsplit)
quantiﬁes how well the group-average ratings data predicts
itself. In the OASIS dataset, mean rsplit values (across 10,000
splits, Spearman–Brown corrected) are 0.963 [0.945, 0.975] for
arousal; 0.992 [0.990, 0.994] for valence; and 0.989 [0.984,
0.992] for beauty. In the Vessel dataset (beauty only), mean rsplit
is 0.862 [0.814, 0.897].
PNAS
2025
Vol. 121
No. 4
e2306025121
https://doi.org/10.1073/pnas.2306025121
3 of 12
Downloaded from https://www.pnas.org by 159.2.192.168 on February 19, 2025 from IP address 159.2.192.168.
A
B
Fig. 2.
Accuracy of model-predicted aﬀect rat-
ings. Each colored point in these plots corre-
sponds to an individual respondent or individual
model.
The
gray
horizontal
bars
are
the
Spearman–Brown split-half reliability noise ceil-
ings for the group-average aﬀect ratings, and
the shaded horizontal cross-bars are the 95%
bootstrapped CI of the mean across points; (A)
shows the accuracy of the most predictive layers
in each neural network model (with points in
orange corresponding to untrained models). For
reference, we show (in black) the mean-minus-
one correlations of individual human respon-
dents to the group average. Here, we see that
the average trained model is (for arousal and
beauty) about as predictive of group-average
aﬀect as the 32.5% most taste-typical human
respondents, and about 70% accurate overall.
Category-supervised models are no more pre-
dictive than self-supervised models, but trained
models are categorically more predictive than un-
trained models. (B) shows predictive accuracies
across layer depth for the trained category and
self-supervised models in our survey. In this plot,
the x axis is the relative depth of layer in the
neural network (from 0, the earliest layer, to 1,
the deepest layer), binned into slices of 10. The
y axis is the average predictive accuracy in that
slice, in units of rPearson. Each point is a model
trained (via category- or self-supervision) on ImageNet. Here, we see that for all 3 aﬀect categories, the deepest layers are the most predictive, with a nearly
monotonic increase in predictivity across (binned) layers.
Squaring our measure of predictive accuracy (r(y,ˆy)) and
dividing it by the square of this split-half reliability (rsplit),
we obtain our measure of explainable variance explained (r2
EVE;
Eq. 1). In the OASIS dataset, mean r2
EVE across models is 0.511
[0.498, 0.524] for arousal, 0.45 [0.434, 0.465] for valence, 0.57
[0.556, 0.584] for beauty. In the Vessel dataset (beauty only),
mean r2
EVE is 0.607 [0.595, 0.619]. Combined across affects (i.e.,
valence, arousal, and beauty), mean r2
EVE is 0.535 [0.525, 0.544].
Taken together, these results suggest that object recognition
models, never trained on affect, have nonetheless learned sta-
tistical proxies of affect sufﬁcient to predict the human affective
response to images with substantial accuracy. A summary of these
accuracies across models relative to our two interrater variability
measures is shown in Fig. 2A.
1.1.2. The most predictive perceptual models explain approx-
imately 2/3 of the explainable variance in human responses.
Though the mean performance across object recognition models
is informative, many of these models (e.g. VGG16) are now
considered obsolete in the machine learning literature. More
modern models that obtain better performance on their primary
tasks (1,000-way or 21,000-way classiﬁcation on ImageNet) can
also provide better feature spaces for decoding affect. Here, we
report the predictive accuracies for these “superlative models.”
For statistical conﬁdence, we subject the most-predictive feature
spaces from each of these models to an additional bootstrapping
analysis, wherein we resample the target group-average human
ratings 1,000 times, and rerun the affect-decoding analysis on
each of these bootstraps. The most-predictive ImageNet-1K
model (a variant of Microsoft’s Swin Transformer) yields a
bootstrapped r2
EVE (averaged across the 4 distinct combinations of
affect category and dataset) of 0.648 [0.621, 0.672]. This same
model trained on the larger ImageNet-21K benchmark yields
an average r2
EVE of 0.673 [0.649, 0.695]. The most-predictive
ImageNet-21K-trained model (and also the most predictive
model among the N = 160 purely perceptual models) is a variant
of ConvNext architecture, and yields r2
EVE of 0.729 [0.702, 0.75].
In short, the most affect-predictive purely perceptual model in
our survey explains more than 2/3 of the explainable variance in
the group-average human affect ratings.
1.2. What Kinds of Perceptual Features Are Most Predictive
of Aﬀect? Having established the overall predictive accuracy
of purely perceptual models, we can now scrutinize individual
differences between models to better assess the kinds (i.e.
provenance) of features that contribute to better or worse
prediction.
1.2.1. Trained models are categorically more predictive of aﬀect
than untrained models. Given the size, complexity, and some-
times rich structure of feature spaces inherent to deep neu-
ral networks, there have been several cases in recent years
in which randomly initialized networks—never trained—have
demonstrated predictive power as robust as that of fully trained
networks (59, 60). While not entirely surprising given certain
architectural priors (e.g. translational invariance in convolutional
neural networks), this high predictive accuracy can sometimes
lead to the impression that neural networks are little more
than stacks of random features (61) on which training has little
impact. Here, we show to the contrary that training matters.
For every ImageNet-trained model (N = 72), we compare that
model’s predictive accuracy with the accuracy of its randomly
initialized counterpart. To test signiﬁcance, we perform pairwise
t tests across each image category, affect rating, and dataset, with
Holm corrections for multiple comparisons. We ﬁnd all pairwise
differences are statistically signiﬁcant (at p < 0.001) with often
massive effect sizes (mean Hedge’s g = 7.43; see Fig. 1C). Not
a single randomly initialized model outperforms its ImageNet-
trained counterpart for any affect rating (arousal, valence, or
beauty).
1.2.2. Representations learned for object and scene recognition
are the overall best representations for predicting aﬀect. The
results we have reported so far have been exclusive to models
trained on object recognition through the ImageNet challenge.
But how does object recognition fare in relation to other tasks in
terms of providing features relevant for the prediction of affective
processes? To answer this, we ﬁrst analyze the Taskonomy models
4 of 12
https://doi.org/10.1073/pnas.2306025121
pnas.org
Downloaded from https://www.pnas.org by 159.2.192.168 on February 19, 2025 from IP address 159.2.192.168.
(62, 63), a set of 24 models all sharing the same base encoder
architecture (ResNet50) and same input data (4.5 million indoor
scenes), but each trained on 1 of 24 different canonical computer
vision tasks. These tasks include variants of autoencoding, edge
detection, keypoint labeling, segmentation, depth estimation,
surface normal estimation, and classiﬁcation, and were found
by the authors of Taskonomy to cluster into one of 4 supersets
(2D, 3D, geometric, and semantic tasks). In this analysis, we
apply our affect decoding to each of the 24 Taskonomy models
(+1 randomly initialized version of the base encoder), and rank
them (as before) according to the cross-validated accuracy of their
maximally predictive layers.
In all 3 affect categories of the OASIS dataset (arousal,
valence, and beauty) and the single affect category of the Vessel
dataset (beauty), object and scene recognition are the top 2
of the 24 (+1) task weights tested (For further details, see SI
Appendix, section 3). To assess the reliability of these rankings,
we again perform a bootstrapping analysis, resampling the human
respondent pool 1,000 times and tabulating the ranks of these
models for each bootstrap. For all 4 sets of affect ratings, object or
scene classiﬁcation was the top model in 1,000/1,000 bootstraps
(pHolm < 0.004). For arousal, a nonclassiﬁcation model was
second in 3/1,000 bootstraps. For beauty in the Vessel dataset, a
nonclassiﬁcation model was second in 272/1,000 bootstraps.
While these results do indeed suggest an advantage for
object and scene classiﬁcation as tasks undergirding better
affect prediction, it is worth noting that the raw predictive
accuracies of all the Taskonomy ResNet50 models—including
the object and scene classiﬁcation models—are substantially
lower than ResNet50 models trained on more diverse datasets
(e.g. ImageNet). Importantly, this lower overall accuracy also
underscores that training dataset (i.e. visual ecology or sensory
experience) is another signiﬁcant mediator of downstream affect
prediction. (For further discussion, see Section 1.4 and SI
Appendix, section 3.)
1.2.3. The features most predictive of aﬀect are the deepest
features. While we have so far reported only the accuracies of
the single most predictive layer per model, this leaves open the
question of where in the model these layers are located (i.e. how
deep they are). In this analysis, we assess predictive accuracy
as a function of depth in the models’ processing hierarchies.
Because the majority of the models in our survey have different
numbers of layers, we quantify layer depth here as a proportion:
the relative position of a given layer from ﬁrst to last, divided
by the total number of layers, making 0 the ﬁrst (shallowest)
layer, and 1 the last (deepest) layer. In the OASIS dataset, the
average depths (across the N = 72 ImageNet-trained models) of
the most predictive layers are 0.934 [0.915, 0.953] for arousal;
0.955 [0.929, 0.98] for valence; and 0.956 [0.938, 0.974] for
beauty (Fig. 1B). In the Vessel dataset (beauty only), the average
depth is 0.887 [0.866, 0.909].
Of course, the means here do not necessarily capture what
could be a multimodal distribution of highly predictive layers
across the network (e.g. high predictive accuracy in early layers,
low predictive accuracy in middle layers, and high predictive
accuracy again in later layers). To further quantify the relationship
between model layer depth (as regressor) and predictive power (as
regressand), we perform a linear regression for all combinations
of model, affect category, image set, and depth. In the OASIS
dataset, the mean coefﬁcients of model layer depth on overall
accuracy are 0.523 [0.5, 0.546] for arousal; 0.438 [0.414,
0.462] for valence; and 0.471 [0.45, 0.493] for beauty. In
the Vessel dataset (beauty only), the mean coefﬁcient is 0.263
[0.247, 0.278]. From the ﬁrst to last layer, the average increase
in accuracy between predicted and actual ratings (r(y,ˆy)) is
0.424 [0.408, 0.439]—a substantial effect that underscores
just how much the feature hierarchy matters for predicting
affect.†
1.2.4. Self-supervised models predict aﬀect as accurately as
category-supervised models, suggesting top–down category-learn-
ing is unnecessary for accurate aﬀect prediction. Given the
superiority of the object and scene classiﬁcation models among
the Taskonomy models, as well as the relative depth of the
most predictive layers in the ImageNet-trained models (up
to and including the softmax layers that produce category
labels), one question that arises is the degree to which ex-
plicit, “top–down” feedback is necessary for accurate affect
prediction. In this formulation, a model’s ability to predict
affect in a given image may simply be a function of the
model’s having been trained (with human labels) to distinguish
identiﬁable, affectively charged categories (e.g. faces, weapons, or
insects).
The performance of the (N = 22) self-supervised models in
our survey—trained on ImageNet images, but without explicit
category labels—allow us to address this in part. Averaging across
all affect and image categories, the mean predictive accuracy
(r(y,ˆy)) of the category-supervised models is 0.685 [0.675, 0.696];
the mean of the self-supervised models is 0.688 [0.641, 0.702].
A nonparametric Mann–Whitney test shows this difference to be
nonsigniﬁcant (W = 568, P = 0.687).
What about the performance of the most predictive models
(including those trained on image sets larger than the 1.2
million images of ImageNet)? The most predictive object-
recognition model (the ConvNext architecture trained on the 14
million images of ImageNet21K) explains 0.729 [0.702, 0.75]
of the explainable variance across ratings. The most predictive
self-supervised model (a variant of FaceBook’s RegNet-Self-
Supervised pretraining of visual features in the wild architecture
trained on 1 billion images) explains 0.688 [0.662, 0.708] of the
explainable variance across ratings. Bootstrapping the difference
between these models 1,000 times across the respondent pool,
we ﬁnd this difference to be nonsigniﬁcant (mean Δr(y,ˆy) = 0.13
[−0.03, 0.18], p = 0.64).
These results show that models trained with explicit category
supervision are as predictive of affect as models trained without.
At minimum, this suggests that a model’s ability to predict affect
is not contingent on the kinds of explicit semantic, symbolic, or
conceptual feedback inherent to “top–down” category-learning.
It does not, however, preclude the possibility that this prediction
is contingent on “bottom–up” implicit category-learning (i.e.
invariances and selectivities learned by contrastive loss): Indeed,
many of the features in adequately trained self-supervised models
that support the linear decoding of affect may also support the
linear decoding of category (64, 65). (For further discussion of
this point, as well as baseline predictions of ratings directly from
image labels, see SI Appendix, section 4.)
1.3. How Robust Is Our Prediction of Aﬀect from Perceptual
Models? In this section, we extend the decoding paradigm to
four subconditions: more directly comparing predictive accuracy
across different image categories (e.g. landscapes, scenes,
artworks); across the different affect categories (arousal, valence,
beauty); when predicting individual respondent ratings (as
†We can, for a more standard measure of eﬀect size, convert our regression coeﬀicients
to standardized betas (훽). In the OASIS dataset, 훽= 0.881 [0.85, 0.911] for arousal; 0.825
[0.793, 0.857] for valence; 0.852 [0.821, 0.884] for beauty. In the Vessel dataset (beauty
only), 훽= 0.749 [0.708, 0.79].
PNAS
2025
Vol. 121
No. 4
e2306025121
https://doi.org/10.1073/pnas.2306025121
5 of 12
Downloaded from https://www.pnas.org by 159.2.192.168 on February 19, 2025 from IP address 159.2.192.168.
A
B
C
Fig. 3.
Decoding robustness across experimen-
tal subconditions. (A) shows the average predic-
tive accuracy of the N = 72 ImageNet-trained
models across the image (super)categories avail-
able in OASIS and Vessel datasets. The shaded
rectangles in the upper half of the plot are
the 95% CIs over the Spearman–Brown split-
half reliability per image category. The shaded
crossbars at the midpoint of the boxplots are
the 95% CIs of the median model accuracy. There
are large and significant diﬀerences in predictivity
across image category, with Scenes and Land-
scapes some of the more predictable categories,
and Person and Art some of the least. The
overall predictivity of a category is proportional
to its noise ceiling (lower noise ceiling entailing
lower average accuracies; for example, in the
categories of the Vessel dataset), but diﬀerences
persist even when noise ceilings are relatively
equal (for example, in ratings of Valence for
the OASIS dataset). (B) shows average predictive
accuracies when adapting the decoding pipeline
to the ratings of individual human respondents,
rather than the group average. Each point in
this case is the average accuracy per human
respondent across the ImageNet-trained models.
While decoding fails for a nontrivial portion of
the human respondents, the median predictive
accuracy is relatively stable at rPearson = 0.4 to
0.45 across respondents. (C) shows the results
of a cross-decoding analysis between datasets,
focalizing the transferability of mappings learned
for group-average ratings on the subcategories of
the Vessel image set to the category of Scene in
the OASIS image set. In each cell is the average
cross-decoding across ImageNet-trained models (with 95% bootstrapped CIs in the brackets below). On the diagonal is the original decoding accuracy when
training and testing on the same image set. These results demonstrate that cross-decoding is most eﬀective between Scenes (in Oasis) and Landscapes (in the
Vessel dataset), arguably the only shared category between image sets.
opposed to group-averages); and when cross-decoding between
datasets (i.e. training on the images of OASIS, and predicting
affect in the images of Vessel, or vice versa). For simplicity, we
report here the results only from the N = 72 ImageNet-trained
models.
1.3.1. There are meaningful diﬀerences in our ability to predict
aﬀect across image category: Landscapes and faces are more
predictable than social scenes and art. So far, we have treated
each of our two datasets (OASIS and the Vessel image set) as
monoliths. But a more granular inspection of the (super- and
sub-) categories in each dataset reveals key idiosyncrasies, par-
ticularly with respect to how “predictable” each category is.‡
Results from this analysis are summarized in Fig. 3A. Here,
we report predictive accuracies (r(y,ˆy)) averaged over affect and
model for illustration: In the OASIS dataset (consisting of the
“Scene,” “Object,” “Person” and “Animal” categories), “Scene”
(the OASIS dataset’s name for landscape) is the most predictable
of the image categories (with a mean r(y,ˆy) of 0.756 [0.744,
0.768]); “Person” is the least predictable of the image categories
(with a mean r(y,ˆy) of 0.608 [0.596, 0.619]). In the Vessel dataset
(consisting of the “Art,” “Internal Architecture,” “External
Architecture,” “Faces” and “Landscape” categories), “Faces” are
the most predictable of the image categories (with a mean r(y,ˆy) of
0.884 [0.882, 0.886]); “Art” is the least predictable of the image
categories (with a mean r(y,ˆy) of 0.451 [0.438, 0.464]). Pairwise
comparisons with Holm corrections for multiple comparisons
show both of these differences to be signiﬁcant, with large effect
sizes (p = 4.14−25, Hedge’s g = 2.34 and p = 3.26−65,
Hedge’s g = 10.7, respectively). In a compelling internal repli-
‡Note: These categories are categories provided by the authors of the original datasets.
We made no modifications to any category from either dataset.
cation across dataset, we see no statistically signiﬁcant difference
between “Scene” in the OASIS dataset and “Landscape” in the
Vessel dataset (p = 1.0, Hedge’s g = 0.0963), the only image
category we might consider “common” to both.
The majority of these differences (especially in the Vessel
dataset) are likely attributable to the divergent levels of interrater
agreement across image category (though see SI Appendix,
section 5 for an alternative explanation). As a measure of this
divergence, we have our mean-minus-one correlations (rMM1).
Both datasets show that human respondents tend to agree
on ratings of affect most substantially for landscapes. In the
OASIS dataset, the mean rMM1 values for landscapes (“Scene”)
are 0.439 [0.413, 0.464] for arousal; 0.807 [0.796, 0.819] for
valence; and 0.697 [0.686, 0.709] for beauty. In the Vessel
dataset (beauty only), the mean rMM1 for landscapes is 0.575
[0.511, 0.640]. In contrast, the mean rMM1 for art (beauty only)
in the Vessel dataset is 0.275 [0.206, 0.344]. In short, human
respondents seem far more divided in their evaluations of art
than of landscapes—a result discussed at length in previous work
(53) (and also in SI Appendix, section 5).
1.3.2. All group-average aﬀect ratings are highly predictable;
Beauty is the most predictable of all. While the features of
perceptual models do yield relatively high predictive accuracies
for all three affect ratings, the overall highest accuracies we
obtain are in predictions of beauty. We quantify this advantage
in multiple ways, mirroring Section 1.1 above. First is the
raw difference in predictive accuracies (r(y,ˆy)): pairwise t tests
(Holm-corrected) between the three kinds of affect ratings (only
available in the OASIS dataset) reveal statistically signiﬁcant
differences in the prediction of beauty versus arousal [t(138) =
5.76, p = 1.03−7, Hedge’s g = 0.962] and in beauty versus
valence [t(140) = 7.69, p = 7.17−12, Hedge’s g = 1.28].
6 of 12
https://doi.org/10.1073/pnas.2306025121
pnas.org
Downloaded from https://www.pnas.org by 159.2.192.168 on February 19, 2025 from IP address 159.2.192.168.
Transforming these accuracies (r(y,ˆy)) into explainable variance
explained (r2
EVE), so as to account for differences in the overall
noise ceilings for each affect category, we compute the same
pairwise t tests again, and show the same statistically signiﬁcant
differences in predicting beauty versus arousal [t(139)
=
4.61, p = 8.98−6, Hedge’s g = 0.770] and beauty versus valence
[t(140) = 9.13, p = 2.09−15, Hedge’s g = 1.52]. These
effect size measures translate to an absolute difference in r2
EVE of
0.0589 [0.0518, 0.0658] and 0.120 [0.115, 0.125], respectively.
Even controlling for differences in interrater variability, then,
beauty emerges as the most predictable of the affect categories.
While the margin is small, this advantage is robust across al-
most all of the image categories in the OASIS dataset, as evidenced
by pairwise comparisons that expand from testing difference in af-
fect category alone to the interaction of affect and image category.
There are signiﬁcant differences (all p < 0.001, Holm-corrected;
mean Hedge’s g = 1.64) in beauty versus arousal and beauty ver-
sus valence in all image categories except for Animal and Object,
where only the difference in beauty versus arousal is signiﬁcant.
1.3.3. Regressions fit to individual human respondents show the
models also predict individual aﬀect ratings, with some excep-
tions. So far, we have reported results that consist of predictions
for group-average affect ratings. In the ideal, these group-averages
approximate an overall central tendency of visually evoked
affect. As with any average, though, they may also be masking
idiosyncrasies at the level of individual human respondents.
To get a better sense of how well our models might predict
individual-level affect, we extracted the most performant layers
from each of the ImageNet-trained models in our survey, and reﬁt
these layers to predict the affective responses of each individual
human respondent. For maximum statistical power, we consider
here results from only the full set of images in the OASIS image
set.§
As for the group-level predictions, predictive accuracies for
individual human respondents vary by affect category, with
beauty again emerging as the most predictable affect (with a
mean r(y,ˆy) of 0.43 [0.419, 0.439] across models and human
respondents), followed by valence (0.404 [0.389, 0.419]), then
arousal (0.381 [0.371, 0.392]). These means obscure a wide
range of accuracies across human respondents. In the case
of beauty, for example, the model’s predictions for the least
predictable respondent were actually slightly anticorrelated with
the respondent’s actual ratings, with mean accuracy (r(y,ˆy))
across models of −0.08 [−0.098, −0.063]. Mean accuracy for
the most predictable respondent was 0.674 [0.662, 0.685]. (See
Fig. 3B for the distribution of accuracies over all respondents).
What factors inﬂuence how well a model is able to predict
an individual human respondent? One central factor inherent
to our data are the correlations between an individual human
respondent’s rating and the overall group ratings – what we have
so far referred to as the mean-minus-one correlations(rMM1)
and what might (with some caveats) be considered a measure
of “shared taste” or “taste-typicality” (54).¶ To assess the
relationship between a respondent’s rMM1 and the accuracy
of a model in predicting that respondent’s ratings, we ﬁrst
average the predictive accuracies per respondent across model.
§Human respondents in the Vessel experiments rated only certain subsets (categories) of
images, making an analysis of individual ratings for the combined images impracticable.
¶The most immediate caveat here is the absence of better measures of intrarater
reliability; in other words, how consistent across time the human respondents are in their
individual ratings. In the absence of this measure, it is unclear whether an individual
respondent’s taste-atypicality is indicative of systematic divergence from the group-
average or erratic behavior at the time of assessment.
We then correlate each respondent’s rMM1 with this average.
Across all three affect ratings, we ﬁnd the relationship between
a respondent’s rMM1 and their mean r(y,ˆy) across models to
be statistically signiﬁcant: with correlations of 0.359 [0.349,
0.368] for arousal, 0.489 [0.471, 0.508] for valence, and (most
substantially) 0.679 [0.669, 0.689] for beauty.
It is worth emphasizing here that this relationship is not
accounted for by differences in the number of images that
the respondents viewed (all respondents viewed between 223
and 225 images), nor in the particular kinds of images they
viewed (since each respondent’s mean-minus-one correlation is
calculated only on the subset of images that particular respondent
viewed). Thus, the statistical power to learn each respondent’s
ratings function is constant across regressions. All that differs is
that function’s learnability: The more “divergent” a respondent’s
response proﬁle, the less predictable that respondent’s ratings
will be on the basis of the features yielded by the models. (For a
more extended discussion and interpretation of this result, see SI
Appendix, section 6.)
1.3.4. Cross-decoding: Regressions fit to one image set can often
predict the other. So far, the results we have presented here are
based on the leave-one-out cross-validated predictive accuracies
computed from the regressions ﬁt to each image set. A more
stringent test of generalizability is whether regressions ﬁt to the
beauty ratings of one image set can predict beauty ratings in the
other. To run this test, we took the most predictive feature spaces
from each of our surveyed models, and used them as a common
set of predictors for a series of regressions we iteratively ﬁt to one
image subset, then tested on another. Each of these iterations
produces a “cross-decoding accuracy:” the correlation between
the predicted and actual ratings of the held out (test) image set
when ﬁtting on the target. With all iterations complete, we obtain
what is effectively a distance matrix, wherein each cell contains
the cross-decoding accuracy between categories (For an example
of a subset of this correlation matrix, see Fig. 3C).
While each cell of this cross-decoding matrix is a result in and
of itself, the main takeaway from the matrix in its aggregate is that
where we would reasonably expect to ﬁnd high cross-decoding
accuracies, we frequently do ﬁnd high cross-decoding accuracies.
For example, let us consider the “landscape” category (scenes)—
the only image category directly shared across the image sets.
Training on landscapes from the OASIS image set produces
an average cross-decoding accuracy of 0.45 on landscapes from
the Vessel image set; vice versa, training on landscapes from the
Vessel image set produces an average cross-decoding accuracy
of 0.47 on the OASIS image set. Though these accuracies are
slightly lower than the ones we obtain when ﬁtting regressions
within image set, their similarities suggest that distributional
differences between the image sets are not so substantial as
to prevent generalization between image set—at least for
landscapes. Intuitively, features relevant for other (more speciﬁc)
image categories, like the art and faces of the Vessel dataset,
consistently fail to generalize: features ﬁt to the more general
category of landscapes from the Vessel dataset, for example,
yield an average cross-decoding accuracy of 0.311 [0.294, 0.329]
across the image categories of the OASIS image set; features ﬁt
to the more limited category of faces yield an average of 0.055
[0.038, 0.072] (see SI Appendix, Fig. 2, section 9).
1.4. Beyond Pure Perception: Exploring the Predictivity of
Hybrid Vision-Language Models. The single most predictive
purely perceptual model (out of the total 160) in our survey
yields a bootstrapped average explainable variance explained
PNAS
2025
Vol. 121
No. 4
e2306025121
https://doi.org/10.1073/pnas.2306025121
7 of 12
Downloaded from https://www.pnas.org by 159.2.192.168 on February 19, 2025 from IP address 159.2.192.168.
(r2
EVE) of 0.729 [0.702, 0.75]. What factors might account
for the approximate quarter (∼0.271) of explainable variance
this model fails to explain? One candidate is language, which
we often use to conceptualize and communicate the affect we
feel in response to things we see. In this section, we explore
the affect predictions of recently developed vision-language
models that learn representations from visual and linguistic input
simultaneously.
1.4.1. CLIP-style language-aligned vision models explain more
variance than pure vision models. Perhaps the preeminent
example of recent developments in hybrid vision-language deep
learning is OpenAI’s Contrastive Language-Image Pretraining
(CLIP) (66), a “multimodal” deep neural network model that
learns to match latent representations of a given image with
a textual caption that accompanies the image. CLIP consists
of a visual encoder (either a vision transformer or a ResNet
50/101 architecture) and a language encoder (typically a BERT
transformer). These encoders are trained simultaneously via
a contrastive loss that pits the latent space embeddings of
the encoded textual caption against the latent space of the
encoded image. This training procedure is notable for producing
representations that are both more robust to the kinds of
adversarial perturbations that typically degrade the performance
of supervised models (67) and are abstract enough, for example,
to equate a photograph and cartoon of the same visual concept
(68)—much like the famous “gnostic” neurons found in the
human temporal lobe (69).
Decoding ratings of affect from the visual encoders of 6
OpenAI-CLIP models, we ﬁnd the representations learned by
these models partially close the gap on the variance left unex-
plained by the purely perceptual models. The best performing
CLIP model (ViT-Large-Patch14) yields a bootstrapped average
r2
EVE of 0.870 [0.857, 0.881] across the 4 combinations of affect
category and dataset, versus ConvNext-Large’s average of 0.729
[0.702, 0.75]. Honing in on individual combinations of image
and affect category, we ﬁnd particularly strong gains in the Vessel
image set. The most predictive CLIP model, for example, yields
r2
EVE of 0.924 [0.637, 1.195] for ratings of the art category.
The most predictive ImageNet-trained model (ConvNext-Base)
yields r2
EVE of 0.617 [0.253, 0.977] for this same category.
1.4.2. Controlling for dataset and architecture, language-aligned
vision models still show an advantage over pure vision models.
A major caveat to the OpenAI-CLIP results is a certain level of
ambiguity as to which exact aspect of the CLIP training procedure
underlies the gains in predictivity. While the abstractions
encouraged by the natural language contrast may seem like the
prime candidate (since the underlying network architectures are
relatively standard), CLIP models are trained on 400 million text-
image pairs. ImageNet-trained models are trained on 1.1 million
images. This raises the possibility that CLIP’s gains are merely
a function of the increased number (or diversity) of training
samples (SI Appendix, section 7).
A more controlled test for the role of language in predicting
affect is available in the form of FaceBook’s Self-supervision meets
Language-Image Pretraining (SLIP) models (70): a collection of
9 models, built with 1 of 3 architectures (ViT-Small, Base, and
Large), and 1 of 3 optimization targets (SIMCLR-style purely vi-
sual self-supervision, CLIP-style language-supervision alone, and
a combination of the two—the eponymous SLIP), all trained on
the same image set (YFCC15M). Comparing across these models
allows us to better isolate the effect of language from the effects
of architecture and input (visual ecology). Bootstrapped pairwise
comparisons between these models demonstrate that pure self-
supervision (SimCLR) performs signiﬁcantly worse than pure
language-supervision (CLIP) in both the Small and Large ViT
architectures for 1,000/1,000 bootstraps (pHolm = 0.009, in both
cases), but by diminishingly small margins in both cases (with
differences in r2
EVE of 0.041 [0.026, 0.056], and 0.046 [0.028,
0.062], respectively). CLIP does not signiﬁcantly outperform
SimCLR in the Base ViT Architecture (with differences greater
than 0 in only 885/1,000 bootstraps (pHolm = 1.0), and yields av-
erage gains in r2
EVE of −0.009 [−0.025, 0.008]). SLIP (the model
that combines self- and language-supervision) outperforms CLIP
and SimCLR for all architectures (pHolm = 0.009, in all cases),
but does so most substantially in the ViT-Large architecture (with
an average gain in r2
EVE 0.121 [0.134, 0.106] over CLIP alone).
These results are a point in favor of language as a signiﬁcant factor
in building representations that are more predictive of affect, but
still do not fully account for the gains of OpenAI’s CLIP, whose
ViT-Large outperforms SLIP’s ViT-Large by another average
gain of 0.121 [0.134, 0.106]—a strong indication that visual
ecology (i.e. the images used to pretrain the model) still matters.
Importantly, we cannot consider hybrid-vision language
models like CLIP and SLIP to be “purely perceptual models.”
Though the visual encoders of these models operate at a
computational level of description almost identical to that of
the other models in our survey (a series of nonlinear, feedforward
transformations of input pixels into a lower-dimension latent
space), the language-contrastive learning target connotes a major
algorithmic divergence. While recent work has begun scrutinizing
how representation in the most anterior stages of the ventral
visual processing pathway may be “aligned” to language in
much the same way CLIP visual encoder is constrained to align
its representations with those of a language encoder (71), the
inﬂuence of linguistic abstraction on perceptual processing largely
remains a mystery. On a more practical level, the training of
CLIP on largely unﬁltered image-text pairs means that affective
information may well be implicit in the optimization function in
a way it demonstrably is not for the other models in our survey.
Consider the case in which a CLIP model learns to align the
textual caption, “a picture of a beautiful sunset,” with an image
of a sunset. The presence of affective labels in the captions could
feasibly facilitate downstream decoding. OpenAI has so far de-
clined to publicly release the dataset on which CLIP was trained,
but it is reasonable to assume these kinds of labels are present
in a statistically nontrivial portion of the training set given the
preponderance of affective tags on publicly posted online images.
The various caveats and considerations that accompany the
testing of CLIP and SLIP models mean we cannot yet conﬁdently
interpret their superiority as direct evidence that the abstractions
of language are necessary to account for the variance unexplained
by our purely perceptual models. However, the performance of
these models does suggest a concrete target for future empirical
investigation, in which self-supervised models trained on much
larger image sets are pitted directly against models trained
with CLIP’s linguistic alignment target, and ideally also trained
with further controls that account for the presence of explicitly
affective language in the textual captions.
2. Discussion
The experience of visually evoked affect (“seeing with feeling”)
is a near-universal phenomenon without a universal deﬁnition,
and one whose provenance remains a matter of hotly contested
debate (3, 9, 72–80). In many ways, the nature of this debate
is mereological: a question of how each constituent part of
8 of 12
https://doi.org/10.1073/pnas.2306025121
pnas.org
Downloaded from https://www.pnas.org by 159.2.192.168 on February 19, 2025 from IP address 159.2.192.168.
our visual affective experiences (i.e. physiology, perception,
cognition) deﬁnes the whole (i.e. the “feeling” itself). As humans,
we cannot help but experience this whole—and as scientists of
human behavior, we cannot decompose the whole back into
its constituent parts to better understand its composition. But
we can circumvent this dilemma (at least partially) by turning
to machines—whose experiences are the product of engineering
choices, and by extension, of choices within our experimental
control. In this work, we demonstrate how the emergent variation
in a large survey of industry-standard machine vision models
allows us to answer two key questions about the computational
mereology of visually evoked affect: Namely, how much of that
affect can be predicted by perceptual computations alone, and
which key ingredients of these computations allow us to make
those predictions as accurately as we do.
The various analyses of this work suggest the answer to the ﬁrst
question (“how much...”) is that perceptual computations alone
can predict a substantial majority of the average human affective
response to images. The answer to the second question (“with
which ingredients...”) is that the key to this prediction is not
just perception per se (i.e. the translation of a given sensory input
into units of behaviorally relevant “meaning”), but representation
learning: the construction of the units of meaning by way of
repeated experience with the patterns that deﬁne the sensory
world. To better grasp why this set of ﬁndings matters for our
understanding of the relationship between perception and affect
more generally, we expand on both of these answers below.
Previous work has already established that machine vision
systems are capable of predicting a wide range of “higher-order”
concepts (and associated behaviors) that many once considered
“beyond the image” (21, 22, 24, 29, 81, 82), including (most
relevantly) the prediction of affect in OASIS images (75) and
the binary classiﬁcation of art versus nonart images (18). Yet,
these studies (which focus on pixel-computable image statistics;
features extracted from single, simpler Deep Neural Network
(DNN) models; or DNN models trained end-to-end to predict
the concepts themselves) did not fully establish just how much of
these “higher-order” phenomena we could (in practice) predict—
that is, the maximum amount of variance these machines could
explain when assessed at their empirical limits. Without this
upper bound, one could easily make the argument that “higher-
order” processes (e.g. cognitive interpretation) might still be the
main or dominant determinants of “higher-order” phenomena
(e.g. affect). But if perceptual computations alone can predict
the majority of explainable variance in “higher-order” behavioral
response patterns, this argument seems quite a bit less tenable.
More important than the raw point values of these predictions,
however, is the motivation they give us to more deeply interrogate
how or even why perceptual models could predict affect so
accurately in the ﬁrst place. Having representational models as
performant as modern deep neural networks at our empirical
disposal is a relatively recent phenomenon. As such, theories that
speciﬁcally account for the behavior of these models are still
somewhat nascent. But theories preceding the advent of these
models do suggest a few reasonable starting points for further
understanding in the future. Theories of predictive processing,
for example, suggest these models may in effect be placeholders
for principled instantiations of perceptual or representational
“priors” (83–85), and that many forms of visually evoked affect
may in large part track with learning from the perceptual
“surprise” that comes with (learnable) deviation from these priors
(86–89). Another set of theories, predicated on seminal work in
“affective prediction” (3) and “microvalences” (90), suggests that
the perceptual models we survey may contain statistical proxies
of the downstream value we can extract from the objects and
affordances of our physical environment. Yet another possibility,
predicated on the computational and theoretical neuroscience
literatures, is that the perceptual models we survey may serve
as reasonable approximations of systems (like many biological
perceptual systems) that encourage or actively enforce some form
of representational sparsity (91–94).
Our own interpretation of the success of purely perceptual
models in the prediction of affect is that these models are deﬁned
by their ability to learn—and in particular, to learn behaviorally
relevant hierarchical structure directly from the statistics of nat-
uralistic input that we can derive from the same kinds of sensory
ecologies inhabited by human observers. Affect will be fundamen-
tally yoked to this learning not because affect is inherent to any
one feedforward pass of the perceptual cascade, but because affect
provides a compass both for learning that has already occurred
and for learning yet to come—whether that learning be of cate-
gories, of affordances, or even of epistemics. [Theories of aesthet-
ics, in particular, stress that something like this may be a primary
reason we experience beauty in the ﬁrst place (74, 95–101)].
This interpretation is supported by a number of the ﬁndings in
this work: Randomly initialized perceptual models, for example,
explain only a small fraction of the variance that fully trained
models explain. Models trained on datasets that do not fully
reﬂect the full statistics of natural images (i.e. the Taskonomy
models) are more predictive than randomly initialized models,
but not nearly as predictive as models trained on more diverse
datasets (such as ImageNet). The success of OpenAI’s language-
aligned CLIP model relative to other language-aligned models
seems also to be a function (in no small part) of its more extensive
training data. Our emphasis on experience here is intentional;
while modern machine vision machine systems can and should
be considered image-computable models, their ability to predict
higher-order image properties is largely not a function of their
ability to extract information from any given single image, but
is instead a function of their ability to learn where (with respect
to the representation of previously seen images) a newly seen
image should be routed. Without learning from the statistics of
experience with the same kinds of visual stimuli we regularly
encounter as humans, these models lose the vast majority of their
affect-predictive power.
Zooming out, then, if we were to situate the variance in
affective experience explained by perceptual machines along an
axis from input (e.g. pixels) to ideation (e.g. judgment), that
variance would likely fall somewhere in the middle—in line
with brain imaging work that ﬁnds correlates of affective and
aesthetic value in or near high-level visual regions (74, 102–105)
and behavioral work that favors hierarchical structure over low-
level visual properties in predicting judgments of naturalistic
stimuli (106, 107). The statistical proxies of visually evoked
affect inherent to our perceptual machines are statistical proxies
circumscribed by input, but shaped by representational priority.
These proxies by their nature will be idiosyncratic to the agent
who relies on them to calibrate behavior. This axis, we should
note, runs parallel to well-established axes in empirical aesthetics
like “formalism versus contextualism” (108, 109) and has its
natural endpoints in work that focalizes raw images on one end,
and work that focalizes pure cognition on the other (50). In
between, we ﬁnd work that focalizes everything from genetics
(110) to cultural vogue and social capital—and work like ours
(95, 111), which focalizes perceptual hierarchies.
Note that while this “resituating” of the computational basis
for “seeing with feeling” may be a story largely familiar to those
more steeped in the science of aesthetics, somewhat latent across
PNAS
2025
Vol. 121
No. 4
e2306025121
https://doi.org/10.1073/pnas.2306025121
9 of 12
Downloaded from https://www.pnas.org by 159.2.192.168 on February 19, 2025 from IP address 159.2.192.168.
our analyses is another result that may come as a bit of surprise
to those more steeped in affective science: that is, the difference
(or lack thereof) in the ability of perceptual models to predict
arousal, valence, and beauty alike. This ﬁnding partly challenges a
conceptual hierarchy of affect in which physiologically grounded
“core affect” (valence and arousal) (4) may be felt even in the
absence of higher-order “conceptual knowledge” (112) and is
in this sense distinct from more cognitively complex forms of
affect and emotion (e.g. beauty) that seem to require conceptual
“thought” (50, 113). The ﬁnding that purely perceptual models
predict both core affect and beauty suggests this theorized
hierarchy (of affective experience) might ultimately be ﬂatter
than it initially appears.
Future work, we hope, will further resolve what exactly our
decoders seem sensitive enough to detect, and in so doing will
further demystify the experience of visually evoked affect with all
the power and precision of the experimental tools researchers have
developed in the search to demystify perception. The meteoric
rise of perceptual machines may be a meaningful next step along
the road to a computationally principled, mechanistic, stimulus-
computable account of what it means to “see” with “feeling,” but
many more steps will be necessary to see, feel, and understand all
together.
3. Materials and Methods
3.1. Image Datasets. As our primary dataset, we use OASIS (51), a set of 900
imagesspanningfourdistinctcategories(people,animals,objects,andscenes),
with normed ratings of arousal and valence from 822 human respondents.
Ratings of beauty (from another 751 human respondents) were obtained from
an auxiliary source (52). In this work, we complement OASIS with a secondary
dataset consisting of 512 images across ﬁve distinct categories (art, faces,
landscapes, internal, and external architecture) (53), but for which only ratings
of beauty (“aesthetic appeal”) are available. This secondary dataset allows us not
only to explore various questions that OASIS does not (e.g. judgments of art
versus judgments of natural scenes) but to internally replicate at least a subset
of the results we obtain with OASIS.
We calculate two forms of reliability as gauges for the comparative
performanceofourmodels.Theﬁrst—“mean-minus-one”reliability(53)—involves
iteratively removing one respondent from the respondent pool and correlating
thatrespondent’sratingswiththeaverageratingsoftherespondentsremaining
(rMM1). The 95% CI over these leave-one-out correlations for all respondents
givesusasenseofhowwell,onaverage,arandomlyselectedhumanrespondent
is able to predict the mean rating for a given set of stimuli (a “shared taste”
ceiling). Our second reliability metric—the split-half reliability (rsplit)—involves
splitting the group-level data in half 10,000 times and correlating each half
withtheother.The95%CIoverthesesplit-halves(correctedwiththeSpearman–
Brownprophesyformula)providesamoreconcreteupperbound(anoiseceiling)
on how well any predictive model could do in predicting the mean rating for a
given set of stimuli. We use both of these thresholds as a point of reference for
the performance of our models.
Further details about dataset collection, including the prompts used to
solicit affective ratings and reliability calculations may be found in SI Appendix,
section 1.
3.2. Deep Neural Network (Feature) Models. Intotal,wesurveyasetof180
distinctmodels(252includingtherandomlyinitializedversionsofadesignated
subset).Thesemodelsaresourcedfromsixdifferentrepositories:theTorchvision
(PyTorch) model zoo (114); the pytorch-image-models (timm) library (115); the
Visual Self-Supervised Learning (Library) (self-supervised) model zoo (116); the
Taskonomy (visualpriors) project (62, 63, 117); OpenAI’s CLIP repository (66);
and FaceBook’s SLIP repository (70). See SI Appendix for full descriptions of the
models.
3.3. Decoding Method: Feature Regression. To predict ratings of arousal,
valence, and beauty for each of the images in our datasets from a given set of
deep net features, we use regularized linear regression with cross-validation.
Our feature regression pipeline consists of 4 distinct phases: feature extraction;
dimensionality reduction; ridge regression; cross-validation and scoring. A
schematic of this pipeline is available in Fig. 1A.
3.3.1. Feature extraction. We consider feature extraction from “every layer”
to mean the sampling of network activity generated after each distinct
computational suboperation in a deep neural network model. This means,
for example, that we consider a convolution and the nonlinearity that follows it
astwodistinctoperationsthatproducetwodistinctfeaturespaces,bothofwhich
we consider candidates for decoding. If a layer returns a tensor with multiple
components (such as a convolutional layer) we ﬁrst ﬂatten the tensor to a single
component, such that the layer represents any given image as a feature vector.
The layer thus represents a dataset of n images as an array F ∈Rn×D, where D
is the dimensions of the feature vector.
3.3.2. Sparse random projection. Forsomedeep-netlayersDisverylarge,and
as such performing ridge regression directly on F is prohibitively expensive,
with at best linear complexity with D, O(n2D) (121). Fortunately it follows from
the Johnson–Lindenstrauss lemma (122, 123) that F can be projected down to
a low-dimensional embedding P ∈Rn×p that preserves pair-wise distances
of points in F with errors bounded by a factor 휖. If u and v are any two feature
vectors from F, and up and vp are the low-dimensional projected vectors, then;
(1 −휖)||u −v||2 < ||up −vp||2 < (1 + 휖)||u −v||2
[2]
Eq. 2 holds provided that p ≥
4 ln(n)
휖2/2−휖3/3 (124). With N = 900 for our
dataset, to preserve distances with a distortion factor of 휖= 0.1 requires
≥5,830 dimensions. Thus we chose to project F to P ∈Rn×5,830 in instances
where D >> 5,830. To ﬁnd the mapping from F to P we used sparse random
projections following (125). The authors show a P satisfying Eq. 2 can be found
by P = FR, where R is a sparse, n × p matrix, with i.i.d elements
rji =



s√
D
p with prob.
1
2
√
D
0 with prob. 1 −
1
√
D
−
s√
D
p with prob.
1
2
√
D
[3]
rji in this case refers to the “random projection” (j) per feature (i). [Note that this
equation is only a slight reworking of the deﬁnition provided by Scikit-Learn
(126).]
3.3.3. Ridge regression with LOOCV. We used regularized ridge regression to
linearly decode ratings of affect, Y, from candidate (dimensionality-reduced)
deep neural network features, P. As our goal was not to identify a particular
regression model for later use, but rather to obtain a best estimate of decoding
(predictive)accuracyfromcandidatefeaturespaces,weutilizedallthedataatour
disposalwithaleave-one-out(generalized)cross-validationprocedure.Forevery
image in our dataset (∀i ∈{1 . . . 900}) we ﬁt the coefﬁcients ˆ훽i of a regression
model on the remaining data, such that Y−i = P−i ˆ훽i + 휖with minimal ∥휖∥
(error). Ridge regression penalizes large ∥ˆ훽∥proportional to a hyperparameter
휆, which is useful to prevent overﬁtting when regressors are high-dimensional
(as with P). We ﬁrst standardized Y and the columns of P to have a mean of 0
and SD of 1. Let P−i and Y−i denote P and Y with row i missing, then each ˆ훽i is
calculated by;
ˆ훽i =

P′
−iP−i + 휆Ip
−1
P′
−iY−i
[4]
Each ˆ훽i is then used to predict the beauty rating from the deep-net feature
projection of each left-out image;
ˆyi = Pi ˆ훽i,
ˆY = {ˆyi}900
i=1
[5]
The hyperparameter 휆we set at 1e4, a value we determined using a
logarithmic grid search over 1e−1 to 1e6 on an AlexNet model that we
subsequently exclude from the main analysis. 휆= 1e4 yielded the smallest
cross-validated error (∥Y −ˆY∥) when averaging across layers. We used
10 of 12
https://doi.org/10.1073/pnas.2306025121
pnas.org
Downloaded from https://www.pnas.org by 159.2.192.168 on February 19, 2025 from IP address 159.2.192.168.
the RidgeCV function from ref. 126 to implement this cross-validated ridge
regression, as its matrix algebraic implementation identiﬁes each ˆ훽i in parallel,
resulting in signiﬁcant speedups (127).
3.3.4. Scoring. In this analysis, we “score” each deep-net layer by computing
the Pearson correlation coefﬁcient between predicted ratings, ˆY, and the actual
group-averageaffectratingsfromthehumanrespondents,Y—ametricwereferto
asr(y,ˆy).ToconvertthisPearsoncorrelationcoefﬁcientintoascorethatrepresents
the proportion of “explainable variance explained,” we divide the square of
this coefﬁcient by the square of the Spearman–Brown split-half reliability that
constitutes the noise ceiling—a metric we refer to as r2
EVE.
Data, Materials, and Software Availability. Previously published data were
used for this work (51–53).
ACKNOWLEDGMENTS. We thank Talia Konkle, George Alvarez, David Field,
andtheirteamsattheHarvardVisionSciencesLaboratoryandCornellUniversity
for helpful discussion and extensive feedback. George Alvarez and Talia Konkle,
in particular, provided extensive feedback for the version of this work included
as part of Colin Conwell’s dissertation work. We are also deeply grateful to
Christopher Hamblin, whose contributions on a related conference proceeding
served as the basis for the majority of mathematical notation in Materials and
Methods.
1.
M. Merleau-Ponty, C. Smith, Phenomenology of Perception (Routledge London, 1962), vol. 26.
2.
L. F. Barrett, M. Bar, See it with feeling: Affective predictions during object perception. Philos.
Trans. R. Soc. B: Biol. Sci. 364, 1325–1334 (2009).
3.
L. F. Barrett, E. Bliss-Moreau, Affect as a psychological primitive. Adv. Exp. Soc. Psychol. 41,
167–218 (2009).
4.
L. F. Barrett, Are emotions natural kinds? Perspect. Psychol. Sci. 1, 28–58 (2006).
5.
J. R. Averill, “A constructivist view of emotion” in Theories of Emotion, (Elsevier, 1980),
pp. 305–339.
6.
J. A. Russell, L. F. Barrett, Core affect, prototypical emotional episodes, and other things called
emotion: Dissecting the elephant. J. Pers. Soc. Psychol. 76, 805 (1999).
7.
W. James, F. Burkhardt, F. Bowers, I. K. Skrupskelis, The Principles of Psychology (Macmillan
London, 1890), vol. 1.
8.
R. S. Lazarus, The cognition-emotion debate: A bit of history. Handb. Cogn. Emot. 5, 3–19 (1999).
9.
L. F. Barrett, The theory of constructed emotion: An active inference account of interoception and
categorization. Soc. Cogn. Affect. Neurosci. 12, 1–23 (2017).
10.
R. W. Picard, Affective Computing (MIT press, 2000).
11.
R. A. Calvo, S. D’Mello, J. M. Gratch, A. Kappas, The Oxford Handbook of Affective Computing
(Oxford Library of Psychology, 2015).
12.
I. Kant, Critique of Judgment (Newcomb Livraria Press, 2008).
13.
C. Redies, J. Hasenstein, J. Denzler, Fractal-like image statistics in visual art: Similarity to natural
scenes. Spat. Vis. 21, 137–148 (2007).
14.
C. Redies, A universal model of esthetic perception based on the sensory coding of natural stimuli.
Spat. Vis. 21, 97–117 (2007).
15.
D. J. Graham, D. J. Field, Statistical regularities of art images and natural scenes: Spectra,
sparseness and nonlinearities. Spat. Vis. 21, 149–164 (2007).
16.
C. Wallraven et al., Categorizing art: Comparing humans and computers. Comput. Graph. 33,
484–495 (2009).
17.
D. J. Graham, C. Redies, Statistical regularities in art: Relations with visual coding and perception.
Vision. Res. 50, 1503–1509 (2010).
18.
A. Brachmann, E. Barth, C. Redies, Using CNN features to better understand what makes visual
artworks special. Front. Psychol. 8, 830 (2017).
19.
C. Redies, S. A. Amirshahi, M. Koch, J. Denzler, “PHOG-derived aesthetic measures applied to
color photographs of artworks, natural scenes and objects” in Computer Vision – ECCV 2012:
Workshops and Demonstrations, A. Fusiello, V. Murino, R. Cucchiara, Eds. (Springer Berlin
Heidelberg, Berlin, Heidelberg, 2012), pp. 522–531.
20.
B. Mallon, C. Redies, G. U. Hayn-Leichsenring, Beauty in abstract paintings: Perceptual contrast
and statistical properties. Front. Hum. Neurosci. 8, 161 (2014).
21.
D. J. Graham, B. Schwarz, A. Chatterjee, H. Leder, Preference for luminance histogram regularities
in natural scenes. Vision. Res. 120, 11–21 (2016).
22.
A. Brachmann, C. Redies, Computational and experimental approaches to visual aesthetics. Front.
Comput. Neurosci. 11, 102 (2017).
23.
V. Hosu, B. Goldlucke, D. Saupe, “Effective Aesthetics Prediction With Multi-Level Spatially Pooled
Features” in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition
(CVPR) (2019).
24.
K. Iigaya, S. Yi, I. A. Wahle, K. Tanwisuth, J. P. O’Doherty, Aesthetic preference for art can be
predicted from a mixture of low-and high-level visual features. Nat. Hum. Behav. 5, 743–755
(2021).
25.
H. AlZayer, H. Lin, K. Bala, “AutoPhoto: Aesthetic photo capture using reinforcement learning” in
2021 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS) (IEEE, Prague,
Czech Republic, 2021), pp. 944–951.
26.
H. A. Geller, R. Bartho, K. Thömmes, C. Redies, Statistical image properties predict aesthetic
ratings in abstract paintings created by neural style transfer. Front. Neurosci. 16, 999720 (2022).
27.
A. Karjus, M. C. Solà, T. Ohm, S. E. Ahnert, M. Schich, Compression ensembles quantify aesthetic
complexity and the evolution of visual art. EPJ Data Sci. 12, 21 (2023).
28.
S. Nara, D. Kaiser, Integrative processing in artiﬁcial and biological vision predicts the perceived
beauty of natural images. Sci. Adv. 10, eadi9294 (2024).
29.
P. A. Kragel, M. C. Reddan, K. S. LaBar, T. D. Wager, Emotion schemas are embedded in the
human visual system. Sci. Adv. 5, eaaw4358 (2019).
30.
N. Murray, L. Marchesotti, F. Perronnin, “AVA: A large-scale database for aesthetic visual analysis”
in 2012 IEEE Conference on Computer Vision and Pattern Recognition (IEEE, Providence, RI, 2012),
pp. 2408–2415.
31.
X. Lu, Z. Lin, H. Jin, J. Yang, J. Z. Wang, Rating image aesthetics using deep learning. IEEE Trans.
Multimed. 17, 2021–2034 (2015).
32.
Z. Dong, X. Shen, H. Li, X. Tian, “Photo quality assessment with DCNN that understands image
well” in MultiMedia Modeling, X. He et al., Eds. (Springer International Publishing, Cham, 2015),
pp. 524–535.
33.
X. Lu, Z. Lin, H. Jin, J. Yang, J. Z. Wang, RAPID: “Rating pictorial aesthetics using deep learning” in
Proceedings of the 22nd ACM International Conference on Multimedia (Association for Computing
Machinery, New York, NY, 2014), pp. 457–466.
34.
S. Bianco, L. Celona, P. Napoletano, R. Schettini, “Predicting image aesthetics with deep learning”
in Advanced Concepts for Intelligent Vision Systems, J. Blanc-Talon, C. Distante, W. Philips,
D. Popescu, P. Scheunders, Eds. (Springer International Publishing, Cham, 2016), pp. 117–125.
35.
W. Wang et al., A multi-scene deep learning model for image aesthetic evaluation. Signal Process.
Image Commun. 47, 511–518 (2016).
36.
S. Kong, X. Shen, Z. Lin, R. Mech, C. Fowlkes, “Photo aesthetics ranking network with attributes
and content adaptation” in Computer Vision – ECCV 2016, B. Leibe, J. Matas, N. Sebe, M. Welling,
Eds. (Springer International Publishing, Cham, 2016), pp. 662–679.
37.
K. Sheng et al., “Attention-based multi-patch aggregation for image aesthetic assessment” in
Proceedings of the 26th ACM International Conference on Multimedia (Association for Computing
Machinery, New York, NY, 2018), pp. 879–886
38.
L. Goetschalckx, A. Andonian, A. Oliva, P. Isola, “GANalyze: Toward visual deﬁnitions of cognitive
image properties” in Proceedings of the IEEE/CVF International Conference on Computer Vision
(ICCV) (2019).
39.
H. Jang, J. S. Lee, Analysis of deep features for image aesthetic assessment. IEEE Access 9,
29850–29861 (2021).
40.
J. McCormack, A. Lomas, Deep learning of individual aesthetics. Neural Comput. Appl. 33, 3–17
(2021).
41.
H. Talebi, P. Milanfar, Nima: Neural image assessment. IEEE Trans. Image Process. 27, 3998–4011
(2018).
42.
P. Lelièvre, P. Neri, A deep-learning framework for human perception of abstract art composition.
J. Vis. 21, 9–9 (2021).
43.
R. Rombach, A. Blattmann, D. Lorenz, P. Esser, B. Ommer, “High-resolution image synthesis
with latent diffusion models” in Proceedings of the IEEE/CVF Conference on Computer Vision and
Pattern Recognition (2022), pp. 10684–10695.
44.
A. Ramesh, P. Dhariwal, A. Nichol, C. Chu, M. Chen, Hierarchical text-conditional image
generation with clip latents. arXiv [Preprint] (2022). https://doi.org/10.48550/arXiv.2204.06125
(Accessed 5 May 2023).
45.
R. Cao, D. Yamins, Explanatory models in neuroscience: Part 1–taking mechanistic
abstraction seriously. arXiv [Preprint] (2021). https://doi.org/10.48550/arXiv.2104.01490
(Accessed 5 May 2023).
46.
R. Cao, D. Yamins, Explanatory models in neuroscience: Part 2–constraint-based intelligibility.
arXiv [Preprint] (2021). https://doi.org/10.48550/arXiv.2104.01489 (Accessed 5 May 2023).
47.
N. Kanwisher, M. Khosla, K. Dobs, Using artiﬁcial neural networks to ask ‘why’ questions of minds
and brains. Trends Neurosci. 46, 240–254 (2023).
48.
C. Conwell et al., Neural regression, representational similarity, model zoology & neural
taskonomy at scale in rodent visual cortex. Adv. Neural. Inf. Process. Syst. 34, 5590–5607
(2021).
49.
C. Conwell, J. S. Prince, K. N. Kay, G. A. Alvarez, T. Konkle, What can 1.8 billion regressions tell
us about the pressures shaping high-level visual representation in brains and machines? bioRxiv
[Preprint] (2023). https://doi.org/10.1101/2022.03.28.485868.
50.
A. A. Brielmann, D. G. Pelli, Beauty requires thought. Curr. Biol. 27, 1506–1513 (2017).
51.
B. Kurdi, S. Lozano, M. R. Banaji, Introducing the open affective standardized image set (oasis).
Behav. Res. Methods 49, 457–470 (2017).
52.
A. A. Brielmann, D. G. Pelli, Intense beauty requires intense pleasure. Front. Psychol. 10, 2420
(2019).
53.
E. A. Vessel, N. Maurer, A. H. Denker, G. G. Starr, Stronger shared taste for natural aesthetic
domains than for artifacts of human culture. Cognition 179, 121–131 (2018).
54.
Y. C. Chen et al., “Taste typicality” is a foundational and multi-modal dimension of ordinary
aesthetic experience. Curr. Biol. 32, 1837–1842.e3 (2022).
55.
M. Schrimpf et al., Integrative benchmarking to advance neurally mechanistic models of human
intelligence. Neuron 108, 413–423 (2020).
56.
S. A. Cadena et al., Deep convolutional models improve predictions of macaque v1 responses to
natural images. PLoS Comput. Biol. 15, e1006897 (2019).
57.
C. Zhuang et al., Unsupervised neural network models of the ventral visual stream. Proc. Natl.
Acad. Sci. U.S.A. 118, e2014196118 (2021).
58.
D. A. Pospisil, W. Bair, The unbiased estimation of the fraction of variance explained by a model.
PLoS Comput. Biol. 17, e1009212 (2021).
59.
S. A. Cadena et al., “How well do deep neural networks trained on object recognition characterize
the mouse visual system?” in Real Neurons & Hidden Units Future Directions at the Intersection of
Neuroscience and Artiﬁcial Intelligence@ NeurIPS 2019 (2019).
60.
K. R. Storrs, T. C. Kietzmann, A. Walther, J. Mehrer, N. Kriegeskorte, Diverse deep neural networks
all predict human inferior temporal cortex well, after training and ﬁtting. J. Cogn. Neurosci. 33,
2044–2064 (2021).
61.
A. Rahimi, B. Recht, “Random features for large-scale kernel machines” in Advances in Neural
Information Processing Systems, J. Platt, D. Koller, Y. Singer, S. Roweis, Eds. (Curran Associates,
Inc., 2007).
62.
A. R. Zamir et al., “Taskonomy: Disentangling task transfer learning” in Proceedings of the IEEE
Conference on Computer Vision and Pattern Recognition (CVPR) (2018).
PNAS
2025
Vol. 121
No. 4
e2306025121
https://doi.org/10.1073/pnas.2306025121
11 of 12
Downloaded from https://www.pnas.org by 159.2.192.168 on February 19, 2025 from IP address 159.2.192.168.
63.
A. Sax et al., Learning to navigate using mid-level visual priors. arXiv [Preprint] (2019).
https://doi.org/10.48550/arXiv.1912.11121 (Accessed 16 October 2022).
64.
Z. Wu, Y. Xiong, S. X. Yu, D. Lin, “Unsupervised feature learning via non-parametric instance
discrimination” in Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition
(CVPR) (2018).
65.
T. Chen, S. Kornblith, K. Swersky, M. Norouzi, G. Hinton, Big self-supervised models are strong
semi-supervised learners. arXiv [Preprint] (2020). https://doi.org/10.48550/arXiv.2006.10029
(Accessed 5 May 2023).
66.
A. Radford et al., “Proceedings of machine learning research” in Proceedings of the 38th
International Conference on Machine Learning, M. Meila, T. Zhang, Eds. (PMLR, 2021), vol. 139,
pp. 8748–8763.
67.
C. Schlarmann, M. Hein, “On the adversarial robustness of multi-modal foundation models”
in Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV) Workshops
(2023), pp. 3677–3685.
68.
G. Goh et al., Multimodal neurons in artiﬁcial neural networks. Distill 6, e30 (2021).
69.
R. Q. Quiroga, L. Reddy, G. Kreiman, C. Koch, I. Fried, Invariant visual representation by single
neurons in the human brain. Nature 435, 1102–1107 (2005).
70.
N. Mu, A. Kirillov, D. Wagner, S. Xie, Slip: Self-supervision meets language-image pre-training.
arXiv [Preprint] (2021). https://doi.org/10.48550/arXiv.2112.12750 (Accessed 8 February 2023).
71.
S. F. Popham et al., Visual and linguistic semantic representations are aligned at the border of
human visual cortex. Nat. Neurosci. 24, 1628–1636 (2021).
72.
R. Reber, “Processing ﬂuency, aesthetic pleasure, and culturally shared taste” in Aesthetic Science:
Connecting Minds, Brains, and Experience (2012), pp. 223–249.
73.
S. E. Palmer, K. B. Schloss, J. Sammartino, Visual aesthetics and human preference. Annu. Rev.
Psychol. 64, 77–107 (2013).
74.
A. I. Isik, E. A. Vessel, From visual perception to aesthetic appeal: Brain responses to aesthetically
appealing natural landscape movies. Front. Hum. Neurosci. 15, 414 (2021).
75.
C. Redies, M. Grebenkina, M. Mohseni, A. Kaduhm, C. Dobel, Global image properties predict
ratings of affective pictures. Front. Psychol. 11, 953 (2020).
76.
M. Skov, M. Nadal, There are no aesthetic emotions: Comment on Menninghaus et al. (2019).
Psychol. Rev. 127, 640–649 (2020).
77.
W. Menninghaus et al., What are aesthetic emotions? Psychol. Rev. 126, 171 (2019).
78.
D. J. Graham, “The use of visual statistical features in empirical aesthetics” in The Oxford
Handbook of Empirical Aesthetics (Oxford University Press, 2019), vol. 19, https://doi.org/10.
1093/oxfordhb/9780198824350.013.
79.
A. Chatterjee, The Aesthetic Brain: How We Evolved to Desire Beauty and Enjoy Art (Oxford
University Press, 2014).
80.
E. A. Vessel, “Neuroaesthetics” in Encyclopedia of Behavioral Neuroscience, S. Della Sala, Ed.
(Elsevier, 2022), vol. 3, pp. 661–670.
81.
D. J. Graham, J. D. Friedenberg, C. H. McCandless, D. N. Rockmore, “Preference for art: similarity,
statistics, and selling price” in Human Vision and Electronic Imaging XV, B. E. Rogowitz, T. N.
Pappas, Eds. (SPIE, 2010), pp. 75271A.
82.
S. Hentschel, K. Kobs, A. Hotho, Clip knows image aesthetics. Front. Artif. Intell. 5, 976235
(2022).
83.
R. P. Rao, D. H. Ballard, Predictive coding in the visual cortex: A functional interpretation of some
extra-classical receptive-ﬁeld effects. Nat. Neurosci. 2, 79–87 (1999).
84.
K. Friston, J. Kilner, L. Harrison, A free energy principle for the brain. J. Physiol. Paris 100, 70–87
(2006).
85.
A. Clark, Whatever next? Predictive brains, situated agents, and the future of cognitive science
Behav. Brain Sci. 36, 181–204 (2013).
86.
L. I. Perlovsky, Toward physics of the mind: Concepts, emotions, consciousness, and symbols.
Phys. Life Rev. 3, 23–55 (2006).
87.
S. Van de Cruys, J. Wagemans, Putting reward in art: A tentative prediction error account of visual
art. i-Perception 2, 1035–1062 (2011).
88.
J. Schmidhuber, “Driven by compression progress: A simple principle explains essential aspects
of subjective beauty, novelty, surprise, interestingness, attention, curiosity, creativity, art, science,
music, jokes” in Anticipatory Behavior in Adaptive Learning Systems, G. Pezzulo, M. V. Butz,
O. Sigaud, G. Baldassarre, Eds. (Springer Berlin Heidelberg, Berlin, Heidelberg, 2009), pp. 48–76.
89.
V. K. Cheung et al., Uncertainty and surprise jointly predict musical pleasure and amygdala,
hippocampus, and auditory cortex activity. Curr. Biol. 29, 4084–4092.e4 (2019).
90.
S. Lebrecht, M. Bar, L. F. Barrett, M. J. Tarr, Micro-valences: Perceiving affective valence in
everyday objects. Front. Psychol. 3, 107 (2012).
91.
B. A. Olshausen, D. J. Field, Sparse coding of natural images produces localized, oriented,
bandpass receptive ﬁelds. Nature 381, 607–609 (1995).
92.
A. J. Bell, T. J. Sejnowski, The “independent components” of natural scenes are edge ﬁlters. Vision.
Res. 37, 3327–3338 (1997).
93.
J. M. Hughes, D. J. Graham, D. N. Rockmore, Quantiﬁcation of artistic style through sparse coding
analysis in the drawings of pieter bruegel the elder. Proc. Natl. Acad. Sci. U.S.A. 107, 1279–1283
(2010).
94.
D. J. Graham, D. J. Field, Sparse coding in the neocortex. Evol. Nerv. Syst. 3, 181–187
(2006).
95.
I. Biederman, E. A. Vessel, Perceptual pleasure and the brain: A novel theory explains why the
brain craves information and seeks it through the senses. Am. Sci. 94, 247–253 (2006).
96.
S. Zeki, J. P. Romaya, D. M. Benincasa, M. F. Atiyah, The experience of mathematical beauty and
its neural correlates. Front. Hum. Neurosci. 8, 68 (2014).
97.
E. Cetinic, T. Lipic, S. Grgic, A deep learning perspective on beauty, sentiment, and remembrance
of art. IEEE Access 7, 73694–73710 (2019).
98.
P. Sarasso et al., Beauty in mind: Aesthetic appreciation correlates with perceptual facilitation and
attentional ampliﬁcation. Neuropsychologia 136, 107282 (2020).
99.
A. A. Brielmann, M. Berentelg, P. Dayan, Modelling individual aesthetic judgements over time.
Philos. Trans. R. Soc. B 379, 20220414 (2024).
100. S. Van de Cruys et al., Visual affects: Linking curiosity, Aha-Erlebnis, and memory through
information gain. Cognition 212, 104698 (2021).
101. S. Van de Cruys, J. Frascaroli, K. Friston, Order and change in art: Towards an active inference
account of aesthetic experience. Philos. Trans. R. Soc. B 379, 20220411 (2024).
102. X. Yue, E. A. Vessel, I. Biederman, The neural basis of scene preferences. NeuroReport 18,
525–529 (2007).
103. A. Chatterjee, A. Thomas, S. E. Smith, G. K. Aguirre, The neural response to facial attractiveness.
Neuropsychology 23, 135–143 (2009).
104. E. A. Vessel, G. G. Starr, N. Rubin, The brain on art: Intense aesthetic experience activates the
default mode network. Front. Hum. Neurosci. 6, 1–17 (2012).
105. O. Vartanian, V. Goel, Neuroanatomical correlates of aesthetic preference for paintings.
NeuroReport 15, 893–897 (2004).
106. E. A. Vessel, N. Rubin, Beauty and the beholder: Highly individual taste for abstract, but not
real-world images. J. Vis. 10, 1–14 (2010).
107. A. Schepman, P. Rodway, S. J. Pullen, J. Kirkham, Shared liking and association valence for
representational art but not abstract art. J. Vis. 15, 1–10 (2015).
108. A. P. Shimamura, I. Shimamura, “Toward a science of aesthetics” in Aesthetic Science: Connecting
Minds, Brains and Experiences (2012), pp. 3–28.
109. C. Redies, Combining universal beauty and cultural context in a unifying model of visual aesthetic
experience. Front. Hum. Neurosci. 9, 218 (2015).
110. L. Germine et al., Individual aesthetic preferences for faces are shaped mostly by environments,
not genes. Curr. Biol. 25, 2684–2689 (2015).
111. H. Leder, B. Belke, A. Oeberst, D. Augustin, A model of aesthetic appreciation and aesthetic
judgments. Br. J. Psychol. 95, 489–508 (2004).
112. S. Duncan, L. F. Barrett, Affect is a form of cognition: A neurobiological analysis. Cogn. Emot. 21,
1184–1211 (2007).
113. I. Schindler et al., Measuring aesthetic emotions: A review of the literature and a new assessment
tool. PLoS One 12, e0178899 (2017).
114. A. Paszke et al., “Pytorch: An imperative style, high-performance deep learning library” in
Advances in Neural Information Processing Systems 32, H. Wallach et al., Eds. (Curran Associates,
Inc., 2019), pp. 8024–8035.
115. W. Ross, PyTorch image models. GitHub. https://github.com/rwightman/pytorch-image-models.
Accessed 16 October 2023.
116. P. Goyal et al., VISSL. GitHub. https://github.com/facebookresearch/vissl. Accessed 16 October
2023.
117. A. Sax et al., Mid-level visual representations improve generalization and sample efﬁciency for
learning visuomotor policies. arXiv [Preprint] (2018). https://doi.org/10.48550/arXiv.1812.11971
(Accessed 16 October 2023).
118. M. Caron et al., “Emerging properties in self-supervised vision transformers” in Proceedings of the
IEEE/CVF International Conference on Computer Vision (ICCV) (2021), pp. 9650–9660.
119. P. Goyal et al., Self-supervised pretraining of visual features in the wild. arXiv [Preprint] (2021).
https://doi.org/10.48550/arXiv.2103.01988 (Accessed 16 October 2023).
120. P. Goyal et al., Vision models are more robust and fair when pretrained on uncurated images
without supervision. arXiv [Preprint] (2022). https://doi.org/10.48550/arXiv.2202.08360
(Accessed 16 October 2023).
121. T. Hastie, R. Tibshirani, Efﬁcient quadratic regularization for expression arrays. Biostatistics 5,
329–340 (2004).
122. W. B. Johnson, Extensions of lipschitz mappings into a hilbert space. Contemp. Math. 26,
189–206 (1984).
123. S. Dasgupta, A. Gupta, An elementary proof of a theorem of Johnson and Lindenstrauss. Random
Struct. Algorithms 22, 60–65 (2003).
124. D. Achlioptas, “Database-friendly random projections” in Proceedings of the Twentieth ACM
SIGMOD-SIGACT-SIGART Symposium on Principles of Database Systems (Association for
Computing Machinery, New York, NY, 2001), pp. 274–281.
125. P. Li, T. J. Hastie, K. W. Church, “Very sparse random projections” in Proceedings of the 12th
ACM SIGKDD International Conference on Knowledge Discovery and Data Mining (Association for
Computing Machinery, New York, NY, 2006), pp. 287–296.
126. F. Pedregosa et al., Scikit-learn: Machine learning in Python. J. Mach. Learn. Res. 12, 2825–2830
(2011).
127. R. M. Rifkin, R. A. Lippert, “Notes on regularized least squares” in Computer Science and Artiﬁcial
Intelligence Lab Technical Reports (2007).
12 of 12
https://doi.org/10.1073/pnas.2306025121
pnas.org
Downloaded from https://www.pnas.org by 159.2.192.168 on February 19, 2025 from IP address 159.2.192.168.
