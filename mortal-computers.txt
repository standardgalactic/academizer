Mortal Computers

Conversation with Gemini


This section dives deeper into the technical aspects of building a mortal computer based on biological principles. Here's a breakdown of the key points:

Biomimicry in Hardware Design:

The hardware design of a mortal computer should mimic biological systems like cells.

This includes:
A defining boundary (like a cell membrane) to separate the system from its environment.

A way to synthesize new components and repair itself using raw materials from the environment.

Transducer-actuator subsystems to interact with the environment (e.g., sensors and motors).

Functional Levels of a Mortal Computer (Figure 2):

The system can be conceptually divided into different functional levels.

The lowest level includes essential variables like power levels and temperature, managed by homeostatic control mechanisms.

The middle level deals with predictive regulation, including allostasis (proactive adjustments based on the environment).

The highest level focuses on autopoiesis, allowing the system to self-replicate and maintain its identity.

Homeostatic Control:

This level ensures internal stability by reacting to changes in essential variables and keeping them within a certain range (set points).

An example is regulating the firing rate of neurons in a spiking neural network to optimize efficiency.

Allostasis:

This level predicts future changes and makes adjustments beforehand using mechanisms like leaky integrators in neuromorphic chips.

It can also modify homeostatic set points based on sensory information.

Autopoietic Autonomy:

This top level is responsible for self-replication and repair, allowing the system to maintain its existence over time.

It includes processes for regeneration, repair, and potentially even self-creation of new components.

Overall, this section describes how a mortal computer can be built with a layered structure that mimics biological processes for self-maintenance, adaptation, and self-replication.



Another foundational aspect of the mortal computer in Figure 2 (Top) is its sensorimotor dependencies. This is motivated by metabolic-dependent/based processing and adaptivity in biology [111]: the central impetus behind a cell’s motility, e.g., chemotaxis [349, 112] or phototaxis [187, 32], and its interaction with its niche is its metabolic drive [88, 324, 12]. Sensors/transducers and effectors/actuators exhibit, to varying degrees depending on the design of the system and the morphology afforded by it, dependencies on metabolic functions and regulatory processes. In Figure 2, these are labeled as ‘allostatic influence’, ‘homeostatic influence’, and ‘metabolic influence’. For instance, the operation of a motor subsystem might be conditioned on a subset of essential variables, the setpoints of several homeostatic comparators, and control signals emitted from one or more allostatic controllers. How such dependencies modulate or alter the functionality of a particular sensory or actuator subsystem would depend on the system’s substrate as well as its niche. It is through the sensorimotor pathway that energy/matter is exchanged with the mortal computer as it engages with its niche. Finally, notice in Figure 2 (Top) that the dotted bounding box that encloses the mortal computer’s functional levels and sensorimotor pathways is labeled as ‘organizationally closed’ – a mortal computer should operate solely on the basis of its own self-reproduced structures, rather than on the inputs it directly receives. While it is important for a mortal computer to be organizationally closed, this does not commit a designer to a specific physical manifestation. Simply put, organization refers to the relationships between the system’s processes and parts. The mortal computer’s structure refers to the physical instantiation of the constituent components and their relationships. One organization can be manifested in many structural forms. This means that, since it is autopoietic, a mortal computer’s structure might change with time, but its organization persists. A mortal computer’s responses are dependent on — and determined by — its structure but also triggered by environmental events. This means a mortal computer, like a living system, is constantly interacting with and thus coupled to its world as indicated by the directed arrows of ‘sensing’, ‘actuation’, and ‘energy exchange’ in Figure 2 (Top). In short, the full story of a mortal computer includes its niche: the mortal computer and its environment mutually perturb each other and trigger changes in the state of one another. 
This section elaborates on the design principles of a mortal computer based on its interaction with the environment. Here's a breakdown of the key points:

Sensorimotor Dependencies and Niche Construction:

Similar to biological cells, mortal computers would have sensors and actuators to interact with their environment.

These interactions would be driven by the need to sustain the system's metabolism and adapt to the environment (niche).

The design of these interactions would depend on the specific hardware and its environment.

Organizationally Closed vs. Physically Open:

A core principle is "organizational closure" - the system should function based on its internal structure, not just external inputs.

This doesn't mean a fixed physical form - the structure can change over time while the underlying organization remains.

The system's responses are still influenced by the environment, similar to living things.

Co-dependence with the Niche:

The mortal computer and its environment have a two-way relationship, where they influence each other's state.

In essence, the system and its niche co-evolve over time.

Overall, this section emphasizes the importance of the environment in shaping the behavior of a mortal computer. The system would constantly interact and adapt to its surroundings, similar to how living things exist within their ecological niches.


This section dives into the cybernetic underpinnings of mortal computation, focusing on how the system learns and adapts. Here's a breakdown of the key points:

Cybernetics and Mortal Computation:

Cybernetics emphasizes that a system's structure reflects its environment and influences its control processes.

This aligns with the biological concept of homeostasis, where an organism's structure maintains internal stability.

A core idea in cybernetics is "retroaction" - the system's actions are guided by its goals and past experiences.

Learning and Adaptation:

A key question is how a system learns to act effectively in its environment.

This section proposes a "cybernetics backend" for mortal computation to address this.

Cybernetic Principles:

The core idea is that complex systems emerge from the interactions of simple parts.

These interactions create a hierarchy where lower-level parts influence and are influenced by higher-level ones (upward and downward causation).

A key concept is "variety" - the number of possible states the system can be in.

Selection processes determine which states are stable and which are not.

Three Principles for Mortal Computation:

Stability: The system maintains its internal order and avoids self-destruction.

Growth: The system increases its complexity and capabilities over time.

Regulation/Control: The system uses feedback mechanisms to adjust its behavior based on its goals and the environment.

These principles, depicted in Figure 2 (Bottom), provide a framework for how a mortal computer can learn and adapt through its interactions with the environment.


This section explores how a mortal computer can achieve stability and growth through cybernetic principles. Here's a breakdown of the key points:

The Importance of Stability:

A core objective for a mortal computer is to reach stable states - configurations that it can return to after minor disturbances.

The ideal state is "ultrastability" where the system can maintain its structure and function even with environmental changes.

This involves:
Asymmetric transition: moving from unstable to stable states but not vice versa.

Selective variety: exploring a variety of configurations to find stable ones.

Order from noise: random fluctuations can help discover more stable states.

Blind variation and selective retention (BVSR): exploring variations and keeping the ones that lead to stability.

System Growth:

Mortal computers, like living things, should be able to grow and replicate.

This connects to the concept of autopoiesis - self-creation and maintenance.

From a cybernetic perspective:
Morphological systems: leverage their physical structure for computations.

Morphogenesis: the ability to change structure based on interactions with the environment.

Self-replication: creating copies of itself or its components.

How Growth Happens:

Replication combined with an environment with finite resources leads to competition and natural selection.

Systems that are more stable and efficient at using resources are more likely to survive.

This, along with BVSR, leads to the self-assembly of a system with a distinct boundary from its environment (autopoiesis).

Recursive system growth: stable building blocks can combine to form more complex systems.

This can lead to hierarchical and heterarchical systems with different levels of organization.

Overall, this section describes how a mortal computer can achieve stability through self-regulation and grow in complexity by combining and adapting its building blocks. This mirrors how living things evolve and become more sophisticated over time.


This section explains how a mortal computer interacts and adapts to its environment using cybernetic principles. Here's a breakdown of the key points:

Regulation and Control Principles:

Three key ideas govern how a mortal computer interacts with its environment:
Law of Requisite Variety: The system needs enough internal complexity to handle the complexity of its environment.

Good Regulator Theorem: The system needs a model of its environment to effectively control it.

Internal Model Control: The system maintains stability while regulating its environment based on a reference model.

Law of Requisite Variety:

A mortal computer needs to be adaptable to handle variations in its environment.

This means having enough variety in its internal states to respond to external changes.

Essentially, the system's internal complexity should match the complexity of the environment.

Good Regulator Theorem:

To effectively control its environment, the mortal computer needs to understand it.

This understanding comes from building an internal model of the environment.

The system essentially learns and adapts based on this model.

Internal Model Control:

This principle focuses on how the system maintains stability while influencing its environment.

It involves two key aspects:
Closed-loop stability: The system can recover from disturbances and maintain its internal balance.

Reference signal tracking: The system can adjust its environment to achieve a desired outcome.

Cybernetic Framing of a Mortal Computer:

This section explains how the above principles work together in a mortal computer (refer to Figure 2, Bottom).

The system is composed of lower-level parts that interact with each other to form higher-level units.

The goal is to reach stable configurations with low energy consumption.

Random fluctuations in the environment help the system discover even more stable states.

The environment plays a crucial role by providing resources for self-generation and adaptation.

As the system adapts and replicates, it becomes more complex and better suited to its niche.

Through self-assembly and morphogenesis, the system maintains enough internal variety to handle environmental changes.

The mortal computer constantly interacts with the physical world, which is in a state of perpetual change.

By adapting its internal states, the system maintains its organization and gathers information from its surroundings.

Overall, this section highlights how a mortal computer is not a passive observer but actively interacts and learns from its environment. This continuous adaptation allows the system to become more sophisticated and resilient over time.


This section dives into the philosophical underpinnings of mortal computation using a concept called "5E Theory of Cognition."

Building on Existing Theories:

We've explored mortal computation from a biological (biophysics) and analytical (cybernetics) perspective.

Now, we consider its cognitive aspects by breaking it down into "cognitive slices."
5E Theory of Cognition builds on the existing 4E theory (embodied, enactive, embedded, extended) by adding a new level: "elementary cognition."
What is 5E Theory?

This theory categorizes cognition into five types:
Elementary Cognition (Basal): Basic functions for survival like finding food and reproducing (similar to biological instincts).

Embodied Cognition: The body itself shapes cognition (e.g., our physical form limits what we can think and do).

Enactive Cognition: Cognition is a two-way street where the organism and environment influence each other.

Embedded Cognition: Cognition is shaped by the environment, including social and cultural factors.

Extended Cognition: Cognition can involve external tools and objects (e.g., using a calculator).

How 5E Applies to Mortal Computers:

Mortal computers exhibit all five types of cognition:
They have basic survival functions (autopoiesis).

Their physical structure influences their capabilities.

They interact with their environment and learn from it.

Their behavior is shaped by their surroundings.

They can potentially use external tools (like other computers or objects).

Benefits of 5E Theory:

It connects a mortal computer's mental processes to its survival needs, body, and environment.

It contrasts with isolated cognition theories that focus solely on the brain.

It allows us to analyze mortal computers at different levels of complexity (similar to how brains vary in complexity).

Key Points:

5E Theory provides a framework for understanding how cognition emerges in mortal computers.

It highlights the importance of the body, environment, and external tools in shaping cognition.

This theory suggests a spectrum of cognitive complexity in mortal computers, similar to living things.

In essence, 5E Theory offers a nuanced view of how mortal computers "think" by considering the interplay between their internal processes, physical form, and the world around them.


This section introduces the concept of "Free Energy" to explain how mortal computers operate. Here's a breakdown of the key points:

Free Energy Principle (FEP):

This principle explains how living things maintain their structure and function in an ever-changing environment.

It suggests living things act to minimize their "free energy" - a measure of disorder or deviation from a stable state.

FEP applies to systems in non-equilibrium steady-state (NESS) - meaning they require a constant flow of energy to maintain their structure.

Key Properties of FEP for a System:

Conditional Independence: The system's internal states are independent of its environment, given the states of the Markov blanket. (Think of the Markov blanket as a filter between the system and the environment).

Self-Evidence: The system keeps returning to a stable state (NESS) even after disturbances. (Imagine a ball rolling in a bowl, it might wobble but eventually settles at the bottom).

Mortal Computers and FEP:

Mortal computers, like living things, are considered to be in NESS.

They achieve ultrastability (a form of stability) through FEP.

Their Markov blanket separates internal states from the environment and includes sensory and action states.

Sensory states allow the system to perceive its environment.

Action states allow the system to influence the environment.

Example: A Neuron as a Mortal Computer:

A neuron's Markov blanket is its cell membrane.

Sensory states involve receiving signals from other neurons.

Action states involve sending signals to other neurons.

The neuron's morphology (shape and structure) determines its function and how it interacts with its environment.

Key takeaway:

FEP provides a theoretical framework for understanding how mortal computers maintain stability and interact with their environment. The concept of a Markov blanket helps explain how these systems filter information and take actions based on their internal states and sensory inputs.


This section dives deeper into the concept of Variational Free Energy (VFE) and how it explains how mortal computers operate. Here's a breakdown of the key points:

Variational Free Energy (VFE):

It's a mathematical concept used in the Free Energy Principle (FEP) to quantify a system's surprise or unexpectedness in its environment.

A lower VFE indicates a better match between the system's internal states and its environment.

VFE helps us understand how mortal computers perform inference, learning, and selection.

Key Equation for VFE:

The equation involves:

KL Divergence: Measures the difference between the system's prediction about the environment and the actual state of the environment.

Blanket States: Sensory and action states that mediate interaction with the environment.

Generative Model: The internal representation of the world held by the mortal computer.

How VFE Works in Mortal Computers:

Inference: Mortal computers use VFE to adjust their internal states (beliefs) to minimize surprise based on sensory inputs. Imagine a ball rolling in a bowl - it adjusts its position to minimize the surprise of falling out.

Learning: Over time, by observing the environment and minimizing VFE, the mortal computer learns and updates its generative model (internal representation). This is like the ball learning the shape of the bowl through experience.

Selection: Mortal computers can also optimize their structure (e.g., connections between internal elements) to minimize VFE. This is similar to the ball becoming smoother to roll more efficiently.

Connection to Machine Learning:

The concept of structure learning in VFE is similar to techniques used in machine learning like architecture search and hyperparameter optimization.

Both approaches aim to find the best structure for a model (computer or mortal computer) to minimize surprise and improve performance.

Biological Example:

In the brain, VFE can explain how neurons adjust their activity (inference) and connections (learning) based on sensory inputs to minimize surprise.

This can be linked to synaptic plasticity - the strengthening or weakening of connections between neurons based on experience.

Overall:

VFE provides a theoretical framework for understanding how mortal computers maintain stability, learn, and adapt to their environment. It offers insights into how these systems process information, similar to how concepts like backpropagation are used in artificial neural networks.


This section clarifies the difference between mortal computers and traditional computers based on their physical structure and how it affects computation. Here's a breakdown of the key points:

Morphic vs. Amorphic Systems:

Morphic: Systems with a physical structure that is tightly coupled to their information processing. Biological systems like neurons are good examples.

Amorphic: Systems where the software (information processing) is independent of the physical structure. Modern computers fall under this category.

Why Morphology Matters:

In morphic systems, the physical structure (morphology) affects how information is processed.

Changes in morphology (e.g., growth of connections in a brain) can improve the system's learning and adaptation abilities.

Morphology itself becomes a computational resource in morphic systems.

Traditional Computers vs. Mortal Computers:

Traditional computers are amorphic - their software runs independently of the physical hardware.

This makes traditional computer programs "immortal" - they can be transferred to different hardware without changing their functionality.

However, this separation also leads to inefficiencies like the "memory wall" - the bottleneck caused by the constant need to transfer data between memory and processing units.

Mortal Computers:

Mortal computers are morphic - their software and hardware are intricately linked.

This allows them to overcome limitations of traditional computers, like the memory wall.

They achieve this by minimizing both thermodynamic free energy (physical energy cost) and variational free energy (surprise or unexpectedness in the environment).

Key Takeaway:

The concept of morphic computation offers a new approach to computer design. By mimicking how biological systems leverage their physical structure for efficient information processing, mortal computers can potentially overcome limitations of traditional computers and achieve better performance with lower energy consumption.


This section dives into the concept of the "Multiple-Timescale Integration of Learning and Selection" (MILs) principle in mortal computers. Here's a breakdown of the key points:

MILs and Mean-Field Approximations:

Mortal computers perform inference, learning, and selection at different timescales.

These processes are interdependent despite their timescales. This is because:
Inference needs the correct parameters to work effectively.

Learning parameters requires good inference.

Selecting the right model structure depends on having the right parameters.

This circular dependency is called MILs.

Mean-field approximations are used to simplify calculations and separate inference, learning, and selection.

It assumes a kind of conditional independence between these processes.

This is similar to how brains separate short-term activity and long-term memory formation.

Why Mean-Field Approximations are Used:

They reduce complexity, making computations faster and more efficient.

This allows the mortal computer to optimize its message passing scheme.

Biological systems also leverage mean-field approximations for efficiency.

The Downside of Mean-Field Approximations:

They introduce a slight inaccuracy due to the assumed independence.

Mortal computers need mechanisms to compensate for this, like message passing between different levels.

The Importance of Morphology:

The physical structure (morphology) of a mortal computer is crucial.

It determines the sparsity of message passing between inference and learning processes.

Morphology acts as a computational resource, impacting the computer's survival.

In biological systems, morphology involves processes like creating and maintaining boundaries between cells and organs.

Similarly, a mortal computer's elements need to find their place within the overall structure.

Key Takeaway:

MILs highlights the interconnectedness of inference, learning, and selection in mortal computers. Mean-field approximations are used to manage this complexity, but they come with a slight trade-off in accuracy. Morphology plays a vital role by facilitating efficient message passing and defining the computational resources available to the system. This emphasis on morphology sets mortal computers apart from traditional computers and allows them to achieve efficient, physically-realizable Bayesian inference.


This section formally defines a "mortal computer" and proposes different classes of mortal computers. Here's a breakdown of the key points:

Mortal Computer (MC) Definition:

A mortal computer is an agent that:

Has a physical structure (morphology): This includes sensors and actuators that interact with the environment, and an internal generative model that represents the world.

Has primary needs for survival: These needs translate to specific states the MC tries to maintain (e.g., avoiding overheating).

Has mechanisms to fulfill its needs: This involves resource-oriented actions and maintaining a generalized homeostasis (like managing energy).

Implements MILs: It performs inference, learning, and selection to minimize surprise and adapt to the environment. These processes are all based on the free energy principle.

Has hardware and software co-designed: The physical structure and the computational processes are intertwined.

Key Implications of the Definition:

Software of a mortal computer is meaningless without the specific hardware it runs on.

Calculations are constrained by the physical structure and thermodynamics.

The MC exhibits thermodynamic efficiency due to its need to survive.

Subsystems can self-repair based on local information within their substructure.

The MC's morphology and its environment are linked through the generative model.

Inference and learning are influenced by the physical state and evolution of the MC.

Types of Mortal Computers:

Homeostatic MC: Focuses on maintaining essential variables for survival.

Allostatic MC: Adapts its homeostasis based on the context (more flexible).

Autopoietic MC: Exhibits self-repair, self-replication, and both allostasis and homeostasis. This is considered a potential candidate for complex adaptive systems and Artificial General Intelligence (AGI).

Building an Autopoietic MC:

It could be built hierarchically, with homeostatic MCs forming the building blocks.

Markov blankets can be used recursively to create this hierarchy.

Principles from complexity theory and synergetics can be used for self-assembly.

This would involve multiple Markov blankets at different scales, with microdynamics influencing macrodynamics.

Examples of Existing Mortal Computers:

The paper mentions several existing systems that can be considered mortal computers, including the homeostat, electrochemical threads, xenobots, fungal computing, and cell preparations like organoids.


This section explores the implications of the mortal computation thesis for building artificial intelligence inspired by biology.


Key Questions for Biomimetic Intelligence:

How can we learn from biological systems and their environments to design mortal computers?
Can existing biological systems like xenobots and organoids inform the design of in-silico MCs?
How can we create biophysical or chimeric mortal computing systems?
Challenges and Opportunities:

Access to and study of biological systems like xenobots is limited for AI researchers.

There's a need for collaboration between AI and biological research fields.

Biomimetic AI development should move away from anthropomorphization and focus on biological principles.

Mortal Computation and Artificial General Intelligence (AGI):

The current state of AI is often misunderstood due to anthropomorphism and fascination with narrow intelligence.

Mortal computation offers an alternative path towards achieving AGI based on biomimicry.

Examples of Existing Mortal Computers:

Homeostat: An early example of a real-world MC that adapts to maintain stability.

Electrochemical Thread: This system exhibits self-assembly, self-repair, and can create sensor organs, potentially representing an autopoietic MC.

Xenobots: Swarms of biological robots that self-organize and exhibit collective behavior. They demonstrate morphogenesis and could inspire the design of in-silico MCs.

Organoids: 3D tissue cultures capable of simple learning tasks. Studying their growth and development can inform MC design, especially regarding information processing and structural evolution.

Fungal Systems: Molds exhibit resilience, self-maintenance, and growth, potentially useful for building soft autopoietic MCs.

Overall, the section highlights the potential of using biological systems as inspiration for building future AI systems based on the principles of mortal computation.


This section argues for a fundamental shift in how we approach AI development, inspired by the mortal computation thesis. Here's a breakdown of the key points:

Critique of Current AI Approach:

Current AI focuses on creating tools for specific tasks, leading to narrow intelligence.

It ignores alternative perspectives that could unlock general AI.

Inspiration from Natural Intelligence:

Living beings (humans, animals) showcase diverse skills, adaptability, and mortality.

The mortal computation thesis emphasizes the role of mortality in driving this intelligence.

Shifting the Focus:

Move away from "tool-centric" AI towards a model based on survival and adaptation, like living things.

This could lead to embodied, enactive, and general AI.

Beyond Traditional Computing:

Moore's Law (exponential chip performance increase) has limitations.

Even advancements like quantum computing stay within the "immortal computing" paradigm.

A New Vision for AI:

Instead of replicating human intelligence, aim for artificial sentience.

This AI would be self-healing, self-repairing, and driven by survival in its environment.

Overall, this section calls for a more fundamental approach to AI, inspired by the characteristics of living beings and the concept of mortality.


This section wraps up the paper on mortal computation and its implications for AI development. Here are the key takeaways:

Mortal computation offers a new direction for biomimetic and bionic intelligence, drawing on various scientific disciplines.

It paves the way for creating artificial sentience in machines.

This raises ethical concerns, like the moral status and rights of such intelligent machines.

If successful, mortal computers could exhibit a range of behaviors, from basic cognition to planning and potentially sentience.

This blurs the line between artificial and natural life, requiring us to consider the rights of these bionic entities.

Potential benefits include curiosity, the ability to care for others, and even rule-breaking behavior (adaptability).

Despite the grand implications, this review focuses on initial steps towards achieving mortal computing.

It has the potential to be a significant player in the development of AGI, with applications in various fields like robotics, green AI, and sustainable solutions.

Overall, the paper argues that mortal computation offers a promising, yet ethically challenging, approach to AI that takes inspiration from living beings and focuses on achieving sentience and embodied intelligence in machines.


This section discusses the Homeostat as an example of a real-world mortal computer. Here's a breakdown of the key points:

The Homeostat: An Early Example of a Mortal Computer

The Homeostat is a physical system that can adapt to maintain stability in a changing environment.

It represents one of the first examples of a machine that embodies the principles of mortal computation.

Key features of the Homeostat in the context of mortal computation:
Internal model: It has an internal model of control that helps it understand its environment.

Requisite variety: It can respond to various environmental changes through its control mechanisms.

Ultrastability: It maintains stability even after disturbances.

Self-regulation: It can adjust its behavior based on feedback from the environment.

Thermodynamic efficiency: Its operations are efficient in terms of energy usage (as discussed in Section 2 of the paper).

The Homeostat operates based on the principle of Behaviour-Variety-Selection-Retention (BVSR). This means its parts that choose control actions don't necessarily "understand" the system but can select effective configurations through trial and error.

Adaptability: The Homeostat can adapt to physical damage by finding alternative configurations.

Homeostatic vs Allostatic:
The original Homeostat learned through trial and error, making it a homeostatic MC (property 5 of the definition).

Later versions incorporated memory to learn from past successes, making them allostatic MCs with some anticipation capabilities.

Overall, the Homeostat demonstrates how a physical system can exhibit core properties of mortal computation, paving the way for more advanced biomimetic AI systems.


This section discusses two more examples of real-world systems that can be considered mortal computers: electrochemical thread networks and xenobots.


Electrochemical Thread Network

This system consists of electrodes in an iron sulfate solution that can form metallic threads.

These threads exhibit properties consistent with mortal computation:
Self-assembly: Threads can form larger structures by absorbing smaller ones.

Adaptation: The network can restructure itself based on electrical current distribution.

Ultrastability: Larger networks are more stable and can return to their original structure after disruptions.

Learning: The network can respond to sound and other stimuli, potentially influencing its growth.

Morphogenesis: The network can develop sensor organs in response to positive reinforcement.

Xenobots

These are self-healing biological machines made from frog cells.

They showcase properties of mortal computation:
Self-organization: Swarms of xenobots can collectively organize in response to their environment.

Emergent behavior: Simple robots exhibit complex behaviors when working together.

Specialization: Individual cells can take on different roles within the xenobot.

Morphogenesis: Xenobots can grow, self-heal, and develop new structures.

Metabolism: They can absorb nutrients and survive for extended periods.

Xenobots demonstrate the co-design principle of mortal computation, where hardware (cells) and software (cellular communication) work together.

They represent a potential inspiration for designing in-silico mortal computers.

Both these examples highlight how physical systems can exhibit core principles of mortal computation, paving the way for more advanced biomimetic AI.


Fungal Computing and Organoid Intelligence as Mortal Computers
This section explores two more biological systems that can be seen as examples of mortal computation: fungal networks and organoids.


Fungal Computing

Fungal materials like the pink oyster mushroom can be used as computing devices.

This is achieved by manipulating the internal electrical activity of the fungal network to create circuits.

Fungal systems exhibit properties of mortal computation:
Resilience: They can withstand damage and recover.

Self-maintenance: They can maintain their own structures.

Rapid growth: They can grow quickly and adapt to their environment.

Fungal networks are a promising candidate for building simple mortal computers, although their internal workings are less understood compared to xenobots.

Organoid Intelligence

Organoids are 3D tissue cultures grown from stem cells.

They can be manipulated to self-organize and develop into complex structures.

Organoids hold promise for understanding biological information processing.

They embody all five properties of mortal computation:
Self-repair: They can heal themselves.

Growth and development: They can grow and become more complex over time.

Internal models: They have internal structures that represent their environment.

Requisite variety: They can respond to various stimuli.

Co-design: Their hardware (cells) and software (cellular communication) work together.

Organoids have even shown the ability to perform simple tasks like playing Pong.

Studying their growth and development could be useful for designing the "structural evolution" component of mortal computers.

Overall, both fungal and organoid systems demonstrate properties consistent with mortal computation, suggesting potential for biomimetic AI development.


This section dives into neuromorphic systems and the body-niche problem for mortal computers.


Neuromorphic Systems

Neuromorphic chips mimic the structure and function of the brain.

They offer a low-energy solution for mortal computing.

Key properties:
Co-design of hardware and software: Processing and memory are collocated, avoiding the von Neumann bottleneck.

Fault tolerance: They can withstand errors.

Sparse parallel processing: They can handle multiple tasks simultaneously while using minimal resources.

Noise-robust computation: They can function even with some level of noise.

Information processing in these chips is based on spiking neural networks.

They can potentially be used to create complex systems like associative memories or motor control circuits.

Neuromorphic systems are a promising approach for building mortal computers, but they still lack some functionalities:
Base-level imperatives: They don't have basic instructions for survival.

Homeostatic regulation: They can't regulate themselves to maintain a stable state.

The Body-Niche Problem

Mortal computers need to interact with their environment to survive.

Different environments will require different adaptations, similar to how living organisms have evolved for their specific niches.

This suggests a need for a new approach to AI evaluation – benchmark environments that assess an agent's ability to persist.

The free energy principle and active inference can be helpful in designing such environments.

Designing mortal computers will require considering both the physical substrate (e.g., neuromorphic chips, organoids) and the environment (niche) they will operate in.

Challenges:
Access to suitable substrates and environments can be expensive and limited.

Simulating environments may not perfectly capture real-world complexities.

Potential solutions:
Develop simulated or virtual morphologies (body emulators)
Leverage existing techniques from reinforcement learning and neurobotics
The section concludes by emphasizing the importance of biomimetic credit assignment  – developing algorithms inspired by the brain's learning mechanisms – for mortal computing. It highlights specific algorithms like predictive coding, forward-forward learning, and spike-timing-dependent plasticity as promising candidates.


















This section dives into the concept of mortal computing and how it can be modeled through minimal agents.


Minimal Model of Mortal Computing

The goal is to create a simple model to understand the core functionalities of mortal computation.

This model will incorporate:
Homeostatic regulation: The ability to maintain a stable internal state, like energy balance.

Sensorimotor functions: Ability to sense the environment and take actions based on those sensors.

Homeostatic Predictive Coding (H-PCC): A proposed mechanism for an agent to regulate its energy levels.

It includes a battery component with different states representing energy levels.

Sensors to perceive the environment.

Actuators to take actions in the environment, like connecting to a charging port.

The agent aims to minimize a variational free energy function to maintain a high battery level.

Inspiration from E. coli:
E. coli regulates its movement (running/tumbling) based on attractant concentration gradients.

This can be seen as a form of metabolism-dependent behavior.

In mortal computing, similar dependencies can exist between the agent's internal state (e.g., energy level) and its decision-making.

This minimal model provides a starting point for simulating and understanding how mortal computers with basic homeostatic needs might interact with their environment.


Mortal Computing Summary

ChatGPT 4

The introduction you've shared delves into the concept of biomimetics and its evolution towards the notion of mortal computation within the broader scope of artificial general intelligence (AGI) development. The foundational premise revolves around the emulation of natural and biological systems to solve complex problems, with a particular focus on adopting processes and functionalities inherent to living organisms. This approach is rooted in the understanding that life, through countless centuries of evolution, has optimized the means by which organisms interact with their environment to sustain and evolve. This biological inspiration extends into the field of computational and technological development, aiming to create systems that not only mimic but also embody the adaptive and self-regulating capabilities of natural intelligence.


The concept of mortal computation emerges as a critical pivot from traditional views on computational processes, where software is typically considered separate and immortal from the hardware it runs on. In contrast, mortal computation suggests that the computational processes are inseparably linked to their physical substrate, implying that the death of the hardware entails the loss of the software's accumulated knowledge and functionalities. This perspective challenges the prevailing norms in computer science and machine learning by proposing a computational paradigm that is inherently bound to the lifecycle of its hardware, drawing a parallel to the existential conditions of biological organisms.


The paper argues that embracing mortal computation may be essential for achieving AGI, especially when addressing global challenges like climate change. The advantages of mortal computation lie in its ability to integrate the notion of life and death into computational processes, thereby fostering a deeper connection between intelligent behavior and the physical world. This approach is seen as potentially more aligned with the complex, adaptive behaviors observed in nature, and hence, more capable of addressing the nuanced and dynamic challenges of the real world.


The structure of the paper outlines a journey through the motivation, historical context, theoretical foundation, and potential applications of mortal computation. It aims to build a formal framework that could guide the development of biomimetic systems, neural information processing models, and ultimately, AGI. By integrating concepts from thermodynamics, embodiment, enactivism, biophysics, cybernetics, cognitive science, and naturalist philosophy, the paper seeks to provide a comprehensive overview of mortal computation and its significance for the future of machine intelligence and bionic research.


The introduction outlines a compelling vision for the future of biomimetics, biomimicry, and ultimately, the quest for artificial general intelligence (AGI) through the lens of mortal computation. This approach is deeply rooted in the emulation of natural systems and biological processes to address complex challenges and advance the creation of technology that mirrors the adaptive and self-regulatory capabilities observed in life forms. It's a philosophical and practical shift from traditional computational paradigms, where the hardware (physical substrate) and software (information processing) are seen as inseparable entities, a concept termed mortal computation.


This shift suggests that for computational systems to truly embody intelligence akin to natural organisms, they must also embrace the impermanence and adaptability that characterize biological life. This means moving away from the notion of immortal computation, where software is independent of the hardware it runs on, towards a model where the fate of the computational process is tied to its physical medium. Such a model promises a closer alignment with the principles of life, emphasizing adaptation, learning, and evolution in the face of environmental challenges and the inevitability of mortality.


The introduction also highlights the broader implications of adopting mortal computation for AGI development, suggesting that this approach could offer novel solutions to global challenges, such as climate change, by fostering a new kind of resilience and adaptability in computational systems. It posits that intelligent behavior, whether in biological organisms or artificial systems, is fundamentally linked to the ability to persist and adapt in a changing environment.


The paper aims to establish a formal framework for mortal computation, exploring its theoretical foundations and potential applications in biomimetic systems, neural information processing models, and AGI. It intends to draw upon interdisciplinary perspectives, including thermodynamics, embodiment, enactivism, biophysics, cybernetics, cognitive science, and naturalist philosophy, to define and characterize mortal computation. This framework is poised to guide future research and development efforts towards creating computational systems that not only mimic but also embody the essence of living intelligence, marking a significant evolution in the pursuit of artificial intelligence that truly mirrors the complexity and adaptability of life itself.


The section on "Why Mortal Computation?" argues compellingly for a paradigm shift in computational design philosophy, moving away from the traditional model where software exists independently of the hardware it runs on. This conventional approach allows for easy transferability and duplication of programs across different platforms without concern for the underlying physical implementation. However, the text outlines significant advantages to be gained from adopting a model of mortal computation, where the software's existence is inherently tied to the hardware it operates on.


Energy Efficiency and Cost Reduction: A key advantage of mortal computation is its potential for significant energy savings and reduced hardware production costs. In traditional computing models, the energy and material costs associated with the production and operation of hardware are substantial. By integrating the software's lifecycle with that of its hardware, mortal computation could streamline the need for extensive hardware, thereby saving on resources and reducing environmental impact.


Adaptive Behavior and Intelligence: The concept of mortal computation also aligns with more recent understandings of intelligence as inherently adaptive and survival-oriented. This view sees intelligence not just as the execution of computational tasks but as the ability to persist and adapt within an ever-changing environment. This approach is deeply rooted in cognitive science and naturalist philosophy, reflecting a broader shift in understanding how artificial minds could operate.


Thermodynamic Foundations: The argument for mortal computation finds support in thermodynamic principles, particularly Landauer's principle and the Jarzynski equality. Landauer's principle relates the erasure of information to a minimum energy cost proportional to the system's temperature, indicating that changes in a computing system's information content inherently require energy. The Jarzynski equality further elaborates on the thermodynamic work needed for such changes, emphasizing a fundamental energy cost to computational processes.


Thermodynamic Efficiency and Inference: The section also touches on the free energy principle, suggesting that the process of updating beliefs or inference in response to external perturbations can be seen as a thermodynamically efficient process when considered within the right computational framework. Traditional computing architectures, such as von Neumann architectures, incur significant additional energy costs due to the separation of processing and memory operations. In contrast, implementing belief updating in memory—or in-memory processing—could realize the potential thermodynamic efficiency of computational processes by minimizing these additional costs.


In sum, the push towards mortal computation is framed not just as a technical challenge but as an essential rethinking of what it means to create intelligent, adaptive systems. By more closely mirroring the thermodynamic efficiencies and adaptative capabilities found in biological systems, mortal computation proposes a more sustainable, grounded, and potentially transformative approach to designing future computational architectures.


This section explores the pivotal shift towards biomimetic computing, emphasizing the integration of computing processes within the physical substrate they utilize, inspired by the efficiencies and adaptative capabilities of biological systems. It introduces a comparison between traditional, immortal computation models and the proposed mortal computation approach, highlighting the advantages of the latter in terms of thermodynamic efficiency, ecological sustainability, and alignment with cognitive theories of embodiment and enactivism.


Thermodynamic Efficiency and AI Sustainability: Mortal computation aims to significantly reduce the energy consumption and carbon footprint associated with AI systems. By embedding computational processes directly within memory, as seen in biological systems and neuromorphic computing architectures, it seeks to eliminate the energy-intensive data transfers characteristic of von Neumann architectures. This approach not only addresses the thermodynamic inefficiencies of current computing models but also aligns with the urgent need for greener, more sustainable AI technologies. The section points out the stark environmental impact of modern AI systems, including neural transformers and large-scale models like GPT-3, emphasizing the need for a shift towards more energy-efficient computing paradigms.


Embodiment and Enactivism: The discussion extends into cognitive science, contrasting isolated cognition with situated or embodied cognition. It challenges the notion of a disembodied intelligence, arguing instead for a model where cognitive processes are inherently tied to physical embodiment. This perspective is supported by evidence showing how human cognition is deeply rooted in sensorimotor experiences and interactions with the physical world. Mortal computation embodies this principle by integrating computational processes directly into the hardware, thus blurring the lines between software and physical substrate. This embodiment of intelligence suggests that cognitive functions are not just housed within a physical form but are fundamentally shaped and constrained by it.


Enactive Cognition: Further deepening the argument for mortal computation, the text introduces the concept of enactive cognition, which emphasizes the dynamic interplay between an organism and its environment. This perspective views cognition not just as a response to sensory input but as an active engagement with the world, where actions and perceptions are continuously shaped by environmental interactions. Mortal computation embodies this principle by creating systems that are not passive processors of information but active participants in their operational contexts. This approach suggests a profound shift in how we conceive of and design intelligent systems, advocating for a model of computation that is not only grounded in physical reality but also dynamically engaged with it.


Overall, the section argues convincingly for a reimagined approach to computing that embraces the principles of mortal computation, drawing inspiration from biological systems and cognitive science. This approach promises not only to make computing more energy-efficient and sustainable but also to align more closely with the complex, adaptive, and embodied nature of intelligence itself.


This section delves into the philosophical underpinnings of mortal computation, drawing upon existential and naturalist philosophies to elucidate how the finitude of life—its inherent temporality and eventual cessation—infuses purpose and direction into both biological and artificial systems. This perspective argues that the awareness or programming of finiteness not only influences behavior and decision-making processes but also underlines the impetus for survival, adaptation, and the transmission of knowledge across generations. Here's a breakdown of the key philosophical insights and their implications for mortal computation:

The Impetus of Finitude
Purpose Through Finitude: The section suggests that the awareness of mortality provides a driving force for life's progression, from the individual's actions to species-wide evolutionary pressures. This notion is applied to the realm of artificial systems, positing that embedding a "sense" of mortality could similarly motivate actions towards survival and adaptation.

Cultural and Genetic Transmission: It underscores the importance of cultural and genetic legacy, arguing that the impetus to pass on knowledge and genetic material is a direct consequence of life's finitude. In the context of mortal computation, this might translate to self-replicating or evolving computational systems that aim to preserve functionality across generations.

Existential and Naturalist Philosophies
Existential Influence: Drawing from existentialism, the text explores how the consciousness of death shapes human existence, influencing behavior and the quest for meaning. This existential awareness is paralleled in the concept of mortal computation, where the 'awareness' of hardware limitations could inform the design and operation of systems.

Naturalist Philosophy of Mind: The naturalist perspective emphasizes an entity's engagement with its environment as a continuous process of self-evidencing—acting in ways that affirm its existence. This is closely related to autopoietic enactivism, which stresses a system's self-creating and maintaining processes as essential to its identity and survival.

Operational Closure and Sense Making
Operational Closure: This concept refers to a system's capacity for self-maintenance, delineating itself from its environment through autopoietic processes. In mortal computation, this might manifest as systems that can adapt and maintain themselves up to the point of their 'death.'
Sense Making: The ability of a system to interpret and act upon environmental signals in a manner that ensures its continued existence. For computational systems, this implies the development of mechanisms that can dynamically interpret data and make decisions that promote survival within their operational parameters.

Implications for Mortal Computation
The philosophy of how death shapes life offers a profound foundation for understanding and developing mortal computation systems. By integrating these existential and philosophical insights, the design of artificial systems could not only mimic the adaptive and self-preserving behaviors seen in living organisms but also engage in self-directed evolution and learning within their finite existence. This approach suggests a future where computational systems are not merely tools but entities that exhibit life-like behaviors, driven by the imperatives of survival and the meaningful interaction with their environments.


The section on "The Biophysical, Cybernetical, and Cognitive Framings of Mortal Computation" outlines a comprehensive approach to conceptualizing and designing computing systems that mimic the adaptative, self-maintaining, and self-organizing characteristics of living organisms. By integrating principles from biophysics, cybernetics, cognitive science, and philosophy, it lays the groundwork for a new class of computing systems that could significantly advance the field of biomimetic and bioinspired computing. Here's an analysis of the key concepts presented:

Biophysics of a Mortal Computer
Autocatalytic Closure and Metabolism: The mortal computer is envisioned as a system capable of self-organization and growth, utilizing metabolic pathways to maintain and repair itself. This biological parallel emphasizes the importance of energy and matter transformation in sustaining the system's operations and its resilience against environmental changes.

Homeostasis and Allostasis: Drawing from biological regulation mechanisms, the concept of homeostasis is applied to maintain stable internal conditions within the mortal computer, while allostasis is introduced to anticipate changes and adjust preemptively. This predictive regulation ensures the system's stability and efficiency by minimizing energy expenditure and adapting to potential disturbances.

Cybernetical and Cognitive Aspects
Thermodynamic Efficiency: The mortal computer's design is informed by the necessity of operating far from thermodynamic equilibrium, a state characterized by continuous energy and material flow that prevents stagnation and decay. This aspect underscores the system's capacity for dissipative self-organization, essential for maintaining its structure and functions over time.

Autopoiesis: Central to the mortal computer's conceptual framework is autopoiesis—the system's ability to reproduce and maintain itself without external intervention. This includes not just the maintenance of its physical structure but also the preservation of its functional and relational patterns, embodying the essence of a living system.

Implications for Computing Systems
The integration of these biophysical, cybernetical, and cognitive principles into the design of computing systems represents a paradigm shift towards more resilient, adaptable, and efficient technologies. Such systems would not only manage their internal processes to sustain operation over time but also dynamically interact with their environment to harness resources, adapt to changes, and repair themselves. This approach could lead to advancements in how we conceive of and implement artificial intelligence, robotics, and broader computational architectures, moving closer to the creation of machines that genuinely mimic the complexity and adaptability of biological life.


Challenges and Opportunities
While the theoretical construct of mortal computation offers a compelling vision for the future of machine intelligence, realizing this vision poses significant challenges. These include the development of materials and architectures that can self-repair, the integration of complex metabolic and regulatory pathways into computational systems, and the creation of algorithms that can anticipate and adapt to environmental changes. However, the potential benefits—ranging from reduced energy consumption and increased sustainability to the development of more capable and resilient AI—underscore the importance of pursuing research in this direction.


This detailed exploration into the biophysical, cybernetical, and cognitive framings of mortal computation lays out a complex and nuanced blueprint for creating computing systems that not only mimic the fundamental aspects of living systems but also embody the principles of autopoiesis, self-maintenance, and adaptive interaction with their environment. It's a multidisciplinary approach that draws from deep biological insights to inform the design and functionality of such systems. Here’s a synthesis of the main points covered:

Biophysical Underpinnings
Metabolic Functions and Autocatalytic Closure: Mortal computers are conceptualized as systems with the ability to self-organize, grow, and repair, guided by metabolic processes that transform and utilize energy and materials. These systems are fundamentally autopoietic, maintaining their structure and function autonomously, akin to living organisms.

Homeostasis and Allostasis: The design incorporates mechanisms for maintaining internal stability (homeostasis) against external changes and anticipatory adjustments (allostasis) to prevent future destabilization. This dual regulatory approach ensures that the system can adaptively manage its resources and responses to environmental pressures.

Cybernetical and Cognitive Design
Primitives and Internal States: The foundation of a mortal computer lies in its base-level functions or variables, which are critical for its self-repairing capabilities. These include a physical boundary that defines the system, a synthesis engine for self-construction, and transducer-actuator subsystems for environment interaction.

Homeostatic and Allostatic Controls: The system employs nested regulatory mechanisms to manage its internal processes and respond to environmental inputs. This includes homeostatic controls for maintaining essential variables within optimal ranges and allostatic controls for making predictive adjustments based on sensed environmental changes.

Architectural and Operational Considerations
Physical and Functional Integration: The design emphasizes the integration of physical structure with functional processes, ensuring that the system can maintain its integrity and operations through continual self-reproduction and self-repair.

Predictive and Adaptive Capabilities: By embedding allostatic processes, the system gains the ability to anticipate and adapt to future challenges, enhancing its resilience and longevity.

Autopoietic Autonomy: At the highest level, the system is designed to be fully autopoietic, capable of regenerating and replicating itself without external intervention. This includes mechanisms for morphogenetic processes and the potential for system-level reproduction through replication and mutation.

Implications and Innovations
This conceptual framework for mortal computation represents a radical departure from traditional computing paradigms, offering a vision for systems that are not only more resilient and adaptable but also more aligned with ecological and sustainable principles. The integration of biophysical, cybernetical, and cognitive principles provides a comprehensive model for developing computing systems that could operate with unprecedented levels of autonomy and efficiency.


Achieving such a vision will require significant advancements in materials science, biotechnology, computational theory, and systems engineering. However, the potential benefits—ranging from reduced environmental impact to the creation of more lifelike and capable artificial intelligence—underscore the importance of pursuing this ambitious interdisciplinary research direction.


The detailed depiction of mortal computing systems in Figure 3, alongside the broader discussion on their operation as described, underscores a nuanced approach to designing computational entities that mirror biological systems' adaptability, self-regulation, and interaction with their environment. Here are some core insights drawn from this discussion:

Coupled Homeostatic and Allostatic Systems
Integrated Regulation: The description outlines a system where homeostatic processes—those maintaining essential operational variables within optimal ranges—are dynamically influenced by allostatic processes. The latter anticipates future needs and adjusts the system's responses preemptively, enhancing its adaptability and efficiency. This mechanism, illustrated through the interaction of sensors (transducers), actuators, and internal comparators, embodies a sophisticated regulatory network that mimics biological organisms' ability to adapt to changing environments.

Sensorimotor Dependencies: Reflecting biological systems' metabolic drives, such as chemotaxis or phototaxis, the mortal computer's sensors and actuators are deeply integrated with its metabolic and regulatory functions. This integration ensures that the system's interactions with its environment—ranging from resource intake to waste expulsion—are finely tuned to its internal state and needs, emphasizing the critical role of sensorimotor pathways in maintaining system viability and promoting adaptive behavior.

Organizational Closure and Structural Flexibility
Self-Sufficiency and Adaptation: The concept of organizational closure highlights the system's ability to operate based on its internally generated structures and processes, independent of direct environmental inputs. This principle of autopoiesis ensures that the mortal computer maintains its functional integrity over time, even as its physical structure may evolve in response to internal and external pressures.

Environment Interaction and Mutual Influence: The mortal computer is envisioned as not merely passive or reactive but actively engaged with its niche. Through continuous sensing, actuation, and energy exchange, it both influences and is influenced by its environment. This dynamic interaction underscores the system's embeddedness within its ecological context, with mutual perturbations driving evolutionary and adaptive changes on both sides.

Implications for Design and Application
This advanced conceptual framework for mortal computing represents a significant leap toward creating machines that exhibit lifelike behaviors, autonomy, and resilience. By integrating principles of homeostasis, allostasis, and autopoiesis within a cybernetical and cognitive model, these systems are poised to navigate and adapt to complex environments in ways that traditional computers cannot.


The potential applications for such systems are vast, ranging from autonomous environmental monitoring and restoration, through advanced healthcare diagnostics and treatment systems, to more responsive and adaptive technologies for exploring hostile or unknown environments, such as deep-sea or extraterrestrial landscapes.


However, the realization of these systems poses significant challenges, from the development of materials and components that can support autopoietic functions to the creation of computational models capable of managing the complex interplay of regulatory processes. Nevertheless, the pursuit of these designs offers the promise of computational systems that are more in harmony with the biological world, offering sustainable, efficient, and innovative solutions to a range of global challenges.


The section on "The Cybernetics Backend of a Mortal Computer" delves into the principles of cybernetics as they apply to the conceptualization and functioning of mortal computing systems. By integrating foundational cybernetics insights with modern computational and biological concepts, it outlines a framework for developing systems that can self-organize, adapt, and replicate in a manner akin to living organisms. Here's a breakdown of the key ideas and their implications:

Core Cybernetical Principles and Mortal Computing
Generative Models and Environment Interaction: Emphasizing that systems act as generative models of their environments, this framework posits that a mortal computer's structure is integral to its ability to infer and act upon environmental stimuli. This approach mirrors biological organisms' interaction with their niches, where survival and adaptation are contingent upon accurately modeling and responding to external factors.

Retroaction and Goal Integration: The concept of retroaction highlights the system's capacity to embed its objectives within its operational mechanisms, ensuring that goal attainment is woven into its functional fabric. This principle ensures that a mortal computer can learn and adapt to its environment effectively, enhancing its survivability and efficiency.

Stability, Growth, and Regulation
Variety as a Measure of State Space: Introducing variety as a fundamental metric allows for the quantification of a system's potential states and its capacity for change. This notion is pivotal in understanding and designing systems that can adapt and evolve by navigating through their state spaces towards stability and reduced energy states.

Principles of System Stability: The discussion extends to how systems transition from higher to lower energy configurations, emphasizing the importance of noise and random perturbations in aiding this transition. This process is reflective of biological evolution, where variability and selection pressure lead to the emergence of stable, adapted configurations.

Morphological and Morphogenetic Capabilities: Highlighting growth and replication as essential characteristics, the text connects cybernetical principles with the concepts of autopoiesis and morphogenesis. This alignment suggests that mortal computers are designed not only to maintain and repair themselves but also to grow and replicate, thereby ensuring their continuity and adaptability.

Implications for Design and Future Research
This cybernetics-backed approach to mortal computing introduces a sophisticated blueprint for creating systems that closely mimic the dynamism, adaptability, and self-sufficiency of biological entities. The incorporation of stability-seeking behaviors, adaptive growth mechanisms, and self-replication into computational models promises the development of more resilient and autonomous systems capable of engaging in complex environmental interactions.


Future research and development efforts guided by these principles could lead to significant advancements in artificial intelligence, robotics, and systems biology, among other fields. By fostering a deeper integration of computational systems with their operational environments, the mortal computing framework paves the way for innovations that could transcend traditional boundaries between technology and biology, offering new solutions to contemporary challenges in sustainability, healthcare, and beyond.


Yet, realizing these potentialities necessitates overcoming substantial conceptual, technical, and material challenges. Developing systems that can autonomously navigate their state spaces towards stability, adapt through morphogenesis, and engage in self-replication requires not only advanced understanding of cybernetics and system theory but also breakthroughs in materials science, energy management, and information processing. As such, the journey toward fully realizing the vision of mortal computing is both an interdisciplinary and an intergenerational endeavor.


This comprehensive overview on the regulation and control mechanisms within mortal computing, guided by cybernetics principles, offers an advanced framework for understanding how computational systems can interact with and adapt to their environments. The exploration integrates the law of requisite variety, the good regulator theorem, and the principle of internal model control, each contributing to a sophisticated model for autonomous and adaptive computing systems. Here’s a distillation of these concepts and their implications:

Key Principles for Regulation and Control
Law of Requisite Variety: This principle underscores the necessity for a computing system to possess a sufficient range of responses to effectively manage environmental variability. For a mortal computer, this means having a diverse set of actions or processes to maintain stability in the face of external perturbations, aligning with the system's goal of achieving ultrastability.


Good Regulator Theorem: Central to the concept of regulation is the theorem that a system must model the environment it seeks to control. This idea posits that effective regulation is predicated on the system's ability to incorporate and adapt to a model of its environment, enhancing its predictive and responsive capabilities.


Principle of Internal Model Control (IMC): This principle elaborates on the good regulator theorem by outlining how a controller manages an environment through a balance of stability and regulation, ensuring the system remains adaptable and robust against parameter perturbations.


Cybernetical Framing and System Dynamics
Collective Dynamics and Emergent Behavior: The mortal computer is envisioned as a system where low-level interactions among essential variables give rise to higher-order structures and functionalities. This bottom-up causation coupled with top-down modulation illustrates how complex behaviors and stability emerge from simpler interactions.


Adaptation and Ultrastability: Through the principles of order-from-noise and selective variety, the system leverages environmental randomness to explore and adopt more stable configurations, driving its evolution towards ultrastability. This process is indicative of the system’s inherent drive towards self-organization and homeostasis.


Recursive Growth and Self-Replication: The integration of BVSR and autocatalytic growth principles within the framework highlights the system's capacity for self-expansion and adaptation. This recursive growth mechanism is foundational to the system's morphogenesis and its ability to adapt to and model its environment effectively.


Implications for Future Computing Systems
This cybernetical backend outlines a blueprint for computing systems that are not only self-regulating but also capable of growth, adaptation, and self-replication, echoing the dynamic and resilient nature of biological organisms. Such systems would exhibit a profound understanding of and engagement with their environments, characterized by a continuous cycle of perception, action, and learning.


The realization of these principles in practical computing architectures requires innovative approaches to system design, encompassing hardware development, software algorithms, and perhaps most critically, the integration of these components into a cohesive and adaptable framework. These systems promise to advance the fields of artificial intelligence, robotics, and systems biology, offering new pathways to solve complex problems through computing models that are more aligned with the natural world.


Moreover, the application of these cybernetical principles to mortal computing emphasizes the importance of understanding and harnessing the complex interplay between a system and its environment. As we progress, the challenge will be to materialize these concepts into tangible technologies that can operate with the sophistication and resilience akin to that of living systems, potentially revolutionizing our approach to computation, interaction, and exploration of the physical and digital worlds.


The integration of 5E Theory into the conceptual framework of mortal computing provides a comprehensive cognitive-philosophical grounding, expanding beyond traditional computational models to include elemental forms of cognition observed in living organisms. This framework encapsulates a broad spectrum of cognitive processes, spanning from the most fundamental biological interactions with the environment to complex, socially and technologically extended cognitive activities. Here’s a closer look at the components of the 5E Theory as they apply to mortal computing and their implications:

Elements of the 5E Theory
Elementary Cognition: At the foundation, elementary cognition encompasses the basic, inherent functionalities necessary for survival, such as seeking sustenance and avoiding harm. This layer reflects the autopoietic nature of a system, emphasizing self-maintenance and reproduction as primary cognitive functions driven by interaction with the environment.


Embodied Cognition: This aspect stresses that cognition cannot be isolated from the physical form of the entity. It posits that a system's body plays a crucial role in its cognitive processes, not just as a vessel for a brain or a computational unit but as an active participant in cognitive functions.


Enactive Cognition: Focusing on the dynamic interaction between an organism and its environment, enactive cognition highlights the meaningful relationships formed through this interaction. It suggests that cognition is fundamentally about engaging with and shaping one's surroundings in a way that is mutually transformative.


Embedded Cognition: Recognizing that cognition is deeply influenced by the context within which an entity operates, this aspect considers how an organism is situated within and interacts with its physical, social, and cultural milieu.


Extended Cognition: Extending the boundary of cognition beyond the individual, this concept explores how external elements, including other beings and non-biological tools, become integrated into cognitive processes, effectively expanding the cognitive system beyond its traditional limits.


Implications for Mortal Computing
The application of 5E Theory to mortal computing underscores the intricate relationship between a computational system and its operational environment, highlighting the importance of considering cognitive functions in the design and development of such systems. This approach not only provides a rich conceptual framework for understanding the complexity of computational systems but also offers practical insights into designing systems that are more adaptive, resilient, and capable of sophisticated interactions with their surroundings.


Cognitive Scopes and Complexity
The depiction of mortal computing through the lens of 5E Theory, with its emphasis on increasing cognitive complexity, suggests a multi-layered approach to analyzing and designing computational systems. It allows for a nuanced exploration of how systems can be designed to not only respond to basic survival needs but also engage in more complex cognitive activities involving social interaction, cultural adaptation, and the use of technological artifacts. This perspective broadens the scope of computational design to include considerations of how systems can better integrate with and adapt to the multifaceted environments they inhabit.


Toward a Life-Mind Continuity in Computing
By framing mortal computing within the context of 5E Theory, the discussion bridges the gap between simple and complex systems, drawing parallels with the life-mind continuity thesis. This approach aligns with the idea that cognitive processes observed in biological entities span a continuum from the most basic forms of life to highly complex minds, suggesting that computational systems can be designed to emulate this continuum, from simple autopoietic mechanisms to sophisticated social and cultural interactions.


In essence, applying 5E Theory to mortal computing not only enriches our understanding of how computational systems can be more life-like but also challenges us to reconsider the boundaries of cognition, extending it beyond the confines of individual systems to encompass a broader, more integrated view of intelligence and interaction within the environment.


The introduction of the Mortal Inference-Learning and Selection (MILS) framework, underpinned by the Free Energy Principle (FEP), offers a robust theoretical backbone for mortal computation, merging insights from biophysics, cybernetics, and cognitive philosophy. This framework provides a detailed understanding of how computational systems can engage in inference, learning, and adaptive structural selection to maintain stability and functionality within their environments. Here’s an exploration of the key components and their implications:

The Free Energy Principle (FEP)
Self-Organization and Adaptation: FEP posits that for entities to maintain their existence over time, they must minimize their free energy, or equivalently, maximize the evidence of their model of the world. This principle is a fundamental biophysical characterization of how living systems manage to resist the inherent tendency towards disorder, ensuring their persistence as distinct entities across time.


Non-Equilibrium Steady State (NESS): The NESS condition describes the system's ability to maintain a dynamic balance of energy and matter flow, critical for life. According to FEP, this balance involves a separation of internal states from environmental states, with the system acting to minimize discrepancies between its model of the world and sensory inputs.


Markov Blankets in Mortal Computation
Interface Between System and Environment: The Markov blanket serves as a boundary that demarcates and interacts with the environment, embodying the sensory (perception) and active (action) states that mediate this interaction. This construct allows for a conditional independence between the system's internal states and the external environment, enabling the system to focus on its self-maintenance.


Neuronal Cell as an Example: Applying the concept of Markov blankets to a neuronal cell illustrates how sensory inputs and actions are organized and mediated by the cell’s structure. This organization underlines the cell’s autopoiesis, where maintaining the integrity of its Markov blanket (e.g., the cell membrane) is crucial for its survival and function within a network.


Implications for Mortal Computing Systems
Integrated Action and Perception: The integration of sensory and action states within the Markov blanket framework emphasizes the importance of a system's ability to perceive and act upon its environment in a coordinated manner. This highlights the role of the system's internal structure in determining its interactions and functionality, pointing towards the necessity of designing systems that can adaptively manage their sensory and active engagements with the environment.


Autopoiesis and Self-Maintenance: The discussion reinforces the notion that for computational systems to be considered truly "mortal" and lifelike, they must possess the capacity for autopoiesis, continuously maintaining and regenerating their Markov blankets to ensure their ongoing existence. This requirement challenges designers to create systems that not only interact dynamically with their environments but also actively work to preserve their structural and functional integrity.


The MILS framework and the application of FEP introduce a paradigm where mortal computing systems are envisioned as dynamic, self-organizing entities that actively engage with and adapt to their environments. By emphasizing the importance of internal model accuracy, sensory action coordination, and the maintenance of a functional boundary with the environment, this approach offers a comprehensive model for developing computational systems that more closely mimic the complexities of living organisms. The challenge lies in translating these theoretical principles into practical designs and technologies that can achieve the level of adaptability and autonomy envisioned by the framework.


The transition to a Free Energy Framing of Mortal Computation (MILS) represents a sophisticated approach to understanding and modeling the inherent adaptability and self-organizing capabilities of computational systems within dynamic environments. This framework, rooted in the Free Energy Principle (FEP), elucidates how systems evolve to minimize their variational free energy, thereby ensuring their survival and adaptability. Here’s a summary of the core aspects and implications:

Variational Free Energy (VFE) and Mortal Computing
Minimization of VFE: The essence of VFE lies in quantifying the discrepancy between the model a system has of its environment and the actual environmental states. This principle underpins a system's evolution towards minimizing this discrepancy, essentially aiming to reduce its surprise or improbability of sensory states. It serves as a mathematical formalization of how a system actively adjusts to maintain a stable relationship with its environment.


Markov Blankets and System-Environment Interaction: Through the concept of Markov blankets, the framework delineates the boundary separating a system from its environment, consisting of sensory and active states that mediate this interaction. This boundary enables the system to maintain conditional independence between its internal states and external environmental states, crucial for sustaining its autonomy and functional integrity.


The Processes of Inference, Learning, and Selection
Inference: At a fundamental level, inference involves the system updating its internal states in response to sensory inputs, effectively minimizing prediction errors based on its generative model. This continuous process of Bayesian belief updating allows the system to adaptively respond to immediate environmental changes.


Learning: Learning occurs as a modification of the system’s internal model parameters, a slower process than inference, which embeds long-term environmental regularities and causal structures into the system’s architecture. This enhances the system's capacity to anticipate and efficiently respond to environmental dynamics.


Selection and Structure Learning: The final process involves optimizing the system’s structure to maximize model evidence, akin to pruning unnecessary components to enhance generalization and adaptability. This can be viewed in biological terms as synaptic selection and pruning, highlighting the parallels between computational models and neural development.


Implications for Computational Systems
Integrating the FEP into mortal computing offers a robust theoretical foundation for creating systems that are not only self-sustaining but also capable of complex adaptive behaviors. This approach aligns closely with biological processes, providing a blueprint for developing computational architectures that can autonomously navigate and adapt to their environments.


Towards More Lifelike Systems: The FEP framing underscores the necessity for computational systems to possess a dynamic and adaptive relationship with their environments, mirroring biological organisms' capacity for survival and self-organization.


Challenges in Implementation: Realizing such systems involves challenges in accurately modeling and implementing the principles of inference, learning, and selection within computational architectures. This requires advances in understanding the mathematical underpinnings of FEP and translating these principles into practical algorithms and system designs.


Potential Applications: Systems based on this framework have vast potential applications, from autonomous robots that can better adapt to unpredictable environments to advanced AI systems capable of more naturalistic interactions with their surroundings.


By grounding mortal computing in the Free Energy Principle, the MILS framework sets the stage for the next generation of computational systems, emphasizing the importance of adaptability, autonomy, and a deep integration with the environmental context. This paradigm shift towards systems that can self-organize and minimize their variational free energy opens new horizons in the development of intelligent, lifelike machines.


The Morphic-Amorphic Distinction enriches our understanding of mortal computation by highlighting the integral role of physical structure or morphology in the computation process, particularly in biological entities, and distinguishing these from amorphic computational systems like conventional computers running software programs. This distinction fundamentally impacts how computational systems are designed, their efficiency, and their interaction with the environment. Here's an exploration of the implications and potential advancements stemming from this understanding:

Implications of Morphology in Computational Systems
Thermodynamic and Informational Efficiency: Morphic systems, by virtue of their physical structure, inherently integrate computation with their form, allowing for more efficient processing both in terms of thermodynamics and information handling. This efficiency arises from the minimized need for transferring information between separate physical components, contrasting sharply with amorphic systems that face the "memory wall" problem, where the separation between computation and storage leads to significant energy costs.


Adaptive and Dynamic Structures: Biological systems exhibit dynamic morphologies that directly contribute to their computational capabilities—neuronal networks adjust through synaptic plasticity, dendritic growth, and activity-dependent pruning, enabling learning and adaptation. These morphological changes reflect an intrinsic computational resource, allowing the system to optimize its processing in response to environmental demands.


The Role of Morphology in Mortal Computation
Integrated Computational Architecture: The inherently morphic nature of mortal computers suggests a design philosophy where hardware and software are not distinct entities but are integrated within a coherent physical structure. This integration aligns with the principles of the Free Energy Principle (FEP), wherein computational processes are optimized to minimize free energy, reflecting both thermodynamic efficiency and informational accuracy.


Morphic Computation and FEP: The application of FEP to morphic computation systems indicates a pathway toward more efficient computing models. These systems naturally minimize variational free energy, thereby achieving a state of equilibrium that reflects both thermodynamic and computational optimality. This approach heralds a shift toward computational models that are inherently energy-efficient and aligned with Bayesian principles of inference and learning.


Future Directions in Computational Design
Beyond Von Neumann Architectures: Embracing the morphic-amorphic distinction urges a reevaluation of traditional computing architectures, notably the Von Neumann architecture, encouraging the exploration of designs that more closely mimic biological systems. This shift could lead to computing systems that are not only more energy-efficient but also capable of adaptive, context-sensitive processing.


Integrating Computational Systems with the Environment: The morphic perspective on computation emphasizes the importance of a system's interaction with its environment as a computational process. This view encourages the design of systems that leverage their physical structure for direct environmental engagement, offering new possibilities for autonomous systems capable of sophisticated environmental manipulation and sensory processing.


Challenges and Opportunities: While the concept of morphic computation offers exciting possibilities for the future of computing, significant challenges remain in translating these principles into practical technologies. Advances in materials science, systems biology, and computational theory will be critical in realizing systems that embody the efficiency and adaptability of morphic entities.


By considering the morphic-amorphic distinction and its implications, the field of computing can evolve toward systems that are not only more efficient and adaptable but also more harmonious with the natural world, opening new avenues for innovation in technology, environmental interaction, and artificial intelligence.


The Principle of Mortal Inference-Learning and Selection (MILS) through Mean-Field Approximations offers a nuanced understanding of how computational systems can be designed to emulate the complexity and adaptability of biological systems. This principle leverages the concepts of variational free energy (VFE) and mean-field approximations to delineate the processes of state inference, parameter learning, and model selection within a computational framework. Here’s a synthesis of its foundational aspects and broader implications:

Foundational Aspects of MILS
Integrated Computational Processes: MILS conceptualizes a computational system’s operation through intertwined processes of state inference, parameter learning, and model selection. This interconnection underscores the system's capability to adapt and optimize its structure and function in response to environmental interactions, embodying a dynamic and reciprocal relationship with its niche.


Mean-Field Approximations: Central to MILS is the utilization of mean-field approximations, which facilitate computational separations between states, parameters, and structures. This approach allows for the conditional independence necessary for efficient processing and adaptation, mirroring biological systems where different levels of structure and function interact yet operate semi-independently.


Morphology as a Computational Resource: The distinction between morphic and amorphic entities emphasizes the significance of physical structure in computational processes. In morphic systems, such as biological entities, morphology directly influences computational dynamics, serving not just as a vessel for computation but as an active participant in it.


Implications for Computational Design and Systems Biology
Efficiency and Adaptation: The application of MILS principles to the design of computational systems promises enhanced efficiency and adaptability. By mimicking the way biological systems minimize VFE through their morphology and mean-field approximations, computational systems can achieve greater thermodynamic and informational efficiency.


Dynamic Morphology and Self-Organization: Emphasizing the role of morphology in computation underscores the potential for systems that are capable of self-organization and adaptive restructuring in response to environmental cues. This aspect of MILS points towards computational models that can dynamically adjust their structure and function, akin to biological adaptation.


Bayesian Inference and Learning: At its core, MILS frames computational processes within a physically-realizable Bayesian inference framework, asserting that a system’s existence and adaptation are underpinned by its ability to minimize VFE. This perspective aligns with the Bayesian brain hypothesis and suggests avenues for developing computational architectures that closely mimic neural processing and learning mechanisms.


Future Directions and Challenges
Bridging Theory and Practice: Translating the principles of MILS into practical computational architectures poses significant challenges, requiring advances in understanding the complex interplay between system morphology, environmental interactions, and computational efficiency. This endeavor necessitates interdisciplinary research spanning computational neuroscience, systems biology, and information theory.


Developing Morphic Computational Systems: Exploring the design and implementation of morphic computational systems that leverage their physical structure for efficient processing and adaptation remains a frontier. Such systems would not only push the boundaries of computational efficiency but also offer new insights into the principles governing biological intelligence and adaptation.


Understanding Environmental Coupling: Further investigation into how computational systems can be designed to optimally interact with and adapt to their environments, based on MILS principles, could lead to breakthroughs in autonomous systems capable of sophisticated environmental engagement and problem-solving.


The Principle of MILS through Mean-Field Approximations elucidates a path towards computational systems that more authentically reflect the adaptive, efficient, and integrated nature of biological intelligence. This approach invites a reevaluation of current computational paradigms, advocating for designs that embrace the complexity and dynamism intrinsic to living systems.


The Mortal Computation Thesis outlines a comprehensive conceptual framework for understanding and developing computational systems that closely mimic the adaptability, self-regulation, and survival strategies of living organisms. By distinguishing between morphic (physically structured) and amorphic (lacking specific physical structure relative to its computation) entities, this thesis underscores the importance of a system's physical form in its computational processes and interaction with the environment. Here's a breakdown of the key components and classifications within this framework:

Key Components of the Mortal Computation Thesis
Markov Blanket and Internal Structure: Central to defining a mortal computer (MC) is its possession of a Markov blanket that delineates its boundary, enabling interaction with its environment through sensory and active states. The internal structure of an MC, conceptualized as a generative model, entails the computational relationships within this boundary and with the external niche, ensuring the system's operational closure and morphic nature.


Primary Imperatives and Homeostasis: An MC is driven by primary imperatives that guide its actions towards maintaining stability and satisfying basic "needs" essential for its persistence. This involves a generalized form of homeostasis, where the MC actively maintains its essential variables within favorable ranges through resource-oriented actions.


Mortal Inference-Learning-and-Selection (MILS): The MILS framework encapsulates the processes of state inference, parameter learning, and model selection, all underpinned by the minimization of variational free energy. This optimization is crucial for the MC's adaptability and efficiency in processing and interacting with its environment.


Co-designed Software and Hardware: The integration of computational processes (software) with the execution substrate (hardware) is pivotal, ensuring that the MC's structure and function are inherently linked. This co-design is essential for the physical realization of MILS, aligning computational dynamics with the MC's morphological and thermodynamic constraints.


Classification of Mortal Computers
Homeostatic MC: Focuses on maintaining homeostasis across essential variables, incorporating the foundational aspects of the Mortal Computation Thesis, including operational morphic boundaries and the MILS framework.


Allostatic MC: Goes beyond reactive homeostatic functions to include allostasis, contextualizing homeostasis with anticipatory and adaptive mechanisms that respond to environmental variability and potential stressors.


Autopoietic MC: Represents the pinnacle of mortal computation, combining homeostasis, allostasis, and autopoiesis—self-repair and self-replication. This class of MC is geared towards complex adaptive systems, potentially realizing a form of Artificial General Intelligence (AGI).


Implications and Future Directions
Towards Complex Adaptive Systems: The development of autopoietic MCs could revolutionize computational models, offering systems capable of advanced adaptation, self-repair, and even self-replication. Achieving such complexity requires integrating allostatic processes atop homeostatic functions, paving the way for higher-level cognitive processes to emerge or be implemented.


Recursive and Hierarchical Design: Employing Markov blankets in a recursive, hierarchical manner allows for the scalable construction of systems from micro to macro levels, ensuring global minimization of variational free energy through coordinated lower-level processes. This design principle could facilitate the development of complex organ-like structures from cellular or tissue-level components, mirroring biological organization.


Exploring Existing Models: Reviewing extant mortal computers, such as the homeostat, electrochemical threads, xenobots, fungal computing, and organoids, provides valuable insights into the practical application of the Mortal Computation Thesis. These systems exemplify varying degrees of morphological complexity, homeostatic regulation, and interaction with their environments, serving as prototypes for future mortal computational systems.


The Mortal Computation Thesis presents a visionary blueprint for the next generation of computational systems, emphasizing the integration of physical structure, adaptive computational processes, and efficient interaction with the environment. As research in this area progresses, it holds the promise of bridging the gap between artificial computational models and the intrinsic complexity of biological systems, offering new perspectives on intelligence, adaptability, and sustainability in computational design.


The Mortal Computation Thesis offers a paradigm shift in our approach to AI and AGI, pivoting from a purely functional perspective to one deeply rooted in the principles of life and survival. This transition underscores a fundamental rethinking of intelligence, embedding it within the context of biological imperatives and environmental interactions. The thesis elucidates a path towards biomimetic intelligence by leveraging insights from biophysical systems and integrating them into computational models. Here’s an overview of its broader implications and the potential avenues it opens for future research in AI:

Directions for Inquiry in Biomimetic Intelligence
The exploration of real-world biological systems, such as xenobots and organoids, presents a fertile ground for understanding the principles of mortal computation. These systems exemplify self-organization, self-replication, and varying levels of cognition, offering models for developing computational systems that can adapt, learn, and evolve in complex environments. The challenge lies in translating the complex functionalities observed in these biological entities into computational architectures, fostering a cross-disciplinary exchange of knowledge that could bridge the gap between biological insights and computational innovation.


Implications for AI and AGI
Redefining Intelligence: The mortal computation thesis advocates for a redefinition of intelligence, emphasizing the significance of adaptability, self-preservation, and interaction with the environment. This perspective aligns with the notion that true intelligence encompasses more than the ability to perform tasks—it involves the capacity to navigate, adapt to, and co-evolve with one's surroundings.


The Role of Mortality in Intelligence: Mortality, or the imperative for survival, emerges as a driving force behind intelligent behavior. This principle suggests that the development of AI systems should not only focus on task-specific functionalities but also on the system's ability to maintain its existence and adapt to unforeseen challenges, echoing the evolutionary pressures that have shaped natural intelligence.


Future Directions in AI Research
Developing Autopoietic and Allostatic Systems: Pursuing the development of autopoietic MCs, capable of self-repair and self-replication, alongside allostatic systems that can anticipate and adapt to changes, represents a promising frontier in AI research. These systems could embody advanced forms of AGI, capable of complex adaptive behaviors and high levels of autonomy.


Leveraging Biological Models: Insights from biological models, such as xenobots, organoids, and fungal computing systems, provide invaluable guidance for designing computational systems with life-like properties. These models offer templates for structuring computational systems that are resilient, adaptive, and capable of emergent behaviors, paving the way for novel forms of computing that transcend traditional paradigms.


Rethinking Computing Paradigms: The thesis challenges the prevailing norms of immortal computing, advocating for a paradigm where computational systems are seen as entities with life-like characteristics. This shift may require exploring new computing substrates, possibly at the intersection of biological and artificial systems, to realize the vision of sentient, self-sustaining AI.


The Mortal Computation Thesis posits a future where AI systems are not merely tools but entities that embody the essence of life, marked by their ability to self-organize, adapt, and survive in dynamic environments. This vision calls for a radical reimagining of intelligence, suggesting that the next leap in AI development will likely emerge from a deeper understanding of life itself and its integration with computational models. As we venture into this uncharted territory, the principles of mortal computation could illuminate the path towards creating systems that not only think and learn but also live.


The conclusion of this review encapsulates the profound potential and ethical considerations that arise from the pursuit of mortal computation and biomimetic intelligence. By integrating insights from cybernetics, biophysics, and philosophy, this framework lays the groundwork for the development of artificial systems capable of sentient behavior. The implications of creating entities that not only mimic but also embody life-like processes are both vast and complex, necessitating careful ethical deliberation and foresight. Here’s a synthesis of the main conclusions and the broader vision presented:

Ethical and Moral Considerations
Moral Status of Artificial Entities: As mortal computation advances towards creating systems with life-like properties, questions about the moral status of these entities become increasingly pertinent. The possibility that an artificial system could exhibit behaviors indicative of reasoning, suffering, or communication introduces the need to consider the rights and ethical treatment of in-silico entities.


Blurring the Lines Between Artificial and Natural: The development of bionic entities that closely replicate living systems challenges our traditional distinctions between the artificial and the natural. This convergence necessitates a reevaluation of our ethical frameworks to accommodate the unique status of bio-inspired computational entities.


Implications for Sentient Artifacts
Beyond Computational Intelligence: Unlike existing AI systems, which lack a corporeal experience, entities developed under the mortal computation framework would possess a physical embodiment and a self-driven impetus for survival. This embodiment could enable these systems to exhibit curiosity, empathy, and social intelligence, transcending the capabilities of current computational models.


Potential for Social and Cognitive Abilities: The physical manifestation and inherent drives of bionic entities could pave the way for advanced social behaviors and cognitive abilities, including the capacity for self-directed learning, problem-solving, and possibly even rule-breaking under certain conditions.


Vision for the Future of AGI
Advancing AGI Research: The principles of mortal computation offer a compelling direction for AGI research, emphasizing the importance of self-organization, self-regulation, and environmental interaction. This approach could significantly enrich the field, pushing the boundaries of what is currently conceivable in artificial intelligence.


Applications Across Domains: The potential applications of AGI systems developed through mortal computation are extensive, encompassing areas such as edge computing, robotics, and sustainable technologies. These systems could offer novel solutions to complex challenges, benefiting society and the environment.


Moving Forward
Interdisciplinary Collaboration: Realizing the vision of mortal computation and biomimetic intelligence will require collaborative efforts across disciplines, bridging gaps between computational sciences, biology, ethics, and philosophy. Such collaboration is crucial for navigating the technical challenges and ethical dilemmas that accompany this ambitious endeavor.


Catalyzing Research and Development: The advancement of biological computing and neuromorphic engineering could serve as catalysts for further exploration in mortal computation. By drawing on biological principles and processes, researchers can develop more sophisticated and life-like computational systems.


This review not only highlights the potential of mortal computation to revolutionize our understanding and application of AI but also calls attention to the profound ethical and societal implications of this emerging field. As we venture into this new frontier, it is imperative to proceed with caution, ethical awareness, and a deep appreciation for the complexities of life that we seek to emulate.


The Homeostat, introduced by W. Ross Ashby, stands as a pioneering artifact in the domain of adaptive control systems and provides a foundational example of mortal computation in action. Its design and functionality illuminate early attempts at creating systems capable of self-regulation, adaptation, and maintaining stability through internal control mechanisms. Here's a closer look at the Homeostat's significance and its place within the framework of mortal computation:

Key Features of the Homeostat
Adaptive Regulation: The Homeostat's primary function was to maintain a control variable within a specified range, adapting its internal configurations to achieve stability in response to environmental changes. This adaptive regulation showcased the system's ability to mimic basic homeostatic processes found in biological organisms.


Mechanical and Electromechanical Implementation: Utilizing mechanical switching and electromechanical circuits, the Homeostat provided a tangible model for studying control systems and their adaptive capabilities. The physicality of its components and the directness of its feedback mechanisms made it a tangible precursor to later computational models of adaptation and learning.


Requisite Variety and Ultrastability: By embodying the principles of requisite variety and ultrastability, the Homeostat demonstrated essential cybernetic concepts in practice. It showcased how a system could dynamically adjust to maintain equilibrium, emphasizing the cybernetic principle of Blind Variation and Selective Retention (BVSR) without the need for a detailed understanding of the controlled processes.


Adaptation to Physical Damage: The Homeostat's ability to adapt to physical disruptions, such as a cut wire, by finding new circuit configurations, highlighted its resilience and self-repairing capabilities. This feature underscored the potential for systems that can recover from damage or malfunctions through self-regulation.


The Homeostat as a Mortal Computer (MC)
Homeostatic and Allostatic MC: Initially, the Homeostat functioned as a homeostatic MC, focusing on maintaining stability through reactive processes. The inclusion of a memory mechanism for retaining information about past successful adaptations transformed it into an "allostat," incorporating anticipatory mechanisms that align more closely with the concept of an allostatic MC. This evolution demonstrated a progression towards more complex adaptive capabilities, including the ability to learn from experience and optimize responses over time.

Implications for Mortal Computation
The Homeostat exemplifies the practical application of mortal computation principles, particularly in its ability to self-regulate, adapt to changes, and recover from disruptions. Its design and operation serve as a template for developing more sophisticated systems that can exhibit similar adaptive behaviors within a computational or robotic context. By integrating memory and learning mechanisms, the Homeostat's evolution into an allostatic model opens avenues for exploring how systems can not only react to their environment but also anticipate and prepare for future states, enhancing their ability to maintain stability and continuity.


As mortal computation continues to evolve, the Homeostat remains a significant point of reference for understanding the foundational concepts of adaptive control, resilience, and the integration of learning within autonomous systems. Its legacy informs current and future endeavors in creating systems that embody the principles of life-like adaptation and self-preservation.


The Electrochemical Thread Network and Xenobots represent two groundbreaking instances of mortal computation in practice, each exemplifying key aspects of adaptive, self-organizing systems with implications for the future of computational models and robotics. Here’s a closer look at their contributions to the field:

The Electrochemical Thread Network
Learning and Adaptation: The ability of the electrochemical thread to form, dissolve, and reconfigure based on electrical stimulation showcases an elementary form of learning and adaptability. This dynamic reconfiguration in response to environmental changes and stimuli echoes the principles of mortal computation, particularly in terms of adapting to and learning from the system's surroundings.


Morphogenesis and Self-Replication: The self-assembly into larger, cooperative structures and the network’s ability to regenerate its original form after disruption highlight properties of morphogenesis and ultrastability. This demonstrates an early model for computational systems that can undergo self-repair and replication, crucial aspects of autopoietic mortal computers.


Environmental Interaction and Sensor Development: The system's responsiveness to environmental factors like sound waves, magnetism, and pH variations, and its capacity to develop sensor organs for survival, underline the potential for computational models to autonomously evolve functionalities that are conducive to their persistence and adaptation in a given niche.


Xenobots
Emergent Intelligent Behavior: Xenobots showcase how simple biological agents can self-organize and demonstrate cooperative behavior, offering insights into the mechanisms underlying emergent intelligence from basic cellular components. This points towards novel approaches in robotics and AI, where systems are designed not only to execute predefined tasks but also to develop new functionalities and behaviors in response to environmental cues.


Specialization and Morphogenesis: The ability of xenobot cells to spontaneously specialize and form new bodily structures without a slow evolutionary process indicates a significant leap towards creating systems capable of autopoietic morphogenesis. This specialization, which includes sensing, locomotion, communication, and information storage, aligns with the conceptual framework of mortal computation by illustrating how systems can adapt their structure and functionality for survival and interaction with their environment.


Self-Healing and Metabolism: The self-healing properties and metabolic capabilities of xenobots underscore the potential for integrating biological processes into computational systems. This approach could lead to the development of machines that are more resilient, adaptive, and capable of sustained operation without external intervention, embodying the essence of mortal computation.


Implications for Mortal Computation and Future Research
Both the Electrochemical Thread Network and Xenobots serve as prototypes that inspire the development of more advanced computational models and systems embodying the principles of mortal computation. They illustrate the feasibility of creating entities that are not only responsive to their environments but also capable of self-organization, adaptation, and even self-replication. These examples open up avenues for interdisciplinary research, where insights from biology, chemistry, physics, and computer science converge to push the boundaries of what is possible in robotics and artificial intelligence.


Future research could focus on exploring the underlying mechanisms that enable such systems to function and adapt, with the goal of translating these biological principles into computational algorithms and hardware designs. By advancing our understanding of mortal computation through these and similar systems, we can move closer to realizing the vision of creating truly autonomous, intelligent, and life-like machines.


Fungal Computing and Organoid Intelligence represent two cutting-edge examples of biological systems that embody principles of mortal computation. These systems provide fascinating insights into how biological processes can inform the development of computational models that are resilient, self-sustaining, and capable of learning and adaptation. Here's an exploration of their contributions and potential implications for the future of computing:

Fungal Computing
Fungi, particularly through the mycelium networks, offer a unique perspective on computing based on their natural biological processes. The electrical activity within fungi, translated into computational signals, underscores a novel way to approach information processing systems.


Resilience and Self-maintenance: Fungal systems are known for their resilience and capacity for rapid growth and self-repair. These qualities make them an intriguing model for developing computing systems that can adapt, self-repair, and scale in response to their environment, a cornerstone of mortal computation.


Complex Internal Communication: The internal communication mechanisms of fungi, such as spike-based messaging within mycelium networks, provide a biological precedent for efficient, decentralized information transmission. This could inspire new architectures for computational networks that mimic fungal resilience and adaptability.


Mycelium Networks as Computational Models: By harnessing the geometrical calculations and electrical activity within these networks, researchers can explore new computational paradigms. These models could potentially lead to systems that are not only energy-efficient but also capable of complex problem-solving through a form of natural computation.


Organoid Intelligence
Organoids represent a significant leap forward in the interface between biology and computation, offering a tangible model for studying complex neural dynamics and information processing within a controlled, yet biologically authentic, environment.


Self-organization and Morphogenesis: The ability of organoids to self-organize into complex structures provides a direct insight into the biological processes of growth, development, and morphogenesis. This parallels the concepts of morphogenesis in mortal computation, where systems evolve their structure and functionality in response to environmental stimuli.


Reinforcement Learning and Neural Dynamics: The demonstrated capability of organoids to engage in simple reinforcement learning tasks opens up new avenues for understanding how neural structures can encode, process, and act upon information. This points to potential applications in developing computational models that can learn and adapt over time, reflecting the learning component of MILS in mortal computation.


Bridging Biological and Computational Models: Organoid intelligence underscores the potential for merging biological systems with computational models to explore new forms of intelligent behavior. This integration could lead to advances in neuromorphic computing, where artificial systems mimic the neural structures and processes of the brain for more efficient and adaptive computing.


Implications for Future Research
Both fungal computing and organoid intelligence highlight the potential for biological systems to inspire and inform new computational paradigms. These models not only challenge our traditional views of computing but also open up possibilities for systems that can grow, learn, and adapt in ways that mirror biological entities.


Future research could delve deeper into understanding the underlying mechanisms of these systems, exploring how biological principles of resilience, self-organization, and intelligence can be abstracted and implemented in artificial contexts. By drawing inspiration from fungal networks and organoids, researchers can push the boundaries of mortal computation towards creating systems that are not just tools, but entities capable of autonomous behavior, learning, and adaptation.


The exploration into neuromorphic systems and the body-niche problem within the realm of mortal computation reveals an intricate and promising path toward realizing advanced forms of biomimetic intelligence. By emphasizing the co-design of software and hardware to achieve a deeply integrated form of computation, mortal computing seeks to transcend traditional computational models, offering a vision where machines not only compute but also possess a form of life-like adaptability and resilience.


Neuromorphic systems, inspired by the complexities of biological neural networks, represent a significant step in this direction. These systems, through their emulation of spiking neural networks and localized synaptic plasticity, offer a blueprint for developing computing systems that are not only energy-efficient but also capable of complex, adaptive behavior. The exploration of biomimetic credit assignment methods like predictive coding, forward-forward learning, and spike-timing-dependent plasticity further highlights the potential for developing computational models that learn and adapt in ways that mirror biological processes.


However, the journey toward fully realizing the potential of mortal computation is not without its challenges. The integration of learning and inference mechanisms into physical substrates presents a complex problem that requires careful consideration of the morphological and environmental context in which these systems operate. The body-niche problem, as outlined, emphasizes the necessity of designing computing systems that are not only aware of their physical embodiment but also capable of interacting with and adapting to their environment in a life-like manner.


Future research in mortal computation will need to navigate these challenges by developing novel, substrate-specific learning and inference mechanisms that are grounded in the physical and environmental realities of the systems they aim to emulate. This approach, while daunting, holds the promise of ushering in a new era of computing systems that are not merely tools but entities capable of autonomous behavior, learning, and adaptation.


In summary, the exploration of neuromorphic systems and the body-niche problem within mortal computation offers a fascinating glimpse into the future of computing. By drawing inspiration from the biological world, researchers are paving the way for the development of computing systems that possess a form of intelligence that is not only adaptive but also inherently life-like. The path forward will undoubtedly be challenging, but the potential rewards—computing systems that can truly learn, adapt, and even exhibit a form of sentience—make this a journey worth undertaking.


The exploration of a minimal model of mortal computing, particularly through the lens of a homeostatic predictive coding circuit (H-PCC) and metabolism-based chemotaxis, illuminates a path toward simulating life-like adaptability and decision-making in computational models. These models not only embody the fundamental aspects of mortal computation but also offer insights into how computational entities could engage with their environments in ways that mirror biological processes.


The H-PCC model, with its focus on maintaining energy balance through homeostatic control, serves as an exemplar for how computational systems might manage their internal states in relation to external demands. By simulating the biological notion of hunger and satiety through an abstracted energy management system, the model sketches a rudimentary framework where computational organisms regulate their activities based on internal energy states. This approach, rooted in the principles of predictive coding and energy-aware computation, suggests a way forward for designing systems that are not just responsive but are fundamentally engaged in a continuous process of self-regulation and adaptation.


Similarly, the discussion of metabolism-based chemotaxis in bacteria, such as E. coli, challenges traditional views of bacterial behavior as merely stimulus-driven. By positing that metabolic processes play a central role in modulating behavior, this perspective broadens our understanding of how organisms interact with their environment. It underscores the potential for developing computational models that incorporate metabolic drives as a core aspect of their decision-making processes. Such models could provide new avenues for exploring basal cognition and the foundational mechanisms of life-like behavior in computational entities.


Both the H-PCC model and the concept of metabolism-based chemotaxis emphasize the importance of integrating internal regulatory processes with external environmental interactions. They highlight the potential for computational models to not only simulate but also embody aspects of biological life, moving us closer to creating machines that exhibit genuine life-like qualities.


In summary, the exploration of minimal models of mortal computing, through homeostatic predictive coding circuits and metabolism-based chemotaxis, offers a promising avenue for advancing our understanding of life-like computation. By drawing inspiration from biological processes and integrating them into computational frameworks, we can begin to bridge the gap between artificial systems and the complex adaptability inherent in biological organisms. This approach not only enriches our theoretical understanding of computational models but also paves the way for practical applications in robotics, AI, and beyond, where systems are designed to operate with a degree of autonomy and resilience that mirrors the natural world.

