Probabilistic Models of Cognition

by Noah D. Goodman, Joshua B. Tenenbaum & The ProbMods Contributors 

Standard Galactic Version

This book explores the probabilistic approach to cognitive science, which models learning and reasoning as inference in complex probabilistic models. 
We examine how a broad range of empirical phenomena, including intuitive physics, concept learning, causal reasoning, social cognition, and language 
understanding, can be modeled using probabilistic programs (using the WebPPL language). 

Contributors
This book is an open source project. We welcome content contributions (via GitHub)!

The ProbMods Contibutors are:
Noah D. Goodman (editor)
Joshua B. Tenenbaum
Daphna Buchsbaum
Joshua Hartshorne
Robert Hawkins
Timothy J. O’Donnell
Michael Henry Tessler

Citation
N. D. Goodman, J. B. Tenenbaum, and The ProbMods Contributors (2016). Probabilistic Models of Cognition (2nd ed.). Retrieved 2022-4-
14 from https://probmods.org/
[bibtex]

Acknowledgments
We are grateful for crucial technical assitance from: Andreas Stuhlmüller, Tomer Ullman, John McCoy, Long Ouyang, Julius Cheng.

The construction and ongoing support of this tutorial are made possible by grants from the Office of Naval Research, the James S. McDonnell
Foundation, the Stanford VPOL, and the Center for Brains, Minds, and Machines (funded by NSF STC award CCF-1231216).

Previous edition
The first edition of this book used the probabilistic programming language Church and can be found here. 

Chapters

1.Introduction
A brief introduction to the philosophy.

2.Generative models
Representing working models with probabilistic programs.

3.Conditioning
Asking questions of models by conditional inference.

4.Causal and statistical dependence
Causal and statistical dependence.

5.Conditional dependence
Patterns of inference as evidence changes.

6.Social cognition
Inference about inference

7.Interlude - Bayesian data analysis
Making scientific inferences from data.

8.Interlude - Algorithms for inference
Approximate inference. Efficiency tradeoffs of different algorithms.

9.Rational process models
The psychological reality of inference algorithms.

10.Learning as conditional inference
How inferences change as data accumulate.

11.Learning with a language of thought
Compositional hypothesis spaces.

12.Hierarchical models
The power of statistical abstraction.

13.Occam's Razor
How inference penalizes extra model flexibility.

14.Mixture models
Models for inferring the kinds of things.

15.Learning (deep) continuous functions
Functional hypothesis spaces and deep probabilistic models

16.Appendix - JavaScript basics
A very brief primer on JavaScript.

17.Appendix - Useful distributions
A very brief summary of some important distributions.

☰

Introduction

What is thought? How can we describe the intelligent inferences made in everyday human reasoning and learning? How can we engineer intelligent
machines? The computational theory of mind aims to answer these questions starting from the hypothesis that the mind is a computer, mental
representations are computer programs, and thinking is a computational process – running a computer program.

But what kind of program? A natural assumption is that these programs take the inputs – percepts from the senses, facts from memory, etc –
and compute the outputs – the intelligent behaviors. Thus the mental representations that lead to thinking are functions from inputs to
outputs. However, this input-output view suffers from a combinatorial explosion: we must posit an input-output program for each task in which
humans draw intelligent inferences.

A different approach is to assume that mental representations are more like theories in science: pieces of knowledge that can support many
inferences   in   many   different   situations.   For   instance,   Newton’s   theory   of   motion   makes   predictions   about   infinitely   many   different
configurations of objects and can be used to reason both forward in time and backward, from final state of a physical system to the initial state.
The generative approach   to   cognition   posits   that   some   mental   representations   are   more   like   theories   in   this   way:   they   capture   general
descriptions of how the world works. A generative model describes a process, usually one by which observable data is generated, that represents
knowledge about the causal structure of the world. These generative processes are simplified “working models” of a domain. Other mental
computations operate over these generative models to draw inferences: many different questions can be answered by interrogating the mental
model. This contrasts to a more procedural or mechanistic approach in which knowledge represents the input-output mapping for a particular
question directly.

It is possible to use deterministic generative models to describe possible ways a process could unfold, but due to sparsity of observations or actual
randomness there will often be many ways that our observations could have been generated. How can we choose amongst them? Probability theory
provides a system for reasoning under exactly this kind of uncertainty. Probabilistic generative models describe processes which unfold with some
amount of randomness, and probabilistic inference describes ways to ask questions of such processes. This book is concerned with the knowledge
that can be represented by probabilistic generative models and the inferences that can be drawn from them.

In order to make the idea of generative models precise we want a formal language that is designed to express the kinds of knowledge individuals
have about the world. This language should be universal in the sense that it should be able to express any (computable) process. We build on the
representational power of programming languages because they describe computational processes and capture the idea that what is important is
causal dependence—in particular programs do not focus on the sequence of time, but rather on which events influence which other events (the
“flow of data”). We introduce randomness into this language to construct a  probabilistic programming language (PPL), and describe conditional
inferences in this language. By using a PPL we are able to parsimoniously describe rich generative model structures and explore the inference
patterns that emerge from the interaction of model and data.

Reading & Discussion: Readings
Next chapter: 2. Generative models 

☰

Generative models

Models, simulation, and degrees of belief

One view of knowledge is that the mind maintains working models of parts of the world. ‘Model’ in the sense that it captures some of the
structure in the world, but not all (and what it captures need not be exactly what is in the world—just what is useful). ‘Working’ in the sense
that it can be used to simulate this part of the world, imagining what will follow from different initial conditions. As an example take the Plinko
machine: a box with uniformly spaced pegs, with bins at the bottom. Into this box we can drop marbles:

The plinko machine is a ‘working model’ for many physical processes in which many small perturbations accumulate—for instance a leaf falling
from a tree. It is an approximation to these systems because we use a discrete grid (the pegs) and discrete bins. Yet it is useful as a model: for
instance, we can ask where we expect a marble to end up depending on where we drop it in, by running the machine several times—simulating the
outcome.

Imagine that someone has dropped a marble into the plinko machine; before looking at the outcome, you can probably report how much you believe
that the ball has landed in each possible bin. Indeed, if you run the plinko machine many times, you will see a shape emerge in the bins. The
number of balls in a bin gives you some idea how much you should expect a new marble to end up there. This ‘shape of expected outcomes’ can be
formalized as a probability distribution (described below). Indeed, there is an intimate connection between simulation, expectation or belief, and
probability, which we explore in the rest of this section.

There is one more thing to note about our Plinko machine above: we are using a computer program to simulate the simulation. Computers can be seen
as universal simulators. How can we, clearly and precisely, describe the simulation we want a computer to do?

Building Generative Models

We wish to describe in formal terms how to generate states of the world. That is, we wish to describe the causal process, or steps that unfold,
leading to some potentially observable states. The key idea of this section is that these generative processes can be described as  computations—
computations that involve random choices to capture uncertainty about the process.

Programming languages are formal systems for describing what (deterministic) computation a computer should do. Modern programming languages
offer a wide variety of different ways to describe computation; each makes some processes simple to describe and others more complex. However,
a key tenet of computer science is that all of these languages have the same fundamental power: any computation that can be described with one
programming language can be described by another. (More technically this Church-Turing thesis posits that many specific computational systems
capture the set of all effectively computable procedures. These are called universal systems.)

In this book we will build on the JavaScript language, which is a portable and flexible modern programming language. The  WebPPL language takes a
subset of JavaScript and extends it with pieces needed to describe probabilistic computation. The key idea is that we have primitive operations that
describe not only deterministic functions (like  and ) but stochastic operations. For example, the  flip  function can be thought of as simulating a
(possibly biased) coin toss (technically  flip  samples from a Bernoulli distribution, which we’ll return to shortly):

flip()
run▼

Run this program a few times. You will get back a different sample on each execution. Also, notice the parentheses after   flip . These are
meaningful; they tell WebPPL that you are calling the  flip  function—resulting in a sample. Without parentheses  flip  is a function object—a
representation of the simulator itself, which can be used to get samples.

In WebPPL, each time you run a program you get a sample by simulating the computations and random choices that the program specifies. If you run
the program many times, and collect the values in a histogram, you can see what a typical sample looks like:

viz(repeat(1000, flip))
run▼

Here we have used the  repeat  procedure which takes a number of repetitions, KK, and a function (in this case  flip , note not  flip() ) and 
returns a list of KK samples from that function. We have used the  viz  function to visualize the results of calling the  flip  function 1000 times.
As you can see, the result is an approximately uniform distribution over  true  and  false .
Using  flip  we can construct more complex expressions that describe more complicated sampling processes. For instance here we describe a process 
that samples a number adding up several flips (note that in JavaScript a boolean will be turned into a number, 00 or 11, by the plus operator  + ):

flip() + flip() + flip()
run▼

What if we want to invoke this sampling process multiple times? We would like to construct a stochastic function that adds three random numbers
each time it is called. We can use  function  to construct such complex stochastic functions from the primitive ones.

var sumFlips = function() {
  return flip() + flip() + flip()
}
viz(repeat(100, sumFlips))
run▼

A function expression with an empty argument list,  function() {...} , is called a thunk: this is a function that takes no input arguments. If we
apply a thunk (to no arguments!) we get a return value back, for example   flip() . Complex functions can also have arguments. Here is a stochastic
function that will only sometimes double its input:

var noisyDouble = function (x) { return flip() ? x+x : x }
noisyDouble(3);
run▼

By using higher-order functions we can construct and manipulate complex sampling processes. We use the ternary operator   condition ? if-true :
if-false  to induce hierarchy. A good example comes from coin flipping…

Example: Flipping Coins

The following program defines a fair coin, and flips it 20 times:

var fairCoin = function() { return flip(0.5) ? 'h' : 't' }
viz(repeat(20, fairCoin))
run▼

This program defines a “trick” coin that comes up heads most of the time (95%), and flips it 20 times:

var trickCoin = function() { return flip(0.95) ? 'h' : 't' }
viz(repeat(20, trickCoin))
run▼

The higher-order function  make-coin  takes in a weight and outputs a function (a thunk) describing a coin with that weight. Then we can
use  make-coin  to make the coins above, or others.

var makeCoin = function (weight) {
  return function() { return flip(weight) ? 'h' : 't' }
}

var fairCoin = makeCoin(0.5)
var trickCoin = makeCoin(0.95)
var bentCoin = makeCoin(0.25)

viz(repeat(20, fairCoin))
viz(repeat(20, trickCoin))
viz(repeat(20, bentCoin))
run▼

We can also define a higher-order function that takes a “coin” and “bends it”:

var makeCoin = function (weight) {
  return function() { return flip(weight) ? 'h' : 't' }
}

var bend = function (coin) {
  return function() {
    return coin() == 'h' ? makeCoin(0.7)() : makeCoin(0.1)()
  }
}

var fairCoin = makeCoin(0.5)
var bentCoin = bend(fairCoin)
viz(repeat(100, bentCoin))
run▼

Make sure you understand how the  bend  function works! Why are there an “extra” pair of parentheses after each  make-coin  statement?

Higher-order functions like  repeat ,  map , and  apply  can be quite useful. Here we use them to visualize the number of heads we expect to see
if we flip a weighted coin (weight = 0.8) 10 times. We’ll repeat this experiment 1000 times and then use   viz  to visualize the results. Try
varying the coin weight or the number of repetitions to see how the expected distribution changes.

var makeCoin = function (weight) {
  return function() { return flip(weight) }
}

var coin = makeCoin(0.8)
var data = repeat(1000, function() { return sum(repeat(10, coin)) })
viz(data, {xLabel: '# heads'})
run▼

Example: Causal Models in Medical Diagnosis

Generative knowledge is often causal knowledge that describes how events or states of the world are related to each other. As an example of
how causal knowledge can be encoded in WebPPL expressions, consider a simplified medical scenario:

var lungCancer = flip(0.01)
var cold = flip(0.2)
var cough = cold || lungCancer

cough
run▼

This program models the diseases and symptoms of a patient in a doctor’s office. It first specifies the base rates of two diseases the patient
could have: lung cancer is rare while a cold is common, and there is an independent chance of having each disease. The program then specifies a
process for generating a common symptom of these diseases – an effect with two possible causes: The patient coughs if they have a cold or lung
cancer (or both).

Here is a more complex version of this causal model:

var lungCancer = flip(0.01)
var TB = flip(0.005)
var stomachFlu = flip(0.1)
var cold = flip(0.2)
var other = flip(0.1)

var cough = 
    (cold && flip(0.5)) ||
    (lungCancer && flip(0.3)) ||
    (TB && flip(0.7)) ||
    (other && flip(0.01))

var fever = 
    (cold && flip(0.3)) ||
    (stomachFlu && flip(0.5)) ||
    (TB && flip(0.1)) ||
    (other && flip(0.01))

var chestPain = 
    (lungCancer && flip(0.5)) ||
    (TB && flip(0.5)) ||
    (other && flip(0.01))

var shortnessOfBreath = 
    (lungCancer && flip(0.5)) ||
    (TB && flip(0.2)) ||
    (other && flip(0.01))

var symptoms = {
  cough: cough,
  fever: fever,
  chestPain: chestPain,
  shortnessOfBreath: shortnessOfBreath
}

symptoms
run▼

Now there are four possible diseases and four symptoms. Each disease causes a different pattern of symptoms. The causal relations are now
probabilistic: Only some patients with a cold have a cough (50%), or a fever (30%). There is also a catch-all disease category “other”, which has a
low probability of causing any symptom. Noisy logical functions—functions built from and ( && ), or ( || ), and  flip —provide a simple but expressive
way to describe probabilistic causal dependencies between Boolean (true-false valued) variables.

When you run the above code, the program generates a list of symptoms for a hypothetical patient. Most likely all the symptoms will be false, as
(thankfully) each of these diseases is rare. Experiment with running the program multiple times. Now try modifying the  var  statement for one
of   the   diseases,   setting   it   to   be   true,   to   simulate   only   patients   known   to   have   that   disease.   For   example,   replace  var   lungCancer   =
flip(0.01)  with  var lungCancer = true . Run the program several times to observe the characteristic patterns of symptoms for that disease.

Prediction, Simulation, and Probabilities

Suppose that we flip two fair coins, and return the list of their values:

[flip(), flip()]
run▼

How can we predict the return value of this program? For instance, how likely is it that we will see  [true, false] ? A probability is a number 
between 0 and 1 that expresses the answer to such a question: it is a degree of belief that we will see a given outcome, such as  [true, false] . 
The probability of an event AA (such as the above program returning  [true, false] ) is usually written as: P(A)P(A).
A probability distribution is the probability of each possible outcome of an event. For instance, we can examine the probability distribution on
values that can be returned by the above program by sampling many times and examining the histogram of return values:

var randomPair = function() { return [flip(), flip()] }
viz.hist(repeat(1000, randomPair), 'return values')
run▼

We see by examining this histogram that  [true, false]  comes out about 25% of the time. We may define the probability of a return value to be
the fraction of times (in the long run) that this value is returned from evaluating the program – then the probability of   [true, false]  from
the above program is 0.25.

Distributions in WebPPL

An important idea is that  flip  can be thought of in two different ways. From one perspective,  flip  is a procedure which returns a sample from a
fair coin. That is, it’s a sampler or simulator. As we saw above we can build more complex samplers by building more complex functions. From
another perspective,  flip  is itself a characterization of the probability distribution over  true  and  false . In order to make this view explicit,
WebPPL has a special type of distribution objects. These are objects that can be sampled from using the  sample  operator, and that can explicitly
return the probability of a return value using the  score  method. Distributions are made by a family of distribution constructors:

//make a distribution using the Bernoulli constructor:
var b = Bernoulli({p: 0.5})

//sample from it with the sample operator:
print( sample(b) )

//compute the log-probability of sampling true:
print( b.score(true) )

//visualize the distribution:
viz(b)
run▼

In fact  flip(x)  is just a helper function that constructs a Bernoulli distribution and samples from it. The function   bernoulli(x)  is an alias
for  flip . There are many other distribution constructors built into WebPPL listed  here (and each has a sampling helper, named in lower case).
For instance the Gaussian (also called Normal) distribution is a very common distribution over real numbers:

//create a gaussian distribution:
var g = Gaussian({mu: 0, sigma: 1})

//sample from it:
print( sample(g) )

//can also use the sampling helper (note lower-case name):
print( gaussian(0,1) )

//and build more complex processes!
var foo = function() { return gaussian(0,1) * gaussian(0,1) }

foo()
run▼

Constructing marginal distributions:  Infer

Above we described how complex sampling processes can be built as complex functions, and how these sampling processes implicitly specify a
distribution on return values (which we examined by sampling many times and building a histogram). This distribution on return values is called
the marginal distribution, and the WebPPL  Infer  operator gives us a way to make this implicit distribution into an explicit distribution object:

//a complex function, that specifies a complex sampling process:
var foo = function() { return gaussian(0, 1) * gaussian(0, 1) }

//make the marginal distributions on return values explicit:
var d = Infer({method: 'forward', samples: 1000}, foo)

//now we can use d as we would any other distribution:
print( sample(d) )
viz(d)
run▼

Note that  Infer  took an object describing how to construct the marginal distribution (which we will describe more later) and a thunk describing
the sampling process, or model, of interest. For more details see the Infer documentation.

Thus  sample  lets us sample from a distribution, and build complex sampling processes by using sampling in a program; conversely,  Infer  lets us
reify the distribution implicitly described by a sampling process.

When  we  think  about probabilistic  programs  we  will often  move back  and  forth  between  these  two  views, emphasizing  either  the  sampling
perspective or the distributional perspective. With suitable restrictions this duality is complete: any WebPPL program implicitly represents a
distribution and any distribution can be represented by a WebPPL program; see e.g., Ackerman et al. (2011) for more details on this duality.

The rules of probability

While  Infer  lets us build the marginal distribution for even very complicated programs, we can also derive these marginal distributions with the
“rules of probability”. This is intractable for complex processes, but can help us build intuition for how distributions work.

Product Rule

In the above example we take three steps to compute the output value: we sample from the first   flip() , then from the second, then we make a
list from these values. To make this more clear let us re-write the program as:

var A = flip()
var B = flip()
var C = [A, B]
C
run▼

We can directly observe (as we did above) that the probability of  true  for  A  is 0.5, and the probability of  false  from  B  is 0.5. Can we use 
these two probabilities to arrive at the probability of 0.25 for the overall outcome  C  =  [true, false] ? Yes, using the product rule of 
probabilities: The probability of two random choices is the product of their individual probabilities. The probability of several random choices 
together is often called the joint probability and written as P(A,B)P(A,B). Since the first and second random choices must each have their 
specified values in order to get  [true, false]  in the example, the joint probability is their product: 0.25.
We must be careful when applying this rule, since the probability of a choice can depend on the probabilities of previous choices. For instance, we
can visualize the the exact probability of  [true, false]  resulting from this program using  Infer :

var A = flip()
var B = flip(A ? 0.3 : 0.7)

Infer({method: 'forward', samples: 1000}, function() {
  var A = flip()

  var B = flip(A ? 0.3 : 0.7)

  return {'B': B, 'A': A}
})
run▼

In general, the joint probability of two random choices AA and BB made sequentially, in that order, can be written as P(A,B)=P(A)P(B|
A)P(A,B)=P(A)P(B|A). This is read as the product of the probability of AA and the probability of “BB given AA”, or “BB conditioned on AA”. 
That is, the probability of making choice BB given that choice AA has been made in a certain way. Only when the second choice does not depend on 
(or “look at”) the first choice does this expression reduce to a simple product of the probabilities of each choice 
individually: P(A,B)=P(A)P(B)P(A,B)=P(A)P(B).
What is the relation between P(A,B)P(A,B) and P(B,A)P(B,A), the joint probability of the same choices written in the opposite order? The only 
logically consistent definitions of probability require that these two probabilities be equal, so P(A)P(B|A)=P(B)P(A|B)P(A)P(B|A)=P(B)P(A|B). 
This is the basis of Bayes’ theorem, which we will encounter later.
Sum Rule

Now let’s consider an example where we can’t determine from the overall return value the sequence of random choices that were made:

flip() || flip()
run▼

We can sample from this program and determine that the probability of returning  true  is about 0.75.

We cannot simply use the product rule to determine this probability because we don’t know the sequence of random choices that led to this 
return value. However we can notice that the program will return true if the two component choices are  [true, true] , or  [true, false] , 
or  [false, true] . To combine these possibilities we use another rule for probabilities: If there are two alternative sequences of choices that 
lead to the same return value, the probability of this return value is the sum of the probabilities of the sequences. We can write this using 
probability notation as: P(A)=∑BP(A,B)P(A)=∑BP(A,B), where we view AA as the final value and BB as a random choice on the way to that value.
Using the product rule we can determine that the probability in the example above is 0.25 for each sequence that leads to return value  true , 
then, by the sum rule, the probability of  true  is 0.25+0.25+0.25=0.75.
Using the sum rule to compute the probability of a final value is called is sometimes called marginalization, because the final distribution is the
marginal distribution on final values. From the point of view of sampling processes marginalization is simply ignoring (or not looking at) intermediate
random values that are created on the way to a final return value. From the point of view of directly computing probabilities, marginalization is
summing over all the possible “histories” that could lead to a return value. Putting the product and sum rules together, the marginal
probability of return values from a program that we have explored above is the sum over sampling histories of the product over choice
probabilities—a computation that can quickly grow unmanageable, but can be approximated by  Infer .

Stochastic recursion

Recursive functions are a powerful way to structure computation in deterministic systems. In WebPPL it is possible to have a stochastic recursion 
that randomly decides whether to stop. For example, the geometric distribution is a probability distribution over the non-negative integers. We 
imagine flipping a (weighted) coin, returning N−1N−1 if the first  true  is on the Nth flip (that is, we return the number of times we 
get  false  before our first  true ):

var geometric = function (p) {
  return flip(p) ? 0 : 1 + geometric(p)
}

var g = Infer({method: 'forward', samples: 1000},
               function() { return geometric(0.6) })
viz(g)
run▼

There is no upper bound on how long the computation can go on, although the probability of reaching some number declines quickly as we go. Indeed,
stochastic recursions must be constructed to halt eventually (with probability 1).

Persistent Randomness:  mem

It is often useful to model a set of objects that each have a randomly chosen property. For instance, describing the eye colors of a set of
people:

var eyeColor = function (person) {
  return uniformDraw(['blue', 'green', 'brown'])
};

  
[eyeColor('bob'), eyeColor('alice'), eyeColor('bob')]
run▼

The results of this generative process are clearly wrong: Bob’s eye color can change each time we ask about it! What we want is a model in which
eye color is random, but persistent. We can do this using a WebPPL built-in:  mem .  mem  is a higher order function that takes a procedure and
produces a memoized version of the procedure. When a stochastic procedure is memoized, it will sample a random value the  first time it is used
with some arguments, but return that same value when called with those arguments thereafter. The resulting memoized procedure has a
persistent value within each “run” of the generative model (or simulated world). For instance consider the equality of two flips, and the
equality of two memoized flips:

flip() == flip()
run▼

var memFlip = mem(flip)
memFlip() == memFlip()
run▼

Now returning to the eye color example, we can represent the notion that eye color is random, but each person has a fixed eye color.

var eyeColor = mem(function (person) {
  return uniformDraw(['blue', 'green', 'brown'])
});

[eyeColor('bob'), eyeColor('alice'), eyeColor('bob')]
run▼

This type of modeling is called random world style (McAllester et al., 2008). Note that we don’t have to specify ahead of time the people whose
eye color we will ask about: the distribution on eye colors is implicitly defined over the infinite set of possible people, but only constructed 
“lazily” when needed. Memoizing stochastic functions thus provides a powerful toolkit to represent and reason about an unbounded set of 
properties of an unbounded set of objects. For instance, here we define a function  flipAlot  that maps from an integer (or any other value) to a 
coin flip. We could use it to implicitly represent the nnth flip of a particular coin, without having to actually flip the coin nn times.
var flipAlot = mem(function (n) {
  return flip()
});

[[flipAlot(1), flipAlot(12), flipAlot(47), flipAlot(1548)],
 [flipAlot(1), flipAlot(12), flipAlot(47), flipAlot(1548)]]
run▼

There are a countably infinite number of such flips, each independent of all the others. The outcome of each, once determined, will always
have the same value.

In computer science memoization is an important technique for optimizing programs by avoiding repeated work. In the probabilistic setting, such as in
WebPPL, memoization actually affects the meaning of the memoized function.

Example: Intuitive physics

Humans have a deep intuitive understanding of everyday physics—this allows us to make furniture, appreciate sculpture, and play baseball. How
can we describe this intuitive physics? One approach is to posit that humans have a generative model that captures key aspects of real physics,
though perhaps with approximations and noise. This mental physics simulator could for instance approximate Newtonian mechanics, allowing us to
imagine the future state of a collection of (rigid) bodies. We have included such a 2-dimensional physics simulator, the function   runPhysics , that
takes a collection of physical objects and runs physics ‘forward’ by some amount of time. (We also have   animatePhysics , which does the same, but
gives us an animation to see what is happening.) We can use this to imagine the outcome of various initial states, as in the Plinko machine example
above:

var dim = function() { return uniform(5, 20) }
var staticDim = function() { return uniform(10, 50) }
var shape = function() { return flip() ? 'circle' : 'rect' }
var xpos = function() { return uniform(100, worldWidth - 100) }
var ypos = function() { return uniform(100, worldHeight - 100) }

var ground = {shape: 'rect',
              static: true,
              dims: [worldWidth, 10],
              x: worldWidth/2,
              y: worldHeight}

var falling = function() {
  return {
    shape: shape(),
    static: false,
    dims: [dim(), dim()],
    x: xpos(),
    y: 0}
};

var fixed = function() {
  return {
    shape: shape(),
    static: true,
    dims: [staticDim(), staticDim()],
    x: xpos(),
    y: ypos()}
}

var fallingWorld = [ground, falling(), falling(), falling(), fixed(), fixed()]
physics.animate(1000, fallingWorld);
run▼

There are many judgments that you could imagine making with such a physics simulator.  Hamrick et al. (2011) have explored human intuitions about
the stability of block towers. Look at several different random block towers; first judge whether you think the tower is stable, then simulate
to find out if it is:

var xCenter = worldWidth / 2
var ground = {
  shape: 'rect',
  static: true,
  dims: [worldWidth, 10],
  x: worldWidth/2,
  y: worldHeight
}
var dim = function() { return uniform(10, 50) }
var xpos = function (prevBlock) {
  var prevW = prevBlock.dims[0]
  var prevX = prevBlock.x
  return uniform(prevX - prevW, prevX + prevW)
}

var ypos = function (prevBlock, h) {
  var prevY = prevBlock.y
  var prevH = prevBlock.dims[1]
  return prevY - (prevH + h)
}

var addBlock = function (prevBlock, isFirst) {
  var w = dim()
  var h = dim()
  return {
    shape: 'rect',
    static: false,
    dims: [w, h],
    x: isFirst ? xCenter : xpos(prevBlock),
    y: ypos(prevBlock, h)
  }
}

var makeTowerWorld = function() {
  var block1 = addBlock(ground, true)
  var block2 = addBlock(block1, false)

  var block3 = addBlock(block2, false)
  var block4 = addBlock(block3, false)
  var block5 = addBlock(block4, false)
  return [ground, block1, block2, block3, block4, block5]
};

physics.animate(1000, makeTowerWorld())
run▼

Were you often right? Were there some cases of ‘surprisingly stable’ towers? Hamrick et al. (2011) account for these cases by positing that
people are not entirely sure where the blocks are initially (perhaps due to noise in visual perception). Thus our intuitions of stability are
really stability given noise (or the expected stability marginalizing over slightly different initial configurations). We can realize this measure of
stability as:

var listMin = function (xs) {
  if (xs.length == 1) {
    return xs[0]
  } else {
    return Math.min(xs[0], listMin(rest(xs)))
  }
}

var ground = {
  shape: 'rect',
  static: true,
  dims: [worldWidth, 10],
  x: worldWidth/2,
  y: worldHeight+6
}

var stableWorld = [
  ground,
  {shape: 'rect', static: false, dims: [60, 22], x: 175, y: 473},
  {shape: 'rect', static: false, dims: [50, 38], x: 159.97995044874122, y: 413},
  {shape: 'rect', static: false, dims: [40, 35], x: 166.91912737427202, y: 340},
  {shape: 'rect', static: false, dims: [30, 29], x: 177.26195677111082, y: 276},
  {shape: 'rect', static: false, dims: [11, 17], x: 168.51354470809122, y: 230}
]

var almostUnstableWorld = [
  ground,
  {shape: 'rect', static: false, dims: [24, 22], x: 175, y: 473},
  {shape: 'rect', static: false, dims: [15, 38], x: 159.97995044874122, y: 413},
  {shape: 'rect', static: false, dims: [11, 35], x: 166.91912737427202, y: 340},
  {shape: 'rect', static: false, dims: [11, 29], x: 177.26195677111082, y: 276},
  {shape: 'rect', static: false, dims: [11, 17], x: 168.51354470809122, y: 230}
]

var unstableWorld = [
  ground,
  {shape: 'rect', static: false, dims: [60, 22], x: 175, y: 473},
  {shape: 'rect', static: false, dims: [50, 38], x: 90, y: 413},
  {shape: 'rect', static: false, dims: [40, 35], x: 140, y: 340},
  {shape: 'rect', static: false, dims: [10, 29], x: 177.26195677111082, y: 276},
  {shape: 'rect', static: false, dims: [50, 17], x: 140, y: 230}
]

var doesTowerFall = function (initialW, finalW) {
  var highestY = function (w) {
    return listMin(map(function (obj) { obj.y }, w))
  }
  var approxEqual = function (a, b) {
    return Math.abs(a - b) < 1.0

  }
  return !approxEqual(highestY(initialW), highestY(finalW))
}

var noisify = function (world) {
  var perturbX = function (obj) {
    var noiseWidth = 10

    if (obj.static) {
      return obj
    } else {
      return  .extend({}, 
_
    }
  }
  return map(perturbX, world)
}

obj, {x: uniform(obj.x - noiseWidth, obj.x + noiseWidth) })

var run = function (world) {
  var initialWorld = noisify(world)
  var finalWorld = physics.run(1000, initialWorld)
  return doesTowerFall(initialWorld, finalWorld)
}

viz(Infer({method: 'forward', samples: 100},
           function() { return run(stableWorld) }))
viz(Infer({method: 'forward', samples: 100},
           function() { return run(almostUnstableWorld) }))
viz(Infer({method: 'forward', samples: 100},
           function() { return run(unstableWorld) }))

// uncomment any of these that you'd like to see for yourself
// physics.animate(1000, stableWorld)
// physics.animate(1000, almostUnstableWorld)
// physics.animate(1000, unstableWorld)
run▼

Reading & Discussion: Readings

Test your knowledge: Exercises
Next chapter: 3. Conditioning 

    
☰

Conditioning

Cognition and conditioning

We have built up a tool set for constructing probabilistic generative models. These can represent knowledge about causal processes in the world:
running one of these programs generates a particular outcome by sampling a “history” for that outcome. However, the power of a causal model
lies in the flexible ways it can be used to reason about the world. In the  last chapter we ran generative models forward to reason about
outcomes from initial conditions. Generative models also enable reasoning in other ways. For instance, if we have a generative model in which X is
the output of a process that depends on Y (say  var X = coolFunction(Y) ) we may ask: “assuming I have observed a certain X, what must Y have
been?” That is we can reason backward from outcomes to initial conditions. More generally, we can make hypothetical assumptions and reason about
the generative  history: “assuming something, how did the generative  model run?” In  this  section  we describe  how a wide variety of such
hypothetical inferences can be made from a single generative model by conditioning the model on an assumed or observed fact.

Much of cognition can be understood in terms of conditional inference. In its most basic form,  causal attribution is conditional inference: given
some observed effects, what were the likely causes? Predictions are conditional inferences in the opposite direction: given that I have observed
some cause, what are its likely effects? These inferences can be described by conditioning a probabilistic program that expresses a causal model.
The acquisition of that causal model, or learning, is also conditional inference at a higher level of abstraction: given our general knowledge of
how causal relations operate in the world, and some observed events in which candidate causes and effects co-occur in various ways, what specific
causal relations are likely to hold between these observed variables?

To see how the same concepts apply in a domain that is not usually thought of as causal, consider language. The core questions of interest in the
study of natural language are all at heart conditional inference problems. Given beliefs about the structure of my language, and an observed
sentence, what should I believe about the syntactic structure of that sentence? This is the  parsing problem. The complementary problem
of speech production is related: given the structure of my language (and beliefs about others’ beliefs about that), and a particular thought I
want to express, how should I encode the thought? Finally, the acquisition problem: given some data from a particular language, and perhaps
general knowledge about universals of grammar, what should we believe about that language’s structure? This problem is simultaneously the
problem facing the linguist and the child trying to learn a language.

Parallel problems of conditional inference arise in visual perception, social cognition, and virtually every other domain of cognition. In visual
perception, we observe an image or image sequence that is the result of rendering a three-dimensional physical scene onto our two-dimensional
retinas. A probabilistic program can model both the physical processes at work in the world that produce natural scenes, and the imaging processes
(the “graphics”) that generate images from scenes. Perception can then be seen as conditioning this program on some observed output image and
inferring the scenes most likely to have given rise to it.

When interacting with other people, we observe their actions, which result from a planning process, and often want to guess their desires,
beliefs, emotions, or future actions. Planning can be modeled as a program that takes as input an agent’s mental states (beliefs, desires, etc.)
and produces action sequences—for a rational agent, these will be actions that are likely to produce the agent’s desired states reliably and
efficiently. A rational agent can plan their actions by conditional inference to infer what steps would be most likely to achieve their desired
state. Action understanding, or interpreting an agent’s observed behavior, can be expressed as conditioning a planning program (a “theory of
mind”) on observed actions to infer the mental states that most likely gave rise to those actions, and to predict how the agent is likely to act in
the future.

Hypothetical Reasoning with  Infer

Suppose that we know some fixed fact, and we wish to consider hypotheses about how a generative model could have given rise to that fact. In
the last chapter we met the  Infer  operator for constructing the marginal distribution on return values of a function; with the help of
the  condition  operator  Infer  is also able to describe marginal distributions under some assumption or condition.

Consider the following simple generative model:

var model = function() {
  var A = flip()
  var B = flip()
  var C = flip()
  var D = A + B + C
  return {'D': D}
}
var dist = Infer(model)
viz(dist)
run▼

The process described in  model  samples three numbers and adds them (recall JavaScript converts booleans to 00 or 11 when they enter 
arithmetic). The value of the final expression here is 0, 1, 2 or 3. A priori, each of the variables  A ,  B ,  C  has .5 probability of being  1  or  0 . 

However, suppose that we know that the sum  D  is equal to 3. How does this change the space of possible values that variable  A  could have 
taken?  A  (and  B  and  C ) must be equal to 1 for this result to happen. We can see this in the following WebPPL inference, where we 
use  condition  to express the desired assumption:

var model = function () {
  var A = flip()
  var B = flip()
  var C = flip()
  var D = A + B + C
  condition(D == 3)
  return {'A': A}
};
var dist = Infer(model)
viz(dist)
run▼

The output of  Infer  describes appropriate beliefs about the likely value of  A , conditioned on  D  being equal to 3.

Now suppose that we condition on  D  being greater than or equal to 2. Then  A  need not be 1, but it is more likely than not to be. (Why?) The
corresponding plot shows the appropriate distribution of beliefs for  A  conditioned on this new fact:

var model = function () {
  var A = flip()
  var B = flip()
  var C = flip()
  var D = A + B + C
  //add the desired assumption:
  condition(D >= 2)
  return {'A': A}
};
var dist = Infer(model)
viz(dist)
run▼

Going beyond the basic intuition of “hypothetical reasoning”, the  Infer  operation in the presence of  condition  can be understood in several,
equivalent, ways. We focus on two: the process of rejection sampling, and the the mathematical formulation of a conditional distribution.

Rejection Sampling

How can we imagine answering a hypothetical such as those above? We have already seen how to get a sample from a generative model, without
constraint, by simply running the evaluation process “forward” (i.e. simulating the process). We can get conditional samples by forward sampling
the entire model, but only keeping the sample if the value passed to   condition  is true. For instance, to sample from the above model “A given
that D is greater than 2” we could:

var takeSample = function () {
    var A = flip()
    var B = flip()
    var C = flip()
    var D = A + B + C
    return D >= 2 ? A : takeSample()
}
viz(repeat(100, takeSample))
run▼

Notice that we have used a stochastic recursion to sample the model repeatedly until   D >= 2  is  true , and we then return  A : we generate and
test until the condition is satisfied. This process is known as rejection sampling; we can use this technique to make a more general function that
implements  Infer , which we access in WebPPL with the  'rejection'  method:

var model = function () {
    var A = flip()
    var B = flip()
    var C = flip()
    var D = A + B + C
    condition(D >= 2)
    return A
}

var dist = Infer({method: 'rejection', samples: 100}, model)
viz(dist)
run▼

Conditional Distributions

The formal definition of conditional probability in probability theory is

P(A=a∣B=b)=P(A=a,B=b)P(B=b)P(A=a∣B=b)=P(A=a,B=b)P(B=b)

Here P(A=a∣B=b)P(A=a∣B=b) is the probability that “event” AA has value aa given that BB has value bb. (The meaning of 
events AA and BB must be given elsewhere in this notation, unlike a WebPPL program, which contains the full model specification within 
the  Infer  call.) The joint probability, P(A=a,B=b)P(A=a,B=b), is the probability that AA has value aa and BB has value bb. So the conditional 
probability is simply the ratio of the joint probability to the probability of the condition.
In the case of a WebPPL  Infer  statement with a  condition , A=aA=a will be the “event” that the return value is aa, while B=bB=b will be the
event that the value passed to condition is  true  (so bb is True). Because each of these is a regular (unconditional) probability, they and their 
ratio can often be computed exactly using the rules of probability. In WebPPL the inference method  'enumerate'  attempts to do this calculation 
(by first enumerating all the possible executions of the model):
var model = function () {
    var A = flip()
    var B = flip()
    var C = flip()
    var D = A + B + C
    condition(D >= 2)
    return A
}
var dist = Infer({method: 'enumerate'}, model)
viz(dist)
run▼

Connection to rejection sampling
The above notion of conditional distribution in terms of rejection sampling is equivalent to the mathematical definition, when both are well-
defined. (There are special cases when only one definition makes sense. For instance, when continuous random choices are used it is possible to
find situations where rejection sampling almost never returns a sample but the conditional distribution is still well defined. Why?)

Indeed, we can use the process of rejection sampling to understand this alternative definition of the conditional 
probability P(A=a∣B=b)P(A=a∣B=b). Imagine that we have sampled NtotalNtotal times. We only keep those samples in which the condition is true,
say there are NB=TrueNB=True of them. Of these some number NA=a,B=TrueNA=a,B=True have the returned value aa. The ratio

NA=a,B=TrueNB=True=NA=a,B=TrueNtotalNB=TrueNtotalNA=a,B=TrueNB=True=NA=a,B=TrueNtotalNB=TrueNtotal

is the fraction of times that A=aA=a when B=TrueB=True. When the number of samples is very large this converges 
to P(A=a,B=True)P(B=True)P(A=a,B=True)P(B=True). Thus the rejection sampling definition of conditional probability implies the above 
(probability ratio) definition.
Try using the formula for conditional probability to compute the probability of the different return values in the above examples. Check that
you get the same probability that you observe when using rejection sampling.

Bayes Rule
One of the most famous rules of probability is Bayes’ rule, which states:

P(h∣d)=P(d∣h)P(h)P(d)P(h∣d)=P(d∣h)P(h)P(d)

It is first worth noting that this follows immediately from the definition of conditional probability:

P(h∣d)=P(d,h)P(d)=P(d,h)P(h)P(d)P(h)=P(d∣h)P(h)P(d)P(h∣d)=P(d,h)P(d)=P(d,h)P(h)P(d)P(h)=P(d∣h)P(h)P(d)

Next we can ask what this rule means in terms of sampling processes. Consider the program:

var observedData = true;
var prior = function () { flip() }
var likelihood = function (h) { h ? flip(0.9) : flip(0.1) }

var posterior = Infer(
  function () {
    var hypothesis = prior()

    var data = likelihood(hypothesis)
    condition(data == observedData)
    return {hypothesis: hypothesis}
})

viz(posterior)
run▼

We   have   generated   a   value,   the hypothesis,   from   some   distribution   called   the prior,   then   used   an   observation   function  likelihood  which
generates data given this hypothesis, the probability of such an observation function is usually called the likelihood. Finally we have returned
the hypothesis, conditioned on the observation being equal to some observed data—this conditional distribution is called the posterior. This is a
typical setup in which Bayes’ rule is used.

Bayes’  rule  simply   says   that,   in   special   situations  where  the   model   decomposes  nicely  into   a  part   “before”   the   value  to   be   returned
(hypothesis) and a part “after” the value to be returned, then the conditional probability can be expressed simply in terms of the prior and
likelihood components of the model. This is often a useful way to think about conditional inference in simple settings. However, we will see
examples as we go along where Bayes’ rule doesn’t apply in a simple way, but the conditional distribution is equally well understood in other
terms.

Other implementations of  Infer

Much of the difficulty of implementing the WebPPL language (or probabilistic models in general) is in finding useful ways to do conditional
inference—to implement  Infer . We have already seen rejection sampling and enumeration, but the AI literature is replete with other algorithms
and techniques  for dealing with conditional probabilistic inference.  Many of these have  been  adapted into  WebPPL  to give  implementations
of  Infer  that may be more efficient in various cases. Switching from one method to another is as simple as changing the options passed to   Infer .
We   have   already   seen   two   methods:  {method:   'enumerate'}  and  {method:   'rejection',   samples:   X} ;   other   methods   include  'MCMC' ,  'SMC' ,
and  'variational' . Sometimes we even drop the method argument, asking WebPPL to guess the best inference method (which it can often do).
The Infer documentation provides many more usage details.

There is an interesting parallel between the  Infer  abstraction, wrapping up the engineering challenge of different inference methods, and the
idea of levels of analysis in cognitive science David Marr (1982). At the top, or computational level, of analysis we are concerned more with the
world knowledge people have and the inferences they license; at the next, algorithmic, level of analysis we are concerned with the details
on how these inferences are done. In parallel, WebPPL allows us to specify generative knowledge and inference questions, largely abstracting
away the methods of inference (they show up only in the options argument to  Infer , when provided). We will further explore some of the
algorithms used in these implementations in Algorithms for inference, and ask whether they may be useful algorithmic levels models for human
thinking in Rational process models. For most of this book, however, we work at the computational level, abstracting away from algorithmic
details.

Conditions and observations

A very common pattern is to condition directly on the value of a sample from some distribution. For instance here we try to recover a true
number from a noisy observation of it:

var model = function(){
  var trueX = sample(Gaussian({mu: 0, sigma: 1}))
  var obsX = sample(Gaussian({mu: trueX, sigma: 0.1}))
  condition(obsX == 0.2)
  return trueX
}
viz(Infer({method: 'rejection', samples:1000}, model))
run▼

You   will   note   that   this   never   finishes.   (Why?   Think   about   what   rejection   sampling   tries   to   do   here….)   In   WebPPL   we   have   a   special
operator,  observe , to express the pattern of conditioning on a value sampled directly from a distribution. In addition to being clearer, it also
gives the implementation some hints about how to do inference.

var model = function(){
  var trueX = sample(Gaussian({mu: 0, sigma: 1}))
  observe(Gaussian({mu: trueX, sigma: 0.1}), 0.2)
  return trueX
}
viz(Infer({method: 'rejection', samples:1000, maxScore: 2}, model))
run▼

It is natural and common to condition a generative model on a value for one of the variables declared in this model (i.e. to   observe  its value).
However, there are many situations in which we desire the greater expressivity of  condition ; one may wish to ask for more complex hypotheticals:
“what if P,” where P is a complex proposition composed out of variables declared in the model. Consider the following WebPPL inference:

var dist = Infer(
  function () {
    var A = flip()
    var B = flip()
    var C = flip()
    condition(A + B + C >= 2)
    return A
});
viz(dist)
run▼

This inference has the same meaning as the earlier example, but the formulation is importantly different. We have directly conditioned on the
complex assumption that the sum of these random variables is greater than or equal to 2. This involves a new value or “random variable”,   A + B
+ C >= 2  that did not appear anywhere in the generative model (the var definitions). We could have instead added a definition  var D = (A + B +
C >= 2)  to the generative model and conditioned (or observed) its value. However this intertwines the hypothetical assumption (condition) with
the generative model knowledge (definitions), and this is not what we want: we want a simple model which supports many queries, rather than a
complex model in which only a prescribed set of queries is allowed. Using  condition  allows the flexibility to build complex random expressions like
this as needed, making assumptions that are phrased as complex propositions, rather than simple observations. Hence the effective number of
queries we can construct for most programs will not merely be a large number but countably infinite, much like the sentences in a natural
language. The  Infer  function (in principle, though with variable efficiency) supports correct conditional inference for this infinite array of
situations.

Factors

In WebPPL,  condition  and  observe  are actually special cases of a more general operator:  factor . Whereas  condition  is like making an assumption
that must be true, then  factor  is like making a soft assumption that is merely preferred to be true. For instance, suppose we flip a single coin.
If we condition on the outcome being heads, then the outcome must be heads:

var dist = Infer(
  function () {
    var A = flip()
    condition(A)
    return A
});
viz(dist)
run▼

However, if we swap  condition  for  factor , we simply make heads more likely:

var dist = Infer(
  function () {
    var A = flip()
    factor(A?1:0)
    return A
});
viz(dist)
run▼

Technically,  factor(x)  adds  x  to the unnormalized log-probability of the program execution within which it occurs. Thus, to get the new 
probabilities induced by the  factor  statement we compute the normalizing constant given these log-scores. The resulting 
probability P(A=true)P(A=true) is:

P(A=true)=e1(e0+e1)P(A=true)=e1(e0+e1)

Play with this example. Can you revise the example to increase the probability of heads?

The  factor  construct is very general. Both  condition  and  observe  can be written easily in terms of  factor . However models are often clearer
when written with the more specialized forms. In machine learning it is common to talk of  directed and undirected generative models; directed
models can be thought of as those made from only  sample  and  observe , while undirected models include  factor  (and often have only factors).

Example: Reasoning about Tug of War

Imagine a game of tug of war, where each person may be strong or weak, and may be lazy or not on each match. If a person is lazy they only pull
with half their strength. The team that pulls hardest will win. We assume that strength is a continuous property of an individual, and that on
any match, each person has a 1 in 3 chance of being lazy. This WebPPL program runs a tournament between several teams, mixing up players across
teams. Can you guess who is strong or weak, looking at the tournament results?

var strength = mem(function (person) {return Math.max(gaussian(1, 1), 0.01)})
var lazy = function(person) {return flip(1/3) }
var pulling = function(person) {
  return lazy(person) ? strength(person) / 2 : strength(person) }
var totalPulling = function (team) {return sum(map(pulling, team))}
var winner = function (team1, team2) {
  totalPulling(team1) > totalPulling(team2) ? team1 : team2 }

print([
    winner(['alice', 'bob'], ['sue', 'tom']),
    winner(['alice', 'bob'], ['sue', 'tom']),
    winner(['alice', 'sue'], ['bob', 'tom']),
    winner(['alice', 'sue'], ['bob', 'tom']),
    winner(['alice', 'tom'], ['bob', 'sue']),
    winner(['alice', 'tom'], ['bob', 'sue'])
])
run▼

Notice that  strength  is memoized because this is a property of a person true across many matches, while  lazy  isn’t. Each time you run this
program, however, a new “random world” will be created: people’s strengths will be randomly re-generated, then used in all the matches.

We can use  Infer  to ask a variety of different questions. For instance, how likely is it that Bob is strong, given that he’s been on a series of
winning teams? (Note that we have added the helper function  beat  as in “team1 beat team2”; this just makes for more compact conditioning
statements.)

var model = function() {
  var strength = mem(function (person) {return Math.max(gaussian(1, 1), 0.01)})
  var lazy = function(person) {return flip(1/3) }
  var pulling = function(person) {
    return lazy(person) ? strength(person) / 2 : strength(person) }
  var totalPulling = function (team) {return sum(map(pulling, team))}
  var winner = function (team1, team2) {
    totalPulling(team1) > totalPulling(team2) ? team1 : team2 }
  var beat = function(team1,team2){ .isEqual(

winner(team1,team2), team1)}

_

  condition(beat(['bob', 'mary'], ['tom', 'sue']))
  condition(beat(['bob', 'sue'],  ['tom', 'jim']))

  return strength('bob')
}

var dist = Infer({method: 'MCMC', kernel: 'MH', samples: 25000},
                 model)

print('Expected strength: ' + expectation(dist))
viz(dist)
run▼

Try varying the number of different teams and teammates that Bob plays with. How does this change the estimate of Bob’s strength? Do these
changes agree with your intuitions? Can you modify this example to make laziness a continuous quantity? Can you add a person-specific tendency
toward laziness?

A model very similar to this was used in Gerstenberg and Goodman (2012) to predict human judgements about the strength of players in ping-pong
tournaments. It achieved very accurate quantitative predictions without many free parameters.

We can form many complex queries from this simple model. We could ask how likely a team of Bob and Mary is to beat a team of Jim and Sue, given
that Mary is at least as strong as sue, and Bob beat Jim in a previous direct match up:

var model = function() {

var strength = mem(function (person) {return Math.max(gaussian(1, 1), 0.01)})
  var lazy = function(person) {return flip(1/3) }
  var pulling = function(person) {
    return lazy(person) ? strength(person) / 2 : strength(person) }
  var totalPulling = function (team) {return sum(map(pulling, team))}
  var winner = function (team1, team2) {
    totalPulling(team1) > totalPulling(team2) ? team1 : team2 }
  var beat = function(team1,team2){ .isEqual(

winner(team1,team2), team1)}

_

  condition(strength('mary') >= strength('sue'))
  condition(beat(['bob'], ['jim']))

  return beat(['bob','mary'], ['jim','sue'])
}

var dist = Infer({method: 'MCMC', kernel: 'MH', samples: 25000},
                 model)
viz(dist)
run▼

Example: Inverse intuitive physics

We previously saw how a generative model of physics—a noisy, intuitive version of Newtonian mechanics—could be used to make judgements about
the final state of physical worlds from initial conditions. We showed how this forward simulation could be used to model judgements about
stability. We can also use a physics model to reason backward: from final to initial states.

Imagine that we drop a block from a random position at the top of a world with two fixed obstacles:

// makes a floor with evenly spaced buckets
var bins = function (xmin, xmax, width) {
  return ((xmax < xmin + width)
          // floor
          ? {shape: 'rect', static: true, dims: [400, 10], x: 175, y: 500}
          // bins
          : [{shape: 'rect', static: true, dims: [1, 10], x: xmin, y: 490}].concat(bins(xmin + width, xmax, width))
         )
}

// add two fixed circles
var world = [{shape: 'circle', static: true, dims: [60], x: 60, y: 200},
             {shape: 'circle', static: true, dims: [30], x: 300, y: 300}].concat(bins(-1000, 1000, 25))

var randomBlock = function () {
  return {shape: 'circle', static: false, dims: [10], x: uniform(0, worldWidth), y: 0}
}

physics.animate(1000, [randomBlock()].concat(world))
run▼

Assuming that the block comes to rest in the middle of the floor, where did it come from?

// makes a floor with evenly spaced buckets
var bins = function (xmin, xmax, width) {
  return ((xmax < xmin + width)
          // floor
          ? {shape: 'rect', static: true, dims: [400, 10], x: 175, y: 500}
          // bins
          : [{shape: 'rect', static: true, dims: [1, 10], x: xmin, y: 490}].concat(bins(xmin + width, xmax, width))
         )
}

// add two fixed circles
var world = [{shape: 'circle', static: true, dims: [60], x: 60, y: 200},
             {shape: 'circle', static: true, dims: [30], x: 300, y: 300}].concat(bins(-1000, 1000, 25))

var randomBlock = function () {
  return {shape: 'circle', static: false, dims: [10], x: uniform(0, worldWidth), y: 0}
}

var getBallX = function(world) {
  var ball = filter(function(obj) { return !obj.static }, world)[0];
  return ball.x;
}

var observedX = 160;

var model = function() {
  var initState = world.concat([randomBlock()])
  var initX = getBallX(initState);
  var finalState = physics.run(1000, initState);
  var finalX = getBallX(finalState);
  observe(Gaussian({mu: finalX, sigma: 10}), observedX)
  return {initX: initX}
}

var initialXDist = Infer(
  {method: 'MCMC',
   samples: 100,
   lag: 10,
   callbacks: [editor.MCMCProgress()]
  },
  model);

viz.density(initialXDist, {bounds: [0,350]})
run▼

What if the ball comes to rest at the left side, under the large circle (x about 60)? The right side?

Notice that the model described above has preternatural knowledge of physics. For instance, it knows exactly how the ball will bounce of the
pegs, even if there are many bounces. Do you think this is a good model of human intuition? If not, how could you change the model to capture
human reasoning better?

Example: Causal Inference in Medical Diagnosis

This classic Bayesian inference task is a special case of conditioning. Kahneman and Tversky, and Gigerenzer and colleagues, have studied how
people make simple judgments like the following:

The probability of breast cancer is 1% for a woman at 40 who participates in a routine screening. If a woman has breast cancer,
the probability is 80% that she will have a positive mammography. If a woman does not have breast cancer, the probability is
9.6% that she will also have a positive mammography. A woman in this age group had a positive mammography in a routine
screening. What is the probability that she actually has breast cancer?

What is your intuition? Many people without training in statistical inference judge the probability to be rather high, typically between 0.7 and
0.9. The correct answer is much lower, less than 0.1, as we can see by running this WebPPL inference:

var cancerDist = Infer({method: 'enumerate'},
  function () {
    var breastCancer = flip(0.01)
    var positiveMammogram = breastCancer ? flip(0.8) : flip(0.096)
    condition(positiveMammogram)
    return {breastCancer: breastCancer}
})
viz(cancerDist)
run▼

Tversky and Kahneman (1974) named this kind of judgment error base rate neglect, because in order to make the correct judgment, one must
realize that the key contrast is between the base rate of the disease, 0.01 in this case, and the false alarm rate or probability of a positive
mammogram given no breast cancer, 0.096. The false alarm rate (or FAR for short) seems low compared to the probability of a positive mammogram
given breast cancer (the likelihood), but what matters is that it is almost ten times higher than the base rate of the disease. All three of these
quantities are needed to compute the probability of having breast cancer given a positive mammogram using Bayes’ rule for posterior conditional
probability:

P(cancer∣positive mammogram)=P(positive mammogram∣cancer)×P(cancer)P( positive mammogram)P(cancer∣positive mammogram)=P(positive
mammogram∣cancer)×P(cancer)P( positive mammogram)
=0.8×0.010.8×0.01+0.096×0.99=0.078=0.8×0.010.8×0.01+0.096×0.99=0.078

Gigerenzer and Hoffrage (1995) showed that this kind of judgment can be made much more intuitive to untrained reasoners if the relevant
probabilities are presented as “natural frequencies”, or the sizes of subsets of relevant possible outcomes:

On average, ten out of every 1000 women at age 40 who come in for a routine screen have breast cancer. Eight out of those
ten women will get a positive mammography. Of the 990 women without breast cancer, 95 will also get a positive mammography.
We assembled a sample of 1000 women at age 40 who participated in a routine screening. How many of those who got a positive
mammography do you expect to actually have breast cancer?

Now one can practically read off the answer from the problem formulation: 8 out of 103 (95+8) women in this situation will have breast cancer.

Gigerenzer (along with Cosmides, Tooby and other colleagues) has argued that this formulation is easier because of evolutionary and computational
considerations: human minds have evolved to count and compare natural frequencies of discrete events in the world, not to add, multiply and
divide decimal probabilities. But this argument alone cannot account for the very broad human capacity for causal reasoning. We routinely make
inferences for which we haven’t stored up sufficient frequencies of events observed in the world. (And often for which no one has told us
the relevant frequencies, although perhaps we have been told about degrees of causal strength or base rates in the form of probabilities or
other linguistic encoding).

However, the basic idea that the mind is good at manipulating frequencies of situations, but bad at arithmetic on continuous probability values, can
be extended to cope with novel situations if the frequencies that are manipulated can be frequencies of  imagined situations. Recall that
probabilistic programs explicitly give instructions for sampling imagined situations, and only implicitly specify probability distributions. If human
inference is similar to a WebPPL inference then it would readily create and manipulate imagined situations, and this could explain both why the
frequency framing of Bayesian probability judgment is natural to people and how people cope with rarer and more novel situations. The numbers
given in the frequency formulation (or close approximations thereof) can be read off a tree of evaluation histories for 1000 calls of the WebPPL
program that specifies the causal model for this problem:

Each path from root to leaf of this tree represents a sequence of random choices made in evaluating the above program (the first flip for
breast-cancer, the second for positive-mammogram), with the number of traversals and the sampled value labeling each edge. (Because this is
1000 random samples, the number are close (but not exactly) those in the Gigerenzer, et al, story.) Selecting just the 106 hypothetical cases of
women with a positive mammogram, and computing the fraction of those who also have breast cancer (7/106), corresponds exactly to   Infer({method:
'rejection'}) . Thus, we have used the causal representation in the above program to manufacture frequencies which can be used to arrive at the
inference that relatively few women with positive mammograms actually have breast cancer.

Yet unlike the rejection sampler people are quite bad at reasoning in this scenario. Why? One answer is that people don’t represent their
knowledge in quite the form of this simple program. Indeed, Krynski and Tenenbaum (2007) have argued that human statistical judgment is
fundamentally based on conditioning more explicit causal models: they suggested that “base rate neglect” and other judgment errors may occur
when people are given statistical information that cannot be easily mapped to the parameters of the causal models they intuitively adopt to
describe the situation. In the above example, they suggested that the notion of a false alarm rate is not intuitive to many people—particularly
when the false alarm rate is ten times higher than the base rate of the disease that the test is intended to diagnose! They showed that “base

rate neglect” could be eliminated by reformulating the breast cancer problem in terms of more intuitive causal models. For example, consider
their version of the breast cancer problem (the exact numbers and wording differed slightly):

1% of women at age 40 who participate in a routine screening will have breast cancer. Of those with breast cancer, 80% will
receive a positive mammogram. 20% of women at age 40 who participate in a routine screening will have a benign cyst. Of those
with a benign cyst, 50% will receive a positive mammogram due to unusually dense tissue of the cyst. All others will receive
a negative mammogram. Suppose that a woman in this age group has a positive mammography in a routine screening. What is the
probability that she actually has breast cancer?

This question is easy for people to answer—empirically, just as easy as the frequency-based formulation given above. We may conjecture this is
because the relevant frequencies can be computed from a simple inference on the following more intuitive causal model:

var cancerDist = Infer({method: 'enumerate'},
  function () {
    var breastCancer = flip(0.01)
    var benignCyst = flip(0.2)
    var positiveMammogram = (breastCancer && flip(0.8)) || (benignCyst && flip(0.5))
    condition(positiveMammogram)
    return {breastCancer: breastCancer}
});
viz(cancerDist)
run▼

Because this causal model—this WebPPL program—is more intuitive to people, they can imagine the appropriate situations, despite having been given
percentages rather than frequencies. What makes this causal model more intuitive than the one above with an explicitly specified false alarm
rate? Essentially we have replaced probabilistic dependencies on the “non-occurrence” of events (e.g., the dependence of a positive mammogram
on not having breast cancer) with dependencies on explicitly specified alternative causes for observed effects (e.g., the dependence of a
positive mammogram on having a benign cyst).

A causal model framed in this way can scale up to significantly more complex situations. Recall our more elaborate medical diagnosis network from
the previous section, which was also framed in this way using noisy-logical functions to describe the dependence of symptoms on disease:

var dist = Infer({method: 'enumerate'},
  function () {
    var lungCancer = flip(0.01)
    var TB = flip(0.005)
    var cold = flip(0.2)
    var stomachFlu = flip(0.1)
    var other = flip(0.1)

    var cough = ((cold && flip(0.5)) ||
                 (lungCancer && flip(0.3)) ||
                 (TB && flip(0.7)) ||
                 (other && flip(0.01)))

    var fever = ((cold && flip(0.3)) ||
                 (stomachFlu && flip(0.5)) ||
                 (TB && flip(0.2)) ||
                 (other && flip(0.01)))

    var chestPain = ((lungCancer && flip(0.4)) ||
                     (TB && flip(0.5)) ||
                     (other && flip(0.01)))

    var shortnessOfBreath = ((lungCancer && flip(0.4)) ||
                             (TB && flip(0.5)) ||
                             (other && flip(0.01)))

    condition(cough && fever && chestPain && shortnessOfBreath)
    return {lungCancer: lungCancer, TB: TB}
})
viz(dist)
run▼

You can use this model to infer conditional probabilities for any subset of diseases conditioned on any pattern of symptoms. Try varying the
symptoms in the conditioning set or the diseases in the inference, and see how the model’s inferences compare with your intuitions. For example,
what happens to inferences about lung cancer and TB in the above model if you remove chest pain and shortness of breath as symptoms? (Why?
Consider the alternative explanations.) More generally, we can condition on any set of events – any combination of symptoms and diseases – and

query any others. We can also condition on the negation of an event (using the JavaScript negation operator  ! ): how does the probability of lung
cancer (versus TB) change if we observe that the patient does not have a fever (i.e.  condition(!fever) ), does not have a cough, or does not have
either symptom?

As we discussed above, WebPPL program thus effectively encodes the answers to a very large number of possible questions in a very compact form.
In the program above, there are 39=1968339=19683 possible simple conditions corresponding to conjunctions of events or their negations 
(because the program has 9 stochastic Boolean-valued functions, each of which can be observed true, observed false, or not observed). Then 
for each of those conditions there are a roughly comparable number of queries, corresponding to all the possible conjunctions of variables that 
can be in the return value expression. This makes the total number of simple questions encoded on the order of 100 million. We are beginning to 
see the sense in which probabilistic programming provides the foundations for constructing a language of thought, as described in the Introduction:
a finite system of knowledge that compactly and efficiently supports an infinite number of inference and decision tasks.
Expressing our knowledge as a probabilistic program of this form also makes it easy to add in new relevant knowledge we may acquire, without
altering or interfering with what we already know. For instance, suppose we decide to consider behavioral and demographic factors that might
contribute causally to whether a patient has a given disease:

var dist = Infer({method: 'enumerate'},
  function () {
    var worksInHospital = flip(0.01)
    var smokes = flip(0.2)
    var lungCancer = flip(0.01) || (smokes && flip(0.02))
    var TB = flip(0.005) || (worksInHospital && flip(0.01))
    var cold = flip(0.2) || (worksInHospital && flip(0.25))
    var stomachFlu = flip(0.1)
    var other = flip(0.1)
    var cough = ((cold && flip(0.5)) ||
                 (lungCancer && flip(0.3)) ||
                 (TB && flip(0.7)) ||
                 (other && flip(0.01)))

    var fever = ((cold && flip(0.3)) ||
                 (stomachFlu && flip(0.5)) ||
                 (TB && flip(0.2)) ||
                 (other && flip(0.01)))

    var chestPain = ((lungCancer && flip(0.4)) ||
                     (TB && flip(0.5)) ||
                     (other && flip(0.01)))

    var shortnessOfBreath = ((lungCancer && flip(0.4)) ||
                             (TB && flip(0.5)) ||
                             (other && flip(0.01)))

    condition(cough && chestPain && shortnessOfBreath)
    return {lungCancer: lungCancer, TB: TB}
})

viz(dist)
run▼

Under this model, a patient with coughing, chest pain and shortness of breath is likely to have either lung cancer or TB. Modify the above code
to see how these conditional inferences shift if you also know that the patient smokes or works in a hospital (where they could be exposed to
various infections, including many worse infections than the typical person encounters). More generally, the causal structure of knowledge
representation in a probabilistic program allows us to model intuitive theories that can grow in complexity continually over a lifetime, adding new
knowledge without bound.

Reading & Discussion: Readings

Test your knowledge: Exercises
Next chapter: 4. Causal and statistical dependence 

☰

Causal and statistical dependence

Causal Dependence

Probabilistic programs encode knowledge about the world in the form of causal models, and it is useful to understand how their function relates
to their structure by thinking about some of the intuitive properties of causal relations. Causal relations are local, modular, and directed. They
are modular in the sense that any two arbitrary events in the world are most likely causally unrelated, or independent. If they are related, or
dependent, the relation is only very weak and liable to be ignored in our mental models. Causal structure is  local in the sense that many events
that are related are not related directly: They are connected only through causal chains of several steps, a series of intermediate and more
local dependencies. And the basic dependencies are directed: when we say that A causes B, it means something different than saying that B
causes A. The causal influence flows only one way along a causal relation—we expect that manipulating the cause will change the effect, but not
vice versa—but information can flow both ways—learning about either event will give us information about the other.

Let’s examine this notion of “causal dependence” a little more carefully. What does it mean to believe that A depends causally on B? Viewing
cognition through the lens of probabilistic programs, the most basic notions of causal dependence are in terms of the structure of the program
and the flow of evaluation (or “control”) in its execution. We say that expression A causally depends on expression B if it is necessary to
evaluate B in order to evaluate A. (More precisely, expression A depends on expression B if it is ever necessary to evaluate B in order to
evaluate A.) For instance, in this program  A  depends on  B  but not on  C  (the final expression depends on both  A  and  C ):

var C = flip()
var B = flip()
var A = B ? flip(0.1) : flip(0.4)
A || C
run▼

Note that causal dependence order is weaker than a notion of ordering in time—one expression might happen to be evaluated before another in
time (for instance  C  before  A ), but without the second expression requiring the first. (This notion of causal dependence is related to the
notion of flow dependence in the programming language literature.)

For example, consider a simpler variant of our medical diagnosis scenario:

var marg = Infer({method: 'enumerate'}, function() {
  var smokes = flip(0.2)
  var lungDisease = (smokes && flip(0.1)) || flip(0.001)
  var cold = flip(0.02)
  var cough = (cold && flip(0.5)) || (lungDisease && flip(0.5)) || flip(0.001)
  var fever = (cold && flip(0.3)) || flip(0.01)
  var chestPain = (lungDisease && flip(0.2)) || flip(0.01)
  var shortnessOfBreath = (lungDisease && flip(0.2)) || flip(0.01)

  condition(cough)
  return {cold: cold, lungDisease: lungDisease}
})

viz.marginals(marg)
run▼

Here,  cough  depends   causally   on   both  lungDisease  and  cold ,   while  fever  depends   causally   on  cold  but   not  lungDisease .   We   can   see
that  cough  depends causally on  smokes  but only indirectly: although  cough  does not call  smokes  directly, in order to evaluate whether a
patient coughs, we first have to evaluate the expression  lungDisease  that must itself evaluate  smokes .

We haven’t made the notion of “direct” causal dependence precise: do we want to say that  cough  depends directly on  cold , or only directly on
the expression  (cold && flip(0.5)) || ... ? This can be resolved in several ways that all result in similar intuitions. For instance, we could first re-
write the program into a form where each intermediate expression is named (called A-normal form) and then say direct dependence is when one
expression immediately includes the name of another.

There are several special situations that are worth mentioning. In some cases, whether expression A requires expression B will depend on the
value of some third expression C. For example, here is a particular way of writing a noisy-AND relationship:

var C = flip()
var B = flip()
var A = (C ?

         (B ? flip(.85) : false) :
         false)
A
run▼

A always requires C, but only evaluates B if C returns true. Under the above definition of causal dependence A depends on B (as well as C).
However, one could imagine a more fine-grained notion of causal dependence that would be useful here: we could say that A depends causally on
B only in certain contexts (just those where C happens to return true and thus A calls B).

Another nuance is that an expression that occurs inside a function body may get evaluated several times in a program execution. In such cases it is
useful to speak of causal dependence between specific evaluations of two expressions. (However, note that if a specific evaluation of A depends
on a specific evaluation of B, then any other specific evaluation of A will depend on some specific evaluation of B. Why?)

Detecting Dependence Through Intervention

The causal dependence structure is not always immediately clear from examining a program, particularly where there are complex functions
calls. Another way to detect (or according to some philosophers, such as Jim Woodward, to define) causal dependence is more operational, in terms
of “difference making”: If we manipulate A, does B tend to change? By manipulate here we don’t mean an assumption in the sense of  condition .
Instead we mean actually edit, or intervene on, the program in order to make an expression have a particular value independent of its (former)
causes. If setting A to different values in this way changes the distribution of values of B, then B causally depends on A.

var BdoA = function(Aval) {
  return Infer({method: 'enumerate'}, function() {
    var C = flip()
    var A = Aval //we directly set A to the target value
    var B = A ? flip(.1) : flip(.4)
    return {B: B}
  })
}

viz(BdoA(true))
viz(BdoA(false))
run▼

This method is known in the causal Bayesian network literature as the “do operator” or graph surgery (Pearl, 1988). It is also the basis for
interesting theories of counterfactual reasoning by Pearl and colleagues (Halpern, Hitchcock and others).

For example, this code represents whether a patient is likely to have a cold or a cough a priori, without conditions or observations:

var medicalDist = Infer({method: 'enumerate'}, function() {
  var smokes = flip(.2)
  var lungDisease = flip(0.001) || (smokes && flip(0.1))
  var cold = flip(0.02)

  var cough = (cold && flip(0.5)) || (lungDisease && flip(0.5)) || flip(0.001)
  var fever = (cold && flip(0.3)) || flip(0.01)
  var chestPain = (lungDisease && flip(0.2)) || flip(0.01)
  var shortnessOfBreath = (lungDisease && flip(0.2)) || flip(0.01)

  return {cough: cough, cold: cold}
})
viz.marginals(medicalDist)
run▼

Imagine we now give our hypothetical patient a cold—for example, by exposing him to a strong cocktail of cold viruses. We should not model this as
an observation (e.g. by conditioning on having a cold), because we have taken direct action to change the normal causal structure. Instead we
implement intervention by directly editing the program: try to first do  var cold = true , then do  var cold = false :

var medicalDist = Infer({method: 'enumerate'}, function() {
  var smokes = flip(.2)
  var lungDisease = flip(0.001) || (smokes && flip(0.1))
  var cold = true // we intervene to make cold true

  var cough = (cold && flip(0.5)) || (lungDisease && flip(0.5)) || flip(0.001)
  var fever = (cold && flip(0.3)) || flip(0.01)
  var chestPain = (lungDisease && flip(0.2)) || flip(0.01)

  var shortnessOfBreath = (lungDisease && flip(0.2)) || flip(0.01)

  return {cough: cough, cold: cold}
})
viz.marginals(medicalDist)
run▼

You should see that the distribution on  cough  changes: coughing becomes more likely if we know that a patient has been given a cold by external
intervention. But the reverse is not true: Try forcing the patient to have a cough (e.g., with some unusual drug or by exposure to some cough-
inducing dust) by writing  var cough = true : the distribution on  cold  is unaffected. We have captured a familiar fact: treating the symptoms of a
disease directly doesn’t cure the disease (taking cough medicine doesn’t make your cold go away), but treating the disease does relieve the
symptoms.

Verify in the program above that the method of manipulation works also to identify causal relations that are only indirect: for example, force a
patient to smoke and show that it increases their probability of coughing, but not vice versa.

If we are given a program representing a causal model, and the model is simple enough, it is straightforward to read off causal dependencies from
the program code itself. However, the notion of causation as difference-making may be easier to compute in much larger, more complex models—
and it does not require an analysis of the program code. As long as we can modify (or imagine modifying) the definitions in the program and can run
the resulting model, we can compute whether two events or functions are causally related by the difference-making criterion.

Statistical Dependence

One often hears the warning, “correlation does not imply causation”. By “correlation” we mean a different kind of dependence between events
or functions—statistical dependence. We say that A and B are statistically dependent, if learning information about A tells us something about B,
and vice versa. In the language of webppl: using  condition  to make an assumption about A changes the value expected for B. Statistical dependence
is a symmetric relation between events referring to how information flows between them when we observe or reason about them. (If conditioning
on A changes B, then conditioning on B also changes A. Why?) The fact that we need to be warned against confusing statistical and causal
dependence suggests they are related, and indeed, they are. In general, if A causes B, then A and B will be statistically dependent. (One might
even say the two notions are “causally related”, in the sense that causal dependencies give rise to statistical dependencies.)

Diagnosing statistical dependence using  condition  is similar to diagnosing causal dependence through intervention. We condition on various values of
the possible statistical dependent, here  A , and see whether it changes the distribution on the target, here  B :

var BcondA = function(Aval) {
  return Infer({method: 'enumerate'}, function() {
    var C = flip()
    var A = flip()
    var B = A ? flip(.1) : flip(.4)
    condition(A == Aval) //condition on new information about A
    return {B: B}
  })
}

viz(BcondA(true))
viz(BcondA(false))
run▼

Because  the  two  distributions  on  B  (when   we  have   different  information   about  A ) are  different,  we  can   conclude  that  B  statistically
depends  on  A . Do  the  same  procedure for  testing if  A  statistically depends  on  B . How  is this  similar  (and  different) from  the causal
dependence   between   these   two?   As   an   exercise,   make   a   version   of   the   above   medical   example   to   test   the   statistical   dependence
between  cough  and  cold . Verify that statistical dependence holds symmetrically for events that are connected by an indirect causal chain, such
as  smokes  and  coughs .

Correlation is not just a symmetrized version of causality. Two events may be statistically dependent even if there is no causal chain running
between them, as long as they have a common cause (direct or indirect). That is, two expressions in a WebPPL program can be statistically
dependent if one calls the other, directly or indirectly, or if they both at some point in their evaluation histories refer to some other
expression (a “common cause”). Here is an example of statistical dependence generated by a common cause:

var BcondA = function(Aval) {
  return Infer({method: 'enumerate'}, function() {
    var C = flip();
    var A = C ? flip(.5) : flip(.9);
    var B = C ? flip(.1) : flip(.4);
    condition(A == Aval);
    return {B: B};

  })
}

viz(BcondA(true))
viz(BcondA(false))
run▼

Situations like this are extremely common. In the medical example above,  cough  and  fever  are not causally dependent but they are statistically
dependent, because they both depend on  cold ; likewise for  chestPain  and  shortnessOfBreath  which both depend on  lungDisease . Here we can
read off these facts from the program definitions, but more generally all of these relations can be diagnosed by reasoning using  Infer .

Successful learning and reasoning with causal models typically depends on exploiting the close coupling between causation and correlation. Causal
relations are typically unobservable, while correlations are observable from data. Noticing patterns of correlation is thus often the beginning of
causal learning, or discovering what causes what. On the other hand, with a causal model already in place, reasoning about the statistical
dependencies implied by the model allows us to predict many aspects of the world not directly observed from those aspects we do observe.

Graphical Notations for Dependence

Graphical models are an extremely important idea in modern machine learning: a graphical diagram is used to represent the direct dependence
structure between random choices in a probabilistic model. A special case are Bayesian networks, in which there is a node for each random variable
(an expression in our terms) and a link between two nodes if there is a direct conditional dependence between them (a direct causal dependence
in our terms). The sets of nodes and links define a directed acyclic graph (hence the term graphical model), a data structure over which many
efficient algorithms can be defined. Each node has a conditional probability table (CPT), which represents the probability distribution of that
node, given values of its parents. The joint probability distribution over random variables is given by the product of the conditional distributions
for each variable in the graph.

The figure below defines a Bayesian network for the medical diagnosis example. The graph contains a node for each   var  statement in our WebPPL
program, with links to that node from each variable that appears in the assignment expression. There is a probability table (“CPT”) for each
node, with a column for each value of the variable, and a row for each combination of values for its parents in the graph.

Simple generative models will have a corresponding graphical model that captures all of the dependencies (and  independencies) of the model,
without   capturing   the   precise form of   these   functions.   For   example,   while   the   graphical   model   shown   above   faithfully   represents   the
probability distribution encoded by the WebPPL program, it captures the noisy-OR form of the causal dependencies only implicitly. As a result,
the CPTs provide a less compact representation of the conditional probabilities than the WebPPL model. For instance, the CPT for   cough  specifies
4 parameters – one for each pair of values of  lungDisease  and  cold  (the second entry in each row is determined by the constraint that the
conditional distribution of  cough  must sum to 1). In contrast, the  var  statement for  cough  in WebPPL specifies only 3 parameters: the base rate
of  cough , and the strength with which  lungDisease  and  cold  cause  cough . This difference becomes more pronounced for noisy-OR relations with
many causes – the size of the CPT for a node will be exponential in the number of parents, while the number of terms in the noisy-OR
expression in WebPPL for that node will be linear in the number of causal dependencies (why?). As we will see, this has important implications for
the ability to learn the values of the parameters from data.

More complicated generative models, which can be expressed as probabilistic programs, often don’t have such a graphical model (or rather they
have many approximations, none of which captures all independencies). Recursive models generally give rise to such ambiguous (or loopy) Bayes
nets.

Test your knowledge: Exercises

Reading & Discussion: Readings
Next chapter: 5. Conditional dependence 

☰

Conditional dependence

From A Priori Dependence to Conditional Dependence

The relationships between causal structure and statistical dependence become particularly interesting and subtle when we look at the effects
of additional observations or assumptions. Events that are statistically dependent a priori may become independent when we condition on some
observation; this is called screening off. Also, events that are statistically independent a priori may become dependent when we condition on
observations; this is known as explaining away. The dynamics of screening off and explaining away are extremely important for understanding
patterns of inference—reasoning and learning—in probabilistic models.

Screening off

Screening off refers to a pattern of statistical inference that is quite common in both scientific and intuitive reasoning. If the statistical
dependence between two events A and B is only indirect, mediated strictly by one or more other events C, then conditioning on (observing) C
should render A and B statistically independent. This can occur if A and B are connected by one or more causal chains, and all such chains run
through the set of events C, or if C comprises all of the common causes of A and B.

For instance, let’s look again at our common cause example, this time assuming that we already know the value of  C :

var BcondA = function(Aval) {
  return Infer({method: 'enumerate'}, function() {
    var C = flip()
    var B = C ? flip(.5) : flip(.9)
    var A = C ? flip(.1) : flip(.4)
    condition(C)
    condition(A == Aval)
    return {B: B}
  })
}

viz(BcondA(true))
viz(BcondA(false))
run▼

We see that  A  an  B  are statistically independent given knowledge of  C .

Screening off is a purely statistical phenomenon. For example, consider the the causal chain model, where A directly causes C, which in turn
directly causes B. Here, when we observe C – the event that mediates an indirect causal relation between A and B – A and B are still causally
dependent in our model of the world: it is just our beliefs about the states of A and B that become uncorrelated. There is also an analogous
causal phenomenon. If we can actually manipulate or intervene on the causal system, and set the value of C to some known value, then A and B
become both statistically and causally independent (by intervening on C, we break the causal link between A and C).

Explaining away
“Explaining away” (Pearl, 1988) refers to a complementary pattern of statistical inference which is somewhat more subtle than screening off.
If two events A and B are statistically (and hence causally) independent, but they are both causes of one or more other events C, then
conditioning on (observing) C can render A and B statistically dependent. Here is an example where  A  and  B  have a common effect:

var BcondA = function(Aval) {
  return Infer({method: 'enumerate'}, function() {
    var A = flip()
    var B = flip()
    var C = (A || B) ? flip(.9) : flip(.2)
    condition(C)
    condition(A == Aval)
    return {B: B}
  })
}

viz(BcondA(true))
viz(BcondA(false))

run▼

As   with   screening   off,   we   only   induce   statistical   dependence   from   learning   about  C ,   not   causal   dependence:   when   we
observe  C ,  A  and  B  remain causally independent in our model of the world; it is our beliefs about A and B that become correlated.

We can express the general phenomenon of explaining away with the following schematic WebPPL query:

Infer({...}, function() {

  var a = ...

  var b = ...

  var data = f(a, b)

  condition(data == someVal && a == someOtherVal)

  return b

})

We have defined two independent variables  a  and  b  both of which are used to define the value of our data. If we condition on  data  and  a  the
posterior distribution on  b  will now be dependent on  a : observing additional information about  a  changes our conclusions about  b .

The most typical pattern of explaining away we see in causal reasoning is a kind of  anti-correlation: the probabilities of two possible causes for
the same effect increase when the effect is observed, but they are conditionally anti-correlated, so that observing additional evidence in
favor of one cause should lower our degree of belief in the other cause. (This pattern is where the term explaining away comes from.)
However, the coupling induced by conditioning on common effects depends on the nature of the interaction between the causes, it is not always an
anti-correlation. Explaining away takes the form of an anti-correlation when the causes interact in a roughly disjunctive or additive form: the
effect tends to happen if any cause happens; or the effect happens if the sum of some continuous influences exceeds a threshold. The
following simple mathematical examples show this and other patterns.

Suppose we condition on observing the sum of two integers drawn uniformly from 0 to 9:

var sumPosterior = Infer({method: 'enumerate'}, function() {
  var A = randomInteger({n: 10})
  var B = randomInteger({n: 10})
  condition(A + B == 9);
  return {A:A, B:B}
})

viz(sumPosterior)
//sometimes this scatter plot is easier to interpret:
viz.scatter(sumPosterior.support())
run▼

This gives perfect anti-correlation in conditional inferences for  A  and  B . But suppose we instead condition on observing that  A  and  B  are
equal:

var sumPosterior = Infer({method: 'enumerate'}, function() {
  var A = randomInteger({n: 10})
  var B = randomInteger({n: 10})
  condition(A == B)
  return {A:A, B:B}
})

viz(sumPosterior)
//sometimes this scatter plot is easier to interpret:
viz.scatter(sumPosterior.support())
run▼

Now, of course, A and B go from being independent a priori to being perfectly correlated in the conditional distribution. Try out these other
conditions to see other possible patterns of conditional dependence for a priori independent functions:

•Math.abs(A - B) < 2

•(A + B >= 9) && (A + B <= 11)

•Math.abs(A - B) == 3

•(A - B) % 10 == 3  (Note:  %  means “remainder when divided by…”)

•A % 2 == B % 2

•A % 5 == B % 5

Non-monotonic Reasoning

One reason explaining away is an important phenomenon in probabilistic inference is that it is an example of  non-monotonic reasoning. In formal
logic, a theory is said to be monotonic if adding an assumption (or formula) to the theory never reduces the set of conclusions that can be drawn.
Most traditional logics (e.g. First Order) are monotonic, but human reasoning does not seem to be. For instance, if I tell you that Tweety is a bird,
you conclude that he can fly; if I now tell you that Tweety is an ostrich you retract the conclusion that he can fly. Over the years many non-
monotonic logics have been introduced to model aspects of human reasoning. One of the first reasons that probabilistic reasoning with Bayesian
networks was recognized as important for AI was that it could perspicuously capture these patterns of reasoning (Pearl, 1988).

Another way to think about monotonicity is by considering the trajectory of our belief in a specific proposition, as we gain additional relevant
information. In traditional logic, there are only three states of belief: true, false, and unknown (when neither a proposition nor its negation can
be proven). As we learn more about the world, maintaining logical consistency requires that our belief in any proposition only move from unknown
to true or false. That is our “confidence” in any conclusion only increases (and only does so in one giant leap from unknown to true or false).

In a probabilistic approach, by contrast, belief comes in a whole spectrum of degrees. We can think of confidence as a measure of how far our
beliefs are from a uniform distribution—how close to the extremes of 0 or 1. In probabilistic inference, unlike in traditional logic, our confidence
in a proposition can both increase and decrease. Even fairly simple probabilistic models can induce complex explaining-away dynamics that lead our
degree of belief in a proposition to reverse directions multiple times as observations accumulate.

Example: Medical Diagnosis

The medical scenario is a great model to explore screening off and explaining away. In this model  smokes  is statistically dependent on several
symptoms— cough ,  chestPain , and  shortnessOfBreath —due to a causal chain between them mediated by  lungDisease . We can see this easily by
conditioning on these symptoms and looking at  smokes :

var medicalDist = Infer({method: 'enumerate'}, function() {
  var smokes = flip(.2);
  var lungDisease = flip(0.001) || (smokes && flip(0.1));
  var cold = flip(0.02);

  var cough = (cold && flip(0.5)) || (lungDisease && flip(0.5)) || flip(0.001);
  var fever = (cold && flip(0.3)) || flip(0.01);
  var chestPain = (lungDisease && flip(0.2)) || flip(0.01);
  var shortnessOfBreath = (lungDisease && flip(0.2)) || flip(0.01);

  condition(cough && chestPain && shortnessOfBreath);

  return smokes
})
viz(medicalDist)
run▼

The conditional probability of  smokes  is much higher than the base rate, 0.2, because observing all these symptoms gives strong evidence for
smoking. See how much evidence the different symptoms contribute by dropping them out of the conditioning set. (For instance, try conditioning
on  cough && chestPain , or just  cough ; you should observe the probability of  smokes  decrease as fewer symptoms are observed.)

Now, suppose we condition also on knowledge about the function that mediates these causal links:  lungDisease . Is there still an informational
dependence   between   these   various   symptoms   and  smokes ?   In   the   Inference   below,   try   adding   and   removing   various   symptoms
( cough ,  chestPain ,  shortnessOfBreath ) but maintaining the observation  lungDisease :

var medicalDist = Infer({method: 'enumerate'}, function() {
  var smokes = flip(.2);
  var lungDisease = flip(0.001) || (smokes && flip(0.1));
  var cold = flip(0.02);

  var cough = (cold && flip(0.5)) || (lungDisease && flip(0.5)) || flip(0.001);
  var fever = (cold && flip(0.3)) || flip(0.01);
  var chestPain = (lungDisease && flip(0.2)) || flip(0.01);
  var shortnessOfBreath = (lungDisease && flip(0.2)) || flip(0.01);

  condition(lungDisease && (cough && chestPain && shortnessOfBreath));

  return smokes
})

viz(medicalDist)
run▼

You should see an effect of whether the patient has lung disease on conditional inferences about smoking—a person is judged to be substantially
more likely to be a smoker if they have lung disease than otherwise—but there are no separate effects of chest pain, shortness of breath, or
cough over and above the evidence provided by knowing whether the patient has lung-disease. The intermediate variable lung disease  screens
off the root cause (smoking) from the more distant effects (coughing, chest pain and shortness of breath).

Here is a concrete example of explaining away in our medical scenario. Having a cold and having lung disease are a priori independent both causally
and statistically. But because they are both causes of coughing if we observe   cough  then  cold  and  lungDisease  become statistically dependent.
That  is,  learning  something   about   whether   a  patient   has  cold  or  lungDisease  will,  in  the   presence  of   their  common  effect  cough ,  convey
information about the other condition.  cold  and  lungCancer  are a priori independent, but conditionally dependent given  cough .

To illustrate, observe how the probabilities of  cold  and  lungDisease  change when we observe  cough  is true:

var medicalDist = Infer({method: 'enumerate'}, function() {
  var smokes = flip(.2);
  var lungDisease = flip(0.001) || (smokes && flip(0.1));
  var cold = flip(0.02);

  var cough = (cold && flip(0.5)) || (lungDisease && flip(0.5)) || flip(0.001);
  var fever = (cold && flip(0.3)) || flip(0.01);
  var chestPain = (lungDisease && flip(0.2)) || flip(0.01);
  var shortnessOfBreath = (lungDisease && flip(0.2)) || flip(0.01);

  condition(cough);

  return {cold: cold, lungDisease: lungDisease}
})

viz.marginals(medicalDist)
run▼

Both cold and lung disease are now far more likely that their baseline probability: the probability of having a cold increases from 2% to around
50%; the probability of having lung disease also increases from 2.1% to around 50%.

Now suppose we also learn that the patient does not have a cold.

var medicalDist = Infer({method: 'enumerate'}, function() {
  var smokes = flip(.2);
  var lungDisease = flip(0.001) || (smokes && flip(0.1));
  var cold = flip(0.02);

  var cough = (cold && flip(0.5)) || (lungDisease && flip(0.5)) || flip(0.001);
  var fever = (cold && flip(0.3)) || flip(0.01);
  var chestPain = (lungDisease && flip(0.2)) || flip(0.01);
  var shortnessOfBreath = (lungDisease && flip(0.2)) || flip(0.01);

  condition(cough && !cold);

  return {cold: cold, lungDisease: lungDisease}
})

viz.marginals(medicalDist)
run▼

The probability of having lung disease increases dramatically. If instead we had observed that the patient does have a cold, the probability of
lung cancer returns to its base rate of 2.1%

var medicalDist = Infer({method: 'enumerate'}, function() {

  var smokes = flip(.2);
  var lungDisease = flip(0.001) || (smokes && flip(0.1));
  var cold = flip(0.02);

  var cough = (cold && flip(0.5)) || (lungDisease && flip(0.5)) || flip(0.001);
  var fever = (cold && flip(0.3)) || flip(0.01);
  var chestPain = (lungDisease && flip(0.2)) || flip(0.01);
  var shortnessOfBreath = (lungDisease && flip(0.2)) || flip(0.01);

  condition(cough && cold);

  return {cold: cold, lungDisease: lungDisease}
})

viz.marginals(medicalDist)
run▼

This is the conditional statistical dependence between lung disease and cold, given cough: Learning that the patient does in fact have a cold
“explains away” the observed cough, so the alternative of lung disease decreases to a much lower value — roughly back to its 1 in a 1000 rate in
the general population. If on the other hand, we had learned that the patient does not have a cold, so the most likely alternative to lung
disease is not in fact available to “explain away” the observed cough, that raises the conditional probability of lung disease dramatically. As an
exercise, check that if we remove the observation of coughing, the observation of having a cold or not has no influence on our belief about lung
disease; this effect is purely conditional on the observation of a common effect of these two causes.

Explaining away effects can be more indirect. Instead of observing the truth value of   cold , a direct alternative cause of  cough , we might simply
observe another symptom that provides evidence for  cold , such as  fever . Compare these conditions using the above WebPPL program to see an
“explaining away” conditional dependence in belief between  fever  and  lungDisease .

Example: Trait Attribution

A familiar example of rich patterns of inference comes from reasoning about the causes of students’ success and failure in the classroom.
Imagine yourself in the position of an interested outside observer—a parent, another teacher, a guidance counselor or college admissions officer—
in thinking about these conditional inferences. If a student doesn’t pass an exam, what can you say about why he failed? Maybe he doesn’t do his
homework, maybe the exam was unfair, or maybe he was just unlucky?

var examPosterior = Infer({method: 'enumerate'}, function() {
  var examFair = flip(.8)
  var doesHomework = flip(.8)
  var pass = flip(examFair ?
                  (doesHomework ? 0.9 : 0.4) :
                  (doesHomework ? 0.6 : 0.2))
  condition(!pass)
  return {doesHomework: doesHomework, examFair: examFair}
})

viz.marginals(examPosterior)
viz(examPosterior)
run▼

Now what if you have evidence from several students and several exams? We first re-write the above model to allow many students and exams:

var examPosterior = Infer({method: 'enumerate'}, function() {
  var examFair = mem(function(exam){return flip(0.8)})
  var doesHomework = mem(function(student){return flip(0.8)})

  var pass = function(student, exam) {
    return flip(examFair(exam) ?
                (doesHomework(student) ? 0.9 : 0.4) :
                (doesHomework(student) ? 0.6 : 0.2))
  };

  condition(!pass('bill', 'exam1'))

  return {doesHomework: doesHomework('bill'), examFair: examFair('exam1')}

})

viz.marginals(examPosterior)
viz(examPosterior)
run▼

Initially we observe that Bill failed exam 1. A priori, we assume that most students do their homework and most exams are fair, but given this one
observation it becomes somewhat likely that either the student didn’t study or the exam was unfair.

Notice that we have set the probabilities in the  pass  function to be asymmetric: whether a student does homework has a greater influence on
passing the test than whether the exam is fair. This in turns means that when inferring the  cause of a failed exam, the model tends to
attribute it to the person property (not doing homework) over the situation property (exam being unfair). This asymmetry is an example of
the fundamental attribution bias (Ross, 1977): we tend to attribute outcomes to personal traits rather than situations. However there are many
interacting tendencies (for instance the direction of this bias switches for members of some east-asian cultures). How could you extend the
model to account for these interactions?

See how conditional inferences about Bill and exam 1 change as you add in more data about this student or this exam, or additional students and
exams. Try using each of the below expressions as the condition for the above inference. Try to explain the different inferences that result
at each stage. What does each new piece of the larger data set contribute to your intuition about Bill and exam 1?

•!pass('bill', 'exam1') && !pass('bill', 'exam2')
•!pass('bill', 'exam1') && !pass('mary', 'exam1') && !pass('tim', 'exam1')
•!pass('bill', 'exam1') && !pass('bill', 'exam2') && !pass('mary', 'exam1') && !pass('tim', 'exam1')
•!pass('bill', 'exam1') && !pass('mary', 'exam1') && pass('mary', 'exam2') && pass('mary', 'exam3') && pass('mary', 'exam4') && pass('mary', 'exam5')
&& !pass('tim', 'exam1') && pass('tim', 'exam2') && pass('tim', 'exam3') && pass('tim', 'exam4') && pass('tim', 'exam5')

•!pass('bill', 'exam1') && pass('mary', 'exam1') && pass('tim', 'exam1')
•!pass('bill', 'exam1') && pass('mary', 'exam1') && pass('mary', 'exam2') && pass('mary', 'exam3') && pass('mary', 'exam4') && pass('mary', 'exam5') &&
pass('tim', 'exam1') && pass('tim', 'exam2') && pass('tim', 'exam3') && pass('tim', 'exam4') && pass('tim', 'exam5')

•!pass('bill', 'exam1') && !pass('bill', 'exam2') && pass('mary', 'exam1') && pass('mary', 'exam2') && pass('mary', 'exam3') && pass('mary', 'exam4') &&
pass('mary', 'exam5') && pass('tim', 'exam1') && pass('tim', 'exam2') && pass('tim', 'exam3') && pass('tim', 'exam4') && pass('tim', 'exam5')

•!pass('bill', 'exam1') && !pass('bill', 'exam2') && pass('bill', 'exam3') && pass('bill', 'exam4') && pass('bill', 'exam5') && pass('mary', 'exam1') &&
pass('mary', 'exam2') && pass('mary', 'exam3') && pass('mary', 'exam4') && pass('mary', 'exam5') && pass('tim', 'exam1') && pass('tim', 'exam2') &&
pass('tim', 'exam3') && pass('tim', 'exam4') && pass('tim', 'exam5')

This example is inspired by the work of Harold Kelley (and many others) on causal attribution in social settings ( Kelley, 1973). Kelley identified
three important dimensions of variation in the evidence, which affect the attributions people make of the cause of an outcome. These three
dimensions are: Persons—is the outcome consistent across different people in the situation?; Entities—is the outcome consistent for different
entities in the situation?; Time—is the outcome consistent over different episodes? These dimensions map onto the different sets of evidence we
have just seen—how?

As in this example, people often have to make inferences about entities and their interactions. Such problems tend to have dense relations
between the entities, leading to very challenging explaining away problems. These inferences often come very naturally to people, yet they are
computationally difficult. Perhaps these are important problems that our brains have specialized somewhat to solve, or perhaps that they have
evolved general solutions to these tough inferences. What do you think?

Example: Of Blickets and Blocking

A number of researchers have explored children’s causal learning abilities by using the “blicket detector” (Gopnik and Sobel, 2000): a toy box
that will light up when certain blocks, the blickets, are put on top of it. Children are shown a set of evidence and then asked which blocks are
blickets. For instance, if block A makes the detector go off, it is probably a blicket. Ambiguous patterns are particularly interesting. Imagine that
blocks A and B are put on the detector together, making the detector go off; it is fairly likely that A is a blicket. Now B is put on the detector
alone, making the detector go off; it is now less plausible that A is a blicket. This is called “backward blocking”, and it is an example of explaining
away.

We can capture this set up with a model in which each block has a persistent “blicket-ness” property, and the causal power of the block to make
the machine go off depends on its blicketness. Finally, the machine goes off if any of the blocks on it is a blicket (but noisily).

var blicketPosterior = Infer({method: 'enumerate'}, function() {
  var blicket = mem(function(block) {return flip(.4)})
  var power = function(block) {return blicket(block) ? .9 : .05}
  var machine = function(blocks) {
    return (blocks.length == 0 ?
            flip(.05) :

            flip(power(first(blocks))) || machine(rest(blocks)))
  }
  condition(machine(['A', 'B']))
  //condition(machine(['B'])) //uncomment to see explaining away!
  return blicket('A')
});

viz(blicketPosterior)
run▼

Try the backward blocking scenario described above. Sobel et al. (2004) tried this with children, finding that four year-olds perform similarly to
the model: evidence that B is a blicket explains away the evidence that A and B made the detector go away.

A Case Study in Modularity: Visual Perception of 
Surface Color

Visual perception is full of rich conditional inference phenomena, including both screening off and explaining away. Some very impressive
demonstrations have been constructed using the perception of surface structure by mid-level vision researchers; see the work of Dan Kersten,
David Knill, Ted Adelson, Bart Anderson, Ken Nakayama, among others. Most striking is when conditional inference appears to violate or alter the
apparently “modular” structure of visual processing. Neuroscientists have developed an understanding of the primate visual system in which
processing for different aspects of visual stimuli—color, shape, motion, stereo—appears to be at least somewhat localized in different brain
regions. This view is consistent with findings by cognitive psychologists that at least in early vision, these different stimulus dimensions are not
integrated but processed in a sequential, modular fashion. Yet vision is at heart about constructing a unified and coherent percept of a three-
dimensional scene from the patterns of light falling on our retinas. That is, vision is causal inference on a grand scale. Its output is a rich
description of the objects, surface properties and relations in the world that are not themselves directly grasped by the brain but that are the
true causes of the input—low-level stimulation of the retina. Solving this problem requires integration of many appearance features across an
image, and this results in the potential for massive effects of explaining away and screening off.

In vision, the luminance of a surface depends on two factors, the illumination of the surface (how much light is hitting it) and its intrinsic
reflectance.  The actual luminance  is the product of the  two factors. Thus luminance is  inherently ambiguous. The visual  system has to
determine what proportion of the luminance is due to reflectance and what proportion is due to the illumination of the scene. This has led to a
famous illusion known as the checker shadow illusion discovered by Ted Adelson.

Despite appearances, in the image above both the square labeled A and the square labeled B are actually the same shade of gray. This can be
seen in the figure below where they are connected by solid gray bars on either side.

The presence of the cylinder is providing evidence that the illumination of square B is actually less than that of square A (because it is
expected to cast a shadow). Thus we perceive square B as having higher reflectance since its luminance is identical to square A and we believe
there is less light hitting it. The following program implements a simple version of this scenario “before” we see the shadow cast by the
cylinder.

var observedLuminance = 3;

var reflectancePosterior = Infer({method: 'MCMC', samples: 10000}, function() {
  var reflectance = gaussian({mu: 1, sigma: 1})
  var illumination = gaussian({mu: 3, sigma: 1})
  var luminance = reflectance * illumination
  observe(Gaussian({mu: luminance, sigma: 1}), observedLuminance)
  return reflectance
});

print(expectation(reflectancePosterior))
viz(reflectancePosterior)
run▼

Now let’s condition on the presence of the cylinder, by conditioning on the presence of its “shadow” (i.e. gaining a noisy observation suggesting
that illumination is lower than expected a priori):

var observedLuminance = 3;

var reflectancePosterior = Infer({method: 'MCMC', samples: 10000}, function() {
  var reflectance = gaussian({mu: 1, sigma: 1})
  var illumination = gaussian({mu: 3, sigma: 1})
  var luminance = reflectance * illumination
  observe(Gaussian({mu: luminance, sigma: 1}), observedLuminance)
  observe(Gaussian({mu: illumination, sigma: 0.1}), 0.5)
  return reflectance
});

print(expectation(reflectancePosterior))
viz(reflectancePosterior)
run▼

The variables  reflectance  and  illumination  are conditionally independent in the generative model, but after we condition on  luminance  they
become dependent: changing one of them affects the probability of the other. Although the model of (our knowledge of) the world has a certain
kind of modularity implied by conditional independence, as soon as we start using the model to do conditional inference on some data, formerly
modularly isolated variables can become dependent. This general phenomenon has important consequences for cognitive science.

Test your knowledge: Exercises

Reading & Discussion: Readings
Next chapter: 6. Social cognition 

☰

Social cognition

The  Infer  operator is an ordinary WebPPL function, in the sense that it can occur anywhere that any other function can occur. In particular, we
can construct an inference containing another inference inside it: this represents hypothetical inference about a hypothetical inference. Nested
inferences are particularly useful in modeling social cognition: reasoning about another agent, who is herself reasoning.

Prelude: Thinking About Assembly Lines

Imagine a factory where the widget-maker is used to make widgets, but they are sometimes faulty. The tester tests them and lets through only
the good ones. You don’t know what tolerance the widget tester is set to, and wish to infer it. We can represent this as:

// this machine makes a widget -- which we'll just represent with a real number:
var widgetMachine = Categorical({vs: [.2 , .3, .4, .5, .6, .7, .8 ],
                                 ps: [.05, .1, .2, .3, .2, .1, .05]})

//this machine tests them to get a good one:
var getGoodWidget = function(tolerance) {
  var widget = sample(widgetMachine)
  return widget>tolerance ? widget : getGoodWidget(tolerance)
}

//if we observe widgets [.6, .7, .8], what tolerance was the tester set to?
var actualWidgets = [.6, .7, .8]
var d = Infer(function() {
  var tolerance = uniform(0.3,0.7) 
  condition(getGoodWidget(tolerance) == actualWidgets[0])
  condition(getGoodWidget(tolerance) == actualWidgets[1])
  condition(getGoodWidget(tolerance) == actualWidgets[2])
  return {tolerance: tolerance}
})

viz(d)
run▼

But notice that the definition of  getGoodWidget  is exactly like the definition of rejection sampling! We can re-write this as a nested inference
model:

// this machine makes a widget -- which we'll just represent with a real number:
var widgetMachine = Categorical({vs: [.2 , .3, .4, .5, .6, .7, .8 ],
                                 ps: [.05, .1, .2, .3, .2, .1, .05]})

//this machine tests them to get a good one:
var getGoodWidget = function(tolerance) {
  return sample(Infer(function(){
    var widget = sample(widgetMachine)
    condition(widget>tolerance)
    return widget}))
}

//if we observe widgets [.6, .7, .8], what tolerance was the tester set to?
var actualWidgets = [.6, .7, .8]
var d = Infer(function() {
  var tolerance = uniform(0.3,0.7) 
  condition(getGoodWidget(tolerance) == actualWidgets[0])
  condition(getGoodWidget(tolerance) == actualWidgets[1])
  condition(getGoodWidget(tolerance) == actualWidgets[2])
  return {tolerance: tolerance}
})

viz(d)
run▼

We are now abstracting the tester machine, rather than thinking about the details inside the widget tester. We represent only that the machine
correctly gives a widget above tolerance (by some means).

In order to preserve the return values of  getGoodWidget  we had to take a single  sample  from the inferred distribution on widgets. Knowing that
we have access to this distribution, we can modify our outer inference to use it directly:

// this machine makes a widget -- which we'll just represent with a real number:
var widgetMachine = Categorical({vs: [.2 , .3, .4, .5, .6, .7, .8 ],
                                 ps: [.05, .1, .2, .3, .2, .1, .05]})

//this machine tests them to get a good one:
var getGoodWidget = function(tolerance) {
  return Infer(function(){
    var widget = sample(widgetMachine)
    condition(widget>tolerance)
    return widget})
}

//if we observe widgets [.6, .7, .8], what tolerance was the tester set to?
var actualWidgets = [.6, .7, .8]
var d = Infer(function() {
  var tolerance = uniform(0.3,0.7) 
  var goodWidgetDist = getGoodWidget(tolerance)
  observe(goodWidgetDist, actualWidgets[0])
  observe(goodWidgetDist, actualWidgets[1])
  observe(goodWidgetDist, actualWidgets[2])
  return {tolerance: tolerance}
})

viz(d)
run▼

This pattern – an outer  Infer  that observes values from an inner  Infer  – is a very general and useful way to represent reasoning about
reasoning.

Social Cognition

How can we capture our intuitive theory of other people? Central to our understanding is the principle of rationality: an agent tends to choose
actions that she expects to lead to outcomes that satisfy her goals. (This is a slight restatement of the principle as discussed in  Baker et al.
(2009), building on earlier work by Dennett (1989), among others.) We can represent this in WebPPL as an inference over actions—an agent reasons
about actions that lead to their goal being satisfied:

var chooseAction = function(goalSatisfied, transition, state) {

  return Infer(..., function() {

    var action = sample(actionPrior)

    condition(goalSatisfied(transition(state, action)))

    return action;

  })

}

The function  transition  describes the outcome of taking a particular action in a particular state, the predicate  goalSatisfied  determines whether
or   not   a   state   accomplishes   the   goal,   the   input  state  represents   the   current   state   of   the   world.   The   distribution  actionPrior  used
within  chooseAction  represents an a-priori tendency towards certain actions.

To make the following examples a bit simpler let’s specialize this to the case where the goal is achieving a particular state, and add a default
for the state input:

var chooseAction = function(goalState, transition, state) {

  var state = (state==undefined)?'start':state

  return Infer(function() {

    var action = sample(actionPrior)

    condition(goalState == transition(state, action))

    return action

  })

}

For instance, imagine that Sally walks up to a vending machine wishing to have a cookie. Imagine also that we know the mapping between buttons
(potential actions) and foods (outcomes). We can then predict Sally’s action:

var actionPrior = Categorical({vs: ['a', 'b'], ps: [.5, .5]})
var vendingMachine = function(state, action) {
  return (action == 'a' ? 'bagel' :
          action == 'b' ? 'cookie' :
          'nothing')
}

var chooseAction = function(goalState, transition, state) {
  var state = (state==undefined)?'start':state
  return Infer(function() {
    var action = sample(actionPrior)
    condition(goalState == transition(state, action))
    return action
  })
}

viz(chooseAction('cookie', vendingMachine))
run▼

We see, unsurprisingly, that if Sally wants a cookie, she will always press button b. In a world that is not quite so deterministic Sally’s actions
will be more stochastic:

///fold:
...
var vendingMachine = function(state, action) {
  return (action == 'a' ? categorical({vs: ['bagel', 'cookie'], ps: [.9, .1]}) :
          action == 'b' ? categorical({vs: ['bagel', 'cookie'], ps: [.1, .9]}) :
          'nothing');
}

var chooseAction = function(goalState, transition, state) {
  var state = (state==undefined)?'start':state
  return Infer(function() {
    var action = sample(actionPrior)
    condition(goalState == transition(state, action))
    return action
  })
}

viz(chooseAction('cookie', vendingMachine));
run▼

Technically, this method of making a choices is not optimal, but rather it is soft-max optimal (also known as following the “Boltzmann policy”).

Inferring goals

Now imagine that we don’t know Sally’s goal (which food she wants), but we observe her pressing button b. We can use   Infer  to infer her goal
(this is sometimes called “inverse planning”, since the outer infer “inverts” the inference inside  chooseAction ).

///fold:

...

var goalPosterior = Infer({method: 'enumerate'}, function() {
  var goal = categorical({vs: ['bagel', 'cookie'], ps: [.5, .5]})
  var actionDist = chooseAction(goal, vendingMachine)
  observe(actionDist,'b')
  return goal;
})

viz(goalPosterior);
run▼

Now let’s imagine a more ambiguous case: button b is “broken” and will (uniformly) randomly result in a food from the machine. If we see Sally
press button b, what goal is she most likely to have?

///fold:
...
var vendingMachine = function(state, action) {
  return (action == 'a' ? categorical({vs: ['bagel', 'cookie'], ps: [.9, .1]}) :
          action == 'b' ? categorical({vs: ['bagel', 'cookie'], ps: [.5, .5]}) :
          'nothing')
}

var goalPosterior = Infer({method: 'enumerate'}, function() {
  var goal = categorical({vs: ['bagel', 'cookie'], ps: [.5, .5]})
  var actionDist = chooseAction(goal, vendingMachine)
  observe(actionDist,'b')
  return goal
})

viz(goalPosterior)
run▼

Despite the fact that button b is equally likely to result in either bagel or cookie, we have inferred that Sally probably wants a cookie. This is a
result of the inference implicitly taking into account the counterfactual alternatives: if Sally had wanted a bagel, she would have likely
pressed button a. The inner query takes these alternatives into account, adjusting the probability of the observed action based on alternative
goals.

Inferring preferences

If we have some prior knowledge about Sally’s preferences (which goals she is likely to have) we can incorporate this immediately into the prior
over goals (which above was uniform).

A more interesting situation is when we believe that Sally has some preferences, but we don’t know what they are. We capture this by adding a
higher level prior (a uniform) over preferences. Using this we can learn about Sally’s preferences from her actions: after seeing Sally press
button b several times, what will we expect her to want the next time?

///fold:
...
var vendingMachine = function(state, action) {
  return (action == 'a' ? categorical({vs: ['bagel', 'cookie'], ps: [.9, .1]}) :
          action == 'b' ? categorical({vs: ['bagel', 'cookie'], ps: [.1, .9]}) :
          'nothing')
}

var goalPosterior = Infer({method: 'MCMC', samples: 20000}, function() {
  var preference = uniform(0, 1);
  var goalPrior = function() {return flip(preference) ? 'bagel' : 'cookie'}

  observe(chooseAction(goalPrior(), vendingMachine),'b')
  observe(chooseAction(goalPrior(), vendingMachine),'b')
  observe(chooseAction(goalPrior(), vendingMachine),'b')

  return goalPrior()
})

viz(goalPosterior)
run▼

Try varying the amount and kind of evidence. For instance, if Sally one time says “I want a cookie” (so you have directly observed her goal that
time) how much evidence does that give you about her preferences, relative to observing her actions?

In the above preference inference, it is extremely important that sally could have taken a different action if she had a different preference
(i.e. she could have pressed button a if she preferred to have a bagel). In the program below we have set up a situation in which both actions
lead to cookie most of the time:

///fold:
...
var vendingMachine = function(state, action) {
  return (action == 'a' ? categorical({vs: ['bagel', 'cookie'], ps: [.1, .9]}) :
          action == 'b' ? categorical({vs: ['bagel', 'cookie'], ps: [.1, .9]}) :
          'nothing')
}

var goalPosterior = Infer({method: 'MCMC', samples: 50000}, function() {
  var preference = uniform(0, 1)
  var goalPrior = function() {return flip(preference) ? 'bagel' : 'cookie'}

  observe(chooseAction(goalPrior(), vendingMachine),'b')
  observe(chooseAction(goalPrior(), vendingMachine),'b')
  observe(chooseAction(goalPrior(), vendingMachine),'b')

  return goalPrior()
})

viz(goalPosterior)
run▼

Now we can draw no conclusion about Sally’s preferences. Try varying the machine probabilities, how does the preference inference change? This
effect, that the strength of a preference inference depends on the context of alternative actions, has been demonstrated in young infants
by Kushnir et al. (2010).

Inferring what they know

In the above models of goal and preference inference, we have assumed that the structure of the world (both the operation of the vending
machine and the (irrelevant) initial state) were common knowledge—they were non-random constructs used by both the agent (Sally) selecting
actions and the observer interpreting these actions. What if we (the observer) don’t know how exactly the vending machine works, but think
that Sally knows how it works? We can capture this by placing uncertainty on the vending machine inside the overall query but “outside” of
Sally’s inference:

///fold:
...

var goalPosterior = Infer({method: 'MCMC', samples: 50000}, function() {
  var buttonsToBagelProbs = {'a': uniform(0,1), 'b': uniform(0,1)}
  var vendingMachine = function(state, action) {
    return categorical({vs: ['bagel', 'cookie'], 
                        ps: [buttonsToBagelProbs[action], 1-buttonsToBagelProbs[action]]})
  }

  var goal = categorical({vs: ['bagel', 'cookie'], ps: [.5, .5]})

  condition(goal == 'cookie')
  observe(chooseAction(goal, vendingMachine), 'b')

  return {buttonA: vendingMachine('state','a'), buttonB: vendingMachine('state','b')}
})

viz.marginals(goalPosterior)
run▼

Here we have conditioned on Sally wanting the cookie and Sally choosing to press button b. Thus, we have no  direct evidence of the effects of
pressing the buttons on the machine. What happens if you condition instead on the action and outcome, but not the intentional choice of this
outcome? (That is, change the  observe  to  condition(vendingMachine('state','b') == 'cookie') ).)

Now imagine a vending machine that has only one button, but it can be pressed many times. We don’t know what the machine will do in response to a
given button sequence. We do know that pressing more buttons is less a priori likely.

var actionPrior = Categorical({vs: ['a', 'aa', 'aaa'], ps:[0.7, 0.2, 0.1] })

///fold:
...

var goalPosterior = Infer({method: 'rejection', samples: 5000}, function() {
  var buttonsToBagelProbs = {'a': uniform(0,1), 'aa': uniform(0,1), 'aaa': uniform(0,1)}
  var vendingMachine = function(state, action) {
    return categorical({vs: ['bagel', 'cookie'], 
                        ps: [buttonsToBagelProbs[action], 1-buttonsToBagelProbs[action]]})
  }

  var goal = categorical({vs: ['bagel', 'cookie'], ps: [.5, .5]})

  condition(goal == 'cookie')
  observe(chooseAction(goal, vendingMachine), 'a')

  return {once: vendingMachine('state', 'a'),
          twice: vendingMachine('state', 'aa')}
})

print("probability of actions giving a cookie")
viz.marginals(goalPosterior)
run▼

Compare the inferences that result if Sally presses the button twice to those if she only presses the button once. Why can we draw much
stronger inferences about the machine when Sally chooses to press the button twice? When Sally does press the button twice, she could have
done the “easier” (or rather, a priori more likely) action of pressing the button just once. Since she doesn’t, a single press must have been
unlikely to result in a cookie. This is an example of the principle of efficiency—all other things being equal, an agent will take the actions that
require least effort (and hence, when an agent expends more effort all other things must not be equal). Here, Sally has three possible actions
but simpler actions are a priori more likely.

In these examples we have seen two important assumptions combining to allow us to infer something about the world from the indirect evidence of
an agents actions. The first assumption is the principle of rational action, the second is an assumption of knowledgeability—we assumed that Sally
knows how the machine works, though we don’t. Thus inference about inference, can be a powerful way to learn what others already know, by
observing their actions. (This example was inspired by Goodman et al. (2009))

Joint inference about knowledge and goals
In social cognition, we often make joint inferences about two kinds of mental states: agents’ beliefs about the world and their desires, goals or
preferences. We can see an example of such a joint inference in the vending machine scenario. Suppose we condition on two observations: that Sally
presses the button twice, and that this results in a cookie. Then, assuming that she knows how the machine works, we jointly infer that she
wanted a cookie, that pressing the button twice is likely to give a cookie, and that pressing the button once is unlikely to give a cookie.

var actionPrior = Categorical({vs: ['a', 'aa', 'aaa'], ps:[0.7, 0.2, 0.1] })

///fold:
...

var goalPosterior = Infer({method: 'rejection', samples: 5000}, function() {
  var buttonsToBagelProbs = {'a': uniform(0,1), 'aa': uniform(0,1), 'aaa': uniform(0,1)}
  var vendingMachine = function(state, action) {
    return categorical({vs: ['bagel', 'cookie'], 
                        ps: [buttonsToBagelProbs[action], 1-buttonsToBagelProbs[action]]})
  }

  var goal = categorical({vs: ['bagel', 'cookie'], ps: [.5, .5]})

  observe(chooseAction(goal, vendingMachine), 'aa')
  condition(vendingMachine('state', 'aa') == 'cookie')

  return {goal: goal,
          onePressResult: vendingMachine('state', 'a'),
          twoPressResult: vendingMachine('state', 'aa'),
          onePressCookieProb: 1-buttonsToBagelProbs['a']
         }
})

viz.marginals(goalPosterior)
run▼

Here we have also returned the posterior belief about how likely one press is to yield a cookie. Notice the U-shaped distribution. Without any
direct evidence about what happens when the button is pressed just once we can infer that it probably won’t give a cookie—because her goal is
likely to have been a cookie but she didn’t press the button just once—but there is a small chance that her goal was actually not to get a cookie,
in which case pressing the button once could result in a cookie. This very complex (and hard to describe!) inference comes naturally from joint
inference of goals and knowledge.

Inferring whether they know

Let’s imagine that we (the observer) know that the vending machine actually tends to return a bagel for button a and a cookie for button b. But
we don’t know if Sally knows this! Instead we see Sally announce that she wants a cookie, but pushes button a. How can we determine, from her
actions, whether Sally is knowledgeable or ignorant? We hypothesize that if she is ignorant, Sally chooses according to a random vending machine.
We can then infer her knowledge state:

///fold:
...

var buttonsToBagelProbs = {'a': 0.9, 'b': 0.1}
var trueVendingMachine = function(state, action) {
  return categorical({vs: ['bagel', 'cookie'], 
                      ps: [buttonsToBagelProbs[action], 1-buttonsToBagelProbs[action]]})
}
var randomMachine = function(state, action) {
  return categorical({vs: ['bagel', 'cookie'], 
                      ps: [0.5, 0.5]})
}

var p = Infer(function() {
  var knows = flip()

  var goal = 'cookie'
  observe(chooseAction(goal, knows?trueVendingMachine:randomMachine), 'a')
  condition(trueVendingMachine('start', 'a') == 'bagel')

  return {knowledge: knows}
})

viz.marginals(p)
run▼

This is a very simple example, but it illustrates how we can represent a difference in knowledge between the observer and the observed agent
by simply using different world models (the vending machines) for explaining the action (in  chooseAction ) and for explaining the outcome (in
the  condition ).

Inferring what they believe

Above we assumed that if Sally is ignorant, she chooses based on a random machine. This is both not flexible enough and too strong an assumption.
Indeed, Sally may have all kinds of specific (and potentially false) beliefs about vending machines. To capture this, we can represent Sally’s
beliefs as a separate randomly chosen vending machine: by passing this into Sally’s   chooseAction  we indicate these are Sally’s beliefs, by putting
this inside the outer  Infer  we represent the observer reasoning about Sally’s beliefs:

///fold:
...

var buttonsToBagelProbs = {'a': 0.9, 'b': 0.1}
var trueVendingMachine = function(state, action) {
  return categorical({vs: ['bagel', 'cookie'], 
                      ps: [buttonsToBagelProbs[action], 1-buttonsToBagelProbs[action]]})
}

var p = Infer(function() {
  var SallyBelief = {'a': uniform(0,1), 'b': uniform(0,1)}
  var SallyMachine = function(state, action) {
    return categorical({vs: ['bagel', 'cookie'], 
                        ps: [SallyBelief[action], 1-SallyBelief[action]]})}

  var goal = 'cookie'
  observe(chooseAction(goal, SallyMachine), 'a')
  condition(trueVendingMachine('start', 'a') == 'bagel')

  return {SallyBeliefButtonA: SallyMachine('start','a')}
})

viz.marginals(p)
run▼

In the developmental psychology literature, the ability to represent and reason about other people’s false beliefs has been extensively
investigated as a hallmark of human Theory of Mind.

Emotion and other mental states

So far we have explored reasoning about others’ goals, preferences, knowledge, and beliefs. It is commonplace to discuss other’s actions in terms
of many other mental states as well! We might explain an unexpected slip in terms of wandering  attention, a short-sighted choice in terms
of temptation, a violent reaction in terms of anger, a purposeless embellishment in terms of joy. Each of these has a potential role to play in an
elaborated scientific theory of how humans represent other’s minds.

Communication and Language
A Communication Game

Imagine playing the following two-player game. On each round the “teacher” pulls a die from a bag of weighted dice, and has to communicate to
the “learner” which die it is (both players are familiar with the dice and their weights). However, the teacher may only communicate by giving
the learner examples: showing them faces of the die.

We can formalize the inference of the teacher in choosing the examples to give by assuming that the goal of the teacher is to successfully teach
the hypothesis – that is, to choose examples such that the learner will infer the intended hypothesis (throughout this section we simplify the
code by specializing to the situation at hand, rather than using the more general  chooseAction  function introduced above):

var teacher = function(die) {

  return Infer(..., function() {

    var side = sample(sidePrior);

    condition(learner(side) == die)

    return side;

  })

}

The goal of the learner is to infer the correct hypothesis, given that the teacher chose to give these examples:

var learner = function(side) {

  return Infer(..., function() {

  
  
    var die = sample(diePrior);

    condition(teacher(die) == side)

    return die;

  })

}

This pair of mutually recursive functions represents a teacher choosing examples or a learner inferring a hypothesis, each thinking about the
other. However, notice that this recursion will never halt—it will be an unending chain of “I think that you think that I think that…”. To avoid
this infinite recursion, say that eventually the learner will just assume that the teacher rolled the die and showed the side that came up
(rather than reasoning about the teacher choosing a side):

var teacher = function(die, depth) {

  return Infer(..., function() {

    var side = sample(sidePrior);

    condition(learner(side, depth) == die)

    return side

  })

}

var learner = function(side, depth) {

  return Infer(..., function() {

    var die = sample(diePrior);

    condition(depth == 0 ?

              side == roll(die) :

              side == teacher(die, depth - 1))

    return die

  })

}

To make this concrete, assume that there are two dice, A and B, which each have three sides (red, green, blue) that have weights like so:

Which hypothesis will the learner infer if the teacher shows the green side?

var dieToProbs = function(die) {
  return (die == 'A' ? [0, .2, .8] :

          die == 'B' ? [.1, .3, .6] :
          'uhoh')
}

var sidePrior = Categorical({vs: ['red', 'green', 'blue'], ps: [1/3, 1/3, 1/3]})
var diePrior = Categorical({vs: ['A', 'B'], ps: [1/2, 1/2]})
var roll = function(die) {return categorical({vs: ['red', 'green', 'blue'], ps: dieToProbs(die)})}

var teacher = function(die, depth) {
  return Infer({method: 'enumerate'}, function() {
    var side = sample(sidePrior);
    condition(sample(learner(side, depth)) == die)
    return side
  })
}

var learner = function(side, depth) {
  return Infer({method: 'enumerate'}, function() {
    var die = sample(diePrior);
    condition(depth == 0 ?
              side == roll(die) :
              side == sample(teacher(die, depth - 1)))
    return die
  })
}

viz.auto(learner('green', 3))
run▼

If we run this with recursion depth 0—that is a learner that does probabilistic inference without thinking about the teacher thinking—we find
the learner infers hypothesis B most of the time (about 60% of the time). This is the same as using the “strong sampling” assumption: the
learner infers B because B is more likely to have landed on side 2. However, if we increase the recursion depth we find this reverses: the
learner infers B only about 40% of the time. Now die A becomes the better inference, because “if the teacher had meant to communicate B,
they would have shown the red side because that can never come from A.”

This model, has been proposed by Shafto et al. (2012) as a model of natural pedagogy. They describe several experimental tests of this model in
the setting of simple “teaching games,” showing that people make inferences as above when they think the examples come from a helpful
teacher, but not otherwise.

Communicating with Words

Unlike the situation above, in which concrete examples were given from teacher to student, words in natural language denote more abstract
concepts. However, we can use almost the same setup to reason about speakers and listeners communicating with words, if we assume that
sentences have literal meanings, which anchor sentences to possible worlds. We assume for simplicity that the meaning of sentences are truth-
functional: that each sentence corresponds to a function from states of the world to true/false.

As above, the speaker chooses what to say in order to lead the listener to infer the correct state:

var speaker = function(state) {

  return Infer(..., function() {

    var words = sample(sentencePrior);

    condition(state == listener(words));

    return words;

  });

};

The listener does an inference of the state of the world given that the speaker chose to say what they did:

var listener = function(words) {

  return Infer(..., function() {

    var state = sample(statePrior);

    condition(words == speaker(state));

    return state;

  });

};

However this suffers from two flaws: the recursion never halts, and the literal meaning has not been used. We slightly modify the listener
function such that the listener either assumes that the literal meaning of the sentence is true, or figures out what the speaker must have
meant given that they chose to say what they said:

var listener = function(words) {

  return Infer(..., function() {

    var state = sample(statePrior);

    condition(flip(literalProb) ?

              words(state) :

              words == speaker(state))

    return state;

  });

};

Here the probability  literalProb  controls the expected depth of recursion. Another ways to bound the depth of recursion is with an explicit
depth argument (which is decremented on each recursion).

We have used a standard, truth-functional, formulation for the meaning of a sentence: each sentence specifies a (deterministic) predicate on
world states. Thus the literal meaning of a sentence specifies the worlds in which the sentence is satisfied. That is the literal meaning of a
sentence is the sort of thing one can condition on, transforming a prior over worlds into a posterior. Here’s another way of motivating this view:
meanings are belief update operations, and since the right way to update beliefs coded as distributions is conditioning, meanings are conditioning
statements.   Of   course,   the   deterministic   predicates   can   be   immediately   (without   changing   any   other   code)   relaxed   to   probabilistic   truth
functions, that assign a probability to each world. This might be useful if we want to allow exceptions.

Example: Scalar Implicature
Let us imagine a situation in which there are three plants which may or may not have sprouted. We imagine that there are three sentences that
the speaker could say, “All of the plants have sprouted”, “Some of the plants have sprouted”, or “None of the plants have sprouted”. For
simplicity we represent the worlds by the number of sprouted plants (0, 1, 2, or 3) and take a uniform prior over worlds. Using the above
representation for communicating with words (with an explicit depth argument):

var allSprouted = function(state) {return state == 3}
var someSprouted = function(state) {return state > 0}
var noneSprouted = function(state) {return state == 0}
var meaning = function(words) {
  return (words == 'all' ? allSprouted :
          words == 'some' ? someSprouted :
          words == 'none' ? noneSprouted :
          console.error("unknown words"))
}

var statePrior = Categorical({vs: [0,1,2,3],
                              ps: [1/4, 1/4, 1/4, 1/4]})
var sentencePrior = Categorical({vs: ["all", "some", "none"],
                                 ps: [1/3, 1/3, 1/3]})

var speaker = function(state, depth) {
  return Infer({method: 'enumerate'}, function() {

    var words = sample(sentencePrior)
    condition(state == sample(listener(words, depth)))
    return words
  })
};

var listener = function(words, depth) {
  return Infer({method: 'enumerate'}, function() {
    var state = sample(statePrior);
    var wordsMeaning = meaning(words)
    condition(depth == 0 ? wordsMeaning(state) :
               .isEqual(
    return state;
  })
};

_

words, sample(speaker(state, depth - 1))))

viz.auto(listener("some", 1))
run▼

We see that if the listener hears “some” the probability of three out of three is low, even though the basic meaning of “some” is equally
consistent with 3/3, 1/3, and 2/3. This is called the “some but not all” implicature.

Reading & Discussion: Readings

Test your knowledge: Exercises
Next chapter: 6.5. Interlude - Bayesian data analysis 

☰

Interlude - Bayesian data analysis

Authors: Michael Henry Tessler; Noah Goodman
Inference by conditioning a generative model is a basic building block of Bayesian statistics. In cognitive science this tool can be used in two ways. If
the generative model is a hypothesis about a person’s model of the world, then we have a Bayesian cognitive model – the main topic of this book.
If the generative model is instead the scientist’s model of how the data are generated, then we have Bayesian data analysis. Bayesian data
analysis  can  be  an extremely useful tool to  us as  scientists,  when  we  are trying to understand what  our data mean about  psychological
hypotheses. This can become confusing: a particular modeling assumption can be something we hypothesize that people assume about the world, or
can be something that we as scientists want to assume (but don’t assume that people assume). A pithy way of saying this is that we can make
assumptions about “Bayes in the head” (Bayesian cognitive models) or about “Bayes in the notebook” (Bayesian data analysis).

Bayesian data analysis (BDA) is a general-purpose approach to making sense of data. A BDA model is an explicit hypotheses about the generative
process behind the experimental data – where did the observed data come from? For instance, the hypothesis that data from two experimental
conditions came from two different distributions. After making explicit hypotheses, Bayesian inference can be used to invert the model: go from
experimental data to updated beliefs about the hypotheses.

Parameters and predictives

In a BDA model the random variables are usually called parameters. Parameters can be of theoretical interest, or not (the latter are called
nuisance parameters). Parameters are in general unobservable (or, “latent”), so we must infer them from observed data. We can also go from
updated beliefs about parameters to expectations about future data, so call posterior predictives.

For a given Bayesian model (together with data), there are four conceptually distinct distributions we often want to examine. For parameters,
we have priors and posteriors:

•The prior distribution over parameters captures our initial state of knowledge (or beliefs) about the values that the latent 
parameters could have, before seeing the data.

•The posterior distribution over parameters captures what we know about the latent parameters having updated our beliefs with the 
evidence provided by data.

From either the prior or the posterior over parameters we can then run the model forward, to get predictions about data sets:

•The prior predictive distribution tells us what data to expect, given our model and our initial beliefs about the parameters. The 
prior predictive is a distribution over data, and gives the relative probability of different observable outcomes before we have seen 
any data.

•The posterior predictive distribution tells us what data to expect, given the same model we started with, but with beliefs that have 
been updated by the observed data. The posterior predictive is a distribution over data, and gives the relative probability of 
different observable outcomes, after some data has been seen.

Loosely speaking, predictive distributions are in “data space” and parameter distributions are in “latent parameter space”.

Example: Election surveys

Imagine you want to find out how likely Candidate A is to win an election. To do this, you will try to estimate the proportion of eligible voters in
the United States who will vote for Candidate A in the election. Trying to determine directly how many (voting age, likely to vote) people prefer
Candidate A vs. Candidate B would require asking over 100 million people (it’s estimated that about 130 million people voted in the US Presidential
Elections in 2008 and 2012). It’s impractical to measure the whole distribution. Instead, pollsters measure a sample (maybe ask 1000 people), and
use that to draw conclusions about the “true population proportion” (an unobservable parameter).

Here, we explore the result of an experiment with 20 trials and binary outcomes (“will you vote for Candidate A or Candidate B?”).

// observed data
var k = 1 // number of people who support candidate A
var n = 20  // number of people asked

var model = function() {

   // true population proportion who support candidate A
   var p = uniform(0, 1);

   // Observed k people support "A"

   // Assuming each person's response is independent of each other
   observe(Binomial({p : p, n: n}), k);

   // predict what the next n will say
   var posteriorPredictive = binomial(p, n);

   // recreate model structure, without observe
   var prior p_  = uniform(0, 1);
   var priorPredictive = binomial(prior p_ , n);

   return {
       prior: prior p_ , priorPredictive : priorPredictive,
       posterior : p, posteriorPredictive : posteriorPredictive
    };
}

var posterior = Infer(model);

viz.marginals(posterior)
run▼

Notice that some plots densities and others bar graphs. This is because the data space for this experiment is counts, while the parameter space is
continuous. What can we conclude intuitively from examining these plots? First, because prior differs from posterior (both parameters and
predictives), the evidence has changed our beliefs. Second, the posterior predictive assigns quite high probability to the true data, suggesting
that the model considers the data “reasonable”. Finally, after observing the data, we conclude the true proportion of people supporting
Candidate A is quite low – around 0.09, or anyhow somewhere between 0.0 and 0.15. Check your understanding by trying other data sets, varying
both  k  and  n .

Quantifying claims about parameters

How can we quantify a claim like “the true parameter is low”? One possibility is to compute the mean or expected value of the parameter, which 
is mathematically given by ∫x⋅p(x)dx∫x⋅p(x)dx for a posterior distribution p(x)p(x). In WebPPL this can be computed via  expectation(d)  for a 
distribution  d . Thus in the above election example we could:

// observed data
var k = 1 // number of people who support candidate A
var n = 20  // number of people asked

var model = function() {

   // true population proportion who support candidate A
   var p = uniform(0, 1);

   // Observed k people support "A"
   // Assuming each person's response is independent of each other
   observe(Binomial({p : p, n: n}), k);

   // predict what the next n will say
   var posteriorPredictive = binomial(p, n);

   // recreate model structure, without observe
   var prior p_  = uniform(0, 1);
   var priorPredictive = binomial(prior p_ , n);

   return p
}

var posterior = Infer(model);

print("Expected proportion voting for A: " + expectation(posterior) )
run▼

This tells us that the mean is about 0.09. This can be a very useful way to summarize a posterior, but it eliminates crucial information about
how confident we are in this mean. A coherent way to summarize our confidence is by exploring the probability that the parameter lies within a

given interval. Conversely, an interval that the parameter lies in with high probability (say 90%) is called a credible interval (CI). Let’s explore
credible intervals for the parameter in the above model:

// observed data
var k = 1 // number of people who support candidate A
var n = 20  // number of people asked

var model = function() {

   // true population proportion who support candidate A
   var p = uniform(0, 1);

   // Observed k people support "A"
   // Assuming each person's response is independent of each other
   observe(Binomial({p : p, n: n}), k);

   // predict what the next n will say
   var posteriorPredictive = binomial(p, n);

   // recreate model structure, without observe
   var prior p_  = uniform(0, 1);
   var priorPredictive = binomial(prior p_ , n);

   return p
}

var posterior = Infer(model);

//compute the probability that p lies in the interval [0.01,0.18]
expectation(posterior,function(p){0.01<p && p<0.18})
run▼

Here we see that [0.01, 0.18] is an (approximately) 90% credible interval – we can be about 90% sure that the true parameter lies within this
interval. Notice that the 90%CI is not unique. There are different ways to choose a particular CI. One particularly common, and useful, one is
the Highest Density Interval (HDI), which is the smallest interval achieving a given confidence. (For unimodal distributions the HDI is unique and
includes the mean.)

Here is a quick way to approximate the HDI of a distribution:

var cred = function(d,low,up) {expectation(d,function(p){low<p && p<up})}

var findHDI = function(targetp,d,low,up,eps) {
  if (cred(d,low,up)<targetp) {return [low,up]}
  var y = cred(d,low+eps,up)
  var z = cred(d,low,up-eps)
  return y>z ? findHDI(targetp,d,low+eps,up,eps) : findHDI(targetp,d,low,up-eps,eps)
}

//simple test: a censored gaussian:
var d = Infer(function(){
  var x = gaussian(0,1)
  condition(x>0)
  return x
})

//find the 95% HDI for this distribution:
findHDI(0.95, d, -10, 10, 0.1)
run▼

Credible intervals are related to, but shouldn’t be mistaken for, the confidence intervals that are often used in frequentist statistics. (And
these confidence intervals themselves should definitely not be confused with p-values….)

Example: logistic regression

Now imagine you are a pollster who is interested in the effect of age on voting tendency. You run the same poll, but you are careful to recruit
people in their 20’s, 30’s, etc. We can analyze this data by assuming there is an underlying linear relationship between age and voting tendency.

However since our data are Boolean (or if we aggregate, the count of people in each age group who will vote for candidate A), we must use a
function to link from real numbers to the range [0,1]. The logistic function is a common way to do so.

var data = [{age: 20, n:20, k:1},
            {age: 30, n:20, k:5},
            {age: 40, n:20, k:17},
            {age: 50, n:20, k:18}]

var logistic = function(x) { 1 / (1+Math.exp(-x))}

var model = function() {

  // true effect of age
  var a = gaussian(0,1)
  //true intercept
  var b = gaussian(0,1)

  // observed data
  map(function(d){observe(Binomial({p:logistic(a*d.age + b), n: d.n}), d.k)}, data)

  return {a : a, b: b}
}

var opts = {method: "MCMC", samples: 50000}

var posterior = Infer(opts, model)

viz.marginals(posterior)
run▼

Looking at the parameter posteriors we see that there seems to be an effect of age: the parameter   a  is greater than zero, so older people are
more likely to vote for candidate A. But how well does the model really explain the data?

Posterior prediction and model checking

The posterior predictive distribution describes what data you should expect to see, given the model you’ve assumed and the data you’ve
collected so far. If the model is able to describe the data you’ve collected, then the model shouldn’t be surprised if you got the same data by
running the experiment again. That is, the most likely data for your model after observing your data should be the data you observed. It is
natural then to use the posterior predictive distribution to examine the descriptive adequacy of a model. If these predictions do not match the
data already seen (i.e., the data used to arrive at the posterior distribution over parameters), the model is descriptively inadequate.

A common way to check whether the posterior predictive matches the data, imaginatively called a posterior predictive check, is to plot some
statistics of your data vs the expectation of these statistics according to the predictive. Let’s do this for the number of votes in each age
group:

var data = [{age: 20, n:20, k:1},
            {age: 30, n:20, k:5},
            {age: 40, n:20, k:17},
            {age: 50, n:20, k:18}]
var ages = map(function(d){d.age},data)

var logistic = function(x) { 1 / (1+Math.exp(-x))}

var model = function() {

  // true effect of age
  var a = gaussian(0,1)
  //true intercept
  var b = gaussian(0,1)

  // observed data
  map(function(d){observe(Binomial({p:logistic(a*d.age + b), n: d.n}), d.k)}, data)

  // posterior prediction for each age in data, for n=20:
  var predictive = map(function(age){binomial({p:logistic(a*age + b), n: 20})}, ages)

  
  
  return predictive
}

var opts = {method: "MCMC", samples: 50000}

var posterior = Infer(opts, model)

var ppstats = mapIndexed( function(i,a){expectation(posterior, function(p){p[i]})}, ages)
var datastats = map(function(d){d.k}, data)

viz.scatter(ppstats,datastats)
run▼

This scatter plot will lie on the x=yx=y line if the model completely accomodates the data. Looking closely at this plot, we see that there are 
a few differences between the data and the predictives. First, the predictions are compressed compared to the data. This is likely because the 
prior encourages moderate probabilities. Second, we see hints of non-linearity in the data that the model is not able to account for. This model is 
pretty good, but certainly not perfect, for explaining this data.
One final note: There is an important, and subtle, difference between a model’s ability to accomodate some data and that model’s ability
to predict the data. This is roughly the difference between a posterior predictive check and a prior predictive check.

Model selection

In the above examples, we’ve had a single data-analysis model and used the experimental data to learn about the parameters of the models and
the descriptive adequacy of the models. Often as scientists, we are in fortunate position of having multiple, distinct models in hand, and want to
decide if one or another is a better description of the data. The problem of model selection given data is one of the most important and difficult
tasks of BDA.

Returning to the simple election polling example above, imagine we begin with a (rather unhelpful) data analysis model that assumes each
candidate is equally likely to win, that is  p=0.5 . We quickly notice, by looking at the posterior predictives, that this model doesn’t accommodate
the data well at all. We thus introduce the above model where   p = uniform(0,1) . How can we quantitatively decide which model is better? One
approach is to combine the models into an uber model that decides which approach to take:

// observed data
var k = 5 // number of people who support candidate A
var n = 20  // number of people asked

var model = function() {

  // binary decision variable for which hypothesis is better
  var x = flip(0.5) ? "simple" : "complex"
  var p = (x == "simple") ? 0.5 : uniform(0, 1)

  observe(Binomial({p : p, n: n}), k)

  return {model: x}
}

var modelPosterior = Infer(model);

viz(modelPosterior)
run▼

We see that, as expected, the more complex model is preferred: we can confidently say that given the data the more complex model is the one we
should believe. Further we can quantify this via the posterior probability of the complex model.

This model is an example from the classical hypothesis testing framework. We consider a model that fixes one of its parameters to a pre-specified
value of interest (here H0:p=0.5H0:p=0.5). This is sometimes referred to as a null hypothesis. The other model says that the parameter is 
free to vary. In the classical hypothesis testing framework, we would write: H1:p≠0.5H1:p≠0.5. With Bayesian hypothesis testing, we must be 
explicit about what pp is (not just what p is not), so we write H1:p∼Uniform(0,1)H1:p∼Uniform(0,1).
One might have a conceptual worry: Isn’t the second model just a more general case of the first model? That is, if the second model has a
uniform distribution over  p , then  p: 0.5  is included in the second model. Fortunately, the posterior on models automatically penalizes the more
complex model when it’s flexibility isn’t needed. (To verify this, set  k=10  in the example.) This idea is called the principle of parsimony or
Occam’s razor, and will be discussed at length later in this book. For now, it’s sufficient to know that more complex models will be penalized for
being more complex, intuitively because they will be diluting their predictions. At the same time, more complex models are more flexible and can

capture a wider variety of data (they are able to bet on more horses, which increases the chance that they will win some money). Bayesian model
comparison lets us weigh these costs and benefits.

Bayes’ factor

What we are plotting above are posterior model probabilities. These are a function of the marginal likelihoods of the data under each hypothesis
and the prior model probabilities (here, defined to be equal:  flip(0.5) ). Sometimes, scientists feel a bit strange about reporting values that are
based on prior model probabilities (what if scientists have different priors as to the relative plausibility of the hypotheses?) and so often
report the ratio of marginal likelihoods, a quantity known as a Bayes Factor.

Let’s compute the Bayes’ Factor, by computing the likelihood of the data under each hypothesis.

var k = 5, n = 20;

var simpleModel = Binomial({p: 0.5, n: n})

var complexModel = Infer(function(){
  var p = uniform(0, 1);
  return binomial(p, n)
})

var simpleLikelihood = Math.exp(simpleModel.score(k))
var complexLikelihood = Math.exp(complexModel.score(k))

var bayesFactor = simpleLikelihood / complexLikelihood
print( bayesFactor )
run▼

How does the Bayes Factor in this case relate to posterior model probabilities above? A short derivation shows that when the model priors are
equal, the Bayes Factor has a simple relation to the model posterior.

Savage-Dickey method
Sometimes the Bayes factor can be obtained by computing marginal likelihoods directly. (As in the example, where we marginalize over the model 
parameter using  Infer .) However, it is sometimes hard to get good estimates of the two marginal probabilities. In the case where one model is a 
special case of the other, called nested model comparison, there is another option. The Bayes factor can also be obtained by considering only the 
more complex hypothesis, by looking at the distribution over the parameter of interest (here, pp) at the point of interest (here, p=0.5p=0.5). 
Dividing the probability density of the posterior by the density of the prior (of the parameter at the point of interest) gives you the Bayes 
Factor! This, perhaps surprising, result was described by Dickey and Lientz (1970), and they attribute it to Leonard “Jimmie” Savage. The method 
is called the Savage-Dickey density ratio and is widely used in experimental science. We would use it like so:
var k = 5, n = 20;

var complexModelPrior = Infer(function(){
  var p = uniform(0, 1);
  return p
})

var complexModelPosterior = Infer(function(){
  var p = uniform(0, 1);
  observe(Binomial({p: p, n: n}), k);
  return p
})

var savageDickeyDenomenator = expectation(complexModelPrior, function(x){return 0.45<x & x<0.55})
var savageDickeyNumerator = expectation(complexModelPosterior, function(x){return 0.45<x & x<0.55})
var savageDickeyRatio = savageDickeyNumerator / savageDickeyDenomenator
print( savageDickeyRatio )
run▼

(Note that we have approximated the densities by looking at the expectation that pp is within 0.050.05 of the target value p=0.5p=0.5.)

BDA of cognitive models

In this chapter we have described how we can use generative models of data to do data analysis. In the rest of this book we are largely
interested in how we can build cognitive models by hypothesizing that people have generative models of the world that they use to reason and
learn. That is, we view people as intuitive Bayesian statisticians, doing in their heads what scientists do in their notebooks.

Of course when we, as scientists, try to test our cognitive models of people, we can do so using BDA! This leads to more complex models in which
we  have  an “outer” Bayesian  data analysis  model  and an  “inner” Bayesian  cognitive model. This is  exactly  the “nested  Infer” pattern
introduced for Social cognition. What is in the inner (cognitive) model captures our scientific hypotheses about what people know and how they
reason. What is in the outer (BDA) model represents aspects of our hypotheses about the data that we  are not attributing to people: linking
functions, unknown parameters of the cognitive model, and so on. This distinction can be subtle.

Test your knowledge: Exercises

Reading & Discussion: Readings
Next chapter: 7. Interlude - Algorithms for inference 

☰

Interlude - Algorithms for inference

Prologue: The performance characteristics of 
different algorithms

When we introduced conditioning we pointed out that the rejection sampling and enumeration (or mathematical) definitions are equivalent—we
could take either one as the definition of how  Infer  should behave with  condition  statements. There are many different ways to compute the
same distribution, it is thus useful to separately think about the distributions we are building (including conditional distributions) and how we
will compute them. Indeed, in the last few chapters we have explored the dynamics of inference without worrying about the details of
inference   algorithms.   The   efficiency   characteristics   of  different   implementations  of  Infer  can   be  very  different,  however,   and   this  is
important both practically and for motivating cognitive hypotheses at the level of algorithms (or psychological processes).

The “guess and check” method of rejection sampling (implemented in  method:"rejection" ) is conceptually useful but is often not efficient: even
if we are sure that our model can satisfy the condition, it will often take a very large number of samples to find computations that do so. To see
this, let us explore the impact of  baserate  in our simple warm-up example:

var baserate = 0.1

var infModel = function(){
  Infer({method: 'rejection', samples: 100}, function(){
    var A = flip(baserate)
    var B = flip(baserate)
    var C = flip(baserate)
    condition(A+B+C >= 2)
    return A})
}

//a timing utility: run 'foo' 'trials' times, report average time.
var time = function(foo, trials) {
  var start =  .now()
_
  var ret = repeat(trials, foo)
  var end =  .now()
_
  return (end-start)/trials
}

time(infModel, 10)
run▼

Even for this simple program, lowering the baserate by just one order of magnitude, to 0.010.01, will make rejection sampling impractical.
Another option that we’ve seen before is to enumerate all of the possible executions of the model, using the rules of probability to calculate
the conditional distribution:

///fold:
...

var baserate = 0.1

var infModel = function(){
  Infer({method: 'enumerate'}, function(){
    var A = flip(baserate)
    var B = flip(baserate)
    var C = flip(baserate)
    condition(A+B+C >= 2)
    return A})
}

time(infModel, 10)
run▼

Notice that the time it takes for this program to run doesn’t depend on the baserate. Unfortunately it does depend critically on the number of
random choices in an execution history: the number of possible histories that must be considered grows exponentially in the number of random
choices. To see this we modify the model to allow a flexible number of  flip  choices:

///fold:
...

var baserate = 0.1
var numFlips = 3

var infModel = function(){
  Infer({method: 'enumerate'}, function(){
    var choices = repeat(numFlips, function(){flip(baserate)})
    condition(sum(choices) >= 2)
    return choices[0]})
}

time(infModel, 10)
run▼

The dependence on size of the execution space renders enumeration impractical for many models. In addition, enumeration isn’t feasible at all
when   the   model   contains   a  continuous   distribution   (because   there   are  uncountably  many  value   that  would  need   to  be   enumerated).   Try
inserting  var x = gaussian(0,1)  in the above model.

There   are   many   other   algorithms   and   techniques   for   probabilistic   inference,   reviewed   below.   They   each   have   their   own   performance
characteristics. For instance, Markov chain Monte Carlo inference approximates the posterior distribution via a random walk (described in detail
below).

///fold:
...

var baserate = 0.1
var numFlips = 3

var infModel = function(){
  Infer({method: 'MCMC', lag: 100}, function(){
    var choices = repeat(numFlips, function(){flip(baserate)})
    condition(sum(choices) >= 2)
    return choices[0]})
}

time(infModel, 10)
run▼

See what happens in the above inference as you lower the baserate. Unlike rejection sampling, inference will not slow down appreciably (but
results will become less stable). Unlike enumeration, inference should also not slow down exponentially as the size of the state space is
increased. This is an example of the kind of trade offs that are common between different inference algorithms.

The varying performance characteristics of different algorithms for (approximate) inference mean that getting accurate results for complex
models can depend on choosing the right algorithm (with the right parameters). In what follows we aim to gain some intuition for how and when
algorithms work, without being exhaustive.

Markov chain Monte Carlo (MCMC)

We have already seen that samples from a (conditional) distribution can be an effective way to represent the results of inference – when
rejection sampling is feasible it is an excellent approach. Other methods have been developed to take approximate samples from a conditional
distribution. One popular method uses Markov chains.

Markov chains as samplers

A Markov model (or Markov chain, as it is often called in the context of inference algorithms) is a discrete dynamical system that unfolds over
iterations of the  transition  function. Here is a Markov chain:

var states = ['a', 'b', 'c', 'd'];
var transitionProbs = {a: [.48, .48, .02, .02],
                       b: [.48, .48, .02, .02],

                       c: [.02, .02, .48, .48],
                       d: [.02, .02, .48, .48]}

var transition = function(state){
  return categorical({vs: states, ps: transitionProbs[state]})
}

var chain = function(state, n){
  return (n == 0 ? state : chain(transition(state), n-1))
}

print("State after 10 steps:")
viz.hist(repeat(1000,function() {chain('a',10)}))
viz.hist(repeat(1000,function() {chain('c',10)}))

print("State after 25 steps:")
viz.hist(repeat(1000,function() {chain('a',25)}))
viz.hist(repeat(1000,function() {chain('c',25)}))

print("State after 50 steps:")
viz.hist(repeat(1000,function() {chain('a',50)}))
viz.hist(repeat(1000,function() {chain('c',50)}))
run▼

Notice that the distribution of states after only a few steps is highly influenced by the starting state. In the long run the distribution looks 
the same from any starting state: this long-run distribution is the called the stable distribution (also known as stationary distribution). To 
define stationary distribution formally, let p(x)p(x) be the target distribution, and let π(x→x′)π(x→x′) be the transition distribution (i.e. 
the  transition  function in the above program). Since the stationary distribution is characterized by not changing when the transition is applied we 
have a balance condition: p(x′)=∑xp(x)π(x→x′)p(x′)=∑xp(x)π(x→x′). Note that the balance condition holds for the distribution as a whole—a 
single state can of course be moved by the transition.
For the chain above, the stable distribution is uniform—we have found a (fairly baroque!) way to sample from the uniform distribution on   ['a',
'b', 'c', 'd'] ! We could have sampled from the uniform distribution using other Markov chains. For instance the following chain is more natural,
since it transitions uniformly:

var states = ['a', 'b', 'c', 'd'];
var transition = function(state){
    return categorical({vs: states, ps: [.25, .25, .25, .25]})
    }

var chain = function(state, n){
    return (n == 0 ? state : chain(transition(state), n-1))
}

print("State after 10 steps:")
viz.hist(repeat(1000,function() {chain('a',10)}))
viz.hist(repeat(1000,function() {chain('c',10)}))

print("State after 25 steps:")
viz.hist(repeat(1000,function() {chain('a',25)}))
viz.hist(repeat(1000,function() {chain('c',25)}))

print("State after 50 steps:")
viz.hist(repeat(1000,function() {chain('a',50)}))
viz.hist(repeat(1000,function() {chain('c',50)}))
run▼

Notice that this chain converges much more quickly to the uniform distribution. (Edit the code to confirm to yourself that the chain converges to
the stationary distribution after a single step.) The number of steps it takes for the distribution on states to reach the stable distribution (and
hence lose traces of the starting state) is called the burn-in time. Thus, while we can use a Markov chain as a way to (approximately) sample
from its stable distribution, the efficiency depends on burn-in time. While many Markov chains have the same stable distribution they can have
very different burn-in times, and hence different efficiency.

The state space in our examples above involved a small number of states, but Markov chains can also be constructed over infinite state spaces.
Here’s a chain over the integers:

var p = 0.7

var transition = function(state){
    return (state == 3 ? sample(Categorical({vs: [3, 4], ps: [(1 - 0.5 * (1 - p)), (0.5 * (1 - p))]})) :
                            sample(Categorical({vs: [(state - 1), state, (state + 1)], ps: [0.5, (0.5 - 0.5 * (1 - p)), (0.5 * (1 - p))]})))
}

var chain = function(state, n){
    return (n == 0 ? state : chain(transition(state), n-1))
}

var samples = repeat(5000, function() {chain(3, 250)})
viz.table(samples)
run▼

As we can see, this Markov chain has as its stationary distribution a geometric distribution conditioned to be greater than 2. The Markov chain
above implements the inference below, in the sense that it specifies a way to sample from the required conditional distribution.

var p = .7

var geometric = function(p){
    return ((flip(p) == true) ? 1 : (1 + geometric(p)))
}

var post = Infer({method: 'MCMC', samples: 25000, lag: 10, model: function(){
    var mygeom = geometric(p);
    condition(mygeom>2)
    return(mygeom)
    }
})

viz.table(post)
run▼

Markov chain Monte Carlo (MCMC) is an approximate inference method based on identifying a Markov chain whose stationary distribution matches the
conditional distribution you’d like to estimate. If such a transition distribution can be identified, we simply run it forward to generate samples
from the target distribution.

Metropolis-Hastings

Fortunately, it turns out that for any given (conditional) distribution there are Markov chains with a matching stationary distribution. There are
a number of methods for finding an appropriate Markov chain. One particularly common method is Metropolis Hastings recipe.

To create the necessary transition function, we first create a proposal distribution, q(x→x′)q(x→x′), which does not need to have the target 
distribution as its stationary distribution, but should be easy to sample from (otherwise it will be unwieldy to use!). A common option for 
continuous state spaces is to sample a new state from a multivariate Gaussian centered on the current state. To turn a proposal distribution into 
a transition function with the right stationary distribution, we either accepting or reject the proposed transition with probability: min(1,p(x′)q(x
′→x)p(x)q(x→x′)).min(1,p(x′)q(x′→x)p(x)q(x→x′)). That is, we flip a coin with that probability: if it comes up heads our next state is $x’$, 
otherwise our next state is still xx.
Such a transition function not only satisfies the balance condition, it actually satisfies a stronger condition, detailed balance. 
Specifically, p(x)π(x→x′)=p(x′)π(x′→x)p(x)π(x→x′)=p(x′)π(x′→x). (To show that detailed balance implies balance, substitute the right-hand 
side of the detailed balance equation into the balance equation, replacing the summand, and then simplify.) It can be shown that the Metropolis-
hastings algorithm gives a transition probability (i.e. π(x→x′)π(x→x′)) that satisfies detailed balance and thus balance. (Recommended exercise: 
prove this fact. Hint: the probability of transitioning depends on first proposing a given new state, then accepting it; if you don’t accept the 
proposal you “transition” to the original state.)
Note that in order to use this recipe we need to have a function that computes the target probability (not just one that samples from it) and the
transition probability, but they need not be normalized (since the normalization terms will cancel).

We can use this recipe to construct a Markov chain for the conditioned geometric distribution, as above, by using a proposal distribution that is
equally likely to propose one number higher or lower:

var p = 0.7

//the target distribution (not normalized):
//prob = 0 if x condition is violated, otherwise proportional to geometric distribution
var target dist
 = function(x){
  return (x < 3 ? 0 : (p * Math.pow((1-p),(x-1))))

_

}

// the proposal function and distribution,
// here we're equally likely to propose x+1 or x-1.
var proposal fn_
 = function(x){
  return (flip() ? x - 1 : x + 1)
}
var proposal dist
_
  return 0.5
}

 = function (x1, x2){

x2) * proposal dist(

_

// the MH recipe:
var accept = function (x1, x2){
  var p = Math.min(1, (target dist(
_
  return flip(p)
}
var transition = function(x){
  var proposed x_  = proposal fn(
x)
_
  return (accept(x, proposed x_ ) ? proposed x_  : x)
}

x2, x1)) / (target dist(

_

x1) * proposal dist(

_

x1,x2)))

//the MCMC loop:
var mcmc = function(state, iterations){
  return ((iterations == 1) ? [state] : mcmc(transition(state), iterations-1).concat(state))
}

var chain = mcmc(3, 10000) // mcmc for conditioned geometric
viz.table(chain)
run▼

Note that the transition function that is automatically derived using the MH recipe is actually the same as the one we wrote by hand earlier:

var transition = function(state){
    return (state == 3 ? sample(Categorical({vs: [3, 4], ps: [(1 - 0.5 * (1 - p)), (0.5 * (1 - p))]})) :
                            sample(Categorical({vs: [(state - 1), state, (state + 1)], ps: [0.5, (0.5 - 0.5 * (1 - p)), (0.5 * (1 - p))]})))
}
run▼

Hamiltonian Monte Carlo

WebPPL’s  method:'MCMC'  uses Metropolis-Hastings by default. However, it is not the only option, nor is it always the best. When the input to
a  factor  statement   is   a   function   of   multiple   variables,   those   variables   become   correlated   in   the   posterior   distribution.   If   the   induced
correlation is particularly strong, MCMC can sometimes become ‘stuck.’ In controlling the random walk, Metropolis-Hastings choses a new point in
probability space to go to and then decides whether or not to go based on the probability of the new point. If it has difficulty finding new points
with reasonable probability, it will get stuck and simply stay where it is. Given an infinite amount of time, Metropolis-Hastings will recover.
However, the first N samples will be heavily dependent on where the chain started (the first sample) and will be a poor approximation of the
true posterior.

Take this example below, where we use a Gaussian likelihood to encourage ten uniform random numbers to sum to the value 5:

var constrainedSumModel = function() {
  var xs = repeat(10, function() {uniform(0, 1)})
  var targetSum = 5.0
  observe(Gaussian({mu: targetSum, sigma: 0.005}), sum(xs))
  return xs
};

var opts = {method: 'MCMC',
            samples: 5000,
            callbacks: [MCMC Callbacks.finalAccept] }
var post = Infer(opts, constrainedSumModel)

_

print(sample(post))
print(sample(post))
print(sample(post))
print(sample(post))

print(sample(post))
print(sample(post))
run▼

The output box displays 10 random samples from the posterior. You’ll notice that they are all very similiar, despite there being many distinct
ways for ten real numbers to sum to 5. The reason is technical but straight-forward. The default version of MCMC used by WebPPL is Metropolis
Hastings (MH). As described above, MH proposes new states and then uses the MH acceptance rule to decide whether to move there. The program
above uses the  callbacks  option to  MCMC  to display the final acceptance ratio (i.e. the percentage of proposed samples that were accepted)–we
see it is around 1-2%. This means there are few “new” states accepted by the Markov chain.

To deal with situations like this one, WebPPL provides an implementation of Hamiltonian Monte Carlo (HMC). HMC automatically computes the
gradient of the target distribution with respect to the random choices made by the program. It uses this gradient information to make coordinated
proposals to all the random choices. This can yield much better proposals for MH. Below, we apply HMC to  constrainedSumModel :

var constrainedSumModel = function() {
  var xs = repeat(10, function() {uniform(0, 1)})
  var targetSum = 5.0
  observe(Gaussian({mu: targetSum, sigma: 0.005}), sum(xs))
  return xs
};

var opts = {method: 'MCMC',
            samples: 100,
            callbacks: [MCMC Callbacks.finalAccept],
            kernel: {HMC : { steps: 50, stepSize: 0.0025 }} }
var post = Infer(opts, constrainedSumModel)

_

print(sample(post))
print(sample(post))
print(sample(post))
print(sample(post))
print(sample(post))
print(sample(post))
run▼

The approximate posterior samples produced by this program are more varied, and the final acceptance rate is much higher!

There are a couple of caveats to keep in mind when using HMC:

•Its parameters can be extremely sensitive. Try increasing the  stepSize  option to  0.004  and see how the output samples degenerate.

•It is only applicable to continuous random choices, due to its gradient-based nature. (You can use WebPPl’s HMC with models that 
include discrete choices: under the hood, this will alternate between HMC for the continuous choices and MH for the discrete 
choices.)

Some technicalities and practicalities

•The successive samples generated by a single MCMC chain can be very similar, technically speaking they have high autocorrelation. A 
way of limiting the effects of this autocorrelation is to simple thin out the samples. This corresponds to the  lag  parameter to 
the  MCMC  inference method.

•In MH autocorrelation often comes from too many rejected proposals. To monitor this we can keep an eye on the acceptance rate. When 
it is too low (rule of thumb: below 0.30.3 for MH, below 0.90.9 for HMC) we will need to adjust the parameters of the algorithm or 
switch to a different method.
•For MH, and related techniques, the very first state of a Markov chain needs to be possible (have non-zero probability according to 
the target model), otherwise transition probabilities are not well defined. This can be surprisingly hard, especially when the 
hard  condition  operator is used. Switching to  observe , where possible, solves this problem (and also improves efficiency during 
sampling).

•In order for MCMC to converge to a stationary distribution that accurately reflects the target model, it must be possible to reach any
(possible) state from any other eventually. This condition is called ergodicity. It is mathematically important, but not something 
practitioners usually need to worry about. (It can generally be guaranteed by randomly restarting the chain occasionally, though this 
impacts efficiency.)

Particle Filters

A particle filter – also known as Sequential Monte Carlo – maintains a collection of samples (aka particles) simultaneously in parallel while
executing the model. (This is different than MCMC, where samples are complete executions, each constructed sequentially from the last.) The
particles are “re-sampled” upon encountering new evidence, in order to adjust the numbers so that the population will be approximately
distributed according to the model. SMC is particularly useful for models where beliefs can be incrementally updated as new observations come
in.

Let’s consider another simple model, where five real numbers are constrained to be close to their neighbors:

var pairwiseSameModel = function() {
  var a = uniform(0, 1)
  var b = uniform(0, 1)
  var c = uniform(0, 1)
  var d = uniform(0, 1)
  var e = uniform(0, 1)
  observe(Gaussian({mu: 0, sigma: 0.005}), a-b)
  observe(Gaussian({mu: 0, sigma: 0.005}), b-c)
  observe(Gaussian({mu: 0, sigma: 0.005}), c-d)
  observe(Gaussian({mu: 0, sigma: 0.005}), d-e)
  return [a,b,c,d,e]
};

var opts = {method: 'MCMC',
            samples: 100,
            callbacks: [MCMC Callbacks.finalAccept]}
var post = Infer(opts, pairwiseSameModel)

_

print(sample(post))
print(sample(post))
print(sample(post))
print(sample(post))
print(sample(post))
print(sample(post))

viz(marginalize(post,function(x){x[0]}))
run▼

We can easily tell that the marginal distribution on the first number should be approximately uniform, yet the sampled values are nowhere
close to that. This is another case where the MH procedure finds it difficult to accept new states. Intuitively, rather than choosing the five
numbers at once, we could choose one number then use the pairwise similarity constraint to choose the next number close to the first, and so on.
Particle filtering will be a (mathematically correct) version of this idea, but only if we rewrite the model to interleave the random choices with
the observations:

var pairwiseSameModel = function() {
  var a = uniform(0, 1)
  var b = uniform(0, 1)
  observe(Gaussian({mu: 0, sigma: 0.005}), a-b)
  var c = uniform(0, 1)
  observe(Gaussian({mu: 0, sigma: 0.005}), b-c)
  var d = uniform(0, 1)
  observe(Gaussian({mu: 0, sigma: 0.005}), c-d)
  var e = uniform(0, 1)
  observe(Gaussian({mu: 0, sigma: 0.005}), d-e)
  return [a,b,c,d,e]
};

var opts = {method: 'SMC',
            particles: 1000}
var post = Infer(opts, pairwiseSameModel)

print(sample(post))
print(sample(post))
print(sample(post))
print(sample(post))

print(sample(post))
print(sample(post))

viz(marginalize(post,function(x){x[0]}))
run▼

Try switching back to the version of the model that does not interleave observations with sampling. Do the samples look worse?

When a particle filter encounters new evidence, it updates its collection of particles. Those particles that predict the new data well are likely
to be retained or even multiplied. Those particles that do not predict the new data well are likely to be eliminated. Thus, particle filters
integrate new data with prior beliefs. However, if few particles capture the new observation well, the particle filter may be forced to
use only these few – this dynamic, called “filter collapse”, results in poor samples. Try reducing the number of particles in the above model.

Consider a more complex example, motivated by radar detection, where the aim is to infer the trajectory of a moving object–the program receives
a sequence of noisy observations and must infer the underlying sequence of true object locations.

The code below generates observations from a randomly-sampled underlying trajectory. Our program assumes that the object’s motion is governed
by a momentum term which is a function of its previous two locations; this tends to produce smoother trajectories.

///fold: helper functions for drawing
...

var genObservation = function(pos){
  return map(
    function(x){ return gaussian(x, 15); },
    pos
  );
};

var init = function(){
    var state1 = [gaussian(300, 1), gaussian(300, 1)];
    var state2 = [gaussian(300, 1), gaussian(300, 1)];
    var states = [state1, state2];
    var observations = map(genObservation, states);
    return {
        states: states,
        observations: observations
    };
};

var transition = function(lastPos, secondLastPos){
  return map2(
    function(lastX, secondLastX){
      var momentum = (lastX - secondLastX) * .7;
      return gaussian(lastX + momentum, 3);
    },
    lastPos,
    secondLastPos
  );
};

var trajectory = function(n) {
  var prevData = (n == 2) ? init() : trajectory(n - 1);
  var prevStates = prevData.states;
  var prevObservations = prevData.observations;
  var newState = transition(last(prevStates), secondLast(prevStates));
  var newObservation = genObservation(newState);
  return {
    states: prevStates.concat([newState]),
    observations: prevObservations.concat([newObservation])
  }
};

var numSteps = 80;
var atrajectory = trajectory(numSteps)

var synthObservations = atrajectory.observations;
var trueLocs = atrajectory.states;
var canvas = Draw(400, 400, true)
drawPoints(canvas, synthObservations, "grey") // observations
drawPoints(canvas, trueLocs, "blue") // actual trajectory
run▼

The actual trajectory is displayed in blue. The observations are in grey.

We can then use  'SMC'  inference to estimate the underlying trajectory which generated a synthetic observation sequence:

///fold:
...

var observeSeq = function(pos, trueObs){
  return map2(
    function(x, trueObs) {
        return observe(Gaussian({mu: x, sigma: 5}), trueObs);
    },    
    pos,
    trueObs
  );
};

var initWithObs = function(trueObs){
    var state1 = [gaussian(250, 1), gaussian(250, 1)];
    var state2 = [gaussian(250, 1), gaussian(250, 1)];
    var obs1 = observeSeq(state1, trueObs[0]);
    var obs2 = observeSeq(state2, trueObs[1]);
    return {
        states: [state1, state2],
        observations: [obs1, obs2]
    }
};

var trajectoryWithObs = function(n, trueObservations) {
  var prevData = (n == 2) ?
    initWithObs(trueObservations.slice(0, 2)) :
    trajectoryWithObs(n-1, trueObservations.slice(0, n-1));
  var prevStates = prevData.states;
  var prevObservations = prevData.observations;
  var newState = transition(last(prevStates), secondLast(prevStates));
  var newObservation = observeSeq(newState, trueObservations[n-1]);
  return {
    states: prevStates.concat([newState]),
    observations: prevObservations.concat([newObservation])
  }
};

var numSteps = 80;
var numParticles = 10;

// Gen synthetic observations
var atrajectory = trajectory(numSteps)
var synthObservations = atrajectory.observations;
var trueLocs = atrajectory.states;

// Infer underlying trajectory using particle filter
var posterior = Infer({method: 'SMC', particles: numParticles}, function() {
  return trajectoryWithObs(numSteps, synthObservations);
});
var inferredTrajectory = sample(posterior).states;

// Draw model output

var canvas = Draw(400, 400, true)
drawPoints(canvas, synthObservations, "grey") // observations
drawLines(canvas, inferredTrajectory[0], inferredTrajectory.slice(1), "blue") // inferred
drawLines(canvas, trueLocs[0], trueLocs.slice(1), "green") // true
run▼

Again, the actual trajectory is in green, the observations are in grey, and the inferred trajectory is in blue. Try increasing or decreasing the
number of particles to see how this affects inference. Also try using MH or HMC for inference.

Variational Inference

The previous parts of this chapter focused on Monte Carlo methods for approximate inference: algorithms that generate a (large) collection of
samples to represent a conditional distribution. Another way to represent a distribution is by finding the closest approximation among a set (or
“family”) of simpler distributions. This is the approach taken by variational inference. At a high level, we declare a set of models that have
the same choices as our target model, but don’t have any conditions (i.e. no  condition ,  observe , or  factor ); we then try to find the member of
this set closest to our target model and use it as the result of  Infer .

To search for a good approximating model, we will eventually use gradient-based techniques. For this reason, we don’t want a set of isolated
models,   but   a   continuous   family.   In   WebPPL   we   declare   parameters   of   a   family   with   param() .   For   instance,   here   is   a   family   of   Gaussian
distributions with fixed variance but different means:

Gaussian({mu: param(), sigma: 0.1})
run▼

Because we want to make sure the family of distributions has the same choices as the target model, we define the two together. This is done
with  guide  annotations to  sample  statements:

var gaussianModel = function() {
  var mu = sample(Gaussian({mu:0, sigma:20}),
      {guide: function(){Gaussian({mu:param(), sigma:param()})}})
  var sigma = Math.exp(sample(Gaussian({mu:0, sigma:1}),
      {guide: function(){Gaussian({mu:param(), sigma:param()})}})) 
  map(function(d) {observe(Gaussian({mu: mu, sigma: sigma}), d)}, 
    data)
  return {mu: mu, sigma: sigma}
}
run▼

This represents both the conditional model, observed data drawn from a Gaussian of unknown mean and variance, and the (unconditional) family:
Gaussian of adjustable mean and variance. If we were to separate them out they would look like this:

//target model:
var gaussianModel = function() {
  var mu = sample(Gaussian({mu:0, sigma:20}))
  var sigma = Math.exp(sample(Gaussian({mu:0, sigma:1}))) 
  map(function(d) {observe(Gaussian({mu: mu, sigma: sigma}), d)}, 
    data)
  return {mu: mu, sigma: sigma}
}

//variational family:
var guideModelFamily = function() {
  var mu = sample(Gaussian({mu:param(), sigma:param()}))
  var sigma = Math.exp(sample(Gaussian({mu:param(), sigma:param()}))) 
  //Note no observes!
  return {mu: mu, sigma: sigma}
}
run▼

Once we have specified the target model and the family we’ll use to approximate it, our goal is to find the best member of the family – that is
the one closest to the target model. Formally we want the member of the family with smallest Kullback-Liebler distance to the target model.
WebPPL has built-in algorithms for minimizing this distance via gradient descent. This is called variational inference.

Here we use WebPPL inference method  optimize  to do variational inference in the above model:

var trueMu = 3.5
var trueSigma = 0.8

var data = repeat(100, function() { return gaussian(trueMu, trueSigma)})

var gaussianModel = function() {
  var mu = sample(Gaussian({mu:0, sigma:20}),
      {guide: function(){Gaussian({mu:param(), sigma:Math.exp(param())})}})
  var sigma = Math.exp(sample(Gaussian({mu:0, sigma:1}),
      {guide: function(){Gaussian({mu:param(), sigma:Math.exp(param())})}})) 
  map(function(d) {observe(Gaussian({mu: mu, sigma: sigma}), d)}, 
    data)
  return {mu: mu, sigma: sigma}
}

var post = Infer({
  method: 'optimize',
  optMethod: {adam: {stepSize: .25}},
  steps: 250,
  samples: 1000}, 
  gaussianModel)

viz.marginals(post)
run▼

Run this code, then try using MCMC to achieve the same result. You’ll notice that MCMC takes significantly more steps/samples to give good
results.

It is worth knowing that if no  guide  family is provided, WebPPL will fill one in by default:

var trueMu = 3.5
var trueSigma = 0.8

var data = repeat(100, function() { return gaussian(trueMu, trueSigma)})

var gaussianModel = function() {
  var mu = gaussian(0, 20)
  var sigma = Math.exp(gaussian(0, 1))
  map(function(d) {
    observe(Gaussian({mu: mu, sigma: sigma}), d)
  }, data)
  return {mu: mu, sigma: sigma}
}

var post = Infer({
  method: 'optimize',
  optMethod: {adam: {stepSize: .25}},
  steps: 250,
  samples: 1000}, 
  gaussianModel)

viz.marginals(post)
run▼

The default guide family is constructed by replacing the arguments of random choices in the program with free parameters, which it then
optimizes. This approach is known as mean-field variational inference: approximating the posterior with a product of independent distributions
(one for each random choice in the program). Though it can be very useful, the mean-field approximation necessarily fails to capture correlation
between variables. To see this, return to the model we used to explain the checkershaddow illusion:

var observedLuminance = 3;

var model = function() {
  var reflectance = gaussian({mu: 1, sigma: 1})
  var illumination = gaussian({mu: 3, sigma: 1})
  var luminance = reflectance * illumination
  observe(Gaussian({mu: luminance, sigma: 1}), observedLuminance)
  return {reflectance: reflectance, illumination: illumination}

                            
}

var post = Infer({
  // First use MCMC (with a lot of samples) to see what the posterior should look like
  method: 'MCMC',
  samples: 15000,
  lag: 100
  //then try optimization (VI):
//     method: 'optimize',
//     optMethod: {adam: {stepSize: .25}},
//     steps: 250,
//     samples: 5000
}, model)

viz.heatMap(post)
run▼

Try the above model with both ‘optimize’ and ‘MCMC’, do you see how ‘optimize’ fails to capture the correlation? Think about why this is!

There are other methods for variational inference in addition to mean-field. We can instead approximate the posterior with a more complex
family of distributions; for instance one that directly captures the (potential) correlation in the above example. To do so in WebPPL we need to
explicitly describe the approximating (guide) family. First let’s look at the above mean field approach, written with explicit guides:

var observedLuminance = 3;

var model = function() {
  var reflectance = sample(Gaussian({mu: 1, sigma: 1}),
                           {guide: function(){Gaussian({mu: param(), sigma:Math.exp(param())})}})
  var illumination = sample(Gaussian({mu: 3, sigma: 1}),
                            {guide: function(){Gaussian({mu: param(), sigma:Math.exp(param())})}})
  var luminance = reflectance * illumination
  observe(Gaussian({mu: luminance, sigma: 1}), observedLuminance)
  return {reflectance: reflectance, illumination: illumination}
}

var post = Infer({
  method: 'optimize',
  optMethod: {adam: {stepSize: .01}},
  steps: 10000,
  samples: 1000
}, model)

viz.heatMap(post)
run▼

Now, we can alter the code in the guide functions to make the  illumination  posterior depend on the  reflectance :

var observedLuminance = 3;

var model = function() {
  var reflectance = sample(Gaussian({mu: 1, sigma: 1}),
                           {guide: function(){Gaussian({mu: param(), sigma:Math.exp(param())})}})
  var illumination = sample(Gaussian({mu: 3, sigma: 1}),
                            {guide: function(){Gaussian({mu: param()+reflectance*param(), sigma:Math.exp(param())})}})
  var luminance = reflectance * illumination
  observe(Gaussian({mu: luminance, sigma: 1}), observedLuminance)
  return {reflectance: reflectance, illumination: illumination}
}

var post = Infer({
  method: 'optimize',
  optMethod: {adam: {stepSize: .01}},
  steps: 10000,
  samples: 1000
}, model)

                            
                            
viz.heatMap(post)
run▼

Here we have explicitly described a linear dependence of the mean of  illumination  on  reflectance . Can you think of ways to adjust the guide
functions to even better capture the true posterior?

By definition a parametric function can be described by some finite number of parameters. For instance, a Gaussian is fully described by two
numbers: its mean and standard deviation. By approximating a complex posterior distribution within a parametric family, we can often achieve
reasonable results much more quickly than Monte Carlo methods. Unlike Monte Carlo methods, however, if the true posterior is badly fit by the
family we will never get good results!

Some technicalities and practicalities

•You might wonder why the results of variational inference look “bumpy” when the approximating family is, for example, nice smooth 
Gaussians? This is because optimization finds the best approximating guide model, but the marginal distribution on return values of this 
best guide must still be estimated. In WebPPL this final step is done by forward sampling from the guide model, hence the final result 
is still samples.

•Performance of variational inference can depend a lot on the parameters of the optimizer, especially the step size.

•Even though guide models must not observe data (or factor/condition), they can actually depend on the data that is observed in the 
target model. This is sometimes called amortized inference.

Heuristics for choosing an algorithm

Given this zoo of different algorithms for doing inference, which should you choose? Here is a simple checklist that can be useful:

•Can you use  enumerate ? If so this is the best choice. It won’t work if you have continuous choices or especially huge discrete state 
spaces.

•Can you do rejection sampling? Try taking one sample and see how long it takes. If it’s reasonable, this is the next best option.

•Do you want inference to be fast and can tolerate bias? If so, try variational inference. Start with mean field and then make fancier 
guide families as you understand your model better.

•If you notice that your model has observations interleaved with sampling (or can be written that way), then give SMC a shot. Keep an 
eye out for filter collapse.

•MCMC tends to be a good fall back (if you run it long enough, it’ll do well…). HMC tends to be better when your model has continuous 
variables. Tune step size carefully by looking at acceptance rate.

In fact, this decision heuristic is pretty much what WebPPL  Infer  does when no method is specified!

This is unfortunately not an exhaustive procedure, and some of the steps can require intuition. (E.g. how do you know if your variational
algorithm is working?) There is a great deal written about diagnostics and rules of thumb for each inference algorithm!

Test your knowledge: Exercises

Reading & Discussion: Readings
Next chapter: 8. Rational process models 

☰

Rational process models

As we noted in an earlier chapter, there is an interesting parallel between the  Infer  abstraction, which separates model specification from
inference method, and the idea of levels of analysis in cognitive science David Marr (1982). For most of this book we are interested in
the computational level of describing what people know about the world and what inferences that knowledge licenses. That is, we treat the
model argument to  infer  as the scientific hypothesis, and the options (including ‘method’) argument as an engineering detail needed to derive
predictions. We can make a great deal of progress with this abstraction.

The algorithmic level goes further, attempting to describe the process by which people draw these inferences – taking the options to   Infer  as
part of the  hypotheses.  While  Infer  specifies  an ideal, different  methods  for inference  will approximate  this ideal  better  or worse  in
different cases; they will also do so with different time, space, and energy trade-offs. Is it reasonable to interpret the inference algorithms
that we borrow from statistics as psychological hypotheses at the algorithmic level? Which algorithm does the brain use for inference? Could it
be MCMC? Enumeration?

If   we   take   the   algorithms   for   inference   as   psychological   hypotheses,   then   the   approximation   and   resource-usage   characteristics   of   the
algorithms will be the signature phenomena of interest.

How is uncertainty represented?

A signature of probabilistic (“Bayesian”) cognitive models is the central role of uncertainty. Generative models, our main notion of knowledge,
capture uncertain causal processes. After making observations or assumptions, Infer captures uncertain answers. At the computational level we
work with this uncertainty by manipulating distribution objects, without needing to explore (much) how they are created or represented. Yet
cognitively there is a key algorithmic question: how is uncertainty represented in the human mind?

We have at least three very different possible answers to this question:

•Explicit representation of probabilities.

•Parametric representation of distribution families.

•Sampling-based representations.
Explicit representations

The most straightforward interpretation of Bayesian models at the algorithmic level is that explicit probabilities for different states are
computed and represented. (This is very much like the ‘enumerate’ method of Infer.) Attempts have been made to model how neural systems
might capture these representations, via ideas such as probabilistic population codes. (See Bayesian inference with probabilistic population codes,
Ma, Beck, Latham, Pouget (2006).)

Yet it is difficult to see how these explicit representations can scale up to real-world cognition.

Approximate distribution representations

Another possible representation of uncertainty is via the parameters of a family of distributions. For instance, the mean and covariance of a
Gaussian is a flexible and popular (in statistics) way to approximate a complex distirbution. (Indeed, we have seen that a mean-field product of
Gaussians can give quick and useful inference result from variational inference.) It is thus possible that all uncertainty is represented in the
human mind as parameters of some family. A version of this idea can be seen in the free energy hypothesis. (See The free-energy principle: a
unified brain theory?, Friston (2010).)

The sampling hypothesis

Finally, it is possible that there is no explicit representation of uncertainty in the human mind. Instead uncertainty is implicitly represented in
the tendencies of a dynamical system. This is the sampling hypothesis: that the human mind has the ability to generate samples from conditional
distributions when needed. Thus the mind implicitly represents an entire distribution, but can only work explicitly with a few samples from it.

We have seen a number of methods for creating dynamical systems that can sample from any desired distribution (rejection sampling, various
Markov chain Monte Carlo methods, etc). This type of representation is thus possible in principle. What behavioral or neural signatures would we
expect if it were correct? And which of the many sampling methods might be neurally plausible?

As a first analysis we can assume that the human mind is always capable of drawing perfect samples from the conditional distributions of interest
(e.g. via rejection sampling) but doing so is costly in terms of time or energy. If we assume that only a few samples are going to be used by any
individual in answering any question, a profound behavioral prediction arises: individuals’ choice probability will match the posterior probability
distribution. Note that this is a somewhat radical departure from a “fully Bayesian” agent. If we assume a choice is to be made, with 100$ reward
for the correct answer and no reward for incorrect answers, a rational (that is utility-maximizing) agent that explicitly represents the answer
distribution should always choose the most likely answer. Across a population of such agents we would see no variation in answers (assuming a

priori identical beliefs). In contrast, an agent that has only a single sample from their answer distribution will choose this answer; across a
population of such agents we would see answers distributed according to the answer distribution!

Let’s see this in practice in a simple example, here each person sees a coin of unknown probability flipped five times, coming up heads four of
them. Each person is asked to bet on the next outcome:

var agentBelief = Infer({method:'rejection', samples: 1000},function(){
    var weight = uniform(0,1)
    observe(Binomial({p:weight, n:5}), 4)
    return flip(weight)
    })

var maxAgent = function(){return agentBelief.score(true)>agentBelief.score(false)}
var sampleAgent = function(){sample(agentBelief)}

print("max agents decision distribution:")
viz(repeat(100,maxAgent))
print("sample agents decision distribution:")
viz(repeat(100,sampleAgent))
run▼

The maximizing agent chooses the most likely outcome by examining the conditional probability they assign to outcomes – the result is all such
agents choosing ‘true’. In contrast, a population of agents that each represents their belief with a single sample will choose ‘false’ about 30%
of the time. This behavioral signature – probability matching – is in fact a very old and well studied psychological phenomenon. (See for instance,
Individual Choice Behavior: A Theoretical Analysis, Luce (1959).)

Vul, Goodman, Griffiths, Tenenbaum (2014) further ask how many samples a rational agent should use, if they are costly. This analysis explores
the trade off between expected reward increase from more precise probability estimates (more samples) with resource savings from less work
(fewer samples). The, somewhat surprising, result is that for a wide range of cost and reward assumptions it is optimal to decide based on only
one, or a few, samples.

Test your knowledge: Exercises

Reading & Discussion: Readings
Next chapter: 9. Learning as conditional inference 

☰

Learning as conditional inference

The line between “reasoning” and “learning” is unclear in cognition. Just as reasoning can be seen as a form of conditional inference, so can
learning: discovering persistent facts about the world (for example, causal processes or causal properties of objects). By saying that we are
learning “persistent” facts we are indicating that there is something to infer which we expect to be relevant to many observations over time.
Thus, we will formulate learning as inference in a model that (1) has a fixed latent value of interest, the hypothesis, and (2) has a sequence of
observations, the data points.

When thinking about learning as inference, there are several key questions. First, what can be inferred about the hypothesis given a certain
subset of the observed data? For example, in most cases, you cannot learn much about the weight of an object based on its color. However, if
there is a correlation between weight and color – as is the case in many children’s toys – observing color does allow you to learn about weight.

Second, what is the relationship between the amount of input (how much data we’ve observed) and the knowledge gained? In psychology, this
relationship is often characterized with a learning curve, representing a belief as a function of amount of data. In general, getting more data
allows us to update our beliefs. But some data, in some models, has a much bigger effect. In addition, while knowledge often changes gradually as
data is accumulated, it sometimes jumps in non-linear ways; these are usually the most psychologically interesting predictions.

Example: Learning About Coins

As a simple illustration of learning, imagine that a friend pulls a coin out of her pocket and offers it to you to flip. You flip it five times and
observe a set of all heads:

[H, H, H, H, H] .

Does this seem at all surprising? To most people, flipping five heads in a row is a minor coincidence but nothing to get excited about. But suppose
you flip it five more times and continue to observe only heads. Now the data set looks like this:

[H, H, H, H, H, H, H, H, H, H]

Most people would find this a highly suspicious coincidence and begin to suspect that perhaps their friend has rigged this coin in some way – maybe
it’s a weighted coin that always comes up heads no matter how you flip it. This inference could be stronger or weaker, of course, depending on
what you believe about your friend or how she seems to act; did she offer a large bet that you would flip more heads than tails? Now you
continue to flip five more times and again observe nothing but heads – so the data set now consists of 15 heads in a row:

[H, H, H, H, H, H, H, H, H, H, H, H, H, H, H]

Regardless of your prior beliefs, it is almost impossible to resist the inference that the coin is a trick coin.

This learning curve reflects a highly systematic and rational process of conditional inference. For simplicity let’s consider only two hypotheses,
two possible definitions of  coin , representing a fair coin and a trick coin that produces heads 95% of the time. A priori, how likely is any coin
offered up by a friend to be a trick coin? Of course there is no objective or universal answer to that question, but for the sake of illustration
let’s assume that the prior probability of seeing a trick coin is 1 in a 1000, versus 999 in 1000 for a fair coin.

var observedData = ['h', 'h', 'h', 'h', 'h']

var fairPrior = 0.999

var fairnessPosterior = Infer({method: 'enumerate'}, function() {
  var fair = flip(fairPrior)
  var coin = Bernoulli({p: fair ? 0.5 : 0.95})
  var obsFn = function(datum){observe(coin, datum == 'h')}
  mapData({data: observedData}, obsFn)
  return {fair: fair}
})

viz(fairnessPosterior)
run▼

Try varying the number of flips and the number of heads observed. You should be able to reproduce the intuitive learning curve described
above. Observing 5 heads in a row is not enough to suggest a trick coin, although it does raise the hint of this possibility: its chances are now a few
percent, approximately 30 times the baseline chance of 1 in a 1000. After observing 10 heads in a row, the odds of trick coin and fair coin are now
roughly comparable, although fair coin is still a little more likely. After seeing 15 or more heads in a row without any tails, the odds are now
strongly in favor of the trick coin.

When exploring learning as a conditional inference, we are particularly interested in the dynamics of how inferred hypotheses change as a
function of amount of data (often thought of as time the learner spends acquiring data). We can map out the trajectory of learning by plotting a

summary of the posterior distribution as a function of the amount of observed data. Here we plot the expectation that the coin is fair in the
above example:

var fairnessPosterior = function(observedData) {
  return Infer({method: 'enumerate'}, function() {
    var fair = flip(0.999)
    var coin = Bernoulli({p: fair ? 0.5 : 0.95})
    var obsFn = function(datum){observe(coin, datum == 'h')}
    mapData({data: observedData}, obsFn)
    return fair
  })
}

var trueWeight = 0.9
var fullDataSet = repeat(100, function(){flip(trueWeight)?'h':'t'})
var observedDataSizes = [1,3,6,10,20,30,50,70,100]
var estimates = map(function(N) {
  return expectation(fairnessPosterior(fullDataSet.slice(0,N)))
}, observedDataSizes)
viz.line(observedDataSizes, estimates)
run▼

Notice that different runs of this program can give quite different trajectories, but always end up in the same place in the long run. This is
because the data set used for learning is different on each run. This is a feature, not a bug: real learners have idiosyncratic experience, even
if they are all drawn from the same distribution. Of course, we are often interested in the average behavior of an ideal learner: we could
average this plot over many randomly chosen data sets, simulating many different learners.

Study how this learning curve depends on the choice of  fairPrior . There is certainly a dependence. If we set  fairPrior  to be 0.5, equal for the
two alternative hypotheses, just 5 heads in a row are sufficient to favor the trick coin by a large margin. If   fairPrior  is 99 in 100, 10 heads in a
row are sufficient. We have to increase  fairPrior  quite a lot, however, before 15 heads in a row is no longer sufficient evidence for a trick coin:
even at  fairPrior  = 0.9999, 15 heads without a single tail still weighs in favor of the trick coin. This is because the evidence in favor of a trick
coin accumulates exponentially as the data set increases in size; each successive  h  flip increases the evidence by nearly a factor of 2.

Learning is always about the shift from one state of knowledge to another. The speed of that shift provides a way to diagnose the strength of a
learner’s initial beliefs. Here, the fact that somewhere between 10 and 15 heads in a row is sufficient to convince most people that the coin is a
trick coin suggests that for most people, the a priori probability of encountering a trick coin in this situation is somewhere between 1 in a 100 and 1
in 10,000—a reasonable range. Of course, if you begin with the suspicion that any friend who offers you a coin to flip is liable to have a trick coin
in his pocket, then just seeing five heads in a row should already make you very suspicious—as we can see by setting   fairPrior  to a value such as
0.9.

Independent and Exchangeable Sequences

Now that we have illustrated the kinds of questions we are interested in asking of learning models, let’s delve into the mathematical structure
of models for sequences of observations.

If the observations have nothing to do with each other, except that they have the same distribution, they are called  identically, independently
distributed (usually abbreviated to i.i.d.). For instance the values that come from calling  flip  are i.i.d. To verify this, let’s first check
whether the distribution of two flips in a sequence look the same (are “identical”):

var genSequence = function() {return repeat(2, flip)}
var sequenceDist = Infer({method: 'enumerate'}, genSequence)
viz.marginals(sequenceDist)
run▼

Now let’s check that the first and second flips are independent, by conditioning on the first and seeing that the distribution of the second is
unchanged:

var genSequence = function() {return repeat(2, flip)}
var sequenceCondDist = function(firstVal) {
  return Infer({method: 'enumerate'},
    function() {
      var s = genSequence()
      condition(s[0] == firstVal)
      return {second: s[1]};
  })

}

viz(sequenceCondDist(true))
viz(sequenceCondDist(false))
run▼

It is easy to build other i.i.d. sequences in WebPPL; we simply construct a stochastic thunk (a random function with no arguments) and evaluate it
repeatedly. For instance, here is an extremely simple model for the words in a sentence:

var words = ['chef', 'omelet', 'soup', 'eat', 'work', 'bake', 'stop']
var probs = [0.0032, 0.4863, 0.0789, 0.0675, 0.1974, 0.1387, 0.0277]
var thunk = function() {return categorical({ps: probs, vs: words})};

repeat(10, thunk)
run▼

In this example the different words are indeed independent: you can show as above (by conditioning) that the first word tells you nothing about
the second word. However, constructing sequences in this way it is easy to accidentally create a sequence that is not entirely independent. For
instance:

var words = ['chef', 'omelet', 'soup', 'eat', 'work', 'bake', 'stop']
var probs = (flip() ?
             [0.0032, 0.4863, 0.0789, 0.0675, 0.1974, 0.1387, 0.0277] :
             [0.3699, 0.1296, 0.0278, 0.4131, 0.0239, 0.0159, 0.0194])
var thunk = function() {return categorical({ps: probs, vs: words})};

repeat(10, thunk)
run▼

While the sequence looks very similar, the words are not independent: learning about the first word tells us something about the   probs , which
in turn tells us about the second word. Let’s show this in a slightly simpler example:

var genSequence = function() {
  var prob = flip() ? 0.2 : 0.7
  var thunk = function() {return flip(prob)}
  return repeat(2, thunk)
}
var sequenceCondDist = function(firstVal) {
  return Infer({method: 'enumerate'},
    function() {
      var s = genSequence()
      condition(s[0] == firstVal)
      return {second: s[1]}
  });
};

viz(sequenceCondDist(true))
viz(sequenceCondDist(false))
run▼

Conditioning on the first value tells us something about the second. This model is thus not i.i.d., but it does have a slightly weaker property: it
is exchangeable, meaning that the probability of a sequence of values remains the same if permuted into any order. When modeling learning it is
often reasonable that the order of observations doesn’t matter—and hence that the distribution is exchangeable.

It turns out that exchangeable sequences can always be modeled in the form used for the last example: de Finetti’s theorem says that, under
certain   technical   conditions,   any   exchangeable   sequence   can   be   represented   as   follows,   for   some  latentPrior  distribution   and   observation
function  f :

var latent = sample(latentPrior)

var thunk = function() {return f(latent)}

var sequence = repeat(2,thunk)

Example: Polya’s urn

A classic example is Polya’s urn: Imagine an urn that contains some number of white and black balls. On each step we draw a random ball from the
urn, note its color, and return it to the urn along with another ball of that color. Here is this model in WebPPL:

var urnSeq = function(urn, numsamples) {
  if(numsamples == 0) {
    return []
  } else {
    var ball = uniformDraw(urn)
    return ball+urnSeq(urn.concat([ball]), numsamples-1)
  }
}

var urnDist = Infer({method: 'enumerate'},
                    function(){return urnSeq(['b', 'w'],3)})

viz(urnDist)
run▼

Polya’s urn is an examples of a “rich get richer” dynamic, which has many applications for modeling the real world. Examining the distribution on
sequences, it appears that this model is exchangeable—permutations of a sequence all have the same probability (e.g.,   bbw ,  bwb ,  wbb  have the
same probability;  bww ,  wbw ,  wwb  do too). (Challenge: Can you prove this mathematically?)

Because the distribution is exchangeable, we know that there must be an alterative representation in terms of a latent quantity followed by
independent samples. The de Finetti representation of this model is:

 = function(urn, numsamples) {

var urn deFinetti
_
  var numWhite = sum(map(function(b){return b=='w'},urn))
  var numBlack = urn.length - numWhite
  var latentPrior = Beta({a: numWhite, b: numBlack})
  var latent = sample(latentPrior)
  return repeat(numsamples, function() {return flip(latent) ? 'b' : 'w'}).join("")
}

var urnDist = Infer({method: 'forward', samples: 10000},
                    function(){return urn deFinetti([

_

'b', 'w'],3)})

viz(urnDist)
run▼

We sample a shared latent parameter – in this case, a sample from a Beta distribution – generating the sequence samples independently given
this parameter. We obtain the same distribution on sequences of draws. (Challenge: show mathematically that these two representations give the
same distribution.)

Ideal learners

Recall that we aimed to formulate learning as inference in a model that has a fixed latent value of interest and a sequence of observations. We
now know that this will be possible anytime we are willing to assume the data are exchangeable.

Many Bayesian models of learning are formulated in this way. We often write this in the pattern of Bayes’ rule:

Infer({...}, function() {

  var hypothesis = sample(prior)

  var obsFn = function(datum){...uses hypothesis...}

  mapData({data: observedData}, obsFn)

  return hypothesis

});

The  prior  samples a hypothesis from the hypothesis space. This distribution expresses our prior knowledge about how the process we observe is
likely to work, before we have observed any data. The function  obsFn  captures the relation between the  hypothesis  and a single  datum , and
will  usually  contain  an  observe  statement.  (The  marginal  probability  function  for  obsFn  is  called  the likelihood.  Sometimes  obsFn  itself  is
colloquially called the likelihood, too.) Here we have used the special operator   mapData  whose meaning is the same as  map . We use  mapData  both
to remind ourselves that we are expressing the special pattern of observing a sequence of observations, and because some inference algorithms
can use this hint to do better learning.

Overall this setup of prior, likelihood, and a sequence of observed data (which implies an exchangeable distribution on data!) describes an  ideal
learner.

Example: Subjective Randomness

What does a random sequence look like? Is 00101 more random than 00000? Is the former a better example of a sequence coming from a fair coin
than the latter? Most people say so, but notice that if you flip a fair coin, these two sequences are equally probable. Yet these intuitions about
randomness are pervasive and often misunderstood: In 1936 the Zenith corporation attempted to test the hypothesis the people are sensitive to
psychic transmissions. During a radio program, a group of psychics would attempt to transmit a randomly drawn sequence of ones and zeros to the
listeners. Listeners were asked to write down and then mail in the sequence they perceived. The data thus generated showed no systematic
effect of the transmitted sequence—but it did show a strong preference for certain sequences (Goodfellow, 1938). The preferred sequences
included 00101, 00110, 01100, and 01101.

Griffiths and Tenenbaum (2001) suggested that we can explain this bias if people are considering not the probability of the sequence under a
fair-coin process, but the probability that the sequence would have come from a fair process as opposed to a non-uniform (trick) process:

var isFairDist = function(sequence) {
  return Infer({method: 'enumerate'},
    function () {
      var isFair = flip()
      var realWeight = isFair ? 0.5 : 0.2
      var coin = Bernoulli({p: realWeight})
      mapData({data: sequence},function(d){observe(coin, d)})
      return isFair
  })
}

print("00101 is fair?")
viz(isFairDist([false, false, true, false, true]))
print("00000 is fair?")
viz(isFairDist([false, false, false, false, false]))
run▼

This model posits that when considering randomness people are more concerned with distinguishing a “truly random” generative process from a
trick process. How do these inferences depend on the amount of data? Explore the learning trajectories of this model.

Learning a Continuous Parameter

The previous examples represent perhaps simple cases of learning. Typical learning problems in human cognition or AI are more complex in many
ways. For one, learners are almost always confronted with more than two hypotheses about the causal structure that might underlie their
observations. Indeed, hypothesis spaces for learning are often infinite. Countably infinite hypothesis spaces are encountered in models of
learning for domains traditionally considered to depend on “discrete” or “symbolic” knowledge; hypothesis spaces of grammars in language
acquisition are a canonical example. Hypothesis spaces for learning in domains traditionally considered more “continuous”, such as perception or
motor control, are typically uncountable and parametrized by one or more continuous dimensions. In causal learning, both discrete and continuous
hypothesis   spaces   typically   arise.   (In   statistics,   making   conditional   inferences   over   continuous   hypothesis   spaces   given   data   is   often
called parameter estimation.)

We can explore a basic case of learning with continuous hypothesis spaces by slightly enriching our coin flipping example. Suppose instead of simply
flipping a coin to determine which of two coin weights to use, we can choose  any coin weight between 0 and 1. The following program computes
conditional inferences about the weight of a coin drawn from a prior distribution described by the  Uniform  function, conditioned on a set of
observed flips.

var observedData = ['h', 'h', 'h', 'h', 'h']

var weightPosterior = Infer({method: 'rejection', samples: 1000}, function() {
  var coinWeight = sample(Uniform({a: 0, b: 1}))
  var coin = Bernoulli({p: coinWeight})
  var obsFn = function(datum){observe(coin, datum == 'h')}
  mapData({data: observedData}, obsFn)
  return coinWeight
})

viz(weightPosterior)
run▼

Experiment with different data sets, varying both the number of flips and the relative proportion of heads and tails. How does the shape of
the conditional distribution change? The location of its peak reflects a reasonable “best guess” about the underlying coin weight. It will be
roughly equal to the proportion of heads observed, reflecting the fact that our prior knowledge is basically uninformative; a priori, any value
of  coinWeight  is equally likely. The spread of the conditional distribution reflects a notion of confidence in our beliefs about the coin weight.
The distribution becomes more sharply peaked as we observe more data, because each flip, as an independent sample of the process we are
learning about, provides additional evidence of the process’s unknown parameters.

We can again look at the learning trajectory in this example:

var weightPosterior = function(observedData){
  return Infer({method: 'MCMC', samples: 1000}, function() {
    var coinWeight = sample(Uniform({a: 0, b: 1}))
    var coin = Bernoulli({p: coinWeight})
    var obsFn = function(datum){observe(coin, datum=='h')}
    mapData({data: observedData}, obsFn)
    return coinWeight
  })
}

var fullDataSet = repeat(100, function(){return 'h'})
var observedDataSizes = [0,1,2,4,8,16,25,30,50,70,100]
var estimates = map(function(N) {
  return expectation(weightPosterior(fullDataSet.slice(0,N)))
}, observedDataSizes)
viz.line(observedDataSizes, estimates)
run▼

It is easy to see that this model doesn’t really capture our intuitions about coins, or at least not in everyday scenarios. Imagine that you have
just received a quarter in change from a store – or even better, taken it from a nicely wrapped-up roll of quarters that you have just picked
up from a bank. Your prior expectation at this point is that the coin is almost surely fair. If you flip it 10 times and get 7 heads out of 10, you’ll
think nothing of it; that could easily happen with a fair coin and there is no reason to suspect the weight of this particular coin is anything other
than 0.5. But running the above query with uniform prior beliefs on the coin weight, you’ll guess the weight in this case is around 0.7. Our
hypothesis generating function needs to be able to draw  coinWeight  not from a uniform distribution, but from some other function that can
encode various expectations about how likely the coin is to be fair, skewed towards heads or tails, and so on.

One   option   is   the   Beta   distribution.   The   Beta   distribution   takes   parameters  a  and  b ,   which   describe   the   prior   toward  true  and  false .
(When  a  and  b  are integers they can be thought of as prior observations.)

var pseudoCounts = {a: 10, b: 10};

var weightPosterior = function(observedData){
  return Infer({method: 'MCMC', burn:1000, samples: 1000}, function() {
    var coinWeight = beta(pseudoCounts)
    var coin = Bernoulli({p: coinWeight})
    var obsFn = function(datum){observe(coin, datum=='h')}
    mapData({data: observedData}, obsFn)
    return coinWeight
  })
}

var fullDataSet = repeat(100, function(){return 'h'});
var observedDataSizes = [0,1,2,4,6,8,10,20,30,40,50,70,100];
var estimates = map(function(N) {
  return expectation(weightPosterior(fullDataSet.slice(0,N)))
}, observedDataSizes);
viz.line(observedDataSizes, estimates);
run▼

We are getting closer, in that learning is far more conservative. In fact, it is too conservative: after getting heads 100 times in a row, most
humans will conclude the coin can only come up heads. The model, in contrast, still expects the coin to come up tails around 10% of the time.

We can of course decrease our priors  a  and  b  to get faster learning, but then we will just go back to our earlier problem. We would like instead
to encode in our prior the idea that fair coins (probability 0.5) are much more likely than even moderately unfair coins.

A More Structured Hypothesis Space

The following model explicitly builds in the prior belief that fair coins are likely, and that all unfair coins are equally likely as each other:

var weightPosterior = function(observedData){
  return Infer({method: 'MCMC', burn:1000, samples: 10000}, function() {
    var isFair = flip(0.999)
    var realWeight = isFair ? 0.5 : uniform({a:0, b:1})
    var coin = Bernoulli({p: realWeight})
    var obsFn = function(datum){observe(coin, datum=='h')}
    mapData({data: observedData}, obsFn)
    return realWeight
  })
}

var fullDataSet = repeat(50, function(){return 'h'});
var observedDataSizes = [0,1,2,4,6,8,10,12,15,20,25,30,40,50];
var estimates = map(function(N) {
  return expectation(weightPosterior(fullDataSet.slice(0,N)))
}, observedDataSizes);
viz.line(observedDataSizes, estimates);
run▼

This model stubbornly believes the coin is fair until around 10 successive heads have been observed. After that, it rapidly concludes that the
coin can only come up heads. The shape of this learning trajectory is much closer to what we would expect for humans. This model is a simple
example of a hierarchical prior which we explore in detail in a later chapter.

Example: Estimating Causal Power

Modeling beliefs about coins makes for clear examples, but it’s obviously not a very important cognitive problem. However, many important
cognitive problems have a remarkably similar structure.

For instance, a common problem for cognition is causal learning: from observed evidence about the co-occurrence of events, attempt to infer the
causal structure relating them. An especially simple case that has been studied by psychologists is elemental causal induction: causal learning
when there are only two events, a potential cause C and a potential effect E. Cheng and colleagues  Cheng (1997) have suggested assuming that C
and background effects can both cause E, with a noisy-or interaction. Causal learning then becomes an example of parameter learning, where the
parameter is the “causal power” of C to cause E:

var observedData = [{C:true, E:true}, {C:true, E:true}, {C:false, E:false}, {C:true, E:true}]

var causalPowerPost = Infer({method: 'MCMC', samples: 10000}, function() {
  // Causal power of C to cause E
  var cp = uniform(0, 1)

  // Background probability of E
  var b = uniform(0, 1)

  var obsFn = function(datum) {
    // The noisy causal relation to get E given C
    var E = (datum.C && flip(cp)) || flip(b)
    condition( E == datum.E)
  }

  mapData({data: observedData}, obsFn)

  return {causal power: 
});

_

cp}

viz(causalPowerPost);
run▼

Experiment with this model: when does it conclude that a causal relation is likely (high   cp )? Does this match your intuitions? What role does the
background rate  b  play? What happens if you change the functional relationship in  obsFn ?

Reading & Discussion: Readings

Test your knowledge: Exercises
Next chapter: 10. Learning with a language of thought 

☰

Learning with a language of thought

An important worry about Bayesian models of learning is that the Hypothesis space must either be too simple (e.g. a single coin weight!), specified
in a rather ad-hoc way, or both. There is a tension here: human representations of the world are enormously complex and so the space of
possible representations must be correspondingly big, and yet we would like to understand the representational resources in simple and uniform
terms. How can we construct very large (possibly infinite) hypothesis spaces and priors over them? One possibility is to build the hypotheses
themselves via stochastic recursion. That is, we build hypotheses by a combination of primitives and combination operations, randomly deciding which
to use.

For instance, imagine that we want a model that generates strings, but we want the strings to be valid arithmetic expressions. Since we know that
arithmetic has as primitives numbers and combines them with operations, we can define a simple generator:

var randomConstant = function() {
  return uniformDraw( .range(
10))
}

_

var randomCombination = function(f,g) {
  var op = uniformDraw(['+','-','*','/','**']);
  return '('+f+op+g+')'
}

// sample an arithmetic expression
var randomArithmeticExpression = function() {
  flip() ? 
    randomCombination(randomArithmeticExpression(), randomArithmeticExpression()) : 
    randomConstant()
}

randomArithmeticExpression()
run▼

Notice that  randomArithmeticExpression  can generate an infinite set of different strings, but that more complex strings are less likely. That is,
the process we use to build strings also (implicitly) defines a prior over strings that penalizes complexity. To see this more let’s use Infer to
reveal the first 100 strings:

var randomConstant = function() {
  return uniformDraw( .range(
10))
}

_

var randomCombination = function(f,g) {
  var op = uniformDraw(['+','-','*','/','**']);
  return '('+f+op+g+')'
}

// sample an arithmetic expression
var randomArithmeticExpression = function() {
  flip() ? 
    randomCombination(randomArithmeticExpression(), randomArithmeticExpression()) : 
    randomConstant()
}

randomArithmeticExpression()
viz.table(Infer({method: 'enumerate', maxExecutions: 100}, function() {
  return randomArithmeticExpression()
}))
run▼

If we now interpret our strings as hypotheses, we have compactly defined an infinite hypothesis space and its prior.

Inferring an Arithmetic Function

Consider the following WebPPL program, which induces an arithmetic function from examples. The basic generative process is similar to the above,
but we include the identity function (‘x’), making the resulting expression a function of ‘x’. At every step we create a runnable function
form ( fn ) and also the previous nice string form ( expr ).

var plus = {fn: function(a,b) {return a + b}, expr: '+'}
var multiply = {fn: function(a,b) {return Math.round(a * b,0)}, expr: '*'}
var divide = {fn: function(a,b) {return Math.round(a/b,0)}, expr: '/'}
var minus = {fn: function(a,b) {return a - b}, expr: '-'}
var power = {fn: function(a,b) {return Math.pow(a,b)}, expr: '**'}
var binaryOps = [plus, multiply, divide, minus, power]

var identity = {fn: function(x) {return x}, expr: 'x'}

var randomConstantFunction = function() {
  var c = uniformDraw( .range(
_
  return {fn: function(x){return c}, expr: c}
}

10))

var randomCombination = function(f,g) {
  var op = uniformDraw(binaryOps);
  var opfn = op.fn
  var ffn = f.fn
  var gfn = g.fn
  return {fn: function(x){return opfn(ffn(x),gfn(x))}, 
          expr: f.expr+op.expr+g.expr}
}

// sample an arithmetic expression
var randomArithmeticExpression = function() {
  if (flip()) {
    return randomCombination(randomArithmeticExpression(), 
                             randomArithmeticExpression())
  } else {
    return flip() ? identity : randomConstantFunction()
  }
}

viz.table(Infer({method: 'enumerate', maxExecutions: 1000}, function() {
  var e = randomArithmeticExpression();
  var f = e.fn
  condition(f(1) == 3);

  return {expr: e.expr};
}))
run▼

This model can learn any function consisting of the integers 0 to 9 and the operations add, subtract, multiply, divide, and power. The condition in
this case asks for an arithmetic expression on variable  x  such that it evaluates to  3  when  x  is  1 . There are many extensionally equivalent
ways to satisfy the condition, for instance the expressions  3 ,  1 + 2 , and  x + 2 , but because the more complex expressions require more choices
to generate, they are chosen less often.

Notice that the model puts the most probability on a function that always returns  3  (f(x)=3f(x)=3). This is the simplest hypothesis consistent 
with the data. Let’s see what happens if we have more data – try changing the condition in the above query to  condition(f(1) == 3 && f(2) == 4) ,
then to  condition(f(1) == 3 && f(2) == 6) .
This model learns from an infinite hypothesis space—all expressions made from ‘x’, ‘+’, ‘-‘, and constant integers—but specifies both the
hypothesis space and its prior using the simple generative process  randomArithmeticExpression .

Example: Rational Rules

How can we account for the productivity of human concepts (the fact that every child learns a remarkable number of different, complex
concepts)?   The   “classical”   theory   of   concepts   formation   accounted   for   this   productivity   by   hypothesizing   that   concepts   are   represented

compositionally, by logical combination of the features of objects (see for example Bruner, Goodnow, and Austin, 1951). That is, concepts could be
thought of as rules for classifying objects (in or out of the concept) and concept learning was a process of deducing the correct rule.

While this theory was appealing for many reasons, it failed to account for a variety of categorization experiments. Here are the training
examples, and one transfer example, from the classic experiment of Medin and Schaffer (1978). The bar graph above the stimuli shows the
portion of human participants who said that bug was a “fep” in the test phase (the data comes from a replication by Nosofsky, Gluck, Palmeri,
McKinley (1994); the bug stimuli are courtesy of Pat Shafto):

Notice three effects: there is a gradient of generalization (rather than all-or-nothing classification), some of the Feps are better (or more
typical) than others (this is called “typicality”), and the transfer item is a ‘‘better’’ Fep than any of the Fep exemplars (this is called
“prototype enhancement”). Effects like these were difficult to capture with classical rule-based models of category learning, which led to
deterministic behavior. As a result of such difficulties, psychological models of category learning turned to more uncertain, prototype and
exemplar based theories of concept representation. These models were able to predict behavioral data very well, but lacked compositional
conceptual structure.

Is it possible to get graded effects from rule-based concepts? Perhaps these effects are driven by uncertainty in  learning rather than
uncertainty in the representations themselves? To explore these questions Goodman, Tenenbaum, Feldman, and Griffiths (2008) introduced the
Rational   Rules   model,   which   learns   deterministic   rules   by   probabilistic   inference.   This   model   has   an   infinite   hypothesis   space   of   rules
(represented in propositional logic), which are generated compositionally. Here is a slightly simplified version of the model, applied to the above
experiment:

// first set up the training and test data:
var numFeatures = 4;

_

var makeObj = function(l) {return  .zipObject([
var feps = map(makeObj, [[0,0,0,1, 1], [0,1,0,1, 1], [0,1,0,0, 1], [0,0,1,0, 1], [1,0,0,0, 1]])
var nonFeps = map(makeObj, [[0,0,1,1, 0], [1,0,0,1, 0], [1,1,1,0, 0], [1,1,1,1, 0]])
var others = map(makeObj, [[0,1,1,0], [0,1,1,1], [0,0,0,0], [1,1,0,1], [1,0,1,0], [1,1,0,0], [1,0,1,1]])
var data = feps.concat(nonFeps)
var allObjs = others.concat(feps).concat(nonFeps)

'trait1', 'trait2', 'trait3', 'trait4', 'fep'], l)}

//here are the human results from Nosofsky et al, for comparison:
var humanFeps = [.77, .78, .83, .64, .61]
var humanNonFeps = [.39, .41, .21, .15]
var humanOther = [.56, .41, .82, .40, .32, .53, .20]
var humanData = humanOther.concat(humanFeps).concat(humanNonFeps)

// two parameters: stopping probability of the grammar, and noise probability:
var tau = 0.3;
var noiseParam = Math.exp(-1.5)

// a generative process for disjunctive normal form propositional equations:
var samplePred = function() {
  var trait = uniformDraw(['trait1', 'trait2', 'trait3', 'trait4'])
  var value = flip()
  return function(x) {return x[trait] == value}
}

var sampleConj = function() {
  if(flip(tau)) {
    var c = sampleConj()
    var p = samplePred()
    return function(x) {return c(x) && p(x)}
  } else {
    return samplePred()
  }

}

var getFormula = function() {
  if(flip(tau)) {
    var c = sampleConj()
    var f = getFormula()
    return function(x) {return c(x) || f(x)};
  } else {
    return sampleConj()
  }
}

var rulePosterior = Infer({method: 'enumerate', maxExecutions: 1000}, function() {
  // sample a classification formula
  var rule = getFormula()

  // condition on correctly (up to noise) accounting for observations
  var obsFn = function(datum){observe(Bernoulli({p: rule(datum) ? (1-noiseParam) : noiseParam}), datum.fep == 1)}
  mapData({data: data}, obsFn) 

  // return posterior predictive
  return map(rule, allObjs)
})

//build predictive distribution for each item
var predictives = map(function(i){expectation(rulePosterior,function(x){x[i]})},  .range(

_

15))

viz.scatter(predictives, humanData)
run▼

In addition to achieving a good overall correlation with the data, this model captures the three qualitative effects described above: graded
generalization, typicality, and prototype enhancement. Make sure you see how to read each of these effects from the above plot! Goodman, et al,
have used to this model to capture a variety of other classic categorization effects (Goodman et al., 2008), as well. Thus probabilistic induction
of (deterministic) rules can capture many of the graded effects previously taken as evidence against rule-based models.

Grammar-based induction

What is the general principle in the two above examples? We can think of it as the following recipe: we build hypotheses by stochastically choosing
between primitives and combination operations, this specifies an infinite “language of thought”; each expression in this language in turn specifies
the likelihood of observations. Formally, the stochastic combination process specifies a probabilistic grammar; which yields terms compositionally
interpreted   into   a   likelihood   over   data.   A   small   grammar   can   generate   an   infinite   array   of   potential   hypotheses;   because   grammars   are
themselves generative processes, a prior is provided for free from this formulation.

This style of compositional concept induction model, can be naturally extended to complex hypothesis spaces, each defined by a grammar. For
instance to model theory acquisition, learning natural numbers concepts, and many others. See:

•Compositionality in rational analysis: Grammar-based induction for concept learning. N. D. Goodman, J. B. Tenenbaum, T. L. Griffiths, and
J. Feldman (2008). In M. Oaksford and N. Chater (Eds.). The probabilistic mind: Prospects for Bayesian cognitive science.
•A Bayesian Model of the Acquisition of Compositional Semantics. S. T. Piantadosi, N. D. Goodman, B. A. Ellis, and J. B. Tenenbaum (2008).
Proceedings of the Thirtieth Annual Conference of the Cognitive Science Society.
•Piantadosi,   S.   T.,   &   Jacobs,   R.   A.   (2016).   Four   Problems   Solved   by   the   Probabilistic   Language   of   Thought.   Current   Directions   in
Psychological Science, 25(1).

There is also no reason that the concepts need to be deterministic; in WebPPL stochastic functions can be constructed compositionally and learned
by induction:

•Learning Structured Generative Concepts. A. Stuhlmueller, J. B. Tenenbaum, and N. D. Goodman (2010). Proceedings of the Thirty-
Second Annual Conference of the Cognitive Science Society.

Reading & Discussion: Readings

Test your knowledge: Exercises
Next chapter: 11. Hierarchical models 

  
  
☰

Hierarchical models

Human knowledge is organized hierarchically into levels of abstraction. For instance, the most common or basic-level categories (e.g. dog, car) can
be thought of as abstractions across individuals, or more often across subordinate categories (e.g.,  poodle, Dalmatian, Labrador, and so on).
Multiple basic-level categories in turn can be organized under superordinate categories: e.g., dog, cat, horse are all animals; car, truck, bus are
all vehicles. Some of the deepest questions of cognitive development are: How does abstract knowledge influence learning of specific knowledge?
How can abstract knowledge be learned? In this section we will see how such hierarchical knowledge can be modeled with hierarchical generative
models: generative models with uncertainty at several levels, where lower levels depend on choices at higher levels.

Learning a Shared Prototype: Abstraction at the Basic
Level

Hierarchical models allow us to capture the shared latent structure underlying observations of multiple related concepts, processes, or systems
– to abstract out the elements in common to the different sub-concepts, and to filter away uninteresting or irrelevant differences. Perhaps
the   most   familiar   example   of   this   problem   occurs   in   learning   about   categories.   Consider   a   child   learning   about   a   basic-level   kind,   such
as dog or car. Each of these kinds has a prototype or set of characteristic features, and our question here is simply how that prototype is
acquired.

The task is challenging because real-world categories are not homogeneous. A basic-level category like dog or car actually spans many different
subtypes: e.g., poodle, Dalmatian, Labrador, and such, or sedan, coupe, convertible, wagon, and so on. The child observes examples of these sub-
kinds or subordinate-level categories: a few poodles, one Dalmatian, three Labradors, etc. From this data she must infer what it means to be a
dog in general, in addition to what each of these different kinds of dog is like. Knowledge about the prototype level includes understanding what
it means to be a prototypical dog and what it means to be non-prototypical, but still a dog. This will involve understanding that dogs come in
different breeds which share features between them, but also differ systematically as well.

As a simplification of this situation consider the following generative process. We will draw marbles out of several different bags. There are
five marble colors. Each bag contains a certain mixture of colors. This generative process is represented in the following WebPPL example (for a
refresher on the Dirichlet distribution, see the Appendix).

var colors = ['black', 'blue', 'green', 'orange', 'red'];

var makeBag = mem(function(bagName){
  var colorProbs = dirichlet(ones([colors.length, 1]))
  return Categorical({vs: colors, ps: colorProbs})
})

viz(repeat(100, function(){sample(makeBag('bagA'))}))
viz(repeat(100, function(){sample(makeBag('bagA'))}))
viz(repeat(100, function(){sample(makeBag('bagA'))}))
viz(repeat(100, function(){sample(makeBag('bagB'))}))
run▼

As this examples shows,  mem  is particularly useful when writing hierarchical models because it allows us to associate arbitrary random draws
with categories across entire runs of the program. In this case it allows us to associate a particular mixture of marble colors with each bag. The
mixture is drawn once, and then remains the same thereafter for that bag. Intuitively, you can see how each sample is sufficient to learn a lot
about what that bag is like; there is typically a fair amount of similarity between the empirical color distributions in each of the four samples
from  bagA . In contrast, you should see a different distribution of samples from  bagB .

Now let’s explore how this model learns about the contents of different bags. We represent the results of learning in terms of the  posterior
predictive distribution   for   each   bag:   a   single   hypothetical   draw   from   the   bag.   We   will   also   draw  a   sample   from   the   posterior   predictive
distribution on a new bag, for which we have had no observations.

var colors = ['black', 'blue', 'green', 'orange', 'red'];

var observedData = [
{bag: 'bag1', draw: 'blue'},
{bag: 'bag1', draw: 'blue'},
{bag: 'bag1', draw: 'black'},
{bag: 'bag1', draw: 'blue'},
{bag: 'bag1', draw: 'blue'},
{bag: 'bag1', draw: 'blue'},
{bag: 'bag2', draw: 'blue'},
{bag: 'bag2', draw: 'green'},
{bag: 'bag2', draw: 'blue'},

{bag: 'bag2', draw: 'blue'},
{bag: 'bag2', draw: 'blue'},
{bag: 'bag2', draw: 'red'},
{bag: 'bag3', draw: 'blue'},
{bag: 'bag3', draw: 'orange'}
]

var predictives = Infer({method: 'MCMC', samples: 20000}, function(){
  var makeBag = mem(function(bag){
    var colorProbs = dirichlet(ones([colors.length, 1]))
    return Categorical({vs: colors, ps: colorProbs})
  })

  var obsFn = function(datum){
    observe(makeBag(datum.bag), datum.draw)
  }

  mapData({data: observedData}, obsFn)

  return {bag1: sample(makeBag('bag1')),
          bag2: sample(makeBag('bag2')),
          bag3: sample(makeBag('bag3')),
          bagN: sample(makeBag('bagN'))}
})

viz.marginals(predictives)
run▼

Inference suggests that the first two bags are predominantly blue, and the third is probably blue and organge. In all cases there is a fair amount
of residual uncertainty about what other colors might be seen. Nothing significant is learned about  bagN  as it has no observations. This generative
model describes the prototypical mixture in each bag, but it does not attempt learn a common higher-order prototype. It is like learning separate
prototypes   for  subordinate  classes poodle, Dalmatian,  and Labrador,  without  learning  a  prototype   for  the   higher-level   kind dog.  Yet   your
intuition may suggest that all the bags are predominantly blue, allowing you to make stronger inferences, especially about  bag3  and  bagN .

Let us introduce another level of abstraction: a global prototype that provides a prior on the specific mixtures of each bag.

///fold:
...

var predictives = Infer({method: 'MCMC', samples: 20000}, function(){
  // we make a global prototype which is a dirichlet sample scaled to total 5.
  // T.mul(d,x) multiplies the values in vector `d` by x
  var phi = dirichlet(ones([colors.length, 1]))
  var prototype = T.mul(phi, colors.length)

  var makeBag = mem(function(bag){
    var colorProbs = dirichlet(prototype)
    return Categorical({vs: colors, ps: colorProbs})
  })

  var obsFn = function(datum){
    observe(makeBag(datum.bag), datum.draw)
  }

  mapData({data: observedData}, obsFn)

  return {bag1: sample(makeBag('bag1')),
          bag2: sample(makeBag('bag2')),
          bag3: sample(makeBag('bag3')),
          bagN: sample(makeBag('bagN'))}
});

viz.marginals(predictives)
run▼

Compared with inferences in the previous example, this extra level of abstraction enables faster learning: more confidence in what each bag is
like based on the same observed sample. This is because all of the observed samples suggest a common prototype structure, with most of its
weight   on  blue  and   the   rest   of   the   weight   spread   uniformly   among   the   remaining   colors.   In   particular,   we   now   make   strong   inferences
for  bag3  that blue is likely but orange isn’t – quite different from the earlier case without a shared global prototype.

Statisticians sometimes refer to this phenomenon of inference in hierarchical models as “sharing of statistical strength”: it is as if the sample
we observe for each bag also provides a weaker indirect sample relevant to the other bags. In machine learning and cognitive science this
phenomenon is often called transfer learning. Intuitively, knowing something about bags in general allows the learner to transfer knowledge
gained from draws from one bag to other bags. This example is analogous to seeing several examples of different subtypes of dogs and learning
what features are in common to the more abstract basic-level dog prototype, independent of the more idiosyncratic features of particular dog
subtypes.

Learning about shared structure at a higher level of abstraction also supports inferences about new bags without observing any examples from
that bag: a hypothetical new bag could produce any color, but is likely to have more blue marbles than any other color (see the  bagN  result
above!). We can imagine hypothetical, previously unseen, new subtypes of dogs that share the basic features of dogs with more familiar kinds but
may differ in some idiosyncratic ways.

The Blessing of Abstraction

Now let’s investigate the relative learning speeds at different levels of abstraction. Suppose that we have a number of bags that all have
identical prototypes: they mix red and blue in proportion 2:1. But the learner doesn’t know this. She observes only one ball from each of N bags.
What can she learn about an individual bag versus the population as a whole as the number of bags changes? We plot learning curves: the mean
squared error (MSE) of the prototype from the true prototype for the specific level (the first bag) and the general level (global prototype) as
a function of the number of observed data points. We normalize by the MSE of the first observation (from the first bag), to focus on the
effects of diverse data. (Note that these MSE quantities are directly comparable because they are each derived from a Dirichlet distribution of
the same size – this is often not the case in hierarchical models.)

var colors = ['red', 'blue'];
var posterior = function(observedData) {
  return Infer({method: 'MCMC', samples: 50000}, function() {

    var phi = dirichlet(ones([colors.length, 1]))
    var prototype = T.mul(phi, colors.length)

    var bagProbs = mem(function(bag){
      return dirichlet(prototype)
    })

    var makeBag = mem(function(bag){
      var colorProbs = bagProbs(bag)
      return Categorical({vs: colors, ps: colorProbs})
    })

    var obsFn = function(datum){
      observe(makeBag(datum.bag), datum.draw)
    }

    mapData({data: observedData}, obsFn)

    return {bag1: T.get(bagProbs('bag1'),0),
            global: T.get(phi,0)}
  })
}

// data include a single sample from each bag.
var data = [{bag:'bag1', draw:'red'}, {bag:'bag2', draw:'red'}, {bag:'bag3', draw:'blue'},
            {bag:'bag4', draw:'red'}, {bag:'bag5', draw:'red'}, {bag:'bag6', draw:'blue'},
            {bag:'bag7', draw:'red'}, {bag:'bag8', draw:'red'}, {bag:'bag9', draw:'blue'},
            {bag:'bag10', draw:'red'}, {bag:'bag11', draw:'red'}, {bag:'bag12', draw:'blue'}]

//compute the posteriors for different amounts of observations
var numObs = [1,3,6,9, 12]
var posteriors = map(function(N){posterior(data.slice(0,N))}, numObs)

//Helper fn to compute the mean-squared error of a posterior
var meanDev = function(dist, param, truth) {
  return expectation(dist, function(val) {return Math.pow(truth - val[param], 2)})
}

//MSE curves normalized by the one-observations error
var initialSpec = meanDev(posteriors[0], 'bag1', 0.66)
var specErrors = map(function(d){meanDev(d,'bag1',0.66)/initialSpec}, posteriors)

var initialGlob = meanDev(posteriors[0], 'global', 0.66)
var globErrors = map(function(d){meanDev(d,'global',0.66)/initialGlob}, posteriors)

//now we generate learning curves! 
print("bag1 error")
viz.line(numObs, specErrors)
print("global error")
viz.line(numObs, globErrors)
run▼

What we see is that learning is faster at the general level than the specific level—that is that the error in the estimated prototype drops
faster in the general than the specific plots. We also see that there is continued learning at the specific level, even though we see no
additional samples from the first bag after the first; this is because the evolving knowledge at the general level further constrains the
inferences at the specific level. Going back to our familiar categorization example, this suggests that a child could be quite confident in the
prototype of “dog” while having little idea of the prototype for any specific kind of dog—learning more quickly at the abstract level than the
specific
  dogs.
This dynamic depends crucially on the fact that we get very diverse evidence: let’s change the above example to observe the same N examples,
but coming from a single bag (instead of N bags).

constrain   expectations   about   specific

  this   abstract   knowledge   to  

  but   then   using

  level,

var data = [{bag:'bag1', draw:'red'}, {bag:'bag1', draw:'red'}, {bag:'bag1', draw:'blue'},
            {bag:'bag1', draw:'red'}, {bag:'bag1', draw:'red'}, {bag:'bag1', draw:'blue'},
            {bag:'bag1', draw:'red'}, {bag:'bag1', draw:'red'}, {bag:'bag1', draw:'blue'},
            {bag:'bag1', draw:'red'}, {bag:'bag1', draw:'red'}, {bag:'bag1', draw:'blue'}]

///fold:
...

//now we generate learning curves! 
print("bag1 error")
viz.line(numObs, specErrors)
print("global error")
viz.line(numObs, globErrors)
run▼

We now see that learning for this bag is quick, while global learning (and transfer) is slow.

In machine learning one often talks of the curse of dimensionality. The curse of dimensionality refers to the fact that as the number of
parameters of a model increases (i.e. the dimensionality of the model increases), the size of the hypothesis space increases exponentially. This
increase in the size of the hypothesis space leads to two related problems. The first is that the amount of data required to estimate model
parameters (called the “sample complexity”) increases rapidly as the dimensionality of the hypothesis space increases. The second is that the
amount   of   computational   work   needed   to   search   the   hypothesis   space   also   rapidly   increases.   Thus,   increasing   model   complexity   by   adding
parameters can result in serious problems for inference.

In contrast, we have seen that adding additional levels of abstraction (and hence additional parameters) in a probabilistic model can sometimes
make it possible to learn more with fewer observations. This happens because learning at the abstract level can be quicker than learning at the
specific level. Because this ameliorates the curse of dimensionality, we refer to these effects as the blessing of abstraction.

In general, the blessing of abstraction can be surprising because our intuitions often suggest that adding more hierarchical levels to a model
increases the model’s complexity. More complex models should make learning harder, rather than easier. On the other hand, it has long been
understood in cognitive science that learning is made easier by the addition of  constraints on possible hypothesis. For instance, proponents of
universal grammar have long argued for a highly constrained linguistic system on the basis of learnability. Hierarchical Bayesian models can be
seen as a way of introducing soft, probabilistic constraints on hypotheses that allow for the transfer of knowledge between different kinds of
observations.

Learning Overhypotheses: Abstraction at the 
Superordinate Level

Hierarchical models also allow us to capture a more abstract and even more important “learning to learn” phenomenon, sometimes called
learning overhypotheses. Consider how a child learns about living creatures (an example we adapt from the psychologists Liz Shipley and Rob
Goldstone). We learn about specific kinds of animals – dogs, cats, horses, and more exotic creatures like elephants, ants, spiders, sparrows,
eagles, dolphins, goldfish, snakes, worms, centipedes – from examples of each kind. These examples tell us what each kind is like: Dogs bark,
have four legs, a tail. Cats meow, have four legs and a tail. Horses neigh, have four legs and a tail. Ants make no sound, have six legs, no tail.
Robins and eagles both have two legs, wings, and a tail; robins sing while eagles cry. Dolphins have fins, a tail, and no legs; likewise for goldfish.
Centipedes have a hundred legs, no tail and make no sound. And so on. Each of these generalizations or prototypes may be inferred from seeing
several examples of the species.

But we also learn about what kinds of creatures are like in general. It seems that certain kinds of properties of animals are characteristic of a
particular kind: either every individual of a kind has this property, or none of them have it. Characteristic properties include number of legs,
having a tail or not, and making some kind of sound. If one individual in a species has four legs, or six or two or eight or a hundred legs,
essentially all individuals in that species have that same number of legs (barring injury, birth defect or some other catastrophe). Other kinds of
properties don’t pattern in such a characteristic way. Consider external color. Some kinds of animals are homogeneous in coloration, such as
dolphins, elephants, sparrows. Others are quite heterogeneous in coloration: dogs, cats, goldfish, snakes. Still others are intermediate, with one
or a few typical color patterns: horses, ants, eagles, worms.

This abstract knowledge about what animal kinds are like can be extremely useful in learning about new kinds of animals. Just one example of a
new kind may suffice to infer the prototype or characteristic features of that kind: seeing a spider for the first time, and observing that it has
eight legs, no tail and makes no sound, it is a good bet that other spiders will also have eight legs, no tail and make no sound. The specific
coloration of the spider, however, is not necessarily going to generalize to other spiders. Although a basic statistics class might tell you that only
by seeing many instances of a kind can we learn with confidence what features are constant or variable across that kind, both intuitively and
empirically in children’s cognitive development it seems that this “one-shot learning” is more the norm. How can this work? Hierarchical models
show us how to formalize the abstract knowledge that enables one-shot learning, and the means by which that abstract knowledge is itself
acquired (Kemp et al., 2007).

We can study a simple version of this phenomenon by modifying our bags of marbles example, articulating more structure to the hierarchical model
as   follows.   We   now   have   two   higher-level   parameters:  phi  describes   the   expected   proportions   of   marble   colors   across   bags   of   marbles,
while  alpha , a real number, describes the strength of the learned prior – how strongly we expect any newly encountered bag to conform to the
distribution for the population prototype  phi . For instance, suppose that we observe that  bag1  consists of all blue marbles,  bag2  consists of all
green marbles,  bag3  all red, and so on. This doesn’t tell us to expect a particular color in future bags, but it does suggest that bags are very
regular—that all bags consist of marbles of only one color.

var colors = ['black', 'blue', 'green', 'orange', 'red'];

var observedData = [
{bag: 'bag1', draw: 'blue'}, {bag: 'bag1', draw: 'blue'}, {bag: 'bag1', draw: 'blue'},
{bag: 'bag1', draw: 'blue'}, {bag: 'bag1', draw: 'blue'}, {bag: 'bag1', draw: 'blue'},
{bag: 'bag2', draw: 'green'}, {bag: 'bag2', draw: 'green'}, {bag: 'bag2', draw: 'green'},
{bag: 'bag2', draw: 'green'}, {bag: 'bag2', draw: 'green'}, {bag: 'bag2', draw: 'green'},
{bag: 'bag3', draw: 'red'}, {bag: 'bag3', draw: 'red'}, {bag: 'bag3', draw: 'red'},
{bag: 'bag3', draw: 'red'}, {bag: 'bag3', draw: 'red'}, {bag: 'bag3', draw: 'red'},
{bag: 'bag4', draw: 'orange'}]

var predictives = Infer({method: 'MCMC', samples: 30000}, function(){
  // the global prototype mixture:
  var phi = dirichlet(ones([colors.length, 1]))
  // regularity parameters: how strongly we expect the global prototype to project
  // (ie. determine the local prototypes):
  var alpha = gamma(2,2)
  var prototype = T.mul(phi, alpha)

  var makeBag = mem(function(bag){
    var colorProbs = dirichlet(prototype)
    return Categorical({vs: colors, ps: colorProbs})
  })

  var obsFn = function(datum){
    observe(makeBag(datum.bag), datum.draw)
  }

  mapData({data: observedData}, obsFn)

  return {bag1: sample(makeBag('bag1')), bag2: sample(makeBag('bag2')),
          bag3: sample(makeBag('bag3')), bag4: sample(makeBag('bag4')),
          bagN: sample(makeBag('bagN')),
          alpha: alpha}
})

viz.marginals(predictives)
run▼

Consider the fourth bag, for which only one marble has been observed (orange): we see a very strong posterior predictive distribution focused on
orange – a “one-shot” generalization. This posterior is much stronger than the single observation for that bag can justify on its own. Instead, it
reflects the learned overhypothesis that bags tend to be uniform in color.

To see that this is real one-shot learning, contrast with the predictive distribution for a new bag with no observations:   bagN  gives a mostly flat
distribution. Little has been learned in the hierarchical model about the specific colors represented in the overall population; rather we have
learned the abstract property that bags of marbles tend to be uniform in color. Hence, a single observation from a new bag is enough to make
strong predictions about that bag even though little could be said prior to seeing the first observation.

We have also generated the posterior distribution on  alpha , representing how strongly the prototype distribution captured in  phi , constrains
each individual bag – how much each individual bag is expected to look like the prototype of the population. You should see that the inferred
values of  alpha  are typically significantly less than 1. This means roughly that the learned prototype in  phi  should exert less influence on
prototype estimation for a new bag than a single observation. Hence the first observation we make for a new bag mostly determines a strong
inference about what that bag is like.

Now we change the  observedData  :

var observedData = [
{bag: 'bag1', draw: 'blue'}, {bag: 'bag1', draw: 'red'}, {bag: 'bag1', draw: 'green'},
{bag: 'bag1', draw: 'black'}, {bag: 'bag1', draw: 'red'}, {bag: 'bag1', draw: 'blue'},
{bag: 'bag2', draw: 'green'}, {bag: 'bag2', draw: 'red'}, {bag: 'bag2', draw: 'black'},
{bag: 'bag2', draw: 'black'}, {bag: 'bag2', draw: 'blue'}, {bag: 'bag2', draw: 'green'},
{bag: 'bag3', draw: 'red'}, {bag: 'bag3', draw: 'green'}, {bag: 'bag3', draw: 'blue'},
{bag: 'bag3', draw: 'blue'}, {bag: 'bag3', draw: 'black'}, {bag: 'bag3', draw: 'green'},
{bag: 'bag4', draw: 'orange'}]

///fold:
...

viz.marginals(predictives)
run▼

Intuitively, the observations for bags one, two and three should now suggest a very different overhypothesis: that marble color, instead of
being homogeneous within bags but variable across bags, is instead variable within bags to about the same degree that it varies in the population as
a whole. We can see this inference represented via two coupled effects. First, the inferred value of  alpha  is now significantly greater than 1,
asserting that the population distribution as a whole,  phi , now exerts a strong constraint on what any individual bag looks like. Second, for a
new  'bag4'  which has been observed only once, with a single orange marble, that draw is now no longer very influential on the color distribution
we expect to see from that bag; the broad distribution in  phi  exerts a much stronger influence than the single observation.

Example: The Shape Bias

One well studied overhypothesis in cognitive development is the ‘shape bias’: the inductive bias which develops by 24 months and which is the
preference to generalize a novel label for some object to other objects of the same shape, rather than say the same color or texture. Studies
by Smith and colleagues (Smith et al., 2002) have shown that this bias can be learned with very little data. They trained 17 month old children,
over eight weeks, on four pairs of novel objects where the objects in each pair had the same shape but differed in color and texture and were
consistently given the same novel name. First order generalization was tested by showing children an object from one of the four trained
categories and asking them to choose another such object from three choice objects that matched the shown object in exactly one feature.
Children preferred the shape match. Second order generalization was also tested by showing children an object from a novel category and again
children preferred the choice object which matched in shape. Smith and colleagues further found an increase in real-world vocabulary as a
result of this training such that children who had been trained began to use more object names. Children had thus presumably learned something
like ‘shape is homogeneous within object categories’ and were able to apply this inductive bias to word learning outside the lab.

We now consider a model of learning the shape bias which uses the compound Dirichlet-Discrete model that we have been discussing in the context
of bags of marbles. This model for the shape bias is from (Kemp et al., 2007). Rather than bags of marbles we now have object categories and
rather than observing marbles we now observe the features of an object (e.g. its shape, color, and texture) drawn from one of the object
categories. Suppose that a feature from each dimension of an object is generated independently of the other dimensions and there are separate

values of alpha and phi for each dimension. Importantly, one needs to allow for more values along each dimension than appear in the training data
so as to be able to generalize to novel shapes, colors, etc. To test the model we can feed it training data to allow it to learn the values for the
alphas and phis corresponding to each dimension. We can then give it a single instance of some new category and then ask what the probability is
that the various choice objects also come from the same new category. The WebPPL code below shows a model for the shape bias, conditioned on the
same training data used in the Smith et al experiment. We can then ask both for draws from some category which we’ve seen before, and from
some new category which we’ve seen a single instance of. One small difference from the previous models we’ve seen for the example case is
that the alpha hyperparameter is now drawn from an exponential distribution with inverse mean 1, rather than a Gamma distribution. This is simply
for consistency with the model given in the Kemp et al (2007) paper.

var attributes = ['shape', 'color', 'texture', 'size'];
var values = {shape:  .range(

11), color:  .range(
_

_

11), texture:  .range(

_

11), size:  .range(
_

11)};

var observedData = [{cat: 'cat1', shape: 1, color: 1, texture: 1, size: 1},
                    {cat: 'cat1', shape: 1, color: 2, texture: 2, size: 2},
                    {cat: 'cat2', shape: 2, color: 3, texture: 3, size: 1},
                    {cat: 'cat2', shape: 2, color: 4, texture: 4, size: 2},
                    {cat: 'cat3', shape: 3, color: 5, texture: 5, size: 1},
                    {cat: 'cat3', shape: 3, color: 6, texture: 6, size: 2},
                    {cat: 'cat4', shape: 4, color: 7, texture: 7, size: 1},
                    {cat: 'cat4', shape: 4, color: 8, texture: 8, size: 2},
                    {cat: 'cat5', shape: 5, color: 9, texture: 9, size: 1}]

var categoryPosterior = Infer({method: 'MCMC', samples: 10000}, function(){

  var prototype = mem(function(attr){
    var phi = dirichlet(ones([values[attr].length, 1]))
    var alpha = exponential(1)
    return T.mul(phi,alpha)
  })

  var makeAttrDist = mem(function(cat, attr){
    var probs = dirichlet(prototype(attr))
    return Categorical({vs: values[attr], ps: probs})
  })

  var obsFn = function(datum){
    map(function(attr){observe(makeAttrDist(datum.cat,attr), datum[attr])},
        attributes)
  }

  mapData({data: observedData}, obsFn)

  return {cat5shape: sample(makeAttrDist('cat5','shape')),
          cat5color: sample(makeAttrDist('cat5','color')),
          catNshape: sample(makeAttrDist('catN','shape')),
          catNcolor: sample(makeAttrDist('catN','color'))}
})

viz.marginals(categoryPosterior)
run▼

The program above gives us draws from some novel category for which we’ve seen a single instance. In the experiments with children, they had
to choose one of three choice objects which varied according to the dimension they matched the example object from the category. We show below
model predictions (from Kemp et al (2007)) for performance on the shape bias task which show the probabilities (normalized) that the choice
object belongs to the same category as the test exemplar. The model predictions reproduce the general pattern of the experimental results of
Smith et al in that shape matches are preferred in both the first and second order generalization case, and more strong in the first order
generalization case. The model also helps to explain the childrens’ vocabulary growth in that it shows how the shape bias can be generally
learned, as seen by the differing values learned for the various alpha parameters, and so used outside the lab.

The model can be extended to learn to apply the shape bias only to the relevant ontological kinds, for example to object categories but not to
substance categories. The Kemp et al (2007) paper discusses such an extension to the model which learns the hyperparameters separately for
each kind and further learns what categories belong to each kind and how many kinds there are. This involves the use of a non-parametric prior,
called the Chinese Restaurant Process, which will be discussed in the section on non-parametric models.

Example: Beliefs about Homogeneity and 
Generalization

In a 1983 paper, Nisbett and colleagues (Nisbett et al., 1983) examined how, and under what conditions, people made use of statistical heuristics
when reasoning. One question they considered was how and when people generalized from a few instances. They showed that to what extent
people generalise depends on beliefs about the homogeneity of the group that the object falls in with respect to the property they are being
asked to generalize about. In one study, they asked subjects the following question:

Imagine that you are an explorer who has landed on a little known island in the Southeastern Pacific. You encounter several
new animals, people, and objects. You observe the properties of your “samples” and you need to make guesses about how
common these properties would be in other animals, people, or objects of the same type.

The number of encountered instances of an object were varied (one, three, or twenty instances) as well as the type and property of the
objects. For example:

Suppose you encounter a native, who is a member of a tribe he calls the Barratos. He is obese. What percent of the male
Barratos do you expect to be obese?
and
Suppose the Barratos man is brown in color. What percent of male Barratos do you expect to be brown (as opposed to red,
yellow, black or white)?

Results for two questions of the experiment are shown below. The results accord both with the beliefs of the experimenters about how
heterogeneous different groups would be, and subjects stated reasons for generalizing in the way they did for the different instances (which
were coded for beliefs about how homogeneous objects are with respect to some property).

Again, we can use the compound Dirichlet-multinomial model we have been working with throughout to model this task, following Kemp et al
(2007). In the context of the question about members of the Barratos tribe, replace bags of marbles with tribes and the color of marbles with
skin color, or the property of being obese. Observing data such that skin color is consistent within tribes but varies between tribes will cause a

low value of the alpha corresponding to skin color to be learned, and so seeing a single example from some new tribe will result in a sharply
peaked predictive posterior distribution for the new tribe. Conversely, given data that obesity varies within a tribe the model will learn a
higher value of the alpha corresponding to obesity and so will not generalize nearly as much from a single instance from a new tribe. Note that
again it’s essential to have learning at the level of hyperparameters in order to capture this phenomenon. It is only by being able to learn
appropriate values of the hyperparameters from observing a number of previous tribes that the model behaves reasonably when given a single
observation from a new tribe.

Example: One-shot learning of visual categories

Humans are able to categorize objects (in a space with a huge number of dimensions) after seeing just one example of a new category. For example,
after seeing a single wildebeest people are able to identify other wildebeest, perhaps by drawing on their knowledge of other animals. The
model in Salakhutdinov et al (Salakhutdinov et al., 2010) uses abstract knowledge learned from other categories as a prior on the mean and
covariance matrix of new categories.

Suppose, first that the model is given an assignment of objects to basic categories and basic categories to superordinate categories. Objects are
represented as draws from a multivariate Gaussian and the mean and covariance of each basic category is determined by hyperparameters attached
to   the   corresponding   superordinate   category.   The   parameters   of   the   superordinate   categories   are   all   drawn   from   a   common   set   of
hyperparameters.

The   model   in   the   Salakhutdinov   et   al   (2010)   paper   is   not   actually   given   the   assignment   of   objects   to   categories   and   basic   categories   to
superordinate categories, but  rather learns this  from the data by putting a non-parametric  prior  over  the  tree of object  and category
assignments.

Example: X-Bar Theory

(This example comes from an unpublished manuscript by O’Donnell, Goodman, and Katzir)

One of the central problems in generative linguistics has been to account for the ease and rapidity with which children are able to acquire their
language from noisy, incomplete, and sparse data. One suggestion for how this can happen is that the space of possible natural languages
varies parametrically. The idea is that there are a number of higher-order constraints on structure that massively reduce the complexity of
the learning problem. Each constraint is the result of a parameter taking on one of a small set of values. (This is known as “principles and
parameters” theory.) The child needs only see enough data to set these parameters and the details of construction-specific structure will
then generalize across the rest of the constructions of their language.

One example, is the theory of headedness and X-bar phrase structure (Chomsky, 1970). X-bar theory provides a hierarchical model for phrase
structure. All phrases follow the same basic template:

XP⟶Spec X′XP⟶Spec X′ X′⟶X CompX′⟶X Comp
Where XX is a lexical (or functional) category such as NN (noun), VV (verb), etc. X-bar theory proposes that all phrase types have the same 
basic “internal geometry”; They have a head – a word of category XX. They also have a specifier (SpecSpec) and a complement ($Comp$), the 
complement is more closely associated with the head than the specifier. The set of categories that can appear as complements and specifiers for a
particular category of head is usually thought to be specified by universal grammar (but may also vary parametrically).
An important way in which languages vary is the order in which heads appear with respect to their complements (and specifiers). Within a language
there tends to be a dominant order, often with exceptions for some category types. For instance, English is primarily a head-initial language. In
verb phrases, for example, the direct object (complement noun phrase) of a verb appears to the right of the head. However, there are

exceptional cases such as the order of (simple) adjective and nouns: adjectives appear before the noun rather than after it (although more
complex complement types such as relative clauses appear after the noun).

The fact that languages show consistency in head directionality could be of great advantage to the learner; after encountering a relatively
small number of phrase types and instances the learner of a consistent language can learn the dominant head direction in their language,
transferring this knowledge to new phrase types. The fact that within many languages there are exceptions suggests that this generalization
cannot be deterministic, however, and, furthermore means that a learning approach will have to be robust to within-language variability. Here is
a highly simplified model of X-Bar structure:

///fold:
...
// the "grammar": a set of phrase categories, and an associating of the complement to each head category:
var categories = ['D', 'N', 'T', 'V', 'A', 'Adv']

var headToComp = function(head) {
  return (head == 'D' ? 'N' :
          head == 'T' ? 'V' :
          head == 'N' ? 'A' :
          head == 'V' ? 'Adv' :
          head == 'A' ? 'none' :
          head == 'Adv' ?'none' :
          'error');
}

var makePhraseDist = function(headToPhraseDirs) {
  return Infer({method: 'enumerate'}, function(){
    var head = uniformDraw(categories);
    if(headToComp(head) == 'none') {
      return [head]
    } else {
      // On which side will the head go?
      return flip(headToPhraseDirs[head]) ? [headToComp(head), head] : [head, headToComp(head)];
    }
  })
}

var data = [['D', 'N']];

var results = Infer({method: 'MCMC', samples: 20000}, function() {
  var languageDir = beta(1,1);
  var headToPhraseDirs =  .zipObject(categories, map(
_
    return T.get(dirichlet(Vector([languageDir, 1 - languageDir])), 1)
  }, categories))

function() {

  var phraseDist = makePhraseDist(headToPhraseDirs);
  factor(observePhrase(phraseDist, data))

  return flip(headToPhraseDirs['N']) ? 'N second' : 'N first';
})

viz.auto(results)
run▼

First, try increasing the number of copies of  ['D', 'N']  observed. What happens? Now, try changing the data to  [['D', 'N'], ['T', 'V'], ['V', 'Adv']] .
What happens if you condition on additional instance of  ['V', 'Adv'] ? How about  ['Adv', 'V'] ?

What we see  in this example is a simple probabilistic model  capturing  a version  of the “principles  and parameters” theory. Because it is
probabilistic, systematic inferences will be drawn despite exceptional sentences or even phrase types. More importantly, due to the blessing of
abstraction, the overall headedness of the language can be inferred from very little data—before the learner is very confident in the
headedness of individual phrase types.

Thoughts on Hierarchical Models

We have just seen several examples of hierarchical Bayesian models: generative models in which there are several levels of latent random
choices  that affect the observed data. In particular a hierarchical model is usually one  in  which  there is a branching structure in the

dependence diagram, such that the “deepest” choices affect all the data, but they only do so through a set of more shallow choices which each
affect some of the data, and so on.

Hierarchical model structures give rise to a number of important learning phenomena: transfer learning (or learning-to-learn), the blessing of
abstraction, and learning curves with fairly abrupt transitions. This makes them important for understanding human learning, as well as useful
for creating artificial intelligence that makes the best use of available data.

Hierarchical Abstraction versus Lambda Abstraction

We have spoken of the earlier choices in a hierarchical model as being more “abstract.” In computer science the  function  operator is often called
lambda abstraction. Are these two uses of “abstract” related?

There is a third notion of abstraction in a generative model which may explain the relation between these two: if we have a designated set of
observations (or more generally a function that we think of as generating “perceptual” data) we can say that a random choice is abstract if it is
far from the data. More specifically the degree of abstraction of an expression in a probabilistic program is the number of immediate causal
dependencies (edges) from the expression to the designated observation expression (note that this is a partial, not strict, ordering on the random
choices).

In a hierarchically structured model the deeper random choices are more abstract in this sense of causal distance from the data. More subtly,
when a procedure is created with  function  the expressions inside this procedure will tend to be more causally distant from the data (since the
procedure must be applied before these expressions can be used), and hence greater depth of lambda abstraction will tend to lead to greater
abstraction in the causal distance sense.

Test your knowledge: Exercises

Reading & Discussion: Readings
Next chapter: 12. Occam's Razor 

☰

Occam's Razor

Entities should not be multiplied without necessity. –William of Ockham

In learning and perceiving we are fitting models to the data of experience. Typically our hypothesis space will span models varying greatly in
complexity: some models will have many more free parameters or degrees of freedom than others. Under traditional approaches to model fitting
we adjust each model’s parameters until it fits best, then choose the best-fitting model; a model with strictly more free parameters will tend
to be preferred regardless of whether it actually comes closer to describing the true processes that generated the data. It then often
generalizes less well – this is called over fitting.

But this is not the way the mind works. Humans assess models with a natural eye for complexity, balancing fit to the data with model complexity
in subtle ways that will not inevitably prefer the most complex model. Instead we often seem to judge models using Occam’s razor: we choose the
least complex hypothesis that fits the data well. In doing so we avoid over-fitting our data in order to support successful generalizations and
predictions.

Fitting curves (or smooth functions) to sparsely sampled, noisy data provides a familiar example of the problem.

The figure above shows three polynomial fits: a 1st-order (linear), a 2nd-order (quadratic), and a 12th-order polynomial. How do our minds
decide which of these functions provides the best account of the data? The 1st-order model captures the rough trend of the data but seems too
coarse; it attributes some of the variation that we see as “signal” to “noise”. The 2nd-order model seems best; it seems to be just complex
enough to fit the main shape of the data without over-fitting the noise. The 12th-order model seems ridiculously over-fit; with 13 data points,
the parameters can be adjusted so as to make the curve pass exactly through every data point, thus taking as “signal” all of what we see as
“noise”. Again we can think of this as a causal inference problem: each function represents a hypothesis about the causal process that gave rise
to the observed data, including the shape of the function from inputs to outputs, as well as the level of noise added to the output.

Occam’s Razor

An elegant and powerful form of Occam’s razor arises in the context of Bayesian inference, known as the Bayesian Occam’s razor. Bayesian
Occam’s razor refers to the fact that “more complex” hypotheses about the data are penalized automatically in conditional inference. In many
formulations of Occam’s razor, complexity is measured syntactically: for instance, it may be the description length of the hypothesis in some
representation language, or a count of the number of free parameters used to specify the hypothesis. Syntactic forms of Occam’s razor have
difficulty justifying the complexity measure on principled, non-arbitrary grounds. They also leave unspecified exactly how the weight of a
complexity penalty should trade off with a measure of fit to the data. Fit is intrinsically a semantic notion, a matter of correspondence between
the model’s predictions and our observations of the world. When complexity is measured syntactically and fit is measured semantically, they are
intrinsically incommensurable and the trade-off between them will always be to some extent arbitrary.

In the Bayesian Occam’s razor, both complexity and fit are measured semantically. The semantic notion of complexity is a measure of  flexibility: a
hypothesis that is flexible enough to generate many different sets of observations is more complex, and will tend to receive lower posterior
probability than a less flexible hypothesis that explains the same data. Because more complex hypotheses can generate a greater variety of data
sets, they must necessarily assign a lower probability to each one. When we condition on some data, all else being equal, the posterior distribution
over the hypotheses will favor the simpler ones because they have the tightest fit to the observations.

From the standpoint of a probabilistic programming language, the Bayesian Occam’s razor is essentially inescapable. We do not judge models based on
their best fitting behavior but rather on their average behavior. No fitting per se occurs during conditional inference. Instead, we draw
conditional samples from each model representing the model’s likely ways of generating the data. A model that tends to fit the data well on
average – to produce relatively many generative histories with that are consistent with the data – will do better than a model that can fit
better for certain parameter settings but worse on average.

The Law of Conservation of Belief

It is convenient to emphasize an aspect of probabilistic modeling that seems deceptively trivial, but comes up repeatedly when thinking about
inference. In Bayesian statistics we think of probabilities as being degrees of belief. Our generative model reflects world knowledge and the
probabilities that we assign to the possible sampled values reflect how strongly we believe in each possibility. The laws of probability theory
ensure that our beliefs remain consistent as we reason.

A consequence of belief maintenance is known as the Law of Conservation of Belief. Here are two equivalent formulations of this principle:

1.Sampling from a distribution selects exactly one possibility (in doing so it implicitly rejects all other possible values).

2.The total probability mass of a distribution must sum to 1. That is, we only have a single unit of belief to spread around.

The latter formulation leads to a common metaphor in discussing generative models: We can usefully think of belief as a “currency” that is
“spent” by the probabilistic choices required to construct a sample. Since each choice requires “spending” some currency, an outcome that
requires more choices to construct it will generally be more costly, i.e. less probable.

It is this conservation of belief that gives rise to the Bayesian Occam’s razor. A hypothesis that spends its probability on many alternatives that
don’t explain the current data will have less probability for the alternatives that do, and will hence do less well overall than a hypothesis
which only entertains options that fit the current data. We next examine a special case where this tradeoff plays out clearly, the  size
principle, then come back to the more general cases.

The Size Principle

A  simple   case   of   Bayes   Occam’s  razor   comes   from  the size   principle (Tenenbaum  and  Griffiths,  2001):  Of   hypotheses  which   generate   data
uniformly, the one with smallest extension that is still consistent with the data is the most probable.

The following program demonstrates the size principle with a very simple model. Here we have two hypothesized sets:   Big  has 6 elements
and  Small  has 3 elements. The generative model chooses one of the hypotheses at random and samples some number of symbols from it uniformly.
We then wish to infer the hypothesis given observed elements.

var hypothesisToDist = function(hyp) {
  return (hyp == 'Big' ?
          Categorical({vs: ['a', 'b', 'c', 'd', 'e', 'f'], ps: [1/6, 1/6, 1/6, 1/6, 1/6, 1/6]}) :
          Categorical({vs: ['a', 'b', 'c'], ps: [1/3, 1/3, 1/3]}));
}

var data = ['a']

var post = Infer({method: 'enumerate'}, function(){
  var hypothesis = flip() ? 'Big' : 'Small'
  mapData({data: data}, function(d){observe(hypothesisToDist(hypothesis),d)})
  return hypothesis
})
viz(post)
run▼

With a single observed  a , we already favor hypothesis  Small . What happens when we increase the amount of observed data? Consider the
learning trajectory:

///fold:
...
var hypothesisPosterior = function(data) {
  return Infer({method: 'enumerate'}, function(){
    var hypothesis = flip() ? 'Big' : 'Small'
    mapData({data: data}, function(d){observe(hypothesisToDist(hypothesis),d)})
    return hypothesis=='Big'
  })
};

var fullData = ['a', 'b', 'a', 'b', 'b', 'a', 'b']
var dataSizes = [0,1,3,5,7]

var probBig = map(function(size) {expectation(hypothesisPosterior(fullData.slice(0,size)))},
                  dataSizes)
viz.line(dataSizes, probBig, {xLabel: 'numDataPoints', yLabel: 'P(Big | data)'})
run▼

As the number of data points increases, the hypothesis  Small  rapidly comes to dominate the posterior distribution. Why is this happening? 
Observations are distributed uniformly over the hypothesized set, the law of conservation of belief and the symmetry between observations 
imply that the probability of a draw from  Big  is 1616, while the probability of a draw from  Small  is 1313. Thus, by the product rule of 
probabilities, the probability of drawing a set of N observations from  Big  is (16)N(16)N, while the probability of drawing a set of observations 
from  Small  is (13)N(13)N. The later probability decreases much more slowly than the former as the number of observations increases. Using 
Bayes’ rule, the posterior distribution over hypotheses is given by:

P(hypothesis∣observations)∝P(observations∣hypothesis)P(hypothesis)P(hypothesis∣observations)∝P(observations∣hypothesis)P(hypothesis)

Because our hypotheses are equally probable a priori, this simplifies to:

P(hypothesis∣observations)∝P(observations∣hypothesis)P(hypothesis∣observations)∝P(observations∣hypothesis)

So we see that the the posterior distribution over hypotheses in this case is just the normalized 
likelihood P(observations∣hypothesis)P(observations∣hypothesis). The likelihood 
ratio, P(observations∣Big)/P(observations∣Small)=(12)NP(observations∣Big)/P(observations∣Small)=(12)N, determines how quickly the simpler 
hypothesis  Small  comes to dominate the posterior.
One way to understand the Size Principle is that probabilistic inference takes into account implicit negative evidence. More flexible hypotheses
could have generated more observations. Thus if those hypotheses were the true hypotheses we would expect to see a greater variety of
observations. If the data does not contain them, this is a form of negative evidence against those hypotheses. Importantly, the Size Principle tells
us that the prior distribution on hypotheses does not have to penalize complexity. The complexity of the hypothesis itself will lead to its being
disfavored in the posterior distribution.

The size principle is related to an influential proposal in linguistics known as the subset principle. Intuitively, the subset principle suggests that
when two grammars both account for the same data, the grammar that generates a smaller language should be preferred. (The name “subset
principle” was originally introduced by Bob Berwick to refer to a slightly different result.)

Example: The Rectangle Game

To illustrate the power of the size principle in inferring the most parsimonious explanation of data, consider learning a rectangle “concept”
(Tenenbaum, 2000). The data are a set of points in the plane, that we assume to be randomly sampled from within some unknown rectangle. Given
the examples, what is the rectangle they came from? We can model this learning as a conditional of the rectangle given the points. Here we plot
the sampled rectangles to give a sense of the distribution:

var observedData = [{x: 0.40, y: 0.70}, {x: 0.50, y: 0.40}, {x: 0.46, y: 0.63}, {x: 0.43, y: 0.51}]

var post = Infer(
  {method: 'MCMC', samples: 150, lag: 100, burn: 100},
  function () {
    var x1 = uniform(0, 1), x2 = uniform(0, 1)
    var y1 = uniform(0, 1), y2 = uniform(0, 1)
    mapData({data: observedData}, function(example) {
      observe(Uniform({a: x1, b: x2}), example.x)
      observe(Uniform({a: y1, b: y2}), example.y)
    })
    return {x1: x1, x2: x2, y1: y1, y2: y2}
  }
)

var img = Draw(500, 500, true);

// draw rectangles
map(function(r) {
  var r = r.value
  img.rectangle(r.x1 * 500, r.y1 * 500, r.x2 * 500, r.y2 * 500, '#99ccff', '#99ccff', 0.01)
}, post.samples)

// draw data points
map(function(obs) {
  img.circle(obs.x * 500, obs.y * 500, 5, 'black', 'black')
}, observedData);
run▼

Explore how the concept learned varies as a function of the number and distribution of example points. Try varying the observed data and seeing
how the inferred rectangle changes:

var observedData = [{x: 0.20, y: 0.60}, {x: 0.20, y: 0.80}, {x: 0.40, y: 0.80}, {x: 0.40, y: 0.60}, {x: 0.30, y: 0.70}]

var observedData = [{x: 0.40, y: 0.70}, {x: 0.50, y: 0.40}, {x: 0.45, y: 0.50}, {x: 0.43, y: 0.70}, {x: 0.47, y: 0.60}]

var observedData = [{x: 0.40, y: 0.70}, {x: 0.50, y: 0.40}]

var observedData = [{x: 0.40, y: 0.70}, {x: 0.50, y: 0.40}, {x: 0.46, y: 0.63},

                    {x: 0.43, y: 0.51}, {x: 0.42, y: 0.45}, {x: 0.48, y: 0.66}]

Compare this to the results of a slightly different causal process for the same observations, known as weak sampling:

var observedData = [{x: 0.40, y: 0.70, polarity: '+'},{x: 0.50, y: 0.40, polarity: '+'},
                    {x: 0.46, y: 0.63, polarity: '+'},{x: 0.43, y: 0.51, polarity: '+'}]

var post = Infer(
  {method: 'MCMC', samples: 150, lag: 100, burn: 100},
  function () {
    var x1 = uniform(0, 1), x2 = uniform(0, 1)
    var y1 = uniform(0, 1), y2 = uniform(0, 1)

    // the concept is now a rule for classifying points as in or out of this rectangle
    var classifier = function(x,y) {
     return (x > x1 && x < x2 && y > y1 && y < y2) ? '+' : '-';
    }

    mapData({data: observedData},function(example) {
      condition(classifier(example.x, example.y) == example.polarity)
    });

    return {x1: x1, x2: x2, y1: y1, y2: y2};
  }
)

var img = Draw(500, 500, true);

// draw rectangles
map(function(r) {
  var r = r.value
  img.rectangle(r.x1 * 500, r.y1 * 500, r.x2 * 500, r.y2 * 500, '#99ccff', '#99ccff', 0.01)
}, post.samples)

// draw data points
map(function(obs) {
  img.circle(obs.x * 500, obs.y * 500, 5, 'black', 'black')
}, observedData)
run▼

Generalizing the Size Principle: Bayes Occam’s Razor

In our example above we have illustrated Bayes Occam’s razor with examples based strictly on the “size” of the hypotheses involved, however,
the principle is more general. The Bayesian Occam’s razor says that all else being equal the hypothesis that assigns the highest likelihood to the
data will dominate the posterior. Because of the law of conservation of belief, assigning higher likelihood to the observed data requires assigning
lower likelihood to other possible data. Consider the following example

var hypothesisToDist = function(hypothesis) {
  return (hypothesis == 'A' ?
          Categorical({vs: ['a', 'b', 'c', 'd'], ps: [0.375, 0.375, 0.125, 0.125]}) :
          Categorical({vs: ['a', 'b', 'c', 'd'], ps: [0.25, 0.25, 0.25, 0.25]}))
}

var observedData = ['a', 'b', 'a', 'b', 'c', 'd', 'b', 'b']

var posterior = Infer({method: 'enumerate'}, function(){
  var hypothesis = flip() ? 'A' : 'B'
  mapData({data: observedData}, function(d){observe(hypothesisToDist(hypothesis),d)})
  return hypothesis
})

viz(posterior)
run▼

In this example, unlike the size principle cases above, both hypotheses lead to the same possible observed values. However, hypothesis A is
skewed toward examples a and b – while it can produce c or d, it is less likely to do so. In this sense hypothesis A is less flexible than hypothesis
B. The data set we conditioned on also has exemplars of all the elements in the support of the two hypotheses. However, because there are
more  exemplars  of elements  favored  by hypothesis  A , this  hypothesis   is  favored  in  the  posterior.  The  Bayesian   Occam’s  razor  emerges
naturally here.

This examples suggest another way to understand Bayes Occam’s razor: the posterior distribution will favor hypotheses for which the data set is
simpler in the sense that it is more “easily generated.” Here more “easily” generated, means generated with higher probability. We will see a
more striking example of this for compositional models at the end of this section of the tutorial.

Model selection with the Bayesian Occam’s Razor

The law of conservation of belief turns most clearly into Occam’s Razor when we consider models with more internal structure: some continuous
or   discrete   parameters   that   at   different   settings   determine   how   likely   the   model   is   to   produce   data   that   look   more   or   less   like   our
observations. To select among models we simply need to describe each model as a probabilistic program, and also to write a higher-level program
that generates these hypotheses.  Infer  will then automatically explore both of these levels of abstraction, asking which models are most likely
to have given rise to the observed data, as well as for each of those models, which internal parameter settings are most likely.

Example: Fair or unfair coin?

In a previous chapter we considered learning about the weight of a coin, and noted that a simple prior on weights seemed unable to capture our
more discrete intuition – that we first decide if the coin is fair or not, and only then worry about its weight. This example shows how our
inferences about coin flipping can be explained in terms of model selection guided by the Bayesian Occam’s razor. Imagine a coin that you take out
of a freshly unwrapped roll of quarters straight from the bank. Almost surely this coin is fair… But how does that sense change when you see
more or less anomalous sequences of flips? We can simultaneously ask if the coin is fair, and what is its weight.

// fair coin, probability of H = 0.5
var observedData = ['h', 'h', 't', 'h', 't', 'h', 'h', 'h', 't', 'h']

// ?? suspicious coincidence, probability of H = 0.5 ..?
// var observedData = repeat(10, function(){return 'h'})

// probably unfair coin, probability of H near 1
// var observedData = repeat(15, function(){return 'h'})

// definitely unfair coin, probability of H near 1
// var observedData = repeat(20, function(){return 'h'})

// unfair coin, probability of H = 0.85
// var observedData = repeat(54, function(){return flip(.85) ? 'h' : 't'})

var fairPrior = .999;
var pseudoCounts = {a: 1, b: 1}

var results = Infer({method: 'MCMC', samples: 10000}, function(){
  var fair = flip(fairPrior);
  var coinWeight = fair ? .5 : beta(pseudoCounts.a, pseudoCounts.b);
  mapData({data: observedData}, function(d){observe(Bernoulli({p: coinWeight}), d==='h')})
  return {prior: flip(fairPrior) ? .5 : beta(pseudoCounts.a, pseudoCounts.b),
          fair : fair,
          weight: coinWeight}
})

viz.marginals(results)
run▼

Try some of the observation sets that we’ve commented out above and see if the inferences accord with your intuitions.

Now let’s look at the learning trajectories for this model:

var fairPrior = .999;
var pseudoCounts = {a: 1, b: 1}

var inferOpts = {method: 'optimize', samples: 1000, steps: 1000, optMethod: {adam: {stepSize: .01}}, verbose: false}
//can also use MCMC, but takes longer to achieve stable results:
// var inferOpts = {method: 'MCMC', samples: 20000, burn: 1000}

var post = function(data) {
  return Infer(inferOpts, function(){
    var fair = flip(fairPrior)
    var coinWeight = fair ? .5 : beta(pseudoCounts.a, pseudoCounts.b);
    mapData({data: data}, function(d){observe(Bernoulli({p: coinWeight}), d==='h')})
    return {fair : fair, weight: coinWeight}
  })
}

var trueCoin = function(){flip(0.9)?'h':'t'}
var fullDataSet = repeat(100, trueCoin)
var dataSizes = [0,1,3,6,10,20,30,40,50,60,70,100]
var predictions = map(function(dataSize) {
  return expectation(post(fullDataSet.slice(0,dataSize)), function(x){return x.weight})
}, dataSizes)
viz.line(dataSizes, predictions)
run▼

In general (though not on every run) the learning trajectory stays near 0.5 initially—favoring the simpler hypothesis that the coin is fair—then
switches fairly abruptly to near 0.9—as it infers that it is an unfair coin and likely has high weight. Here the Bayesian Occam’s Razor penalizes
the hypothesis with the flexibility to learn any coin weight until the data overwhelmingly favor it.

The Effect of Unused Parameters

When statisticians suggest methods for model selection, they often include a penalty for the  number of parameters (e.g., AIC). This seems like a
worrying policy from the point of view of a probabilistic program: we could always introduce parameters that are not used, and therefore have no
effect on the program. For instance we could change the above coin flipping example so that it draws the potential unfair coin weight even in the
model which gives a fair coin:

var fairPrior = .999;
var pseudoCounts = {a: 1, b: 1}

var observedData = ['h']

var results = Infer({method: 'MCMC', samples: 10000}, function(){
  var fair = flip(fairPrior)
  var unfairWeight = beta(pseudoCounts.a, pseudoCounts.b)
  var coinWeight = fair ? 0.5 : unfairWeight
  mapData({data: observedData}, function(d){observe(Bernoulli({p: coinWeight}), d==='h')})
  return {fair : fair, weight: coinWeight}
})

viz.marginals(results)
run▼

The two models now have the same number of free parameters (the unfair coin weight), but we will still favor the simpler hypothesis, as we did
above. Why? The Bayesian Occam’s razor penalizes models not for having more parameters (or longer code) but for too much flexibility – being
able to fit too many other potential observations. Unused parameters (or parameters with very little effect) don’t increase this flexibility,
and hence aren’t penalized. The Bayesian Occam’s razor only penalizes complexity that matters for prediction, and only to the extent that it
matters.

Example: Curve Fitting

This example shows how the Bayesian Occam’s Razor can be used to select the right order of a polynomial fit.

// a0 + a1*x + a2*x^2 + ...

var makePoly = function(as) {
  return function(x) {
    return sum(mapIndexed(function(i,a) { return a * Math.pow(x, i) }, as))
  }
}

var observedData = [{"x":-4,"y":69.76636938284166},{"x":-3,"y":36.63586217969598},{"x":-2,"y":19.95244368751754},{"x":-
1,"y":4.819485497724985},{"x":0,"y":4.027631414787425},{"x":1,"y":3.755022418210824},{"x":2,"y":6.557548104903805},
{"x":3,"y":23.922485493795072},{"x":4,"y":50.69924692420815}]

var inferOptions = {method: 'optimize', samples: 1000, steps: 2000, optMethod: {adam: {stepSize: .01}}}
//can also use MCMC, but takes longer to achieve stable results:
//var inferOptions = {method: 'MCMC', samples: 400, lag: 200}

var post = Infer(inferOptions,
  function() {
    var coeffs = repeat(4, function() {return gaussian(0,2)})
    var order = discrete([0.25,0.25,0.25,0.25]) //uniformDraw([0,1,2,3])
    var f = makePoly(coeffs.slice(0,order+1))

    var obsFn = function(datum){
      observe(Gaussian({mu: f(datum.x), sigma: 2}), datum.y)
    }
    mapData({data: observedData}, obsFn)

    return {order: order,
            coeff0: coeffs[0],
            coeff1: coeffs[1],
            coeff2: coeffs[2],
            coeff3: coeffs[3]}
  }
)

print("observed data:")
viz.scatter(observedData)
print("degree and coefficient marginals:")
viz.marginals(post)
run▼

Try the above code using a different data set generated from the same function:

///fold:
...
var coeffs = repeat(4, function() {return gaussian(0,2)})
    var order = 2
    var f = makePoly(coeffs.slice(0,order+1))
var xs =  .range(-
4,4.1)   
_
var data = map(function(x){return {x:x,y:gaussian(f(x),2)}},xs)
viz.scatter(data)
print(data)
run▼

You can also try making the data set smaller, or generate data from a different order polynomial. How much data does it take tend to believe
the polynomial is third order?

Example: Scene Inference

Imagine you are in a world of colored blocks that typically looks something like this:

