Nature Human Behaviour | Volume 8 | April 2024 | 644–656
644
nature human behaviour
Article
https://doi.org/10.1038/s41562-024-01815-w
Human languages with greater information 
density have higher communication speed 
but lower conversation breadth
Pedro Aceves 
  1 
 & James A. Evans 
  2,3
Human languages vary widely in how they encode information within 
circumscribed semantic domains (for example, time, space, colour, 
human body parts and activities), but little is known about the global 
structure of semantic information and nothing about its relation to human 
communication. We first show that across a sample of ~1,000 languages, 
there is broad variation in how densely languages encode information 
into words. Second, we show that this language information density is 
associated with a denser configuration of semantic information. Finally, we 
trace the relationship between language information density and patterns 
of communication, showing that informationally denser languages tend 
towards faster communication but conceptually narrower conversations or 
expositions within which topics are discussed at greater depth. These results 
highlight an important source of variation across the human communicative 
channel, revealing that the structure of language shapes the nature and 
texture of human engagement, with consequences for human behaviour 
across levels of society.
Language is the primary medium through which human information 
is communicated and coordination is achieved1–4. One of the most 
important language functions is to categorize the world so messages 
can be communicated through conversation. While we know a great 
deal about how human languages vary in their encoding of information 
within circumscribed semantic domains5,6 such as colour7–10, sound11, 
number12, locomotion13,14, time15–17, space18–21, human activities22,23, 
gender24, body parts25 and biology26,27, little is known about the global 
structure of semantic information28 and its effect on human commu-
nication29.
The words of a language carve the vast space of meaning into 
concepts for communication. When we hear a sentence such as “I play 
soccer, monopoly, and the violin”30, the words may strike the English 
speaker as natural representations of their underlying conceptual 
information and related associations. This traditional, universalist 
perspective5 posits that conceptual information is represented by 
language through a straightforward mapping between words and 
underlying concepts, invariant across cultures31–34. Yet, translating this 
sentence into Spanish leads to the following sentence: “Yo juego fútbol 
y monopoly, y toco el violín” (in English, “I play soccer and monopoly,  
and I touch the violin”). Within this circumscribed example, the word 
‘play’ represents more conceptual information in English than in 
Spanish by also containing the act of violin playing, suggesting the 
variability of conceptual information across languages (Fig. 1a). This 
alternative, non-universalist perspective posits that cultures differ in 
how they carve experience into concepts and that languages vary in 
their correspondence between words and conceptual information35–37. 
Despite documented evidence of this variation across circumscribed 
knowledge domains using small language samples7,9–27,34,38,39, as well as 
recent larger-scale analyses that broaden the domains, the number of 
languages or both5,40, analyses of linguistic relativity have tended to 
ignore the degree to which the words of one language encode more 
or less conceptual information than another. Greater information 
density entails more polysemy, where words reference more meanings 
Received: 19 September 2022
Accepted: 3 January 2024
Published online: 16 February 2024
 Check for updates
1Department of Management and Organization, Carey Business School, Johns Hopkins University, Baltimore, MD, USA. 2Department of Sociology & 
Knowledge Lab, University of Chicago, Chicago, IL, USA. 3Santa Fe Institute, Santa Fe, NM, USA. 
 e-mail: paceves@jhu.edu
Nature Human Behaviour | Volume 8 | April 2024 | 644–656
645
Article
https://doi.org/10.1038/s41562-024-01815-w
The Huffman algorithm allows us to translate each document from 
the parallel translation into a string of 0s and 1s that efficiently encodes 
each word, character or other symbol on the basis of its frequency 
or probability of recurrence. Coding common symbols with fewer 
bits (0s and 1s) than rare symbols minimizes the overall code length, 
which enables comparison of different language codes (for example, 
English and Spanish) for a given translation. Languages that encode 
fewer bits are more informationally dense in that each bit represents 
more conceptual information. Because every corpus contains distinct 
information content, and every document exists in English as well as 
one or more other languages, we used the bit size ratio of the secondary 
language to English as a normalized information density measure to 
enable comparison across all corpora. For ease of interpretation, we 
multiplied the ratio by −100, such that larger values indicate greater 
informational density. We created this language information density 
measure using data from 18 diverse corpora (~14 billion tokens total) 
representing 998 languages from 101 language families and covering 
knowledge domains as diverse as medicine, technology, economics, 
politics, law and entertainment.
across the language. We know little about cross-linguistic information 
density and even less about its relation to human communication and 
knowledge creation.
We here begin to fill this lacuna by providing large-scale evidence 
to document wide variation in the degree of information density across 
the world’s languages (step 1 in Table 1). Information density refers to 
the average amount of conceptual information per language unit38,39, 
a notion closely related to other information-theoretic measures of 
language such as entropy and efficiency41,42. To systematically estimate 
the information density of languages, we used 18 diverse parallel trans-
lation corpora that contain equivalent text in English and many other 
languages (Methods). As the purpose of translation is to preserve the 
information, force and meaning of an expression in a code appropri-
ate to the target language and culture43, we compared how pairs of 
languages differ in their linguistic encoding of the same conceptual 
information. We estimated information density by using the Huffman 
coding algorithm44, an information-theoretic measure that translates 
word symbols from each language into their most efficient binary code 
given the symbol distribution within the document.
soccer
play
monopoly
violin
fútbol 
juego
monopoly
toco
violín
Compressed conceptual space
Expansive conceptual space
English
I play soccer, monopoly, and the violin 
Spanish
Language:
11 1111
1
1
1 11
11 = 25 bits
= 20 bits
1
111
1 111 111
1
0
000 00
00 0
0
00 000 00 0
00
Input message: Yo juego fútbol y monopoly, y toco el violin
Output Huﬀman code:
Lexical space
Conceptual space
–70
–80
–90
–100
–110
–120
–130
–140
200
175
150
125
100
75
50
25
0
Frequency
Information density
Information density
–220
–180
–140
–100
–60
a
b
c
Fig. 1 | Broad variation in the information density of human languages.  
a, Simplified illustration of information density, semantic density and how they 
are related. Information density is estimated by taking parallel translations of 
an input message (that is, the documents in our parallel translation corpora) 
and using the distribution of words in the document to compute an optimal, 
prefix-free binary code for the words. We then translate the message into binary 
digits using this code. Informationally denser languages will have messages with 
fewer bits. Then, by taking the bit size ratio between the messages (times −100 
so that denser languages have larger values), we arrive at a measure of language 
information density. The consequence of informationally denser languages is 
that they compress the conceptual space. In the example above, because the 
word ‘play’ in English represents more conceptual information by also referring 
to the activity of moving a bow across the strings of a violin, the conceptual 
information of ‘monopoly’ and ‘soccer’ gets nudged closer to the conceptual 
information related to ‘violin’. Our information density measure carries out this 
procedure at scale across massive, parallel aligned corpora, giving us an estimate 
of the information density across the entirety of a language. b, Distribution of the 
language information density measure. Larger values indicate informationally 
denser languages. c, Geographic distribution of linguistic information density. 
Larger values (towards the red end of the continuum) indicate informationally 
denser languages. Some regional clustering occurs (for example, in Europe, 
southeast Africa and Southeast Asia).
Nature Human Behaviour | Volume 8 | April 2024 | 644–656
646
Article
https://doi.org/10.1038/s41562-024-01815-w
Next, we asked whether the information density of a language is 
associated with the density of its conceptual, semantic space (step 2 in 
Table 1). Conceptual space is the multidimensional space characteriz-
ing the distance between word and concept meanings in a language. The 
distribution of concepts within this space is what allows us to say, for 
example, that the ‘play’ concept is closer to the ‘violin’ concept in Eng-
lish than in Spanish. Following the distributional hypothesis, which pos-
its that word meanings are proximate as a function of shared context45, 
we estimated conceptual density with neural word embedding models 
built according to this principle46. Word embedding models have previ-
ously been demonstrated to represent rich semantic associations with 
geometric distances, reproducing human analogies47,48, cultural asso-
ciations49 and bias50,51, and revealing polysemy or multiple use52,53. Word 
embedding algorithms use word co-occurrences within a document 
and a neural network architecture to train a high-dimensional vector 
for each word in the corpus (Methods). The output of the algorithm is 
a vector space wherein words with similar syntactic uses and semantic 
meanings tend to be close together in the space, and words can have 
multiple dimensions of similarity and difference. The expectation is 
that languages with informationally denser words will tend towards 
conceptual spaces that are likewise denser, where the normalized 
distance between each word in the space is smaller, such that the aver-
age associational possibilities and polysemous uses for each word are 
greater52. We calculated this semantic density on the same document 
corpora for which we calculated information density. We note that it 
is not a foregone conclusion that information and conceptual density 
will correlate. Information density relies only on the frequency of each 
word across the corpus; conceptual density relies on the diversity of 
contexts in which each word resides. If they strongly correlate, it will 
mean that the more times a word is used, the more ways it is used.
We then inquired whether representing the same conceptual 
information in fewer bits, as informationally dense languages do, 
sends information more quickly through spoken communication 
(step 3). The information-theoretic expectation is that this should 
be the case, as fewer bits should be able to flow more quickly through 
a fixed-bandwidth channel54. In the same way that an electronic 
document will be downloaded more quickly if it contains fewer bits,  
languages that encode the same information more densely (that is, 
in fewer bits) should be able to communicate that information more 
quickly. To test this expectation, we measured the time it took to speak 
the audio Bible in 265 languages—that is, to communicate the same 
message encoded in different symbolic systems—and related this to 
the information density measured in step 1 above.
We further considered whether differences in conceptual infor-
mation density shape not only the speed of communication but also 
discursive and conversational patterns in real-world communica-
tion, focusing primarily on the conceptual breadth of information 
brought to bear on a conversation (step 4). Recent work conceptual-
izes conversations as probabilistic random walks through the topic 
space of conversations53,55,56. At every point in a conversation, there is 
a micro-topic of discussion, and each word in a language is in the set 
of words that could be used to talk about any given topic. Each poten-
tial topic therefore has a probability distribution over the words that 
could be used within that topic. This is consistent with a more micro, 
cognitive view, where research on long-term memory retrieval57 is 
carried out through verbal fluency tasks, finding that sets of semanti-
cally similar words are generated together58,59 through a random-walk, 
associative retrieval process60,61. This means that words function as 
cues to activate associations in memory. Each word or subsequent set 
of words activates a new set of potential words from which to proceed. 
Consistent with optimal foraging theory62,63, the process of associative 
semantic search is carried out by exploiting local word patches until 
cues are depleted57,64 and the individual undertakes global exploration 
in search of new word patches. The process that occurs during con-
versation is an extension of this process, but instead of internal word 
cues, the cues come from the utterances of others. As each new word 
is spoken, it serves as a cue that activates new traces in the memory of 
all participants, opening new possibilities for movement through the 
conceptual space during the conversation. We therefore expect that 
informationally denser languages will exhibit conversations that circle 
and retrace over narrower micro-topics, given that informationally 
denser words will facilitate deeper discussion of any given topic by 
bringing more information to bear on that topic.
We traced how individuals traversed the conceptual space of a 
language in conversation, using text from over 6,000 conversations 
in 14 languages to measure (1) the total conceptual breadth of the 
Table 1 | Theoretical and methodological framework for studying the relationship between language information density 
and communication
Step
Rationale
Dataset
Finding
1. Examine variation in information 
density (Huffman encoding of 
words) across the world’s languages.
Establish a substantial empirical 
distribution of information encoding 
density across languages.
Parallel corpus covering 15 
knowledge domains across 
998 languages.
Information density varies broadly across 
the world’s languages (Fig. 1b).
2. Explore the relationship between 
language information density and 
the semantic density of conceptual 
information.
Provide evidence that languages that 
encode more conceptual information 
within their words create closer 
associations across the space of 
conceptual information.
Parallel corpus covering 15 
knowledge domains across 
998 languages.
Language information density is 
positively associated with semantic 
density (Fig. 2).
3. Investigate the relationship between 
language information density and 
communicative speed.
Provide evidence of an expected 
information-theoretic relationship 
between information density and the 
speed with which communication travels 
through a channel.
Audio Bible in 265 languages.
Language information density 
is positively associated with 
communicative speed (Fig. 3).
4. Consider the relationship between 
language information density and 
semantic breadth in real-world 
conversations.
Provide evidence that the encoding of 
conceptual information evidenced in 
steps 1 and 2 correlates with patterns 
of communication in real-world 
conversations.
More than 6,000 real-world 
conversations in 14 languages.
Conversations in informationally denser 
languages cover a narrower conceptual 
range (Fig. 4a,b).
5. Consider the relationship between 
language information density and 
the semantic breadth of knowledge 
output from a social collective.
Provide evidence that the encoding of 
conceptual information evidenced in 
steps 1 and 2 correlates with patterns 
of real-world knowledge creation and 
communication.
Over 95,000 Wikipedia articles 
in 140 languages.
Collectives speaking informationally 
denser languages produce conceptually 
narrower articles (Fig. 4c).
Nature Human Behaviour | Volume 8 | April 2024 | 644–656
647
Article
https://doi.org/10.1038/s41562-024-01815-w
conversation and (2) the average conceptual distance travelled in 
each conversational turn. We measured the conceptual breadth of 
a conversation in multiple ways using embedding models. First, we 
measured the centroid vector of all words used during a conversation. 
We then measured how far away, on average, each unique word was 
from this centroid vector. This left us with a measure of the radius, 
or breadth, of the conceptual space activated during the conversa-
tion. We also measured conversational breadth by how far a con-
versation moved in the embedding conceptual space from the first 
few utterances of the conversation to the last few utterances of the 
conversation. Second, we measured the average conceptual distance 
travelled in each conversational turn by averaging the conceptual 
distances between the first and second utterances, the second and 
third utterances, and so on until the last utterance in the conversation. 
We also considered the relationship of these findings on many more 
languages with a simple conversational simulation described in the 
Supplementary Information.
Finally, we asked whether these conversational patterns are related 
to the knowledge output of social collectives that use the language in 
their communication (step 5). If a social collective uses an information-
ally denser language to communicate, then their collective communi-
cation probably traverses the conceptual space in a denser, narrower 
pattern as expected above, leading to collective outputs resembling the 
communication process that generated them. We tested this expecta-
tion by measuring the conceptual breadth in over 95,000 Wikipedia 
articles authored in 140 languages.
Using large-scale computation, artificial intelligence techniques 
and massive, parallel corpora, we here show substantial variation in 
the information density of languages (step 1) and that this variation is 
related to their semantic density (step 2), highlighting consequences 
for human communication and coordination. We demonstrate that 
higher-density languages communicate information more quickly 
than lower-density languages (step 3). Then, using over 6,000 real-life 
conversations across 14 languages (step 4) and 95,000 Wikipedia arti-
cles across 140 languages (step 5), we show that topics are discussed 
more narrowly and deeply in denser languages, with conversations and 
articles retracing and cycling over a narrower conceptual terrain. These 
results demonstrate an important source of variation across the human 
communicative channel, suggesting that the structure of language 
shapes the nature and texture of conversation, with important conse-
quences for the behaviour of groups, organizations, markets and socie-
ties. Table 1 summarizes the theoretical and methodological framework 
of our study, highlighting the different analytical steps, their rationale, 
the dataset used to test each step and the study’s findings.
–250
–225
–200
–175
–150
Language information density
All knowledge domains
–125
–100
–75
–50
300
250
200
150
Semantic density
100
50
200
esk
qvw
iku
cni
tel
heb
tam
yad
aratur
zul
mcb
rus
nep
eng
300
120
115
110
105
100
95
110
105
100
95
250
200
150
100
50
0
175
150
125
100
75
50
25
125
120
115
110
105
103
108
102
120
112.5
125
106
104
102
100
98
120
115
110
105
100
95
110.0
107.5
105.0
102.5
100.0
97.5
110
100
90
100
98
96
94
106
104
102
100
98
102
101
100
99
103
104
104
102
106
115.0
112.5
110.0
107.5
105.0
102.5
100.0
104
102
100
101
100
99
98
97
102
100
98
102
101
100
99
–115
–130
–140
–120
–100
–120
–110
–100
–130
–120
–110
–100
–90
–110
–100
–90
–100
–80
–100
–110
–120
–110
–105
–110
–100
–90
–110
–100
–90
–110
–100
–90
–100
100
125
120
115
110
105
100
–160
–200
–130
–120
–110
–100
–150
–100
–140
–120
–100
Bible_Full
r = 0.70
r = 0.71
r = 0.48
r = 0.22
r = 0.46
r = 0.77
r = 0.92
r = 0.74
r = 0.81
r = 0.54
r = 0.45
r = 0.60
r = 0.64
r = 0.42
r = 0.57
r = 0.73
r = 0.35
r = 0.78
Bible_NT
TED
Subs16
UN
EuroParl
EU_JRC
EU_DGT
EMEA
ECB
News9
News11
KDE
GNOME
Tatoeba
EU_Bookshop
Subs18
ParaCrawl
–80
–120
–110
–100
–90
–120
–110
–100
–120
–110
–100
–90
–80
–110
–100
–90
–110
–100
–90
–80
spa
cmn
apr
hin
ban
vie
poh
tha
jpnanv
cof
cbu
guh
kog
gbi
maz
jic
kss
crn
cbc
agm
noa
lhi
sab
apn
akh
sua
Fig. 2 | Relation between information density and semantic density by 
knowledge domain. Across the 18 corpora in our sample, informationally denser 
languages have conceptual or semantic spaces that are likewise denser. For 
all plots, the y axis marks semantic density while the x axis indicates language 
information density. The language codes are provided in Supplementary Table 2.  
The upper left plot includes the observations of all documents and includes the 
estimated effect of information density (ID) on semantic density (SD) as a cubic 
function (that is, SD ~ α + β1ID + β2ID2 + β3ID3 + ε), with 95% CIs represented by the 
shaded region. Colours in the upper left plot correspond to 101 distinct language 
families and manifest clustering of semantic and information density within 
family. A full description of the language information density measure  
(x axis) can be found in ‘Estimating language information density’ in Methods. 
A full description of the semantic density measure (y axis) can be found in 
‘Language information density and semantic density’ in Methods.
Nature Human Behaviour | Volume 8 | April 2024 | 644–656
648
Article
https://doi.org/10.1038/s41562-024-01815-w
Results
Variation in information density
Using a large-scale sample of 998 languages representing 101 language 
families and a broad diversity of knowledge domains, we found sub-
stantial variation in linguistic information density across the world’s 
languages (Fig. 1b). This finding is consistent with limited work that has 
estimated the information density of languages using modest language 
samples and small textual corpora38,39. We further show consistency in 
language information density rates across diverse knowledge domains. 
We found that information density is highly correlated across knowl-
edge domains as diverse as religion, banking, medicine, government, 
computer programming, news, movies and public speeches (Supple-
mentary Fig. 1). This suggests that language information density is 
domain-independent and applies to all observed knowledge areas 
within each language. Information density represents variation distinct 
from previously measured linguistic variation within language. It posts 
correlations of ρ = 0.11 with a composite measure of complexity that 
codes each language as simple if reliant on a lexical strategy with few 
grammatical distinctions and complex if reliant on a strategy with 
many65,66 (available for 528 of our languages), ρ = 0.24 with a meas-
ure of fusion that captures the degree to which a language relies on 
phonologically bound markers such as prefixes and suffixes instead 
of independent ones67, and ρ = −0.04 with a measure of informativity, 
which assesses the amount of explicit and obligatory distinctions a 
language makes, including those of politeness and remoteness67 (both 
available for 380 of our languages). We included these measures as 
controls in the models that follow.
Language information density and semantic density
We found broad variation in the global structure of conceptual space 
across languages (Supplementary Fig. 2). Some languages more 
densely interlink diverse conceptual subspaces, while others are 
sparser and more fragmented. This variation is strongly associated 
with language information density. Figure 2 displays raw associations 
between language information density and density in conceptual space 
across corpora, showing strong associations in all cases. We tested a 
mixed-effects regression with observations nested within languages 
and languages nested within language families while controlling for the 
number of words in the corpus and other attributes of the corpus and 
language. We found that a one-unit increase in information density is 
associated with a 1.02-unit increase in conceptual space density (95% 
confidence interval (CI), 0.84, 1.2; P < 0.001; Supplementary Table 5). 
The association remains after we control for the morphological com-
plexity, fusion and informativity of the languages as detailed above 
(β = 0.98; 95% CI, 0.88, 1.08; P < 0.001; Supplementary Table 7). Thus, as 
more conceptual information is encoded into the words of a language, 
concepts within that language become more closely associated as 
suggested in Fig. 1a.
While prior research has measured information density manu-
ally at the syllable or morpheme level39,68, this is impractical across 
many languages and massive corpora like ours, which prohibit 
manual coding. To ameliorate the concern that words might not be 
the most fundamental units of meaning, and in line with prior work 
tracing information-theoretic measures of language69–71, we also cre-
ated our language density measure at the single-character level. This 
yielded similar results between information density and conceptual 
space density, with a coefficient of 0.6 (95% CI, 0.46, 0.75; P < 0.001;  
Supplementary Table 6). The robustness of these results to an alterna-
tive specification of information density suggests consistency across 
word and subword measures of information density.
Language information density and communicative speed
Prior research has found limited association between language informa-
tion density and communication speed on the basis of modest texts in 
fewer than 20 languages38,39. In contrast, we found large variation in the 
speed of communication across languages, and this speed is associated 
with information density (Fig. 3). Using a random-intercepts model, 
we found that a one-unit increase in information density is associated 
with a 0.84-unit decrease in duration, which equates to an increase in 
communicative speed (95% CI, −1.19, −0.49; P < 0.001; Supplementary 
Table 12). We also tested the same model but with the character-level 
information density measure, finding consistent results (β = −0.77; 95% 
CI, −0.97, −0.57; P < 0.001; Supplementary Table 13). We further tested 
a model with controls for the morphological complexity, fusion and 
informativity of the language, again with consistent findings (β = −1.02; 
95% CI, −1.35, −0.68; P < 0.001; Supplementary Table 14).
These findings are especially interesting when understood in 
the context of speech production and comprehension. During the 
process of human communication, speech encoding (that is, articula-
tion, or the process of turning information into speech sound) is the 
slowest part of speech production and comprehension, meaning that 
humans can think of what to say and understand what somebody else 
has said faster than it takes to say it72. A processing-time asymmetry 
thus exists between the mechanical work of speech articulation and 
the cognitive work of comprehension and inference. This asymme-
try has been dubbed the articulatory bottleneck72 and suggests that 
if inference-making is cheap (in time required) and articulation is 
expensive, then any increase in articulatory speed should likewise 
speed up inference-making. It further suggests that the proportion of 
communication reliant on ampliative pragmatic inference will vary 
by language depending on information density, with higher-density 
languages requiring more inference about what is spoken and why.
Language information density and conversational breadth
We found evidence that informationally denser languages cover a nar-
rower conceptual range across all conversations. Controlling for dura-
tion and the number of turns taken, conversations in informationally 
denser languages cover a conceptually narrower range of discussion 
(β = −0.053; 95% CI, −0.08, −0.03; P < 0.001; Fig. 4a and Supplementary 
Language information density
Communicative speed
300
sab
gbi
cbu
mcb
yad
cni
crn
yaa
guh
yva
cof
kog
tbc
cbc maz
jic
noa
kss
ban
cbi
anv
jav
hin
tha
nep
jpn
rus
ara
fraspa
zho
por
ell
275
250
225
200
175
150
125
100
–200
–180
–160
–140
–120
–100
–80
–60
eng
bul
turamh
heb
mal
Fig. 3 | Association between information density and communicative 
speed. The y axis is measured as duration, so smaller values equal greater 
communicative speed. Informationally denser languages that pack information 
into fewer bits thus communicate more quickly. The language codes are provided 
in Supplementary Table 2. The plot includes all available observations and 
the estimated effect of information density on communicative speed (CS) as 
a quadratic function (that is, CS ~ α + β1ID + β2ID2 + ε) with 95% CIs represented 
by the shaded region. A full description of the language information density 
measure (x axis) can be found in ‘Estimating language information density’ in 
Methods. A full description of the duration measure for communicative speed  
(y axis) can be found in ‘Language information density and communicative speed’ 
in Methods.
Nature Human Behaviour | Volume 8 | April 2024 | 644–656
649
Article
https://doi.org/10.1038/s41562-024-01815-w
Table 22). This effect is robust to the inclusion of controls for mor-
phological complexity (β = −0.058; 95% CI, −0.09, −0.03; P < 0.001; 
Supplementary Table 23); environment including population size, 
total precipitation and mean temperature (β = −0.054; 95% CI, −0.08, 
−0.03; P < 0.001; Supplementary Table 24); and cultural attributes 
including indulgence and long-term orientation (β = −0.058; 95% CI, 
3
2
1
0
3
4
2
1
0
kaz
vie
ben
ceb
ibo
tgl
grn
kat
amh
tel
tur
tam
lit
zul
kaz
vie
ben
ceb
ibo
tgl
grn
kat
amh
tel
tur
tam
lit
zul
Language (sparsest to densest)
Language (sparsest to densest)
Conversational breadth
3
2
1
0
Breadth
3
2
1
0
Distance
Average distance travelled
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1.0
1.1
1.2
Language (sparsest to densest)
mkn
mai
snd
gle
guj
hin
jvn
vie
arb
qvc
urd
pan
fra
nep
tgk
kaz
som
spa
mlt
ita
cat
ell
ron
ben
jav
cym
por
uig
glg
bre
sin
nds
yor
che
nhg
afr
ukr
gla
ces
mar
deu
slk
nld
ceb
slv
sot
sqi
pol
bos
pam
new
eng
rus
tgl
mkd
bul
isl
nob
hun
fas
epo
dan
swe
tam
lav
ind
hrv
kat
tat
bcl
zsm
bel
mon
mlg
lit
tur
srp
est
heb
ckb
fin
swh
kan
hye
ilo
tel
mal
lat
ara
quy
amh
mya
khm
0.50
0.75
1.00
Breadth
a
b
c
Wikipedia article breadth
Fig. 4 | Effect of information density on conversational patterns and 
knowledge output. a, Association between information density and the 
conceptual breadth of conversations, with density estimates represented by 
vertical ridgelines to capture the modal pattern of association (censored below 
a cut-off of 1% of the maximum density). A full description of the conversational 
breadth measure (y axis) can be found in ‘Language information density and 
conversational breadth’ in Methods. b, Association between information density 
and the average conceptual distance travelled during each conversational turn, 
with density estimates represented by vertical ridgelines to capture the modal 
pattern of association (censored below a cut-off of 1% of the maximum density). 
A full description of the average distance travelled measure (y axis) can be found 
in ‘Language information density and conversational breadth’ in Methods. 
c, Association between information density and the conceptual breadth of 
Wikipedia articles, also plotted with density estimates represented by vertical 
ridgelines. As with the decreased breadth of conversations in a, the knowledge 
output of conversations in informationally denser languages is likewise narrower 
in conceptual breadth. A full description of the Wikipedia article breadth 
measure (y axis) can be found in ‘Language information density and knowledge 
output breadth’ in Methods.
Nature Human Behaviour | Volume 8 | April 2024 | 644–656
650
Article
https://doi.org/10.1038/s41562-024-01815-w
−0.08, −0.04; P < 0.001; Supplementary Table 25). These effects are 
both significant and substantial. Increasing informational density by 
one standard deviation (10) yields an increase of more than two thirds 
(0.675) of a standard deviation in conversational breadth. When we ran 
an ordinary least squares model predicting conversational breadth 
with all controls described, the R2 was 0.55 (Supplementary Table 27). 
Introducing information density to this specification increased the R2 
to 0.92 (Supplementary Table 27), adding considerable explanatory 
power to the model. These results indicate that the greater the informa-
tion density of the language spoken in these conversations, the more 
conceptually narrow the conversations were across their entire content.
We next examined whether the conversations were also narrower 
in terms of their starting and ending locations in conceptual space. 
This goes beyond the previous measure to understand not just the 
total breadth of the conversation but also the movement between its 
conceptual starting and ending points. We found the same pattern, 
wherein conversations in informationally denser languages depart 
less from the initial topic of discussion by the end of the conversa-
tion. This effect was robust to how we operationalized the beginning 
and ending of the conversation, with similar results regardless of 
whether we measured the conceptual distance between the first four 
and last four utterances (β = −0.023; 95% CI, −0.04, −0.008; P = 0.002;  
Supplementary Table 29), the first eight and last eight (β = −0.012; 95% 
CI, −0.021, −0.004; P = 0.005; Supplementary Table 30), utterances two 
through six and the last six through the last two utterances (β = −0.02; 
95% CI, −0.03, −0.007; P = 0.002; Supplementary Table 31), utterances 
four through eight and the last eight through the last four utterances 
(β = −0.02; 95% CI, −0.03, −0.006; P = 0.003; Supplementary Table 32),  
or utterances six through ten and the last ten through the last six utter-
ances (β = −0.02; 95% CI, −0.03, −0.005; P = 0.004; Supplementary 
Table 33). In all of these cases, the conversations in informationally 
denser languages conceptually departed to a lesser degree from the 
initial topic of conversation.
The above results led us to conceive of a language generation 
model in which conversational discourse topics are sticky, with indi-
viduals tending to remain within a topic rather than moving away from 
it. We hypothesized that in denser languages, participants will be bet-
ter enabled and more likely to discuss the same topic from different 
points of view and by bringing more diverse information to bear on it. 
From this perspective, conceptually denser languages facilitate the 
mobilization of neighbouring conceptual information, leading to a 
more thorough discussion of any given topic. When this occurs, con-
versations will be less likely to move to new topics, leading to narrower 
but deeper conversations overall. To explore this, we tested whether 
conversations in informationally denser languages activate and engage 
more deeply with the topics of discussion, staying to a greater degree 
within the conversational topics. We traced the average conceptual 
distance moved between the first and last utterances of conversations, 
averaging the conceptual distance between the first and second utter-
ances, the second and third utterances, and so forth until the final utter-
ance. We found evidence that information density is related to narrow 
movements, with conversations in informationally denser languages 
engaging more deeply with each topic of conversation (β = −0.04; 
95% CI, −0.06, −0.02; P < 0.001; Fig. 4b and Supplementary Table 28). 
These findings corroborate results from simple conversational simu-
lations estimated for many more languages (Supplementary Text and  
Supplementary Table 43).
Language information density and knowledge output breadth
Finally, we found that collective knowledge outputs reflect the same 
conceptual dynamics found in the conversations that produced them. 
We traced the conceptual breadth of over 95,000 Wikipedia articles 
collectively written by hundreds of thousands of contributors in 93 lan-
guages. When we investigated whether the final articles written through 
online conversations exhibit different patterns of information content 
depending on the language used to write them, we found support for 
the idea that informationally denser languages are associated with 
knowledge articles on Wikipedia that cover narrower conceptual terrain 
(β = −0.01; 95% CI, −0.02, −0.003; P = 0.008; Fig. 4c and Supplementary 
Table 36). These results are robust to the inclusion of language struc-
ture controls for morphological complexity, fusion and informativity 
(β = −0.009; 95% CI, −0.01, −0.006; P < 0.001; Supplementary Table 37). 
Increasing informational density by one standard deviation (10.5) yields 
an increase of more than two thirds (0.673) of a standard deviation in 
article conceptual breadth. When we ran an ordinary least squares 
model predicting article conceptual breadth with all of the controls, 
the R2 was 0.56 (Supplementary Table 41). Introducing information 
density to this specification increased the R2 to 0.71 (Supplementary 
Table 41), significantly increasing the explanatory power of the model.
Discussion
In summary, we report broad variation in how human languages 
encode information. While some languages more densely weave 
together diverse conceptual terrain, drawing together broad areas 
of conceptual space, others are sparser and more fragmented, sepa-
rating distinct areas of meaning. We have shown that this variation 
in conceptual encoding is associated with important conversational 
dynamics, including speed of communication and patterned move-
ment through the conceptual terrain of conversation and exposi-
tion. Because informationally denser languages contain more 
conceptual information per bit, more information flows through a 
fixed-bandwidth communication channel, leading to faster commu-
nication. Higher-information-density languages are also associated 
with conversations that intensively traverse a narrower portion of 
conceptual space. The set of possible things to say about any one 
topic is larger in denser languages, and opportunities for cycling are 
more numerous, leading conversation participants speaking these 
languages to trace and retrace the same conceptual terrain. Finally, 
we have shown that these conversational patterns are observable in 
expository knowledge outputs produced by collaborative groups, such 
that knowledge produced in higher-information-density languages 
more intensively covers a narrower region of conceptual space.
Our study has focused on mapping the variation in information 
density across the world’s languages and tracing its relationship with 
communication and conversation dynamics (the second arrow in 
Fig. 5). Nevertheless, our study raises the question of what drove 
the observed differences in information density (the first arrow in  
Fig. 5). The first possibility is that languages have evolved differing 
information density values simply by chance. Such a proposal would 
be supported by a “phono-semantic monkey” conception of language 
evolution73. The short version of this perspective is that a monkey’s ran-
dom typing on a keyboard would engender Zipfian word distributions 
without recourse to any kind of functionalist explanation. According to 
this argument, observed variation in language information density is 
random, with languages randomly distributed across the “Goldilocks 
zone” permitted by human cognitive capacity and communicative 
need74. One point of evidence in support of this argument is that, in our 
data, we observe languages spoken in societies at advanced levels of 
cultural and economic development located both above and below the 
mean value in our data. In addition, we observe that languages spoken 
in simpler, less developed societies are also located at the lower and 
upper ends of the distribution, suggesting that there do not appear 
to be measured evolutionary or functionalist forces pushing towards 
more or less information density.
The second possibility, however, is that within the “Goldilocks 
zone”74 environmental and social forces have shaped the amount of 
information density within languages. For example, prior work has 
documented how the number of language users, geographic spread 
and degree of language contact relate to aspects of morphological 
complexity65,66,75. Evidence from our data for this position includes the 
Nature Human Behaviour | Volume 8 | April 2024 | 644–656
651
Article
https://doi.org/10.1038/s41562-024-01815-w
much narrower location of the world’s most widely spoken languages 
in the middle of the information density distribution, suggesting that 
modern culture and society have evolved a more optimal information 
density. This observation suggests that as the number of concepts 
required for encoding in language increases, the amount of informa-
tion to be encoded per language unit approaches an optimum in the 
centre of the distribution. Exploring mechanisms that might account 
for this narrowing of information density among the most widely 
spoken languages is an open avenue for future work.
To investigate the possibility that broader environmental fac-
tors shape language information density, we added environmental 
measures65,66 to our data, including the size of the speaker population, 
number of neighbours, spatial perimeter of the language, mean tem-
perature of the language’s climate, yearly precipitation and length of 
the growing season. When we tested a random-intercepts model pre-
dicting information density, with observations nested within languages 
and languages nested within language families, we found that the land 
area (β = 3.2; 95% CI, 0.4, 5.9; P = 0.02; Supplementary Table 42), land 
perimeter (β = −9; 95% CI, −16, −2; P = 0.009; Supplementary Table 42) 
and mean temperature (β = −1.3; 95% CI, −1.8, −0.77; P < 0.001; Supple-
mentary Table 42) manifest statistical significance. Furthermore, the 
map in Fig. 1c suggests regional differences in the variation of informa-
tion density, with some regions converging towards a narrow band of 
values (for example, southern India and southeast Africa) and others 
showing much broader variation (for example, southern Mexico and 
Central America). These regional differences along with broader envi-
ronmental forces at play in language evolution highlight the potential 
for future research to more deeply investigate the social and environ-
mental forces at play in shaping language information density.
Our findings also suggest that because language information den-
sity is associated with patterns of communication, it might non-trivially 
shape individual cognition (for example, creativity, memory and 
search), other unexamined patterns of interaction and collective per-
formance among human groups. In cognition, one can imagine that 
varying density of conceptual information, which shapes associational 
probabilities among language concepts, may influence many cogni-
tive behaviours such as how individuals seek out novel information 
for problem-solving, judgement and evaluation; how they store and 
activate conceptual memories; and how they engage in creative tasks76. 
In terms of social interaction, variation in language information density 
may shape patterns of conflict and collaboration by altering the mobi-
lization of concepts and ideas for social engagement. To the extent 
that the words and ideas of others during conversation function as 
cues to retrieve one’s responses, differences in encoded information 
density could reshape interactive information activation and social 
encounters. Considering our demonstrated relationship between 
language structure and conversational speed and breadth, language 
structure may play a causal role in shaping patterns of social interac-
tion and collective performance, broadening the linguistic relativity 
hypothesis beyond cognition. We hope this study can inspire investiga-
tions that extend linguistic relativity to social interaction and collective 
performance in collective knowledge, intelligence77, and the evolution 
of culture, technology and the built environment.
Limitations
While our observational empirical design provides support across a 
large number of languages for the claim that language information den-
sity relates to processes of social interaction and collective cognition, 
it does not allow us to make causal claims regarding this relationship. 
To establish causation, future research would benefit from two kinds of 
studies. Laboratory studies across smaller sets of languages can provide 
control over important conversational factors that stand to nudge 
patterns of interaction through language. For example, the McGrath 
task circumplex identifies tasks that groups can engage in, ranging 
from cognitive tasks that require greater judgement and creativity to 
executive behaviours that require greater planning and coordination78. 
Across the broad range of tasks in which groups engage, some will 
require greater support for open collaboration, while others will require 
the capacity to manage conflict. Experimental studies that standardize 
the nature of the task will enable deeper insight into whether and how 
language information density shapes patterns of communication and 
social interaction. By contrast, research that engages in deeper eth-
nographic study of smaller language sets located on different ends of 
the information density continuum could provide important insights 
regarding how language information density relates to the pragmatic, 
cultural and socio-structural factors that shape communication and 
social interaction in daily life. Both laboratory and ethnographic studies 
would enrich our understanding of processes through which language 
structure may shape communication, social interaction and human 
performance across the many domains of collective life.
A second limitation is that the current study is agnostic about 
whether subdomains of conceptual meaning within and across  
languages might vary in terms of information density. While the cur-
rent study suggests that information density tends towards a con-
sistent level across different knowledge domains within a language 
(see the large correlation coefficients in Supplementary Fig. 1 across 
different corpora), it is possible that at fine-grained levels, some sub-
domains of a language could be substantially denser than others. For 
example, poetry and sociology are probably denser than logic and 
physics79,80. Understanding whether subdomain density is consistent 
across languages or whether there are substantial differences across 
which subdomains densely encode information would better shape 
Environmental
factors
Historical
Geographic
Demographic
Social 
Cultural
Language
information
density
Communication
and social interaction
patterns
Conversation breadth
Conversation topic
depth
Communication speed
Turn-taking
Topic selection
Feedback
Conflict
Cooperation
Collective
performance
outcomes
Creativity
Decision-making
Judgement
Problem-solving
Coordination
Negotiation
1
2
3
Fig. 5 | Antecedents and consequences of language information density. 
Future research opportunities lie in understanding three interrelated questions. 
The first question involves understanding which environmental factors have 
shaped the distribution of language information density. The second question 
has to do with understanding how language information density (and other 
language structure attributes) shapes patterns of communication and social 
interaction such as turn-taking, topic selection, conflict and cooperation. The 
third question asks how language information density and its influence on 
communication patterns influence collective outcomes such as performance in 
decision-making, judgment and problem-solving.
Nature Human Behaviour | Volume 8 | April 2024 | 644–656
652
Article
https://doi.org/10.1038/s41562-024-01815-w
expectations for how social interaction and knowledge creation take 
place across language cultures.
Conclusion
Our findings call for an expansion of the linguistic relativity hypothesis 
beyond the cognitive framework, bringing the idea into the realm of 
communication, interaction, collaboration and collective action. The 
structure of conceptual information within language shapes not only 
individual cognition7,11,16,18,81,82 but also how humans communicate and 
interact with one another, influencing the space of what collectives think 
and how they behave29. From coordination, cooperation and collabora-
tion to conflict, competition and disruptive innovation, the structure of 
information in language may impact the character, success and failure 
of human collectives by weaving the texture of their communication.
Methods
No statistical methods were used to predetermine sample size.  
Randomization and blinding were not possible, given the observational 
nature of the study.
Estimating language information density
Dataset of parallel translations. The following parallel translation 
corpora were collected to estimate the linguistic information and 
semantic densities: the complete Bible, including the Old and New 
Testaments83–85; the New Testament83–85; news transcripts (News9 
and News11)83; web text83,86; movie subtitles (Subs16 and Subs18)83,87; 
TED talks83; example sentences for foreign language learners; United 
Nations83,88; European Central Bank83; European Medicines Agency83; 
European Union bookshop83; European Union Directorate General for 
Translation83; European Union Joint Research Centre83; European Par-
liamentary Proceedings83; GNOME software files83; and KDE software 
files83. Supplementary Table 1 documents the number of languages and 
language families represented in each corpus. In total, these corpora 
represent 998 languages in 101 language families.
Measuring language information density. To estimate language 
information density, we drew on tools from information theory, which 
have been increasingly used in the study of language and meaning41,42,89. 
We used the text of each corpus above and counted how many times 
each word appears. We then used this distribution to generate the most 
efficient binary code for words in each document using the Huffman 
coding algorithm44. This algorithm creates the most efficient prefix-free 
binary code with which to encode a given symbol distribution. For-
mally, the input to the algorithm is a symbol set A = {a1, a2, …, an} of size 
n, with weights W = {w1, w2, …, wn}, where wi = weight(ai), i ∈ {1, 2, …, n} 
and is the proportion of each ai in symbol set A. In the case of language, 
the symbols can be characters, morphemes or words. Because our 
interest lies in how conceptual information is encoded, we used the 
symbol distribution of the unique words in the transcript. The output 
of the algorithm is a code C(W) = {c1, c2, …, cn}, where ci is the binary 
codeword for ai. We let L(C(W)) = ∑
n
i=1wi × length(ci) be the weighted 
path length of code C and satisfy the condition L(C(W)) ≤ L(T(W)) for 
any code T(W). The output of the Huffman coding algorithm, then, is 
the shortest binary code for a given symbol distribution.
When every word in a text is translated into its Huffman code, 
we are left with a document of 0s and 1s and can count the number of 
binary digits (bits) in each Huffman-coded document. The larger the 
bit size of a document, the less informationally dense the language 
is, as this means that each bit contains less conceptual information. 
The smaller the bit size, the more informationally dense the language 
is, as this means that more conceptual information is contained in 
each bit. To make the measure comparable across corpora, we took 
the English document as the baseline bit size for each dyadic transla-
tion in a corpus, and every other language took on a ratio value rela-
tive to English. That is, the English value is the denominator, and the 
ratio of each language is arrived at by using its document bit size in 
the numerator. We created this measure for 998 languages for which 
parallel translation corpora existed. For ease of interpretation of the 
statistical tests, we multiplied this ratio by −100, leading to larger val-
ues being associated with informationally denser languages. Figure 1b 
displays the distribution of languages across the information density 
continuum. It is worth noting that this approach is closely related to 
other information-theoretic approaches to studying how information 
is distributed within the words of a language42.
Language information density and semantic density
Measuring language semantic density. To measure the density of the 
conceptual space, we created a 300-dimensional vector-space model 
of the text in each document used to construct the information density 
measure, using a continuous bag-of-words approach47,90,91 and the fol-
lowing parameters: epochs, 10 (number of iterations over the corpus); 
window, 5 (number of words before and after the focal word). These 
kinds of models project the word co-occurrences in a text into a multidi-
mensional vector space wherein syntactically and semantically similar 
words tend to be close to each other and wherein words can have multiple 
degrees of similarity. Because these models can capture multiple degrees 
of similarity, they provide a useful representation of the conceptual 
space of a language as captured in any given corpus, given the multidi-
mensional nature of word meaning. Vector-space models such as this 
effectively represent the conceptual relationships between the words 
of a language, capturing semantic regularities such as V-king − V-man 
+ V-woman = ~V-queen. The expectation is that languages with infor-
mationally denser words will tend towards conceptual spaces that are 
likewise denser, as each word in the space is likely to have on average more 
associational possibilities to every other word. The associational possibili-
ties in higher-information-density languages are partly a consequence 
of words that appear in multiple contexts and that help bring together 
disparate locations in the multidimensional spaces of these models.
Once the vector-space model for each individual document was 
learned, we measured the average cosine similarity (which ranges 
between −1 and 1) between 10,000 random word pairs for words in 
the vocabulary of the text. Larger values indicate closer distances. 
This cosine similarity measure characterizes the average distance 
within the multidimensional conceptual space of each document.  
A larger cosine similarity indicates that concepts tend to share closer 
meaning associations to each other, with a more compressed space 
between concepts. A smaller cosine similarity, in contrast, indicates 
that more space has to be travelled to reach any given conceptual loca-
tion. As with the linguistic information density measure, to make the 
measure comparable across corpora, we took the English document 
as the baseline density for each dyadic translation within a corpus, 
and every other language took on a ratio value relative to English. For 
ease of interpretation, we multiplied the ensuing value by 100. Larger 
values indicate denser conceptual spaces.
Language family control. To control for language family in our mod-
els, we used data from Glottolog, a comprehensive catalogue of the 
world’s languages, language families and dialects92.
Morphosyntactic complexity, fusion and informativity controls. 
To account for relevant typological features, we mobilized three 
previously used measures. The first is a measure of morphosyntac-
tic complexity, which uses 27 morphosyntactic variables and codes 
each strategy as either simple (if it relied on a lexical strategy or few 
grammatical distinctions) or complex (if it relied on a morphologi-
cal strategy or many grammatical distinctions)65,66. The value of the 
measure is then the sum of the number of complex strategies in each 
language. This measure was available for 528 of our 1,390 observations.  
The second is a measure of fusion, which measures the degree to which 
a language relies on phonologically bound markers such as prefixes and 
Nature Human Behaviour | Volume 8 | April 2024 | 644–656
653
Article
https://doi.org/10.1038/s41562-024-01815-w
suffixes instead of phonologically independent markers67. Languages 
with more phonologically fused markers lead to larger values of fusion. 
The third is a measure of informativity, which measures the amount of 
explicit and obligatory distinctions that a language makes, including 
features such as politeness distinctions in pronouns and remoteness 
distinctions in past and future tenses67. The greater the number of 
distinctions, the larger the value. The second and third measures were 
available for 380 of our 1,390 observations.
Statistical analysis. In this study, we observed language information 
density estimates that are nested within languages, which are them-
selves nested within language families, leading to the potential that 
each observation is not independent. Because it is possible that attrib-
utes of the corpora available in each language differentially influence 
the relationship, we fit a three-level mixed model with random inter-
cepts at both the language-family and language-within-language-family 
levels. We estimated this model using Stata’s mixed command, with 
standard errors estimated using the vce(robust) option93, which uses 
the Huber–White sandwich estimators94,95. We followed the same  
modelling strategy for steps 4 and 5 below.
Language information density and communicative speed
Dataset and measure of audio file duration times. To measure the 
communicative speed of a language, we collected the duration time of the 
audio Bible spoken in different languages. The audio Bible duration data 
were manually collected from metadata of the MP3 files found at word-
project.org and faithcomesbyhearing.com. We have audio duration data 
for 261 languages representing 51 language families. As with the measures 
above, we again took the ratio of these durations relative to English to 
make them comparable across websites and multiplied by 100. Higher 
values indicate longer durations and thus slower communicative speed.
A potential drawback of this measure is that we have only one 
observation per language and therefore cannot account for the distri-
bution of individual speaking styles within a language. Nevertheless, 
we believe that this weakness is overcome by the fact that we were 
able to estimate communicative speed at greater scale across a large 
sample of languages and for samples of spoken text that are orders of 
magnitude longer than has been done before39,68. This helps address 
the shortcomings of prior measures, which relied on much smaller 
samples of spoken text (a few sentences) in only a handful of languages.
Statistical analysis. Because we have only one value of communicative 
speed for each language, we computed the mean value of information 
density for all measures within a language. For this model, language obser-
vations are nested within language families. Our estimated model then has 
random intercepts at the language-family level. We estimated this model 
using Stata’s mixed command, with standard errors estimated using the 
vce(robust) option93, which uses the Huber–White sandwich estimators94,95.
Language information density and conversational breadth
Dataset of conversations. Our conversational data came from 
the Babel Program of the Intelligence Advanced Research Projects 
Agency. These natural conversations are between individuals from 
broad age ranges, include speakers of both genders and take place in 
a variety of environments including the street, the home, the office, 
public places and vehicles. We used conversations from the follow-
ing 14 languages: Amharic96, Bengali97, Cebuano98, Georgian99, Gua-
rani100, Igbo101, Kazack102, Lithuanian103, Tagalog104, Tamil105, Telugu106, 
Turkish107, Vietnamese108 and Zulu109. These languages represent eight 
language families. The mean number of conversational turns across all 
conversations is 78 with a standard deviation of 26.
Measuring the conceptual breadth of a conversation. For each set 
of conversations within a language, we measured the conversational 
breadth of every conversation as follows. We took the transcripts of the 
conversation and converted the text into a list of words, and from this list 
we measured the centroid vector of the unique words in the list. We then 
measured the cosine similarity of each unique word to the centroid vec-
tor using the fastText embedding models pre-trained on the Wikipedia 
corpus of the language110. We took the mean cosine similarity (times −1, so 
that larger values indicate greater breadth) between all unique words and 
the centroid as an indication of the unstandardized conceptual breadth of 
the conversation. To standardize by the density of the conceptual space 
itself, we took the ratio of the conceptual breadth of the conversation 
over the density of the spaces trained on our parallel corpora.
Measuring the conversational distance in a conversation. To 
measure the conceptual distance covered during the conversation, 
we followed a similar procedure to measuring the conceptual breadth, 
but instead of measuring the cosine similarity of unique words to 
the centroid vector we measured the distance between the first four 
utterances of a conversation and the last four utterances. We used the 
same Wikipedia embeddings as above111. For robustness, we created 
the same measure for the first and last eight utterances. As with the 
conversational breadth measure, we likewise standardized these by 
the conceptual density of the language itself. For additional robust-
ness, we measured the second through sixth, fourth through eighth 
and sixth through tenth conversational turns at the beginning and end 
of the conversation, meaning that the very first and very last turns of 
conversation were not taken into account. The consequence of these 
additional measures is to move us away from the potential of captur-
ing mere conversational formalities such as hellos and goodbyes and 
towards capturing the actual topics of conversation.
Measuring average distance travelled. We measured the average con-
ceptual distance moved during a conversational turn by measuring the 
cosine similarity between the first and second utterances, the second 
and third utterances, and so on until the final utterance. As before, we 
normalized this measure by taking the ratio of this cosine similarity 
over the density of the conceptual space of the language. Figure 4b 
displays this distribution of average distance moved by conversational 
turn in the conversations of each language.
Environmental controls. To control for the potential effects of envi-
ronmental variables that might influence patterns of social interac-
tion, we controlled for population size, precipitation and mean yearly 
temperature65,66,112.
Culture controls. To control for the potential effects of cultural 
dimensions on patterns of communication and social interaction, 
we added controls for indulgence (which existed for 13 of the 14 
languages) and long-term orientation (which existed for 12 of the 
14 languages)113,114. Other cultural variables such as power distance, 
uncertainty avoidance and masculinity were not available for many 
of the languages in our conversation corpora, so we did not include 
them in the analysis.
Statistical analysis. We followed a similar statistical modelling strat-
egy here as we did for step 2 above. In this study, we observed conversa-
tions that are nested within languages, leading to multiple observations 
of conversations within a language. We therefore fit a two-level mixed 
model with random intercepts at the language-family level. We esti-
mated this model using Stata’s mixed command, with standard errors 
estimated using the vce(robust) option93, which uses the Huber–White 
sandwich estimators94,95. The complete specification and associated 
output can be found in the supplementary materials.
Language information density and knowledge output breadth
Measuring the breadth of Wikipedia articles. To measure the 
breadth of Wikipedia articles, we used the following procedure. For 
Nature Human Behaviour | Volume 8 | April 2024 | 644–656
654
Article
https://doi.org/10.1038/s41562-024-01815-w
each available pre-trained embedding model provided by Facebook’s 
fastText team110 and available in our corpus of parallel translations 
(93 languages in total), we used the Wikipedia Python API to import 
and read the text of 1,000 random articles in that language. For each 
of these articles, we split the text into a set of words and measured 
the cosine similarity of 100 random word pairs. We used the average 
cosine similarity of these 100 word pairs (times −1, so that larger val-
ues indicate greater breadth) as a measure of the conceptual breadth 
of the article. As before, we also normalized this measure by the 
density of the conceptual space of that language.
Statistical analysis. We followed the same statistical modelling  
strategy here as we did for step 2 above, fitting a random-intercepts 
model with Wikipedia articles nested within languages and languages 
nested within language families.
Reporting summary
Further information on research design is available in the Nature 
Portfolio Reporting Summary linked to this article.
Data availability
The datasets analysed in the current study are available at the  
following links: for parallel corpora, https://opus.nlpl.eu/; for con-
versations, https://www.ldc.upenn.edu/; for audio duration, https://
wordproject.org and https://faithcomesbyhearing.com; for language 
family and location, https://glottolog.org/; for Wikipedia articles, 
https://pypi.org/project/Wikipedia-API/; for language fusion and 
informativity, https://github.com/OlenaShcherbakova/Sociodemo-
graphic_factors_complexity/tree/v2.0/data; and for morphological 
complexity, https://github.com/mllewis/langLearnVar. The lan-
guage information density, semantic density and communicative 
speed measures can be found at https://github.com/peteaceves/
Language_Density_and_Communication.
Code availability
The code used to create the language information density and semantic 
density measures was written in Python v.3.7.2 and can be found at 
https://github.com/peteaceves/Language_Density_and_Communica-
tion. The statistical models were run in Stata v.17, and the code can be 
found in the Supplementary Information.
References
1.	
de Saussure, F. Course in General Linguistics (Open Court, 1986).
2.	
Bloomfield, L. Language (Holt, Rinehart & Winston, 1933).
3.	
Sapir, E. Language: An Introduction to the Study of Speech 
(Harcourt, Brace, 1921).
4.	
Levelt, W. J. M. Speaking: From Intention to Articulation (MIT Press, 1989).
5.	
Thompson, B., Roberts, S. G. & Lupyan, G. Cultural influences on 
word meanings revealed through large-scale semantic alignment. 
Nat. Hum. Behav. 4, 1029–1038 (2020).
6.	
Youn, H. et al. On the universal structure of human lexical 
semantics. Proc. Natl Acad. Sci. USA 113, 1766–1771 (2016).
7.	
Winawer, J. et al. Russian blues reveal effects of language on color 
discrimination. Proc. Natl Acad. Sci. USA 104, 7780–7785  
(2007).
8.	
Kay, P. & McDaniel, C. K. The linguistic significance of the 
meanings of basic color terms. Language 54, 610–646 (1978).
9.	
Davidoff, J., Davies, I. & Roberson, D. Colour categories in a 
stone-age tribe. Nature 398, 203–204 (1999).
10.	 Kay, P., Berlin, B., Maffi, L. & Merrifield, W. in Color Categories 
in Language and Thought (eds Hardin, C. L. & Maffi, L.) 21–56 
(Cambridge Univ. Press, 1997).
11.	
Dolscheid, S., Shayan, S., Majid, A. & Casasanto, D. The thickness 
of musical pitch: psychophysical evidence for linguistic relativity. 
Psychol. Sci. 24, 613–621 (2013).
12.	 Bock, K., Carreiras, M. & Meseguer, E. Number meaning and 
number grammar in English and Spanish. J. Mem. Lang. 66, 17–37 
(2012).
13.	 Malt, B. C. et al. Talking about walking: biomechanics  
and the language of locomotion. Psychol. Sci. 19, 232–240 
(2008).
14.	 Malt, B. C. et al. Human locomotion in languages: constraints on 
moving and meaning. J. Mem. Lang. 74, 107–123 (2014).
15.	 Casasanto, D. & Boroditsky, L. Time in the mind: using space to 
think about time. Cognition 106, 579–593 (2008).
16.	 Fuhrman, O. et al. How linguistic and cultural forces shape 
conceptions of time: English and Mandarin time in 3D. Cogn. Sci. 
35, 1305–1328 (2011).
17.	 Lai, V. T. & Boroditsky, L. The immediate and chronic influence 
of spatio-temporal metaphors on the mental representations of 
time in English, Mandarin, and Mandarin-English speakers. Front. 
Psychol. 4, 142 (2013).
18.	 Levinson, S. C. Space in Language and Cognition: Explorations in 
Cognitive Diversity (Cambridge Univ. Press, 2003).
19.	 Levinson, S., Meira, S. & The Language and Cognition  
Group. ‘Natural concepts’ in the spatial topological domain—
adpositional meanings in crosslinguistic perspective: an exercise 
in semantic typology. Language 79, 485–516 (2003).
20.	 Majid, A., Bowerman, M., Kita, S., Haun, D. B. M. & Levinson, S. C. 
Can language restructure cognition? The case for space. Trends 
Cogn. Sci. 8, 108–114 (2004).
21.	 Feist, M. I. Space between languages. Cogn. Sci. 32, 1177–1199 (2008).
22.	 Majid, A., Boster, J. S. & Bowerman, M. The cross-linguistic 
categorization of everyday events: a study of cutting and 
breaking. Cognition 109, 235–250 (2008).
23.	 Saji, N. et al. Word learning does not end at fast-mapping: 
evolution of verb meanings through reorganization of an entire 
semantic domain. Cognition 118, 45–61 (2011).
24.	 Lewis, M. & Lupyan, G. Gender stereotypes are reflected in the 
distributional structure of 25 languages. Nat. Hum. Behav. 4, 
1021–1028 (2020).
25.	 Enfield, N. J., Majid, A. & van Staden, M. Cross-linguistic categorisation 
of the body: introduction. Lang. Sci. 28, 137–147 (2006).
26.	 Brown, C. H. Language and Living Things: Uniformities in Folk 
Classification and Naming (Rutgers Univ. Press, 1984).
27.	 Berlin, B. Ethnobiological Classification: Principles of Categorization  
of Plants and Animals in Traditional Societies (Princeton Univ. 
Press, 2014).
28.	 Kemp, C., Xu, Y. & Regier, T. Semantic typology and efficient 
communication. Annu. Rev. Linguist. 4, 109–128 (2018).
29.	 Enfield, N. J. Linguistic relativity from reference to agency. Annu. 
Rev. Anthropol. 44, 207–224 (2015).
30.	 Hofstadter, D. & Sander, E. Surfaces and Essences: Analogy as the 
Fuel and Fire of Thinking (Basic Books, 2013).
31.	 Li, P. & Gleitman, L. Turning the tables: language and spatial 
reasoning. Cognition 83, 265–294 (2002).
32.	 Gleitman, L. & Fisher, C. in The Cambridge Companion to 
Chomsky (ed. McGilvray, J. A.) 123–142 (Cambridge Univ. Press, 
2005).
33.	 Pinker, S. The Language Instinct (HarperCollins, 1994).
34.	 Berlin, B. & Kay, P. Basic Color Terms: Their Universality and 
Evolution (Univ. California Press, 1969).
35.	 Evans, N. & Levinson, S. C. The myth of language universals: 
language diversity and its importance for cognitive science. 
Behav. Brain Sci. 32, 429–448, discussion 448–494 (2009).
36.	 Davidson, D. On the very idea of a conceptual scheme. Proc. 
Addresses Am. Phil. Assoc. 47, 5–20 (1973).
37.	 Lupyan, G. & Dale, R. Why are there different languages? The 
role of adaptation in linguistic diversity. Trends Cogn. Sci. 20, 
649–660 (2016).
Nature Human Behaviour | Volume 8 | April 2024 | 644–656
655
Article
https://doi.org/10.1038/s41562-024-01815-w
38.	 Pellegrino, F., Coupé, C. & Marsico, E. Across-language perspective 
on speech information rate. Language 87, 539–558 (2011).
39.	 Coupé, C., Oh, Y. M., Dediu, D. & Pellegrino, F. Different languages, 
similar encoding efficiency: comparable information rates across 
the human communicative niche. Sci. Adv. 5, eaaw2594 (2019).
40.	 Lewis, M., Cahill, A., Madnani, N. & Evans, J. Local similarity and 
global variability characterize the semantic space of human 
languages. Proc. Natl Acad. Sci. 120, e2300986120 (2023).
41.	 Gibson, E. et al. How efficiency shapes human language. Trends 
Cogn. Sci. 23, 389–407 (2019).
42.	 Bentz, C., Alikaniotis, D., Cysouw, M. & Ferrer-i-Cancho, R. The 
entropy of words—learnability and expressivity across more than 
1000 languages. Entropy 19, 275 (2017).
43.	 Bellos, D. Is That a Fish in Your Ear? Translation and the Meaning of 
Everything (Penguin Books, 2011).
44.	 Huffman, D. A. A method for the construction of minimum- 
redundancy codes. Proc. IRE 40, 1098–1101 (1952).
45.	 Harris, Z. S. Distributional structure. Word World 10, 146–162 
(1954).
46.	 Jurafsky, D. & Martin, J. H. Speech and Language Processing 
(Stanford Univ., 2018).
47.	 Mikolov, T., Sutskever, I., Chen, K., Corrado, G. S. & Dean, J. 
Distributed representations of words and phrases and their 
compositionality. In Advances in Neural Information Processing 
Systems Vol. 26 (MIT Press, 2013).
48.	 Pennington, J., Socher, R. & Manning, C. Glove: global vectors 
for word representation. In Proc. 2014 Conference on Empirical 
Methods in Natural Language Processing (EMNLP) 1532–1543 
(Association for Computational Linguistics, 2014).
49.	 Kozlowski, A. C., Taddy, M. & Evans, J. A. The geometry of culture: 
analyzing the meanings of class through word embeddings. Am. 
Sociol. Rev. 84, 905–949 (2019).
50.	 Caliskan, A., Bryson, J. J. & Narayanan, A. Semantics derived 
automatically from language corpora contain human-like biases. 
Science 356, 183–186 (2017).
51.	 Bolukbasi, T., Chang, K.-W., Zou, J. Y., Saligrama, V. & Kalai, A. T. 
Man is to computer programmer as woman is to homemaker? 
Debiasing word embeddings. In Advances in Neural Information 
Processing Systems 4349–4357 (MIT Press, 2016).
52.	 Hamilton, W. L., Leskovec, J. & Jurafsky, D. Diachronic word 
embeddings reveal statistical laws of semantic change. Preprint 
at https://arxiv.org/abs/1605.09096v6 (2016).
53.	 Arora, S., Li, Y., Liang, Y., Ma, T. & Risteski, A. Linear algebraic 
structure of word senses, with applications to polysemy. Trans. 
Assoc. Comput. Linguist. 6, 483–495 (2018).
54.	 Shannon, C. E. A mathematical theory of communication. Bell 
Syst. Tech. J. 27, 379–423 (1948).
55.	 Mnih, A. & Hinton, G. Three new graphical models for statistical 
language modelling. In Proc. 24th International Conference 
on Machine Learning 641–648 (Association for Computing 
Machinery, 2007).
56.	 Arora, S., Li, Y., Liang, Y., Ma, T. & Risteski, A. A latent variable 
model approach to PMI-based word embeddings. Trans. Assoc. 
Comput. Linguist. 4, 385–399 (2016).
57.	 Davelaar, E. J. & Raaijmakers, J. G. W. in Cognitive Search: 
Evolution, Algorithms, and the Brain (eds Todd, P. M. et al.) 177–194 
(MIT Press, 2012).
58.	 Romney, A. K., Brewer, D. D. & Batchelder, W. H. Predicting 
clustering from semantic structure. Psychol. Sci. 4, 28–34 (1993).
59.	 Howard, M. W., Jing, B., Addis, K. M. & Kahana, M. J. Semantic 
structure and episodic memory Ch. 7. in LSA: A Road Towards 
Meaning (eds McNamara, D. & Dennis, S.) (Erlbaum, 2007).
60.	 Abbott, J. T., Austerweil, J. L. & Griffiths, T. L. Random walks on 
semantic networks can resemble optimal foraging. Psychol. Rev. 
122, 558–569 (2015).
61.	 Hills, T. T., Todd, P. M. & Jones, M. N. Foraging in semantic fields: 
how we search through memory. Top. Cogn. Sci. 7, 513–534 (2015).
62.	 Charnov, E. L. Optimal foraging, the marginal value theorem. 
Theor. Popul. Biol. 9, 129–136 (1976).
63.	 Pirolli, P. L. T. Information Foraging Theory: Adaptive Interaction 
with Information (Oxford Univ. Press, 2007).
64.	 Harbison, J. I., Dougherty, M. R., Davelaar, E. J. & Fayyad, B. On the 
lawfulness of the decision to terminate memory search. Cognition 
111, 416–421 (2009).
65.	 Lewis, M. & Frank, M. C. Linguistic niches emerge from pressures at 
multiple timescales. In Proc. 38th Annual Conference of the Cognitive 
Science Society 1385–1390 (Cognitive Science Society, 2016).
66.	 Lupyan, G. & Dale, R. Language structure is partly determined by 
social structure. PLoS ONE 5, e8559 (2010).
67.	 Shcherbakova, O. et al. Societies of strangers do not speak less 
complex languages. Sci. Adv. 9, eadf7704 (2023).
68.	 Pellegrino, F., Coupé, C. & Marsico, E. A cross-language 
perspective on speech information rate. Language 87, 539–558 
(2011).
69.	 Schürmann, T. & Grassberger, P. Entropy estimation of symbol 
sequences. Chaos 6, 414–427 (1996).
70.	 Shannon, C. E. Prediction and entropy of printed English. Bell 
Syst. Tech. J. 30, 50–64 (1951).
71.	 Kontoyiannis, I., Algoet, P. H., Suhov, Y. M. & Wyner, A. J. 
Nonparametric entropy estimation for stationary processes and 
random fields, with applications to English text. IEEE Trans. Inf. 
Theory 44, 1319–1327 (1998).
72.	 Levinson, S. C. Presumptive Meanings: The Theory of Generalized 
Conversational Implicature (MIT Press, 2000).
73.	 Caplan, S., Kodner, J. & Yang, C. Miller’s monkey updated: 
communicative efficiency and the statistics of words in natural 
language. Cognition 205, 104466 (2020).
74.	 Brochhagen, T. & Boleda, G. When do languages use the same 
word for different meanings? The Goldilocks principle in 
colexification. Cognition 226, 105179 (2022).
75.	 Bentz, C., Dediu, D., Verkerk, A. & Jäger, G. The evolution of 
language families is shaped by the environment beyond neutral 
drift. Nat. Hum. Behav. 2, 816–821 (2018).
76.	 Olson, J. A., Nahas, J., Chmoulevitch, D., Cropper, S. J. &  
Webb, M. E. Naming unrelated words predicts creativity. Proc. 
Natl Acad. Sci. USA 118, e2022340118 (2021).
77.	 Woolley, A. W., Chabris, C. F., Pentland, A., Hashmi, N. & Malone, T. 
W. Evidence for a collective intelligence factor in the performance 
of human groups. Science 330, 686–688 (2010).
78.	 McGrath, J. E. Groups: Interaction and Performance (Prentice-Hall, 
1984).
79.	 McMahan, P. & Evans, J. Ambiguity and engagement. Am. J. 
Sociol. 124, 860–912 (2018).
80.	 Murray, D. et al. Unsupervised embedding of trajectories captures 
the latent structure of mobility. Preprint at https://arxiv.org/
abs/2012.02785 (2020).
81.	 Lucy, J. A. Linguistic relativity. Annu. Rev. Anthropol. 26, 291–312 
(1997).
82.	 Lucy, J. A. Language Diversity and Thought: A Reformulation of the 
Linguistic Relativity Hypothesis (Cambridge Univ. Press, 1992).
83.	 Tiedemann, J. Parallel data, tools and interfaces in OPUS. In 
Proc. 8th International Conference on Language Resources and 
Evaluation (eds Calzolari, N. et al.) 2214–2218 (European Language 
Resources Association, 2012).
84.	 Christodouloupoulos, C. & Steedman, M. A massively parallel 
corpus: the Bible in 100 languages. Lang. Resour. Eval. 49, 
375–395 (2015).
85.	 YouVersion https://www.bible.com/ (2017).
86.	 ParaCrawl (NTT Communication Science Laboratories, accessed  
1 June 2017); https://www.paracrawl.eu/
Nature Human Behaviour | Volume 8 | April 2024 | 644–656
656
Article
https://doi.org/10.1038/s41562-024-01815-w
87.	 OpenSubtitles https://www.opensubtitles.org/ (2017).
88.	 Rafalovitch, A. & Dale, R. United Nations General Assembly 
resolutions: a six-language parallel corpus. In Proc. MT Summit XII 
(Association for Computational Linguistics, 2009).
89.	 Juola, P. Measuring linguistic complexity: the morphological tier. 
J. Quant. Linguist. 5, 206–213 (1998).
90.	 Mikolov, T., Chen, K., Corrado, G. & Dean, J. Efficient estimation  
of word representations in vector space. Preprint at  
https://arxiv.org/abs/1301.3781 (2013).
91.	 Mikolov, T., Yih, W.-T. & Zweig, G. Linguistic regularities in 
continuous space word representations. In Proc. NAACL-HLT 2013 
746–751 (Association for Computational Linguistics, 2013).
92.	 Hammarström, H., Forkel, R., Haspelmath, M. & Bank, S.  
Glottolog v.4.4. Max Planck Institute for Evolutionary Anthropology 
https://glottolog.org (2021).
93.	 Chen, X., Ender, P., Mitchell, M. & Wells, C. Regression  
with Stata. UCLA  https://stats.oarc.ucla.edu/stata/webbooks/reg/ 
(2003).
94.	 Huber, P. J. The behavior of maximum likelihood estimates  
under nonstandard conditions. In Proc. 5th Berkeley  
Symposium on Mathematical Statistics and Probability Vol. 1  
(Eds Le Cam, L. M. & Neyman, J.) 221–233 (Univ. California Press, 
1967).
95.	 White, H. A heteroskedasticity-consistent covariance matrix 
estimator and a direct test for heteroskedasticity. Econometrica 
48, 817–838 (1980).
96.	 Bills, A. et al. IARPA Babel Amharic Language Pack 
IARPA-babel307b-v1.0b (Linguistic Data Consortium, 2019); 
https://doi.org/10.35111/ehfb-ka57
97.	 Bills, A. et al. IARPA Babel Bengali Language Pack 
IARPA-babel103b-v0.4b (Linguistic Data Consortium, 2016); 
https://doi.org/10.35111/5jdb-wp44
98.	 Andresen, L. et al. IARPA Babel Cebuano Language Pack 
IARPA-babel301b-v2.0b (Linguistic Data Consortium, 2018); 
https://doi.org/10.35111/f0b3-5398
99.	 Bills, A. et al. IARPA Babel Georgian Language Pack 
IARPA-babel404b-v1.0a (Linguistic Data Consortium, 2016); 
https://doi.org/10.35111/dcr5-ga44
100.	Andresen, L. et al. IARPA Babel Guarani Language Pack 
IARPA-babel305b-v1.0c (Linguistic Data Consortium, 2019); 
https://doi.org/10.35111/qdg9-7a64
101.	 Adams, N. et al. IARPA Babel Igbo Language Pack 
IARPA-babel306b-v2.0c (Linguistic Data Consortium, 2019); 
https://doi.org/10.35111/7988-wd73
102.	Bills, A. et al. IARPA Babel Kazakh Language Pack 
IARPA-babel302b-v1.0a (Linguistic Data Consortium, 2018); 
https://doi.org/10.35111/rwmc-nm96
103.	Benowitz, D. et al. IARPA Babel Lithuanian Language Pack 
IARPA-babel304b-v1.0b (Linguistic Data Consortium, 2019); 
https://doi.org/10.35111/m5qd-dk93
104.	Conners, T. et al. IARPA Babel Tagalog Language Pack 
IARPA-babel106-v0.2g (Linguistic Data Consortium, 2016);  
https://doi.org/10.35111/mp23-rd11
105.	Bills, A. et al. IARPA Babel Tamil Language Pack 
IARPA-babel204b-v1.1b (Linguistic Data Consortium, 2017);  
https://doi.org/10.35111/3j2w-kb06
106.	Bills, A. et al. IARPA Babel Telugu Language Pack 
IARPA-babel303b-v1.0a (Linguistic Data Consortium, 2018); 
https://doi.org/10.35111/vm6x-za86
107.	 Andresen, J. et al. IARPA Babel Turkish Language Pack 
IARPA-babel105b-v0.5 (Linguistic Data Consortium, 2016);  
https://doi.org/10.35111/mb8z-6p26
108.	Andrus, T. et al. IARPA Babel Vietnamese Language Pack 
IARPA-babel107b-v0.7 (Linguistic Data Consortium, 2017);  
https://doi.org/10.35111/yrqp-r555
109.	Adams, N. et al. IARPA Babel Zulu Language Pack IARPA-babel206b- 
v0.1e LDC2017S19 (Linguistic Data Consortium, 2017); https://doi.
org/10.35111/te29-8988
110.	 Bojanowski, P., Grave, E., Joulin, A. & Mikolov, T. Enriching 
word vectors with subword information. In Transactions of the 
Association for Computational Linguistics Vol. 5 (Association for 
Computational Linguistics, 2017).
111.	 Grave, E., Bojanowski, P., Gupta, P., Joulin, A. & Mikolov, T. 
Learning word vectors for 157 languages. In Proc. International 
Conference on Language Resources and Evaluation (Eds Calzolari, 
N. et. al.) (European Language Resources Association, 2018).
112.	 Gordon, R. G. Ethnologue, Languages of the World (SIL International, 
accessed 1 October 2017); https://www.ethnologue.com
113.	 Hofstede, G. Culture’s Consequences: Comparing Values, Behaviors, 
Institutions and Organizations Across Nations (SAGE, 2001).
114.	 Hofstede, G. Culture’s Consequences: International Differences in 
Work-Related Values (Sage, 1984).
Acknowledgements
We thank Z. Chen (Stanford University) for her excellent research 
assistance and are grateful for comments from C. Chambers  
(Johns Hopkins University), M. Lewis (Meta), D. Casasanto (Cornell 
University), G. Lupyan (University of Wisconsin-Madison), J. L. Martin 
(University of Chicago), A. Sharkey (Arizona State University),  
J. Murphy (RAND Corporation) and J. Chu (Massachusetts Institute 
of Technology). P.A. also thanks the judges of the 2017 INFORMS/
Organization Science Dissertation Proposal Competition for their 
feedback and acknowledges support from the National Science 
Foundation Doctoral Dissertation Research Improvement Grant (no. 
1702788). The funder had no role in study design, data collection and 
analysis, decision to publish or preparation of the manuscript.
Author contributions
P.A. and J.A.E. designed the research. P.A. analysed the data. P.A. and 
J.A.E. wrote the paper.
Competing interests
The authors declare no competing interests.
Additional information
Supplementary information The online version  
contains supplementary material available at  
https://doi.org/10.1038/s41562-024-01815-w.
Correspondence and requests for materials should be addressed to 
Pedro Aceves.
Peer review information Nature Human Behaviour thanks N. Enfield 
and the other, anonymous, reviewer(s) for their contribution to the 
peer review of this work. Peer reviewer reports are available.
Reprints and permissions information is available at  
www.nature.com/reprints.
Publisher’s note Springer Nature remains neutral with regard to 
jurisdictional claims in published maps and institutional affiliations.
Springer Nature or its licensor (e.g. a society or other partner) holds 
exclusive rights to this article under a publishing agreement with 
the author(s) or other rightsholder(s); author self-archiving of the 
accepted manuscript version of this article is solely governed by the 
terms of such publishing agreement and applicable law.
© The Author(s), under exclusive licence to Springer Nature Limited 
2024
1
nature portfolio  |  reporting summary
April 2023
Corresponding author(s):
Pedro Aceves
Last updated by author(s): Dec 13, 2023
Reporting Summary
Nature Portfolio wishes to improve the reproducibility of the work that we publish. This form provides structure for consistency and transparency 
in reporting. For further information on Nature Portfolio policies, see our Editorial Policies and the Editorial Policy Checklist.
Statistics
For all statistical analyses, confirm that the following items are present in the figure legend, table legend, main text, or Methods section.
n/a Confirmed
The exact sample size (n) for each experimental group/condition, given as a discrete number and unit of measurement
A statement on whether measurements were taken from distinct samples or whether the same sample was measured repeatedly
The statistical test(s) used AND whether they are one- or two-sided 
Only common tests should be described solely by name; describe more complex techniques in the Methods section.
A description of all covariates tested
A description of any assumptions or corrections, such as tests of normality and adjustment for multiple comparisons
A full description of the statistical parameters including central tendency (e.g. means) or other basic estimates (e.g. regression coefficient) 
AND variation (e.g. standard deviation) or associated estimates of uncertainty (e.g. confidence intervals)
For null hypothesis testing, the test statistic (e.g. F, t, r) with confidence intervals, effect sizes, degrees of freedom and P value noted 
Give P values as exact values whenever suitable.
For Bayesian analysis, information on the choice of priors and Markov chain Monte Carlo settings
For hierarchical and complex designs, identification of the appropriate level for tests and full reporting of outcomes
Estimates of effect sizes (e.g. Cohen's d, Pearson's r), indicating how they were calculated
Our web collection on statistics for biologists contains articles on many of the points above.
Software and code
Policy information about availability of computer code
Data collection
The only data in the paper that were collected through software was that used for the analysis of Wikipedia articles. We used Python version 
3.7.2 as well as the Wikipedia API, which was accessed through https://pypi.org/project/Wikipedia-API/ All of the other data was hand 
collected.
Data analysis
Data was cleaned using Python version 3.7.2. Data graphs were created using Python version 3.7.2. Statistical analyses were carried out in 
Stata version 17. The code used to create the measures in the paper can be found at https://github.com/peteaceves/
Language_Density_and_Communication. 
For manuscripts utilizing custom algorithms or software that are central to the research but not yet described in published literature, software must be made available to editors and 
reviewers. We strongly encourage code deposition in a community repository (e.g. GitHub). See the Nature Portfolio guidelines for submitting code & software for further information.
2
nature portfolio  |  reporting summary
April 2023
Data
Policy information about availability of data
All manuscripts must include a data availability statement. This statement should provide the following information, where applicable: 
- Accession codes, unique identifiers, or web links for publicly available datasets 
- A description of any restrictions on data availability 
- For clinical datasets or third party data, please ensure that the statement adheres to our policy 
 
The datasets analyzed during the current study are available at the following links:  
- Parallel corpora: https://opus.nlpl.eu/    
- Conversations: https://www.ldc.upenn.edu/    
- Audio duration: https://wordproject.org   https://faithcomesbyhearing.com    
- Language family and location: https://glottolog.org/    
- Wikipedia articles: https://pypi.org/project/Wikipedia-API/  
- Language fusion and informativity: https://github.com/OlenaShcherbakova/Sociodemographic_factors_complexity/tree/v2.0/data 
- Morphological complexity: https://github.com/mllewis/langLearnVar
Research involving human participants, their data, or biological material
Policy information about studies with human participants or human data. See also policy information about sex, gender (identity/presentation), 
and sexual orientation and race, ethnicity and racism.
Reporting on sex and gender
We gathered pre-existing, published human generated texts across languages, but do not have estimates for the sex and 
gender distribution of speakers/authors. 
Reporting on race, ethnicity, or 
other socially relevant 
groupings
We gathered pre-existing, published human generated texts across languages, but do not have estimates of the 
demographics of the speakers/authors. 
Population characteristics
See above.
Recruitment
Our study was observational. We did not recruit participants. 
Ethics oversight
The study was deemed exempt by the University of Chicago Social and Behavioral Sciences IRB.
Note that full information on the approval of the study protocol must also be provided in the manuscript.
Field-specific reporting
Please select the one below that is the best fit for your research. If you are not sure, read the appropriate sections before making your selection.
Life sciences
Behavioural & social sciences
 Ecological, evolutionary & environmental sciences
For a reference copy of the document with all sections, see nature.com/documents/nr-reporting-summary-flat.pdf
Life sciences study design
All studies must disclose on these points even when the disclosure is negative.
Sample size
Describe how sample size was determined, detailing any statistical methods used to predetermine sample size OR if no sample-size calculation 
was performed, describe how sample sizes were chosen and provide a rationale for why these sample sizes are sufficient.
Data exclusions
Describe any data exclusions. If no data were excluded from the analyses, state so OR if data were excluded, describe the exclusions and the 
rationale behind them, indicating whether exclusion criteria were pre-established. 
Replication
Describe the measures taken to verify the reproducibility of the experimental findings. If all attempts at replication were successful, confirm this 
OR if there are any findings that were not replicated or cannot be reproduced, note this and describe why.
Randomization
Describe how samples/organisms/participants were allocated into experimental groups. If allocation was not random, describe how covariates 
were controlled OR if this is not relevant to your study, explain why.
Blinding
Describe whether the investigators were blinded to group allocation during data collection and/or analysis. If blinding was not possible, 
describe why OR explain why blinding was not relevant to your study.
3
nature portfolio  |  reporting summary
April 2023
Behavioural & social sciences study design
All studies must disclose on these points even when the disclosure is negative.
Study description
Our data involve quantitative measurements assessed on text and audio files. Across 998 languages, we measure information 
density, semantic density, and communication speed for parallel translation corpora in order to demonstrate that higher density 
languages communicate information more quickly, and that conversations in dense languages retrace and cycle over a narrower 
conceptual terrain. 
Research sample
The following parallel translation corpora were collected from https://opus.nlpl.eu/ in order to estimate the linguistic information 
and semantic densities. (1) complete Bible (Old and New Testaments); (2) New Testament; (3) news transcripts ; (4) web text; (5) 
movie subtitles; (6) TED talks; (7) example sentences for foreign language learners; (8) United Nations; (9) European Central Bank; 
(10) European Medicines Agency; (11) European Union bookshop;(12) European Union Directorate General for Translation; (13) 
European Union Joint Research Centre; (14) European Parliamentary Proceedings; (15) GNOME software files; and (16) KDE software 
files.  
 
To measure the communicative speed of a language, we collected the duration time of the audio Bible spoken in different languages. 
The audio Bible duration data was manually collected from metadata of the MP3 files found at wordproject.org and 
faithcomesbyhearing.com. 
 
Our conversational data came from the Babel Program of the  Intelligence Advanced Research Projects Agency, collected from the 
Linguistic Data Consortium Catalog.  
 
Wikipedia article data was collected through the Wikipedia Python API. 
Sampling strategy
We used complete corpora where available; no explicit sampling was performed. 
Data collection
Data collection involved accessing large published collections of archival documents in 998 languages.
Timing
We now state that "Our data were passively accessed and accumulated between 9/1/2016 and 11/10/2023."
Data exclusions
No data were excluded from the multilingual corpora mentioned above. 
Non-participation
No participants were involved in the study. The research was archival (e.g., measurements were made on pre-existing, public 
documents).
Randomization
Randomization was not relevant because this was an observational and not an experimental study in which our measurements are 
assessed on all available language documents (and not a randomized subset). 
Ecological, evolutionary & environmental sciences study design
All studies must disclose on these points even when the disclosure is negative.
Study description
Briefly describe the study. For quantitative data include treatment factors and interactions, design structure (e.g. factorial, nested, 
hierarchical), nature and number of experimental units and replicates.
Research sample
Describe the research sample (e.g. a group of tagged Passer domesticus, all Stenocereus thurberi within Organ Pipe Cactus National 
Monument), and provide a rationale for the sample choice. When relevant, describe the organism taxa, source, sex, age range and 
any manipulations. State what population the sample is meant to represent when applicable. For studies involving existing datasets, 
describe the data and its source.
Sampling strategy
Note the sampling procedure. Describe the statistical methods that were used to predetermine sample size OR if no sample-size 
calculation was performed, describe how sample sizes were chosen and provide a rationale for why these sample sizes are sufficient.
Data collection
Describe the data collection procedure, including who recorded the data and how.
Timing and spatial scale
Indicate the start and stop dates of data collection, noting the frequency and periodicity of sampling and providing a rationale for 
these choices. If there is a gap between collection periods, state the dates for each sample cohort. Specify the spatial scale from which 
the data are taken
Data exclusions
If no data were excluded from the analyses, state so OR if data were excluded, describe the exclusions and the rationale behind them, 
indicating whether exclusion criteria were pre-established.
Reproducibility
Describe the measures taken to verify the reproducibility of experimental findings. For each experiment, note whether any attempts to 
repeat the experiment failed OR state that all attempts to repeat the experiment were successful.
Randomization
Describe how samples/organisms/participants were allocated into groups. If allocation was not random, describe how covariates were 
controlled. If this is not relevant to your study, explain why.
4
nature portfolio  |  reporting summary
April 2023
Blinding
Describe the extent of blinding used during data acquisition and analysis. If blinding was not possible, describe why OR explain why 
blinding was not relevant to your study.
Did the study involve field work?
Yes
No
Field work, collection and transport
Field conditions
Describe the study conditions for field work, providing relevant parameters (e.g. temperature, rainfall).
Location
State the location of the sampling or experiment, providing relevant parameters (e.g. latitude and longitude, elevation, water depth).
Access & import/export
Describe the efforts you have made to access habitats and to collect and import/export your samples in a responsible manner and in 
compliance with local, national and international laws, noting any permits that were obtained (give the name of the issuing authority, 
the date of issue, and any identifying information).
Disturbance
Describe any disturbance caused by the study and how it was minimized.
Reporting for specific materials, systems and methods
We require information from authors about some types of materials, experimental systems and methods used in many studies. Here, indicate whether each material, 
system or method listed is relevant to your study. If you are not sure if a list item applies to your research, read the appropriate section before selecting a response. 
Materials & experimental systems
n/a Involved in the study
Antibodies
Eukaryotic cell lines
Palaeontology and archaeology
Animals and other organisms
Clinical data
Dual use research of concern
Plants
Methods
n/a Involved in the study
ChIP-seq
Flow cytometry
MRI-based neuroimaging
Novel plant genotypes
Describe the methods by which all novel plant genotypes were produced. This includes those generated by transgenic approaches, 
gene editing, chemical/radiation-based mutagenesis and hybridization. For transgenic lines, describe the transformation method, the 
number of independent lines analyzed and the generation upon which experiments were performed. For gene-edited lines, describe 
the editor used, the endogenous sequence targeted for editing, the targeting guide RNA sequence (if applicable) and how the editor 
was applied.
Seed stocks
Report on the source of all seed stocks or other plant material used. If applicable, state the seed stock centre and catalogue number. If 
plant specimens were collected from the field, describe the collection location, date and sampling procedures.
Authentication
Describe any authentication procedures for each seed stock used or novel genotype generated. Describe any experiments used to 
assess the effect of a mutation and, where applicable, how potential secondary effects (e.g. second site T-DNA insertions, mosiacism, 
off-target gene editing) were examined.
Plants
