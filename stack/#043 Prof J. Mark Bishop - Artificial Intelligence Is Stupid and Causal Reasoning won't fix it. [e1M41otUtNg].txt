Do you remember the taste of the last orange you ate?
Do you remember the warmth of that welcoming cup of tea that you had when you came in from
the cold November rain?
Do you remember the smell of the hair of the first love that you kissed?
I'd like you to try and hold on to these sensations because I'm going to come back to them.
Welcome back to the Machine Learning Street Talk YouTube channel and podcast with me,
your host Dr Tim Skaaf. If you want to be exposed to completely new ideas and challenged intellectually
then this episode is probably the episode for you. Last time we had a philosopher on the show
we had absolutely atrocious viewing numbers but I'm a big believer that we need to be
challenging our preconceptions on absolutely everything. Pedro Domingo said that there were
only five tribes in artificial intelligence and he didn't even consider the other tribe
which not many people talk about, cybernetics. Cybernetics is the science of communications
and automatic control systems in both machines and living things. We're going to discuss AI
across three key dimensions today. Computability, understanding and the phenomenological experience
or consciousness. When we talk about artificial intelligence what we basically mean is the science
and engineering. We're trying to engineer machines to do things that we might say is clever.
Professor Mark Bishop does not think that computers can be conscious or have phenomenological
states of consciousness unless we're willing to accept panpsychism which is the idea that
mentality is fundamental and ubiquitous in the natural world or put simply that you're
goldfish and everything else for that matter has a mind. Panpsychism postulates that distinctions
between intelligences are largely arbitrary. Mark's argument is distinct from Searle's argument
that computers cannot understand and also from Roger Penrose's view that some tasks which humans
perform are simply non-computable. He thinks that there is no objective fact of the matter
about which computations a physical system is computing. This is because of the observer
relative problem which Mark will outline in great detail in today's episode. Many of the
ideas we're going to discuss today are anathema to the current modus operandi in artificial
intelligence research. Just the reading list we got from Mark today will keep us busy for the next
year. Mark's work in the philosophy of AI led to an influential critique of computational approaches
to artificial intelligence through a thorough examination of John Searle's the Chinese remarkument
and we'll be discussing that in great detail later. Mark is also the scientific advisor to
Fact 360, a startup deploying artificial intelligence using natural language processing
for e-discovery or detecting malicious insiders by subtle changes in language in human communication
networks. Insiders who might pose a threat to your organization and they use sophisticated
graph analysis to do that. Mark just published a paper called artificial intelligence is stupid
and causal reasoning won't fix it. He makes it clear in this paper that in his opinion computers
will never be able to compute everything, understand anything or feel anything. For much of the 20th
century the dominant cognitive paradigm identified the mind with the brain. You, your joys and
your sorrows, your memories and your ambitions, your sense of personal identity and free will are
in fact no more than the behavior of a vast assembly of nerve cells and their associated
molecules. You're nothing but a pack of neurons and that was according to Crick in 1994. The church
Turing hypothesis stated that every function which would naturally be regarded as computable
could be computed by the universal Turing machine. If only computers could adequately
model the brain then the theory goes it ought to be possible to program them to act like minds
with its myriad of features running the gamut from causal learning, reasoning and understanding.
Even Bengio observed in 2019 that we know from prior experience which features are the salient
features and that comes from a deep understanding of the structure of our world. The church Turing
hypothesis has triggered an explosion of interest in biologically plausible neural networks. We had
Dr. Simon Stringer on the show last week talking about the spiking dynamics, the temporal binding
circuits which emerge when you create some of these biologically inspired neural networks but
I'm not just talking about the biologically inspired versions, even the relatively pedestrian
vanilla neural networks that we all know and love on this channel. This has all been a huge
focus point for the last 50 years or so. AI eschatologists like Ray Kurzweil and Nick Bostrom
believe that there might be an intelligence explosion where all of humankind will inevitably
be crushed like ants, although viewers of this channel will well know Francois Chollet's response
to this view. Alan Turing deployed an effective method to play chess in 1948, many decades ago,
but since then we've seen little progress in getting machines to actually genuinely understand,
to seamlessly apply knowledge from one domain into another. Judea Pearl believes that we won't
succeed in realizing strong AI until we can equip systems with a mastery of causation.
He thinks we need to move away from simplistic probabilistic associations to machines which
can reason causally. He even proposed a so-called ladder of causation which is seeing, doing and
imagining, which I feel is almost self-explanatory actually. Unfortunately for Pearl, DeepMind have
already demonstrated several times a reinforcement learning system which can perform causal reasoning
and counterfactual analysis. It seems obvious to me because if you're interacting with a system,
then of course you can learn the causal factors. I'd completely take Pearl's point that with
traditional machine learning where you're not interacting with a system, you can't learn any
causal factors. That seems quite intuitive. Anyway, all of this is small fry compared to the
point which Professor Bishop wants to make. The idea that these silicon ensconced algorithms can
become thinking machines becomes a little bit bizarre once you realize that a machine has no
choice in what it does. Computation is not an objective fact of the world, it's observer relative.
Even Wittgenstein said that the meaning of a computation is in its use. He thought that
understanding could not be a process and therefore it cannot be a process of symbol manipulation.
Whether a given individual understands is often external to that individual. Mark's intuition
is that evolution, autonomy and environmental interactions give rise to phenomenological
consciousness. He thinks that we cannot live inside a computer simulation because he can
feel the sensation of cool air on his face. So Mark thinks that the meaning of computation
becomes relative and lies in its use by humans. Mark gives several examples in the show this
evening demonstrating precisely why he thinks this. So I think Mark's main contribution is this
dancing with Pixie's reductio ad absurdum. He quotes Hillary Putnam. In 1988 the influential
American philosopher Hillary Putnam published a paper in which she showed that under the influence
of gravitational waves and cosmic rays and the subatomic particles that make up all the objects
of our world, your seat, the seat you're sitting on, the very clothes you're wearing, the room that
we're in, they're all containing a rich dance of subatomic particles, a dance that never repeats
itself. And Putnam realized that this is analogous to a state machine going through an infinite series
of non-repeating states. So it then seems to me that if a computer, a terminator perhaps, is conscious
purely as a result of moving through a series of computational state transitions, then if I
know the input to that machine with input fixed, I can generate exactly the same series of state
transitions with any large counter like a car's marlometer or following Hillary Putnam's move
with any open physical system. So if a machine is conscious merely as a result of following some
computation, then consciousness is everywhere in the bricks of this building, the clothes that
you're wearing, the very seat you're sitting on, they are all experiencing the zing of that orange,
the warmth of that cup of tea, and the memory of your first love's kiss. If machine consciousness
is possible, everything, even the smallest grain of sand, is filled with an infinitude of conscious
experiences. Bishop interprets Putnam's result to mean that computationalism demands that every
physical system is host to a multitude of conscious minds, which he refers to as little pixies.
Since a computationalist believes that to be a conscious mind is just to implement the right
kind of computation, not only would we be surrounded by pixies, but the vast majority of
conscious experience would be realized in these pixies. Since any physical system is
implementing any and all computations simultaneously, then all possible conscious minds must be
instantiated simultaneously in every physical object. For Bishop, this is the most patently absurd
manifestation of panpsychism, and thus demonstrates that computationalism must be false.
So it seems like a contrarian position that Bishop is saying that computation is very much in the
eye of the beholder, whereas most of us think that computation goes on inside our brains.
Anyway, the key takeaway from the Dancing with Pixies reductio ad absurdum is that computation
doesn't have those phenomenological conscious states. A finite state automata cannot give rise
to conscious experience, unless conscious experience is in everything. Bishop says that he's an embodied
entity, which is to say he's not just thinking in his brain, he thinks with his body and his body
in the world. In today's episode, we also talk about some of the greats of computability,
mathematics, and logic, starting with Alan Turing on computability. He described a machine
called a discrete state machine. I now call it Turing's discrete state machine because that was
the first time I read about it in his work. So over any short time period, we can replicate the
behavior of the different state transitions of Turing's discrete state machine with any other
one such as a digital automata. But because when we added input, the number of possible state
sequences grew exponentially, we can't easily do the same thing when you have a machine with input.
But then I realized, if I know the input to one of these machines,
that combinatorial state structure collapses again just to a simple list of state transitions.
He invented this interesting thought experiment called the discrete state machine,
and he had this physicalist desire to explain all of humanity via a computer program.
And interesting, what he learned later on in his career about the non-computability of numbers
led to a significant amount of tension for him later on in life and his children.
The American philosopher, John Searle, was so exasperated that anyone might seriously entertain
the idea that computational systems, purely based on the execution of appropriate software,
no matter how complex, might actually understand... it was ridiculous. He formulated the now
infamous Chinese room experiment, and we'll go into this in some detail in the show,
but essentially he said that syntax is not sufficient for semantics, and that programs
are not formal, and minds have content. Therefore, programs are not minds, and computationalism
must be false. Now, most of the Chinese room argument is the first proposition, which is that
syntax is not sufficient for semantics, and we will come back to that later.
Another interesting character is Godel. Godel's first incompleteness theorem famously stated
that any effectively generated theory capable of expressing elementary arithmetic cannot be
both consistent and complete. In particular, for any consistent, effectively generated,
formal theory, F, that proves certain basic arithmetic truths. There is an arithmetic statement
that is true, but not provable, in the theory. And this can be used almost anywhere, and it's
often referred to as the Godel sentence for a particular theory, and it was used in anger
by Roger Penrose. He made the Godelian argument that mathematical insight cannot be computable.
He said that the mental procedures whereby mathematicians arrive at their judgments of
truth are not simply rooted in the procedures of some specific formal system. And he followed up by
saying, human mathematicians are not using a knowably sound argument to ascertain mathematical
truth. Anyway, I really hope you enjoyed the show today. I'm absolutely honored that Mark came on
to discuss this with us. I'm very interested in the philosophy of AI and the philosophy of mind.
Make sure you read some of the material that Mark has signposted, and I'll link those in the
description. Remember to like, comment, and subscribe, and we'll see you back next week.
Mark is interested in the philosophy of mind and artificial intelligence.
Sorry, that's my Siri. It seems to be very interested in getting involved in this conversation.
Mark is interested. Now it's playing music. Because AI is stupid. That's why AI is really,
really stupid. And actually, that's a great segue for our conversation today, because Mark, our guest
today, also thinks that AI is really, really stupid. But anyway, Mark is interested in the
philosophy of mind and artificial intelligence and rails against what he calls computationalism.
We'll get to that in a sec. Machine consciousness and panpsychism. In 2010, Mark was elected to the
chair of artificial intelligence and simulation of behavior, which is the world's oldest AI society.
He's been invited to advise on policy at the UN, the EC, and also the UK government. He's published
three academic books, 200 articles and won Â£3 million worth of research funding. He serves as
the associate editor of nine international journals and his research has spanned the practice and
theory of artificial intelligence. He's regularly asked to comment on AGI, particularly in response
to these AI eschatologists. Of course, we were speaking about this a few weeks ago. So folks
like Hawkins and Musk and Kurzweil, who warn of an existential threat of an intelligence explosion.
Now, in one of Mark's recent papers, he concluded that cognitivism, which is the whole idea of
viewing the brain as a computer and its concomitant computational theory of mind, is inappropriate.
And instead, we should emphasize the role of foundational processes such as autonomy, exploration,
autopiosis. And that's a strange word as well, isn't it? So that means a system capable of reproducing
and maintaining itself by creating its own parts and eventually further components and social embeddedness
in giving rise to a genuine understanding of our lived world. So in summary, Mark thinks that
computational theories of mind cannot explain human cognition. He thinks that the claims of its
research is that, you know, genuine conscious mental states can emerge purely in virtue of
carrying out a specific series of computations. He thinks that those claims are egregious.
Now, I discovered Mark about a few months ago because he's published this paper called Artificial
Intelligence is stupid and causal reasoning won't fix it. It's actually a really cool paper because
it's a tour de force of all of the computational and philosophical issues surrounding AI at the
moment. And he kind of kicks off in the paper by saying that AI is a brand tag, it's becoming
ubiquitous. But a corollary of this is that there's widespread commercial deployments where AI gets
things wrong, whether it's autonomous vehicle crashes or chatbots being racist or automated
scoring, you know, processes discriminating on gender. And of course, we have a whole load
of people saying that we can improve it. So Judea Pearl and Gary Marcus, they say that deep
learning is just curve fitting. It's just reasoning by association. And, you know, if only we could
build computer systems that took things a step further and thought about time and space and
causality. But Mark takes the AI skepticism to a whole new level because he thinks that machines
cannot and will never understand anything. Professor Mark Bishop, welcome to the show. In
your paper, you talk about Cric and you talk about church and ensuring giving rise to computational
ism. What did those folks say? Well, in my paper, I start off with the idea that it's become known
as Francis Cric's astonishing hypothesis that you and everything that we are is defined by
the set of particular sets of neural firing patterns at any one instance. And if we run
with that idea to its logical conclusion, it would seem to be that if we have an appropriately
high fidelity simulation of just the brain, we abstract from the brain with all the dirty chemicals,
the neurotransmitters, the serotonins and the like, we abstract from all that and just look at
the neural firings, we've got everything, everything else drops out for free. And a lot of people
surprisingly buy into this, to this idea. And in fact, it's one of the, one of the hypotheses
that pushed the human brain projects and one of the biggest European Union funding grants of all
time a few years ago, it was over a billion, if I remember rightly, by only my cram. And that
courted a lot of controversy with some people saying when the EU was putting a lot of its eggs
in one basket. And a lot of people had doubts as to how much real science that program would
deliver. And I think Ham died a very interesting presentation to the group, the human brain project
group, a number of years ago, because I was arguing, as Tim outlined in the introduction, that
we were not likely to get conscious states. In fact, I think there are good a priori arguments
for believing we won't get conscious states are very computational simulation, no matter what
that simulation is, no matter how fast it is, or no matter what algorithm it is, unless we have
to bite the bullet and we accept a very vicious form of panpsychism, the idea that conscious
phenomenal states are living in everything, the very cup of tea of coffee that I'm drinking at
the moment as its own mental states. And the similar likes to pin my colors to the scientific
mask, I find it somewhat implausible to believe that my coffee is conscious of me drinking it.
And so we're led to reject that horn. And if we reject the horn of panpsychism, then unfortunately,
we're led to, in my opinion, we're led to reject the idea that the mere execution of a computer
program can bring forth conscious states. So that's kind of in a nutshell, the executive summary,
if you like, of an argument I described as dancing with pixies.
That purports to show that unless we're willing to accept panpsychism, computers will never have
phenomenal states of consciousness. Now that is a distinct argument from the argument that people
like Sirle makes that says computers can't understand. And it's also distinct from people like
Penrose who say there are certain things that people can do. And Penrose very famously talks about
mathematical insight that are fundamentally non computable. So it's my own minor contribution
to the debate. And that is I don't believe that computers can be conscious and less willing to
accept a very nasty form of panpsychism. So I think some of that impulse, right, to
break things down, you know, and to find the absolute minimum component that can implement
everything in humor. I mean, that's a very Western kind of analytic, you know, scientific approach
to start with. And so if we don't want to throw out analytics as a whole, like in what gap or
let's say, what do we need to add to kind of artificial neurons, if you will, in order to
or even to the computational paradigm itself? So what do we need to add to say the Turing
machine or Lambda calculus, you know, concept in order to be able to create consciousness? What's
missing? It's usually complicated stories, as you can imagine. And I'm hoping that at some point
in time, we'll get to go into engage in a little bit more depth on what the dances with pixies
reducture actually says, because otherwise it just sounds like an airy, hands waving
philosophical statement that I'm making. And it might be quite interesting to go into the nuts
and bolts of why I believe that argument works. But to come back to Keith's point,
by many people, I think, working in the field as a young teenager, back in the 70s,
I taught myself to program and had an unhealthy interest in science fiction. And you put those
two things together, and you're led to the belief that I have very strongly as a teenager that we
would build thinking conscious machines, and that one day they would they would come to tyrannize
mankind and then slavers and go on to be the next stage of evolution, if you like. I'm not alone
in that. I'm sure many people have had similar entertains, similar fantasies. And it wasn't
until I went to university that my choice of degree at university was informed by these
interests of mine, peculiar interests of a teenage male. And I went to read cybernetics at
the University of Reading. It was the only place in the UK where you could read cybernetics at the
time. And that was a great education, not least as some of the people my tutor, for example,
Alex Andrews was one of the was an early first round neural net pioneer from the sort of 40s
and 50s and famously gave a couple of great papers at the mechanization of thought conference of
people like Minsky and Rosenblatt were all out. So I was surrounded by old school academics who
were from that first wave of new people working in neural networks. And that was a really interesting
place to work. One of the guys that taught us a subnet is kind of an engineering and it touches
on all sorts of things, but it works sort in the UK is very much an engineering discipline.
And so one of the things we had to do from year one was build computers, but not perhaps as you
might think of building computer, are you going to get on board and putting a few chips in perhaps
but literally starting out with TTL and building your own half out of building your own address
decoders and building literally computer from individual TTL chips. And once you've done that,
you get a very low level but real engineering perspective of what's going on in a computer.
You don't tend to think anything too fancy. The idea that these things are thinking machines
start to become a bit bizarre. Because you realize the machine has no choice in what it does. If I
imagine like a balance a ruler on a pivot that's balanced and press the ruler down one side,
the other side comes up, it can't do anything but that. And when you see that the operation of
the logic gates in the computer work in exactly the same way, that seems to be a very different
mode of operation to that which we're used to entertaining when we think about human cognition.
That said, all those thoughts were further down the intellectual line for me when I was
engaging as an undergraduate. I'm just interested in building these things. And I guess it wasn't
till my third year, I was doing a joint degree in some editing a few of the science. And then
one of my lecturers seemed to be the science was a guy I went on to be professor of theoretical
computing at Oxford, a girl called Richard Byrd. And he introduced me to the notion of
Turing non-computability. And also introduced me to Gerd Lecher Bach, but that's another
terrible, very interesting book, which I'm sure you're all very familiar with. And that was quite
an intellectual shock because prior to doing that course, I'd had this sort of, I was very
modest in man, and I had this idea that you give me a problem at enough time and I'll boss you
out of the computer program, not solve it. And the idea that there could be problems that were
fundamentally non-computable was a shock. And it took a while for that shock to actually see
pain. I was a bit skeptical, even though I knew what to write to get the reasonable mark in the
exam. I can't say my heart was completely wedded to the notion of non-computability. Not least,
because the proofs of these are kind of weird as well when you get into them. There's lots of
self-reference and things, and they seem a bit sort of bizarre. They did to me as a young undergraduate.
But then Roger Penrose, and I came back to read from a PhD at Reading, I was lucky to meet Roger
Penrose. Just around the time he was publishing The Emperor's New Mind, or just before that time.
And we got chatting, and I realized that my intuitions about the horror that non-computability
might pose were being echoed by someone who was even, at that time, known as being a bit of a
polymath, the guy that taught Stephen Hawkins, amongst other things, and worked with him.
And the fact that this guy was also echoing some serious reservations
that were based on good leaner ideas, and based on showing non-computability,
stuck a chord with me. But the real big intellectual shock to my, again, I started out
in a PhD in neural networks with the intention of building a thinking, living, proving,
conscious machine. And then over that period of doing that PhD, my position changed 180 degrees.
So after meeting Penrose, the next big thing was I went to a conference at Oxford in 19,
or when would it be? It was around the time that parallel distributed processing first came out,
and Ruma Hart McClendon came over for the first time in the UK to describe backprop. So this is
how long ago it is. I'm really quite old-skilled in all these things. So yeah, it was the first
presentation about profit at Oxford. It's a massive conference. There was, you know,
it was a set-out conference for the main room, which probably held about 7,800, was sold out.
And they had two overspilled theaters. We were in the second of these overspilled theaters with
video links to the main stage. And we heard these presentations about profit, which is all very
exciting. But the thing that blew my mind, having been brought up in an engineering discipline,
cybernetics, was listening to two philosophers, because that was quite odd. And there are two
philosophers with dinner and soul. And I've never heard a presentation like it, because unlike the
sort of kind of measured, dry presentations of how to control the server mechanism or
a new algorithm for quicksort or whatever the hell it has to be in our computer science and
engineering presentations, I soon learned that philosophers argue in a much more
fisticuff kind of way. And it was a shock, but also very engaging. And so I came across,
you know, Searle describing his Chinese room argument, which resonated with me at the time,
and hearing Dennis poo poo this at the time in his own unique way. And so around that time,
my thesis, I began to entertain, began to question where I was going, am I, and am I team? And are
people other people in the world who are doing exciting things when you all know, are we going
to build these thinking machines? And actually over that time, and in the few years that followed,
I reversed that opinion. And the core intuition that drove all that I guess really was Searle's
Chinese room argument. No, I think you'll find a lot of sympathy. A lot of people that actually
work, you know, very close to the AI, or, you know, what's referred to as AI, when we're trying to
sell things, you know, we'll raise an eyebrow and skepticism, because, you know, we know GPT is just
a big pile of linear algebra. You know, the, the talk about the transformational effects really
come from the business side. You're among good company here. But to kind of go back, you said
you wanted to touch on this dancing with pixies argument that you raised earlier. And this kind
of links into the notions of panpsychism, and how this relates to, you know, if you, if you want
to take the idea that a computer can be intelligent, that it can think that it can understand things,
then you end up concluding that anything can kind of have this. You run us through that,
like very briefly, people haven't read the paper, pants like his argument, the dancing with pixies
fallacy, and kind of like how this all kind of flows into the conclusion that computers cannot
be intelligent. Yeah, one of the actions in which this argument is built is the idea that
computation is not an objective fact of the world. It's observer relative. So I first of all want
to give you a couple of examples that I think will underscore that axiom one, because a lot of
people reject that's a nonsense computation. What a computer does is a fact of the matter.
So again, I'll go back to my undergraduate days when we had to build
before we use TTL, we literally have to build these logical gates out of transistors.
So it doesn't get much more basic than that. So imagine you built some, a set of transistors to
perform the following electronic logic, you have two inputs to the circuit and one output,
call the inputs A and B and the output A. If both inputs A and B are zero volts, the output is zero
volts. If A is five volts and B is zero, the output is zero. If A is zero volts and B is five,
the output is zero. If A and B are five volts, the output is five volts. What logical function,
guys, is that performing?
And?
You might be right. It's performing and if we assume that no volts is false and five volts is true.
Now, if I tell you that you are actually wrong in your assumption that no volts was true
and five volts with false, what logical function is that performing?
Oh, who remembers their bullion? There we go. Oh, that's right. It's performing. You're not.
So in other words, the computational function, this bit of electronics is doing,
is contingent on the observer relative mapping between the electronics and the world, right?
If I use a zero volts, false, five volts true mapping, it doesn't end. If I use the inverse of
that, it's doing an or. You cannot tell a Martian from Planet Mon couldn't look at that and say,
that's an AND gate. That's an OR gate without knowing that mapping. And that mapping is subjective.
I might have one mapping. Alex might have another. Keith might have another one.
In fact, a great Israeli computer scientist, Oran Shigiri, extends this argument pathologically
and looks at multilevel logics. And the problem gets really weird if you go down that route.
But I'll just stick to the simple case with the AND and the OR function.
So that's one of the reasons why I said fundamentally, I mean, it seems to me just
axiomatic. I just baffled when people tell me this is not the case. But I still occasionally
meet people who dispute this. There's a second follow through argument. And it's built on work
of Winograd and Flores in their book, Understanding Computers in Cognition,
when they start to think about what is a word processor. And I've reframed the argument a
little and I think about what is a chess program. I don't know that any of you guys are old enough
to remember in the 70s, we used to have these little chess, plastic chess computers that
were square. They had little holes on a bit of board and little tiny little plastic chess pieces.
When you made your move, you lifted piece up and plonked it where you wanted to go,
and then the computer would light up the piece you had to move and where that was to go.
Using one of these gadgets, I could quite happily play. I'm not very good chess player,
so I could happily get thrashed by these machines day in, day out and enjoy that thrashing, so to
say, to speak. And I could use that piece of computational equipment to play chess with.
Now, in the UK, there's a famous conceptual artist by the name of Tracy Emin, who does a lot of
work with neons. I don't know whether you guys have come across at work at all. And also in the
60s, there was a big movement in what's called kinetic art, where you're in cybernetic art,
where people interacted with art pieces. So now, what can be noticed to me, Tracy's sabotaged my
chess computer. She's ripped the innards out, and she's now wired all the inputs to pressure
pads in an art exhibition, in an art gallery, and all the outputs to neon strips. So when people
walk over these pressure pads, different neon lights come on and off. Now, there was no sense
that you can possibly, it seems to me that you could possibly say that when I walk around the
art gallery, I'm playing chess, I'm interacting with a bizarre piece of abstract art, certainly
not playing chess. So it doesn't seem to me there's anything intrinsically chess-like in this device.
Yes, it was engineered very carefully, so if I knew what I was doing, I could play chess with it.
I could use it in other ways as well. And the problem gets even worse if you've come across
isomorphic games. That's, in my opinion, probably all you know is Noughts and Crosses. Now, imagine
you've got a Noughts and Crosses game on your iPhone, and you've got it like I have a six-year-old
daughter who's just about got her head around Noughts and Crosses. I can keep her occupied for,
I was going to say half an hour, that'd be a exaggeration for five minutes, say,
giving her this thing, and she'll play Noughts and Crosses happily again, and she says,
oh, daddy, I'm bored. What am I going to do? I say, ah, well, I've got another game I can show you.
But she says, daddy, you've only got one game on a computer, so I'm going to play it against the
computer. Do not worry about that. We've got the Noughts and Crosses. I've got another game. I'm
going to call it Computer Wist. Now, imagine you lay the deck of cards out, ace through to nine,
and we take it in turns to pick cards from this deck. The winner is the first person who can get
15, get cards to seven to 15. It transpires that if you've got a program that can play Noughts and
Crosses with a suitable mapping, you can get it to play a perfect game of Computer Wist. So,
given the grid, just like a magic square, where all the verticals, horizontals, and diagonals add
up to 15, you then plot your computer go, marking a one square, and choose your go,
you can tell you which card to pick next, and you can play a perfect game of Computer Wist.
So I can use that same computer program with my mapping to play a perfect game of Computer Wist.
So you cannot say in advance, without knowing what I'm going to do with that program, whether I'm
going to play tic-tac-tac or Computer Wist. So I think those three arguments together make, to me,
a persuasive case to paraphrase Wittgenstein, that the meaning of a computation is in its use by human
computer users. The phrase that, as you want to, you're all aware from Wittgenstein, I'm paraphrasing
it's one of the investigations where it makes the claim that the meaning of a word is its use in
by human players with human language games. And I think the same applies to computation. The meaning
of a computation lies in its use that we as human users of computers put that too. So that's kind of
setting the stage. So I think there's always going to be this mapping at the physical level,
and then there's always the idea of what we're going to use a computation to do,
and that's a very social human activity. So that's setting the stage where I want to go with it,
answering the pixies. Now, the next move I make, I know you'll have all have read
Competing Machinery and Intelligence, Turing's famous 1950 paper. Everyone's at least looked
through this, and, well, Turing first outlined the Turing test, what became known as the Turing
test. Also in that paper, he outlines the operation of a very simple machine, Turing's discrete state
machine, as it became known, and this is a beautifully simple machine. It's a disk-like device,
and it can go around in 120-degree intervals, and it can stop at the 12 o'clock, the 8 p.m.,
and the 4 p.m. position as it moves around a clock, and it can exist in each one of those
discrete positions. And we can describe the operation of that machine as a finite state
automaton. If the machine is in state A, and the next clock tick is going to go to B, if it's at B,
the next clock tick is going to go to C, and if it's at C, it will go back to A again.
Now, if we want to, we can arrange that when the machine is in computational state A, it will
do something. When it's in computational state B, it will do something. Turing envisages it when
it's in computational state A, and light would come on. You also imagine there being simple input
to the machine, like a big lever brake mechanism that you could have on or off. So if the machine
was in state A and the brake was on, it would remain in state A. If the brake's off, it would
go to state B. One of the first interesting things, again, like computation, when you're
given a Turing discrete state machine, to read off the computational state, A, B, C, we need a
mapping between the physical position of the lever machine and the computational state to
which it refers. So we may define computational state A to be the 12 o'clock position, in which
case when the levers there were in A, or maybe it's at B, and the rest thing follows through,
but we always need that mapping. We've always got to do these mappings between the physics of
what's going on and the computational state that we're instantiating. So now we've got this machine
without the brake, it just goes to A, B, C, A, B, C, A, B, C, A, B, C. And that's
interesting enough. If you're interested in inputless finite state automaton, they're not
very exciting machines, all they can do is go through a cyclic series of states forever and
unbranching series of state transitions. What is interesting is that the appendix to Hilary
Putnam's representation of reality is a little known proof. This shows how we can, effectively,
how we can map the operation of any inputless finite state automaton onto a large digital counter.
Putnam goes further and shows how we can map it onto any open physical system. An open physical
system being a physical system that's open to gravitational waves and all the rest of it,
electromagnetic spectrum impinging onto it. But for simplicity, let's just consider the,
without lots of generality, let's just imagine we can map the operation of any inputless FSA onto a
bloody large digital counter. How does Putnam do that? Well, let's take Turing's machine.
It just says, if the computation is in state A, I'm going to map that to the digital counter state
zero, zero, zero. If it's in state B, I'll map that to counter state one. If it's in state,
computational state C, I'll map that to counter state two. And then the A again will go to three,
the B again will go to four, and the C again will go to five. And then we get over any finite time
period, we can replicate the state transitions of our digital discrete state machine by the numbers
we're psyching through on our digital counter. And again, you might answer, so what? That doesn't
seem particularly threatening result for computationalism at first sight, because real
computations, much more complex devices than inputless finite state automata.
Well, in a paper called, Does a Rock Interment Every Inputless Finite State Automata, David
Chalmers responds to this argument in an interesting way. And he says that, yeah, I'll
concede, if you like, that we can implement really trivial machines like inputless finite
state automata using Putnam's mapping. But when we want to look at machines with input,
this breaks down because we get a combinatorial explosion of states that we need. And
Chalmers introduces a very neat construction called the combinatorial state automata,
which we can implement using Putnam's mapping, but an exponential increase the number of states
that we need. And the combinatorial state automata is sensitive to initial conditions.
And so could be genuinely said, if we could implement it, we have an infinite state to
implement it, could generally be said to be implementing a computation with input. But at
the cost of every time step of the computational, you need an exponent, your number of states
grows exponentially. And Chalmers makes the point that after a very short number of states,
we'll run out of the number of states needed is bigger than the number of atoms in the known
universe. And hence Putnam's mapping must fail. And that's kind of why I entered the debate,
because I made an incredibly trivial, all the hard work had been done long before I came
to play with this game, so to speak. But my only trivial modification to Putnam's argument that
to me makes it robust, the charm, as it says, is to say this, well, if we look at any real machine
of which it's claimed has genuine mental states, conscious states, as it interacts with the world.
And this, this intuition was brought real for me. Because again, some people dispute the fact
that there are people who believe that there are serious scientists who believe in the machine
consciousness program. There definitely are. And I used to, my head of department is at
Reading Cybernetics with one of those people, a guy called Kevin Warwick. And we at Reading
and built these little simple robots that moved around a corral controlled by a neural network.
And Kevin said, well, these got roughly the same number of neurons as a slug. And it's pure
human bias, if you say a slug has conscious experience, and these robots didn't. And I
thought that was a ludicrous claim. And that inspired me to move and develop this dancing
with Pixie's Reductio. So to come back to the case, I said, right, then, Kevin, if you say
your robot as it moves around the corral over a finite time window, T1 to TK, experience is
something that it is like to be a robot bundling around a corral, not bumping into things. I don't
know what that is, but that's just imagine it has some conscious experience. What I, what I can do
is log all the inputs to that machine. And then I'll play them back to them. So I now lifted
the robot out the corral, I've disconnected all its sensors, if you like, and actuators,
and I'm just injecting into the robot the states, it would have got where it was in
around the corral on itself. Test again, does the machine still have conscious state? Well,
yeah, of course it has. It's reading the numbers from a latch. The data was originally taken from
an ATD converter for argument sake. We're now plonking that data in there from a data rejection
system. But the computer still has the phenomenal states. So Kevin Warwick asserted. And that,
unfortunately, then did the problem that he was going to encounter, because if that was the case,
all that were really interesting, we can take, we can collapse the exponentially growing number
of states that Chalmers showed we would have. If you actually want to implement fully all aspects
of a computation using Puttner's mapping, if we just try to look at the particular computational
trace, we just need the inputs to that machine that pertain to any over time as the machine,
it is a little thing. And then we can remove all the counterfactual states. And once we've
done that, we've got a linear series of state transitions that we can reliably map
using Puttner's mapping. And hence, if it's the case that Kevin Warwick's little robot was conscious,
then so must our account to be conscious. And then after Puttner, any open physical system.
So that in a nutshell is the DWP reductio. It's interesting that in all these seminal debates
to me about AI, about penrose, about cell and about my own small contribution,
there's a lot of confusion, people can very easily misinterpret what's being said. A lot of people
got hung up about does a rock genuinely implement a computation? And I think to me, Chalmers
completely proved that it does not. No problem with that. Can we make a rock with a suitable
mapping implement an arbitrary series of state transitions? Yes, I think we can. And I think
we can make any count to do that. And because we always use a mapping, whatever system we use,
I don't think I'm doing anything. There's no sleight of hand involved here. But all computational
systems involve an observer relative mapping somewhere along the line to get them to work,
whether it's only assigning a logical two to five volts and a logical false to not volts.
So I don't think the use of a mapping is something that, you know, and there's no sleight of hand
involved in that. Given that I confused an arbitrary complex series of state transitions.
So then the question is, if you're a physicalist and I approach this problem originally as someone
who, you know, I like to think of myself as if I'm not a mysterious, I don't want to appeal to
some supernatural forces to bring forth my consciousness. And at one point in time, it was
the case that well, if you don't believe in functionalism or computationalism, then you've
got to believe in supernatural effects. Well, that is no longer the case. Cognitive science has
moved on a lot since the 1960s. There's an awful lot of new tools in town. And these are really
exciting tools in my view. And you highlighted a few in the introduction, Tim, but things like
the embodied, inactive, embedded and ecological approaches can go a long way to answering or
giving his insights into these questions without having to bring forth particularly
supernatural notions. So I'm going to put that to one side. We don't no longer face the choice of
either accept computationalism or accept mysterious. Putnam's rock. I mean, there's a lot of interesting
responses to it. But I want to point out a couple of things or maybe just ask you about a couple
of things. So one thing is that first, I think what your goal is and correct me if I'm wrong
with the pixie dancing with pixies argument is to say simply that if we accept, say,
Turing complete computation, or even in this case, finite state machines, but I think we can probably
go one step further, if we accept that effectively computable systems can implement consciousness,
then we also have to accept panpsychism, correct? That's that's what I tried to show,
because once you have a system that you that you play, like my boss Kevin said, that machine is
conscious. I can look at what happens as that machine interacts with the world. I log all the
inputs to it, I can trace the flow of the execution flow of that of the machine code that
control the robot. And then I can implement an arbitrary series of state transitions that would
do exactly that with an appropriate mapping with a digital mere digital counter. So if that machine
is conscious, then my digital counter would push this mapping must be conscious. Right. So that's
the position I arrived at. Now, when chatting to David Chalmers about this, he said, Oh, no, no, no,
you've gone off the road, because we need the full potential of the computation to be there
for functionalism to hold. Now, this is quite a mysterious view. In fact, it was so mysterious
that when he first said it, he had to repeat it about three times, because I'm not the quickest
to uptake. And I found it so bloody bizarre what he was saying. But when I did unpick what he was
saying, for Chalmers, you actually need, once you effectively slice off the potential counterfactual
actions by saying, Well, I know the input at this point in time, I know the input at that,
I'm going to replace the counterfactuals in my program by direct go to statements, if you like,
or just just omit snip them from the from the program. To me, that couldn't possibly affect
the phenomenal state of the system, because really, what he's saying that non entered branches of a
computer program, right, have a causal effect on the phenomenal state of your machine. But the bizarre
thing is, that's what David's saying. That's it. You've got to have the potential for counterfactuals
there. Otherwise, we don't have the machine genuinely instantiating phenomenal states.
So if we just if we just assume in argument, though, that people don't or that we don't accept
panpsychism, okay, and that these arguments prove that if we accept computationalism, it implies,
you know, panpsychism, okay, we also have mathematical results that say,
let's say, Turing computation or effective computation encompasses all computation,
like there is no, you know, there is no other kind of computation, unless there's hyper computation,
right. And so I think what we're saying, I believe and correct me if I'm wrong, but
I believe what we're saying is that there exists hyper computation, and that human minds are
performing hyper computation. So just just take this back to the rock for a second. One issue
with that mapping, right, is that rocks actually have physical states that may be real numbers,
you know, they may have values that in and of themselves are not computable. You know, they
can have positions and states and quantum states that they have values that are essentially defined
by an infinite precision real number and therefore are not even accessible to computability to start
with, like even describably, right. So are we saying because I'm always looking for where
consciousness is hiding, if you will, like, are we saying that it's hiding in sort of real valued
states, you know, maybe quantum states like Penrose would say, perhaps a microtubules or
something like that, you know, is that is that a form of hyper computation? And is that where
our consciousness derives from? Where do we draw the dividing line? Because that's something that's
like I've read quite a few of your papers in preparation as you do when you're going to speak
with someone. And this dividing line where we go from, you know, computational and essentially
impossibly into like impossible to achieve intelligence or understanding or any measure
thereof, to the point where we have an intelligent system that can understand its world and sort of
redefine itself and redefine its world. That distinction is not terribly clear. And as we
dive into this question, I want to drive towards where this distinction lies, if this distinction
exists. Well, to pick up on Keith's point, first, I think I'm neutral. I mean, Penrose has given
the positive thesis as well as a negative one. So famously in the emperor's new mind, he gives
he gives his first version of a goodly the in argument that reports to show that mathematical
insights non computable. And then says, Well, this suggests to me that non computability lies at the
heart of what it is to be human. And then with Stuart Hamroff, they outline a positive thesis
which which reports to show that non computability can arise in the brain through the orchestrated
collapse quantum collapse in the microtubule skeleton of a brain neurons. I'm perhaps neutral
on this. I know that when Penrose held the psych symposium on his work in 1995, and it attracted
a lot of responses well over 20. And I don't think any of his logical work was seriously
brought into question. So the that is is interpreted, even though that was actually
a naive interpretation of girdle compared to the work that he put out in shadows of the
man is a much more nuanced approach to the argument in my opinion. But nonetheless,
it wasn't seriously criticised nearly everybody criticised his positive thesis.
So I get Penrose is a clever guy. It seems interesting. I'm not going to hang Michael
is onto that flag particular if it works great. I've been more drawn to modern approaches to
cognitive science, which look at the end. And there isn't unfortunately a very quick
six page paper that can can lead people gently into this. But there are four schools that all
this work really started out with the work of a roboticist from MIT called Ronnie Brooks,
who wrote a classical paper, which you guess you guys are familiar with called intelligence
without representation, which basically looked at you instead of trying to when I did robotics,
old fashioned old school representational robotics as a young past grad. A lot of our work was
trying to build take data from sensors and build rich internal models of an out there world.
And I remember we spent all our budget on buying the biggest fattest computers we could possibly
afford at the time and strapping them onto these poor little autonomous vehicles, and they were
laden down with computer power, and they moved absolutely tragically slowly. We're going back
in the early 90s now, but they were they were pathetically slow things, you know, really
embarrassingly bad. Because a lot of their work was trying to build up these models,
looking at what's on a bill models of them. Now, I know that these days we can do that kind of
thing bloody quickly, but back in the day, you couldn't and Brooks thought, well, do we need
to do it? And in that paper, he argued that we didn't why build the representation, we can use
the world as its own representation. And in the sense that sort of paved the way for thinkers like
Francisco Varela, then in a book called The Unbodied Mind, with Evan Thompson and Eleanor
Roche, to start thinking about different ways of doing cognitive science. And The Unbodied Mind
is a mind blowing book. It's quite a, it throws your whole view in the same, you know,
in an analogous way to go to Leche-Bart can be quite mind blowing as a young kid. This is mind
blowing as well. But in a kind of a weird way, because it actually questions the existence of
a fixed pre given out there world. And that was quite a shock to me when I first came across
these ideas, not being a trained continental philosopher. My first mode of engagement was
with Varela, who incidentally started out as a theoretical biologist, and then
someone very active in the a life community. So I think his initial work has also been from
a science perspective, but he engaged quite deeply with with the European philosophy, which
at that point in my life, I was sort of ignorant of. And so this led to a development of alternative
schools of what cognition is all about. And the inactive school is one that I'm interested in.
And it says that, you know, effectively, we can look at one sort of
in itself is these days split into numerous sub approaches. One of these from developed by a
guy called Kevin O'Regan and Alvin Airee argue that visual consciousness is something that we
do. So they're moving away from the idea that vision is like interpreting like your eye getting a
scene from the world on your brain having some little like cinema, which you're then interpreting
what all these little bits do, they make the case that vision is more akin to an activity is what
we do. It's it's guided sensory motion exploration of the world. Varela itself was particularly
interested in a broad being in ideas of autonomy, how can how can how can things become meaningful?
There's all these very complicated debates that I think to try and come back to your question,
Keith, and your question, Alex, you need to touch on, but it's really challenging to
touch on them in an intelligible way in a relatively short period of time.
You would at least agree, though, that, you know, your contribution, penrose, etc. points very
strongly that there's something embodied, there's something physical that we haven't quite figured
out yet. Maybe it's microtubules, maybe it's something else, you're agnostic to that. But
there's something physical that allows my intuition is to do with autonomy. And this is why we bring
that idea of ultra poesis in that Tim mentioned at the beginning, ultra poesis is again, in the 70s,
Umberto Acherana, the Chilean side musicians Umberto Acherana and Francisco Varela came up with
a theoretical device for delineating life from non life. Because astonishingly, this has been a
really difficult problem, you'd think it was probably solved 100 years ago, it hasn't been.
Even as recently as when Margaret Bowden was writing on this, one school tried to say life is
and then give enumerated a list of properties that have to metabolize, it has to reproduce,
Varela Acherana looked at it from a different perspective. Well, what fundamentally life is
a system that has a circular organization, it's got to be able to maintain its own boundary
of itself and the other. And it has to encapsulate the rules, the automatic rules that maintain
that boundary in the face of a changing environment. We spoke to Friston, Carl Friston,
quite recently, he was talking about Markov blankets, which is quite interesting about
how do you define the boundaries of a physical system? And you know, does a hurricane have
a Markov boundary? And what we're talking about here in a very general sense is defining boundaries
between what lives and what doesn't live and what is meaning and what isn't meaning and what is
understanding and what isn't understanding. And it's very philosophical, it's quite difficult to
pin this down. If you look at a Maturana and Varela's book, it's about 60 pages, their original
treatment, auto-precision cognition from the 70s. And it is very dense. It's not a waffly
philosophical book, it's quite hardcore, quite mathematical. Yeah, I think they do do an interesting
job at pinning down what, if you like, what it might be for something to be alive. And this
has been, this challenge which was explored in the embodied mind has since been developed by Evan
Thompson, who's an American, an interesting philosopher who wrote a book called Mind in Life,
where the argument is laid out that life is a continuum and wherever you have this
continuum of life, then you have a proto-mentality. And I'm kind of drawn to that. I think it's a
very persuasive argument. And then we have to look at the question of what constitutes autonomous
systems and why should it matter if an autonomous system has a phenomenal sense of what it is like
to be. Here, if you like, you can link into the work of a guy I've just recently come across who
wrote to me a few weeks ago, he used to run a big lab in France, AI lab in France, Mikhail Troublay,
I think his name is. And he argues that we need phenomenal consciousness to
arbitrate between different actions if you're sending a robot to a Mars that's got to be
completely autonomous and it's got to react appropriately in different, in unknown environments
all sorts of different threats. Effectively, the robot's got to have to know something that
it's a state that it's good to be that makes the robot feel pleasant and a state that's horrible,
that's dangerous, that might cause death to the robot. It has to, we use the phenomenal
sense of what that feels like, we can then use that to arbitrate between different actions.
And this has actually, by the way, is an idea that was brought, I first came across through a
paper by Daniel Dennett called Cognitive Wheels the Frame Problem of AI, when he looks at what
must be known to us to arbitrate on what you call the cookie problem. Imagine you've got a big jar
of cookies and some little kids like my six-year-old daughter and two families, one next to each other
and in one family, when the child goes for a cookie, the family beats it, smacks it relentlessly
until it's in tears and never goes, it doesn't have any more cookies after that. And the other one,
they're very sort of touchy feeling, oh no, please don't have another cookie tarquin and
occasionally tarquin does go and have another cookie. Dennett asks the question, why is it
that beating your kid causes that child not to go for the cookie jar anymore?
And we know that well because being beaten is something deeply unpleasant and you don't
particularly, unless you're a masochist, want to do things that are going to bring this
feeling of pain about you. But then Dennett said, well, how do we know that? We can arbitrarily
hard wire six facts in, so I could hard wire into my computer program. If something else biffs me,
then I'm going to increase my pain by one and if pain gets over a certain threshold, I will do
that action again. That's incredibly brittle, it's literally arbitrary. But unless we have
phenomenality, unless we have access to phenomenal states and know that getting biffed hurts or
going over rough terrain if you're a marginal robot, it takes you around a bit, you have to just
sidestep that by hard wiring effectively, hard coding as engineers and the system is no longer
autonomous now, we're having to define what it has to do for all these different possible states.
That's the price we have to pay. So I think you can argue that evolution is as blessed as with
phenomenal consciousness, so that we can act autonomously. And that's the way that, I guess,
after Evan Thompson and Varela and Tourboulay, the view that I come to. So we need consciousness
to enable us to succeed evolutionarily. Well, haven't we just fallen foul of the kind of prime
axiom of software engineering at this point that every problem is just a level of abstraction away?
Because, I mean, we could, I forget which paper it was, you had a really great
conclusion in one of your papers where you essentially said, if we built a bunch of robots
that looked and behaved just like us, they're autonomous, they laugh at our jokes, they respond,
but the thing that defines the difference here or the difference between us and them would be
that when we're laughing and feeling we feel it, it's phenomenological, when they do it,
it's not, it's a simulation. But, you know, leading into this argument, it's like, well,
to have these Martian robots that have autonomy and can succeed, they need to have this phenomenological
state. Isn't this really just a software engineering problem away from being solved?
I think when I wrote that paper, I was, I mean, it's only very recently, literally, and I want to,
I'm hoping to work at least to reach out to Michael to see if we can do something together.
Quite excited by these essays he's sent me, I think they make a, I don't know why,
just haven't occurred to me that this could be a reason why consciousness has evolved.
He makes a very persuasive case. I'd love to claim it as my idea, but it absolutely isn't,
but I think it's a very beautiful one. I need to understand more and either he will push it or
perhaps we might do something together, I don't know. But, yeah, so I'm a little bit skeptical
that without that, the glue of consciousness, that we could get a machine to act as a similar
chroma view in all possible cases. I was arguing from the stronger, I guess, in that paper that,
well, that's just assuming that we can. I think engineering wise, I'm a little bit skeptical
now, following Michael's work, that that is going to be possible. But also to come back to another
point that relates to the, are we going to talk about the Chinese room at all, or are you assuming
that's boring for all your readers? No, no, I think that's one of the most important things,
because we're talking now about consciousness and the various different boundaries between
what is and what is not consciousness. But I think the other one that's really important is
understanding and the boundaries between what is and what isn't understanding. You say in one of
your papers, what does it mean for a central processing unit to understand? Does it understand
the program and its variables in a manner of analogous to cells understanding of this rulebook?
But this cells rulebook thing, it describes a procedure, as you say in one of your papers,
that if carried out accordingly, allows cell to participate in an exchange of uninterrupted symbols,
squiggles and squiggles, which to an outside observer, look as if cell is accurately responding in
Chinese to questions in Chinese about stories in Chinese. In other words, it appears as if cell
in following his rulebook actually understands Chinese, even though cell trenchantly continues
to insist that he does not understand a word of the language. So this has been used, I think,
very reliably. And you had a paper actually recently introducing a couple of other responses,
because there were four responses to cell's argument, right? There was the robot reply,
the systems reply, the brain simulator reply, the combination reply. And in your recent paper,
you talked about robots and animats. In the Target VBS article, there was a lot more than
four. So when he wrote the paper, came up with four possible counter arguments, which are the
classic ones that Tim just outlined. But from memory, there must have been over 20 people,
really big names in philosophy and AI who responded to that VBS Target article. People like Marvin
Minsky, McCarthy, Denner, obviously, God, I can't, I'm ashamed to say I've forgotten, but
big, big names who wrote different responses. There's a lot more than the four that cell,
but I mention these because what's been bizarre, I edited today with John Preston,
you edited on the 21st anniversary of the Chinese Reum argument, John Preston and I put together
an edited collection of responses to at 21 years on from leading AI scientists, cognitive
scientists and philosophers. And I still think that's a good collection of essays that we picked
and good collection of people to contribute to that volume. And again, and the intervening goes
between this Chinese Reum argument coming out and that volume coming out and then between
that volume coming out in 2002 and 2002 and now, really, and I've talked about this as you can imagine,
in many places, most of the UK universities and quite a few in Europe and one or two in America.
And nearly all the most formidable responses that I've come across really go back to responses
that Sirle actually predicted. And by far the most common and probably the most, I think the
strongest responses is some variance on what became known as the systems reply that you mentioned him.
Can you just quickly define what the systems reply is? Do you mind if I just go over again,
as you read up really, really quickly the essence of the argument, but I'd like just to sort of
unpack it slightly more slowly. So Sirle, imagine Sirle, to set the scene, Sirle's a monoglot English
speaker, the only shamefully like myself, shamefully because I'm married to a Greek lady,
I still can pretty well only communicate in English at best. And then Sirle can as well. So he imagines
himself locked in a room and this room has got effectively got a letterbox instead of a door
to which he could communicate with the outside world and in the room of three piles of papers.
And on these papers are strange symbols that Sirle doesn't know what they are. We know,
there's people reading about the experiment, hearing about it, that these are actually Chinese
ideographs. But to Sirle, they're just uninterpreted squiggles and squiggles. You've got no idea what
they are. So you've got these three piles of things. And on the desk, there's a big grimoire,
book that tells Sirle how to correlate symbols from the first pile with symbols on the second.
And then other rules that tell him how to correlate symbols on the first pile and with
the second pile and also linking symbols on the third pile. And other rules that tell him how
to take symbols from one of these piles and stick them to people through their throats to the outside
world. Well, unbeknownst to Sirle, the first pile defines a script, a second in Chinese, a second
pile describes a story in Chinese, and the third pile describes questions about that story in Chinese.
And the symbol Sirle was told by the book to give to people in the outside world
answers to questions to that story in Chinese. And Sirle's point is that if we concede, so he's
arguing again from the strongest, it says, okay, let's concede, but that rule book, however,
it's defined. And again, a lot of people got home because they thought Sirle was purely talking
about a naive pattern matching program. If this symbol doesn't then do that, actually Sirle makes
it clear, if you read the paper carefully, that he wants us to stand for any conceivable computer
program. This was the first reaction that I had, I had an allergic reaction to it, because
as we know from talking to Wally Tsubba, you know, that we can't write the damn compiler for
the language. It's too complicated. So it's not possible really for even us to explicitly understand
and verbalize the rules that we use in language. And you said yourself that artificial intelligence
practitioners were incredulous at the extremely kind of, you know, simplistic view of Sirle that
you could have this, you know, low level rules described. Yeah, I mean, if that's
because they didn't read the paper carefully, because Sirle makes it absolutely explicit,
that he generalizes, he gives a simple example to sort of just get you thinking about the problems.
And imagine, that's again, this is the world that, you know, some people have tried to do language
understanding in this very naive way. But Sirle wants the thought of the stand for any possible
program. All we're doing is, the rule book tells Sirle how to manipulate uninterpreted symbols
and put uninterpreted symbols out of the door. How it does that, whether it's implementing a neural
network, whether it's simply a genetic algorithm, whether it's implementing Wally Tsubba's sense-based
word effect, sense-based word effect, compositional understanding, natural language designing,
whether it's doing GPT-3 kind of operations, it's irrelevant. Sirle says, whatever your program is,
that's what's in the book. And at the end of the day, that program will tell me how to respond to
questions in Chinese with answers in Chinese. If I follow that program carefully, don't make
any mistakes, I'll give answers out the door. And if your program's any good, it will give answers
that are indistinguishable from those a natural and native speaking Chinese person would give,
even though, as Sirle trunchedly insists, I, following this program, is not
enabling me to get even the toe-hold in Chinese semantics. All I've been doing is like a mega-fast
idiots of wrong, manipulating uninterpreted symbols around and sticking some symbols that I don't
know what the hell they are through a letterbox in the net to the outside world.
Does that imply then that it's just observationally impossible to determine whether a black box is
conscious? Well, this is a, I wrote a cancer argument to Susan Schneider's
Turing Test for Machine Consciousness, where I make that exact claim. This was in frontiers,
in one of the frontiers journals a couple of years ago, because Susan says, oh, we can ask
her ideas, if we ask questions that are about particularly human activities relating to
phenomenal experience, we'll be able to tell whether this machine is, she gives a procedure
for doing this. I would say, well, if whatever set of questions there is a new have for deciding
whether your machine is conscious, I'm going to sit, unbeknownst to you, and watch you ask them.
I'm then going to go away and write a little program in basic, because I'm good at writing in
basic, that says, if question one says bladi bladi bladi, are you Susan's first question,
give this answer, which is the answer that a really complicated machine consciousness program
gave. So in fact, I have a lookup table. But of course, Susan doesn't know, because I sneakily
switch the machine. So she asks her questions, thinking she's talking to a really complicated
machine consciousness thing. She asks the questions which she claims will tell her whether this
machine is conscious, but she's actually just interrogating with a really simple lookup thing.
And at the end of the day, so the answer she wants is, yeah, that's conscious, but she's
just been talking to a lookup table. It's, yeah, I think that you're quite right, Keith. I think
isn't obvious to me, how we're going to be able to do a test for machine consciousness purely on
the basis of external observation, in the absence of anything else, because we cannot if we're
Machiavellian, we can always cheat. The thing is, when you were talking about it doesn't have
semantics, I want to one pick that a little bit as well, because you said the robot rover on Mars.
The semantics there were the state spaces of all of the sensory experiences. And then you said
it's brittle, and there's an alignment problem. And I can understand that it's very similar to the AI
alignment argument. But this is different. If you if you can replicate, let's say you're talking to
a black box, and you can't distinguish whether or not it has consciousness or whether it has
understanding. Why is there an issue of semantics in that case? Ha, you're wrong for me. I thought
you're going to say, how can I tell that anybody understands us or is conscious, which is actually
one of the core responses that's all anticipated in the Chinese room argument.
That is actually the obvious question as well. Presumably, you think that we are conscious,
because we might exist in a computer simulation, right?
Well, I don't think we can, because I know that if I slap my face, it hurts, right? And I know
that machines can't instantiate phenomenal consciousness. I made a paper called Refuting
Digital Ontology, just after an invited talk at the Royal Society workshop on the Incomputable,
hosted by Barry Cooper, who is the leader of the Turing Centenary celebrations in the UK and
Worldwide. And I made this very argument then that it isn't obvious to me, I think that it's
clearly obvious to me that we're not in a computer simulation, because I feel. And if my dancing with
pixels reduct your argument is correct, unless I'm willing to accept panthechism, then computations
can't realise sensation. So either my dancing with pixels reduct your is wrong, in which case I'm very
happy, sad, but in the end, I'll be happy when you show me where I'm going wrong.
Or if I'm right, then we're not living in a computer simulation. So I don't think we are
computer simulations. Furthermore, it's an axiom of cognitive science that other minds exist.
So it's not for me to have to explain why I believe that you three guys have phenomenal
conscious states. That's part of cognitive sciences is acknowledging that you do and
trying to come up with a theory that explains why these occur and how they occur.
The conscious state argument to kind of perhaps play devil's advocate a bit here. To me, when I
first came across the Dancing with Pixies argument, this wasn't an argument that, for me,
implied rocks had understanding or intelligence. To me, it implied that we don't, there's nothing
that has intelligence or understanding, right? What the intuition I'm wanting you to come to is
that computation doesn't have those phenomenal consciousness states. I actually, the argument,
again, interestingly, what a reviewer said this of one version, I think of the 2009 cognitive
computing paper, aren't you arguing too strongly? Doesn't this prove that nothing can have consciousness?
No, it doesn't. It just says that the operation of a digital computer, a finite state automata,
to pin it down more precisely, cannot give rise to conscious experience unless conscious
experience is in everything. That's all it says. Now, if you don't bite the bullet and say, well,
actually, I think I'm more than a finite state automaton. Thank you very much, Alex. There's
more to me. I'm an embodied entity. I don't just think in my brain, I think with my body
and my body in the world. And again, there's a beautiful result that came from a guy, and I'm
going to argue, this is a professor at Goldsmith's, and I'm going to take his work and extend it,
probably in ways that he would be uncomfortable. So this is my interpretation of work that's
published in a number of papers in Nature by Jules Davidoff on color perception by the Himba tribe
in his work in Africa. I'll describe this beautiful experiment, because I would argue
Jules is very cautious, academic, and he doesn't make wild proclamations that I'm
more comfortable looking at his work. But basically, Jules did this beautiful
experiment over a long period of time working with the Himba tribe in African color perception.
And when I first saw them, again, they blew my mind because they basically
appear to support the idea to me that language informs the not just the way that we package the
world, the way that we label it, that's kind of obvious, but the way that we see the world.
So how did Jules's work show this? Well, he took a series of color slides, like Munsell
colored slides, if you're familiar with these, well-precised blocks of color, and on one piece of
paper, for argument's sake, there were different shades of yellow, and on the one there, for
argument's sake, there were different shades of green, all equally different from one side of
the next with the same color difference, uniformly, so they went from a second dark green, if you
like, to a light green or whatever, the uniform color difference is one of the slides. And on
the sort of ochre yellowy one, if you showed that to Europeans, said which is the odd one out?
If you looked really carefully, not always, but often you'd say the one at the two o'clock
position. But it took you a lot of time, and you literally actually, in your mind's eye,
compare all the slides with each other, and if you were lucky, you said, yeah, it was that one,
but I've done this test and I mean, sometimes I don't even see it myself, it doesn't jump out,
it's not obvious, it doesn't pop out, and some of the times I make that decision
wrongly. Now, you show that to the Himba, and they sort of smile, go bang, it's the two o'clock one,
immediately, now we've called it places, it leaps out of them, and then we get the green ones. Now,
on the green ones, there's a blue style. So they show that to Europeans, what's the odd one out?
The blue one. Show that to the Himba, and the guy's got a stick, and he's rubbing his head,
and rubbing his imaginary beard, and I can't work it out. Now, when you look at that from a
Western linguistic perspective, how can the person not see the blue one is the odd one out? Well,
it turns transpired, the Himba got very restricted, or very limited colour vocabulary, compared to
Europeans, and no doubt if you took someone from the Himba and threw them into London after a period
of time, they would perceive the blue pop out, you meet just as quickly as we do. It's nothing to
do with the Himba's colour system, it's an effect brought on by language. But to me, what makes this
really powerful is that it's a pop-out effect, right? The Himba immediately get the one that's,
the colour boundary that's important to them pops out to them, because they've got a name for it,
and it's a, I can't remember what it was now, whatever name, it's an important thing, and it
pops out at them straight away. For us, the blue-green colour boundaries are important, and that pops
out to us straight away. And that, because it's a pop-out effect, that says that it's, to me, it's
actually, what it is like to see the slide, the Himba cannot possibly be seeing green and blue,
and the way that I am, otherwise the blue, presumably, would pop out of them, because they
must be seeing something bloody weird, but not, they aren't seeing those green and blue tiles
like I am, in their mind's eye, for want of a better word. And suddenly, when we look at these
sort of, oh, three yellow ones, they must be seeing that differently for me, because I can't
tell the odd one out with RB and quickly in the way that they can. Now, I think this gives us a
very strong evidence to me, or at least this is the way that I would like to use Jules's work,
and I must underscore that I don't think he would be comfortable with any of this, but this is the
way that I like to use it, to say that, you know, that the language is actually affecting how we see
the world. So when I say that our understanding of the world is not just, it's not just what's
going on in the brain, the brain is instrumental to how we see and perceive and interact with the
world, but it's not just that, it's our entire body, and also the body in the environment,
and also within a social network of language users. All these things come together to enable
us to form perceptions of what it's like to see, and what it's like to speak, to the idea that we
could just reduce this to mere neural fireings, or even worse, just some bloody manipulation of
symbols by a computer, it's just ludicrous as far as I'm concerned. I can see the logical
thought train here, so you're saying Jules said that language informs the way we see the world,
you know, one potential problem with that. That's my interpretation on his work. That's fine,
that's fine, but I suppose where I'm going with this is that it does, it's starting to get towards
this very kind of ultra relativist, constructivist view of the world, and I want to put an anchor
there. The reason why it's interesting is when you're talking about sensory states,
that it doesn't actually seem like such a bad thing, because everything you're saying there
completely makes sense. Of course, depending on where you are in the world, and your language,
and so on, you might have very, very different sensory states, and you might experience color
differently, but then there's like a topology, isn't there? So then you start getting into
understanding and semantics, and then you start getting into knowledge and truth,
and at some point, you know, people might start to disagree with you that we should have a kind
of ultra relativist view on what informs those things. So where do you draw the line there?
Right, well, one of the lovely things about being an academic is that you get to work with
some clever people. I'm not clever, but I've been blessed by working with a number of people who
bloody well are. And at the moment, I've got a postgraduate who's mind-boggling. I like to
say I supervise them the other way around. We're going for a discussion, and I come out feeling
I've had my mind blown away, and then he's sitting there looking quite chuffed with how
the whole discussion's gone. Now, his particular thesis has raised some really interesting questions,
and he runs with his very postmodern ideas in extremis. And on one particular
see-for-vision, I'm diabetic, and he was trying to make out, so he could give me this argument
where my diabetic is just an enactment of a certain practice, and I'd say,
that's nonsense. I'll die if I don't do this. How can you decide? The cell call thing,
the cell call conspiracy, if I reap off a tall building, I'm going to bloody well die. It's not
a social concert for this. But anyway, Paolo's work is very, very nuanced, and he does make
some radical claims. When you make radical claims, you've got to be very, very, very careful about
how you use the language. So I'm certainly not going to try and paraphrase his thesis in five
minutes on here, because it's an incredibly nuanced piece of work. But I just want to take one aspect
from it that I think is interesting, because I think it reflects on the world that we're living
in now, and gives us an insight, if it's possible, insight into how Trumpism and the echo chamber
culture has taken root. Because Paolo, and I'm not saying that you go along with this, I'm just
trying to report this as best I can from our discussions. Paolo thinks that we don't just have
epistemic perspectives on a universally shared world, where each of us ontologically can have our
own distinct ontologies. And why does that matter? Well, if it matters, says Paolo, if you start
looking at how this is reflected on Twitter, if you have a community of people, and one of my
other postcards, a guy called Christa Mayer, did a film called Right Between Your Ears, which I
can't recommend hardly enough, looking at an end of the world, documentary feature on the end of
the world cult, on an end of the world cult in America, and how these guys were saying then,
read in the Bible, the world's going to end on a certain day, and of course it didn't, and how
they then dealt with that, and then the leaders, that was going to end again in six months' time,
yet again it didn't. And Chris's film was just looking at how people can arrive at these surreal
beliefs, and I think there's something akin to that with the Q and On movement, and you had this
idea that, you know, so now Trump was going to lead, and the result was going to be overturned,
and then it wasn't, well, something else is going to happen. If you're not in that, how can people
have these bizarre ideas? Well, Paolo, if you get in the community of like-minded people, if we go
back to our basic philosophy and go back into a correspondence theory of truth, so much as a
coherence theory of truth, where a proposition is true because it coheres with the body of,
with the propositions that you take to be true, when you're interacting on a daily basis with
people in your Facebook bubble, who all share the same beliefs that us outside that bubble
look bloody bizarre, they're reinforcing this thing, so your view of the world, your ontological
view of the world becomes really real, you're drawn into this, and that's why people get so
aggressive about it, because it's not, as an academic might abstractly discuss the truth or
falsity of, I don't know, Newton's Law's emotion or something, this is what the world is, you're
questioning something fundamental about these people's experience of the world. We've kind of
covered a couple of big names tonight, we've had Turing come up and Sirle, the other big name that's
kind of been floating in the background is Girdle, in terms of like Girdle Asherbach, the
famous book on human creativity, and you kind of point to the Gordellian, we'll skip the other
guys, but like the Gordellian argument against the possibility of machine intelligence, just for
the guys out there that kind of want to get a bit of a grounding on why his name's been coming up so
much, can you kind of just give us a very brief primer on how Gordellian incomputability kind of
relates to the impossibility of machine intelligence, and how that kind of ties into the other arguments
we've been hearing tonight. Yeah, I think in the Archive paper, which you guys, I think,
hopefully signed posted at the beginning, Art of Intelligence is stupid, there's a, as best as I
can give it, a one paragraph summary in mathematical form of the Gordellian argument, and I don't think
it'd be helpful to go through that line by line now if your viewers are interested, I refer them
to the detail that's in that paper, but to just sort of talk at a slightly more abstract,
wider view, I think, you know, I'm sure Girdle and Turing were both the word of the implications
of their work on logic in Girdle's case and on computability in Turing's case.
And in fact, there are some people who think for Turing, and possibly for Girdle as well,
that some people have made the claim that this disconnect between Turing's avowedly
physicalist to desire to explain all of human mentality via a computer program, plus what
he'd learned professionally about the existence of non-computable numbers led to a big tension
in Turing's life. And there's a documentary that's looked at dangerous ideas, I think it was called,
that explored Girdle, Turing, and Kantor in that context, people who came up with ideas
in their own work that challenged intuitions they had about the world.
But coming off of Girdle, although I feel certain that Girdle and Turing are both in their mind
gone through the implications of their work, as far as I know, one of the first times where this
was actually cashed out in terms of an academic paper was by the Oxford philosopher John Lucas
in a series of exchanges with another philosopher called Paul Benekaraf in the 60s, where to cut
to the chase, I think that Lucas' argument is much more blunderbuss and less sophisticated than
Penrose's version of this. So I commend anyone interested to look at Chatters of the Mind rather
than go back to Lucas. But Lucas basically says, for any any consistent mathematical system, there
will be sentences in that system that we outside of it can see to be true, the Girdle sentence of
that system, but which we can prove can never be shown to be true by that system, unless the system
is inconsistent. And so that Lucas says, you give me any version of computation that you like,
I can, I as a human can step outside of that and see things about that computational system
that it provably cannot know for itself. And that's, I think, the first place where this
argument sort of took off. Penrose, his own take on this is a little bit more nuanced,
so you like to imagine looking at, in Chatters of the Mind, he looks at computations of one
parameter and looks at the question of whether that, he looks at the halting problem, in fact,
if you're familiar with that, on a function of one variable. And that can, you know,
is there a set of rules that will allow us to deter, to tell of any function of one parameter,
whether that function is computable or non computable. And he shows how that leads to a
contradiction. And if we follow the lines of the proof, we see that it comes to a point where we
see a particular statement on the operation of computation K, given K is input, we can see
something about that computation that it cannot possibly terminate, that cannot be shown to be
true following the rules of that system itself. And I think that's an interesting argument. And
as I said, it seems to me when I've read commentaries on Penrose, most people have criticised
his speculations on quantum physics, rather than what he has to say about Girdley and logic. And
in fact, Girdley and Penrose was famously invited to give the keynote at the centenary,
the Vienna conference, honouring Girdley for the centenary. So I think if there was some school
by error, because occasionally he goes, oh, Penrose is wrong. And I think that Penrose is not
stupid. And I think if there was a school by error in his work, I would like to think you've
probably found it by now. But I can recall, when I was at the University of Reading, having an
immensely long exchange with someone about Penrose, his ideas, I won't name them, but they were at
the University of Sussex. And after this has gone on for months and months and months, and I was
saying, look, look at this, look at this, look at this paper. And in the end, the guy turns back
to his advice too short to waste it reading what Penrose actually wrote. And I come across this so
often, people think they know what Penrose has said, or they think they know that Silverhead,
and they haven't bothered looking at the source material on either. And yeah, this is an issue
that comes up time and time again. And that was quite a shock to me, though, when this guy did
eventually concede that he hadn't actually read Penrose at that point in time. I hope he hasn't
actually hadn't. Fantastic. Well, Professor Bishop, it's been an absolute pleasure. Honestly,
thank you so much for joining us today. And I hope to get you back on the show soon.
Well, thank you. I really, I can't seem to be very rambly and unfocused description. I've
tried to think how you're going to get something interesting out of all this, but that's your
genius behind the editing machine, I feel. I hope it comes over okay.
No, honestly, it's absolutely fascinating. I think we don't really have much of an opportunity to
talk about philosophy of mind to talk about some of these deeper issues in AI. And I think this
is a really fascinating framework, actually, to think about some of the various different
focus point. I mean, you know, we had Pedro Domingo's on last week talking about that.
That was awesome. I really enjoyed that. I read the paper that you foregrounded and also watched
the little video that you did accompanying it. And yeah, I thought that was a really interesting,
deeply interesting paper. Effectively, you know, I'm making the case that any
gradient descent train neural networks doing nothing more than a look at interplay between
its training points effectively, which is a profound statement. And I need to let the
dust settle over that. But that was brilliant. It's nothing else. I mean, yeah, reading that was
a great thing for me to do with you guys. It's good for deflating some of the deep learning hype
to just to kind of like contextualize. But also with regards to the night, you've fleshed out
probably 10 people's reading lists or people's reading lists for the next sort of 10 years.
I wouldn't worry too much about rambling. All that means is that there's a lot of ground to cover
and not a lot of time to cover it in. This is the reason why I invited you on because a lot
of these things are very impenetrable. And when I read your paper, it's like a Google Maps kind of
point by point stepping stones list of all of the all of the important things that I need to know
in order to, you know, to get my head around this. And I think it's actually a really great
starting point for people that are interested in philosophy of mind and understanding and
consciousness and so on to read that paper that you wrote. If I read one book on modern cognitive
science, I'd say look at Evan Thompson's mind in life. Unfortunately, it's a bloody fat book,
but I think it's that is a genius book. But I just there's two watching a number of your
podcasts now and I'm really enthralled by all of them.
