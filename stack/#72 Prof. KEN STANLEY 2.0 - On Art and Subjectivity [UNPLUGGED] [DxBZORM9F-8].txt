To achieve your highest goals,
you have to be willing to abandon them.
Today, we're publishing the first
in a small series of conversations
with Professor Ken Stanley.
Reading Ken's book, Why Greatness Cannot Be Planned,
was one of the most intellectually awakening moments
of the latter part of my life.
It really turned my thinking upside down,
making me question many deeply held beliefs
which I had previously.
The episode that we did with him
was many months in the making
and perhaps the greatest ever episode of MLST.
Ken argued in his book that our world
has become saturated with objectives.
The process of setting an objective,
attempting to achieve it
and measuring progress along the way
has become the primary route
to achievement in our culture.
It's not like he's saying that objectives are bad, per se,
especially if they're modest.
But what he thinks are that when goals are ambitious,
which is to say they are unknowable, complex or abstract,
or put more simply,
that they entail discovery, creativity,
invention, innovation, or happiness,
then the search space becomes deceptive,
which is to say the measure itself
becomes a false compass
which blinds us to the stepping stones
which we should actually take.
I mean, is the key to artificial intelligence
really related to intelligence?
Does taking a job with a higher salary
really bring you closer to being a millionaire?
The problem is that the stepping stones
which lead to ambitious objectives
tend to be pretty strange.
They don't resemble the final end state.
Vacuum tubes led to computers, for example,
and YouTube started as a video dating website.
In a sense, creativity itself
is actually a search problem.
Is it possible to explore a search space intelligently
without using an objective
to align towards discovery
and away from the trap of preconceived results?
Greatness is possible
if you're willing to stop demanding
what greatness should be.
The greatest moments and epiphanies in our life
are so often unexpected and unplanned,
serendipity can play an outsize role in our lives.
At the end of Joel Lemon and Ken Stanley's 2011 paper,
Abandoning Objectives,
they concluded that it was almost like a riddle.
Novelty Search suggests a surprising new perspective
on achievement, which is to say,
to achieve your highest goals,
you must be willing to abandon them.
I always get engineers on my team to read this paper
and it's illuminating in so many ways.
I love using the visual analogy of a maze
to represent the search problem of life
and stepping stones in that maze
as being either potential objectives or end states.
Of course, we don't know about the existence
of most of the objectives
and the fog of war blinds us
from seeing far ahead into the maze.
The fundamental problem is the missing information problem.
But as we'll soon find out,
not just the information itself,
but how we represent it, understand it,
experience it and even know it.
The fascinating thing is that if we were an oracle
and we knew exactly which steps to take in life,
we could become billionaires within months
or we could achieve anything we wanted to.
The only inconvenient thing stopping us
from realizing our dreams
is that the space of possibilities is exponentially large.
It's very expensive to sample many trajectories
in that space.
So we tend to get stuck in certain sections of the maze
for long stretches of time.
Ken thinks that the most valuable commodity in search
is information.
We must accumulate as much of it as possible.
There's an arrow of informational complexity
and natural evolution.
And Ken was the first machine learning researcher
to take seriously the growing yet controversial view
in biology that adaptive selection
does not explain the arrow of complexity in nature.
We're standing in a maze right now.
There are many doors within walking distance
which lead to unthinkable greatness.
We simply haven't walked over and open them.
We are existence proofs of general intelligence.
Every single one of us has a brain,
but just like the infinite number of doors
we could open in the maze of life,
it took evolution billions of years
and gazillions of individual life forms to create our brains.
Now, Ken is known for his pioneering research
in the open-endedness space.
You might recall the poet paper,
the pairwise open-ended trailblazer.
It was the first ever episode of MLST that we spoke about that,
but that's an example of this line of research.
Open-endedness might seem like a nebulous term,
but you can think of it as being an AI system
which doesn't have a boundary in its state space
and doesn't ever finish accumulating information.
Sir Arthur Stanley Eddington,
the notorious astronomer and physicist
born in the late 19th century,
spoke about the subjectivity in science.
His point is that science only tells us a sliver
of what's really happening in the world around us,
and we should be a lot less arrogant in our claims
to understand it and our attempts to formalize it.
Physics, he argued,
can never reveal the true nature of things,
rather it deals with relations between observables,
which are subjectively selected by the human mind.
He says that he was inclined to attribute
the whole responsibility for the laws of mechanics
and gravitation to the mind
and deny the external world any share in them.
He went on to say,
the laws which we have hitherto been unable to fit
into a rational scheme are the true natural laws
inherent in the external world,
and mind has no chance of molding them
in accordance with its own outlook.
Subjectivity is everywhere,
and yet in so many ways we delude ourselves
that we are transforming subjectivity into objectivity.
Imagine a fisherman catching fish in a small ocean.
Depending on the size of his net,
he might reason that there's nothing very small in the ocean
because those objects are slipping through his net.
What we can agree on is that our understanding
of physics breaks if we zoom in too much
or zoom out too much,
so at best we have a frame of reference,
which Eddington would argue is observer-relative.
Surprisingly little in our world can actually be objectified
without using abstract motifs.
Have you ever asked a philosopher
to define what it means to be real,
or what it means to exist?
I can guarantee you that they'll be reaching.
They'll say things like something exists
if it has a causal effect on the world,
or if it can be measured,
or if it's not illusory,
which is to say it is as it appears,
or perhaps that it's genuine in big air quotes.
What does it mean to be intelligent?
What is life?
Describe an ecosystem.
What is British culture?
Describe your mind.
Describe your conscious experience.
Have you heard of the parable
of the blind men and the elephant?
Any attempt to formalize a complex phenomena
lead to excluding large parts of the truth.
What fascinated me about this conversation
with Ken this evening
is I got a much deeper understanding of his philosophy.
He led by saying that he thought
it's worth questioning
whether artificial intelligence
is even a science or not.
Just let that sink in for a moment.
I'd been too focused on my mental framework
of objectives, behavior, and actions.
The broader story is that Ken thinks
society and institutions
are scared of any subjectivity.
Subjectivity in general.
He thinks that attempts to formalize complex,
emergent, and integrative phenomena
like intelligence, consciousness, life, society,
the mind, or anything else for that matter,
only deludes us and blocks us
from potentially discovering a deeper reality later.
It's a very human trait to seek to understand
the world around us to varying degrees of self-delusion.
Before we understand a particular phenomena,
it's almost impossible to come up
with a good scientific and formal definition.
How could we possibly define something
which we don't understand well?
This is the paradox in computer science and philosophy.
The more we seek something, the more it eludes us.
It sounds almost anti-intellectual, doesn't it?
This idea that we should reject formalization.
But Ken thinks that many phenomena
will only be trivialized by vacant attempts
to formalize them through oversimplification,
because inevitably we'll chop off many aspects
which might not be captured by the formalization.
I had always associated Ken's philosophy
with the missing information problem
rather than the representation problem.
The missing information problem
is that we don't know something
or we only know part of the truth.
The representation problem is that we simply don't understand
or at least we can't verbalize what we're experiencing.
Ken says that the corporate world is dominated
by the attempted veneer of objectivity.
If you're trying to land an engineering role in Fang,
your tolerance for ambiguity and subjectivity
is the single most selective feature
of the level which you get higher than.
If you can solve clearly defined problems, you're a level four.
If you can find the problems, you're a level six.
If you can find the areas, you're a level seven.
And if you can find people who can find the areas,
you're a level eight or a level nine.
I give loads of open-ended tasks to my team,
discover people in the organization
who are subject matter experts in domain X,
build an operating model for knowledge sharing
and standardization.
What does good look like?
It's possible we don't know it yet,
which is the missing information problem.
But it's also possible that we know it,
but we're not able to verbalize it,
which is the representation problem,
inherent with all complex phenomena.
It's quite interesting actually,
I'm building a code review platform called Merge.
And it's easy to objectify all of the low level metrics
such as how many customer engineers we support
or how efficiently we're sharing knowledge.
All of the higher level outcomes
are like a better engineering culture.
They're emergent properties, right?
It's a bit like pornography.
You know it when you see it.
So now I give you Professor Ken Stanley.
We have a nose for the interesting.
That's how we got this far.
That's how civilization came out.
That's why the history of innovation is so amazing.
Everything washes out when we start ruling by committee.
Like we have to allow people to follow their passions
to their extremes.
And yet we run society as if this actually
makes any sense at all.
I think the gradient of interestingness
is probably the best expression
of like the ideal divergent search.
You get to this problem that like,
I don't know how to formalize interestingness,
which you get to then are proxies for interestingness.
That not everything that's novel is interesting,
but just about everything that's interesting is novel.
It is in my personality and nature
to want to overthrow this,
I guess we could say tyranny of objectives.
So today we have an incredibly special guest.
Indeed, my hero in AI, Professor Kenneth Stanley.
Now the Kenneth Stanley show that we filmed,
which is to say episode 38,
was my favorite episode that we've ever done.
Reading Kenneth's book,
Why Greatness Cannot Be Planned
and preparing for that show
triggered an incredible amount of intellectual growth for me.
And a hallmark of that,
when you learn something profound,
is that you start recognizing it
in many other domains of your life.
I mean, you remember when you learned probability theory
for the first time
and you started seeing exponential distributions
absolutely everywhere.
So I was just saying to Kenneth
that when we listened back to the show last night,
it was probably the best example ever
of a show we overprepared for,
which is to say that during the interview,
we were just bamboozling Kenneth,
we were so excited,
we almost couldn't control ourselves.
And I think in particular,
Keith and Kenneth reach a common ground actually
in the sense that divergence and convergence
don't have to be hyperbolic
or entirely mutually exclusive.
Now, Kenneth thinks that we have a nose for the interesting.
That's how we got this far.
It's the basis of all innovation
and the secret of our society.
Kenneth believes that the concept of deception in search,
which is to say getting stuck in local optimans
and indeed deluding ourselves
that we even know what good looks like in the first place
is what regularly sends us into brick walls.
Kenneth thinks that institutions are full of gatekeepers
and the gatekeepers only want to see your objectives
and metrics.
Any expert on search would know
that it's completely naive
and yet we still use this approach
for the most complex problems in our society.
Committees wash out everything.
We need to let people follow their interests to the extremes
and risk has to be tolerated
in order to make the greatest discoveries.
Anyway, Professor Kenneth Stanley,
it's an absolute honor to have you back on the show.
Thank you for having me.
So it's been great.
I really enjoyed being on it.
I'm glad to be back here.
I mean, even the thing you made in front of that show
was awesome.
I showed that to my son and he was like,
what the heck is that?
He's like, that's not my dad.
And I was like, that it?
He's like, you never seen me in that kind of environment,
but it was pretty cool.
Oh yeah.
So we painted a cafe in behind you,
but we did that first thing in Blender,
but I had to learn Blender just for that.
It was the first thing I ever did.
And what I started doing afterwards,
I wish that we did your show a tiny bit later.
I started using my virtual reality headset.
So I've got an Oculus Quest,
and then I use Google tilt brush,
and then I can kind of create a beautiful 3D environment.
I can put slides and videos and all kinds of stuff up there.
And that would have been perfect for that
because I wanted to show all of the stepping stones
in the divergent search in a 3D environment.
But that's a cool thing is we're getting more sophisticated
and we're starting to use Manum to do animations
because we want to be able to really kind of,
at least for some of the episodes,
tell the story in a very visual and educational way.
So we're kind of going back now and revisiting
some of our favorite episodes.
And can we tell parts of it again
with this new technology, if you will,
or new techniques that we have skills at.
That's cool.
I got to tell you, just on a personal note,
because you mentioned your son saying that's not my dad,
part of my motivation for doing these YouTube shows
is that I want there to be something
where my kids can go, maybe when I'm gone or whatever,
and look back and say, oh yeah, that was my dad.
That's what he sounded like.
That's what he looked like.
That's how we talk.
It's a really nice thing.
Yeah.
Yeah, yeah.
The problem is no one understands
what we're talking about.
Well, maybe they will.
One of my friends and family,
they say, oh, I tried to watch it,
but I just couldn't understand what you were talking about.
It's so funny because from my perspective,
we're really making things accessible
and dumbing things down as much as possible,
and I still think of myself as completely clueless,
but it's quite deceptive, isn't it,
that you don't realize you're talking another language.
Yeah, it's funny.
I mean, my mom, I said, here's this video,
I mean, she has no idea what this is,
but she showed her sister who's like 80 years old
and didn't know anything.
And her sister was just like,
it seemed like you were really winning in that discussion,
even though I have no idea what you're talking about.
That's cool.
Well, that's good.
At least the tone sounded like I'm winning.
Well, yeah, I mean, it's kind of funny.
Well, speaking of winning,
and that's kind of the question I wanted to ask you,
is that it's one of those ideas,
Tim mentioned these kind of ideas
that the first time you hear them,
and after you kind of digested them a bit,
then you start seeing them everywhere,
and you start seeing connections to it everywhere.
And I think your ideas are like that,
at least they were for me.
And I started coming to this line of thinking
I just wanted to ask you about,
just to see if it's completely off base,
or if there's any, maybe any kind of truth to it,
which is, and I know I keep repeating this, Tim,
and I apologize,
but one of the most beautiful quotes I ever heard
was from Claude Shannon.
Many decades ago, I heard this,
and he said,
we have knowledge of the past,
but we cannot change it.
We have no knowledge of the future,
but we control it.
And I realized back then that there's this duality,
and it's reflected there in that asymmetry of time,
but also in the way I think about science and engineering.
So there are the two sides of a coin
where scientists use engineering to control
in order to gain new knowledge,
whereas engineers take the knowledge that we have,
and they use it to gain control,
by building a better building,
crossing a water stream or whatever.
And it's also similar to the exploration
versus exploitation trade-off.
And I'm coming around to this way of thinking that,
yeah, objectives can be quite harmful
when you're in that exploration phase,
trying to learn new knowledge,
because just as you point out,
who knows what the stepping stones are gonna be like?
And all the kind of, let's say, quote, unquote,
counter-examples that I were thinking of before,
it's because they fall more in the engineering side of things,
which is, okay, look, we have some knowledge,
and we do have a very specific goal
we're trying to do with that knowledge,
build a faster rocket or whatever.
And so, yeah, we just go down this kind of very
refinement, objective-based kind of goal,
but it's really not gonna lead us necessarily
to new knowledge or new insight.
Is this kind of a fair dichotomy or not?
Yeah, I mean, nice cute to hear you're coming around
to that way of thinking somewhat.
It's like a while in a bit.
Hey, I'm open-minded.
It's, yeah, it's interesting to connect to Shannon.
That I appreciate, and there's an interesting connection.
And I think the dichotomy is pretty fair,
that like in search of discovery,
yeah, a lot of engineering principles
are not really the best tools for innovation necessarily,
especially basic, like basic exploration.
And it's true also that it seems like engineering
culturally kind of pervades scientific education,
at least in some sciences.
I don't know, like I only know about my own experience
in computer science, but like I was in
computer science engineering major at U Penn
that was like computer science engineering is like one thing.
And so, yeah, like a lot of engineering philosophy
is like stuck in with the science.
And maybe there is like some conflation there.
But I also think like another thing to consider,
which might be a little bit more out there
and controversial is that I think it's worth
questioning whether artificial intelligence
really is only science.
And is that actually what we're doing?
Like a lot of the discussion about AI will come back to
that's not scientific or there's a kind of attack
or a way of looking at like an idea where like
we can sort of invoke science knobbery to put something down.
And one interpretation is just that maybe there,
it was a good scientific insight,
but somebody just missed it or something.
They're not seeing the big picture.
But another interpretation is that actually
this is not science and it shouldn't be caged inside of that
only that way of thinking.
And I've actually written about this was much less
received much less attention when I wrote about it,
but I thought a lot about it.
Like that in some ways I think AI is has a lot
of connection to art.
And I don't mean just in the sense that like you can use AI
to generate art.
I mean, you can do that obviously.
But I mean, in the sense that like art is about
the reproduction often, not always,
but often it's about the reproduction of natural artifacts
in an artificial way.
Like, I mean, when you paint a picture of like an apple
or something like a still life
and nobody's expecting it to be a real apple.
Like that's not the question.
And nobody even expects there's necessarily
a faithful reproduction of an apple.
Or let's say like, you know, Van Gogh's starry night,
it doesn't look anything like a real starry night
that I bet anybody on earth has ever seen
even if they're on drugs.
And yet what does it do?
What is the value of starry night then?
Since it's not accurate.
Scientifically, it's not a very good job.
Well, it actually gives you new insight
into something about starry nights
that you hadn't really thought about.
And that's what's thrilling about it,
is that like it gives you a new perspective
on something that you're familiar with.
AI is also concerned with the reproduction
of natural phenomena in artifice.
It's the same thing.
And in a similar way, like an algorithm is,
I think it's just another way.
It's kind of like another painting.
It's just a painting that's expressed
rather than through brush strokes.
It's expressed through code.
But if you look at it that way,
you can understand that like,
it can be an interpretation of nature
without being accurate.
And that can still be valuable
just as starry night is valuable.
And the reason it can be valuable back to science
is because some of the thoughts it gives you
will be stepping stones
that then lead back to scientific insights.
So we're straddling between science and art here.
But the thing is,
there's not necessarily a harmful thing to do.
And so we should actually consider
whether like to the extent
that an algorithm actually is an artistic inspiration,
which I think a lot of what I was doing was,
like when I was building things like NEET,
I was inspired by like the increases in complexity
in nature.
I was never under the delusion
that NEET is an accurate depiction
of how evolution works in nature.
But it's kind of an ode to nature in a way,
like the way a painting might be.
But it's never discussed in that way.
You said in your presentation last time
that you were becoming a little bit more radicalized
because you were doing it to not computer scientists,
artists.
And all of a sudden they were saying to you,
oh my God, Kenneth, like,
finally, my parents keep saying to me,
why are you doing art?
Why are you wasting your time?
And now I understand.
Now I actually have a sense of purpose.
It means something.
And I think what you were saying a second ago,
because it's quite interesting, you said,
or a starry night.
And you could say that's a kind of model
or a kind of representation.
But is it fair to say that you're actually more interested
in analogizing the process itself?
Like analogizing the process of discovery in AI with art.
Is that what you mean?
Yeah.
I think there's an analogy between art and AI.
It's more than just process.
It's like, it is a branch of art.
Like it's literally about like reproducing natural phenomena
in artifice, which is what artists do.
The difference is that the,
the, I mean, art is, you know, you might say it's like,
there's more of an aesthetic emphasis
or something in art, but I mean, that's just a choice.
I mean, that's what we're doing in AI.
We're interested in, so one exception though,
I should say for the artists who are listening,
which are probably few, but I want to acknowledge
that that's not the only thing that art is concerned with.
When I wrote about this, I actually,
I wrote a paper on this.
It was one of the weirdest things I did
because I actually went to like a real library
and was researching art history and things like that.
I felt like a real scholar or something in the library.
But I, so I got a lot of, I got some criticism from artists
and like one of the things they were pointing out was that
it's not always about the reproduction of nature.
Like there's some modern art
that has nothing to do with reproducing anything.
It's just about like the pure aesthetics.
So let's just acknowledge that.
That's not all art is about, but much of art
throughout art history has been about that.
It's certainly a part of art is to do,
is to reproduce natural phenomena in artifice.
And that's what art AI is.
So it's not just a connection.
I think it is art.
It's just that we're not willing to discuss that side of it
because we're very proud of ourselves for being scientists.
It makes us feel good about ourselves.
But the reason I start thinking about this
is because I was getting in some arguments with people
where the kind of the crux, the breaking point
where they tried to basically stop it
and say this is the end of the argument
was basically to say that what I was saying
was not scientific.
Or like this is not a scientific question.
It can't be objectively analyzed.
You can't get an objective,
you can't get falsification on this question.
So it's basically not subject to debate.
Like let's not talk about it because it's pointless.
And I felt like that is just pure cowardice.
Like you are just saying you are afraid to inquire
in directions where you don't know how to falsify.
To me, that's just cowardice.
And I do not like associating science with cowardice.
And I started thinking about,
am I really doing science?
Like maybe there's another view here.
I don't know that I was like fully just trying
to generate hypotheses and validate or falsify them.
There's an artistic side to,
I think creating these algorithms.
And I think it might be healthy to acknowledge that.
It's interesting to think about.
I think your frame of reference
doesn't seem to fit into the paradigm of science
in so many ways.
And I remember you said on the show last time
that you were exasperated that pickbreeder
was not recognized as being scientifically useful.
And there was another convergent committee version
of pickbreeder.
And then the images just looked like wallpaper backgrounds.
To your point before, I kind of agree with you
that when you look at what happens in the process of art,
it seems very intelligent.
But I'm trying to understand where the intelligence lies.
Because I'm interested in the idea of it being embodied,
for example, and an emergent phenomenon.
So clearly when human beings collaborate together
in this divergent way, that's something very interesting.
But what would happen if the agents producing the art
were much less intelligent than humans?
Do you think that could still lead to a kind of intelligence
at a larger scale?
Yeah, and I just will also preface by just saying
that I'm not against science.
I do like being associated with science.
I don't want to be now categorized as an artist only.
I just think that we're straddling both here.
And the mechanisms of science clearly have served us well
in many cases.
So I don't want to be associated with kooky views
that we should get rid of science or something like that.
I just think it expands our horizons
to understand what we're doing and how it relates to art.
And also just before to answer the question,
I think it's also important to note that I'm not just making
the generic point that there's an aesthetic aspect to science,
which people have written entire books about.
People have pointed out before, not me,
but there's famous books that have been written about how
science has an artistic side, like an artistic inclination
can help you as a scientist.
I mean, this is not new to point these things out.
But I'm trying to make a much more literal point
that AI itself, specifically AI, with the word artificial
in front of it, is really about art in a strong way.
And this is what has not been acknowledged.
This point has also come up in mathematics.
And for the life of me, I can't think of whether it was
Penrose or maybe even Gauss or somebody,
but a legendary mathematician made this point.
They were demoting the fact that there wasn't
more creativity in mathematics.
Because if you look at some of the greatest achievements
of mathematics, they were these things that were created
that were entirely new, like calculus from Leibniz or Newton.
I mean, just these new creations, new ways of thinking
about things that were really inspired
from a more artistic sense.
And he was making the same point that that's mathematics.
And to kind of say, well, no,
if it doesn't have a certain level of rigor,
it doesn't approach mathematics.
Well, in order to get to those forms of rigor,
you first have to have that stepping stone,
that inspiration that generates something brand new.
And I gotta figure this out,
but I forget who it was and they made this point.
So if it applies even to mathematics,
it has to apply also to things
that are more grounded in reality.
So I think your point completely stands.
And it is sad that people tried to dismiss it as,
well, that's not science or that's not mathematics.
There's an interesting response to it's like,
good, it's not science.
It's like, I wonder where that leads exactly.
It's like, I'm not pretending to be talking about science.
It's like, it's still so we should discuss.
Good, it's not already known, I'm doing something new.
Yeah.
So where does the intelligence lie?
I think was where I was getting out.
I'm really fascinated by this idea
that it's not in the brain that it's in the process.
And I think your ideas are going in that direction.
Yeah, I remember your question.
Yeah, yeah, so like if there was a less intelligent
kind of move towards or less intelligent type of agent
that was involved in an artistic evolution,
like what would be the quality of that?
Yeah, I think, I do think it's possible
for there to be artistic, it's complicated
because like art is subjective.
I mean, we're making it for ourselves.
So like if there were things that aren't us
that are doing something artistic,
like presumably they're making it for themselves.
So it might not be interesting to us.
I'm not sure it would, it might be interesting to us,
but it would be a process worth paying attention to
in some way and I do believe that
because it would have these properties of trends
and stepping stones and like an evolutionary process,
like a phylogeny would result from that.
Whether it's of interest to us is,
I mean, just the artifacts themselves,
I'm not totally sure that they would be,
because a lot of art does often reference things,
like I said, in nature, because that's what our experience is.
I mean, if you have no experience of nature,
art can't be about that then.
So what is it about?
It could be about other things,
just mathematical relationships.
Some small subset of art is, like I said,
like I mean, some modern art has nothing to do
with referencing anything in the world.
So maybe some people would appreciate that,
but even those things seem to relate
to some emotional resonance or something like that.
Like these beings may have no emotion.
So I don't know where you'd enjoy this,
but from a kind of like just like analyzing
the process point of view, I think it might be interesting
because there would be such a process.
It's so interesting because the rubber meets the road.
We're talking about this, all of these brains
operating together in this divergent process.
And you were just saying that an artist possibly
has some kind of phenomenological resonance
with a situation.
And I'm interested in the kind of continuum
between brains and the kind of algorithms
we produce in computer science,
because I think we could all agree that neural networks
and optimization algorithms,
they don't seem very human-like
and they certainly don't think the way humans do,
even if they can produce intelligent, seeming behavior.
So you really lean into this idea
of trusting our instincts and subjectivity.
And a cynical reading is that our subjectivity
is essentially random.
And a random search would be the most divergent search,
but that would clearly be rubbish, right?
So there must be some kind of continuum
between a totally random search
and a principled divergent search.
So how would you kind of articulate and reason about that?
Yeah, it's true that any kind of divergence
that pushes too far towards a randomness,
I don't think would be interesting.
And I don't think that's what artists,
I don't think that's what human artists are doing,
because I don't think it would be interesting.
It's just that art is,
they are really concerned with what's interesting, I think.
But without the constraints of science,
so it's not supposed to prove something
that you're trying to figure out whether it's true or not.
It's just supposed to lead to some kind of insight
or feeling or something, it depends on the artist.
And of course that then points back to things we care about,
because we're humans and we care about having insights.
And of course the things we want to have insights about vary,
but like generally art that we appreciate
like leads you to having some realization
that is generally commonly held,
like that people would agree is interesting.
And so the artists are exploring that
and that leads to other ideas that might not be art.
You know, I think, yeah, like a scientist
can be inspired by art to think about a phenomenon
or somebody else can be inspired architect,
obviously many times inspired by arts.
There's lots of inspiration that comes out of art,
if you feel philosophical as well,
like about like how the world works and what matters.
There's plenty of topics to talk about
that aren't falsifiable in a scientific sense.
And you know, with AI algorithms,
they, I think secretly a lot of the explanation
for what has risen and fallen within AI is that,
not the actual scientific results,
is that people have resonated, I like that word,
because that's really about art.
It's not about correctness or accuracy, it's about resonance.
And people have resonated with certain algorithms.
It's like they just felt it.
It got to a point, let's just look at an artistic realization,
not a scientific realization,
like where it resonated with some sense
of what intelligence is for you.
And it's not like the whole thing.
Nobody got the whole deal of intelligence, obviously,
but some part of it like resonates.
And that can be extremely inspiring.
And I think explains certain like inflection points
in the history of machine learning,
where I think it was resonance really that explains it.
Yeah, and I think it's really interesting
what you're saying and I think there's a tendency
of some to again dismiss this type of thing
as not science, woo woo, whatever.
But I think that stems from being insufficiently Darwinian
in the sense that, look, whatever's up in our brain,
it's the beneficiary of a billion years of evolution.
And these insights and intuitions that we have,
even if we're not conscious of them,
maybe they're not happening at a level
that we can analytically break down consciously
and think about could still be extremely useful
and extremely valuable.
And so I have no doubt believing
that when a human mind sees an algorithm even,
it can perceive some connections to some abstract concepts
that have been proven out through evolution
as being highly, highly useful.
And so you may be seeing those connections.
Is that, I mean, does that capture potentially a fair
and scientific justification
of why we should pay attention to intuition
and artistic intuition?
Yeah, I think it's fair.
I think though I wouldn't only couch it in terms of evolution,
I think it's broader than just from evolution.
Like it's from our experience also,
like experience since you were born.
And that's your memories and the feelings that you've had
over the course of your life that I think enter into.
Of course, just some evolution explanation
for how you process those experiences.
But the experiences are also part of the background
for what you appreciate and find interesting in your life.
And yeah, I think that that is, as you say,
an important part of the history of ideas, even in science.
And what I guess, what's actionable about that though,
is that it's interesting to think about the extent to which
we should actually allow or facilitate discussion
on this level, not at this meta level,
like we're talking about just, should we do this?
But at the level of here is what resonates to me,
like about the specific thing.
This is why it's interesting, like as a reviewer,
like I don't care if it gets like 5% less accuracy
on this set, it's super interesting because XYZ,
it reminds me of something, powerfully reminds me.
Like can we have discussions like this?
We can't right now.
And so it's interesting just to think about that,
like would it help to facilitate progress?
Would it stymie progress?
Because I think most people,
their first good instinct is it's bad for progress
because it opens the floodgates of sort of like unregulated,
like unempirical type of speculation.
We're all afraid of that.
But I think we should be cautious,
but I think we're too afraid of it.
And that like we can handle this
because like we actually know about what we're talking about.
Like that's the thing that makes this valid.
Like it's again, like if it's just some random person
on the street, I wouldn't want to have an aesthetic discussion
with algorithms, but if it's experts,
I don't understand why we're not allowed to have
aesthetic feelings and relating things in like analogizing.
Things like that seem perfectly fine.
We should be able to do that.
I'd say even if you can't do that, there's a problem.
Like what, why are you an expert?
Like if you can't make analogies
and actually talk about what's interesting
or inspiring about the work.
Okay.
One thing that I'm wrestling with a little bit here is,
I mean, Douglas Hofstadter, the famous Douglas Hofstadter,
he once said that he was terrified
that AI might be disappointingly simple to mechanize.
And I think a lot of the stuff that we're talking about here
is, I mean, we're talking about subjectivity,
but also we're talking about the externalization
of intelligence.
So rather than it being encapsulated
in an individual brain, a lot of it is emergent
and can be thought of as something completely different.
I'm concerned about the lack of free will
for want of a better, so we've been dealt
with the experiences and the environment
that we have in life.
And to a certain extent, is it still intelligent
if our cards are marked?
If everything has been mapped out in my life
as a function of the environment that I'm in
and my life experiences, I guess I have this
just as Douglas Hofstadter did,
I have this very fanciful idea in my mind
of what intelligence is, that it's infinitely nuanced
and we have this phenomenological experience.
And, you know, for example, Hofstadter spoke
about Chopin, this beautiful piece of music
and what the infinite nuance and subtlety
that must have gone through his mind when he created it.
And wouldn't it be horrible if that was just the result
of quite a simple process
that you could define using code?
But this is an interesting question.
I think, yeah, it's clear that there's a movement
in machine learning towards that kind of perspective
of basically simplicity.
And it goes back before deep learning.
I mean, people were observing that they say, well,
like, you know, cortical circuits like in the brain
all share like a huge amount of similarity.
It's like, it is possible it's all the same algorithm
all throughout and there's just some simple explanation.
And then, like, when we see like things
like really large, large language models
that are basically like uniform architectural structures
that just get bigger, it seems to,
it seems to point in that direction
that like it's not like a bunch of really complex,
rich subtlety like going on through the system.
We don't know though yet, we don't know.
I mean, the jury's still out,
we haven't actually gotten to human level.
And I think that, yeah, and I mean,
you can also point to evolutionary processes too.
And they're also simple, like there's a simple thing
and this explains everything,
like it's not really that interesting of a thing
in and of itself.
I don't, I guess to me, it's just,
we would like to know the answer to this.
Like, can you actually get these things to work
through very simple processes?
That's probably really important to know,
like just in terms of being able to do machine learning.
But I don't think for me it would be
that disappointing one way or another, I think.
Cause I think the subtlety is still in there.
It just came in through a different channel.
Like, okay, maybe the subtlety is not in the architecture.
I personally think there is subtlety in the architecture.
That's my guess, like it's not gonna be super simple.
But let's say it doesn't have to be,
it could be all like uniform.
But then like what you do and what you care about
can still be, like the constellation of stuff
that you learned, which you learned over your lifetime,
like what that amount to in aggregate
can still be, I think, highly rich and subtle
in its connectivity.
It's just a structure that emerged from a simple process
which allowed it to emerge,
but then the structure itself is complex.
So I don't think it would diminish sort of
like the grandiosity of like what we are to me.
I could see other people might think otherwise,
but it's okay.
I don't know what the actual truth is,
but that wouldn't necessarily bother me.
Yeah, I agree with you.
It doesn't decrease the grandiosity of what we are.
Consciousness, you're talking about?
I was surprised that the guy's done so much conversation.
I didn't realize it struck a chord.
I find that, I think,
we don't know what the motivation is behind that tweet.
It's, I'm assuming it was meant to provoke conversation.
There's no depth in that tweet at all,
but it's not because it's dumb.
It's because like it's a tweet
and there's no room to actually talk
about all the complexity of the issue.
And so I'm assuming that he's not actually making an argument
that he even thinks is like persuasive one way or another.
He's just provoking discussion.
And as such, I think it's effective.
It provoked discussion certainly.
It got a lot of discussion going
and allowed people to show their cards on this.
And I'm actually curious about
what people's cards are on this.
Like people don't talk about this that much.
I think it's not immediately germane
to making progress in machine learning.
So it's in some ways you might think it was a waste of time
because we can't really use this discussion to get anywhere
and people are busy trying to publish papers and stuff.
But I'm just personally curious
like about what people think in this field
because it's obviously relevant
like to what we're trying to do.
I mean, maybe we connected
into quite a lot of the symbolic community.
And on LinkedIn, everyone was just posting saying,
oh my God, that, you know,
this hype is getting out of control,
the runaway train of deep learning.
No, it's like, oh, you're basically just like
feeding into the hype.
I could interpret that to be completely independent
from deep learning hype.
Well, it's just basically saying like,
is there a threshold that's crossed
where there actually is like conscious phenomenon happening?
And it doesn't mean that what we're doing
is right at all right now.
It's still an interesting question.
I love the way I would interpret it.
Because it is provoking this conversation
that, you know, we need to be a bit more defined
than what we mean by consciousness
or at least to think about it.
So I thought it was for the purpose that can things,
which is to provoke conversation.
And I will admit, it provoked me to tweet something.
I actually don't tweet much,
but I actually tweeted something last night
because I just couldn't resist it unconsciousness.
What did you tweet?
I don't know if it'll upset you, maybe.
I don't know.
Yeah, tell us, tell us what did you tweet, what's upset him?
What did I tweet?
I'd have to look at my phone
to remember that what I said exactly.
Trigger warning, Tim, be careful.
Yeah, so this is actually connected to our discussion
in some subtle way here that we've been having
because like, my tweet on consciousness,
I was pointing out that I noticed
like since the original consciousness tweet,
like a lot of people making off-handed comments,
basically dismissing consciousness is not a good topic
because it lacks an objective measure.
This is an easy way to get out of this.
And the thing that's being missed here
is that it's precisely why it's fascinating.
Like it is the phenomenon of subjectivity.
It cannot have an objective measure
unless we're talking about a superficial aspect of it.
But the interesting aspect of it
is the part that's hard to talk about.
And so it's literally what it's like from the inside.
And so the idea that we cannot discuss that
is an interesting idea,
is exactly the kind of cowardice that I'm talking about.
We're using science to block us
from exploring something that's uncomfortable.
And if science lacks the tools,
like if that's what we're saying,
it lacks the tools to address consciousness
because it is subjective.
That's not an indictment of consciousness as a concept.
It's an indictment of science.
That's right.
You've got me there.
I must admit, that's a very clever response.
Yeah, I don't know how to respond to that.
But I mean, I personally like to think of consciousness
as being the, you know, like qualia
and the subjective phenomenological experience.
And I thought it was a stretch to say
that something like GPT-3
could possibly have any kind of subjective experience.
I find it a stretch that you have subjective experience.
I mean, or did I do?
I mean, why a bag of atoms as a subjective experience,
like qualia, if you have no idea why that would be?
This is the problem, right?
So we know quite a few people and they are so cynical
and they argue that intelligence is just a parlor trick,
that GPT-3 is intelligent and we're not really intelligent.
And when we think we're thinking,
we're just doing some hash table lookup
and it's all a trick.
Yeah, I mean, there are people that have argued
that consciousness is just like a trick.
And we're just confused when we think we're conscious.
It's really nothing special going on.
Yeah, I mean, if you take that view,
then none of this is very interesting.
I don't take that view
because I believe that there are qualia,
but that's just a belief I can't prove anything.
But then again, how could I?
It's a subjective discussion.
And so like the real issue at hand here
is like whether we think that subjective phenomena are real,
like do they actually exist?
To me, it's worth discussing,
even though it's actually outside of the bounds
of current science.
I don't know any way that we can look at this empirically,
but I don't find that ambiguity uncomfortable.
I think it's interesting.
I like things that are ambiguous.
That's where we start learning things.
I have to hand it to you.
You've really got me there.
Okay, well, that's good.
Yeah.
Made a point.
Yeah, I mean, part of this,
and some of those points are somewhat old,
like the point has been made,
I think many times that science's lack of ability
to describe consciousness is not an indictment of consciousness.
It's an indictment of science.
I mean, there's things missing from it.
We need to expand it a bit.
That you're quoting a tweet there,
but someone probably said it before.
Yeah, it's kind of an old indictment of it,
but I think what I often see is that,
people often cling to kind of extreme definitions of things
because if they're confronted with a middle ground
that's completely reasonable, it's just boring.
Like they almost just can't accept that that's the answer.
For example, for me, consciousness, from my perspective,
it's definitely a pattern of neural activity in the brain,
and it's probably one that's doing something
like analyzing the neural activity
of other parts of the brain and or itself, and that's it.
Like it's, you know, what's the big mystery here,
but that definition's almost like too easy and too reasonable,
and then we have to start talking about,
yeah, but what does it feel like to be that,
you know, that pattern of neurons?
And I saw this happen like in a debate
between Daniel Dennett and Sam Harris about free will, okay?
Where Daniel Dennett is saying,
look, to me, free will is the fact
that you can evaluate options and you evaluate those options
and one path is taken based on that evaluation.
So for example, a chess program,
if it's evaluating the board possibility
and doing a Monte Carlo tree search
and it evaluates one as being the best option
and it takes that path, that's free will.
In other words, it's freedom of parameter space.
It's, you know, freedom of options.
And Sam Harris' only response to that is like,
well, okay, but that's not what people think.
Like that's not what somebody on the street says is free will.
They think it's this magical thing that, well, who cares?
I mean, who cares what people think is surprising?
Like there's people that believe all kinds of things
that don't have any type of scientific basis
or mathematical basis.
We find a very reasonable definition of free will
that's totally compatible with the reality
and the pragmatic experience of free will.
And yet because it's surprising to some people
or because it's too boring, we just refuse to accept it.
Yeah, well, you noted that like people tend to gravitate
to extreme positions, it's pretty clear politics too
for a lot of reasons, but I also think there's something
about human nature where people don't like to say,
I don't know.
And that's like, it's really interesting, I think,
that we don't, we don't admit and we don't know.
People respond with certainty to things
that we have absolutely no idea about.
And I feel like that is much more of the issue
with consciousness is that we really don't know.
And I disagree with like Sir Dennis' position
because it's about, he's claiming to know.
And I think that it's actually most courageous to say,
I don't really know what's going on here.
The reason I think it's totally reasonable to say
we don't know is because look at polarizes all of us.
Like this is one of those issues where experts
can come out of completely different extremes
and there's no consensus.
And like when that's happening,
probably nobody knows what's going on.
We have not come to consensus yet.
And so I think there is something deeper going on here
that needs to be addressed and it's not simple
and certitude is not the right response.
So to me, it's just, I would say I don't really know,
but I find interesting to delve into what it is
that I don't know, like the details of what we don't know
because that's where it gets interesting.
There's lots of things we do know,
which is what tends to get rehashed
when we respond with certitude.
Like I know what I know,
but I'm more interested in what I don't know.
Well, let me just follow up there.
So this was a debate between two extremists.
One extremist saying there's no such thing as very well,
it's an illusion.
And the other one, actually a middle ground,
but at least extreme from a certainty perspective,
which is saying here's the definition.
But the reason why I gravitate towards that position
is because it at least provides us
with an operational paradigm
by which we can do exactly what you're suggesting,
which is explore what we don't know.
So if we take it as like, okay,
here's a working definition of free will.
Now let's find all the areas where it breaks down,
explore them scientifically.
It's at least useful, right?
Whereas an extreme position saying,
no, nothing is free will, it's an illusion, it's not useful.
I can't do anything with that.
I think it's quite hard to have a definition.
I was challenging one of my friends yesterday, Kenneth,
about imagine you wanted to become a billionaire,
give me an objective to optimize.
And it's really, really difficult
because you can start to scratch around
and talk about diversity and information, accumulation,
all the stuff, novelty, interestingness,
but you're really scratching around.
And it's the same thing here.
We're talking about consciousness and free will.
And these are very subjective things.
And when Keith was talking about the Dennett,
Sam Harris debate,
because it's not like free will is about,
you know, maximizing the expected reward.
Although perhaps if you created an agent to do such a thing,
it would, maybe you could tune it to behave in such a way
as humans behave.
But from my perspective, free will is about the subjectivity
and about the agency, those two things.
And I can't really describe those two things
in any more detail than that.
Yeah.
Well, one distinction that I think I want to make is that
I would separate in my kind of like questioning
and thinking free will from consciousness.
Like I think it's two different questions for me.
I could see why you might want to combine them,
but I just think they're different.
I've thought much more about consciousness than free will.
So on that, I think I'm not,
I haven't really thought through
how to address free will very well.
But my consciousness, I think, to me,
it's about the issue that's really problematic
is when it comes to like quality and things like that.
Like there are other aspects of consciousness
we might talk about that I think are less problematic,
but that is very mysterious and I feel unresolved.
But in case we're making a general point here
about these kinds of discussions,
and yeah, I think the general points
that he's making are reasonable.
We have David Chalmers coming on the show next month.
Oh, I was gonna say, did you read his book?
I mean, that's like, no, that was a book.
I really liked that book
because it is about not knowing.
The book is basically trying to tell,
that's how I interpret it.
I'm no philosopher,
so maybe I don't even understand what I'm reading,
but my interpretation was basically a big argument
about why we should admit
that we really don't know what's going on.
There's very few books like that.
I love a good book about not knowing things.
I don't know if you guys know that,
like one of the very first neuroevolution experiments,
because I was in the field of neuroevolution,
was Chalmers.
He actually did it long before all the stuff he's famous for.
Wow.
He re-evolved the rules of back propagation.
Amazing.
Excited him many times for that.
Amazing.
He comes up absolutely everywhere.
He's such an interesting guy.
I wanted to talk a little bit about some of that stuff,
because there's not a lot of new work
in your space at the moment.
And I suppose like one way to frame the question
is clearly poet and enhanced poet are fascinating.
And they are much more divergent
than many other algorithms.
But are you aware of anything
which has been artificially created,
which is extremely divergent?
I mean, more so than poet even,
or is that currently the state of the art?
Mm-hmm.
Well, there are still things going on.
Like one place to look is under the name quality diversity.
Like you can find a website.
It's called, I think it's called quality diversity optimization.
Unfortunately, the word optimization,
I wouldn't have put in there,
but that's what they're calling it.
And that's basically about,
QD algorithms are basically about like novelty,
like novelty seeking things combined
with the notion of quality.
And so you can see like the latest there.
Like those are divergent at some level,
like almost every paper that there's like 150,
I think the last time I looked.
But the thing about it is it is not yet connected
to the mainstream of machine learning,
which is why you're not hearing about it
or noticing it as much.
That's always been a problem historically.
Going back to neuro evolution,
it's often evolutionary.
But there are some,
there is some drift out of that, I think recently.
Like people have sent me preprints.
I think something's gonna come out soon, for example,
trying to build on like the idea of poet.
And like that are much more kind of machine learning,
reinforcement learning oriented.
And so there will be things.
There's a trickle coming out in that direction.
And then, and now we're seeing like,
there's open-ended learning symposium workshops,
like popping up at mainstream conferences.
I know I'm speaking at, I guess,
I clear, I think there's gonna be a workshop
on open-endedness.
And so there's definitely some momentum.
I think it's still early and could fizzle out,
but you're seeing stuff.
But in any case, to the question,
do we actually see something more open-ended than poet?
I think the answer is no currently.
I'm not aware of everything going on,
but that's a pretty,
like maybe like there's some things
that improve on it in one way or another,
but I wouldn't call it more open-ended.
No, I think that that's a pretty high bar.
Like, and it's, there is, there is headroom, I think,
to be more open-ended than poet,
but it's a high bar, it's really hard.
And I think when people, when people see poet,
they focus, from machine learning perspective,
they focus more on the curriculum learning aspect of it.
Like the curriculum learning aspect
is like a certain perspective you could have.
And you could think about it as like,
how do we get something really intelligent
for a certain kind of problem that has generality?
Like I think about something like that.
And this is like, give some clues in that direction.
But that's not really going towards the part
that like inspires me to the open-endedness side of it.
You know, which what I really want to see
is that it just continues to invent
like totally out of the blue crazy stuff forever.
And that like seems to get less mind share,
like that kind of question.
Maybe because it's not very practical
that nobody's really sure what we're even talking about.
Like what crazy things do you actually want to see?
But I would, that's the kind of thing
where I don't think we're seeing a lot of push or progress.
And the curriculum learning, I do think we see things
and they are interesting within that context.
Yeah, the curriculum learning thing fascinates me
because I remember talking at thing ICML 2019,
and he was saying, look on poet,
there's an example of an agent and we have this curriculum.
And sometimes we need to kind of shift
between a very kind of complex environment
and then back to a simple environment
in order to solve this particular problem.
But you spoke about generality.
And I would still argue that the kind of program
learned by poet doesn't have generality,
but the process which produced it does.
So Francois Chollet has this measure
of intelligence conception.
And he has this idea of intelligence being a process.
So there's like a meta-learning process.
And then it can produce skill programs
which can then work in any particular situation.
So I mean, is that similar to your mindset?
So I agree that it doesn't have generality.
That's totally true.
It's a totally about hyper-specialization.
This gets to actually the art aspect,
the art discussion we were having before.
To me, that is artistically appealing
because it's evocative of nature
where like you're not going for a super generalist in general.
Each niche is basically like a hyper-specialized niche
which interestingly eventually led to extreme generality,
like us.
Like we have an extreme level of generality in certain ways.
Like in certain ways, not photosynthetics.
We don't have that kind of generality,
but we have intelligent generality,
but it went through hyper-specialization.
If you go back through the ancestry,
you're looking at hyper-specialists,
not generalists that are trying to become
more and more intelligent.
Like you're looking at things like flatworms,
not generalists in any sense.
Like it's a new reorganization of the body plan.
And so I find, first of all,
just from an artistic perspective,
find the depiction of something that is about
like continually branching
and having just interesting aesthetically
and something that we should create.
Like we should create things that do that.
But what I notice is that always people point to that
as a weakness and say,
well, there's a caveat here.
It's very specialist-oriented.
You know, why not go for generality?
And actually you could.
This is like a fairly intuitive notion
that like, yeah, we can get divergent curricula,
but try to focus it back down to a centralized point
where we're trying to get generality.
I mean, I'm not gonna give it exact way you can do that,
but this is like an intuitive concept, I think,
to think about doing that.
But the point I wanna make though is that,
look, like some really great kinds of generality,
the stepping stones are through specialists.
What are we gonna do about that?
Like especially like us,
like you could claim that like,
we're just gonna go straight to hyper-generalization.
Like that's where we're trying to get our super generalists
or something like that
by getting more and more and more and more general.
Maybe you're right, like deep learning is magic.
Like we just add more data.
It's not that simple though,
because the fact that we're admitting
we need a curriculum means we don't have the data,
so we have to get it somehow.
But you also have to just, there's something interesting.
You have to admit there's something interesting
about when hyper-specialization actually leads to generalization
and this kind of paradoxical stepping stone principle,
that the things that don't resemble what you want
ultimately are the stepping stones that get you to it.
And hyper-specialization is like a really powerful thing,
because it allows you to make assumptions.
Like you can assume something about the environment
you're entering before you entered
because you're a specialist in that environment.
And I think that it can be a disability or a liability.
Like if you actually go into environments,
having no assumptions whatsoever,
so you have to be ready for all possible contingencies
under the sun, that's what generalization means
in a super general sense.
Like would you want airline pilots to not be sure
whether they're flying a stunt jet or a passenger jet?
Like they've got to do some checks up front
to see which scenario they're in.
Like if you're just a passenger pilot,
you don't do those checks.
Like you know what you're doing.
And so I think there's reason to talk more
about this issue of like the specialization of poet
is actually an interesting facet of it,
and not necessarily just like a liability
that we have to get around.
I've just thought of an interesting connection
that hadn't occurred to me before,
but there is a link between specialization and divergence.
Because if you think about it, a general agent,
that's the equivalent of the committee
that you hate so much.
And what you were just saying with evolution,
starting with specialization actually allows you
to explore many, many more interesting stepping stones.
But the thing I want to get to you though
is intelligence must be specialized.
Certainly even in conceptions like AIXI,
it's framed in terms of being able to perform tasks
in certain environments.
There's no such thing as general intelligence.
So if you were an alien being
and you came down to planet Earth,
would you really see that much of a difference
between our kind of intelligence and photosynthesis?
So that is really interesting.
It clearly originates from specialization.
Like I don't think you can deny that,
that like the explanatory apparatus
are through specialization.
Why is it what it is?
Like it's related to the environment we're in.
I mean, that must be true, obviously.
So it has to do with optimizing within that environment.
But I feel like what's going on is like something to do with,
like from that specialization
has emerged real generality.
Like I feel like our intelligence is sufficiently general
to move outside of anything in our environment at all.
It gets to this question like where people sometimes say
like there are certain things we cannot understand.
Like it's impossible, like as human beings.
It's usually like, well, why can't we,
why would there be things we can't understand?
Well, it's like they're just so far outside of our environment.
Like they have nothing to do with anything in the experience.
I think I don't really fall into that, Keeb.
I think we have the capacity to understand literally anything.
Well, given enough information,
like obviously we can't know about things
that we can't actually like observe at all.
So we don't know those things.
But like if I was given information,
I don't believe I could understand the concept.
Like I could understand where did the universe come from?
If you told me what happened,
like I think there is some,
it has nothing to do with the kind of situation I come from.
But I think I have the capacity to generality.
So I think it's really interesting
that like somehow a degree of generality
emerged from this specialization, which goes beyond
just being good in this environment.
Could I distinguish though,
because Jeff Hawkins made this point in his book as well,
that what's interesting about humans
is for the first time knowledge
and genes have been separated.
So I completely appreciate what you're saying,
that we can understand the universe and everything in it.
But our behavioral intelligence
is still very much tied to our environment.
I don't know whether you're familiar with James Lovelock
and his Gaia theory.
And he essentially thinks of all life on the planet
as being kind of like an ecosystem or a meta ecosystem.
So could you think of humans
as just being a product of our environment in that same way?
Is our intelligence limited by the environment we're in?
I mean, we are,
our intelligence is,
it has like some areas where it's more elastic than others,
I guess, because of our environment.
I think that would be true.
Like there's some things that are easier to grok
than other things,
because the things that are easier to grok
are more aligned with where we come from.
And so that's like what conceptually we're adapted for.
Like things like the difference
between the third dimension and the fourth dimension.
Like it's easy to reason in three dimensions.
It's quite hard to reason in four dimensions.
So we just aren't really adapted to that.
Does it mean that I can't understand?
I don't think it means I can't understand.
But it's just not as flexible.
It's not as elastic.
And so I guess it's going somewhere in the middle.
Maybe just a little bit of Keith's going to the middle.
I don't, like, yeah, I don't think either extreme,
like we were completely specialized.
I don't think so.
We're like absolutely like generalist
in the most like flexible sense.
No, we're somewhere in the middle.
But I think that the toolbox we have is sufficient,
I do believe, to ultimately capture anything.
I do think we could do that.
Yeah, so I would argue it's probably an open question.
I think this may be a case where
maybe you're being too certain
because when you talk about like these dimensionality things,
you know, I know there are many mathematical structures
that exhibit very different,
you know, fundamentally different behavior
and say five dimensions versus six dimensions versus eight.
And I think, sure, people have figured that out.
And we did that by externalizing that intelligence,
writing down symbolics, doing a bunch of equations.
But I think it would be fair to say
that no human being has ever claimed
that they could grok that in their mind.
Like they can do it by virtue of this externalized
intelligence, but to really hold it in their brain
and kind of intuit over it, you know,
my guess would be there probably are limitations
to what we can do.
And that's one of the things that excites me
about the potential for HEI.
I don't know if we're ever gonna get to it,
but if we ever do get to it,
it would be really interesting to see
what it's capable of, unshackled by the fact that it,
you know, that we evolved in a three plus one
dimensional environment, you know, to survive.
Yeah, good points.
Those are really good points.
That's interesting to think about.
Yeah, you're right.
I hadn't thought of that angle about the AI
as its potential to break out of the box.
And that is an interesting thing about AGI.
So yeah, okay, point taken.
There may be, yeah, so I am saying something extreme
if I claim that we can understand everything.
That is extreme.
I guess I'll still stick with my claim,
but it might take a lot of effort, I guess.
The way I see it, and I think you may be right about this,
at least from this perspective,
is that it may well be the case that something like,
you know, second order logic or category theory
or these sorts of logics that we've already discovered,
it may turn out to be the case
that they're mathematically sufficient
to describe any conceivable phenomenon
that we'll observe in this universe.
And so I guess I would say you could be right
that our languages and our methods that we developed
kind of externalize from us
and something that we participate in
have reached this kind of ultimate level of generality.
I just think it's a little bit beyond
what a single human mind,
at least at this phase of our evolution,
can comprehend.
That's interesting,
because that actually starts to go into this issue
of what it means to understand
and this debate about do these AIs really understand
and yeah, there's like different kind of levels of that.
So it's true that when I say like,
we can understand everything,
it's a little unclear what I mean by understand.
Like does it mean just apply the right logical language
to describe the phenomenon,
even though we don't really get that like flash feeling
of like, wow, I really get it.
Maybe they're right, maybe that is out of reach.
Keith, why don't you just go and look up
our definition of understanding?
I can give you the definition of reasoning
that we came up with,
which is the ability to derive new knowledge
from existing knowledge and experience.
But this reasoning thing is a big thing.
Do you know when we say neural networks don't reason?
A lot of it has to do with this notion of extrapolation
and people talk about the very geometric notion
of extrapolation,
but we're talking about being able to execute a function
in some logical, discrete space.
So to be able to take something we know
and extrapolate it into a new situation,
spoken like Gary Marcus,
and I'm sure you've had this conversation
with Gary many times.
Yeah, Gary has very strong feelings
about understanding this, true.
Yeah, very interesting feelings about that.
So it just took me a bit to go look up
because we did have some episode
where we were really getting into
defining some of these concepts with Gary.
Right, and at least here's how I define these.
So maybe we get your take on, it could be fun.
So it's that reasoning is the act of deriving new knowledge
from prior knowledge, plus new information,
semantics, a mapping from structures,
whether mathematical, logical, symbolic,
or other structures to physical reality,
and understanding we had as the act of deriving
new semantic mappings from prior semantics,
plus new knowledge.
Bit esoteric, but that's how we defined it.
So understanding is really was the ability to like,
okay, if I have a world model
that understands something about physics,
that like gravity exists and balls roll and whatever,
and somebody gives me some new knowledge,
which says, hey, this ball is actually hollow.
From kind of my understanding of physics,
I can now, and I just use the word,
because I understand this semantic model
and you give me this new knowledge,
I can now derive new semantics.
I can say, well, the ball is gonna behave now in this way.
In other words, I have a new mapping
to this physical world because I've gained new knowledge.
That's kind of the way in which we perceived understanding.
So with like, this was in the context of natural language,
you know, processing, let's say,
or systems that do that, GPT-3 or whatever,
because it doesn't have this semantic model of the world,
if you say something like the beer fell off the table,
it may not be able to derive that now the floor is wet
and somebody might slip if they fall,
that that requires this kind of extra level of understanding.
Yeah, and that's filling in the gaps as well.
So a lot of NLU people say that the one thing
neural networks can't do is extrapolate
over the missing information.
And that's a great example.
So you could reason that we've just knocked
the beer off the table and now the floor is wet.
So there's a kind of exponential space
of missing information that we need reasoning to fill in.
Okay, I have to admit that I,
it's probably disappointing you
because I'm not, I don't really like definitions.
I just never find definition discussions engaging
or really helpful to me.
I really, again, I often find that appeal to definition
is often just a way of escaping an uncomfortable situation.
And I want to go towards the uncomfortable situation.
So it's like, we often will say,
well, no one's really clearly defined consciousness.
First, before I'm going to discuss it,
you need to define it to my satisfaction.
Well, okay, we're obviously not going to discuss it then.
Like, I'll never satisfy you.
Right, right.
And so it's often in the AI,
decades of discussion, what is intelligence?
And we see it in open-endedness,
we start having a problem like in this small field,
like there's open-endedness workshops that come
and like half the papers were just like,
what is long pages and pages of definition in terms.
It's like, well, are we going to ever do anything?
Or just we're going to argue about this for the next decade.
Like what are we even talking about
is what we're going to talk about.
And I feel like a lot of this is not necessary.
I feel like I can talk about consciousness.
I can't define it for you to your satisfaction.
I can talk about intelligence.
I can make progress on intelligence.
I can talk about open-endedness.
I don't care really what the definition is.
It's just you're going to use it to stop me from talking.
So the only thing that I'll defend there is that,
I mean, again, I'm a pragmatist.
I believe in defining things
to the extent necessary to communicate.
And so we have to have,
it's kind of like going back to the whole,
hyper-specialization leading to innovation.
If you just have a divergent thing,
I think I made this point in our first video,
you're just going to end up with a universe of gray goo.
Like the really fascinating thing is that
because there are these constraints
of you need to survive in order to pass on your information,
you wind up with this kind of beautiful tapestry
of hyper-specialized things that recombine
to become more general.
And it's far more interesting than either extreme,
like either the gray goo or like the nothing, you know?
And so as far as definitions go,
I think that I don't like to sit there
and pedantically argue forever
about what the definition of intelligence is,
but we need to have enough of a definition
that we can make progress in learning
and kind of doing scientific discoveries.
So things like the beer falls off the table and it's wet.
Well, if a system can't figure that out,
we notice that it can't figure out kind of a class of things.
And that class of things has something in common.
Then we give it a name and maybe it's understanding
or whatever, I don't care.
But it's just a way of talking about that class of problems
of things that it's not able to achieve
because then we can try and figure out how to do that.
Yeah, yeah.
I think that the symbolists though,
that when they come up with formal arguments,
it's not, that doesn't come first.
They notice that neural networks can't do something,
which is to say they can't fill in the missing gaps.
And then they come up with a formalism
to express why that is.
And also many of these symbolists believe
that there are kind of platonic abstractions
that exist in the universe.
They think that mathematics is discovered, not invented.
So that kind of formal apparatus
is how they understand the world.
Well, I feel like I should try to solidify my attack
on definitions since it's obviously fairly,
again, radical thing to say.
I would acknowledge, like in the sense that Keith is saying
that again, like I don't want to be a prank.
Like obviously you need to define some terms sometimes.
Like that's obviously clear.
That should be completely clear.
I'm not against that, like, you know,
especially like you're gonna derive something
and you're writing a paper
or you have a certain set of assumptions,
you need to know what they are,
like in order to prove that it actually is true
and not true, if that's what you're trying to do,
like that makes total sense to me.
So I'm not a blanket saying we shouldn't have definitions.
But I think the thing about definitions that's interesting,
like a lot of things is that there just isn't solid ground,
like in terms of like, they're just generally good for you.
Like they can be good for you or they can be bad for you.
They can be a tool of clarification
or they can be a tool of obfuscation.
And it depends how you use them.
And I often find them to be tools of obfuscation.
Like especially when we're talking about things
we don't know, which is once again the problem,
which is what I'm interested in.
I want to talk about things that we don't know.
And that seems where people get really passionate
about definitions.
So it's like, what does it mean to understand?
We don't know, I don't know.
What I do know though, I'm confident
there's such a thing.
There is understanding, it might be a continuum,
maybe it's not just a binary concept.
Beyond that, I don't really know what it means.
And so I would be interested to talk in depth,
like what does this really mean?
Like let's look at this.
But no, we have to like, we just end up
in this like a big argument about the definition.
And I find that in that case it's obfuscation
because it's fear.
Because really, like if your whole pitches,
like your thing that you get a lot of traction on
is basically attacking the fact
that things don't understand,
then you might be a little uncomfortable
if we really start dissecting what you mean.
And so you should just comment us and just tell us like,
hey, you're not even being clear in your terms here.
And just stop the argument in its tracks.
And that's the kind of definition I don't like.
I mean, I'd rather just like, look, we agree there's,
I don't know exactly what it is,
you don't know exactly what it is.
Let's talk about it anyway, it's uncomfortable.
But there is such a thing, that's what we should agree on.
Do we agree there's understanding?
Like, does understanding really exist?
Well, see, that's the problem is,
right, but that's the problem is you can run into fun,
some folks that will go that extreme and say,
there's no such thing as intelligence.
It's all just, yeah, but look,
I agree with you in principle, which is again,
I like definitions insofar as they're necessary
to enable communication.
So I'm totally on board with the idea
that we need to be talking about things.
And that's why I kind of like say
the coherence theory of knowledge
because it acknowledges, look,
we're never gonna get to the foundation beyond
which there's no other foundation that we can imagine.
We just need to understand far enough
and define things far enough that we can make progress.
So I'm a little bit,
I can completely understand where you're coming from, Kenneth.
You think that we have a fundamental fear of the unknown
and we're hiding behind our formalisms.
And my only worry is that it seems
a little bit anti-intellectual
because you can make an observation
that an AI model is not behaving the way we are.
There's loads of assumptions there.
We assume that we are behaving intelligently
and we are doing things the right way.
But then you can say,
oh, let's resist any formalism
to try and break this down analytically.
Do you see the conflict there?
Yeah, absolutely.
I mean, it's a dangerous position
because it's clear that some formalism is necessary.
Like I tried to concede, I mean, walking a tightrope.
So it's not, yeah, it can't be,
I can't make this blanket claim
that formalism needs to be completely thrown out the window,
that that would just destroy my credibility.
But the point is, these are just pendulum,
these are pendulum, what is the plural?
Pendulums, these are pendulum,
they swing in different directions.
So we become so enamored with this,
which is a useful tool,
but to the point where it actually becomes
like a form of obfuscation.
And I do believe definition is like that.
Like definition, it's not always and it's not everybody,
but like a lot of the time when like,
there's an uncomfortable issue,
like we immediately jump to definition to obfuscate.
I think consciousness is one of the greatest examples of that.
It's like, there's clearly a mystery here.
And I agree, there are a few people
who would disagree, there's any mystery at all.
But to me, it's clear there's a mystery.
Don't need to have a definition to know there's a mystery.
We could get into why it's mysterious.
That's more interesting to me than what the definition is.
But if you jump at me with this definition stuff,
you're going to stop me from getting into that.
I feel like that's pure obfuscation.
This is one of the greatest unknown things
in the entire scientific world, like consciousness.
It's one of the greatest mysteries of all time.
And like, so arguing about definitions,
is that really where we're going to go with this?
Like we don't need,
maybe we don't have a good definition yet
because we don't even know what we're talking about.
That's part of why it's so mysterious.
I think this is quite interesting though,
because with consciousness,
we clearly don't have a mental apparatus.
You know, like for example,
if someone just took some hallucinogenics
and they just had a completely crazy visual experience,
they wouldn't have any words
or any mental framework to hang this off.
Whereas when we're talking about an apparatus
to describe intelligent behavior,
we absolutely do have an apparatus to hang things off.
So I guess, is that a spectrum in your mind?
Yeah, there is a spectrum.
I would also concede that.
And with consciousness, yeah, it's true
that we lack it, it's much worse for us, it's true.
Because it's ineffable,
which is another way of saying, you can't put it into words.
Like all the things we're discussing have no words.
Like the blueness of blue.
Like I can't actually describe it.
You can't break it down into parts or say what it is.
And so that makes it extremely difficult
like to actually get into anything formal about it.
Whereas intelligence, I would grant that you can,
to some extent, like you can actually point to things
that can be reduced to words or symbols or formalism.
And so it's a little bit better on that slippery slope.
It's higher up and like less dangerous.
But still, I think it's still on a slippery slope.
Like there's, I don't think we,
although we can talk about intelligence
more easily than consciousness,
I still don't think we really fully grasp this either
or really have the words to really get it
what we're talking about.
We don't understand ourselves well enough
to understand what we're talking about.
And so it's, there's still a little bit of this room
for obfuscation, where we fail to address the mystery itself
by just deciding to focus on the definition.
Yeah, I mean, I think if we did understand it
or had a clear definition,
we wouldn't be having so many interesting papers
coming out like on the measure of intelligence.
And that was the thing that I liked about Chalet's paper is,
hey, as long as it's an operationally useful measure,
then that helps us.
It turns out, while it's kind of a nice framework
to think about things, there isn't yet a measure of it.
You know, kind of working on that, he's working on that.
But again, I love the idea that we just,
you know, the goal here, like you said earlier,
is really to talk about things.
You know, it's to communicate, it's to learn,
it's to make progress, it's to explore,
and to some degree, to make life better, to exploit.
But it's all about just doing almost the minimal necessary
to enable communication and exploration, I think.
Yeah, yeah, yeah.
So I totally agree.
Like the ultimate point is to explore.
And yeah, I just like to go to places
that are ambiguous, like on purpose for center.
I feel like that's what we're supposed to do.
Like if we're talking about science or art,
those are like the really interesting, uncomfortable places
where you're gonna learn something.
It's almost like how in physics,
physicists always wanna go to where two great theories
collide and nobody knows what happens.
Like what happens at the surface of a black hole
where quantum mechanics and general relativity collide.
There's a lot of unknown and ambiguity there.
That's where the real progress is gonna come from.
You're sitting right on the boundary of chaos and order.
You're trying to straddle that straddle line.
Exactly, exactly.
Well, Professor Kenneth Stanley, it's always an honor.
Thank you so much for joining us.
Thank you, that was super fun.
Thank you.
Thank you so much.
Amazing.
