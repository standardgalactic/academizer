Welcome back to Machine Learning Street Talk. I'm your host, Tim Scarf.
Now, today on the show, we're joined by Dr. Walid Saber, who's just written a review of the book,
Machines Will Never Rule the World. Artificial Intelligence Without Fear, written by Jobs
Landgreb and Barry Smith. Now, Dr. Saber will be discussing his review, which provides a detailed
analysis of the book's arguments that strong AI is impossible. In his review, Dr. Saber acknowledges
the argument made by Landgreb and Smith that anything we engineer is ultimately a system which
can be mathematically modeled and described. He then goes on to discuss the complexity of
modeling mental processes, which the authors argue are dynamic, adaptive, continuously evolving,
and constitutes systems whose behavior affects and is affected by the environment they function in.
He also touches on the notion of granularity, arguing that complex systems are all the way up
from specific components of the mind to the mind itself, and that no known mathematics can model them.
Dr. Saber then delves into the complexities of language and open interactive dialogues,
asserting that language is a prerequisite for any artificial general intelligence,
but that linguistic communication itself is a complex system that no mathematics can model.
He doesn't subscribe to the argument that interactive bots can be built in narrow domains,
since responses and the overall context cannot be predicted in any meaningful way.
Dr. Saber has two reservations, as to the conclusions made by Landgreb and Smith. He
questions their use of the word never and suggests there could be a new mathematics that mental
processes require that is yet to be discovered. He also doesn't believe that the fact that
complex behavior cannot be mathematically modeled precludes the possibility of building such systems
as evidenced by the intentional programming language LISP, and he also considers the possibility
of hypercomputation in validating the church-turing hypothesis. We'll talk about that a bit later.
Finally, Dr. Saber expresses his regrets that the book didn't go into further detail
on the frame problem in AI and calls for further research into belief revision in complex systems.
Join us today as we speak with Dr. Wallid Saber about his review of this book that we've just
been speaking about, Machines Will Never Rule the World. And by the way, I think very highly of
Wallid. I think he is one of our most loved guests. He is a polymath. He has an incredible
breadth of knowledge across so many fields, you know, from AI and computer science to mathematics
to philosophy to linguistics. He really is a rare breed and he also brings a very interesting
contrarian view, I would say, to the current kind of modus operandi or the zeitgeist in the
community at the moment. He's a breath of fresh air. Anyway, if you haven't already, consider
subscribing to our YouTube channel or indeed rating our podcast on your favorite podcasting
platform if you happen to be listening to us. So anyway, without any further delay, I give you
Dr. Wallid Saber. Welcome back to MLST, folks. We have the unmistakable Wallid Saber,
the legend that is Wallid Saber. But we also have Mark from our Discord community. And Mark,
would you like to introduce yourself? Hi, guys. I'm Mark McGwill, philosopher, cognitive scientist,
and software engineer at Big MLST. Awesome. Welcome, Mark. You know, things have been a bit
of a blur, but I think this is right, Tim. Like, I think Wallid was, you know, the first or one of
the first really big names that we had come on the show, right? I don't think so, but the most
controversial, let's put it this way, the one that made probably it was so predictable, what was, I
mean, probably the first one that broke that predictive model. Well, I just remember, I mean,
I remember Tim and I being like, super, because we didn't know you, right, before then. And I
remember us being like, so excited that you agreed to come on the show. You know, of course,
now that we know you, we're kind of like, ah, whatever, it's just Wallid. But we were following
all of ourselves that you were coming on the show, we're like, oh my God, this is so awesome.
To be honest with you, I never thought I would, I would, yeah, I never thought I would have,
my opinion would matter that much, to be honest with you. And it all started by creating this
medium blog. And I started spitting out stuff that, hey, do you guys know that there's this and
this and that? And I was surprised how much it, even by people that are
living off and making a living and they preach and write papers on what I'm
attacking. And they would say, don't mention my name, by the way, it's all private messages.
But apparently, I said a couple of things that touched people, but you know.
Yeah. Yeah, I was at New York's last week and the amount of people that came up to me and said,
I love the show, Tim, Wallid's my favorite guest that you've had on. Because Wallid just provides
a completely different perspective. Because we're bred on empiricism and neural networks. And
part of the reason I want to get you back on, Wallid, is to counteract some of the,
I mean, we've been speaking to a lot of deep learning people recently. So we need to counteract
that a little bit. Yeah, that one shocked the hell out of me. I mean, I have people like myself
at this event. I mean, if you said ACL maybe, yeah, okay. But I mean, this is the deep learning
meeting, right? I mean, so I was shocked. But yeah. Yeah, indeed, indeed. Although I have
some, some, some more positive views of DL now. Oh, go on. Yeah, I mean, look, I breaking news,
breaking news. Positive, although I still have my reservations as to AGI and all that stuff. But
I have been completely impressed with the developments in large language models. I have to
admit that sometimes I say, what the hell is this? Now, technically, let me tell you what's
happening. And I'm working on something that probably would quantify where can this go? How
much can you, how much will scale? The bottom line is this, these guys have impressed the hell
out of me and they have proven that scale does matter. I mean, now these large language models,
if you take language from lexical to to syntactic to semantic to pragmatic levels,
they have definitely mastered syntax. And this is not a small feat. I mean, this is huge.
They have proven that if I read tons of text written by humans, I can figure out the grammar
of language. And they have done that. That's huge. Okay. So, and I don't like here where I don't like
people, I don't want to mention names, but people that supposedly are in my camp, right,
insisting on refusing to see the elephant in the room. No. Large language models have proven
that if I ingest terabytes of text, I will figure out syntactic rules. They have done that.
Now. Okay. And here's where technically, and you have to admit, I mean, they,
as a matter of fact, they probably know syntax now more than many college graduates.
Okay. These are from data. That's a huge experiment in cognitive science that
no matter how, how, I mean, you can't be religious in this. You have to be scientific.
I see the proof that these large language models by ingesting tons of text written by humans,
they have figured out the syntax of language. End of story. I have, there's an existential proof.
Go on open AI, try Da Vinci to order in G3 or whatever you like to try.
There's syntactic competency is beyond belief. I'm shocked every time I use it. Okay.
Now that's not the end of language understanding. There's semantics and then there's pragmatics. Wow.
So I'm working on something to quantify. So now we have, I don't know, we're up to a trillion
parameter that allowed me to master syntax. Now let's see semantics and semantics can be broken
down to have you guys figured out reference resolution, have you guys figured out scope
resolution prepositional phrase attachments. There are so many pain points and semantic
processing. Can we quantify how many more parameters we need to conquer semantics
and then pragmatics. There are things like
the teenager shot the policeman and he immediately fled away. Now possibly both can,
the he can be the policeman. He fled away to escape further injuries. I mean that can happen.
But most likely the one that's led away is the teenager. That's pragmatics because we know in
the world we live in, if you shoot someone, they're going to try to capture you and you try to flee.
Right. That's not semantics. That's way beyond. How many parameters beyond semantics do you need
to capture that? If you can put a, if you can come up with a rough number, I mean it could be
a number that's manageable, that's doable by more scaling, which would be an interesting result.
But it could be that it's a number beyond the universe we live in, which means guys
accept that you can master syntax and a little bit of lexical semantics. You can figure out the
meaning of some words, but to do full understanding with pragmatics. We're talking about numbers that
we might have to wait 2000 years. Yes, in theory it works. So basically I'm trying to work now on
which is going to be very difficult to quantify because they have proven that scale did improve
syntax. No doubt, not improve it. They've almost mastered syntax. But how far can this go? I mean
can you quantify how far can this go scientifically without saying let's try with more, let's try with
more, which is not, it's not going to be easy to do. Anyway. So I'm curious, can I push back a little
bit on the syntax? You said if they've mastered syntax and I'm kind of okay, I mean I guess you
have an existing proof and there's like a behavioral kind of proof that it doesn't
have very syntactic sentences. And obviously if you're in something with whatever billion of
parameters, but would you say what those rules are? Could we write them down? Probably not,
right? Because it's a billion different numbers of weights. No, no. So you don't have the old
tool kind of generative grammar approach. And also it's not the way humans learn language.
And could you reverse engineer or tweak it? We don't know what it is. It's a black box.
So okay, there's a behavioral tense in which it knows. But isn't it the way humans do learn
language? I mean, I think it's more, it's more related to the way humans learn language than,
I mean, I was 20 until I knew grammar. I mean, we use language without without knowing
grammatical rules. So there is an argument that humans don't learn grammar. That it's,
you know, pretty native. And all we do is tweak a few parameters. And then we add vocabulary over
to years, tweaks or whatever. That's the, you know, the Chomsky and generative grammar approach.
And so I don't think that we have a billion parameters that we tweak over 20 years. I
don't think that's how we work. And we have, I mean, there's amazing competency of
children at two years of age, you know, with language they have, you've said, you've,
you're writing, you pointed this out, actually, children know is absolutely amazing, you know,
straight out of, you know, it is amazing, but it is amazing. And I, and I didn't change the way I
think about it. And we have innate stuff. But here's the change that these guys have
made me go through. It's a minor change, but it's, it's, it's not that subtle, actually. Here's the
thing. I was told before these new results are coming out that, look, we do have innate stuff,
which took us three, 400,000 years of evolution. All we're doing by ingesting all this text is
we're simulating, right, these 300,000 years. So give us a chance to simulate this innateness,
if you want, in a way, in a way. Okay, I, that argument was said long time ago, and I thought,
come on, you're chasing infinite. What happened with the real difference in my mind now is,
they have proven that they conquered one beast in language. Nobody can dispute that.
Can I have a go at disputing it? So in the Polition and Foda Connectionism critique,
they spoke about productivity, you know, the infinite cardinality of language. There was
recently a deep mind paper talking about the Chomsky hierarchy, and deep neural network,
I mean, RNNs are regular languages, but you know, I think transformers and the rest of them are
at the bottom of the hierarchy. So quantitatively, we know we haven't conquered infinity. So why,
with such a shallow horizon, are they doing so well? I agree. Here's, here's the thing.
Language use, language is infinite. But probably the long tail of, probably 90% of ordinary language
use, right, can be figured out from the stuff that we write. So they will never capture all of language.
Yes. But they might reach the level of a competent educated man like us in language competency.
So, all I'm saying is, what they have achieved is a huge
result in terms of the big question of scale and big data. They have definitely proved that
if I see enough data, I will learn something, and something that's not trivial.
Look, you know where I'm coming from. You're talking Foda and Polition. I mean,
you're preaching to the choir, right? But I have to be a scientist too. I mean, I don't like,
I'm following Gary Marcus, and he's like, I don't like people that minimize what happened.
Like, I'm a scientist, right? I see a big result. I say, wow, right? And look, we're talking,
nobody bashed deep learning more than me, especially large language models. I mean, I'm like,
I was saying, this is silly, right? But I have to say, they have proven something to me at least,
which is huge, because I know how difficult language is. I am impressed equally.
Wouldn't you say it's an engineering, an engineering triumph rather than scientific?
It's an engineering triumph. But here's the point, Mark. I think it's a little bit more.
That's the only thing I'm trying to, I'm not saying, look, I didn't give up on, I can get to
the criticism later. So don't put me on in that camp yet, right? Or, or ever, right? Because I
know, I know theoretically, theoretically, mathematically, you cannot understand language
this way. All I'm saying is, in terms of cognitive science, what happened, and what is happening as
we speak, is not nothing. It's a huge, for example, if I can ingest a lot, again, what they proved
is, is that while the two are related, so it's one thing scale from tons and tons of data,
I can learn something that is not trivial, that to me has been proven. The point I'm making is not,
the point I'm making is not that they solve the language problem. Sorry, go ahead.
Yeah, I want to jump in here a little bit, because from my perspective, I think part of why
you're saying it's, it's huge is because I think it was a huge step for you personally.
Because I know, you know, from the past, like talking to you, like you've had a much more
extreme view, you know, on the capabilities of large language models than, for example, myself.
Because for me, I don't see anything new here. It's kind of like, I'll give you an example
outside of syntax, just for a moment. So just transcription, because that's what Tim and I
happen to be working on quite a bit right now. In other words, transcribing audio into text.
All the state-of-the-art models are pretty much sitting around each other at about 90%
you know, accuracy right of transcription. But here's the thing is that's for
people speaking relatively common languages with a relatively standard accent. Okay, as soon as you,
as soon as you bring someone in the room that has an accent or speaks a, you know,
with maybe like some type of a challenge, like a speech challenge or this side of the other thing,
it becomes garbage again. And like for Tim and I, or there's noise in the background,
music playing in the background. And as Tim and I have probably hammered, you know,
to death and beaten a dead horse on our channel like so many times, we've never doubted that
machine learning can learn like the bulk of the curve where it really, really struggles is in all
these edge cases and the corner cases and the periphery where it can easily, it's very brittle,
right, in those kind of areas. Like this is the point we've been making out for a long, long time.
And so the fact that like massive trillions of parameters and terabytes of data was able to
learn 90% or more, 95% or whatever is syntax. Okay, I get it. Like from a linguist perspective,
that was a, you know, maybe a big triumph or something, but I'm still always about that other
like 5%. And the problem with the approach of deep neural networks is to get that other 5%
is like 100 times as many more parameters, whereas like using, whereas using more generalized,
abstracted, you know, methods that we haven't yet really discovered.
You're hitting it on the nail. And that's why I'm working out on quantifying this because
now we are doing exponential growth in the number of parameters for not even linear growth in the
accuracy, even logarithmic. I agree with you 100%. So that other 10% might require 2000 years of data
that we don't even have. That's what I'm working on. How far can this go? Because the function
is against them now. Like, I mean, we're increasing GPU power and the number of data
that we're ingesting exponentially for a minute increase in accuracy, which is
that's the end logarithmic. And this is, this is part of the announcers that we have to go through.
So look, all my reservations that I had before apply. So I'm being misunderstood. All I'm saying
is simple. These guys, what they have done is not as trivial as I thought initially. Okay,
so let me, let me really be very careful in what I'm saying because now I have a following. I don't
want to lose it. No, I'm not, I'm not changing scientifically where I was. I mean, science is
science. And I know theoretically, I don't get into things like intentionality and these models
understand nothing about the word. I'm talking about syntax only, by the way, syntax only.
And some coherence, when they patch things together, the coherence is amazing. They're not
patching things together that don't relate at all. So I'm talking about syntax and coherence
and syntax, okay? And a touch of semantics, right? My point, let me repeat it so that I'm not
misunderstood. They have proven something that many cognitive scientists
would never accept, never ever. But this existential proof
has told many cognitive scientists, don't dismiss learning from data only,
blind, no labeling, some aspects of language, actually very impressive aspects of language.
These guys have proven that. And me as a cognitive scientist, I have to admit because I see it.
I see from data alone, these systems have learned non-trivial aspects of language.
Now, how do you interpret that? Where do you take it? What do you conclude from it?
We can debate that. But all I'm saying is, I have seen something that I never thought I would see,
that just ingesting text in these deep networks, you can actually figure something non-trivial
about language. That has been done. I mean, you can say there are pigs that fly, okay?
Prove me wrong. I saw them. Prove they don't exist. Well, I can. But existential proofs are
the most powerful proofs. It's an existential proof, proof by doing. I'm showing you language
competency by ingesting text on it. So this dismissive
character that all these are, what is the phrase that Melanie uses? Not Melanie. Stochastic
parrots. No. Oh, Bender. Emily Bender. Emily Bender. No, these are not stochastic parrots
anymore for me. I am seeing, look, if I go through the tests on conducting, I have 20 pages of tests
on every aspect. And they get better. I mean, I am seeing things that
are lexical ambiguity. They've almost resolved it. We were at the baseball stadium last night.
We had a ball. They knew that ball. It's not the baseball. I'm seeing things like,
what the hell is this? And if anybody can test these systems, I can with all humility.
I'm trying my best now to make them fail, which was not the case just a month ago.
All I'm saying is I'm seeing something that I never thought I would see
as a cognitive scientist, as a computational linguist.
Let me put it this way. To see this capability now, you have to bring back Montague,
Frigli, Marvin Minsky, John McCarthy, all the pioneers of logic and AI, put them together
and give them a thousand bright engineers. And they will not do this.
In a minute, we're going to get onto your book review, but you are just alluding to the problem
of semantics and pragmatics. And also, I want to bring in symbol grounding as being the next
potential brick walls. Could you just talk to that a little bit more?
Well, look, symbol grounding was an issue in symbolic systems. You're using symbolic systems.
So you're saying CAT, CAT, it's reference based semantics. So I'm going to use CAT to refer
to a concept called CAT. And then the concept called CAT is a frame in most systems,
in frame-based systems with properties. It's a mammal. It's a thing that has this and this
kind of fair whiskers, blah, blah, blah. It's the intention of what a CAT is. And then symbol
grounding came like, okay, you're defining CAT, a symbol in terms of symbols. So where do we go?
It's like a dictionary to read the definition of a word. I have to know all the words,
so I might go and... So it's a cyclical representational system. It's not grounded in anything
in the end. It's a closed... Basically, it's a system that defines itself, like what the hell's
going on here, right? Symbol grounding was CAT has to be associated with something real outside.
That's a real CAT. In symbolic systems, we don't have that, right?
We can get into symbol grounding. It's a huge subject on its own, like where do meanings...
Where do words get their meaning from? Is it embodied? Is it experiential? Does it have to be...
Can a deaf and a blind person ever understand the meaning of something? So that's a huge...
I mean, we spoke to Andrew Lampinen and he was getting into Pierce's triad semiotics,
this embodied relativistic notion of grounding, which actually I'm developing a bit of a taste
for personally, but you're very skeptical about that. Could you just sketch that out?
I don't think that's the issue, grounding. I mean, people make a lot of it and like our common friend,
Bishop, Mark Bishop, that you will never understand the meaning of something if you don't live in the
environment and it's... That has never been... I don't believe so. That's why we call it artificial
intelligence, right? I mean, we're never going to have the intelligence of a human being. We're
never going to have a robot that really chokes when they see their nephew after six years, right?
I mean, and that's... That was never the one. That's why we're building artificial intelligence,
not human intelligence. So this whole argument about grounding and embodiment and I will never
understand what pain is because a robot will never really feel pain. To me, that's besides the point.
I'm not building artificial life. I'm building an artificially intelligent machine that will do
things in a way that you will say, what the hell was that? Probably that's how I should be defined.
That's it. What... Who did this, right? That's it. It feels pain or it doesn't feel pain or it will
never know what crying is, like so. So at least I come from this angle. I'm not into
building artificial humans. I'm an engineer. I'm into building artificial intelligence systems.
Systems can reason, right? In the environment we live in, solve problems intelligently and
problems that usually require human intelligence. Like I'm into... I would like to see a world where
we don't have accountants. Come on. We don't have doctors. I open my mobile. I have a doctor.
I converse with them intelligently and they tell me exactly what to do. Done. Nobody goes to medical
school anymore. Everybody should write poetry and play music and enjoy the beach. That's it.
That's the AI I'm interested in. We will never build robots that will understand
love and so. So to me these are arguments that
they're irrelevant. We're building artificial intelligence. When we did calculators we never
gave a damn how we do it in the mind and we have calculators that can beat
any mathematician in doing a division of two prime numbers each of which is 20 digits.
Yeah. I think it's more where your area of interest is. I mean, so yours is in the engineering.
Right. And what's coming to mind right now is our conversation with Professor Chomsky where he said,
you know, like, yeah, these are great feats of engineering. I mean, I like bulldozers too.
They just don't have anything to do with science. Right. Like so, I mean, sure, like
or philosophy, you know, for that matter. So I mean, I think some of these questions,
yeah, maybe they don't have a lot to do with building AIs that do a bunch of useful things.
But but they have a lot to do with philosophy or science or, you know, whatever mathematics,
you know, for that matter. You know, this is a bridge to the book because
because the the authors were misunderstood from their title, and I told them that privately.
No, not privately. I want to tell them that because I got comments from people privately
that the title is misleading. The title assumes they are anti AI, not really. They're saying
roughly what I'm saying. I'm not interested in building an artificial human. We can never do
that problem. Right. So all this. And this is important because people are trivializing. I mean,
you have people talking about AGI from five years ago. And all we had was something that can do
amazing pattern recognition. That's it. So it's important for us to say, Hey, guys, cool it down.
Do you know what you mean when you talk machines that surpass human intelligence?
This is not just the word you throw out, because you're impressed with
a system that can recognize cats from dogs. Come on, take it easy. Slow down, right? And this
book is about that. It's like, do you know what it means to have a system that can feel and
and react instantly real time to changing situations around them? And do you know what
you're talking about? So, so yeah, I'm interested in the engineering side of AI. And that's what
makes me impressed by something like a DaVinci 2 or 3 as an as an AI enthusiast. I look at this
and I say, Wow, we've never been able to reach this milestone. This is a huge milestone. That's
how I look at it. Will it be, will it be the solution for the language understanding problem? No,
because language understanding in the full sense of the word understanding,
the way we speak now, the way we were speaking now involves a lot more than mastering symptoms.
But, but they did master a big aspect of language. And I can see it. I can try it. And I'm trying to
make it fail now in syntax and even some coherence, some mild, let's call them mild semantics.
And it's very impressive. So the question now becomes for AI researchers, not just engineers is,
is this, is this scalability scalable? Is this scalable? Will is this approach scalable?
So much data and so much compute power. They mastered syntax, more or less, I think they did,
at least as much as a competent language user. So my first question is, like, do you see natural
language as essentially computable, as in like cheering machine computing? Or do we need some
other kind of new mathematics to describe it? For example, hyper computation, whatever that might
be. In other words, is there a generative grammar or algorithm or set of rules that
generate our valid sentences? When it comes to language itself, I think language is a formal
language. Yes, there is a, there is a compiler for natural language that can be built, like we
have built one for Java, C sharp. So this is Montague, yeah. Yeah, I believe Montague was
right, although Montague was was attacking the semantics part. Okay, he touched a little bit
on intention and then not much on pragmatics, but Montague and it took me years. And I had to be
advised by a very smart philosopher, audition that stop saying Montague didn't deal with this.
Montague was never in the business of reference resolution. That's pragmatics. Montague was trying
to prove there is a formal system and algebraic system. And he used lambda calculus, strongly
typed system that I can use to compose language like I do with arithmetic or calculus or anything.
There's a logic that a mathematics for language, which is a huge thing. Montague was not a trivial
semanticist in the history of language. He was huge.
So would you say that Montague is doing for language semantics of language, what chance
he did for syntax of language? Exactly, exactly. And the common denominator interesting between
them is someone that Chomsky himself admires a lot Barbara Partee, who was he did her PhD with
Montague. She's a Montagovian Montague semantics. But she and she she did say almost the same phrase
he said, what Montague did for semantics is equivalent to what Chomsky did for syntax.
Yes. Okay, exact, almost exact phrase. You think he was right about semantics? Yes. So for example,
there's a computable definition of what is a pile of sand. Right. No, no, no, no, no, no, no.
See, I'm like, I'm not sure you're right about that. No, no, no, no. Hold on. Let's not get Montague
was not a lexicographer or an ontologist or he said, whatever your meaning for something is.
Okay. He didn't even care. Montague never did ontology and conceptual and like, what,
how do you define the meaning? What is a book? Look, let's, and took me, I'm telling you, took
me three years to appreciate what Montague was doing. And I, and my thesis was on Montague
semantics, the masters before the PhD. Here's what Montague did, Keith, and you'll appreciate
Montague said, whatever your meaning for the individual words, the lexical meaning.
Okay. So cat means C. Okay. You go with your psychologist and, and, and, and cognitive
scientists and ontologists and disagree about the meaning of a cat. Finally, you come to me
and you say, we have a meaning for cat and it's C. Following me, Montague never cared about what is
the nature of things outside. Right. Whatever your meaning for these individual concepts are.
Right. Here's how you make, you get the meaning of a whole mathematically. I'll give you a simple
example that will make you appreciate what I'm talking about. John refers to a person. Right.
The, the neighbor next door refers to a person. The neighbor next door that just moved from
California refers to a person. The neighbor next door that knows John very well and, and drives
for the LTD is a person. All of them. How can you have this phrase and John
refer to a person and have the same semantic type composition in a way that never fails.
Like you do in arithmetic, he wanted to prove that natural language is a formal language.
He developed a semantic algebra that makes this long phrase refer to the, in the end,
to an object that has the same semantic type as John mathematically. If you do it, it never fails.
The details of this were genius. Okay. So Montague then made the big claim
natural language is a formal language. Give me some time. I'll work out the full algebra.
You go then and decide what the individual meanings are. I don't care.
Montague never gave a damn about cognitive science and, and knowledge and, and he was a logician.
He wanted to prove there's a calculus underneath naturally calculus of meanings.
You decide on the meaning. I'm telling you, it took me a while. I thought he's doing semantics.
What is the meaning of this in Montague? He said he never cared. He was doing an algebra of meanings
regardless of what the meanings are. Okay. So, but, but his project was
huge. Montague was trying to prove there's a, there's a, there's an algebraic system behind
language, like any other formal language, like you can get an arithmetic expression and build
a tree for that, evaluate it and get the final meaning. Natural language works the same way,
except it's not that simple. That's all. So his project was huge and he wasn't misunderstood.
So he was really doing semantics, right? That's semantics. Pragmatics is a different thing.
What do you think is the, the core unique property of natural human language?
Would you agree with Chomsky on it being digital infinity?
Yeah. Okay. The infinite thing in the productivity.
To me, no, it's, I'm half Chomsky and half something else. To me, no, the real,
real unique thing about language. And that's why even if Montague succeeded, that's half the battle.
It's not in the semantics, although that's huge.
To me, it's the pragmatic side, the abductive inference. I mean, we, we use induction and
we use deduction and we always ignore abduction. Abduction is the unique, is the
humanly unique reasoning capability. I mean, rats do inductive reason. They, they,
to a certain extent, that's how they learn a few things inductively, really.
All the lower species do inductive reasoning to a certain extent.
And some of them do some deductive reasoning, if this then this, but at a very shallow level,
of course, abductive reasoning is uniquely human. And that's the part of language understanding.
Right. Which means reasoning to the best explanation. Abductive reasoning is I reach a
conclusion, not inductively by induction or, and not deductively, I deduced it. But I reached
this conclusion because it's possible. It can happen. And it is the best conclusion I can come
up with, given everything else I know. Abductive reasoning is the real reasoning methodology that
makes us unique as human. We reason to the best, we reason, it's called reasoning to the best
explanation. Right. So that's, that's Pierce and others. I mean, Pierce was the pioneer of
abductive reasoning or abduction. But I'm talking about an abduction has come to have two sort of
tracks. And there's abductive reasoning in the, in the traditional philosophical charts, Pierce.
But there's abductive reasoning as it used to be called in the 80s, when case-based reasoning came
out and expert systems who, there was something called EBL, explanation-based learning. And it
was even a learning technique, which is really reasoning to the best explanation. Basically,
I have to make a decision. Actually, Jerry Hobbs, you guys heard me mention his name several times
before, who's, I think, huge in semantics, has a paper when he was at SRI with other
luminaries too. The title is interpretation as abduction or understanding as abduction.
And basically he shows how all the difficult, all the challenges in language understanding
beyond semantics. So we're done with Montague. Now I'm doing the final understanding of what makes
sense given because every expression has several meanings. Even if I did the semantics perfectly,
I have to choose the most plausible meaning from all the possible meanings. That's pragmatics.
And the way you do that very well is in language. We do abductive reasoning. We say,
I'm left with three meanings, three possible meanings, syntax excluded, 200 syntax trees,
semantics excluded, few invalid semantic expressions. And I'm left with three still,
three possible meanings. They can all happen in the world we live in. Which one is the most
plausible? We do this abductively. Which meaning is the most likely meaning given the context and
what I know? That's the last challenge in language. So we need to, we need to add the abductive model
which we humans do. I go back to the teenager shot of policemen. Both meanings, both interpretation
can happen, right? Either one can flee, right? But most likely it's the teenager that fled away
given what I know and given that's abductive reasoning. But semantically, both can happen.
Yes, I do want to emphasize something that Wally briefly mentioned, but I think it's very important
to mention is that there's two senses of abduction. And they differ in the following way,
which is like kind of the more modern sense, which is what Wally's been talking about like
pretty much this whole time, is abduction used to justify hypotheses. Okay, but the older and
original sense of it and still an equally important one is abduction for generating hypotheses. And
this, this like ability to generate hypotheses is something that's extremely powerful and,
you know, so far uniquely human. Hold on, let me just, let me just finish here,
which is this like, this is like something where Einstein is just sitting there pontificating on
like, you know, how the heck can light be the same no matter how the earth is moving and blah,
blah, blah, and comes up without a thin air. Okay, like this hypothesis that, you know,
that, that, that relativity applies, right? That the physical laws are the same no matter what your,
what your reference frame is. So that ability to, like this ability to almost that people talk
about sort of pull from thin air, this kind of intuitive leap to something that's that ends
up being like a grain, you know, grand new theory. That's also abduction. Right, but in both cases,
Keith, and I agree with you, that's the old view of what abduction, abductive reasoning was to science.
But in both cases, you're choosing from possible. Oh, no, no, no, just, just a minute, because
this is where I, I think I probably quite disagree with you, which is the modern sense of abduction
to me is much more similar to just inference like to a Bayesian. So in other words, you give me a
whole slew of hypotheses and I can tell you which hypotheses should be preferred, you know, just
on the basis of marginalization and strict, like Bayesian theory, no problem with that. It's not
actually abduction. It's just, you know, inference, right? Just rules of inference. Whereas, whereas,
whereas just a minute, generating that, that space in the first place is unique and very different
from inference, like the ability to produce, you know, from nothing models to consider,
that's the core of abduction from my point of view. But okay, so you're saying the same thing,
but indifferent. These possibilities that you generate are valid possibility. So abductive
reasoning, I don't know if they're valid until I do the inference. You're generating a pool of
possibilities. That's the step, generating a pool of possibilities. Fine, fine, fine. But, but in the
end, Keith, I think we're saying the same thing. It's just a terminology. In the end, you're choosing
from a set of possible valid hypotheses. Induction is you don't know where you're going until you
get there. In abductive reasoning, you are, whether it's the old way or the modern way,
in the end, what's common between them is, I have a set of possibilities. I will,
I will use abductive reasoning to decide which is the most plausible. In a sense,
you're scoring them. And you're saying, from all these possibilities, this is the most plausible.
Yeah, but see, you keep assuming the, you keep positing that you have a bunch of possibilities.
And I'm saying those possibilities have to come from somewhere. And where they come from is abduction.
Oh, okay. It depends on the domain and language. They come from what we know is true. Okay,
I see your point. But where they come from depends on the domain of reasoning. So in many cases,
they come from what we know is true, right? Or they come from evidence or, okay, I agree.
Yeah, I guess, I guess it's just important to know there are these two senses of abduction,
and, and don't forget about both of them, because they're both.
Right. And that's why that's why abduction, like induction, as opposed to deduction, abduction
and induction are both approximate. You can never have 100%. Because in the end, you're assigning
a score, you're saying. So both of them are probabilistic, in a way, or, or they have a
certain uncertainty measure. So when you're doing abductive reasoning, even in language,
I make a decision as this is the right interpretation given the context. But it's what we call
could be wrong. You might have eaten the ball of the baseball game. When I read further, I change
my further my first interpretation. So we have so in language is not monotonic. Actually, we do
non monotonic reasoning and in the sense that I might override my first decision. So, so, but
all of all of that is pragmatics. And we, and we do this in conversation. I two, three sentences
after I understand really fully what you said before, because I remade the interpretation.
And well, do you have any any clocks on where, where it is saying from or, or basically the
evolution of language or if you like the evolution of this abductive athlete, do you have any ideas
or, you know, is it unique to humans? It seems it is, you know, what are your unique to humans?
Definitely. Definitely. I mean, animal language, animal symbolic languages have been studied thoroughly.
And two things here's where the genius of photo comes in productivity. I mean, language have
a finite set of symbols and they're not productive. They don't do compositions.
And this ties to animals, your time up. No animal, no, no non human animal has a productive
language. In other words, I have a set of symbols. And if I can compose them, I, I can make a new
symbol language. Animals don't compose things because they don't decompose them one day or
two. They have a finite, it's a hash table. If I make the symbol, I mean this, if I make this,
okay, no matter how sophisticated it is, because they don't have recursion, they don't have
infinity that can't deal at that level with complexity. Some of them have a larger lexicon
than others. Okay, but that's still the same paradigm. So productivity, in other words,
this capacity to learn, we were just talking about John or the neighbor next door, or the neighbor
next door that just came from California, I can, I can productively make a person out of three
sentences. And in the end, they collapse to a John, right? That productivity doesn't exist in any
species except humans, which means compositionality, which means systematicity, which means all of
that. So it's unique to human, definitely this has been established. And it came with thought,
that's the if and only if, that's why we're the only species that really reason. I mean, okay,
I have people insist that animals think and they reason. They're not really reasoning, okay,
only humans reason and thought and language came together. It's sort of like if and only if
there are some, there's some proof, even anthropologists and
they say it looks like language was detected when tools and and some basic machinery was
detected first. So the human mind at some point had this capacity to think and language came
with it. It was like almost at the same time. So it's uniquely human, definitely now. Where did
it come from? Wow. I think it was the
the need really to express thoughts, like at some point, we started having thoughts that we want
communicate. So the external artifact we see outside, whether it's English or
ancient Greek or Latin languages evolve for societal reason and all that. But the external
artifact that we use to communicate thoughts came out of the need of the internal language
that started to develop what photo calls it the language of thought mental ease.
And we so we had that thing going on inside who and then we had to communicate. We started with
weird sounds who and then we scribble things on the wall to communicate. And then that thing
developed until we started making symbols like, okay, if I say this, that means this.
I don't know the exact process. I'm not a biologist or evolutionary linguist or but
I think thought is the key here. So there's a language of thought. And these external things
are because linguistic research has also shown that there are many universals in language,
regardless of what the languages, even if they are completely different systems like
Asian languages and Latin, these languages, they all have a verb and action. They all have
objects and agents of the action. They all have events and events have duration time and place.
So there are a set of cognitive, I call them universal cognitive primitives, right? There's
always an object there somewhere or an agent of an activity. Now how you express it in different
languages? Besides, these are universals. That's, that's the language of thought. That's the internal
language, which has to be the same. And objects have properties and all that. So there are universal
primitives. And we instantiate them in different languages differently, but that's to me secondary.
Okay, okay, that's great. Pam, Waleed, could you talk a little bit about your recent book review?
A colleague that I never worked with, but a colleague in the field. To review this book,
and I looked at it and I said, oh, I have enough on my plate. This is not an easy book.
But then I, because I liked it, I said, yeah, I'd like to write it. And in the end,
it turned out to be not as technically involved as I thought. It's sort of,
and I'm saying that not to, to be negative, but it's sort of the same argument over and over.
The gist of the argument is quite simple, actually. And they try to prove it from different vantage
points than in the book, from a biological, sociological, psychological, mathematical.
But the gist of the book is any talk of AGI is wishful thinking. And it's beyond anything we
can ever develop mathematically, so as to engineer it in any, in any realistic way.
They make good arguments throughout. There are many examples of the basic idea is that
all the mathematics we know, right, mathematics available to us, cannot model
not just the entire mind, but even subsystems in the mind, language being one of them.
And so it's all complex systems within complex systems in a complex environment,
the system around us that we interact with. And none of it can be modeled mathematically,
none of it even at any level. So forget doing AGI that can interact with us in an intelligent way.
Now, you can do controlled narrow AI, right? You can build very intelligent machines that
can do amazing stuff. But any talk of AGI, strong AI, is just talk until, unless, and they admit
that, unless we come up with a new mathematics that we never even knew at the scale of Leibniz
calculus or Newton, like we're talking about a new mathematics that we never conceived of, right?
Which they say most likely all evidence says that's not gonna happen, right? So now you can get into
why. So that's their claim. And why they, they say that all these systems are complex systems.
And in complex systems, the idea is that these are, first of all, dynamic systems.
They work in a dynamic environment. They are continuously evolving and adapting, right?
They are self feeding systems. These are not systems that only take input output.
These systems change their behavior. And I gave an example from list. These systems are systems
that change their behavior, their algorithms, if you want, they change their mind from a stimulus.
So I might, and that's why I said they, I would have liked to see a discussion on the frame
problem because the frame problem in the eye is about this. How can I reason in a dynamic and
uncertain environment and react dynamically, although what I do in the environment might
affect what I believe about the environment in real time. And they're right. There is no mathematics
we know of now. That's why we don't have a solution for the frame problem. So
this kind of cyclical cause and effect cannot be modeled by anything we know in mathematics.
And this I agree with them. They, they give an example, if I may just an example in language,
for example, language, we know, if I have a dialogue, okay, we all agree that the interpretation
of any utterance requires having the context in mind as, as part of the, part of the input to
the evaluation of the meaning is the context as an extra parameter, right? Now the context
is changing based on something I cannot predict, which is the response to, of, of some participant
in the dialogue. There is no meaningful way of predicting how someone might respond.
So in other words, the context is mathematically not defined, but I need it in the interpretation.
Thus, no language understanding, no language understanding, no AGI, because they believe
language understanding is a prerequisite. So the, the conclusion, I mean, you can question
every step in this inference they come to, but they give language as an example, but we have
social behavior. I can give an example. They have a nice example and social behavior. Here's an
example of a complex system that cannot, you know, we don't have any mathematics that can model.
We're staying in a queue in a clinic, an emergency room. What do they call them? These ER.
So, but there's a queue because they all have emergencies, right? Now the social behavior,
then the social norm is that in the queue, okay, we all have, we all have urgent issues.
But in the end, I came first, right? Okay, so that's a social norm. And, but can a robot understand
that if someone fainted, really, I mean, it's almost gone, right? Our social norm accepts
that this person violates the queue order, right? This is something dynamic that happens,
like the queue is this way. And how can a robot update the rules and not kill someone because
they violated the order of the queue? In other words, these interactions, these cyclical cause and
effect are very complex, that no mathematical model or the example I said in language, they
prove this cannot be done. Context is needed to interpret everything. I cannot predict what the
context will be because I cannot predict you respond to my, so it's unpredictable, they call it
erratic, almost random. So there is no mathematics that can model it. And there are many aspects
to the mind, whether it's social reasoning, language, and then they conclude there cannot be
a system that we can model on volume and machines, because we don't have the mathematics to model it.
And these, they go into deep learning. And they give examples even like deep learning, no matter
how much data you ingest, you can never predict the future. You're lucky if you can do a good job
on the past and even forget the future. And definitely forget the, sorry, the present.
So definitely forget. Can I jump in for a minute because I have a couple comments. So
one is, would you agree that this is quite synonymous with Douglas Hofstadter's strange
loops and the whole reference to self-referential systems? Because I mean, complex systems,
a big part of them is they usually are, they do have feedback loops. And at some scale,
they become self-referential. And so they will involve self-reference, yeah?
Yeah, okay. My other point I want to make is this, I have quite a bit of sympathy towards
the viewpoint of this book that you're talking about. With one exception, which is I'm still
optimistic that we can discover a mathematics that may help us out. And so I always think to
the foundation series by Isaac Asimov, because in there, they discover a science in a mathematics
called psycho history, which at least allows them to predict complex systems of a certain scale
and larger. So in the book, it's sort of like planet scale and larger, they're able to actually
predict these complex sociological systems and human behaviors and how they're going to interact
beyond that scale. And it's really fascinating. I make that point. I highly recommend that series
today, because it's very fascinating because they talk a lot about sort of what if you had this
science, what might it look like, et cetera. And in there, there's like this little tiny
microscopic thing that's beyond the predictability of psycho history that comes in and kind of mucks
up the works and creates anomalies that they have to constantly keep combating against.
So I think if anybody wants a fictional take on a possible mathematics of this, like I would
recommend the foundation series. Yeah, I make this point. I agree with their argument. We're
trying to model complex systems in the sense of cyclical cause and effect. We don't have anything
that can model them intelligently. And I give an example in list. In list, I can write a program
that changes itself at runtime, because this is intentional. I can, the whole program can be at
parameter, which I can look at it. Well, code is data. That's why I can manipulate the program
itself and go look at it after execution and see different programs than the one I wrote.
It's amazing. So if I, so I can write programs in this that no one can understand
and model and do program verification. So I think the argument that, okay, I can see your point.
But like Keith said, never is a long time. Why say we cannot come up with a new mathematics?
I can see at one point someone discovering, yeah, at the level of Newton differential
calculus. Why not? Which could happen. So the word never for me, it's hard to digest.
Maybe an AGI will discover the mathematics to create itself.
And the other point is the other point is, which is another point that John McCarthy wants.
Who said we have to understand what we built? Here's what I mean. Do we understand ourselves?
We don't. So why not build a scary intelligent machine that we don't really understand,
like my list program. So what I'm saying is I had I had an issue with them saying
that precludes AGI. No, it doesn't. In theory, I can build a complex intelligent machine like us
in many respects. Doesn't feel pain. Hey, who cares? But it's scarily intelligent.
And we don't understand how it works. So what? This can happen. I can build something I don't
understand. So in theory, I have two issues with their book that this never and this absolute
decision that we're done, we can never get there. No, we might build something we don't understand
by discovering some new weird mathematics. So okay, I agree with you that it's a complex thing
that we will never understand. But so what? But what I loved about the book is it's a sobering book.
I mean, it really is a balancing book compared to the hype and the simplicity you see out there.
I mean, you you recommend it or highly because I mean, I didn't need that much sobering. I know
any talk of AGI is like, hey, take a break. Enjoy your paycheck, but don't make silly
statements like this, right? Although I thought I thought you might have fallen off that wagon
at the beginning of this conversation. I'm a defender of the faith. But so it's I recommended
to people that need it like me, I needed it too. It's a sobering book. Like, this is how complex
what you're trying to do is okay, guys. So before you go out and say language understand. And the
nice thing is they took aspects of the mind, just language itself is a beast that we cannot conquer.
So imagine the whole mind and the granular thing they go through it. I mean, it's all
it's complex systems all the way down or all the way up if you want. So
language is a complex system on its own part of the mind, which is a complex system on its own
part of the human living organism, which is a complex system on its own. So and at every level,
the complexity, we don't have a mathematics for that's the gist of their argument. So people
that make these big claims about AI need to read it. Guys, cool it down. Cool down. You have not
solved problems that occupy the most penetrating minds in the history of enemy from Emmanuel
Count to you have not solved these problems. Cool it down. You can build narrow AI, very narrow AI,
and all this transferability, transferability. I mean, if you're good at chess, I know people
that are good at chess and they're almost good at nothing else. Not okay. So forget this. If I'm
good at chess, I can be a smart doctor. No. So we are a very complex machine. So this book is a good
sobering book, mathematically speaking, philosophically speaking, so that people will tone down
what they're saying and start speaking science instead of media gibberish, right? Deep learning
will soon be able to do everything. I mean, from a scientist. Well, it seems like a council of despair
almost. Is there any optimistic or positive hopeful aspects to it? No, I that part I don't like
this never, right? I mean, I am a believer that we can do AGI, but not a human like AI. We might do
a very powerful AI that in many ways is more powerful. I mean, we've done that now. I mean,
machines are now superior to us in many respects and respects even that they require intelligence,
not a bulldozer that can lift more than me that will have to do cognitive tasks better than us.
We have go right or or finding patterns and data at the scale that no human can do. So
so we are building intelligent machines, but can we conquer things like language like autonomous
driving was a failure. It's a big upset for AI because they trivialize the problem that's right.
We can go. Yes. Well, that and that's kind of what I wanted to get to, you know, Mark kind of in
response to you, which is I take these kind of sobering, these sobering things and look, I mean,
the book sounds great, and I'm definitely going to get it and read it. But some of these these
thoughts, you know, many people have had, you know, many times over the years, right? And I've
recognized that there are these limitations. But I think like part of part of why I think books like
this are actually have an optimistic kind of side to them is I hope I hope they encourage people to
get more creative. Okay, like, like stop just trying to dump every single dollar you have into
yet another parameter, you know, into yet another thousand or billion parameters in a model, like
let's take some of our resources like sure, let's keep doing that engineering, but let's take some
portion of our resources here and invested in like crazy ideas. And I know Tim, actually,
they make this point. Because it's like, like, can it's can a Stanley kind of type thing, right?
Like just go out there and try to do something crazy to find that mathematics that we need,
right? Which is let's get creative. Let's work on crazy things. Let's have crazy ideas. Let's work on
hybrid systems. Let's not give up on, you know, neuromorphic, you know, systems and computer
or whatever, like, let's spread out, let's spread out a bit. Because of the fact that if we just
keep going down this direction of ever larger Turing machines, like, that may not be the solution.
Right, actually, they make this point exactly in different ways that if anything, their goal is to
let people widen their horizon. So many aspects of this problem that guys, if if we keep going,
this is not going to get us there. And that's why they argued mathematically,
philosophically. And I think they have a good argument. This is not going to take us there.
But let's explore that's that's so that and being so religious about this will will hinder any other
possibility. So all overall their argument is a good argument. I think everybody should read
this book that's interested in AGI as a goal. What we have cannot ever take us there. They prove
this mathematically. I mean, to me, they proved it in language on. So we need something new. And if
you want to do something new, we can't just stay in this corner and with this, that's not going to
get us there. So in a way, it's not a negative book. It's a sobering book, I will use the term sober.
Well, and encouraging of more variety and more daring and more creativity and
right, they don't push too much on that. But but indirectly, the indirect
net result, if people appreciate the argument will be to look
and explore other ways. So in a way, it's more positive than negative. And by the way,
stating a mathematical fact is never negative. Don't be so sure about that.
If they're saying we're always on the edge of being canceled for stating like mathematical
facts. So yeah, but I mean, if they're saying that what we're doing now will never get us to AGI,
that's not negative. You're saying we need something else, we need something more.
Look, it could have saved us, they use this phrase, money down the drain. Autonomous driving is a case,
is a good case. Yeah, billions, we're talking non trivial money guys, we're talking more than
the budgets of some European countries. Just imagine the scale. And those guys went bust,
right? Why? Because they trivialized the autonomous truck. An autonomous car is an autonomous
agent, guys. It's an it's an agent trying to reason in a dynamic and uncertain environment,
and has on the fly to change to do belief revision, change its strategy, because of something new
that came up. All of that is from seeing the tree and the stop sign. It's all vision.
Yeah, so this is actually encouraging, because now for all the Uber drivers out there and
long haul truck drivers, your job is safe. Like it's not going to be replaced anytime soon.
Yeah, it's amazing. And a few years back when I was still in the valley,
yeah, I was in Mecca in Silicon Valley. And I would talk to superb PhDs in neuroscience and
big AI engineers at top companies. They were so excited that we're at level four in a year or
two. And this was six years ago. I say, guys, this will not happen. They say, why are you so
negative? I said, you cannot have an autonomous agent on the road without solving the frame
problem. How do I revise everything I know, because of this new event? And this has to happen
real time. We don't have a solution for the frame problem. All you're doing is you have cars on a
railway. We have autonomous cars now. It's called the train. We have autonomous flight.
If I'm not in a dynamic and uncertain environment reasoning, yeah, I can have autonomous anything.
We call it railway. We call it Amtrak. It's autonomous. You press a button and it goes.
If we're talking about reasoning in the streets of San Francisco,
you have to face the frame problem or you will kill people.
Anyway, on that sobering note, I think a lot of stuff goes unnoticed in San Francisco,
so I'm not sure about that. Probably that's the least of them. No, but the scale of money that
went, this is the value of this book, the scale of investment. If 10% of that was put on another
approach, hey, you weird guy with this weird idea, take 10% of what we're throwing down the drain
and explore something else. Show me. That's where I'm at, too. We have to diversify the effort.
There's a huge impact here, societal impact. We're wasting billions of dollars just because I
don't want to listen to anyone else. It happened in the chatbot industry, which I'm more familiar
with than autonomous driving. Chatbot this, chatbot that, and there was an explosion. It was like
a blob, like the internet thing. Now we can't get away from them. Every website we go to,
it's like, leave me alone. Nobody wants to use them because we know how they work.
They're, these are stochastic parents, but I mean, it's, yeah, like literally just going to a
fac and hitting control F is like more, more effective for me. I'm trying to interact with a
chatbot. Their media, their search engines by key phrases you put, and they bring you a link,
and they say, read this. This is your answer. They are search engine, basically. But again,
the amount of money, because I lived in that industry, the amount of money spent on chatbots
will scare the hell out of anybody. You combine that with autonomous driving, both dead, almost
zero. We're talking billions and billions and billions. And you talk to any one of them in the
highest, in the middle of the fever. They won't listen to you. I have people now calling me back
and saying, you were right. Yeah, after $200 billion. So, so science is important. Engineering
is important. But science is important too. That's where this, the value of this book is.
Guys, hacking alone will not do the whole thing. You're a bright engineer. You can hack your way
through what we know is true. That's the space you can play with. You cannot hack your way in a bigger
set of possibilities that are, you didn't verify that you can go there. You can, an engineer can
be creative within a Venn diagram that the scientists drew for them. That's the difference
between science and engineering. The scientist draws the Venn diagram. And that's the value of
philosophers, at least the analytic philosophers, that know logic and metaphysics and quantum
mechanics and philosophers that are on the technical side. They know how to draw the Venn
diagram. You as an engineer, if you're wasting your time here, that's called money down the drain.
Play inside the Venn diagram. Otherwise, you're just an over enthused engineer who should go back
and study compatibility. It's kind of like how patent examiners can easily reject anything that
comes in that claims to violate the second law of thermodynamics, right? Well, listen, Wally,
I think we sincerely appreciate your time today. And also, Mark, thank you for joining
us and asking questions. We should do this again. Thanks, Wally. Yeah, I really appreciate it.
So, thanks everybody. Always fun, guys. I see. Peace.
