David Chalmers is a Professor of Philosophy and Neuroscience at New York University
and an Honorary Professor of Philosophy at the Australian National University.
He's co-director of the Centre for Mind, Brain and Consciousness, as well as the Philpapers Foundation.
His research focuses on the philosophy of mind, especially consciousness, of course,
and its connection to fields such as cognitive science, physics and technology.
He also investigates areas such as the philosophy of language, metaphysics and epistemology.
With his impressive breadth of knowledge and experience, David Chalmers is a leader in the philosophical community.
So there has been all this controversy and debate recently about whether language models might be conscious.
And I've actually been especially interested in not so much people's gut intuitions one way or the other,
but the reasons and the evidence on both sides.
First, what is the evidence that these language models might be conscious?
And second, what is the evidence or the reasons for saying they're not conscious?
When it comes to the reasons in favour, there are a few things.
Well, you can get these systems to say they're conscious.
On the other hand, you can also get these systems to say they're not conscious.
It's not clear what weight that carries.
Especially the most strongest consideration in favour of consciousness in these systems is they're showing signs of general intelligence,
the ability to do many things.
In the study of consciousness, we take the domain general use of information to be one marker of consciousness.
That's maybe one strike in favour.
When it comes to reasons against, there are many, many objections that people have raised.
One objection is that these language models aren't biological and biology is required to be conscious.
I mean, that would be a very general reason that would prevent any artificial consciousness.
So I end up setting that one aside, but there are many other reasons.
I say, what is the X such that you need X to be conscious and these large language models don't have X?
Well, besides biology, there's maybe having senses and a body.
Right now, language models don't have senses, don't have a body.
On the other hand, it's not terribly difficult.
People have already built extensions of these language models,
call them vision, language, action models that deal with sensory information in the form of images and which control a body,
maybe hooked up to a robot body or a virtual body.
So I think in this case, it looks like even if the large language models may lack what's required for consciousness,
these extended versions, what I call LLM plusses, extended large language models might overcome those objections.
And I think there are a series of objections to language models that take the same form.
People argue that they don't have world models or self models.
People argue they don't have recurrent processing or a global workspace.
Maybe most deeply, they don't seem right now to be unified agents with fundamental goals of their own.
But all of these seem to me to be temporary issues.
Maybe today's language models lack them, but tomorrow's extended language models may well have all of these properties.
They may have world models and self models.
They may have recurrent processing and a global workspace.
They may even have unified agency.
So I think there's a research program here of trying to build AI systems that have all those X's,
which are currently missing from large language models.
And then the question is, just so we build within, say, 10 years or so, a system which has all of those X's,
it needn't even be human level intelligence.
Maybe just we'll have something like fish level cognition or intelligence because, you know, most people think fish are conscious.
Just say we build a system like that that has the sensors, the body, if only a virtual body,
the world models and the self models, the recurrent processing, the global workspace, the unified agency.
Will it then be conscious?
We might have these systems by, say, 20, 30 to 10 years or so.
Some people are going to say those systems are not conscious.
If so, I ask them, what's missing?
Interesting, but for Charm is this conscious experience, what's the cash value of it?
Well, exactly. I've never managed to work that out.
And that contradicts human first person accounts.
Another maybe interesting question is one you prompted with the AGI comments.
So to what extent do you think consciousness is a necessary condition for AGI,
assuming one can achieve it, or could one get to AGI without the consciousness path?
It's a really interesting question, and this interacts with the question of the philosopher zombie,
the system which has all the behavior of a human being, but none of the consciousness.
I've argued that zombies are at least conceivable.
We can imagine a system that behaves like us without consciousness,
but that doesn't mean that such a system could actually exist in this world.
I'm inclined to think that in this world, intelligence and consciousness go very tightly together.
I mean, maybe there are some systems.
My colleague Ned Block has considered the idea of a giant lookup table
stores every possible conversation passes the Turing test.
That would not possibly be conscious.
My own view is that any system with reasonable mechanisms that generate human-like intelligent behavior
will probably be conscious.
But if it turns out that maybe massive scaling yields pure feed-forward transformers
with incredible capabilities, but no consciousness,
I mean, that's not something I can totally rule out.
And I do think this is a potential concern if you're thinking about the role of AGI in the future
If it's possible that we're going to have beings which are highly intelligent,
which lack consciousness entirely,
then their role with respect to say moral, social, legal issues is going to be very delicate and hard to adjudicate.
Philosophical zombies are imaginary beings that are physically and behaviorally
indistinguishable from conscious humans, yet lack conscious experience.
This concept has been used to explore the implications of certain thought experiments,
such as the Chinese Remargument,
which posits that a computer program can pass the Turing test, yet still lack consciousness.
To build a zombie, one could imagine a cabinet full of raw materials
from which to construct a perfect copy of a human,
all memory, knowledge and personality would be embedded into the physical system
and the clone would behave as if it were conscious.
However, our intuition tells us that when the parts were unassembled,
there would be no consciousness present.
The concept of a philosophical zombie has been widely discussed in the philosophical literature,
yet there is no consensus on the matter.
Proponents of the idea of a philosophical zombie suggest that if such a being were possible,
it would suggest that consciousness is an emergent property of certain physical systems
and not an intrinsic property of the universe.
This would have implications for how we understand the nature of consciousness
and its relationship to physical systems.
Opponents of the idea of a physical zombie point out that there's no empirical evidence
that such a being could exist.
They argue that if there was a philosophical zombie,
then a physical system that is behaviorally and physically indistinguishable
from a conscious human would still lack consciousness.
This would suggest that consciousness is an intrinsic property of the universe
and not an emergent property of certain physical systems.
The debate over the concept of a philosophical zombie continues to rage in the philosophical community.
While it's unclear that this issue will ever be resolved anytime soon,
it's an intriguing concept that has been used to explore the relationship
between consciousness and physical systems.
The zombie is a system which has all the intelligence and the outward behavior of a human being,
but not of the consciousness.
And the thought is relevant here in New Orleans,
wouldn't it be a drag to be a zombie?
Not of the experience of a human being.
So it goes, I act like you act.
I do what you do.
But I don't know what it's like to be you.
What consciousness is, I ain't got a clue.
I got the zombie blues.
For a very long time, philosophers and scientists have shared a common interest in
puzzlement over the hard problem of consciousness.
Physicality seems insufficient to be the source of the subjective, coherent experience
that we call consciousness.
David Chalmers invented the phrase, the hard problem,
in an attempt to describe this very phenomenon.
Finding a way to link mental states to neurophysiological activity
is the hard problem in a nutshell.
It's beyond the scope of human inquiry to determine why it is that any kind of physical action should
and presumably does give rise to subjective experience.
Consider the hypothetical situation of two sets of identical twins who have the same DNA,
upbringing and educational experiences.
The twins will have quite distinct subjective experiences of the world.
This exemplifies the challenge of explaining how subjective sensation may result from objective physical action.
Now you might just counter and say, this example is simply a matter of complexity.
However, the Chinese room argument is another thought experiment
which might shed light on the hard problem of consciousness.
In this hypothetical scenario, a person is locked in a room
and given a series of instructions and symbols
and an external observer is provided the output
after the individual has been requested to modify the symbols in accordance with the instructions.
Now, despite the fact that the individual in the room is manipulating symbols,
the thought experiment shows that he doesn't understand
since he does not have the requisite phenomenal experience.
Consciousness is a hard problem that has been discussed for millennia but never resolved.
Some say that it simply can't be done
while others think that a mix of neuroscience, philosophy and psychology will eventually get us there.
No matter what the solution to the hard problem of consciousness is,
it's clear that it's a difficult problem to solve.
Okay, consciousness.
I started this, my original title for the talk is the word sentience.
In the end, I decided that word is just too confusing,
even more confusing than the word consciousness.
So I went with consciousness instead.
But as I use the terms, those two things are roughly equivalent.
Consciousness is sentience which is subjective experience.
A being as conscious in the philosopher Thomas Nagel's phrase,
if there's something it's like to be that being.
If that being has subjective experience,
like the experience of seeing, of feeling, of thinking.
Nagel wrote a famous article called, What is it like to be a bat?
He says it's very hard to know exactly what a bat's subjective experience is like
when it's using sonar to get around.
Nevertheless, most of us believe there is something it's like to be a bat.
It is conscious, it has subjective experience.
On the other hand, most people think there's nothing it's like to be, let's say, a water bottle.
The bottle does not have subjective experience, it's not conscious.
Now it's clear that the modern human has a huge problem with not knowing.
There'll always be a tension between having a theory of the world in hand,
which makes excellent scientific predictions,
but which bucks against our evolved intuitions.
John Locke, Emmanuel Kant and John Roles all have argued that an intelligible framework
is necessary for humans to make moral and rational decisions.
Such a framework provides us with a sense of stability and order,
as well as a sense of meaning and purpose.
Nietzsche, however, questioned the role of the personality of the philosopher in his own philosophy.
He argued that the innermost drives of a philosopher's nature can shape their morality,
suggesting that the search for a new foundation of values, knowledge and beliefs
must come from within the individual.
In his essay, What's It Like To Be A Bat,
Thomas Nagel considers this subject from the vantage point of a bat.
Since our experiences are so different, he says,
we can never really comprehend what it's like to be a bat,
since bats use echolocation rather than vision.
Nagel claims that their perception of the world is fundamentally different from our own.
The bat, he says, has a much richer sensory experience than we do,
because it can pick up on a wide variety of objects and surfaces, many more so than we can.
Since our experiences are so different from a bat's,
Nagel concludes that we can never know what it's like to be one.
According to him, the only way to know what it's like to be a bat is to become one,
which is obviously not conceivable.
He believes that we can only do our best to envision what it's like to be a bat and to fly around the world.
Nagel's inconceivability argument has its advantages and disadvantages.
On the one hand, it's a great way to test the boundaries of our knowledge.
Understanding the world is more complicated requires admitting that there are certain things that we just can't fathom.
However, the inconceivability argument might be seen as a barrier to our human ability to comprehend the universe at all,
much like how Noam Chomsky said that rats would be unable to solve the prime number maze.
Accepting that there are certain things which we will never be able to grasp
may prevent us from understanding and expanding our frontiers of knowledge.
The most significant and defining quality of mental phenomena is consciousness,
making it impossible to solve the mind-body problem.
It's impossible to comprehend the material foundations of mind via reductionist theories,
since they ignore the subjective nature of things.
This subjective nature is linked to a particular point of view and cannot be represented by reductionist theories.
Exploring the topic in the context of an example which highlights the difference between subjective and objective conceptions
is useful for illustrating the relationship between subjectivity and making visible the relevance of subjective qualities.
The bat is used by Nagel because it exemplifies the difficulty of the subjective nature of experience
by providing a spectrum of activity and sensory experience so different from ours that it's particularly striking.
Now, since the raw material of our imagination comes from our own experiences, its scope is necessarily truncated.
We can't infer the bat's inner existence from our own, and if we attempt to envision what it's like to be a bat,
all we have is our own limited mental capacity.
Nothing in our current make-up allows us to contemplate what it might be like to exist in the state of interpolated batness,
even if such a transformation were theoretically possible.
So, while we may make some assumptions about what it must be like to be a bat based on our own understanding of the animal's anatomy and behavior,
we recognize that each individual bat's experience must have a unique subjective quality which is beyond our cognitive horizon.
Some scientific realities may defy description even in the broadest terms of human experience.
Most people can't fathom the nature of a person's experience, who's been deaf and blind from birth.
Even sentient bats or Martians from another planet would have a pretty hard time imagining what it's like to be human.
It's possible for us to have faith in the reality of things, whose precise nature we can't yet fathom,
but reality may be known from just one vantage point.
It's unclear how the organism's physiology may somehow convey the underlying nature of experiences.
To simplify is to advance towards more objectivity, a precise understanding of the true essence of things.
It's not clear what's meant by objective nature, right?
The objective nature of an experience, if it's not understood in relation to the subject's unique perspective.
Even a scientist from Mars who's never seen a rainbow, a bolt of lightning or a cloud, Nagel said,
couldn't miss the fact that they're all examples of physical events.
Lightning's objective nature is not depleted by its outward form.
A more direct link seems to exist between one's experiences and their beliefs.
We don't characterize the world around us in terms of specific sensations or our senses take away from it,
but rather in terms of the broad impacts it has and the traits which may be detected by methods other than the human senses.
We feel that the more our representation doesn't rely on a human or anthropocentric perspective, it kind of becomes more objective.
It's conceivable to forsake an anthropocentric perspective in favor of something more abstract while maintaining the same understanding.
Nagel said that not all experiences follow the pattern of relinquishing a subjective experience in favor of a more objective one
in order to better comprehend the same events, because it is fundamental to the human experience.
The uniqueness of the human viewpoint is something which we can't ignore indefinitely.
Modern psychology's behaviorist approach may be traced back to an attempt to replace subjective mental experience
with something more scientifically rigorous, like a theory of mind.
There must be some essential similarity between going through certain physical processes and having a thought occur in your head
if thoughts are actually physical processes.
Nagel said that the failure of physicalist ideas, which rest on an incorrect kind of like objective examination of mind,
proves nothing due to our lack of a conceptual framework.
Physicalism remains an incomprehensible viewpoint to him.
Nagel said we should try to develop an objective phenomenology not dependent on empathy or our imagination.
I mean, if you were to describe what it's like to see to someone who's been blind their entire life, you would use this phenomenology.
Perception's structural properties may lend themselves to a dispassionate analysis.
To get some insight of our own experience, it's important to build notions which are distinct from those which we acquire via first person learning or experience.
Nagel concluded that the insights into the physical substrate of experience may be made possible by an objective phenomenology.
So I'll just talk about language models in general, which is a topic I don't need to introduce to this crowd.
Typically, these are going to be with transformer architecture, multi-head self-attention.
GPT-3 is still the best known one, but I take it GPT-4.
Any day now people will also talk about extended large language models.
Extensions of what I'll call LLM pluses.
These are models that add further capacities to the pure text or language capacities of a language model.
Now there are very familiar vision language models that add image processing, language action models that add control of a physical or a virtual body.
There are large language models extended with things like code execution, database queries, simulations, and so on.
And in many ways I'm interested in not so much, not just the large language models today,
but the capacities of the systems that may be developed in the coming years, which will include these LLM pluses models.
And my questions will be questions like, well, first, are current large language models plausibly conscious?
But maybe even more importantly, could future large language models and extensions thereof be conscious?
And what challenges need to be overcome on the path to conscious machine learning systems?
I really see this, I mean, there are challenges here and there are objections,
but I really want to think of this also as a constructive project, one that might ultimately leave to a potential roadmap to consciousness in AI systems.
So the thing I've sometimes seen characters in GPT-8 models do is sometimes there's something strange happening in the story.
Like, you know, you have some stories of characters and then suddenly saying, you know, the model is sized in a refrigerator token and so on.
And you're wondering, characters will notice that, and sometimes they'll question whether they are in a simulation.
The characters in a GPT-8 generator in the story will be like, this is just a simulation, you know, am I real?
And stuff like this, in a way.
Like, how does this relate to the idea that complex simulation?
Are the characters inside of GPT potentially engines or entities that have some amount of mind or consciousness?
David, I had dinner with Conor Leahy on Friday night and he's one of the founders of a Leufer AI.
And they famously recreated GPT-3 type language models, which is to say, you know, these statistical, autoregressive self-attention transformers models,
which is a bit of a mouthful, trained on a bunch of internet text.
And some of their models are pretty large, actually.
They just released a 22 billion parameter version called GPT-NeoX.
Now Conor really likes to think that GPT-3 is an example of general intelligence.
He's a true believer and therefore convinced that we should be concerned with AI alignment as a top priority.
He said something which really fascinated me, which relates to your book, Reality Plus.
He said that in many of the fictional stories that he's generated of GPT-3,
the character sometimes becomes self-aware that they're in a simulation, if you like.
So when they see an unexpected word in the sequence, you know, they kind of have that realization that they're in a simulation of reality.
So it's kind of glitchy and the simulated residents are aware of the glitches.
So I think in all likelihood Conor has an overactive imagination, but I thought it was a fascinating thought.
When I spoke with Keith about it yesterday, he said it reminded him of the idea of being fooled by randomness.
There was a famous example with the ghosts in Pac-Man many years ago where people were convinced that there was some kind of intelligent behavior
and the ghosts were coordinating with each other, which actually turned out not to be the case at all.
So I think one of the themes of our conversation today will be intention versus extension, or causal structure versus correlation,
which is to say this idea that just because something appears to be conscious or intelligent in its physical configuration of the output
doesn't mean it necessarily is.
So GPT-3, conscious or not, I'm open to the idea there's consciousness even in very simple systems.
Like even the worm, like one of these basic C. elegans, it has 300 neurons.
It's sensitive tonight to light. It engages in simple action and so on.
I'm at least open to the idea that that worm has some element, very simple element of consciousness.
And then you go to ask yourself if a worm with 300 neurons can possibly be conscious,
what about GPT-3 with 100 billion plus parameters?
It's so much more complex than the worm.
There's so much going on.
It has so many capacities to do, to communicate, to talk, to reason, even to explain.
You might think, okay, well, if a worm is conscious, then GPT-3 has got some element of consciousness too.
On the other hand, GPT-3 is not very much like an agent.
It doesn't correspond to a person that has consistent beliefs or desires or actions.
It's more of a chameleon, right?
Because you can set up your prompt engineering on GPT-3 to get it to take on any kind of persona you like
with any set of beliefs and desires that you like.
So maybe GPT-3 is not so much like an agent or a person, but a meta-agent that can take on many different forms,
depending on exactly how it's engineered.
I would not be inclined to think that right now GPT-3 has anything like, say, the consciousness of a human being.
That would require a kind of coherence and unified intelligence that right now I think is beyond GPT-3.
But does it have some basic element of consciousness?
I certainly wouldn't rule that out.
Well, I think we would agree, though, and you said this explicitly in the book,
that there's no reason not to believe that it will be possible at some point for consciousness to be in silico.
Is that true?
So what I wanted to try and do is bring the perennial zombie question, the philosophical zombie, into the context of reality plus or a simulation.
And my question is this, and it's really about trying to find the explanation for consciousness.
So suppose as a result of us posting this video here, the architects, whoever created this simulation that we're in,
watch the video and they say, I think it's about time to have a little chat with David and they come to you in a dream because they can do that.
And they present to you evidence that you're convinced that you are actually conversing with the architects.
They explain, look, you guys are actually running on this neutron star supercomputer that we built.
And by the way, we so happen to still use C code.
And they show you the C code for agents in this world, for people.
Here's all the C code that's running your brain and your neurons and everything.
And this module over here, and they show you a module source code and they say, this is the consciousness module.
Now, not everybody's running this.
In fact, you have one running, but a lot of people, we don't bother installing them.
And they're just actually zombies because we didn't attach, you know, this consciousness module to them, but you're running a consciousness module.
Would you accept that that bit of code, you know, however long it is this module, 500 lines, 10,000 lines, would you accept that that code explains consciousness?
I would really need to know what their evidence is and what their what their science is.
I mean, maybe these beings could convince me that I am actually part of a simulation if they presented the code for the simulation and they did amazing stuff.
There were messages in the sky, they turned the Empire State Building, which is right outside my window.
They turned it, they turned it upside down in the in the sky.
Then maybe that would be pretty good evidence.
I'm in a simulation.
I mean, it wouldn't be totally conclusive.
Maybe they just give me some great drugs or maybe my whole life had been in reality.
But just for a moment, just for tonight, they put me into a virtual reality that was totally convincing and presented me with all this fake evidence of turning the Empire State Building upside down.
So I don't know if it would be totally convincing, but yeah, I think I could probably get pretty good evidence that I'm in a simulation.
But then you're telling me, okay, now they've got some theory of consciousness.
They think that they've put a consciousness module into me, but not into some other beings.
But then I interact with those beings and they're they're perfectly normal.
I don't know if those beings actually say, tell me that they're conscious.
I'm going to be inclined to believe them just as I do with other human beings.
So if they tell me that they think that these beings are philosophical zombies, beings that lack consciousness, I'm going to want to know why.
Merely programming it, merely programming the beings isn't going to tell us that.
We're going to need a theory of consciousness for that.
I mean, it's really important in these discussions.
Maybe I should have done this to start with to distinguish consciousness and let's say intelligence.
Intelligence is all about, you know, behavioral capacities, the things you can do, carrying on a conversation, taking place, sophisticated, goal directed action and so on.
But intelligence here is objective.
It's something you can measure from the objective point of view, ultimately coming down to behavioral capacities, whereas consciousness, as I understand it, is subjective.
Consciousness is a matter of how things feel from the inside.
When I carry on this conversation, I can see you guys.
I have some visual subjective experience of you.
I can hear you.
I can hear the sound of my own voice.
I can feel my body.
I got thoughts running through my head.
That's all the subjective experience of consciousness.
So, you know, it may well be that take GPT-3.
It's got rather sophisticated capacities for intelligence already, but it's totally unclear whether it's conscious, whether it has subjective experience.
Maybe it has a bit in the trouble is because intelligence is very easy to operationalize and measure from the outside.
Consciousness is not.
And you can illustrate that with the idea of the philosophical zombie, which is behaving in extremely sophisticated ways, but has no subjective experience on the inside.
I don't actually believe in philosophical zombies in the sense I don't think that there are intelligent beings without consciousness around us, but I do think it's a coherent hypothesis.
And that does mean that whenever we create, say, an artificial intelligence with even very sophisticated intelligence, you're always going to be able to raise the question, is it conscious?
To answer that question, we need a theory of consciousness.
Okay, but the scenario here, though, was you've been convinced by these beings that they are actually the architects of the simulator, and they're literally showing you the source code for the consciousness module.
But I think what you're telling me is you still wouldn't believe it unless they presented you with a theory that you could personally understand.
Yeah, why think that being a good simulator makes you a good theorist of consciousness?
I could, you know, maybe potentially I could build a great universe simulation eventually and simulate brains really well and so on.
That doesn't mean that I've suddenly solved the problem of consciousness.
I'm still going to know, you know, which bit, which processes in the brain or elsewhere correspond to consciousness and why merely being a great simulation engineer doesn't yet turn you into a scientist.
Once we've got those simulations of brains and of the world and so on, maybe there'll be amazing things we can do to try to put these systems in different circumstances, see how they behave, see when they respond, see which kind of simulations actually report being conscious.
And maybe we could use that as really great data for a theory of consciousness.
But I don't think it suddenly resolves the problem, just being able to build, say, a brain simulation.
Hey, Louisa, I'm David Chamas.
Your father just introduced himself to me and he tells me you have a profound philosophical analysis of consciousness and qualia that you've been doing at your school.
So I just want to very much encourage you to keep going with this project and see if you can help us fit around the nature of consciousness.
I'm looking forward to your future career as a philosopher.
Yes, also from Ilya's Twitter thing, because your daughter brought the tweet from Ilya.
Because it's my thing from AI, said why not start with this?
And said it was the same kind of starting the thing.
And now it becomes very interesting for the philosophers, because now it has some actuality and stuff.
I think it's a really cool topic now.
You and your daughter have found something.
You will collectively figure out AI consciousness.
We don't know how to approach it, because it's really a hard topic to define it, because I was a math guy.
And it's not so well defined and I don't like it.
What don't you like?
The definitions of qualia.
Consciousness and qualia.
You need an operational definition.
In terms of input and output.
Give me something I can hand off to my engineer.
I would like it.
I understand the instinct.
I did put forward a call for consciousness benchmarks.
Can I just click this off?
Yeah, if we had some common bit.
I don't think you're going to find an uncontroversial benchmark.
But even having a few different benchmarks for different kinds or aspects or theories.
That's absolutely right.
I agree.
Then we can train up more and more systems, get closer and closer on those benchmarks.
Then we can argue about whether they're conscious.
But the subjectiveness here, you cannot look into another thing.
It's secondary measurements.
Exactly.
It makes it hard.
It would be a lot easier if we had the consciousness meter.
I'd just wave it at your head, searching for consciousness.
Yeah, exactly.
We'll get a readout.
Okay, there is self-consciousness.
But we don't have this.
But physics is the same.
You don't see atoms.
You don't see all this stuff.
David, can I just start with a question?
A lot of your arguments about the existence of intelligence are based on functionalism.
Which is to say, similar to if you read a book by Peter Norvig.
It says, oh, an intelligent system has planning.
It has reasoning.
It has sensing.
It has perception.
Therefore, it's intelligent.
And I feel that this is a form of behaviorism, but a slightly more sophisticated form of behaviorism.
And then I feel that some of this has been projected into, I think, your conversation the other day on consciousness.
Which is to say, first of all, could we have a Turing test for consciousness?
But also, could we have a functional approach to define whether something is conscious?
And the thing is, I'm a little bit, my intuition is to be skeptical of this.
But I have to be completely honest.
I have no principled way of describing why I'm skeptical about it.
So if you could play devil's advocate, how would you attack your own position on that functional view?
Well, I actually make a distinction between consciousness and intelligence.
Intelligence is a matter of the outputs you produce, given any inputs, especially goal-directed behavior, the ability to solve problems.
Consciousness is the subjective experience.
So when it comes to consciousness, the easy problems are explaining the behavior, the reactions.
The hard problem is explaining subjective experience.
So actually start by drawing the distinction that you want to draw between, say, experience and function.
That said, when it comes to consciousness in other people, we don't have direct evidence of their consciousness.
All we can go by is their functioning.
And so in general, what another person says is evidence of their consciousness.
If you tell me that you're conscious at a given time, or even if you show a certain sophisticated, reactive behavior to a certain stimulus,
we'll usually take that as evidence that you are conscious.
So I'd say that the view I was arguing the other day was not that we should define consciousness as some function,
but we should look for those functions which are the best evidence of consciousness in other people, but also in animals and ultimately in AI systems.
Would it be fair to say that you would argue that it's a sufficient condition, but not a necessary condition?
Function is a sufficient condition for experience.
I mean, not sufficient across the board.
The famous philosophical zombie will be a creature which has all the functioning, but none of the experience.
So that's at least conceivable.
My own view is that those systems can't exist in the real world.
So in the real world, certain kinds of functioning may be sufficient conditions for consciousness.
The coin of the realm in the science of human consciousness is verbal report.
If someone says they're conscious, you believe them, unless there's reason to believe otherwise.
Or if they say they're conscious of a given stimulus.
But humans we know are conscious.
It makes it easier.
Non-human animals are already tricky.
At least they're related to us.
So we think we've got some grounds for this.
But AI systems, we start from the position of being totally unsure whether they're conscious or not.
So, you know, Turing had the genius of proposing at least one sufficient condition behaving just like a human.
To me, that would be good enough.
Fascinating.
But what short of that actually counts, then we start to argue.
Yes, indeed.
Well, I mean, with the functionalism, it's really interesting how much of it is to do with human intelligibility rather than the actual thing which exists.
I feel that a lot of it is taking something which is very complex and beyond our cognitive horizon and creating or reducing it to a cognitive framework which is compatible with the templates that we have in our own brains.
Consciousness itself is, yeah, very mysterious and subjective.
And on the other hand, our tools for understanding the brain are mostly cognitive and informational.
But there do seem to be strong correlations at least between the two.
You replicate all that informational or cognitive structure.
You replicate the brain.
You will replicate consciousness.
Yes.
So the two seem to be tied together.
What we need to do is find the neural correlates of consciousness.
Those bits of the brain that seem to go along with being consciousness, being conscious, and maybe even more fundamentally, the informational correlates of consciousness, the very general mechanisms that support consciousness.
And then if we find the informational correlates, say, of consciousness in human beings, maybe then we can project those to AI systems, see what we do know very well.
The informational properties of an AI system try to build an AI system that has the right kind of informational properties to serve as an informational or a computational correlate of consciousness.
Then we might have some evidence that is conscious.
It still won't be conclusive because how do we know for sure that that is, in fact, the computational or informational correlate of consciousness?
But it would be some evidence.
If you could say something to Ilya Sutskeva right now, what would you say?
Well, I want him to define the scale of consciousness for us.
If neural networks are going to be slightly conscious, then fantastic.
That means we've got a scale.
And Ilya is a smart guy, so I want to see his definition of the mathematical scale and the mathematical structure of consciousness so that these neural networks come in at a certain epsilon.
I myself am quite sympathetic with the idea that consciousness could run very deep in nature.
If panpsychism is true, nothing is at zero.
Even particles have got a little bit.
On that measure, today's neural networks, even yesterday's neural networks were slightly conscious.
Today's neural networks maybe slightly and a bit unconscious.
The problem with panpsychism is it's such a parsimonious and beautiful theory.
We've just done a show on the Chinese room argument.
And in the simplest possible terms, what would you say to someone like Saul who says that subjective experience requires biology?
I mean, it's a coherent hypothesis that we don't know what the physical basis of consciousness is.
One view is just biological, another view is informational, and then there are other views besides.
I strongly inclined towards the informational view, partly because the structure of consciousness I find seems to be so deeply tied to information processing.
My visual consciousness has a very complex structure of objects and qualities located in space.
It seems to very closely track the informational properties of the visual system.
And it's just hard to see for me how the biology is essential.
For example, we can imagine gradually replacing the biological components of our visual system by, say, silicon chips that play the same role,
keeping the information processing the same.
To me, it's very plausible that would actually preserve consciousness.
We even have the thought experiment where you gradually replace every neuron in the brain by a silicon chip.
I suppose Saul and others are going to have to say that your consciousness gradually degrades during this process.
But I think if these silicon chips are good enough simulations of the neurons that they actually replicate all the information processing,
by far the most plausible thing is this will replicate the consciousness too.
And if it's possible that any old silicon system can be conscious, then it looks like that will refute the biological view that biology is required.
Fascinating.
And could you also just touch on why you think consciousness is one of the only examples of a strongly emergent phenomenon,
which is to say not deducible from truths in the lower level domain.
I know we touched on this when we spoke last time, but I would love to just get a slightly fresher perspective.
It's not going to be fresh, I'm sorry, because this is a very old issue for me.
Nothing I can say about this will be fresh.
But most of the things, most of the phenomena we try to explain in the world are matters of structure and function.
And when it comes to, say, explaining life, we need to explain how it is a system can reproduce, how it can adapt, how it can metabolize, how it can grow.
These are all kind of functional questions analogous to the easy problems of consciousness.
And those are perfectly well suited for explanation in physical terms, because physics gives you a complicated story of structure and of dynamics.
But when it comes to consciousness, it looks like we have to explain something that goes beyond structure or function dynamics.
We've got this, those are the easy problems when it comes to consciousness.
We also have this hard problem, subjective experience.
It seems even once you explain the structure of the function, the dynamics of the cognitive system, there's still this further question.
Why subjective experience?
I think what makes it unique is it's not really a question about structure and function at all.
Yeah, I think some people who believe in global workspace think that it is somehow essential for consciousness.
Maybe that's what someone like Joshua Benjio thinks.
I heard you on the content say once, yeah, if we got to powerful enough systems that didn't need this low capacity workspace, then maybe we wouldn't need consciousness anymore.
And there I think the thought is there's somehow tying consciousness by definition to slow processing in a low capacity system to system.
But I'd be more inclined to say we still have subjective experience.
It would just be very fast and rich.
There's already a debate between people about how fast and rich is consciousness poor and slow and fast and rich.
I'm kind of on the side where it's already pretty fast and pretty rich.
It feels like that for people, I suppose.
It actually relates to one of my, well I guess the next question, which is I'd heard recently talking about animal ethics and talking about insect consciousness.
It just hadn't really occurred to me that any other animals might have a faster rate or even a richer consciousness than people might.
But I just heard it sort of speculated that well.
Insects might have.
Insects might, you know, maybe it's the way that their brains are smaller and neurons are closer together.
That means they might actually have a richer and deeper experience than we do.
An individual hand would have a richer conscious than it would be.
I don't think I've heard that one before.
It was the first time I'd heard it.
What do you think about it?
Well, it's certainly people saying by sheer weight of numbers, the number of insects in the world means that insect consciousness would continually outweigh human consciousness.
We should do it all out of resources to making sure insects don't suffer.
Yeah, I haven't heard the reasoning in favor of individual insects being more conscious.
It's been like this.
So simple and integrated maybe that their consciousness has a certain purity.
I guess it is related to these notions of integration and speed and messages are being sent and signals being processed.
I know zero neuroscience to actually substantiate any of this.
My colleague Jeff Sebo is starting up a center at NYU.
We saw the NYU talk.
He introduced it.
One of the specialties is going to be non-human consciousness in animals and his real specialty is insects.
So I think we're going to be a lot of focus on insect consciousness there.
And I guess if I can ask one more, given your personal credence probability estimates, do you think there are significant ramifications for studying AI and how researchers go forward like to studying, probing, deploying language models?
But you're talking about some ethical and safety issues.
I mean, yeah, there's a million very familiar issues that already arise for any human level AI, whether it's conscious or not.
We've got to make very sure it's going to be aligned with our goals.
It's going to be so powerful that it's going to have the ability to bring about whatever it wants to bring about.
So we've got to make sure it wants to bring about the right things.
I guess one question is whether consciousness changes that in any fundamental way.
I'm not sure that it fundamentally changes the questions of, say, of our safety, which may work the same whether the systems are conscious or not.
It does add a whole new moral dimension, which is worrying about the AI systems themselves.
It turns out that we're building AI systems already, which are conscious, that we may need to consider, well, how are we training them?
Does it turn out that every time they get negative feedback, every time they backpropagate, they're actually suffering?
I certainly don't rule that out.
And yeah, that would be potentially a moral disaster.
And even if they're not conscious yet, if they're going to be conscious in, say, 10 years, could it be that in building these more sophisticated language models,
we're producing an enormous amount of suffering.
I mean, I think this is the case where I think we want to have better theories with a better theory of consciousness.
We might also have better theories of the basis of certain kinds of consciousness, like affective consciousness, suffering, and so on.
Perhaps that could be used to try to organize the way we use these systems so that the negatives taste they go through or minimize.
But that's a bit of a pipe dream at the moment.
I think the ethical consequences are fairly enormous.
I'm not myself an ethicist, but I do hope that ethicists think pretty seriously about these issues.
Jeff Sandor at NYU is going to be thinking a lot about exactly these issues, the moral consequences of consciousness in AI systems as well as animals.
Something like that's an issue you're pretty interested in, too.
I'm interested enough, certainly.
I care a lot about safety of systems.
It's mainly why I work on explainabilities, because I think this is instrumentally useful.
What sort of thing are you doing with explainability?
There are really so many angles.
The work we're presenting here is focused on supervising model explanations for tasks.
You might get models doing some complicated tasks.
It gives some kind of explanation of its behavior to you.
What we're doing is lining those explanations up with human explanations for the same task.
You're trying to get the model to behave in a more human-like way.
It gets roughly towards aligning its own behavior and reasoning, particularly with human reasoning.
This is actually changing the person thing in the model to make it more and more explainable.
Not just giving feedback at the final output where it's correct or incorrect, but trying to supervise the features and reasoning process.
Does it work?
Yeah, it works.
And do you take a hit in performance?
No, it improves performance.
It improves generalization because you specifically want to come up with tests
where the only way you're going to generalize successfully is by using the correct reasoning process that a human would.
So it improves performance.
Okay.
Yeah.
That's good.
Is this true in general and explainability work?
There's sort of a discussion.
A lot of terms get thrown around.
There's this post hoc explainability approach, which the idea is to apply it to any model
and you don't have to change the objective.
You don't have to change the training.
You don't have to worry about losing performance.
There's another basically camp at this point that is suggesting the post hoc isn't enough.
What we need to do is change the training.
And maybe you do lose a little bit of performance because you're constraining the optimization.
You don't just care about task performance.
You care about this additional objective.
The objectives have to trade off a little bit.
Then maybe you lose performance.
I think the debate at a high level is kind of interesting, but for any particular problem,
it's like, well, I guess the point I really want to make is the debate's a little too high level.
The actual details of the problems don't match up well to this general accuracy and
to really trade off.
So what's big and new and exciting and explainability work?
I think some of the interesting stuff is getting really low level with representations and weights
and figuring out, for a given representation, what does it encode for?
The single unit level?
Single neuron, single layer.
Some of this type of further is localization work.
So you have a specific behavior in mind.
Do you want to know what component in the model is responsible for this?
I always feel that seems like a very cool project, but also seems so limited because
so much of what the model is doing is not of a local level.
Right.
So you want to build up the understanding.
So you might start with smaller units and then ask, this is a little bit of like
Chrysalis circuit research.
Try to understand what the brain is doing by looking at what single neurons are doing.
It helps a little bit.
You get inside.
This is why neuroscientists have voxels that are like a million neurons.
You might need a higher level of granularity to start with.
But it turns out to be really hard to do explainability work at that.
Yeah.
So single layers are somewhere in between?
Things go in both directions because maybe this entire layer is responsible for something.
The model only has 30 layers.
One of the layers is responsible for something.
So then you might meet in the middle where it's like these neurons, but also this layer.
You try to exactly define the scope of the behavior or the function on this being carried out.
Yeah.
There's a lot of analogies to neuroscience for it because it is like reverse engineering
to a larger extent.
Cool.
That's exciting.
Yeah.
