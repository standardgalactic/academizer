Hattie Jo, a PhD student at the University of Montreal and Miele, has set out to understand
and explain the performance of modern neural networks, believing it to be a key factor in
building better, more trusted models. Having previously worked as a data scientist at Uber,
a private equity analyst at Radar Capital, and an economic consultant at Cornerstone Research,
she's recently released a paper in collaboration with the Google Brain team titled,
Teaching Algorithmic Reasoning via InContext Learning. Now in this work, Hattie identifies
and examines four key stages for successfully teaching algorithmic reasoning to large language
models, formulating algorithms as skills, teaching multiple skills simultaneously,
teaching how to combine skills, and finally teaching how to use skills as tools.
Now through the application of algorithmic prompting, Hattie has achieved remarkable results
with an order of magnitude error reduction on some tasks, compared to the best available
baselines. Now this breakthrough demonstrates algorithmic prompting's viability as an approach
for teaching algorithmic reasoning to large language models, and may have implications
for other tasks requiring similar reasoning capabilities. Anyway, I hope you enjoy this
conversation that I had with Hattie over at NeurIPS a couple of weeks ago. Enjoy.
Amazing. Well, Hattie, we're here at NeurIPS. Yes. Welcome to MLST. Thank you. Please introduce
yourself. I'm Hattie Jo. I'm currently a PhD student at NILA and a part-time researcher
at Google Brain right now, as well as student researcher. Oh yeah. Fantastic. And I've been
reading your paper, Teaching Algorithmic Reasoning via InContext Learning. Give us the elevator,
bitch. The elevator, bitch. So I think typically in the past, people have thought that large
language models are not great at doing symbol manipulation or actually doing reasoning the
way humans do. And a common example people point to for this failure is to show that models can't
even do addition properly, even though it's trained on billions of tokens, billions of parameters.
And in this paper, we try to teach the model how to solve these problems by learning an algorithm
for them and see if it can generalize to a harder problem, generalize out of distribution,
which is a common way to see if the model is actually using the correct algorithm to solve
these tasks. So it's not just like fitting to the training distribution and finding
spears features, it's actually executing an algorithm. So we do that through some prompting
strategies and show that actually can learn how to do addition, contrary to previous belief.
Yeah, I wanted to speak about the previous belief. So with large language models, I'm
vacillated back and forth from being skeptical and being very optimistic. I was originally very
skeptical and I'm becoming more optimistic as time goes on. I interviewed a lady from UC Irvine
called Yasaman Rezegi and she did a really interesting bit of work actually kind of comparing
the term frequencies as they appeared in the corpus to the arithmetic as a function of the
number of digits. And because OpenAI never released their data set statistics, so it wasn't possible
to do that. And she found that there was like a linear correlation. But I think what's really
interesting now in your work and some of the other work like Chain of Thoughts Prompting
and Scratch Bad Prompting, that now we're in this new regime where we're kind of telling
the language model. It's almost like we're treating the language model a bit like a kind of compiler
and we're giving it the program and then it's actually extrapolating and it's doing things that
it wasn't directly taught. Right. Yeah. I think that's the first step, right? You want to,
like if you can give the model a program, you want to see that the model can use that program
and apply it to new situations. And so that's kind of the reasoning out of the distribution
component. But I think also ideally you can imagine we want the model to be able to discover
algorithms that we don't know ourselves and, you know, that's a whole other frontier, right?
But this is the first step too on that path, I think. Okay. Well, why don't we start by defining
algorithmic reasoning? Sure. So I think about it as just the way you solve a task is by
using a particular algorithm. And algorithms are input independent, which means that
basically for any input distribution, using that same algorithm will get you the right answer.
And so performing reasoning by running an algorithm is what we refer to as algorithmic
reasoning. And of course these right now apply to tasks that can be solved by an algorithm.
But you could also imagine cases where you have a soft algorithm where the steps are not so
rigidly defined, but there is an overarching problem solving structure that you can follow.
Yeah. Could you refer to that? I mean, maybe I would refer to that as like inductive
algorithmic reasoning. So it's where, so on one side of the spectrum, you actually have the
algorithm and you write every single step explicitly using some kind of code. And now
we're talking about this regime where we are giving examples of an algorithm and we're describing the
steps somewhat vaguely when we're trying to be as clear as possible using language. So
where are we kind of following on that continuum? Well, I'll say you take an algorithm of addition.
Yeah. But you actually, so you have an algorithm defined where you like start by taking the
first digit, do something with it, and then get an answer and then move on to the next one.
When I think about a soft algorithm, it can look something like that, except
you take the digit and the something you do with it is not defined explicitly, but it might
require some abstract pattern matching that large language models are particularly do that.
So it can use the same process of breaking down the problem in a very specific way
to generalize, but the individual steps are not actually encoded in code because it's abstract.
So if you can do that, then you can use this approach to tackle tasks where
you can't write an algorithm or you can't write a program for, and that's like very exciting,
I think. Can you give me an example of where we can't explicitly write the algorithm for something?
Well, it's hard to come up with a good example because it's supposed to be
abstract in some sense, but these will not be questions of
that you would normally tackle with by writing a program. So like it could be a soft reasoning,
maybe like, I guess maybe even the great school math board problems, there is no algorithm
or a simple algorithm at least that you can write that will solve like math work problems,
which are like, if you have four apples, I gave you twice the amount of apples you have,
how many do you have now? But if you define a way to tackle these questions at each individual step,
the model can decide like how to apply that flexibly based on the particular question.
Yeah, that might be a good thing to have. But yeah, right now I don't know,
there is no good benchmark for these kind of things.
In your paper, you came up with a new algorithmic prompting technique and you designed some
experiments and your technique works significantly better than some of the other in-context prompting
techniques. Can you sketch that out for us? Yeah, so the intuition is very simple actually.
When we look at the addition algorithm, the one that we learned in school,
we know that you start with the rightmost digits, you add them up, if it's greater than 10,
you have a carry of one, and then you add the carry to the next sum and so forth.
So a scratchpad like approach for addition will show the trace from running this algorithm.
So it will show the first sum is this, and the carry is this. But it doesn't explain how those
values are derived. But for us, it feels really natural because we're so familiar with it. But
for a model, trying to infer the rules from seeing a couple examples of it, that's a very
underspecified problem. There are many rules that could explain perfectly these observations.
So algorithmic prompting basically explains each step of an algorithm using some examples,
and within each step tries to be as detailed as possible and tries to disambiguate as much
as possible what you want the model to do. And it turns out that when you provide more details
to the model, you're sort of constraining the model's interpretation of disinformation so that
there's only one way to apply this. And that with that, you can get more robust behavior from the
model and reason very well out of the distribution. Yeah, because it's often said that deep learning
models do not reason. And I think what people mean by that is that you get this phenomenon of
shortcut learning and models do the right things for the wrong reasons. And it seems to me that
what we're doing here is by imputing the kind of the structure of how to reason into the prompt,
we're robustifying its behavior out of distribution. Yeah, well said. That's exactly right.
Fascinating. So what do you think of this problem of shortcut learning in general? Because, you
know, like Melanie Mitchell said, there are two modes of understanding, essentially. We have
this anthropomorphic mode of understanding, which is using causality and it's very sample efficient,
and we have a way of understanding the world. And we have a bit of an intuition that large
language models don't reason the way that we do. But is it necessarily a problem? And is it
cheating in your view using in-context learning? Or do you think that because we haven't had to
train the model again, what's the problem, right? And is it an issue?
Yeah, I mean, I guess there's many forms of reasoning and algorithmic reasoning is only a
subset of that. And I think if the model can output the reasoning process and show that the
answer that arrives at is following the output of that process, then it's hard to argue that it's
not doing reasoning, right? It might work differently under the hood, but it follows the
similar process. Now, you know, some methods you output a rationale, but the final answer is
actually different from what the explanation suggests it should be. And maybe you can point to
that and say, oh, the model isn't actually using this. It's just like giving you something that
you asked for, but then the final answer is still using a shortcut. But with the algorithmic reasoning,
we see that that's not the case. And so, yeah, the more you constrain things, maybe the more
you remove shortcuts from the model. But an interesting question, I think, is you can get
this behavior using in-context learning, which I think you, I suspect you can't really do from
fine tuning or some sort of weight training. I think when you do that, you'll most likely just
overfit on the training distribution. I wanted to ask you, you know, it's a bit of an open-ended
question to say what's going on inside large language models. But what's so exciting to me
is that you get all of this emergent, strange behavior. No one would have imagined five years
ago that we could do all of this prompt engineering on a kind of autoregressive language decoder.
And the model is trained to do something really quite trivial, yet as a byproduct, all of this
crazy stuff emerges in its internal representation. And all of this algorithmic reasoning capability
seems to be like a side effect of that training. How does that even happen?
Oh, that's a good question. It's possible that in order to fit that large training data set,
the pre-training data set, you have to find regularities in content, I guess, that humans
generate. And I think some of these abilities come out of those regularities that you learn.
So the ability to refer to the pattern of a previous passage in context and, you know,
see what the relationships are in that pattern and apply the same relationships when you input
that circuit, I think, is just very useful as a way to summarize that large data set. And so,
because you're forced to compress all that data into a model, a set of weights,
I think these regularities emerge somehow. I don't know exactly how. But I think, yeah,
I mean, this is an interesting speculation because then you can say with much larger models where
there is no capacity constraint at all and you fit on the same data set, it's not going to learn
very interesting behavior because it's able to just fit the model without capturing the underlying
patterns. And maybe that's why you actually need more training data and training longer rather
than like the optimal scaling is not right now in the model size, the amount of data.
Because you want like the right bottleneck for your representation. And I think maybe that's where
these emerging capabilities are coming from. Interesting. And also the amount of training
as well as the amount of data. I wondered what is your intuition on the practical kind of
computational limitations of large language models. So at the moment, they're trained to
transform us. And there have been some pretty cool critiques of connectionism by Fodor and
a few other people. And it's basically along the lines of neural networks can't represent
infinite objects, which kind of distinguishes them from Turing machines. So they can't compute
the nth digit of pi is a fairly good example. But the amazing thing is that we're doing all of this
stuff with algorithmic reasoning. And we haven't found the ceiling yet. It's just getting better
and better. And I think it's almost creating this. Well, I mean, I'm becoming quite hopeful,
actually, because I don't know where the limit is, but I suspect there is a limit quite soon.
How do you think about what the realistic computational limits are?
I think the fact that now we have in context learning is interesting because
that allows us to have, I guess, adaptive amount of computation. And so if you have,
let's say you have infinite context length, then you can sort of maybe do infinite computation
in that case, right? Now, is infinite context lens possible? Probably not. But then you can
find clever ways to distill that information. You can find clever attention mechanisms.
And so I think maybe there's a computational limit, but you can always find new ideas that
make existing methods more efficient. And so yeah, I have no idea when you would hit that
limit, but it's probably very far into the future. Amazing. And so, Hattie, we're here in
Europe. What have you taken from the conference so far? And what are you most excited about,
you know, going forward? Yeah, I don't know, because I haven't checked out the posters
very much yet. But I'm excited for the math AI workshop, which is many other papers exploring
this idea of doing math with language models. And yeah, there's some, you know, very, very
impressive work there. So I'm excited for that. I'm excited to meet people and see what they're
thinking about and maybe get some ideas for what to work on next. Yeah, I'm also really interested
in the math stuff I spent about an hour speaking with Marcus, is it Marcus Barb,
who works under Christian Sagadia, I think he's in your group. Yeah, they're doing some,
that's right, yeah, they're doing some really interesting stuff with basically doing mathematical
conjecturing, you know, like representing mathematical expressions in large language
models and being able to generate new ones. It's the kind of thing that you, again, would think
was science fiction five years ago and like remarkably it works. Exactly. And then by the way,
with the mathematical conjecturing, Marcus was saying that unlike with large language models,
it only has to be right one in a hundred times because they can formally verify it.
So it's almost like the bar is actually lower in that sense. Right. Yeah, I mean, that's where
the language models informal reasonability is really useful. Right. Yeah, like the pattern
matches actually useful in a lot of cases. That's really cool. Cool. Well, actually,
this has been amazing. Thank you so much for coming on the show and telling us about your
research. Oh, thanks for having me. It was fun. Looking beautiful. So Marcus, it's so nice to
meet you. Can you introduce yourself? Hi, I'm Marcus. I work for Google Research together with
Christian Segedy in the end to formal team. Generally, we're working on trying to solve math
basically by trying to translate natural language mathematics into formal
mathematics and in these formal representations of mathematics, we can
check proofs and use that as a feedback signal for the understanding of mathematics.
And most recently, I've been working on long context models like the memorizing transformer
trying to get these models like makes models sensitive to the exact definitions and other
lemmas that they might use for your improving. And that's an ongoing effort, hopefully more
available soon. Can I just say, I'm so jealous that you work with Christian. I mean, folks
will remember that we had a conversation with Christian. I think it was about 18 months ago and
it's one of my favorite episodes of MLST. So you're a very lucky man indeed. Yes. Yes. It's great to
work with Christian. It's a lot of fun. Amazing.
