Hi, I'm Peter Domingos, I'm a professor of computer science at the University of Washington
and machine learning researcher, probably best known as the author of the master algorithm,
popular science introduction to machine learning.
I'm here at NeurIPS 2022 of the vast amounts of stuff that's happening here.
The two that I found most interesting and are closest to my own research are Neurosymbolic
AI and symmetry-based learning.
Okay, Professor Pedro Domingos, it's an absolute honor to have you back on the show.
Pedro is a professor at the University of Washington and give us a quick introduction
to yourself, to your experience here at NeurIPS so far, and what's top of mind for you?
I'm a machine learning researcher, I've worked in most of the major areas.
I've also written a popular science book on machine learning called the master algorithm.
I'm having a lot of fun here at NeurIPS listening to various talks like David Chalmers'
On Consciousness and Geoff Hinton's Nonsleep and looking forward to the rest of it.
Awesome, I'd love to get your thoughts on Chalmers in a bit actually, but the first
thing I wanted to talk about just because it's top of mind is this whole galactica situation.
So first of all, I was speaking with Ian the other day and I think it's a little bit unfair
that Meta really bear the brunt of this. OpenAI have just released this new chat GPT
bot which suffers from similar failure modes and it just kind of feels that they're not
getting anywhere near as much stick as Meta is. Well, I think I agree with you.
I think the brouhaha about galactica is way overblown. That system is really largely harmless.
It's just another large language model that's designed for actually something that to me as
a scientist is very interesting. I would love to have a system like that to help me out with
certain things and I think it's a step in the right direction and I think the brouhaha, however,
is an instance of people jumping the gun on a lot of these AI things in a way that to me is very
excessive. Having said that in a way they set themselves up for it in a way that they need
and have, they kind of over claimed what it did and the problem with these LLMs is that they
generate a lot of stuff that looks good but can be completely wrong and in a way there's no
worse place to do that than in writing scientific articles. So when they came out with it, they
should have been more careful about how they frame it. I think they took concerns sort of like this
competition and one upping each other on who comes up with the frilliest demo and that kind of
backfired. So they shouldn't have had to withdraw it. I think that's all pathetic and
hopefully they've learned the lesson that next time they will do it slightly differently.
Yes, okay. Gary Marcus has been very loud about this on Twitter so he's really pushing the point
about misinformation and the thing is as well, I don't want to characterize the ethics folks as
having monolithic views because they don't have monolithic views and I also think that a lot of
the ethics guidelines for large language models are very reasonable. I interviewed the CEO of
Coheir the other week, Aidan Gomez. I went through their terms and conditions and policies all very
reasonable. The only sticking point for me is the misinformation one. I think the kind of the moral
valence of it is in its use and especially with misrepresentation. I don't like this paternalism
telling me what's good for me. I've just lost out on using a really cool tool basically.
I completely agree with you. I would take it even further. I do not want other people deciding for
me what is misinformation and what is therefore allowed to be said because it's misinformation
or not. For a couple of reasons. One is that these people who claim to be big critics of
misinformation, a lot of them are misinformers themselves and the bottom line is that you always
have your ideology that informs what you think is true and false and I don't want any but every
one of us in a democracy should be deciding for themselves what is true and what is false and
what is valid and what isn't. I have no fear of attempts to misinform me as long as I have a
multiplicity of sources. The biggest misinformation danger is when you have only one monolithic
source of truth, whatever it is, which is unfortunately what a lot of these anti-misinformation
people I think consciously or unconsciously want. Give me ten things, nine of which are
misinformation. I can do the job of figuring out which one I think is valid. Give me only one of
those things and chances are nine in ten that it is misinformation and then I have no chance to
overcome it. So this whole attack on things because they're misinformation and I mean I understand
the impulse. They're like why have all this falsehood flying around but the way to overcome that
falsehood is not by censoring it. You should know this. We should be having to refight all of this
over again in the context of social media and large language models and so on.
So you said something really interesting which is that this notion of a pure truth or a monolithic
truth and there's this concept of epistemic subjectivity or things observe a
relative, even complex phenomena like intelligence, no one understands what it is. You can't reduce it
to one particular thing. People have different views on it. So this notion that there is a pure
monolithic truth of the world I think is horrifying. Well I would put it slightly differently.
So first of all there's a question, is there one reality or not? Is there truth or is there
my truth and your truth? I understand the impulse to talk about my truth and your truth but I think
so what is really true we don't know but as I think the most useful including socially useful
working hypothesis is that there is a single reality and the single truth but it's extremely complex
so no single one of us can get at it. So what we need is many different people coming at it
from different angles but with the premise that we need to try to make these things consistent.
So just saying oh we have different truths and there's no reality that is actually very counter
productive because it gives everybody a path to just believe whatever wacky thing they want
and then the consequences of that when you have to make the real decisions are very bad.
At the same time I agree with you if I think that I have access to that truth and everybody just
needs to you know couch out to it that is very dangerous so I think we need to entertain these
two ideas that there is a truth but it's very complex and no one has a monopoly on it and the
key is you know like objective truth is what different observers can agree on and now we can
figure out what it is that we agree on and that way we make progress in understanding reality
and we also tend to make more of the right decisions because we're closer to the truth.
Okay but do you do you see it as I mean I think the reality thing is is interesting but do you
just see it cynically as gatekeeping as in having a clerical class controlling.
Oh absolutely no absolutely I mean and if that that's precisely the danger that I was referring
to is that if you let me put it this way if ever there is and this is a commonly mooted
proposal right and not even proposed like are we going to have a truth commission
of people who decide what is true on whatever Twitter or something right that is that that is
a really alarming thing because there is no commission that can do that what they're going
to do is they're going to impose their version of reality on everybody else which unfortunately is
what a lot of these people want to do they convince that they have the truth and they want to impose
it on the rest of us and that is really alarming we know historically what happens when people
succeed in doing that. Yes yes but I suppose my point with the gatekeeping is it almost gets you to
the actual truth of the matter is irrelevant it's actually about power but what's your take on I don't
know whether you think this is putting it too strongly but this being a form of industrial
kind of gaslighting kind of kind of you know in an Orwellian sense trying to shape people's
reality through you know language culture and interactions on the internet. I think a lot of
it is deliberate some of it is I mean I'm an optimist about human nature at the end of the day
maybe and you know maybe with justification maybe without I think so there's this postmodern view
that it's all about power and it's certainly partly about power but I think a lot of the people
doing this unfortunately or maybe fortunately they are they're not seeking power for its own
sake they have a set of beliefs that they think is right and then the means you know the unjustified
the means right that's the problem so that gatekeeping you know and that gaslighting happened
not because not for their own sake they happened for the sake of a cause and now there's two
problems with this is that these days the causes on behalf of which this is being done in my view
are largely wrong right but whether they're right or wrong right the problem is that this is just
noxious in its own right and also then a lot of sort of like again personal desire for power and
promotion and prevailing over others then of course you know hitches a ride onto this. Yeah
interesting I mean we'll get we'll get into consequentialism in a minute because I think
there's quite an interesting journey we can go there but I wanted to cite Francois Chrolet
I'm a big fan of him he just tweeted saying I'm not too concerned of whether what I read is right
or wrong I can figure that part out myself I'm interested in things that are useful thought
provoking novel sometimes the most creative thinkers have a bias towards wrongness but
they're still worth reading would you agree with that? Yes I largely agree with that so as I was
saying before you know I can tell for myself or I can do that exercise of freaking out what
is right and wrong the most important thing that I want is to not miss out on things that I
don't want to miss out on like you know the known unknowns and the unknown unknowns
the biggest killer is the unknown unknowns so the if anybody trying to learn or understand
something therefore person or organization or society right if all they do is move the unknown
unknowns to known unknowns they've already gone an enormous distance right and so I appreciate
people who I disagree with first of all because that's how you sharpen ideas yeah but also because
they they they may just bring things to my attention that if we were all conforming to the
more majority view would not come to our attention and then those more often than not are the ones
that kill you yeah that's so interesting I mean that there's a real analogy here even this might
be tenuous but between symbolism and and and connectionism or you know rich Sutton said we
shouldn't be hand crafting our AI systems we should kind of let them emerge and it's a similar
thing with our moral framework but you're kind of saying that it should be emergent from low-level
complexity and diversity and and interestingness and and there is another school of thought which
is that we should be top down and and we already have a representation I actually think it should
be it needs to be a combination yeah we need to have both this is one of those debates that in
some sense puzzles me because to me the obvious answers that we need both and then and then and
if you read the master argument this is what I do I look at the different paradigms of machine
learning and I don't come out in favor of any of them because actually I think we need ideas from
all of them and then we need to combine them into something coherent and like if you look at psychology
like your brain does bottom up and top down processing and if it only did one of them either
one it wouldn't work and I think as we try to build a larger intelligence it's the same thing
we definitely need the bottom up part and you know by volume the ball the bottom up part is
going to be bigger so if you could only have one that would probably be you know the choice
but the but the but the top down part is also very important if you go all the way back the top
down part probably started this bottom up and got synthesized and improved but now we need that loop
the loop is actually very important well that's interesting so in in in your book I guess I want
to sketch out different types of AI architecture so you know you get universalist to this kind of
deep mind idea that a very simple underlying algorithm could produce everything and then you get
you know hybrid folks on the other side of the spectrum and then there's an integrated approach
in the middle like where would you kind of place yourself on that continuum I would place myself
very much in the frame of mind well let me put it this way I don't know but which is nobody does
right if somebody tells you that they know how we're going to get to intelligence you should
be suspicious right away but what do I think is the most promising approach and the one that
ideally would be the best one if we can pull it off it's there being a single algorithm yeah
so at that level I very much sympathize with with what is effectively deep minds agenda
now we're at part with a lot of these people is that I don't think the deep the the the algorithm
that we need is as simple as many of those people think it is and I don't think it exists
it probably is probably the case that the algorithm that we really need at the end of the
doesn't even look that much that like any of the things that we have now so I think
hopefully there is such an algorithm but we're still far from it interesting I mean that they
they would cite the example of evolution as being a very simple underlying algorithm although
Ken Stanley would say that people misunderstand evolution so I agree with them at that level in
fact in the master algorithm I have a chapter where I go over you know the objections and the
reasons to believe that there is a master algorithm a lot of the majority of the people even in the
field are skeptical of that notion even though I would claim that effectively that's what they're
pursuing people like Rich Sutton and Jeff Hitton I asked a bunch of people before I wrote the book
and they do believe in this idea of having a master algorithm a lot of people believe
that but intuitively a lot of people believe that no it doesn't there is no such thing right
and I I understand that intuition but I think it's
well I don't think it's a well-founded intuition let me put it that way but in in a sense we know
there is such a thing because look at cellular automata look at what we've already done with
with deep learning I think the context is is there such a thing that will produce what we want well
yes so yeah to take your you know your example or you know deep minds example of evolution
right yeah I do so in the book I mentioned empirical evidence that there is a master
algorithm an exhibit one is evolution right if you think of evolution as an algorithm which by
the way is a very old idea I think it was George Bull that said you know God does not create animals
and plants he creates the the algorithm by which animals and plants come about he didn't use the
word algorithm but that that's essentially what he said this I think is right on right and another
example is your brain if the algorithm doesn't have to be something as simple as as backprop
and you're a materialist like most of the scientists are your brain if the master
algorithm is an algorithm that can learn anything you do then your brain is that algorithm right
but then there's another one which is even more fundamental right but I think from the point of
view of this debate is very illuminating which is the laws of physics why stop at evolution right
the laws of physics are the master algorithm right evolution is very complicated in fact what I think
about you know evolution in AI currently is that evolution in reality is much more complex than we
give it credit for yeah which is why a lot of our current generic algorithms don't work that well
but the laws of physics at this level are much simpler and if you think about it from the laws of
physics comes evolution and comes all the intelligence that we have it's very intriguing why
that happens and why the laws are such that that happens but even just the laws of physics already
a master algorithm now what you what you could say and many people immediately say is like oh but if
you start from there you know you'll never get anywhere right but then you can say like evolution
is the laws of physics sped up in a certain direction and then our reinforcement learning
is like evolution people have pointed out the similar except it's faster and in a way what
we're trying to do now in machine learning is the same thing yet again except only even faster
but what are the consequences of I mean let's say it is actually a very high resolution algorithm
so it's something that appears to be completely unintelligible to in respect of the output phenomena
is that is that even a good place to be because you know just like with cellular automata
there's no real paradigmatic relationship between the underlying rules and the emergent
phenomenon right so is that really even something we want you know I think there is so we don't know
but I think people and this is very common among connections is to say this stuff is also complex
that we can't possibly have a handle on it we just have to let it happen and I think that is not
giving enough credit to to our human brains right we are incredibly good at making sense of things
that in the beginning don't I mean over and over and over in the history of science and technology
you start out with things that you don't understand very well at all but then over time we kind of
change our representation of the world to make those things actually be intelligible to us
and and we should not at priori assume that that's not going to be the case here so for example
cellular automata amazing things come out of whatever the game of life that seem completely
disconnected from those but they aren't right and you know there's you know there's there's various
depths at which you to go you could go into this they will probably at the end of the day be some
large element of this that we can't figure out very well but we can figure out enough that we
have a handle on it so this singularity notion there at some point AI is just completely beyond
our understanding I tend not to buy it I don't think it will be completely beyond our understanding
but it's an analog back to our Twitter discussion like because we can only understand it through
it's like having views on a mountain range you know the view looks different depending on where
you're standing and it's the same thing with with the emergent phenomena in a cellular automata
no very good and you know the classic example this is the is the blind man and the elephants right
and and that's that's actually the metaphor that is in the book is I say you know the different
tribes are like different blind men right but but precisely so I is one of the blind men I can see
part of the elephant but it'd be who's going to also talk to you who see another part of the elephant
and then each of us understands a little bit more of the elephant than we would if we were on our
but most importantly we collectively which is what really matters actually understand maybe not
the elephant completely but much more of the elephant than either one either any of us would
alone right and and it's certainly a lot better than just giving up and say like oh we're never
going to understand this strange thing that's in front of us interesting but that's a great
argument to what you were saying before so it's beyond our cognitive horizon therefore we need
to have diversity of aspect there's a yes there's a question of it's whether it's beyond the
cognitive ability of a single human yeah and then there's the question of it's beyond the cognitive
ability of an entire society of humans and obviously there'll be things that are beyond the
the cognitive ability of a single human but not beyond the cognitive ability of a society also these
days we have computers so our cognitive power is augmented by our machine so we can understand
things or bring things to the point where we understand them to a degree today that we couldn't
a hundred years ago right now that is a fascinating point so it's beyond our cognitive horizon
individually but it might not be beyond the cognitive horizon of loads and loads of humans
on the internet you know the wisdom of crowds but we don't I mean how do we know that the crowd
understands well we know well that's the the in some sense the beauty of this right is that
we never what is the crowd really understanding right and again once the crowd is augmented by
machines like machine learning algorithms right we can ask what do we as a society equipped with
all of our you know large language models and so on and so forth what do we really understand right
now at some level you can't answer that question individually because you are just an individual
but right there's a couple of very important things that we shouldn't forget one is that
you could one thing you can do and that that I do do say like do I now actually even just
individually understand things better than I did before when it was just me looking at it
and the answer to that is almost invariably yes right so there is a big gain to be here there
and the second one is that you ultimately you tell by the consequences right and like for
example take a deep network right and you may not know how it works but if it's doing medical
diagnosis you can tell whether it you know gets the diagnosis right more often than than it did
before or more often than another model so we as a society may not you know we individuals may not
very understand very well what we as a society understand but we can see the consequences
and at some level that's the point yeah so on that I mean that sounds like a bit of an appeal to
behaviorism and I'm we're going to talk about that in respect of charmers as well but it also
brings us back to you know we were talking about empiricism versus rationalism and nativism and
all of these topics would you place yourself in in that camp of being a nativist and a
rationalist or completely the other way no absolutely not again this is one of one of the
points that I you know go back to is there are the empiricists and there are the rationalists
and you could see naively machine learning as being the triumph of the empiricists
but it actually is not there are very fundamental reasons why it's not and I really do think
and this is not just think there's this thing called the no free lunch steer right and if you
take those things seriously the solution has to be a combination of empiricism and rationalism
I don't think either side alone has or even can have the whole answer so very much we need both
of those and if you're a pure empiricist or a pure rationalist I'm already suspicious of you
wonderful um coming back to what france was uh uh said in in his quote he said you know producing
things that are thought provoking novel and and all the rest of it and I was speaking to some
alignment folks yesterday and we'll pivot to that in a minute but the the big thing for me after
doing an episode on sales the chinese room is you know where does intentionality come from and
chomsky talks about agency for example we do things that are appropriate to the situation but not
caused by them so from my perspective all these generative models all these large language models
and so on the creativity the the real spark of genius still comes from us right we've just kind
of like um you know the the boring bit of actually doing the task is is now um delegated to the
algorithm I would disagree with that I think you are um I mean your position is very reasonable
and actually I would say probably the most common but I think when you do that you are giving us too
much credit and the large language model is too little we tend to have this notion that
creativity is something magical in fact I remember for many years so quick parenthesis
in the previous life I was a musician so you know I in some sense know about and a lot of my job was
composing songs right and I was always at the same time I was already studying AI and I couldn't
help but connect it to right and and think about like what would an AI look that was able to compose
music right and and talking to late people who are not musicians they think that composing songs
is some kind of magic thing that comes from you know whatever the great beyond and it's not
it's a very human enterprise and it can very well be automated it's actually now you know people
I used to say to people like people always say like oh creativity will be the last thing that
we automate because we humans can do it and there's no machine school then be like no it's
going to be the opposite you'll automate creativity long before many other things and
we're there now right in just the last so I think when you let me put it this way
your prompt to the LLM let's say is like the grain of sand to the oyster right you should
not give yourself credit for having made the pearl because it put the grain of sand in there
that's a that's a brilliant analogy right so it is still the LL we need to we want we can critique
how creative it is or not and there's a lot to be said there are a lot of progress to be made
but but we need to give it credit for what it does right it is well or not so well right maybe
it's more of an illusion that we're giving credit for and whatnot but that text or that image or
whatever they were created by the AI and in many ways the thing that was created by the AI is no
worse than than what would have been created by an artist if I gave them the prompt okay well on
on that I agree with you I mean Melanie Mitchell had this wonderful anecdote from the Google Plex
when she was with Douglas Hofstadter and he was talking at the time about you know how he would
be devastated if an AI could produce a shopper piece you know which was indistinguishable from
from from one which he actually created and of course that did happen yeah but but but then
we get into this discussion of where does it start right where does it start computers only
do what we tell them to do right they've been trained and actually I was speaking to to set
about this as the other day that you know all of the abstractions all of the things that the
computers and the models do they are crystallized snapshots of things that humans have previously
done and we've written the computer pro code so where does the creativity start well but we by
that standard we humans also only do what we're told to do we do what we're told to do by our
genes our genes do what they're told to do by evolution which does what it's told to do by the
laws of physics right and right right and now again this gets back to this notion that there's
nothing magic about creativity creativity really is to a large extent cutting and pasting stuff and
satisfying consistency constraints between them and I'm not just saying this in the
asteroid like long before the modern era there's this guy called David Cope you know
professor of composer and professor of music at UC Santa Cruz who created these programs that
exactly they would write they can write this was pre-machine learning right it was list code
that what it did was basically have rules about how music should be and then it takes snippets and
combines them right you could say it's just parroting those bits but the truth is at the end
of the day and you can choose you say like give me something in the style of Mozart and it creates
something by that looks indistinguishable from from what Mozart did but all it's doing is this
kind of recombination of pieces so we humans we we have too much respect for appreciation for our
own intelligence that's also what we're doing yeah I think I agree with you mean first of all
intelligences are receding horizon and there's the McCorduck effect and I agree with all of that but
yeah I think it's a similar thing to how we anthropomorphize large language models and even
you know it's tempting to say large language models are slightly conscious and we'll talk
about that in a minute but maybe like we also anthropomorphize our own agency right we have
like a little bubble around ourselves and we kind of delude ourselves that we exist as as an
individual unit with agency disconnected from the rest of the world well precisely the problem with
how we largely take AI today this has always been the case by the way is that we have a
new resistible notion to anthropomorphize anything that behaves even remotely like us
we're the only intelligent things that we know so if something starts behaving intelligently
then we project onto it all of these other human characteristics same with consciousness same with
creativity we don't know anything else that's creative besides us so once a machine starts
behaving creatively we cannot help but project a lot of things onto it it's just reasoning
by analogy right so it's a kind of analogical so like you're like me in this respect so you
probably are in this other spec now the good news is that we always start out with this kind of very
quick reasoning by analogy but after a while we actually start to build a model of the real thing
so AI for the public at large right now is very new but gradually we'll come to a point where we
where we zero in on what AI really is rather than just the shallow analogies that we initially
used to try to understand okay well I'll try it from a slightly different angle so you know
we were just saying Seoul makes the argument that it's a biological property and that's where
intentionality and consciousness comes from and it's a requisite but we'll leave that for the
time being let's let's go the you know the Fodor and the Gary Marcus and the Chomsky route and
they would argue that creativity is basically this this notion of or even analogy making by extension
is this notion of being able to select from you know a set which has an infinite cardinality
and the only and as you know neural networks can't represent infinite sets because they're
finite state automators therefore they make the move we need to have this compositionality
what do you say to that well there's a lot to unpack there I think we definitely need
compositionality right if somebody asked me make a list of how there's some things that
actually is essential for intelligence compositionality would be one of them right and this of course
is the thing that people like you know Chomsky and Gary and we're not really care about right
having said that I think first of all there is no such thing as an infinite set right like
infinite set is an infinity is a useful but extremely dangerous and confusing mathematical
tool right in the real world there is no such thing as an infinite anything and there never
will be so I would just choice this at well yes creativity and almost anything we can do in AI
is selecting from a very large set not infinite but very large right and now but now we don't
just select like one full element at a time we compose it out of pieces and that's actually
where the intelligence comes in interesting I don't want to go too far down the digital
physics route but we did just have Yoshua Bakon and I mean just just to reclamify on that would
you place yourself in in that camp that the universe is digital made of information
uh valid question I certainly think the universe is finite yeah I think
whether I mean to you know like Seth Lloyd says the universe the universe is a computer right
and I think that is true or false depending on what you take the word computer to mean right
the universe is not little so if you if you say that the universe is digital or is a computer as
kind of like an analogy that lets us understand it better I'm all for that I don't think the
universe is little you know maybe here's a way to put this the universe is a computation
like I don't know what the computer is or if there is one now the universe is digital in the
sense that deep down at the most basic level the universe is made of discrete things okay is this
like the it from bits the John Wheeler type hypothesis uh yes um I mean if you read that
paper uh it he's I mean John Wheeler was a brilliant person again a very you know to get
back to François Chalice's you know tweet he was very good at coming up with his provocative notions
right yeah and the it from the thing of course is like newly unvoked today and I do so at that level
I do agree that looking at the universe is being made of information is very useful and in particular
if you want a grand unified master algorithm in some sense the only way that I at least can see
of doing that is by seeing everything as information yeah so I think and in fact there's
something that I am working on that looking at everything as information is a very productive
thing to do yeah but but I my caution is that information is one aspect of everything so I
can give you a theory of everything that's based on information but it's not truly a theory of
everything it's the theory of one aspect of everything and I think there's a lot to be done
there but again we shouldn't forget what we're leaving out when we focus on that aspect yeah I
mean we've spoken a lot on the show about you know um uh Penrose's view and obviously Sal's view
that arises from from biology and and I know if Keith was here he would argue strongly that he
believes in in continue and therefore we would need you know hyper computation to to have this
universe there would be an interesting discussion to have because I really don't see where there is
physical or any evidence for continue of any kind the evidence is always that continue are useful
approximately but always underlying the continue is a discrete reality you take a sensor of anything
right you know quantum mechanics is like the quintessential example of this right what do we
measure at the end that it's always discrete events right like it's the detection of a photon
by you know by whatever detector right could be a model clue of Dobson or a ccd or whatever
but it's it's a it's a change of state it really is a bit oh interesting well how would you
contrast that you know um Stephen Wolfram has got this idea of um digital physics and you know
maybe and and again unfortunately we have to use arguments from behavior you know to kind of say
well we've we've got potentially a um a graph cellular automata and it creates this beautiful
emergent structure which is very much like the universe um but you know um Scott Aronson would
make the argument that he's discounting quantum mechanics I mean what would you say to that
so I think Steve Wolfram's theory is very interesting and he gets some things right that a lot of
other physicists don't in particular that the universe at heart is discrete so I'm very much
with him on that aspect of his agenda and thank god there's someone like him and a number of
others you know going that route right they're the minority in physics but actually I think if
you look at just what has happened the last 10 years things are very much moving in this direction
and I think they're going to move more right now having said that his specific theory I think has a
lot of shortcomings and I don't think it's the ultimate theory or maybe even the best path to a
theory you know to a discrete theory of the universe Scott Aronson's critique in that regard I think
misses the point right it's interesting because it's interesting that you should like pair those
two because Steve Wolfram in a way is a physicist to become a computer scientist and Scott is the
other way around right and I think you know like I greatly admire both of them and I'm friends with
both of them I've had many discussions with them I think you know just to very you know cruelly
caricature things in a way the problem with Steve is that he has bought into the computer science
assumptions too much and the problem with Scott is that he has bought into the quantum physicist
assumptions too much right so if you really think carefully and rigorously about quantum mechanics
all that continuous mathematics is there just to make discrete predictions so the continuity
may be useful it is useful I'm not arguing against the use of continuity in our mathematics in fact
we'd be nowhere if we didn't have it we just have to remember that it's an approximation it's a useful
fiction right so quantum mechanics in no way invalidates Steve Wolfram's theories right the problem
however is that he has not entirely you know ever since cellular automata days right he was always
saying like oh you know the laws of physics will come out of this cellular automata right and people
had these objections and and you know now he and others have partly answered some of them but the
truth is at the end of the day he the only way to answer that objection is to say look here is how
quantum mechanics arises from my discrete model of the world and I think this will happen but it
hasn't happened yet interesting okay we'd love to get Stephen on the show actually he's got a he's
got a new book out now which is kind of like expanding on on his previous work but um okay I
was having a chat with um some alignment folks yesterday and it's something that I'm a bit naive
to but as I said I've just read a book um I think it's called the rationalist's guide to the to the
universe and it kind of talks all about the um the early embryonic stages of Robin Hansen and
Nick Bostrom and Eliezer Jukowski and the Les Ron community and you know the info hazards and um
you know I don't even have heard of Rocco's Basilicist and all of this line of thought basically
and um yeah so where to go with this now I kind of um put forward that part of my problem with
their conception is that it relies on this rational agent making trajectories of optimal decisions
and um also they um they tend to be utilitarianism and uh utilitarianists and consequentialists
and um yeah I just wondered what what's your kind of like high level take on this
well there's many aspects to this right I think let me put it this way I a rational agent right
is an agent that maximizes a maximum is expected utility right the definition of rationality is
that it's you know expected utility maximization right and there is a lot of content to this right
and you know people in many fields like economics and and and you know AI right they make good use
of it it doesn't answer the question of what is the utility that you're maximizing right so
if you give me utility function right now if you maximize it you're rational you can it can be bound
you can be boundedly rational because you're and indeed that's this is the interesting and prevalent
cases that you can only maximize it within bounds and you have to make compromises as this
fast and whatnot but still you're rational if you don't do that you are irrational right so
rational is a very you know like so many mistakes that we make as a society as individuals would be
avoided if only we were rational in that sense so at that level I sympathize very much with that
view of the world having said that there's a huge gaping hole in the middle of this which is like
but what is your utility function right and and you know one attitude is to say like oh that's for
you to decide you know you you tell what me what utility function is but but then you you're you're
entitled to say well like but like my whole problem is that I want to figure out what my utility
function should be and and at that point this whole theory of rationality just doesn't help you
at all the utility function is an input right so another question becomes what is your utility
function right and then there's a very related but as Hume said very different question which is
like what should your utility function be like should is a very loaded worth here right and
then what what usually happens is things like this is that our notions of morality and so on
are trying to impose a should on you a utility function that you should have because it serves
the utility of the society now from the point of view the society this is good right but from the
point of view because the society hopefully will live and prosper if its elements contribute to its
utility not just their own right but it still doesn't answer the question so you can you're
entitled to ask so what does answer the question right and my view on this is that none of these
people you know these people include Kant and Bentham and you know Plato and everybody right
they you can't do that right so to me the supreme reality of life supreme maybe is a bad word but
like the overarching reality is evolution right everything we are is created by evolution and
as somebody famously said nothing in biology makes sense except in light of evolution nothing in
morality makes sense except in light of evolution not just biological evolution even though that's
part of it but also social and cultural evolution so at the end of the day the question that you
need to ask yourself is like is which utility functions are fitter and those are the ones that
will prevail so so let's let's go there that's really interesting now um you're known as a
skeptic of collectivist thought right we know and there's this interesting dichotomy we were talking
about of you know monolithic truth but um the utility functions interesting as well because
in a sense it i mean i know these folks are consequentialist but in a sense that's more
leaning towards deontology i did it again deontology um you know which is this idea that
that we have kind of like a principled approach to um to morality and i'm skeptical as well that
it's possible to create such a utility function because it wouldn't really be parsimonious but
how do you wrestle that that you have a simple utility function even though you believe in in
diversity of ideas oh no i didn't say simple go on crucial point the utility function could be
extremely complex and in fact the utility function so first of all there's there's more than one
level to this you to let's say you believe in utilitarianism right which i don't but have you
know compared to the others it's probably the least bad right yeah believing you tell if you
believe in maximizing utility function that in no way sanctions collectivism collectivism is one
particular strength that historically came out of that right and and and you know again and bentham
is responsible for it but it's this notion that you should have a utility function in which everybody
counts equally this is now making a choice of utility function which is different from having one
okay but i think you're saying something quite interesting as well which is that at the moment
the the utility is a function of market value which is very much inspired by
adam smith's hidden hand of the market but but i think your your views against collectivism is very
much against this idea of equality of outcome and that's definitely not what you're saying no i mean
that's even going beyond that right equality of outcome is actually irrational frankly to
we could go into that but you know you mentioned the market right and the market is the side's
utility again that is a one way to decide i mean there is also a very critical approximation to
what you really want so actually all we have with capitalism or carnimism at this point
in terms of utility function are very imperfect right and that's even saying it generously and
really our job is to try to come up with something better which i totally think we can right and by
the way one very salient question here which again uh for economists it's very salient is this
question of like should you have one utility function overarching controlling everything
even if it's complex right or should you not right and and that that i that i think is a very
interesting question right and there are there are good arguments in both directions right so let
me just give you one silly thing example which then i think also generalizes two other things
does your brain have a single utility function and i think the answer is no no you could say from
evolutionary point of view the overarching utility is fitness but then the way that caches out in
your brain is that your your genes need to control this adaptive machine right in such a way that it
gives you give you give the machine freedom right to do things that the genes by themselves couldn't
but at the same time at the end of the day that machine has to subserve the propagation of those
genes and the way you do this right at least the way evolution seems to have done and i think it
makes a lot of sense is that you don't just have one utility you have several ones which correspond
to your emotions and then they fight it out so i actually think there's this connection between
rationality and the emotion that people don't make which is that you know your emotions are really
your utility functions you just have different ones that cater to different things right you know
fear and anger and so on and so i think in reality we actually have multiple utility functions but
because again it gets to this problem that what we're trying to do is approximate something that
is very complex and difficult to get at maybe it is just one but we're better off trying to
approximate it with 10 or 20 different things than just trying to nail that one thing that's
really interesting and it is your view then on having this diversity of utility functions
analogous to your views on the master algorithm huh it's analogous but you're actually talking
about different dimensions right you could make a table where on one side you have all the different
utilities and then the on the other side you have the algorithms and now you can pair off any
any one of them i can say i'm gonna you know pursue this you know minimize your fear using
symbolism or minimum so any combination is valid hmm really really interesting okay and then let's
get into meritocracy for example so at the moment we do have the market system and presumably you
think that some people do genuinely have more market value than others for sure no and by the
way i think i i'm definitely a big believer in meritocracy i think um but what does it mean to
right very good so let's let's get that down first right meritocracy so
our goal is to have the society that that functions best and provides best for everybody right
and i mean we could refine even that but let's just take that for now as our assumption right
but then if that is the case one of your primary goals maybe even the most important one
is to get everybody to contribute the most they can right meritocracy is often seen as like i'm
going to rank or the people and at the top is the greatest genius and at the bottom is the
most useless person and this is wrong right meritocracy is a many-dimension thing yeah right
the goal of meritocracy is to find for everybody what they're best at doing so that they can do it
maximize everybody's contribution to society right and and this is a very complicated process
there isn't a single scale of intelligence or anything else having said that it very much
is the case that some people are better for some things than others right and if you deny that you
are actually in the process of destroying the society and making it dysfunctional so the attack
i find the attacks on meritocracy extremely disturbing right and a lot of them are you know
i've talked with many people who have those beliefs right and the number one thing that they say is
basically boils down to like oh meritocracy isn't perfect so we should junk it something being not
being perfect has never been a reason to junk it it's a reason to improve it so there's a lot of
room to improve in meritocracy but if you throw it away you are destroying society well i mean this
this this um you can trace this back to our argument about utility but um the thing is though
if we had um a value function which represented actual market contributions or even societal
contributions that'd be one thing but would you agree that that we have a lot of um game playing
at the moment so utility is based on playing the success game or the dominance game or the
virtue game as will store said in this book so we've got these kind of emerging games and it's
like it's not really um representing utility well absolutely so so far we've been talking
about utility right but what happens in the real world is that there are multiple agents
each with their own different utility and at this point what you have is game theory right game
theory is just what you have when there's not a single optimization going on but multiple
optimizations which are partly contradictory maybe partly not so the best way to understand
everything that we've been talking about including society and evolution and even what happens inside
your brain is as a big game a much bigger and more complex game than game theorists and economists
and so on and evolutionary biologists right prominently have been able to handle in the past
but I think they are very much in the right track and we can understand a lot of these phenomena
that you're referring to as they are games being played by people that have certain utilities right
and now you are going to impose your you know like and it's a game right I don't you don't know who's
going to win until you actually do the linear program and and figure out how this is gonna you
know and of course games are you know in reality you know most games are not single round games
right they're they're continuing games right so things get very very interesting but this I think
is the most productive way to look at all of this okay good good but but then some some might say
that this is a platonic way of looking at the world and the world is actually much more complicated
than that and again we're kind of fooled by randomness because we're anthropomorphizing the world
and we're kind of framing it as a game it might be much more complicated than that and I've already
said this a couple of times but you know the concept of power for example did when Napoleon
said I want the men to march into this country into this country is it just a simple kind of
chain of command that goes down no it's not it's so much more complicated than that well yes but
that's I'm actually I'm not even sure what you mean by when you say it's much more complicated than
a game again when I say a game maybe what comes to your mind is something simple like you know
prisoner's dilemma two two players two moves it's a game with you know with a vast number of players
each with a vast number of moves interesting but but I think this gets to the core of what
the rationalists talk about they have these thought experiments they talk about prisoners
dilemma they have that I forget the name of that game where there's the two boxes and you have to
choose the box I forget that but I guess what I'm saying is is that if you do have this rationalist
conception of the world and think about it in terms of game theory just like the um the symbolists do
and the people who handcraft cognitive architectures do or even with causality for example we create
these variables it's all anthropomorphic well um I would not so let me let me put it this way
right you can model almost anything can is an important word here you can model almost anything
in the world in any domain from physics to psychology to sociology name it as optimizing
a function whether you should is a debatable question but you can right but now what really
happens is that there are many different optimizations going on at the same time
all the way from maximizing entropy to me deciding what I have for lunch today
and now what you have is all of these interlocking optimizations and that's what I'm calling game
theory right one of those optimizations I'm Napoleon I want to conquer Russia you're the
Tsar of Russia you don't want to be conquered right and then we play a very complicated game
which includes other agents like your soldiers which maybe you know I a French soldier you
know want to conquer Russia but I also want to stay alive whereas an opponent really couldn't
care less whether I in particular stay alive or not as long as he conquers Russia in the end
so this very complex game I think is what goes on I don't think framing things in this way is
anthropomorphizing I mean in fact I think this is our best hope to not anthropomorphize things
although at the end of the day I think you can look at almost anything and see a ghost of enter
for my physician there but if there's a less anthropomorphic way to look at the universe
then through this lens I'd be interested to see what it is well the the only reason I'm saying
this is first of all I want to play devil's advocate a little bit and we even spoke about
the blind men and the elephant a little while ago and I'm sure folks on the left as they did
they criticized Ayan ran for example and they said that she had this very transactional way of
you know viewing the world there's this kind of Nash equilibrium of of self-interested actors and
are we guilty of doing that are we are we kind of like cutting off many aspects of the truth by
doing this I guess that's what I'm saying so we are always cutting off some aspect of the truth
when we look at anything in any way right which is not a reason to to look at nothing in no way
right so I think this is a very productive way to look at things but not the only one it doesn't
exhaust what there is to be said but I personally feel like it's the one where the most progress
can come from interesting now that's sort of like Ayan Randian simplification of the world
looking at things this way does not imply oversimplifying them on the contrary I would actually
say it gives us a handle on how to go into the complexity and not get lost and and not devolve
into like platitudes or oversimplifying ideologies right the fact that there's a mathematical
component to this is very important right mathematics when you can apply it gives you a
very solid handle on things we are now at the point where we can handle a lot of things mathematically
slash computationally that we couldn't before so when von Neumann invented game theory right
he said this is the future of the social sciences and so far it hasn't been but I think we're actually
now at the point where partly because we have the data right we actually can now usefully apply
this point of view in a way that we couldn't before how far it takes us we'll see it's not the only
possible to look at things but I do think it's probably the most productive at this point
interesting okay so coming back to this rationalist school of thought one thing that I'm interested in
is is morality but let's let's go one step at a time so I think Bostrom came up with this idea of
instrumental convergence which is this notion that in the pursuit of doing a particular task
the the intelligent system might actually potentially kill everyone on the planet or do
adjacent and this is this is where the interesting thing comes from so one task but adjacent
multitask ability and you know like potential intelligence and so on so there was an example
of a cauldron so you know you've got someone filling up a cauldron and in the pursuit of
filling up the cauldron to just the right level they might kill the person who looks after the
cauldron room just so that the agent could do it more efficiently um are you are you cynical about
that or what do you think no I'm not cynical about that but let me put it this way I don't lose any
sleep worrying about the you know paperclip factory that's going to take over the world right
I think you have to take that as a philosopher's thought experiment right you know the philosophy
being nick in this case right I think there's a there's a real danger that there's putting its
finger on but it's not but it's also um mistaking reality for something else so let's look at both
parts of that right the real danger is that um if you give an AI an oversimplified a hugely
oversimplified objective function and at the same time a very large amount of power bad things will
happen and we need to worry about that right so and by the way this is already a problem today in many
maybe more modest ways but many but also more relevant frankly right and so what do you do right
first of all is the truly function needs to be as rich and as complex and as subtle as the people
that it's trying to serve right so you as long as what you have to take a really world example today
social media who's you know are all designed to just maximize engagement right you have an
enormous amount of AI at the service of maximizing engagement it's a very I understand why companies
do it and partly they have the right to we can get into that but the point is the the consequence
it's ignoring too many things right so one line of defense against is like you have to enrich
your truly function until it's like a bit and then is this is an open-ended problem right when I
we're never going to have the final utility function is something that the AIs have to be
continually you know AI's I think Stuart Russell said this and I agree like they should spend half
their time figuring out what the utility function is and then the other half maximizing it whereas
today it's like I wrote down my utility function in one line and now I spend this enormous amount
of power maximizing it so that's one line the other line or like one other line is you have
to put constraints on the machine hard constraints you can't win the pursuit of this utility function
you can think of it as like you know terms with infinite weight in the utility function
you can't go outside this and then the other one is the single biggest reason why I sort of like
this paperclip experiment is silly is that you know along with that paperclip factoring the world
there are going to be a million other AIs you know each of which is doing the same thing so
none of them is ever going to acquire the power to cause that damage unless it's doing something
very different from just trying to make paperclips so at some level that example is extremely
unrealistic and leads us down the wrong track right loads of places to go there but um first
of all I think you do believe in AI alignment then because you're saying exactly the same as what
they do which is that we need to have the utility function that represents the the richness of the
human condition so that's the first thing so so essentially you're all on board with alignment
well I believe in AI alignment in one sense of it many different things get go under that
umbrella of AI alignment right I think in the near term thinking of things and I mean if I like
let me put it this way if AI alignment is just trying to have a really accurate utility function
then yes and then the machines are optimizing that function absolutely right yeah and in the
near term I think talking about AI alignment is a little I mean the problem that I have with the
concept of a like of an alignment is that goes far beyond that it tends to see AIs as these
independent agents that we have to align their goals to ours right yeah and and and if that just
cashes out as like you know here's the utility function that's fine but the problem is AIs are
not independent agents AIs are our tools well just to push back on that a little bit because
I always had that conception of these folks I thought I was arguing against people who believed
in a pure monolithic intelligence and that and they a lot of them are transhumanists actually
and they say that they they want to you know ensure human flourishing through the kind of
the use of of AIs in tandem almost as a kind of extended mind from from David Chalmers and but
but then I really wanted to get into their fears of recursive self-improving intelligence and
superintelligence because when you do have this kind of heterogeneous approach to humans and
machines there are going to be bottlenecks everywhere now I like to think of it a bit like
the market efficiency hypothesis which is that you reach an equilibria where you know the the
individual actors in the market become more efficient will become more efficient programmers
because we're using codecs but we will reach a limit surely well to touch on transhumanism for
just a second right because I do agree to kind of at least sociologically a lot of that crowd is
the same right let me put it this way right and I'm sure this is a controversial statement but
maybe in the long run the AI should take over the world why are we so arrogant that we think
whatever the AI is it should always be there to serve us you know we are a step if you take the
long view of this right we're a step in evolution right we're amazing maybe I'm a human chauvinist
but I do think we are amazing but we're not the last word right so you know the other day I tweeted
something that you know is you know maybe provocative but it's like I think encapsulates
which is I said that the killer app of humans is producing AI maybe our role in evolution
is that we're going to produce an AI right that is you know the next level of whatever you like
consciousness intelligence etc etc right and so the notion that in the very long term the
eyes that should still be there to subserve us by this point of view is actually silly right
right but a lot okay so I mean a lot a lot of folks let's say the ethics folks would find it
horrifying that and I was speaking to Irina actually yesterday and she she said something
a little bit tongue-in-cheek which is that which is who sorry Irina from Montreal Mila and Irina
Arish right oh yeah I know her yeah we were classmates that you see her amazing yeah she
I really love her but um no she was kind of joking that we should almost align human values to the
to the AGI values but but well that that I find alarming well I think she was saying it tongue
in cheek I'm not alarmed by a lot of things but yeah but but but what do you what do you think about
this um uh this ethical concern that if that if it is the case that you believe that you know we're
just one rung on the ladder and you know transhumanism is more AI than it is human um people would
find that horrifying well um I I understand why people would find that horrifying and I mean again
we have to distinguish the short from the meaning from the long term when I say something like this
I'm talking about the very long term right trying to make human subservience subservient to AI today
is a horrifying idea right now I think the reason a lot of people are horrified with this idea period
right is natural but in my view naive is just they are seeing humans as the end goal if humans
are the end goal then the idea that they should be subservient to developing the the next level
of AI is horrifying if you have a moral system where humans are the the the be all and end all
then all of this is horrifying but again if you take the long view of evolution humans are not
the be all and end all okay I mean eventually this might take us to the effective altruism discussion
but um I think as we were saying Sam Harris recently had a podcast talking about the FTX
disaster and and he was kind of making the argument that we're all consequentialists even
even if we don't realize it but there are different degrees of consequentialism and I think a lot of
the um uh the ethics folks at the moment they really really don't like what's going on with
with long termism and it's because there's this slippery slope of the kind of horizon of consequentialism
so with Nick Bostrom you know he came up with this number that there could be simulated humans
you know living on other planets in the future it's a very big number I think it's like you know
it's got it's got a lot of zeros on it and what's what's to stop us from just making the argument
and what's to stop ai's from making the argument that those simulated lives have more value than our
lives okay there's a lot to unpack there um so but let's take this one step at a time I very much
by the idea of effective altruism on principle right I think that is the way to go about a lot
of things I think in some ways uh if you are not an effective altruist maybe unconsciously
you are being irrational or maybe evil right if you believe in altruism I mean like think about
both parts of that right if altruism is good then then let's say we take that right and then why
should be why should you be in favor of ineffective altruism right if you want if you're an altruist
if you want the good of other people you should try to do the best you can right and so for example
I very much by the notion that like you want to you know make the most money you can so then you
can give away that money as opposed to volunteering at the soup kitchen volunteering at the soup
kitchen for say someone with a phd machine learning is an ineffective form of altruism now
having said that I think the focus on the long term has been in many ways I mean certainly the
long term is important right but but the problem with the whole effective altruism movement is
that it got overly focused on that and we can talk about why and then even even and then a
further mistake is that it got overly focused on these supposedly existential dangers that are much
less of a big deal than people think like AI so between effective altruism and fixating on AI
is an existential danger lies a huge gulf I'm for effective altruism I think you know the long
long term you know there's ins and outs there right and then this focus on like these existential
dangers is is is very problematic you know for example you know to get back to the you know
Bostromian notion of like all these minds that matter more than us and whatnot there is a basic
idea right that like any economist knows which is that you have to discount the future and the
question is what your is what your discount rate is right and if your discount rate is high right
those those minds matter not at all and now why do you have that discount rate the primary reason
is that there's uncertainty about the future right I have to weigh the the certain benefit
of helping you today with the increasingly hypothetical benefit of helping your mind there's
less and less likely to exist in the future so in many of those cases the present and the short
term do win okay but a couple of things to contrast that so a lot of effective altruism is this idea
that we're born with faulty programming right so we we have these views you know like we have this
concept of moral value and it gets discounted in space and time right so we need ways of
overcoming our programming but you were saying that we should be thinking about this but contrast
that with your you know with with your statement about ian rand earlier right so ian rand was very
very transactional because I think the the folks that criticize this movement are suspicious that
we are actually being a bit more like ian rand but with the guise of of altruism and I think
and they they think of the ftx disaster as being kind of like evidence of that
a lot of different points there the ftx disaster actually has nothing whatsoever to do with any
of this right Sam bankman feed was one guy or is one guy funny that I use the past tense
he's one guy who believed in effective altruism good for him right he was I mean the whole
ftx thing was also obviously I mean yeah we could get into that but the point is you should not
I understand why people's image of effective altruism would be tainted by what happened with
sam bankman feed but really shouldn't be right you know an idea is not responsible for the
you know mistakes that its believers making in in in unrelated domains point one point two
transactionalism there is nothing in what I've said whatsoever that implies transactionalism
in fact the opposite right I think relationalism is is actually the the key concept and part of this
is that uh games are not one shot your games are played in a repeated way and famously for example
if you play things like prisoners of the lemon were not repeated like you're like you know
cooperate defect and whatnot as soon as you start bringing in these other things like that
make things more realistic you actually start to get behavior that is much more you know what's
the way to put it rational in some ways and human and whatnot right another one is that
traditional economics which I think iron was influenced by viewed and still views the world
is linear but the world is nonlinear once you start seeing the world is nonlinear all of these
things really change you know the face of them changes right so I think we have to look at all
these concepts in this view right and and we want to focus on the long so I so I so to get to go
back to your first point right we are born with faulty programming part of our fault and that's
what if you know effective alteration is there to overcome right part of our faulty programming
is that our discount rate is too high because we evolved in a world where your time horizon was
very short the fact that it's too high doesn't mean that we should make it zero and care only
about the future but what would you know the ethics folks who advocate for gatekeeping and
paternalism couldn't you just say that they're doing the same thing well you should ask them
right but wouldn't wouldn't they wouldn't they lead by saying our programming is faulty and
therefore you know we need to know I mean look uh we can so part one we can debate whether our
programming is faulty or not and why and so to just start by touching on that our programming is
faulty so our programming is not faulty in the sense that we evolved for a particular set of
conditions right and that evolution may not be complete or optimal etc etc but roughly speaking
we are not faulty in that sense the reason because evolution is doing its job right we have all
those impulses for a reason right now the problem is that we unlike any other species we actually
have actually succeeded in creating a world that is better for us but at the same time and this is
the problem we are actually now adapted to a different world from the one that we live in
so the faulty program just comes from the fact that we evolved for one set of conditions for
example among many other examples where your time horizon was very short and now we live in a very
different world and so our job as rational people that's what our rational minds are for among other
things is to now adapt ourselves to the world that we really are in so that we do things that
are rational in the world that we're in right so now the fact that our programming is faulty
there's not there's not say anything about what are the faults and how you fix them and what these
people have I think is first of all the wrong notion of what our faults are and then on top of
that the wrong notion of how to fix them okay now I want to get into the utility function again now
again one of the things that makes me skeptical is this notion of immutability both of of what
we're doing and in the case of of what we've been speaking about with with utilitarianism
what the utility function is now you are kind of hinting to something interesting before which is
that that it might be diverse and it might also be self-updating but I'm constantly asking myself
question how does that work and who gets to say well so very much I think it's complex and it
should be self-updating right we're never going to final itself if you buy this notion that the
ultimate arbiter is evolution then utility functions are subject to evolution right so you
think about it or you can't think about it should wrong world you can't think about this and it's
useful to think about this in the following ways to first approximation the number one entity that's
evolving is utility functions what you have in the world at any point is a population of utility
functions right yeah and now they combine they evolve you have next generation of new functions
and then and then there's also how the utility function gets optimized there is also subject
to evolution right and now how the utility function is optimized changes a lot faster and
this is a lot more complex than the utility function itself which is the point right so
at a certain time horizon it's reasonable to approximate utilities as being fixed like for
example the utilities that are encoded in your brain are fixed by your genes right so in the
context of our present you know human moment and effective optimism or not it makes perfect
sense to think of utilities fixed but it is evolving and not just on you know eon timescales but by
the generation right you know things evolve by the generation okay but it's still relatively
glacial and I take your point that there's a kind of divergence between the world we live in and the
programming that we've got but then okay let's imagine that we create a new population and
I guess what I'm saying is is that you think that the utility function should emerge
and evolve but I would argue for some kind of morphogenetic engineering where it's a kind of
hybrid between something which is emergent but something which we can nudge oh I mean I'm glad
you brought that up nudging is a form of emergence you or self are emergent and the things that you
do are emergent as well everything is emergent right utilities are emergent maybe the laws of
physics aren't emergent some people say even those are right like you know we live in a
universe with these constants because blah blah blah right so but to first approximation every
single thing that we've been talking about is emergent we make a distinction between emergent
and designed because that is anthropomorphic right is this things that we do are not emergent
actually no when you nudge something that is an emergent behavior right we are emergent as well
right so everything that is human you know so he's a very good way I think to think about a lot of
things which I first saw you know in Richard Dawkins which is although he really didn't go into this
and I wish he had like this notion of the extended phenotype right technology is our extended phenotype
so all these things that we do right all these things that we build including as and whatnot
they are extensions of our phenotype so if you take the long view all of you know technology is
by all is the continuation of biology by another means so when you make this distinction between
emergent and not emergent and top down and bottom up it's all emergent interesting well
when we recently did a show on on emergence and it's a topic of interest to me personally
and there's there's weak emergence and strong emergence and there's you know like the view of
weak emergence so there's some you know surprising macroscopic phenomena maybe something which
transiently emerges and Wolfram would add in the whole you know computational irreducibility angle
and then with the strong emergence charmers would say it's something which is paradigmatically
surprising it's something which is not deducible for many fundamental truths in the lower level
domain but I just wondered like how do you think about emergence well I think that is a very the
distinction between weak and strong immersion is a very useful one right and and I would actually
phrase it in slightly different terms which is starting from physics right I think most physicists
and scientists believe in weak emergence well could I could I had this Sabine Hossenfeldt
had a paper and she frames it with this idea of the resolution of physical theories so like
like a lower resolution theory is weakly emergent from a higher resolution well exactly and you
know like I like Sabine but this is not her idea right this far predates all of us here right and
again it's it's a very interesting history and a very important concept now so my point was that
I think few people have a quarrel with the notion of weak emergence in the sense that you know I can
give you a theory of everything in the form of whatever string theory let's take a candidate
right but no string theory claims that that's a theory of everything in the sense that like now
to study biology or psychology or sociology you should just study string theory no one believes
that right there's actually interesting things to be said there but but let's not let's look at
let's look on there for a second right there are these levels that emerge weakly in the sense that
they are determined by the lower levels they're just so much more complex that you're better off
focusing on the menu now there's this other notion which to me is the really interesting one
which is that there is there are phenomena that are at the higher levels that are just not reducible
to the lower levels yeah right so the true emergent is in some sense is someone who believes the
latter and now you can ask the question like do you believe in that or not right and I think
to give the very short answer first is that ultimately there's probably no way of knowing
but pragmatically you're actually probably better off treating the world as if it has
strong emergence and now strong emergence actually a very strong segment to make is to say and by the
way going down to the lowest levels to make things very clear you don't need to think about biology
or society or consciousness or anything condensed matter physics right the particle physicists
tend to believe that what they do is what everything reduces to you talk to the condensed
matter physicists this was actually an interesting discussion that I had with Scott you know Aronson
because like he was very much on the we're both computer scientists but he was very much on the
side of the particle physicists on the side of the condensed metaphysics what they will tell you
over and over again they see is things that you cannot explain using quantum mechanics
and now people say like oh but you can always explain things in quantum mechanics you just
haven't done the calculations but the point is precisely you can't do the calculations right
the calculations are chaotic yeah I have a theory I can come up with 500 theories of these
phenomenon semiconductors and whatnot and like I never actually get to test them because the
computations diverge before I get to test them so for all intents and purposes it is strong
emergence whether truly that came from below is unanswerable because you can't compute the
predictions well we spoke about that so and I think Keith would call that semi-strong emergence
which is like you know whether it's computationally reachable from from the lower resolution to the
tonight from the high resolution to the lower resolution but no Sabine in her paper a case
for strong emergence she was talking about singularities as being a really good example of what
might be strong emergence and the philosopher mark bedau I think said that strong emergence is
ridiculous it's basically an affront on physicalism well certainly you know strong
emergent and physicalism or let's just call it reductionism reductionism yeah strong emergence
and reductionism are incompatible yeah and and and and we scientists tend to be reductionists
yeah right now now you like and at some level I'm both a reductionist and someone who is willing
to believe in strong emergence again I don't believe in strong emergence I just don't see a
way to disprove it right and where the semi unlike you know if there's an empirical way to distinguish
semi-strong from strong emergence I'd be very interested to know what it is but now I think
the thing that is very important that a lot of people including a lot of physicists and scientists
don't see is that we have this hypothesis that everything can be reduced to the laws of physics
as we know it we should not forget that it's just a hypothesis and it's a hypothesis that again
counter to a lot of people say is very very very very far from established and usually people
say like oh but you know look at all the successes of the laws of physics and blah blah and then I
say like you know putting on my machine learning hat the sample that you've used to validate the
laws of physics is extraordinarily biased in the direction of simple systems okay so you can't make
this claim of if the data was iid I could say with great confidence these laws apply universally
but I haven't done it it's more like I've just landed in a new continent and I've sealed up all
the rivers and I say I know what this continent looks like you've never climbed the mountains
you've never gone in the jungle so like this notion that the laws of physics capture everything
about daily life we just don't know how exactly maybe it's true but it could also equally well
be completely false brilliant well you gave a bit of a hint to this earlier actually because you used
the word relationism right which which is basically the or relationalism which maybe should be you know
shortened to relation relation but yeah I think Rosen is a great advocate of this and he has a
whole kind of category theory calculus for for describing living systems and and also we spoke
to bog coic the quantum physics professor from Cambridge and he was talking about this concept
of Cartesian togetherness which is another category or framework but I just wondered like you know
did does that inform your your your view well relationalism at least in one you know way of
defining the term very much informs my view right and one way to come at this is to say
the world is not made of independent entities actually let's just start with machine learning
which is a very concrete way to look at this a very large part maybe even the largest part of
my research in the last 20 years has been to do away with the assumption of iid data right that the
world is made of independent entities in particular society is made of independent agents etc etc right
now we make this assumption both as human beings you know to some extent and certainly very much
so in science because it makes life easier the math is way way way easier when you assume independence
but it's a blatantly false assumption right unfortunately a lot of for example economics
prominently has embedded in it this notion that the world is a bunch of independent agents and it
just doesn't work like that and moreover it's a distinction that is full of consequences a society
and economy is a network of agents and almost all the action is in their interactions until you
really start taking that seriously you really don't understand the world again I have no
call with classic economics as a first approximation it's exactly what it should be right but then
and by the way you should also not just throw it away and say like oh this is garbage like some
people say you have to go the next stage it's actually now we have the mathematical and computational
tools to do and understand it as being a system of interacting agents and all of the questions
that we are talking about including you know in you know in evolution even in physics right
you know a piece of condensed matter is a network of interacting spins etc etc you name it so
the relations are at the heart of it and moreover like as I said a lot of my work is we now have
the representations the learning inference algorithms to handle things that are big piles of
relations and the whole world is better understood in those terms and we just need people to catch
up with that you know once you do that you get into things that can easily be computational
intractable and so on and so forth but there's a lot of things that we can do there and a lot more
that we'll do so at this level I think relationalism is really should be a cornerstone of our
understanding of the world in a way that it hasn't been in the past okay and which existing I mean
complexity science brings to mind but I mean which existing techniques and areas can folks
look into to take that on board well you know Markov logic which is what I developed for this
purpose essentially and I and I do think you know you know this is my talking about my work so you
should naturally be suspicious but I think it's the best that we have and and I think by a wide
measure compared to anything else that we have so far okay and can you sketch it out yeah so this
to sketch it out in the simplest terms right we want to combine all the all the traditional
goodies that we have from assuming the world is IID with the power to model you know relationships
there are themselves potentially very complicated the way we do the Markov logic is there's the
logical part we actually do not need to solve a new the problem of how to represent and do
inference with relations we have first order logic for that first order logic is the language of
relations that's actually the term that is used right and how the relations depend predicates
sometimes they're called predicates but let's just call them relations right we have a formal
language to talk about relations and by the way essentially all of computer science can be reduced
to that you give me your favorite you know whatever knowledge representation data structure etc etc
and I can you me I anybody who knows can immediately say how to do that in logic so that's one part
the other part is the statistical you know machine learning probabilistic aspect of the world right
and then again going all the way back to physics right all of these things that we deal with are
essentially special cases of what are variously called Markov networks which is where the name
Markov comes from or graphical models or log linear models gives distributions Boltzmann machines
like right all of these things are essentially the same right that whole neck of the woods is
captured by Markov networks let's call them that and Markov logic is combining Markov networks
with first order logic in a single language which you can now do everything with okay okay so just
just to like push back a tiny bit so in the past we've tried to create let's say things like
psych which is a knowledge representation of the world folks like Montague have tried to do semantics
using first order logic to set some you know varying degrees of success and then we have the
grounding problem we were just talking before about you know like even Searle said this that
you have kind of epistemic objectivity and subjectivity and some things are observer relative
like even economics is observer relative so with this kind of formalism how would that work
you know so very good the problem with or the main problem with a lot of these things that
that you mentioned like you know certain types of semantics and whatnot that are based essentially
on first order logic right is that they're too brittle yeah in fact the problem with symbolic
AI is that it's too brittle yeah and this is exactly what Markov logic fixes it fixes it by
making it statistical when I give you a logical statement now I'm no longer for example simple
logical statement you know a smoking causes cancer right in English this is a valid statement
smoking does cause cancer but actually once you translate it to logic for every x smokes of x
implies cancer of x it's false because some smokers don't get cancer right what this really was meant
to be all along is a statistical statement that says smokers are more likely to get cancer
so so the way we overcome a lot of those problems is precisely that we take all of this logic which
again the language exists we don't have to change it we can but we don't have to and we make it
statistical as a result of which it's no longer brittle or at least now it's only as brittle as
machine learning and graphical models or not it's not as brittle as you know traditional symbolic AI
okay and so we speak into a lot of go-fi people and I mean Wally Subba for example he's a rationalist
and what's interesting about the rationalist is they they hate any form of uncertainty right
they think in absolute binary you either know it or you know I mean let me push back on that
there's this again you need to distinguish you know a general field or idea from its subtypes
right yeah there is a type of rationalist that hates uncertainty big mistake big big mistake
there's a type of rationalist that you know uncertainty is what they you know like
an uncertainty calculus is a type of rationalism you know some of the best you know AI philosophy
etc is just that so there is no incompatibility at all between rationalism and uncertainty in fact
if if rationalism if being rational is maximizing expected utility notice the expected in there
right you cannot be rational if you ignore the uncertainty interesting okay but then what about
the the resolution of modeling I mean smoke smoking's a really good one so us humans we
anthropomorphize things we understand the world in macroscopic terms using macroscopic ideas that
we understand and that kind of leads to a certain type of modeling and that modeling presumably would
be represented at that resolution you know using this formalism oh sure and what's the question
well it seemed again like I'm intuitively suspicious that we were just saying the world
is a complex place and with a lot of causal modeling for example a lot of the art is understanding
what is relevant and what is not relevant what is relevant might just be kind of you know relevant
to us no well what is relevant is what is relevant relative to your utility function
okay again it gets back to that precisely the whole problem is that the world is infinitely
complex and we have only finite computational resources whether it's in our brains or our
computers or whatever right so now what do you do right you are forced to oversimplify the world
not just simplify but oversimplify right but now the whole arc that's actually a good word to use
even if it's done with computers is how do you not only simply oversimplify as little as you can
but pick out the simplifications that are least harmful to your objective by the way the art of
the physicist physicist would tell you is precisely doing this right physicists are very good at
deciding what to simplify and in fact almost I think at some level almost any good scientist
this is what they do right so and now how do I decide what and how to simplify is by relevance
to my utility function right I want to ignore parts of the world that do not affect my utility
function number one right and for example the notion of conditional independence which is the
foundation of graphical models that's what the whole idea is is like once I know these things
I don't have to know about those others thank god right okay but if Ken Stanley was here he says
that the great thing about evolution is it's divergent it's not convergent it's discovering
new information and my worry is is with a system like this with any form of anthropomorphic design
would inevitably become convergent and it might look like oh those things over there that we're
ignoring don't matter but actually they might really matter if they got introduced into detail
well I wouldn't say that maximizing expected utility is anthropomorphic
right in fact it's one of the least and I think maybe there's some degree of anthropomorphic
is almost anything we do and you know the progress of science is becoming less and less
anthropomorphic and we should keep pushing on that but I would say that maximizing expected
utility is one of the least anthropomorphic things we can do well this this is actually
a really interesting point because one of the key tenets of the rationalist movement and their
conception of intelligence because you know all of the other definitions of intelligence are
anthropomorphic so you know that there's based on behavior capability AI principle
function is a big one you know from from Norfolk and this this is this is the the
principle based AI which is just making rational moves so why why is there such a push to be as
you know to be as as the least amount anthropomorphic oh the push is not to be at least in my
view being less anthropomorphic is not a goal because that's not the goal the goal is to be
as accurate and complete as we can in modeling the world right we're just trying to understand
the world better right for whatever purpose maybe for its own sake maybe for the purpose
of the utility and the evolution and so on right but that's the goal the problem is that and this
has been the problem since they won right they won of humanity is that because we anthropomorphize
the world that gets in the way of understanding how it really works right if I say the wind is
some god blowing right I understand right that's all they could think of but it's a big
obstacle to understanding what the wind really is like there's a pressure their friends etc etc
right and we've done away with a lot of anthropomorphism by way one of the problems that we're always
having is that it's always pushing back right you know there's always you know again intuitively
we have a very strong tendency to enter poor masses like as much as science broadly construed
is a great victory it's always in danger from this right but even within science we've gone to
from doing away with the obvious forms of anthropomorphism and anthropomorphism to having
many things that are still dead that are less obviously anthropomorphic but still are right
but if there's something anthropomorphic that's actually is accurate then more power to it
interesting yeah and then I guess we have so many cognitive priors right in our in our brains that
give us a cone of attention which has it would just completely anthropocentric
well very good so those priors and maybe a better term is heuristics right our brains are full of
heuristics that evolution put there for a good reason because those heuristics work right but
they are heuristics so they have failure modes right and we need to understand what is that those
heuristics really are getting at so that we also so that we use them when they're good but then when
they're not good we use something else brilliant brilliant so Pedro we're here at NeurIPS this week
and could you just like sketch out some of the some of the things you've seen and I also know
that you're a huge fan in that there's a neuro symbolic algorithm that you want to tell us about
so let's let's hear it so I indeed I've been enjoying NeurIPS this week one of the big things
in AI in the last several years has been neuro symbolic AI which you probably will not surprise
by the fact that I very much believe in so and I believe this since I was a grad student and the
whole idea of neuro symbolic there was something that nobody was interested in right and now suddenly
everybody's which I think is a good development and this is the idea that if we really want to solve
AI by some definite if we want to get to like human level intelligence etc etc we need to have both
you know like for example deep learning is not enough right there are symbolic reasoning
capabilities that we have and that are essential yeah and and we need to get them and I think you
know intelligent connectionists like I don't know Yoshua Benjo you know Yann LeCunna said they don't
disagree with this but but but one way to look at this is say we're just going to realize those
you know capabilities using purely connectionist means right and what I see happening in that direction
unfortunately is a lot of reinventing the wheel so I do think you know symbolic AI got wedged for
some reasons including brittleness and you know and we have learned from that at the same time
they did discover and understand a lot of things that are extremely relevant so it's just not good
science to ignore it so I'm working on an approach to combine you know symbolic AI with with deep
learning again this is a popular exercise these days there are many interesting approaches out there
as much as I sympathize with them I think they're all very far from solving the problem they are
over complicated and not powerful enough okay so you know I've been working on an approach called
tensor logic that I do believe is as simple as it can be and and as general as it as it can or needs
to be and and this you know it really is a deep unification of the two things in the sense that
it's not just that you combine them using you know a neural model that calls a symbolic one
or vice versa which is a lot of what these things that you have today do and a lot of the claims that
like oh this system is neuro symbolic which it is it's like you know alpha go is neuro symbolic
because some of what it does is symbolic but I'm talking about something much deeper which is once
you start doing AI learning inference representation in tensor logic there's just no distinction
between symbolic and neural at all anymore can you explain that so tensor logic I'm just inferring
from that that the the primary representational substrate is a continuous vector space is that
right are you encoding discrete information into the vector space so it's a vector space yeah right
in fact this was the original term that we had for this was vector space logic but then we changed
it to tensor logic because it's much more appropriate but it's it's vector space in the abstract
algebra sense of vector space not in the traditional you know vectors of numbers but but anyway so
as the name implies right tensor logic is a combination or unification of tensor algebra
on the one hand and logic programming on the other so is it similar because bob kowek had a
similar idea using like tensor outer products is it that kind of it's related but I think it goes
well beyond okay and the basic idea is actually pretty simple and it's just the following right
without going into too much you know technical detail all of deep learning can be done using
tensor algebra yeah you know plus univariate nonlinearities right so we've got the tensor
algebra to do that all of symbolic AI can be done using logic programming and moreover it has
been done using logic program so if you can unify these two things this part of the job is done
right and as it turns out you can unify them shockingly easily because a tensor and they so
tensor algebra is operating on tensors you know inductive logic so logic programming and then
for learning inductive logic program and symbolic AI they are all operating on relations yeah right
so what is the relationship between the tensor and the relation right a relation is just this
and efficiently represented sparse Boolean tensor so at this point we actually know that
the foundation of these two things is actually the same if your tensor is is Boolean and is very
sparse now i'm better off representing it with a relation but at a certain level of abstraction
nothing has changed right so by this prism you can look at logic programming and logic programming
is doing tensor algebra okay um just to help me understand this a little bit so um you know the
main criticism of using a neural network um as a combined computational and memory substrate
is that they it's a finite state automator so without having the um the augmented memory
like a Turing machine you can't represent infinite objects that's the main reason the symbol is you
know that's the main argument they use so wouldn't that argument still be leveled against you well no
because the i'm glad you brought that up because there is a very common misconception okay if you
realize that there is no such thing as infinity right and in particular there is no such thing as
an infinite you know memory that problem doesn't arise so there's the so the unfortunately a lot
of theorists including computer theorists they they foster this misconception right yeah there's
the chomsky hierarchy right with finite automata at the bottom and Turing complete you know Turing
machines wobble at the top right if your Turing machine has only a finite tape it's a finite
automata so everything is just finite automata let's get that out of the way right a lot of what
people do is like completely mistaken because of that now the fact that everything is finite
automata does not mean that everything is equally good some representations are far more
efficient compact etc etc for certain purposes than others and the whole game here is that like
i'm not going to solve a i find an automata the question is like what do i need to do
not because i need to go to a higher level the chomsky hierarchy because in reality they don't
exist but because you know i mean if you have infinite resources you could solve a yeah with
a lookup table but would you would you not i mean for example um there was this deep mind
paper that mapped architectures to different levels of the chomsky hierarchy transformers
i think were you know fsa's uh rnn's actually were one step higher they could they could represent
regular languages and they got context free languages i mean do you do you think there's
any meaningful distinction between those language levels as i said there is a meaningful
distinction but it's not the distinction that people usually make right because once you i mean
you can debate whether the universe is finite but certainly computers are finite so as far as
anything that you're going to run on the computer there truly is no distinction at this theoretical
level between a Turing machine and a finite automata that does so like you i can reduce and
people have there are papers reducing you know any of these things to any of the others right so
like it's a fairly trivial exercise so at that level those distinctions are completely meaningless
however they are meaningful in the sense that for many purposes i am better off having an rnn
than having you know a transformer and for many purposes i'm better off have so like let's take
you know propositional logic versus first order logic right if there's no such thing as infinity
first order logic is reducible to propositional logic but that does not mean that it's useless
because it can represent a lot of things exponentially more compactly than proposition logic
if i want to represent the rules of chess in first order logic it's a page right yeah if i want to
represent them in propositional logic it's more pages than you can have okay well i think that
that's a very very good point but i mean just just a devil's advocate from the psychologist you
know do you remember that that photo um felician connectionism critique paper arguing productivity
and systematic productivity is all about the infinite cardinality of language i mean presumably
you would agree that language has an infinite cardinality no well again another instance of
the same problem productivity is very important but the point to just be a little precisely for a
second is is to be able to generate a vast number of things beyond the ones that you started with
vast not infinite in fact mathematically infinity is not a number
infinity is just a shorthand for something that is so large that it doesn't matter how large it is
okay i mean at the end of the day i'm not a mathematician but surely mathematicians would
push back on this because you know infinity is is a quantity in mathematics no i mean again
people in every field mathematicians physicists computer scientists are all are often guilty of
they they they they have this notational shorthand or like you know this terminological shorthand
that serves them well but then they and then they use that and then the newer generations come along
and the and the public also right they don't even realize that what's being talked about is a little
bit different infinity is a perfect example any serious mathematician will tell you that
infinity does not have the properties of a number so for example if i multiply infinity by two i
still get infinity there's no number that that happens to yeah right so infinity is is not a
number right when i say infinity is not a number mathematicians might quibble about the way i'm
stating it but this is a this is a mathematical truth right infinitely truly isn't out i'm being
co-local of course when i say that it's a shorthand for something that is so large that it doesn't
matter how large it is when you take limits you know in calculus in anything and the limit of
this blah blah as i go to infinity this is exactly what i'm doing i'm going to the point where i'm
saying like at this point it doesn't matter how large the number is the result will be the same
okay and in this way infinitely is an extraordinarily useful concept so i'm not here to rail against
infinity i'm just saying like we really need to understand i mean like let me give you a very
banal example right um from the point of view of you know what to have for lunch right because some
things cost more than others um elon musk is infinitely rich he does not have infinite money
but it makes no difference whatsoever whether he has whatever a hundred billion or two hundred
billion to what he's gonna have for lunch you know like a you know a street person who has five
dollars to them like their fortune is not infinite because it very much matters what
lunch costs right so this is the real sense of infinity which we can and should use but we
shouldn't confuse it with like oh but then your your you know like your formalism is incomplete
because it doesn't encompass infinity yeah it doesn't need to infinity doesn't exist okay okay
well let's come at it from from the other from from the composition you know compositionality and
and um systematicity so that's all about being able to do you know like their main argument was
when you have a symbolic representation you can kind of reuse um the the previous representations
downstream uh composition uh compositionally and when you take a discrete symbolic representation
and you kind of encode it in the envelope of a vector space you have a real problem doing that
because it's it's now like um it's irreversible that transformation right you can't go back to the
to the original variables well it is reversible if you realize that all those real numbers are
actually finite right so notice that real number there's nothing less real than a real number real
numbers are imaginary right real numbers are numbers with infinite precision which is a monstrosity
and many people have said this including mathematicians and physicists right the notion of an
infinite of a number with an infinite number of digits is just monstrous and again in particular
on a computer even if you use you know uh you know like numbers with unlimited floating point
precision right it's limited by the size of your memory so this transfer from which is actually
very important that again that's what tensor logic largely is about from purely symbolic structures
to embeddings in a vector space right yeah that vector space is still finite so there's actually
nothing irreversible about what happened there interesting okay so how can people um you know
find out more information about this and can you just sketch out you know just just just to bring
it home to people where they could actually use it and how how it would be you know better than
what they can currently do right the answer to the first question unfortunately is this this is not
published yet but but hopefully it will be soon so for the moment there is no very good place to
point people to okay unfortunately yeah but but that that hopefully will be fixed soon the the um
the question of where to apply it is our goal for this is that this should become the language
or hope I should say our hope is that this will become the language we're doing
just about anything in AI so for example if what you want to do is actually nothing symbolic but
you just want to build a convent you can express a convent incredibly elegantly in tensor logic
yeah like if you think of for example tensor floor or pytorch versus numpy right they they allow that
thing to be said much more compactly compared to tensor logic they are as bad as numpy is compared
to them right same thing on the symbolic side but of course the real action comes and all the problems
where you have both components the problem with all those problems which ultimately is every problem
in AI right you're always like what happens today that is very frustrating and that's what we're
trying to overcome is like you start from one of these sites these days mainly the connectionist
one which you have a good mastery of and then the other side for example the symbolic one the knowledge
representation the reasoning the composability you just hack yeah and your hack solution is terrible
you're like you're inventing the wheel you're making it square you're trying to make it turn
but it's square right it's just you know it's a disaster and within so logic you can actually have
a very well founded very well understood basis on either side so now you don't have to hack either
side now there's of course two things that you're going to have to hack at the end of the day because
at the end of the day you know AI is intractable and things are heuristic but this you know is
you know you know this notion of a trade-off there's very important in engineering right like
people have been exploring different points on this trade-off curve the point of tensor logic is that
whatever your application is we're moving we're moving you to a better trade-off curve
it's still a trade-off curve but but it dominates the old one for any given x you have a better y
and vice versa okay and just help me understand because we'll move over to you know the discrete
program search and some of Josh Tannenbaum's work in the moment but there are two schools of
thought right there's discrete first and there's continuous first you're on the continuous substrate
but usually the reason for the continuous substrate is stochastic gradient descent
learnability etc etc and like help me understand so are you saying we start with a symbolic
representation and then we encode it into the envelope so where did learning come into it
very good so in tensor logic you can do broadly speaking two kinds of learning you can learn
the structure of these tensor equations as we call them using inductive logic programming techniques
again that whole technology is there and then once you have that you can learn the numbers by
back prop in particular where it's called back propagation through structure because the structure
can vary from example to example but we know what the type parameters are so all of the machinery
of inductive logic programming and all the machinery of gradient descent and deep learning
or not they're both there available to be used as you traditionally have okay what if I made the
argument though that it's almost like the inductive logic you know like the program search that's the
hard bit so if you've already got the program why do I then need to put it into a vector space
no actually these are also at the end of the day in machine learning we're always trying to
learn a program of some kind right the question is like what is the easiest way to do that and
precisely the problem with ILP as with symbolic logic as that's really a couple of problems one is
that if all that you do you learn programs that are too brittle and we don't want them to be brittle
right and the other one is that each type of search has it has its limitations so in particular
in symbolic AI including ILP we tend to use a lot of combinatorial optimization types of search
right what we in AI call search is discrete search and that is good in some ways but also
very limited in others the same thing is true of gradient descent right and now to go to that for
just a second gradient descent is not a continuous optimization algorithm it's not right gradient again
those rule numbers are not infinite precision there's actually nothing continuous going on
in the computer gradient descent truly literally rigorously mathematically is a discrete optimization
algorithm it takes discrete steps the assumption that gradient descent depends on which is that
there's the infinitesimally small updates do not hold and moreover in machine learning as a
numerical analysis we are constantly dealing with this fact that there's a mismatch between
our mathematical conceptual model of the space that we're working with as continuous with the
reality of the computer that is not continuous so gradient so now this is not but gradient descent
still is a different optimization technique with some very important advantage in particular the
key right the power of gradient descent comes from the fact that to move from my current
point to a better one I don't need to try out all the neighboring points because that takes
order of the time of the neighboring points I have a closed form way to compute what is the best
one yeah right and then I move there and this is absolutely brilliant right like we don't want to
let go of that right this is you know Newton's enlightenment is bright idea right the price of
that is that in order to do that you have to make this approximation which again calculus is an
approximation it assumes that certain effects are second order and can be ignored now ironically when
you learn a large deep network these days you're actually in a regime where they cannot be ignored
right because these infinitesimal changes are not that infinitesimal because you take a finite step
right the gradient descent is always taking finite steps which is why it's a discrete algorithm
and and once you take that finite step for any reasonable learning rate the total effect of
the approximations that you've made typically swamps the step that you're taking so the assumption
of calculus that we descent is founded on is actually false now in some ways this invalidates
a lot of our intuitions in many ways and again this remains to be resolved a lot of why gradient
descent works better than people expect it to is in fact that it's doing something else it's
doing stochastic search partly because of vs gd as opposed to being batched partly because of things
like this okay well this is really interesting a couple of places we can go but first of all I
remember you you did the the paper and that that introduced elements of ntk theory as well which
might be an argument against the the discreteness of the optimization but also I wanted to trade
off the the two types why is that an argument against the discreteness well isn't there a
with ntk isn't there like a closed form solution doesn't that kind of like erode the discreteness
of the optimization no I mean so there's several things here but like if you have a closed form
solution absolutely brilliantly go for it yeah right there's nothing having a closed form solution
in no implies that it's continuous or discrete or any other thing right oh it doesn't so so let's
say there was a closed form solution and and it was like an infinite kernel when it represented
some neural network doesn't that erode the argument so first okay so first of all in the work that
so the work that I've done that I think you're referring to is like I have a proof that every
model learned by gradient descent is a kernel machine yeah right and it's something called
a path kernel which is the integral of the of the neural tangent kernel you know over the
overgrading descent right yeah and now the neural tangent kernel does not assume that your network
is infinite most of the theory that people have done with it assumes that the network is infinitely
wide but might but but the the definition absolutely does not require that and none of what I do and in
fact that's part of why you know of its power is that it assumes no infinity of anything it's for
any architecture that you use and in particular you know finite architectures okay interesting okay
so hence the discreteness but right can we come back to this contrasting of the discrete program
search and and the you know stochastic gradient descent on on a vector space now in the vector
space there are certain characteristics you know there are certain symmetries and even though it's
a discrete search through the space I would argue that it's still continuous in nature it has certain
characteristics so contrast those those two forms of precisely so uh exactly I I mean I think you've
put your finger in now the whole point of these continuous spaces right is not that they're continuous
because again that's that's a fiction is that they have a certain locality structure yeah
that you can exploit to very good effect and this is exactly what we're going to send us right now
that locality structure doesn't have to be infinitesimal right you don't need points to be
infinitely close for all this to apply approximately and again there never are and it's always an
approximation now the question is do you want to make these locality assumptions or not right
making them buys you certain things right but it's also potentially unrealistic in some ways
right now this actually to take a very concrete instance of this think of space right we model
space in science in physics and in anything as a continuous thing which it is not right which is
not to say that and by the way physicists are coming to this conclusion right these days the
prevailing views is the it from big thing is that like it's you know space arises from entanglement
etc etc like space is not the fundamental reality right and now I think that where this is inevitably
going one way or another is that we realize that space is discrete right but and this is key it has
certain properties including symmetries like translations invariance rotation events etc etc
that whole approximately or exactly but if those hold and a whole bunch of things like that then
you have you know your latent variable structure right is very well approximated by our notion
of continuous space in which case it would be foolish to not use it right to formulate the
laws of physics and to do computer vision and so on and so forth but at the same time right
if we believe in it too literally we walk ourselves into a blind alley so concretely look at computer
vision right people in the universities of computer vision started out trying to do with
differential equations and for your analysis and all of that good continuous stuff right
because that was the obvious thing to do right and it failed that doesn't work that's why we need
things like deep learning and you know mark of random fields that are discrete grids that use
you know to model the images and whatnot because you are along with the approximate continuity
you also often have large discontinuities and if you can only model the world continuously
then you can't you don't know what to do and the problem precisely that you have all these
phenomena that are like this including you know in vision but also in in turbulence and
condensed metaphysics and so on you got to realize that there are discontinues and not try to
shoehorn them into continuity when that's no longer appropriate. Interesting okay well can
can we bring in ILP and can you contrast like the kind of function spaces that are learnable
in both methods yeah so ILP so let me actually preface this with the following people in every
one of these schools of AI tend to have this view that I can represent everything in the world using
my approach so I can like look prologue is too incomplete so why do you need neural networks
but I can also say neural networks are too incomplete so why do I need prologue and in fact
kernel machines have a represented theorem that says you can approximate any function blah blah
right so everybody has one of these represent their theorems right that says I can represent
anything right so in particular you can do right I mean look first our logic was invented by by
frage essentially to to model the real numbers so it can almost by definition model real numbers
right anything you might want to say about real numbers and and weight in descent and neural
networks and in fact people have even done this so you can say it all in in in logic programming
right so why not just do the well precisely because certain things are much more easily
done in other ways right so what you have to ask about anything but then about you know not the
logic programming in particular like what things are well represented in this way like compactly
represented and then in such a way that learning them and doing inference with them is easy but
and those things are different for logic programming and for things like deep learning which is why
we need a unification of both so what is things like logic programming and ILP good for right
it's precisely I mean it's many things but the key thing is it's precisely for learning
pieces of knowledge that can then be reused and composed in arbitrary ways this is the huge power
symbolic AI that connectionism does not have right it's like I learned the fact here I learned a
rule there and tomorrow you ask me a question and I combine that fact actually several rules by rule
changing right there's a whole proof tree of rules that could have come from very different places
and I do a completely novel chain of inference that answers your question this is spectacular right
and this is surely core to what intelligence is all about and the symbolists know how to do it
the connectionists don't but if I was a connectionist I'd be like you know I know if it was a good one
and the better ones like Yoshio Benji are doing this right is like go and try and understand what
those people understand so you can then not combine it with those other ideas yes yeah yeah I completely
agreed so a huge part of intelligence is this symbolic you know extrapolation
yeah so how do you bring abstraction into this because the thing that I always get caught on
is that the traditional go-fi vision was to you know handcraft the knowledge and actually what we
need is dynamic knowledge acquisition and we need the ability to create abstractions on the fly rather
than just what we do now which is crystallizing existing human abstraction how could we do that
well abstraction traditionally was and still is a central topic in symbolic AI
right like be precise I mean I think nobody questions that having levels of abstraction
someone is very important the only question is how so if you look at classic knowledge representation
planning etc etc abstraction is all over the place if you look at things like reinforcement
learning and I mean even like you know the whole idea or hope of a convent is that it captures
objects at multiple levels of abstraction at least to some degree in reality it doesn't right but
that's what people are trying to do and and not quite doing right well good let's let's let's
touch on that then so I mean certainly in Jan McCoon's view I spoke with Jan the other day he's
got this autonomous path a paper and you know his system is learning abstractions but they're
abstractions which are deducible from base abstraction priors like objectness and you know
basic visual priors and so there's there's this assumption that everything is deducible from
the priors that we put into the model but I have this kind of intuition that abstraction
space is much larger than that yeah I mean so I would even say that if if you arrive at your
abstractions solely by deduction you have a very impoverished notion of abstraction in fact most
of inductive learning is forming abstractions and from abstractions at the most basic level
something very trivial is like I have an example described by a thousand attributes if from that
user rule that uses only 10 I've abstracted the way the other 990 right but but if if um if a
symbolist was here they would talk about intention versus extension and they would say that you know
you're selecting from this infinite set of possible attributes you couldn't possibly
represent all of the attributes and I said I mean just to give you a concrete example you know you
could have a a a a a a you know don't I mean you you just have like this again I like I hate to
bring up infinity again because that's always what these folks bring up but um how could you
select from from a set that large well I don't need to because it is finite but what I need to do is
so but there is actually a good example and you know infinity does not bother us at all at all
there because what it's like if my training set right is a set of strings and those strings are
a a a a a a right going up to whatever number you want to pick like you know a million or a
quadrillion you know or a google right then are is your learning algorithm able to induce that the
the language that these rules come out of right the grammar is you know it's a series of A's right
you and I can do that immediately you know most deep networks have no end of trouble doing that
even though it's that basic so it is a very good example of what symbolic learning and reasoning
can do versus connection is you don't need to go anywhere near infinity to actually have that be a
very elegant example well let me bring up just one other we've touched on a lot of great things
right there's one in this space of things that we've been talking about there's one that I think
is very important which I believe you're also a fan of and I very much am and I think it's going to
you know maybe going back to the question of what I'm interested in that's happening at at
new reps right now or not so new symbolic AI is definitely a big one yeah another big one and
to my mind maybe these are the two biggest ones are most interesting is is what I call symmetry
based learning and these days is is more popular known by the by the by the name of like geometric
deep learning and things like that I think to view geometric deep learning as a special case of
symmetry based learning but this idea of I think let me you know to go straight to the punch line
we know that for example AI and machine machine learning in particular
have as foundations things like you know logic probability optimization and I think another
foundation is symmetry group theory in fact I was having you know dinner with with max welling
just the other day who who of course we've also interviewed and is you know like a great
you know person in this area and we you know I think we have very similar views on this
well Pedro yesterday and taco cohen was sitting where you're sitting so there you go yeah again
I remember talking with taco at some mycimal many years ago where he had published one of the first
papers on this and I was like and he seemed a little disheartened by the lack of interest that
people had and I said to him just wait this is going to be big and we're there now right and
it's going to be even bigger I think but also I think to become bigger and again to jump straight
to the punch line most of the work including me that people have done to date has been exploiting
known symmetries like you know translation invariance is the quintessential example for example we
have something called deep affine networks that generalize con nets to you know you know rotation
you know scaling etc etc this is all well and good but I think if this is and if you look at
newreps today for example most is in that vein yeah and there's a lot of good work to be done
there but if that's all we ever do we will always remain a niche in AI with certain very good
applications like science applications where we know that certain symmetries hold and what not
max and taco are doing things like that but I don't want to just do that I really you know
I'm trying to make progress towards human level AI and I think the key there is to discover symmetries
from data yeah and I think most of us agree with this look it's a hard problem right but that's
what we're here for we want to discover symmetries from data and you know there's an interesting
you know discussion of how to do that you know I have a number of ideas and a number of people have
then the power of discovering symmetries right connecting back to our early conversation is that
symmetries can individual symmetries can be very easy to discover because they're often very simple
but then right by the group axioms axioms you can compose them arbitrarily yeah which means I can
for example by learning a hundred different symmetries of a cat from a hundred different
examples then I can compose them and correctly recognize as a cat something that is extremely
different from any concrete example of a cap that I saw before could I push back on a tiny bit so
I mean in the geometric deep learning proto book I mean they spoke about you know the various
symmetries of groups like SO3 you know preserves translations and angles you know like how primitive
and how platonic are these symmetries and aren't they just like obvious in respect of the domain
that that you're in no very good so this is actually a key question symmetry group theory is one of them
it's a central area in mathematics that and it's very highly developed and it's the foundation
of modern physics like the standard model is a bunch of symmetries and so on but the way and
there is an exhaustive listing of what all the possible symmetry groups are discrete ones you
know continuous ones you know so-called league groups etc etc so at that level this is not naive
because people already have a handle on what the space is right but crucially for our purpose is
for AI that's not enough because precisely because those again the analogy with logic is
actually a very good one here first order logic is to brittle right and and plain symmetry group
theory the way people have mostly applied so far is also to brilliant for the same reason so for
example right something like you know people almost always immediately come up with so like oh
I understand you know I like symmetries would allow you to recognize you know perturbed digits
but a six is not a nine so something like if you just take nice symmetry group theory and
it's like well arbitrary composability as I was just talking about like well now you've just said
that a six you've lost the ability to distinguish a six from a nine yeah right now what we need
precisely is to combine symmetry group theory with the other things like statistics and optimization
and say something like the following the space of things that you can compose is unlimited you
can have you know limited compositions but for example you pay a cost for composing more symmetries
and now when you find the least cost path and that's how you're going to match things or
you know your digit becomes less and less probable to be in six the more you've rotated it right so
now we know how to do all of that very well so we know symmetry group theory very well we know how
to do all these probabilistic costs minimizing blah blah blah things machine learning very well
we just need to combine it to the same way that we have previously combined these things with
first order logic so I'm glad you brought in the cost that that was really really good so um
it there were trade-offs everywhere I mean for example if you want to make the models more fair
and you know prioritize the the low frequency attributes on the long tail the headline accuracy
goes down same thing with robustness if you robustify a model the headline accuracy goes down
same thing with symmetry groups if you introduce other symmetry groups you know that the headline
accuracy um goes down so it all comes back to the bias variance trade-off at the end of the day
and you know where is the limit here like how how much can we optimize these models and what does
good look like the bias variance trade-off is a very useful tool right but but it's not the
deepest reality right the the the way to think about bias variance is that what again talking
about this notion of a trade-off curve there's a trade-off between bias and variance right which
is in some sense unavoidable right in machine learning if you have finite data you're trying to
learn powerful models bias variance is a trade-off and it's a very consequential trade-off in the
sense that for example the things that work best with small amounts of data tend not to work best
with large amounts of data right this is something that we should all you know grow up knowing in
machine learning but so many mistakes have been done because of that because people study things
in the easy or historically that's all they had right and then they're very surprised when something
that seemed not very good like say deep learning right tends out to be better when you have a
large amount of data or they believe in like silly things like you know Occam's razor version that
you know accurate you know simply is more accurate and whatnot so a lot of mistakes have been
been made because of lack of understanding of this having said that what you really want
is to move to a better trade-off curve between bias and variance which you can if you get at
what the reality is right so the real game in machine once you're evaluating your learning
figuring out you're like how much to prune and whatnot or how much to regularize bias variance
is very important but before that the most important question is like what we're trying
to do here is figure out what are the inductive biases what are the regularities that the world
really has at least approximately that we build our algorithms on top of that and then if you give me
a better one than I have now I'll still have a bias variance trade-off but I'll be in a in a curve
where for the same variance I can have less bias and vice versa and that's where the real action is
oh interesting but I didn't quite understand that because you know bias and variance they are
mutually exclusive and I thought at first you were saying well if we understand what the biases
are better you know the the prototypical symmetries of the world we live in then then then we can have
more bias without having an approximation error basically the confusion arises because bias is
a very unfortunately overloaded term right this is not even getting into the psychological notion
of bias like in you know Danny Kahneman's work or even the sociological notion of bias like racial
biases gender biases and whatnot so we need to distinguish so like I just used you know my bad
the word bias into completely different senses completely but not unrelated that's the thing
right one of them is the statistical notion of bias right that you know there there really is a
trade-off between the two right there's a sum of squares blah blah blah right the machine learning
notion of inductive bias it's the preference that you have for certain models of our others
which is really just another way of saying your priors whether they are assumptions or
knowledge right you know maybe actually they're saying instead of bias they're like
what you really want to do is figure out what are the priors why the model classes where the
preferences right the biases are kind of preference that really line up with the world
in reality or the domain and therefore let you move to a better trade-off curve
among statistical bias and statistical variance amazing well Pedro just tell us a little bit
what have you seen at NeurIPS and how's the week been for you so we've already touched on
some of the interesting things that I saw in particular some of the areas that I'm interested
in the thing about NeurIPS is this of course is that it's a vast conference and in the early days
I used to at least go through the proceedings and look at you know the title and maybe the
abstract of every paper and this is not impossible right now these days if all you do is try to walk
through the poster sessions you never get to the end right I haven't been to a single poster
session in this NeurIPS where actually you know actually got through all I like to go through
the poster sessions quickly once yeah and then you know just to see what's there and then go
back to the ones that I found really interesting I haven't actually been able to even finish that
walk through because they're so vast right you're also running to people which is part of the point
and talk and whatnot but like when there's 500 you know posters in every session and there's
3000 papers in the conference it becomes very hard to find the ones that are most relevant
of course an easy thing to do is look at the you know what they I mean something about NeurIPS
this year that I honestly thought was absolutely terrible like a really really terrible idea is
that you know it's a it's a hybrid conference and you know their idea of a hybrid conference is
that there are no talks the talks are all virtual next week and nips is nips this year you know
to a first approximation was one big poster session which I mean to me this is just an
incredibly bad idea yeah so in that sense I haven't gotten as much out of nips by this point of the
conference as I would have in most years right there's also looking at the papers that were
usually selected as oral but this time they call them oral equivalent because there are no oral
papers but they still want to have that distinction and you know the number of those papers these
days is 160 or something which is you know bigger than nips and ICML were you know some years ago
and usually from those papers some of them kind of like jump out at you as being great and very
relevant I've only looked at them briefly right so you know don't don't you know quote me on this
if you will but none of those have jumped out to me you know as like oh yeah this sounds like
something really brilliant and and that I want to dig into but there probably are many I just
haven't you know really have a chance to look at them yet yeah I mean I have a similar reaction I
mean it feels like um that we're at the point of saturation and there are loads and loads of
micro variations on the same idea it's completely overwhelming but what I find is that it's a very
social experience when I walk through the posters I just immediately becoming grossed in conversation
and hours go by and I just think oh my god what have I just been doing for the last and that's
the real point is the posters are very good you know you know it's like the grain of sand and the
oyster the post is the grain of sand the oyster is the conversation that you have with the person
at the post or with or with other people around there to touch on another point that you made
that I think is actually important so you know New Europe's and ICML and so on are bigger today
than they've ever been right actually not strictly true because this recently surprisingly they tend
to say it's gone down a lot you can you know we can ask we can ask we can and should ask why but
we need to scale we you know like there are bigger conferences like the new science conference is
one conference and it's 35 000 people you know every year and they make it work I don't think and
it's good to experiment I think you know New Europe's at the scale that is today can work but it is
not working very well one of the ways in which it's not working very well is that we need to think a
lot more and I understand this is working it's hard and people have day jobs or not so this is not
you know a criticism in that sense of life we need to really work on making it easy for people to
find the papers that are relevant to them like number one number two and maybe even more important
there is more machine learning research today than ever but in some sense the diversity of that
research is in some ways lower than ever so another point that you brought up and I think is very
important to do with the scaling of New Europe's and the machine learning community is that we have
in just raw numbers more machine learning and AI research going on today than ever before by
an order of magnitude but in terms of diversity there's probably less diversity in the research
now than there was before which is a tragedy right so I understand why people have kind of like
converge to deep learning you know I'm a huge fan of deep learning right I was doing it before it
was cool as they say and whatnot but the extent to which like 90% of the community not just in
machine learning but AI but AI is not just pursuing and not even deep learning but a
special type of deep learning which is which you might call applications of backprop is extremely
undesirable right we have you know an infinite number of micro improvement papers along a
particular direction that is almost certainly a local optimum right and we're just digging into
that local optimum with ever more papers and never you know more you know minimum publishable
units when this large amount of manpower that has come into the field or you know or is moving around
we really need to have a greater diversity of research in machine learning within deep learning
within AI and so like we are making very poor use of our research you know manpower right now
and we see that very much at NeurIPS today yeah I mean Sarah Hooker talked about the hardware
lottery you know being stuck in a basin of attraction determined by hardware but there's
also an idea lottery it might just be the case that NeurIPS historically has always been very
connectionist anyway I mean well it hasn't right that's one of the ironies but it's okay I wasn't
aware of that okay oh absolutely not I mean in fact the joke is right that NeurIPS started
in the 80s it was called Neural Information Processing Systems right and by the 90s it
should have become BIPs for patient information processing systems right there was this study
that they did at one point of predictors of acceptance and rejection among words in the
title and the biggest predictor of rejection was the world neural really and this was very famous
in the field because indeed if you could you know 1990 something you were submitting papers to NIPs
with the world neural in the title you didn't know what you were doing and then in the in the
2000s right it became BIPs or should have become BIPs sorry KIPs kernel information processing
systems yeah and in fact I remember having lunch with Yoshio Bingo at the ICML in Montreal in 2009
and and we were talking about this right the fact that every decade and you know not a new paradigm
but another one of the same paradigm seems to now be on top right and you know you know he asked
like so what is the next decade going to be and I said it's going to be DIPs deep information
processing systems and then we both laughed and I could tell that I believe this but he Yoshio
Bingo was actually skeptical of this so you know the deep and little did we know right if somebody
told us that you know this is going to be on the page of the on the front page of the New York Times
in a couple of years would be like what are you smoking right so the way to which this decade has
been DIPs is just mind-blowing but looking forward right and to this point of you know diversity
in research approaches I think if you extrapolate naively from the past the next decade will be
about something else and the trillion-dollar question is what what is that else going to be
amazing okay um you watched Charmers talk right yeah what's your high level view
I thought it was a nice talk I thought it was a very appropriate talk for an opening talk at the
conference actually if New Europe's had like some conferences a dinner talk right which is supposed
to be interesting but not as you know deep or as technical as others this would have been the perfect
dinner talk from Europe's because the topic is very current right our machines sentient and you
know who better to talk about it than Dave Charmers right the world's expert on on on on
consciousness right and and by and large I thought the talk was excellent in fact you know when
journalists ask me questions you know consciousness is like one of their top three right along with
terminator and you know unfairness or something like that right and I will point them to this
talk because it kind of like lays out you know the you know the ground and you know it's good for
people to at least have that those things in mind at the end of the day so I think of course the
notion that lambda was sentient is you know ridiculous as as most of us do you could ask a
slightly more fine when question was if if if if consciousness is on a continuum right which I think
Dave believes in and if you believe in like this you know IT theory and phi and whatnot you know
phi is never zero right so there's always some consciousness right panpsychism and whatnot
I'm not saying I believe in that we could we could go into the but like if you believe in that then
you can ask well on that scale you know where is lambda yeah where are these large language models
and and and surely higher than previous AI systems right but in my view still very very very far
and I think what you want to keep in mind is that consciousness the not does not increase
continuously precisely there's these transitions where you go you know more is different is the
is the famous you know phrase about emergence right consciousness is very much an emergent
you know phenomenon and I think what happens is that there are points at which your consciousness
will leap maybe a thermostat does have consciousness like you know or you know or purpose or whatever
right like like people in like people like McCarthy for example had had had that as an example
but the amount of consciousness is minuscule and and that and the way I will put that is that
these large language models still have not passed that first threshold
interesting so so in a similar way to some of the discussion about large language models
there are kind of scaling breaks in the levels of consciousness me charm has made the comment
though that rather than it being a pure continuum he said that a bottle was not conscious but then
there was a kind of no yes so very key point scaling is part of it but not only it's not just
so your cortex to a first approximation is a monkey brain scaled up right there was a module
there that evolution discovered and it really paid to keep making more and more of it and we can
easily speculate why but the point is so let me contrast two things right which is true for
consciousness but also for just AI in general like a lot of people are scaling believers and like
open AI is the poster child of this in a quite conscious ways like we're just going to scale
the heck out of things and then a lot of people like you know Gary Marcus being a good example
did they just completely poo poo that they say like oh no this is a joke right and I think the truth is
that scaling is good right again you know part of what we are our intelligence is scaling but but
but the question is what are you scaling and the things that we're scaling today it doesn't
matter how much we scale them we never get to human level intelligence or consciousness so I think
we need some fundamentally different algorithms if you want to think at the level of algorithms
or fundamentally different architect architectures if you want to think about it in a way and then
scaling those up at some point will give us consciousness if you leave that it's possible
for a computer to be conscious but we're not there yet either in terms of the scaling although
actually scaling is actually the easier part of this way we're actually at the point where a computer
can have the same amount of computing power that that your brain does which was not the case before
but the bigger deeper problem and the more fundamental one is like we need the architecture
to scale right and this is where I sympathize you know with people like Jeff Hinton who's just you
know playing with you know ideas using mathematics and very small examples which in some ways
sounds very underpowered but I think it's people like that they are going to come up with the things
that we then scale as in fact it was David Röhmelhardt doing that kind of work that invented
backprop right if he hadn't invented backprop this whole industry would not exist so what I think
is that the real backprop the real master algorithm is not there yet and we need to discover that
first and then we and then when we scale that up which will not be trivial but will be much
easier by comparison then we'll have you know human level intelligence consciousness etc
interesting okay and so Charmes is a structuralist computationalist so you know he thinks information
not biology and he's also a functionalist right so you know which is very similar to to behavior
and you know Hilary Putnam made the move that you can kind of like represent a computation in any
open physical system and he kind of like used that you know if you if you follow that line of thought
it almost trivializes computationalism because it you know it leads to panpsychism very very quickly
so so first of all I mean what what's your take on this idea that information could give rise to
intelligence and consciousness so I agree like most scientists and I think in particular most
computer scientists that to a first approximation the substrate does not matter and in particular
you're not going to convince me that something is not conscious just because it's not biological
there is no reason to think that only biological things can have consciousness now the deeper
problem and you know indeed the heart problem is that so as Dave Chalmers defined it conscious so
there's a basic fork here which you've alluded to which is if consciousness is subjective experience
then all these questions about consciousness are ultimately unresolvable because only I have
my subjective experience I know that I'm conscious no one can persuade me of the contrary I don't
even know if you are conscious let alone some machine right so if if consciousness is an
intrinsic property of something that cannot be evaluated from the outside then we're doomed
we're never going to answer this question and maybe that is the case right so I'm not saying
that's false and you need to always keep that in mind but now if we're going to make any kind of
progress right we need to look at what are to to generalize a well-known term the external
correlates of consciousness right one of those which has been well studied by people like
Christoph Koch and and so on and I think that's a very good direction is the neural correlates
of consciousness right what goes on in your brain that correlates with consciousness and we've made
a lot of progress with that you can also talk about what are sort of like the informational
computational correlates of consciousness are there computational structures that support
consciousness and the ones that don't I think that is also useful thing to do less develop it
actually inches dispense psychism because it's not like everything is consciousness just because
it can compute some computations after this emergence and these you know phase transitions
may give rise to consciousness whereas others it doesn't matter how much of them you have they will
never be conscious so I think this is also a very useful way to make progress on this question
and one to which AI versus you know a neuroscience or psychology is very well suited to interesting
so on on the functionalism point and I think chalmers has been very very consistent he uses
this kind of calculi to reason about intelligence as well so a system is intelligent if it can
perform reasoning if it can perform planning if it has sensing and so on so we have this
collection of functions and then he's kind of like moved this over to the domain of of consciousness
so similarly if a system performs these these functions and is used in a positive and a negative
way so some functions would indicate an absence of consciousness and some functions would you know
lead to the presence of consciousness and it's kind of like leading towards a you know touring
test for consciousness I mean do you kind of support that that so yeah that's a very interesting
question in fact you know I was having dinner with with Dave after his talk and I actually
brought this up because it wasn't clear from from from his talk and I said look this is the answer
that I usually give to journalists when they ask me you know will machines ever be conscious and
whatnot and and ask me if you and then ask me if he agreed with it and actually expected him to
disagree but but I think again don't want to put words in his mouth but but that he agreed right
and the answer is the following is that human beings right as we've discussed have an amazing
tendency to anthropomorphize things it's reasoning by analogy and what happens I used to say this is
what's going to happen at this point is this is what is already happening is that as soon as a machine
behaves externally even vaguely like it's consciousness we immediately start treating
it as if it's consciousness so if you look forward 10 20 50 years from now we will just treat ai's as
if they're consciousness and people won't even ask that question they will assume as I conscious in
the same way that we assume that each other that we're conscious right and but then and so like
from that pragmatic external point of view maybe the question is answered right but you could be a
philosopher or like sort of like a very you know a rigorous you know technical person and so like no
no no no I really want to know if things they may look you know conscience from the outside but are
they really right but that question as far as I can tell unfortunately at the end of the day is
probably unanswerable now there's a middle ground between these two things that maybe is where we
wind up and to me sounds like probably the best thing that we're going to be able to do which is
that like our understanding of the neural information olex et cetera correlates of
consciousness evolves to a point where we have the feeling that we do understand consciousness
it's not just the late person calls this consciousness even though haha it's not like
lamb does not conscience you know poor bozo et cetera et cetera it's like you know there are
many analogies to that in the history of science there used to be a lot of things that that were
like magical right and we were like oh we're never gonna stand like life was magical right life did
not obey the laws of physics it's just something else right this sounds laughable right now but it
wasn't laughable at all then right and now it's not like we've understood everything about life
very far from it when it's like there's dna and their cells and then and this is how it all arises
right and I think we're at the point in consciousness where it's like oh consciousness is some so
beyond us right I think we will get you know there will be a structure of dna moment in the history
of the study of consciousness and I think I think things like phi and this you know it3 and what not
they're very brave attempts to make progress in this direction I think you know like julio
turn on in a way is you know very deluded in thinking that he has nailed what consciousness is
right I think you know phi maybe is an upper bound on consciousness but with steps like this
hopefully at some point and very much with the help of AI right AI is really useful for this
because it's a brain that might be consciousness that we have a lot of control of and you can do
experiments that you can't you know with people right so I think we will make at least some progress
in that direction for sure maybe to the point where we feel that yes we do understand what
consciousness is we're not asking ourselves that question anymore and then we can point to things
and say this is consciousness this is that kind of consciousness that amount of consciousness
and so on yeah that's really interesting I agree we're making a lot of progress in getting a handle
on this and although um the the biggest game in town is still the computationalism game and as
you say historically the only alternative was mysterious and my friend professor mark bishop
that he said that that's one of the reasons why he's become interested in the foreseeing
cognitive science because for the first time it's given him a kind of robust alternative to
computationalism but just coming back quickly um you know as charmele's reference thomas nagel
you know which is that um it is something it is like to be a bat uh what do you think about that
so um I'm not sure your question is but let me what what are you mean do you do you agree that
there is something it is like to be a bat oh absolutely right so there is more and more than
that right there is something that it's like to be a bat and it's very different from being a human
right and we grossly underestimate right again we do this thing that again it's a heuristic it
works very well it's like we project ourselves into the bat because what else could we do right
but then what you see is a bat seen through the mind of a human right and in fact there's this
famous I would say even more famous you know you know notion from from Wittgenstein right
that if the lion could talk I would not understand anything that the lion was saying yeah because
his world is so different from mine now I actually think I think this is a very important position
to as a reference point right certainly defensible one and you know Wittgenstein was a good defender
of it but I actually think that this is going too far I think ultimately I mean never be able
to completely know what it's like to be a lion but we can make a lot don't underestimate us either
right we can make a lot of in was into understanding what it's like to be a lion much more than we
understand today same thing for a bat and you know you could have also asked us for a fruit fly
right in a way a fruit fly is more different from us than a lion but it's easy to understand right
because at some level that thing is so simple that we can understand what's going on with it because
it's not that deep yeah that's a beautiful quote actually um so so closing this off do you think
that large language models are slightly conscious or will be in the near future I think language
I think large language models are not slightly conscious by the reasonable you know everyday
definition of the world slightly meaning that their consciousness so I think that either their
consciousness is just zero right if somebody asked me like you know how much you know consciousness
does you know lamb they have tell me in one word and the answer would be zero right but another
answer which is hardest thing from the first one is epsilon right maybe it has a very tiny amount
of consciousness but it's so tiny that it doesn't even qualify as slightly again this gets back to
what his architecture is it actually gets to a lot of things but for purposes of this discussion
right lamb then these large language models are not very different from a big lookup table
any big lookup table is not conscious now I mean there are a lot of interesting
distinctions that you can make it well what if what I have is an efficient approximation to
lookup table isn't that what your brain is right and and I would say yes and and then people say
well but then why is your brain conscious but not the lookup table right and and precisely the
interesting question is that the consciousness comes about from the fact that you have to concentrate
all of this information you know you know in real time into something you know very compact
and that leads to action continuously right so to put this in another way maybe God is in
consciousness because he doesn't need to be right if you're omnipotent and omniscient you don't need
to be conscious you are effectively just a lookup table exactly and I loved your response earlier
about the grain of sand and the oyster I thought that was a beautiful way of looking at it and and
having recently studied so I mean personally I think it's a lot to do with intentionality and
agency but I remember you responded to that just final quick final quick question what's your
definition of intelligence so let me start with a technical definition which is unfortunately not
widely known enough and not appreciated enough but I think it's a really important one to have
right intelligence is solving NP complete problems using heuristics this is the real
technical definition of AI right and there's a lot packed into that right the fact that it's NP
complete problems and the fact that it's using heuristics if your problem is solvable with the
lookup table with polynomial algorithms you don't need intelligence and there's no intelligence
there it's when you start solving hard problems using heuristics that you're getting into the realm
of intelligence moreover NP complete is not the same as exponential right the crucial thing about
an NP complete problem that connects very directly to our entire discussion of utility and whatnot
is that the solution is easy to check this is the key if you're working on problems whose
solution isn't impossible to check effectively I can't even tell if you're intelligent or not
the whole thing about intelligence in humans and machines is that how you solve the problem
requires a lot of intelligence a lot of computing power and whatnot but then I can easily check the
solution that hang on but could that say step away from behavior then if you're saying that you
know like you have the percepts the state and the action and you're saying the state is also important
no so to answer that head on intelligence is not behavior right intelligence to to give a slightly
more general definition and then there's several and they all have their merits intelligence is the
ability to solve hard problems then more concretely it's NP complete problems and using heuristics
but like for example if you create an AI system that cures cancer it doesn't behave in the sense
that a human and a robot behave but but you know it's damn intelligence it's more intelligent than
we are right it would be childish to deny intelligence to that system no matter how it solves cancer
if it sounds a ridiculous simple if it finds a ridiculously simple way to solve cancer then it's
even more brilliant right in fact the simpler your outcome the more intelligent you are right it takes
intelligence to produce something simple wow concretely in many circumstances in particular
evolution right intelligence manifests itself as behavior there's a sequential decision making
problem there's an agent in the world that said a certain stuff being a stochastic parrot and I
think also from you know theoretical reasons by analyzing what a transformer can represent and
how it can learn my best guess which could be wrong again I don't think anybody has the
answer to this and it's interesting question is that those let's transformers right not LLM
scholars let me use more of like a task rather than the you know than the architecture transformers
have a certain limited ability to do compositionality very limited to compared to full logic
programming etc but exponentially better than something like an ordinary multilayer perceptron
yeah and if you just I mean even a multilayer perceptron or any learning algorithm is more
than a stochastic parrot because it's general the whole point of machine learning is to generalize
beyond the data if you generalize correctly beyond the data you're not just a parrot anymore
and you know I think it's not an accident that that term stochastic parrot came from Emily Bender
my linguistics colleague at UW who does not understand machine learning she's a classic
linguist of the chompskin variety who does you know does not fundamentally understand what I think
you know she might disagree what machine learning is all about and she would probably look at any
learning algorithm and say that it's a stochastic parrot missing the fact that the whole point of
machine learning and the thing that we focus on from you know beginning to end is generalizing
and as soon as you're generalizing correctly even if you have no compositionality you're
already doing something that's that has a little bit of intelligence and that's beyond a stick what
a parrot would do yeah I mean to be fair it's not it's not a binary and at the time I thought
they were stochastic parents as well I've updated my my view and you were talking as well about
creativity there's a kind of blurred hyperplane of creativity and we discussed you know where that
hyperplane sits but you know what's really interested me I've interviewed quite a few people
that are working on working on in-context learning in these language models and it seems like these
language models are almost almost like a new type of compiler you know you're writing a program
inside the language prompt and they seem to work extremely well outside of the training range if
you're doing like basic multiplication tasks I think it is useful to look at them as a new type
of compiler in fact I've been saying for a long time that you know like there's there's this continuum
from programming an assembly code to high-level languages to doing AI right the point of AI is
to continue along that path to making the language that computers speak ever closer to ours so that
we can just program them by talking to them or writing things at them right yeah having said that
I think that you know what goes on in the innards of a transformer right is actually still
very primitive for lack of a better word right there's a lot of so something I tweeted that
got a lot of follow-up from people like Yan and Gary and and who the pro because they were all
bringing in their own angles so this was like I said and I think this is an interesting question
it's like the interesting question about transformers is what needs to be added to them
to get real intelligence so we should not deny what they have like the attention mechanism in
particular right and the embeddings and the context so like there are two very important
things in transformers that are beyond what was in neural networks 10 years ago and and our key one
of them is attention right attention is a real advance and the other one is context specific
embeddings right each of these ideas is important in its own right and combining them to get as
very powerful right again because the context sensitive embeddings get that the similarity
part of intelligence the attention combined with the context sensitivity of the embeddings
gets at the compositionality part so so they do have so they are a couple of steps forward on the
road to human level intelligence but there are many more and rather than you know either saying
like oh they're just parrots they don't do anything we're saying like we've almost solved the eye
what we really should we should try to understand better you know how how the you know the attention
and the context is dependent embeddings work which we don't but we also need to focus like
now what are we still missing because we definitely are and that's really where most of our focus should
be yeah I completely agree and and also just in defense of end room and I think she's she's a
brilliant linguist and and I personally think having that diversity of views from different
people is is useful no I mean I so I very much think that having a diversity of views is very
important and I think something that I'm always saying to my deep learning friends who
can't stand you know who hate the guts of Gary Marcus is we really really need informed critics
yeah and very typically your informed critics are not people in the field we are experts but
then we also suffer from the distortion of being experts it's people in the just in adjacent areas
and people like linguists and psychologists are very much those people they're in adjacent areas
enough to have a good critique of AI so for example something that Jan is always throwing at at
Gary Marcus that kind of doesn't sit well with me says like well you should try building a real
system sometime and you can't criticize us until we do if we take the attitude that only engineers
can criticize engineers we're doomed right having said that there is a very big distinction between
the knowledgeable informed critics like Gary Marcus and the not so knowledgeable not so well
informed ones which unfortunately Emily is an example I mean she's my colleague at UW and I've
talked with her about some of these things and her criticism of machine learning unfortunately
like a lot of people comes from a place of actually not fundamentally understanding it very well
but people do people do say that Gary isn't an expert in deep learning and and that he's you
know attention seeking what would you say to that no he's he's he's not an expert in deep learning
and you know so like I agree with some of his criticisms I disagree with with others probably
on balance I disagree more with him than I agree but so first of all there is a value having critics
like that number one but then number two the reason his criticism I mean it would be better if he
was also an expert in deep learning and made the same criticisms and then the problem is that often
his criticisms are wrong because he has a mental model of deep learning that is already outdated
or is oversimplified right but that to some degree is unavoidable but the thing that makes his
criticism valuable is that he's doing it at a level where on a good day on a bad day his
criticisms miss the mark but on a good day which is the ones that matter his criticism is actually
useful because it's at a level where you don't need to understand the details it's like you claim
to be producing intelligence I as a psychologist know a lot about intelligence that's what I study
for a living right he knows more about aspects of intelligence than I do yeah and and and from that
point of view what you're doing is lacking and and and that I mean like he's written the whole books
about you know again because this goes back to when he was a PhD student and you know and symbolic
learning and whatnot there are very you know the deep learning folks have repeatedly underestimated
how well he understands some of these problems because as a psychologist in particularly interested
in language learning he's actually thought very long and hard about them oh I know so I've read
his his book and I mean we've had him on the show three times which book um the algebraic mind yeah so
relevant one yeah as a psychologist you know he spent a lot of time studying how children learn
rules right and he talks very elegantly about a compositionality and we've spoken about this
it's irrefutable and I agree with him and we've supported him I guess um some of the things he
argues are based on ethics politics and virtue and and some of the things like compositionality I think
are irrefutable I mean I think irrefutable is a very strong word I wouldn't say that they're
irrefutable I would say that they have they have very strong backing which the connectionists have
not been able to effectively refute but some of the criticisms that they have you know meaning
people like Pinker and Prince and whatnot famously of connectionists in the 80s some of them are still
valid which is very salient but some of them not really and again to go back to the daddy of this
whole school of thought who's Chomsky right his you know he made his name basically panning
things like you know Markov models of language in Graham models which you could say large language
models are just a very glorified version of right but and at the time you could that criticism was
very apt and and you know and timely and it was useful right but but but and in famously like
is like you can't learn a context free grammar but context free grammar is what you do well
actually now we know formally that you can learn a context free grammar and and you know
because you only have to learn it probabilistically which is what we do and what our systems do
so his criticism was just you know mathematically off the mark but also when you look at systems
that do speech language etc etc it is that statistical approach that he made his name
panning that has prevailed and for reasons that we understand very well and large language models
are just the latest greatest expression of that so at that level a whole Chomsky and Pinker
Gary Marcus view of things not only is it not irrefutable it has been refuted
hmm okay let's just quickly come back to your definition of intelligence so um solving NP hard
problems I assume you would zoom out a little bit and you know it's more of a meta learning
algorithm so the ability to so to solve different problems
yes so it's if very good point if all you have is the ability to solve one NP complete problem
that does not qualify as general intelligence right there's there's like these days this is a
common definition to make there's different difference between you know narrow intelligence
and general intelligence and AGI and whatnot right and if you only solve one NP complete
problem very well you have narrow intelligence is the way I would put it but you do not have
general intelligence general intelligence is precisely the ability to solve a limitless variety
of problems all that have this characteristic of they're hard to solve but the solution is easy
to check right I mean if you have the ability to solve problems whose solution isn't easy to check
then maybe you're intelligent but I can't decide whether intelligent or not interesting okay and
actually Gary did he put a paper about 20 years ago talking about how neural networks can't
extrapolate I think it was when he encoded numbers with a with a binary encoding or whatever
and we've been on a bit of a journey on on this so we had Randall Bellistriero I've
interviewed him yesterday he's got this paper called the spline theory of neural networks it
basically says that a neural network decomposes an input space into these input-activated
polyhedra and when we first read that we felt that it kind of indicated Francois Chollet's
assertion that neural networks are locality sensitive hashing tables and they only generalize
within you know these tiny polyhedra and Randall's now updated this view to say in contrast to
decision trees these hyperplanes they actually inform a lot of information in the extrapolative
regime outside of the training range so I always thought it was the inductive priors that gave
the extrapolative performance on on neural networks by photocopying the information everywhere
and so like you know this is a great example of where you know Gary might update his videos because
even basic MLPs are far more extrapolative than anyone realized this is a very interesting question
but the way I would put it is that in that regard in some sense both of the sides are right and the
reason they're both right is that we're in very high dimensional spaces yeah and we're in a very
high dimensional space the follow thing can happen which is you know you have a data point
and you generalize to a vast region around that data point and it's unfair to characterize these
things as saying they just interpolate in some sense they really do extrapolate but at the same
time that vast region that they generalize correctly to is an infinitesimal fraction
of the much much vast the reason that they have not generalized to be you and I can
so so you got to keep that distinction in mind and then in particular right I like to say that
deep learning is nearest neighbor in curved space and both parts of that are very important right
so you know Jan Likun was famous you know during the glory days of kernel machines
for saying that kernel machines are just glorified template matches right and of course they didn't
earn him any friends but he was right they really are just glorified template matches
kernel machine is really a souped up more mathematically elegant and blah blah blah
version of nearest neighbor right and the nearest neighbor is just a template matcher
the beauty in the power of nearest neighbor though is that there is a neighborhood within
within which often it generalizes very well right now I think what Jan was missing and I
probably still is is that coordinates and deep learning they are still just a they are also
glorified nearest neighbor except more glorified and the way in which they're more glorified
which is very important is that they are doing nearest neighbor in curved space they are still
just doing you know the generalization by similarity which he could argue is all that
machine learning does is generalizing by similarity another notion of similarity can vary right
but the important thing that they've done is that nearest neighbor just uses some distance
measuring the original space whereas the neural networks are warping the space to make the problem
easier for the nearest neighbor you know essentially dot product based similarity
computation that they're actually doing oh sure but you're very much arguing this is the way
François Chouelet puts it that you know you have all of these transformations and and you kind of
distort the space you know to represent the data manifold and you know you want it to
you stop sgd at the right time so that you approximate the data manifold and you can do this
kind of latent space you know interpolation on the geodesic of that manifold but you know
Randall's idea is completely away from that idea of you know these models learning this curved space
and so if you do slice the space up with these hyper planes rather than it being a locality
prior which is what you're talking about these hyper planes give you globally relevant information
to things that are you know miles away from the training data yes so but these two perspectives
are are more similar than you might think because I can take a distorted version of space and decomposing
to polyhedron right and one or the other might approximate what's really going on better I mean
these new networks do form curved spaces except they're in practice they're not curved because
they find it but but ignoring that right when let me put it this way an eloquent example of this is
if you look back at the original space right again treat this thing as a black box where does it
generalize to does it generalize only two things new networks as we have them today does it generalize
correctly only two things that are locally near the data point or you can generalize well to things
that are far right and the thing is that with nearest neighbor you buy you know almost intrinsically
you only generalize period at all to things that are local the beauty of of deep learning and of
the space working that's going on is again going back to this notion of the path kernel is that
you're actually doing a nearest neighbor computation not just in a space that's what but you're doing
it in the space of gradients which actually means that you can generalize correctly to things that
are very far from your examples except they look similar in gradient space a very simple example
of this is a sine wave right if I try to learn the sine wave using nearest neighbor I need an infinite
number of examples right because you know like what I've learned over here helps me not at all with
the next turn of the sine wave like that continuous extrapolation right at some point there's this
disaster where if the last piece of the sine was going up I just keep going up and getting more and
more wrong right and in fact this kind of thing does happen in neural networks but they also have
the power to say like and this again this also happens which is I'm going to transform this
space more into a more intelligent one which is the space of the slopes right and now if I've seen
one cycle of the sine wave with some density of examples by similarity in that transformed space I
generalize correctly and trivially to every other turn of the sine wave so there's a very big
fundamental difference between the two interesting and you think with an MLP it would be possible
to have that kind of extrapolative generalization on a sine wave well so people have studied this
in multiple ways and the problem so the question is it depends on what are the
basis functions that it's using yes so something that we didn't allude to at all in this conversation
but analyze all of this is like what is your choice of basis functions right and the thing is
an MLP with the traditional say sigmoid or allude basis functions will not learn this no matter
for obvious reasons right and again you can represent it right the representative theorem
is there like you like the sine wave is just one sigmoid and then another one you know with a minus
sign and then another one but the data doesn't let you learn it yes right if as a basis function
you have sine waves which is nothing unimaginable that's what a Fourier transform is then then you
can learn it so easily it's not even funny so it depends dramatically on the basis function
and the question really becomes what are the basis functions and the and the architect of it let me
generalize correctly through a lot of things including this such that for example and this is
a very simple test is like I can nail a sine wave with a small number of examples without it being
one of my basis functions yeah exactly and then this all comes back to you know we're talking
about inductive prize and the bias variance trade-off and even symmetries actually I mean the
taco cohen once said that you know if you encode all of the the symmetries into the label function
then you would only need one labeled example so it's always a trade-off between how much induction
are you doing well interesting you should say that I understand why he says that and it's
not technically wrong but I would say that practically what you need is is is a such a set
of symmetries per region of the space right per cluster right but but but you know in another way
I would actually make an even stronger statement which again is very very perfectly mathematically
sounds same when you say is an object is just the sum of its symmetries or a function if you
tell me all the symmetries every last one of an object you've defined the object so if I can
learn the symmetries at that level I don't need anything else of course as we already discussed
that's not the whole answer likewise with any function if you tell me all the properties of
the function there are there you know and to be more precise all the symmetries of a function
at some point you've told me the whole function and vice versa from the function I can you know I
can read out all the symmetries that it has in principle doing that in practice can be you know
very difficult and subtle thing to do that's a beautiful thing to say you give me the symmetries
and I'll give you the object yeah exactly amazing professor Pedro Domingos thank you so much for
joining us today it's been an honor thanks for having me amazing
