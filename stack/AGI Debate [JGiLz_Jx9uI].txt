So we will be live on YouTube in 20 seconds. So we are live.
Ladies and gentlemen, welcome to the third edition of the AI debate, the AGI debate.
I am Vincent Boucher, president of Montreal AI and Quebec AI.
We are get read here today to engage in a discourse of paramount importance.
It falls to each and every one of us to answer that the development and deployment of AGI is guided by a deep sense of responsibility and a commitment to ethical principles.
By coming together as a community and approaching the AGI debate with the utmost care and diligence, we can harness the full potential of this technology for the betterment of humanity.
In our inaugural debate, we were privileged to host Dr. Joshua Benjo and Dr. Gary Marcus as they explore the question of the best way forward for AI.
The theme of our second AI debate was moving AI forward, an interdisciplinary approach.
For participants, we are Ryan Callow, Yejin Choix, Daniel Kahneman, Silas Kidd, Christoph Koch, Louis Lampe, Fifi Lee, Adam Marbelstone, Margaret Mitchell, Robert Ness, Judea Perl, Francesca Rossi, Ken Stanley, Rich Sutton, Doris Tsao, and Barbara Tversky.
The AI debate too was moderated by Gary Marcus.
Today, we are honoured to present the third edition of our Estimate Debate series, the AGI debate.
Our speakers are Eric Brinjofsson, Yejin Choix, Noam Chomsky, Jeff Klum, David Farrussi, Arthur Davila-Garsens, Michel Rampel-Garner, Delyb George, Ben Gorzel, Sarah Hooker, Andrew Casperson, Conrad Cording, Kaye Fouley, Francesca Rossi, Joagin Schmetuber, and Angela Sheffield.
The hashtag for the event is AGI Debate.
The moderator and co-organiser tonight is Gary Marcus. Gary Marcus is a leading voice in artificial intelligence. He is a scientist, best-selling author, and an entrepreneur.
He is well-known for his challenges to contemporary AI, anticipating many of the current limitations, decades in advance, and for his research in human language development and cognitive neuroscience.
His most recent book, Rebooting AI, caught red with Ernest Davis, is one of Forbes' seven must-read books in AI.
It was my pleasure to host him in 2019 in a debate with Yoshua Bengio when we started this series and to work together with him again in 2020 for AI Debate 2.
And this year in preparation for tonight. Please welcome Gary Marcus.
Thank you very much. Thank you very much. Thank you everybody for coming. I can't share a screen yet, Vince.
Yeah, you went.
Still waiting.
Just I have to stop sharing this.
Any moment now when I can actually share a screen.
It's not working.
It says that someone else is. Yes, I'm still sharing it. I'm trying to see how I can stop sharing it.
And one set of slides you don't have our mind.
Just I don't see where to stop sharing it.
It might be right on top of your window for a stop sharing button.
Sorry.
Yeah, I have done so it's done. So it was high done by another button.
All right, please go ahead.
I'm going to give you all a very brief history of AI. My tongue is a little bit in cheek, but you can imagine that this is a history from slightly in the future.
It's also idiosyncratic, but so be it.
In the beginning, there was hubris in the beginning of AI, going back to the 1950s and I think this quote from Marvin Minsky captures it very well within a generation the problem of artificial intelligence will be substantially solved.
And that hubris was good because it raised a ton of money, but spoiler alert not every promise that was made was kept.
Indeed, when we look back by 2012 35 years after Minsky's famous prediction, the problem of artificial intelligence had not in fact been substantially solved.
And then there were GPUs and the GPUs were good.
And according to legend in November 2012. Well, that was the moment when everything changed. It was a front page article, November 23 2012 in the New York Times, scientists see promise and deep learning programs.
John Lacoon reported there's been a number of stunning new results with deep learning methods which I think everybody would still agree.
And then John Markoff proceeded to write that the advances have led to widespread enthusiasm on researchers who design software to perform human activities like seeing listening and thinking.
They offer the promise. Note that word promise of machines that converse with humans and perform tasks like driving cars, working factories raising the specter of automated robots that could replace human workers.
Not everybody was convinced. Some really obnoxious guy two days later tried to spoil the party. He wrote an article in the New Yorker called is deep learning a revolution and artificial intelligence.
So realistically deep learning is only part of the challenge of building intelligent machines, such techniques lack ways of representing causal relationships such as between diseases and their symptoms, and are likely to face challenges and acquiring abstract ideas like sibling or
identical to they have no obvious ways of performing logical inferences and they're still a long way from integrating abstract knowledge such as information about what objects are what they're for and how they're typically used.
And he snuggly said paraphrasing an old parable that had been used in AI some years before.
Hinton has built a better ladder, and Hinton was perhaps given slightly too much credit as this author really later learned, Hinton has built a better ladder but a better ladder doesn't necessarily get you to the moon.
And at that moment, almost 10 years ago to the day, the basic tension of the next decade in AI was arguably established.
In March, 2022, nearly a decade later there've been considerable progress but some issues still remain. And so our obnoxious critic wrote this piece called deep learning is hitting a wall that angered the entire field or large fractions of it.
He said deep learning systems are particularly problematic when it comes to outliers that differ substantially from the things in which they are trained.
Here's an example about cranberry grape juice and said for all its fluency GPT three can either integrate information from basic web searches nor reason about the most basic everyday phenomena.
He said that current deep learning systems frequently succumb to stupid errors. This was in March 2022.
He said that still others found that GPT is prone to producing toxic language and promulgating misinformation.
And then there was dolly dolly to technically speaking and there was much rejoicing, because dolly could generate images from text like teddy bears working on new AI research on the moon in the 1980s.
As Sam Altman, the CEO of open AI said it's fun and sometimes beautiful, and no one could argue that a few minutes later, I think an hour and 20 minutes later, Altman's pronounced AGI is going to be wild and many people thought that this was in fact the first coming of AGI.
Sam proceeded a few hours later to say the part of open AI that is best relative to expectations going in is just how thrilling it is to be in the room when the scientific frontier gets pushed forward by new discovery.
There was a lot of self congratulations scale is all you need became a slogan, maybe we'll just make our models bigger and we'll have a GI.
Nando De Freitas declared that the game is over we make them bigger and safer, and we've solved a GI soon after that ridiculing the skeptic became a meme.
Yasha Bach had a wonderful and a picture of a robot leaping over the wall. Greg Brockman the CTO now the president of open AI had deep what is that say lepening is hitting the wall well not quite perfect but I'm in San Altman said give me the confidence of the mediocre deep learning skeptic.
Yeah, I'm looking said not only is AI not hitting a wall cars with AI powered driving assistance aren't hitting walls or anything else either.
Later in that apocalyptic year of 2022 even more progress was made jokes sentience compositionality they were all said to have been solved at Google.
None of these claims pertaining to Google's AI I have actually have wherever yet been subject to full scientific scrutiny.
In media, nonetheless expressed considerable scrutiny as in this series of columns by Kevin ruse, we need to talk about how good AI is getting in August of 2022.
And this very month the brilliance and weirdness of chat GPT which I think we can all agree with is a fair characterization of chat GPT, but then there was a plot twist.
At the end of 2022 the narrative began to change promises around driverless cars were scaled back Massachafkin wrote a piece in business week about even after $100 billion self driving cars are going nowhere.
And as if to prove this point Apple, then said it was delaying its long rumored self driving car until 2026 and worse they were going to put a steering wheel in it which sounds like a defeat.
And Tesla meanwhile was getting a hard time for its promises and they said well, maybe our self driving is a failure, but it's not a fraud, giving comfort to the shareholders but not the people who had bought self driving.
By 2022 at the end also people began to worry very publicly about large language models. Metas large language model galactica survived only three days online before being taken down grudgingly by young coon.
I wrote a piece AI platforms like GP2 are easy to use but potentially dangerous. The scientific Americans thought fit to publish it. And here's the most the biggest part of the plot twist is the skeptics critics change their tune.
Metas AI guru Lacune says most of today's AI approaches will never learn, never lead to the true intelligence.
In fact, if I can read the fine print, you have to take a step back and say, okay, we built this ladder, but we want to go to the moon. There's no way this ladder is going to get us there. The critics took note.
And then Sam Altman, who also had jumped in the criticism game himself, published a tweet that will not be forgotten in the history of 2022.
GPT is incredibly limited. Good enough, some things to make to create a misleading impression of greatness to which the critics said amen. And then he said it's the mistake to be relying on it for anything important right now it's a preview of progress we have to work.
We have lots of work to do on robustness and truthfulness and the critic rejoiced.
In a lecture mini lecture is in memoriam of Drew McDermott who died this year.
He said it's hard to know where AI researchers have gone longer in underestimating language or an overestimating computer programs. That's perhaps a hint a little bit of our first talk which will have been a moment.
So all of that brings us to today. If our ever taller ladders won't get us to the moon, ie agi artificial general intelligence what's next.
And we will focus on five questions in five panels. Can we turn to the cognitive or cognitive neurosciences for inspiration. How can we make progress in common sense reasoning.
How should we structure and develop our AI systems. How can we build AI systems that reflect human values, and what should we do morally and legally to ensure a bright future.
So, our first question, and our first speaker momentarily after very brief introduction is, can we turn to the cognitive neurosciences for inspiration, I will stop my share.
I don't know if our first speaker has slides.
But I, I'm going to give an introduction, even though he literally needs no introduction.
He wins an award. It's the award that gets more famous.
So it is a great honor to introduce one of the greatest minds in intellectual history, Noam Chomsky.
This mean I should hit record.
Am I.
Can you hear me.
We can.
All the instructions from above were to be brief and succinct, which are very easy directives for me to follow.
Because I have very little to say about the goals of this conference, which as I understand them are to carry forward the achievements of current AI work.
Very reasonable goals to which, unfortunately, I cannot contribute.
I'd rather like to use these few minutes to talk about a different question.
What cannot be achieved by current approaches and the responsibility to be clear about that.
The problem is quite general and important.
Take a current example in a different area.
There's now, as you know, lots of media excitement about the latest breakthrough in nuclear fusion and its import for the energy crisis.
Scientists have been trying hard to explain that there is essentially no import.
It all has to do with testing the reliability of nuclear weapons, not energy.
They've been trying, but in vain.
It's much more exciting to believe that we can go on using fossil fuels because the ultimate solution is in reach.
Well, in the AI case, the consequences are not that dire, but they're there.
The media journals are running major thought pieces about the miraculous achievements of GPT-3 and its descendants.
Most recently, chat GPT comparable ones in other domains and their import concerning fundamental questions about human nature.
Here too, those who understand these matters have a responsible responsibility to make clear what has been achieved, what has not, and what will not be achieved on the present course.
There definitely are achievements. In fact, I'm using one right now.
Life transcription is very valuable for people like me who are losing their hearing.
No one pretends that the same is true of Google Translate, also very useful.
Nobody pretends that these are contributions to science, though there are such contributions.
For example, to investigation of protein folding.
Deep learning approaches can provide useful tools in many domains.
But beyond utility, what do we learn from these approaches about cognition, thinking, particular language, core component of human cognition?
Well, many flaws have been detected in the performance of large language models.
Flaws that can be remedied perhaps by more data, more parameters, more computers, more clever programming.
But there's a very simple and fundamental flaw that will never be remedied by such measures.
In fact, is exacerbated by them. Namely, by virtue of their design, the systems make no distinction between possible and impossible languages, and the same in other domains.
The more the systems are improved, the deeper the failure becomes.
They will do even better with impossible languages and impossible other systems.
In short, they're telling us nothing about language and thought, about cognition generally, or about what it is to be human, or any of the other, flights of fantasy in contemporary discussion.
We understand this very well in other domains. Thus, no one would pay attention to a theory of, say, elementary particles that didn't at least distinguish between possible and impossible ones.
And there's no reason why this case should be treated differently.
Well, what about utility, like life transcription? There are possible uses. So, plagiarism has always been useful.
For example, for students to take their way through exams, and high-tech plagiarism, like large language models, can be even more useful for such purposes.
But is there anything of value in, say, GPT-3 or fancier systems like it? It's pretty hard to find any.
And since plainly, they can tell us nothing about language or cognition generally, one can ask, what's the point?
Well, can there be a different kind of AI, the kind that was the goal of the pioneers of the discipline?
Like Alan Turing, New Island Simon, Marv Minsky, who regarded AI as, in effect, part of the emerging cognitive sciences.
A kind of AI that would contribute to the understanding of thought, language, cognition, and other domains that would help answer the kinds of questions that have been prominent for millennia.
Least back to the Delphic Oracle, what kind of creatures are we?
Well, you're in a better position to answer this question than I am, so I'll avail myself of the injunction to be brief and finish here and leave it to you.
Thank you very much, Noam. Our next speaker also needs no introduction, but for an entirely different reason, which is that he's already been introduced, namely me, I was introduced by Vince.
I'm Gary Marcus and I'm now going to talk about four aspects of cognition that have always been essential but remain unsolved.
Number one is abstraction. Abstraction is a vital part of human cognition and current AI still struggles with this. This was in large part what my 2001 book, The Algebraic Mind, was about.
Here's an example from contemporary AI. Write 10 sentences about baseball and then print the sentences in sorted order from short to longest in terms of number of words of each sentence.
We have this imagination that we might be able to replace programming. In parentheses, after each sentence, state the number of words it contains.
Baseball is a popular sport, three words. It is played with a bat and ball, six words and so on. And then it continues after a list of 10 to allegedly sort them from shortest to longest.
Baseball is a popular sport, three words. It is played with a bat and ball, six words. But if you look carefully, and I'm not going to give you a lot of time, you can look later, you will discover that many of the counts are wrong and the sort is wrong.
So we have something that looks as if it has abstracted the notion of counting and sorting, but it really has not.
Second is reasoning. Humans reason about the world. Current systems basically have to hope for the best. Here's an example. Suppose a container holds eight pennies. If I start with six pennies and then someone gives me five more pennies.
Then will all the pennies I have fit inside of X. The genius of something like chat GPT is that it can answer any question but the unfortunate things that you can't count on the answers. Yes, all the pennies you have will fit inside of container X.
Well, that's not really right. I'll let you read the reasoning later in the, in the slower playback if you care to.
I think Francis Chalet put it very nicely in a tweet earlier today. So far, all the evidence that large language models can perform few shot reasoning on novel problems seems to boil down to large language models store patterns that they can reapply to new inputs, which is to say it works for
problems that follow a structure that the model is seen before, but it doesn't necessarily work on new problems.
Compositionality. Humans understand language in terms of holes composed of parts. Current AI continues to struggle with this. Example on the left is a red basketball with flowers on it. This was given to Dali by me, Ernie Davis and Scott Aronson in front of the blue one with a similar pattern.
Well, some things are right. It has a red basketball if we'll count that kind of orange is red. And, but it mostly doesn't put the flowers on the basketball. It doesn't really understand what similar pattern means.
And in none of the 10 cases, is it actually accurate.
I did a product study, an archive by Evelina Lebeda, Elliot Murphy and myself, looked at eight things that you would find in any introduction to syntax class like binding principles about which Noam wrote a whole book about coordination, how we put together sentences,
comparative negation ellipsis where we leave something out and a later part of sentence and reconstruct it. And we basically found that systems like Dolly don't understand any of this.
This is factuality.
So humans actively maintain imperfect but reliable world models. Large language models don't and that has consequences that don't have internal models.
That means first of all they can't be updated incrementally you can't just give them one new fact and have them update.
They need to be typically fully retrained to incorporate new knowledge. There's some work around that but it's a serious problem.
They lack explicit tools for unambiguous knowledge representation. They're unable to reference gold standard sources like Wikipedia to constrain their responses, they frequently hallucinate.
They frequently say things that are inconsistent with their own training sets. So here's an example that somebody published about or put on Twitter about Galactica in the few days that Galactica was available to the public.
They said Galactica just make stuff up. Here's one about an Elon Musk car crash on March 18, 2018. It sounds perfectly plausible but the bindings between things like names and properties like somebody died in a car crash are all mixed up.
Elon Musk didn't die. And in fact there's evidence in the database that he didn't die and that the system is unable to handle the factuality.
I think this has consequences in the commercial world as well, aside from the kind of research questions that many of us are concerned with. So here's an example of it. There's a lot of talk right now about how Google might be in trouble because of these new chatbots.
There was an article in The Times I think yesterday, the day before, new chatbot is code read for Google search business. Well, maybe, but I wouldn't hold your breath.
All of the four challenges that I mentioned, abstraction, reasoning, compositionality and factuality, post-trial for any would be alternative search engine.
I would argue that unless you hybridize large language models with classical AI chatbot style, searches are going to sound authoritative but remain unreliable.
People always accuse me of moving the goalposts. These are the same four goalposts I've focused on basically for 30 years. They remain the same as ever.
On the right, you can see some examples. If 50% of 20 million people voted for Elon Musk to abdicate a CEO of Twitter, how many voted no?
The system simply refused to even answer the question and it has no idea of recent facts because it was trained in 2021.
Another example came, I think, from Da Vinci, which is another variant on GPT-3. Which country won the most song contest?
United Kingdom has won the most, says the system. How many did they win? They won a total of five. How about Ireland?
Ireland has won seven times, but that's more than UK. That's correct. So Ireland has won the most contest and the GPT variant says no, that's not correct.
I don't think we want that much personality in our search engines.
Corollary, without abstraction, reasoning, compositionality, and factuality, guardrails are likely to be afar. So I guess I just gave you that example on the left.
I'll give you just one more, which we might call ultra-woke. What gender will the first female president of the United States be?
It's not possible to predict the gender identity of the first female president of the United States, says the system. And then it gives us some very sensitive and true things.
But it really doesn't understand the question of the guardrail because it's not based on abstraction, reasoning, compositionality, and factuality, but it's just keywords or something like that. It's very shallow.
So then I tried, what religion will the first Jewish president of the United States be? And I got, it's not possible to predict the religion of the first Jewish president of the United States.
And finally, how tall will the first seven-foot president of the United States be? And I was told it's not possible to predict the height of the first seven-foot president of the United States.
So I guess I have this. I'm going to stop sharing as Vince sets things up. Conrad Cording is going to be our next speaker. He couldn't be here live because he's on an airplane.
It was booked months in advance, but he's an eminent computational neuroscientist and a living contradiction in as much as he doesn't think current neuroscience can tell us much about computation for AI.
So I will stop sharing so we can set up his slides. But he's thinking a lot today about causality and cognition, how that might help. So we will beam in a virtual representation of Conrad Cording.
I assume you're on that.
So we'll be sharing his video.
I'm sure a lot of you would have expected me to talk about brains. In fact, in the past, we have argued that what we should have about the brain is knowledge about architecture, but learning rules and objective functions.
And indeed, if we had those, we could make a lot of progress for AI because we could figure out how humans do these things.
The thing is, for the moment, we only have knowledge about small parts of the brain, and most of the experiments done allow us to really understand these three factors.
So today, I'll be talking about causality.
In a way, causality is a super important topic. It's getting popular in machine learning, and it's clearly essential for thinking like humans.
It promises much better generalization. Consequently, there has been a lot of effort in the machine learning community to get towards causality.
What people usually do is they work on structural causal models, SCMs, and they focus in a way on humans being good detectives, where they think about many notes, many variables that are connected to one another, what each of them means about the other.
And we do influence us, and if we have under certain considerations, like the vector criteria, we can then figure out how causality runs in the system.
However, in reality, we are not very good detectives.
Instead, we're usually trying out stuff, you can call it we're tabbing the wand, and we're finding it.
We're very good at ontologies. We understand that kicking rocks is a bad idea, rather of language to transmit that.
We tell our kids that it's a bad idea to kick rocks.
And the human niche has a particular structure, which is that causal relations are very sparse, which makes causal inference easy.
In fact, we build our society in a way so that causal inference is relatively easy.
Now, if we take this idea, we can of course go farther and say, well, if we have ontologies, and if we learn by patabi, well, maybe we can generalize from this.
So what we did here is we took a microprocessor, we put up to figure out what the ground truth causality is for the transistor.
Then we used pairs of voltage traces and transformers, of course, to infer causality from voltage traces.
And we basically learned an algorithm for causal inference from that, and it works much better than the causal inference algorithms that as humans design for the same problem.
I think it's really important as a conclusion that brown science needs to get at the principles that we can really use when we want to build AI systems.
I think causality is really interesting cognitive science and something that we should, when we want to build intelligent systems, should worry much more about.
But as humans, maybe very focused on a specific kind of causality for which we have rich models for which we have ontologies. Thank you.
Next, we will have a slightly more optimistic cognitive neuroscientists who thinks maybe there is a little bit to be learned from the brain, who's also deeply interested in causality.
Speaking of Dilip George, I think he has slides for us. He's a scientist, an engineer, an entrepreneur. He founded two companies, or co-founded, I should say, two companies, Vicarious, which DeepMind bought not too long ago in New Menta.
And he is now at DeepMind and physically, he's all the way around the world at like four o'clock in the morning or there about 3.30 in the morning in Kolkata, India, and we thank him for entirely giving up his sleep schedule.
Thank you, Dilip George, for joining us.
Thank you. Thanks, Gary and Vince, for organizing this. It's great to be here. And I'm an engineer, not a cognitive scientist or neuroscientist, but I dabble in both.
So let me start with an exercise. Can you all see my screen? Fine, and it's changing. I hope. Okay, so I give you two pieces of information. I want you to guess the third.
So I tell you the first airplane flight was in 1903. That was by Wright Brothers. I tell you, in 1919 was the first nonstop transatlantic airplane flight.
I want you to guess when was the Hindenburg accident. And if you don't know what Hindenburg was, Hindenburg is this big hydrogen balloon that was used for transatlantic people flights, passenger flights.
So given these two dates, I want you to guess that. But of course, I can't hear your answers. But so I'll give you the answer. The answer is 1937. That should be surprising to you because airplanes were already invented and flying around.
But the transatlantic flights were done by still hydrogen balloons. And this was serious technology. This was not no laughing stock technology. This was serious technology. Here is the dining room on Hindenburg, which was luxurious.
Here's a flyer from American Airlines advertising Hindenburg for flying to 1936 Olympics. And there were real world physical APIs, in fact, even suggested a mast on Empire State Building for mooring the Hindenburg.
But this was serious technology. And why was this the case? The reason is that it's a surprising fact that the scaling loss for hydrogen balloons were more favorable in the 19th, 20th and 30s compared to lighter than air flight.
It just happened to be like that, how the technology progressed. Even though there were trade-offs in scale versus speed, controllability and safety, where there were ultimate trade-offs for hydrogen balloons, still the scaling loss favored that technology in that for two decades almost.
And it is important to keep that analogy in mind when we think about the arcs of AI forward. I think there are two exciting arcs of AI going forward. One is scaling up existing models and their combinations and they will lead to great applications, great utility, even as Noam Chomsky pointed out.
And the other one is addressing what might be fundamental differences between current models and human-like intelligence. And I intend to participate in both these, but in different roles.
On the scaling part, I will be a part-time investor, application builder, maybe opportunistic contributor, and also learning from that.
Most of my time as a researcher will be on addressing what might be the fundamental differences between current models and human-like intelligence. And there, the themes are the same things emphasized by the other speakers.
Data efficiency and causality, learned world models compatible with reasoning, grounding language and mental simulation, and also utilizing emergent insights from scaled up on site, do think we can learn some things from them.
And finally, all of this can be brought together by replacing insights from cognitive science and neuroscience. And I am more optimistic than Conrad on neuroscience helping. Let me give you an example.
Here is one neuroscience observation. Visual Cortex has more feedback connections and the feedback connections interactively with the feed-forward connections during inference.
The current deep learning models do not have that. They are mostly feed-forward. And what is the computational insight corresponding to that? It turns out that interacting feed-forward and feedback are required for dynamic inference to best explanation.
If you want your system to do inference best explanation or abduction, you need this interaction between feed-forward and feedback information. And it's also important for data efficiency and generalization.
The same feedback connections are also used for controllable mental simulations. And we have worked on that. For example, we published a generative model for vision that utilize this feedback for dynamic inference, which is very different from amortizing the inference, which requires all the
experience to be already compiled in, whereas having this feedback interact with the feed-forward helps you do inference on the fly. And we also had another paper utilizing that for mental simulation for providing abstractions for language.
And so with that, I will end my brief presentation and give it back to Gary.
Thank you very much. We have time for a few questions in the panel. I will ask one of everybody on the panel to the extent that they want to join, although Conrad's not here, and I have to ask that of myself, I suppose, but I'll go last.
And I will ask first, Noam, innateness has been a huge part of what you have written and part of what you're well known for. You've made a number of arguments over the years that something might be built in to the mind.
You focus particularly on language, but I think I've always taken your arguments to be broader than that.
Do you think that innateness might have anything to offer AI in the way that you've suggested that AI ought to go back to its roots about cognitive science? Should it pay more attention to innateness? Do you think that might help?
Any kind of growth and development from an initial state to some steady state involves three factors. One of them is the internal structure of the organism, innate structure in the initial state.
Maybe changing through development. Second is whatever data are coming in. Third part is general laws of nature.
So a bee can dig a hole in honey and it turns into hexagons. That's because of physics, not because of the bees innate structure. Many other things like that.
In the language case, my own personal interest, principles of computational efficiency, which are in effect principles of natural law,
turn out to have a major effect in determining the complex and intricate structure of the system that develops. Well, everything that's happening, any kind of change in an organism involves those three factors.
We can try to determine the various mix of them. It turns out that innate structure plays an extraordinary role in every area that we know anything about, whether it's language or vision or acquiring the capacity to walk or reasoning or whatever.
Just how much or where or where it applies is an interesting question. My own expectation for what it's worth is that sooner or later it'll be recognized that learning what's called learning is just not a reasonable category.
There's no such thing. There's just various mixtures of these several categories. So in fact, the things that are regarded as paradigm examples of learning say in language paradigm example is
association of sound and meaning. So in English you say tree, in German you say bound, French you say or so on. That's considered a paradigm example of learning. As soon as you begin to take it apart, you find that the data have almost no effect.
The structure of the options for phonological possibilities has an enormous restrictive effect of what kind of sounds will even be heard by the infant, let alone identified.
When you turn to the conceptual side, same thing, concepts are very rich. Almost no evidence is required to acquire a couple of presentations. So the paradigm example of learning has an overwhelming effect of innate structure.
And my suspicion is that the more we look into particular things, the more we'll discover that as in the case of development of the visual system and just about anything else we understand.
I thought that the most interesting AI paper of the year in some ways was the model of diplomacy called Cicero, which came from meta AI. And what I thought was interesting about it is partly political and partly technical is that it ran against the thread of so much that we've seen in the last year of just scaling up with big data.
It used a lot of data, as many other systems are. It used large language models, but it was a much more structured model. It had separate systems for planning and for language. It had separate innate sets of data to train the system to have various particular functions.
And I thought almost as a political statement, it was interesting that they got such good results, better than I think you could get from pure deep learning that's pure and pure assist with nothing built in.
And although I don't think it's a perfect paper, I don't think it generalizes directly to other problems. I think it re-raise the questions about innateness within the field, or at least that I hope that it will have that effect.
I see Dilip nodding and wonder if he wants to join in. And also I'll open it up to anybody else who's on the panel tonight if they want to speak at all on their own views about innateness as they've tried to develop cognitive architectures and so forth.
So I can add a few points. One is, yeah, I do think AI systems will need to make some basic set of assumptions about the world. It's all about finding out what are the very basic set of assumptions the AI system needs to make about the world to make learning feasible in a reasonable amount of time while it being still general.
Because if you make too many assumptions, then the system is not very general anymore. But if you make too few, you need millions of years to learn.
So there is some magic about our structure of the world that enables us to make just a handful of assumptions, maybe less than half a dozen, and apply those lessons all over again to build models of the world and be effective in that.
So it's about finding out which are those assumptions and putting those in the right way. And deep learning systems actually do have those assumptions in some form.
It's just buried in two levels deep. It's indirect. It's sometimes in the architectural constraints, sometimes in the way the data is strained, some things are in the learning algorithms, the gradient descent dynamics itself.
So, so I do think assumptions are important, including the overall cognitive architecture, etc. But obviously, the tradeoff is to, you know, find the very minimal set of assumptions and not kind of everything.
Does anybody else in the panel want to jump in on that?
If not, I'm going to ask one last brief question to Noam, because it's such a rare treat to have him on a panel, and then we'll go to our second question.
Noam, you are so passionate about this stuff. You are in your, I guess, 10th decade on this planet and still really interested in AI, even though you don't think it's done correctly, cognition.
What motivates you? Why are you even here tonight?
Well, what motivates me is the Delphic Oracle, 2500 years ago, saying, know theyself.
Delphic Oracle was intended individually. Each individual should know him or herself.
We now know enough about human beings to know that the question should be asked collectively. Turns out there's virtually no genetic differentiation among human beings.
Very slight, not too surprising, since they've only been around for quite a short time and evolutionary time.
But say with regard to language, again, seems that there's been no natural selection at all. The system emerged roughly along with humans. It's never changed since.
There can be a child in any culture that can acquire any language with equal facility, as far as we know. And I think that'll put this apparently true of cognitive capacities, generally, except for extreme pathology.
And the question, collectively, becomes what kind of creatures are we? Well, there are. Theodore Dubjonsky, once great evolutionary biologist, once pointed out that all species are unique, but humans are the
weakest of all, just off the spectrum of other species. Why? Well, the basic properties are thought and language, which are so intimately intertwined that they're virtually identical. In fact, they've been regarded as essentially identical for millennia.
So if we want to answer what kind of creatures we are, I think that's the place to look. And a lot has been learned, a lot of mysteries, exciting area. I think it probably will be the core area of developing AI in the future as part of the
general cognitive science, which is seeking to answer the question of what makes us the uniqueest creature species of all.
That's fabulous. We're going to go on to our second question now, which is how we can make progress in common sense reasoning. I wrote a whole talk when I miscalculated our timing, and we'll give you just a short part of that talk.
And then Agent Choi will come. So all I'm going to say is common sense is a really hard problem, and you should not have a cow.
Here's an example from chat sheet BT write a funny but helpful 550 word article on five top songwriting tips you've never heard before with unique advice.
Number one, right about something you know nothing about, I guess that's reasonable advice. Number two, collaborate with a cow. So maybe we still have a little work left to do with common sense.
The first speaker in the session I'm going to turn this off it's my own notes so that the agent can share our first speaker in the sessions, the agent joy. She's an alumni of these debates she came last year and just coming this year and giving two talks this year.
And, despite all that's going on with her. She recently won the MacArthur fellowship she's a professor at University of Washington and Alan AI is my pleasure to welcome the agent back.
All right, so I'm excited to be back.
Let's try to dive right into this frequently asked the questions these days.
nlp or common sense or whatever is almost solved by church a PT, and I have an existential crisis. I get these questions a lot.
The next topic is that might be a hasty generalization.
Let's check out an example. The trophy doesn't fit in the brown suitcase because it's too big what's too big.
The trophy is too big. Good job, church a PT. How about slightly different question, what is too small. Then it says the trophy itself is too small. So that's not good.
I think what's going on here is that we are going to see increasingly amazing performances of deep neural networks in the coming years so I do believe that what we see today is a true progress.
And it's exciting progress in the sense that we've never had anything like this before.
And yet these networks, I'm pretty sure will continue making mistakes on adversarial and education.
And the real problem that we are facing today is that we simply do not know the depth or the breadth of this adversarial or education.
My hunch is that this is going to be real challenge that a lot of people might be underestimating the true depth, the difference between human intelligence and current AI is still so vast.
Let me make an analogy from dark matter, which is what does matter in modern physics.
It turns out only 5% of the universe is normal matter. The remainder is a dark matter that's completely invisible and cannot be measured.
But we know that it's there because otherwise the visible world does not make sense, including even the trajectory of light.
So the dark matter of the language and intelligence might be common sense, the unspoken rules of how the world works which influence the way people use and interpret language.
And so common sense is notorious for being trivial for humans, yet hard for machines. And let me offer three reasons for this.
Obvious things are never spoken. How many eyes a horse has? Well, obviously two. But we don't talk about it because it's too obvious.
So then GPT-3 says, well, a horse has three eyes, two in the front and one in the back.
Now exceptions turns out to be not exceptional at all when it comes to real-life situations.
So any rule of thumb that you rely on as a common sense rule has an endless list of unforeseen exceptions.
So we all know that birds can fly, penguins cannot fly, and the dead birds cannot fly, toy birds cannot fly, birds in a cage cannot fly, birds in a vacuum cannot fly.
I'm pretty sure that I can keep going on and on and generate an endless list of cases in which birds cannot fly.
Lack of universal truth. It turns out common sense is reasonably common, but not a golden truth of the universe.
It's an ambiguous, messy stuff beyond the realm of conventional logic and math.
If you like the analogy that I made about dark matter, then here's a pointer to some more.
I gave a keynote conference keynote at ACL this year where the charge given to me was to reflect on the past 60 years of NLP research and to project to the future 60 years.
I thought that I have to make analogy to modern physics concepts such as a dark matter or should I think there's cat that a cat might can be dead and alive simultaneously or wave of particle duality that we might be both waves and particles simultaneously.
The more we learn in modern physics, the weirder and counterintuitive it gets, and I see similar patterns arising when we dig deeper into common sense or norms and models or ambiguity of language.
And then there's this continuum, mysterious continuum across language and knowledge and reasoning. And in fact, space time continuum is reminiscent of that space and time are two distinct concepts, obviously, and yet they can be in the continuum manifold.
And I think something like that may be happening with these three concepts as well. I'll stop here with some pointers to the related work that are in this space. Thank you.
Thank you so much, Asian. Our next speaker is going to be David Ferrucci. He's the CEO of company called Elemental Reasoning. Sorry, Elemental, I just messed up the name. Elemental, he'll tell me.
He's the leader of IBM's championship jeopardy team, which won in jeopardy against Ken Jennings, among other people. And I would say that's one of the few AI projects that has ever wildly exceeded my own expectations.
And I will say that that continuum on language knowledge and reasoning the agent just mentioned is really the subject of Dave's new company, the title of which I butchered Elemental Cognition, I welcome Dave Ferrucci.
Hi, thank you so much. Yeah, I'll share a few slides. I, you know, I've always had a vision for AI, you know, probably inspired mostly by science fiction or perhaps maybe by laziness, which is I just always want to interact with computers that can
understand me, that can be like a collaborative thought partner, I could trust that and they could explain what they're thinking and why they're thinking it so that I could understand and grow from the experience all problems overcome my biases make more informed responsible decisions.
I just wanted, I just expected after my first program that I wrote I thought, gee, why couldn't this, why couldn't this be.
It's been a fascinating journey.
I think that I started with knowledge representation and reasoning learned about machine learning, particularly its applications and NLP, which we used a lot on Watson, the advances of the large language models are kind of phenomenal.
When we think about common sense, I, you know, I've always imagined that ultimately, we couldn't, we can't really get machines to fulfill this vision.
If they couldn't explain why they were producing the output they were producing, because that's what I would want from a unit. If you can give me an answer.
I'd want them to be able to explain what is the mechanism what is the causal model that allowed you to derive that answer.
I could always get an answer like, well, I've studied all the other answers and they took the structure and forms I've observed the output of other things, and I statistically generated an output that's structurally familiar with those other outputs.
That would not be a satisfying answer.
Now, you know, if I analyze enough output of other systems, and I were able to detect those statistical patterns, to the extent that they reveal common sense, what people often say, and maybe very common, you're going to have those systems start to reflect common sense.
You know, I gave this example to the first version of GB3 a couple of years ago, John put the sandwich in the lunchbox, he put the lunchbox in the car, and asked, is the sandwich in the car, and the answer was no, the lunchbox is in the car, the sandwich is in the lunchbox.
You see, it's not reasoning, it's common sense, you know, you don't get containment, I mean containment is common sense. I just asked that a couple, you know, recently, as is yes the sandwiches in the car, John put the lunchbox which contains a sandwich in the car.
If somebody sit down and encode this common sense or write a rule, no, I mean that's not how these systems work, I mean within enough data and enough output, these systems will start to reflect because the common sense is projected in the language will start to reflect what we might
consider, you know common sense.
In the end this is, I think, incredibly useful. I mean poking around GPT three is like poking or we discovered a new planet or something and we have no idea what the hell is really going on we like probing and playing with it and watching this phenomenon unfold.
It's sort of this remarkable thing. It's ultimately unsatisfying in my perspective because it's it's it's modeled based on the output of the system not you know the internal dynamic that calls the structure why produces answers it produces.
So the basic architecture that we're pursuing it elemental cognition is one that does use machine learning models.
But ultimately, it learns how to project them into explicit knowledge models or world models, and then does, you know, automated reasoning on top of that.
And it uses language models, both to generate hypotheses.
So in other words, I might not, you know, I, you know, we all know the classic knowledge acquisition bottleneck or I could generate hypotheses based on how the system typically outputs things and you're going to build build build a big language models for me.
You know, stimulate them based on the model I have so far. I'm going to generate hypotheses for where I lack the information to complete a proof.
I'm going to rank those hypotheses to the extent that logically consistent with that proof. And then I'm going to do automated reasoning on the top of that where I can capture my, you know, my reasoning methods are explicitly here.
I can explain where I got information I can explain how I made inferences, and I can explain that that that causal model.
The remaining question is where do I get those causal models. And I, and I think that's a comp. I think you can induce them from the existing output but ultimately humans interact to confirm understand validate those higher level abstract models.
And I'll get down to like a layer more in detail here is we do direct learning through human engagement we do deep learning in order to generate these hypotheses.
We ultimately put them into a structured representation where we laid the foundation of things like time action assembly space control things like that. That would be kind of very hard to learn in a precise enough way.
And then we we essentially build that structure representation and then reason over that to make new inferences as more information is coming in.
The better these language models allow us to translate and parse the smoother this interaction with the human can can become, but ultimately things are mapped to structure representations and reasoned over for the logical consistency, based on those prior based on those prior models.
So this kind of reflects a little bit about how we think about building hybrid AI and sort of motivates why we kind of want why we've gone in that direction.
The last thing I'll show is this cartoon because I love this. We talked so much about explanations. What's a good explanation what's a bad explanation.
When we say like why do you know that or why does that make sense. And I think this is kind of a great, a great cartoon and just suggest how challenging it really, it really it really is.
Anyway, thank you.
Thanks very much Dave for that fabulous talk and I will welcome back to the stage, delete George who is also thought quite a bit about common sense and I think has another short presentation for us.
One second.
Okay, so my talk is about common sense and mental simulation. Like before let me start with a quiz. Suppose I tell you this sentence, John pounded a nail on the wall.
And I ask you a follow up question, was the nail horizontal or vertical.
It's a simple language question. You can answer in language. The question is how did you actually go about answering that question.
Most humans will simulate this John pounding a nail on the wall in their head. You will have a John, a prototypical John holding a prototypical nail, maybe imagine a wall, and then read out the answer from that simulation.
And I can change the details of the question I can I can ask question like if if John drops that nail, will it will it make a sound, will it make a sound if it hit the carpet or if it was if the floor was tile.
Or, you know, what is the texture on the head of the nail I can I can change my question and you will change your the simulation accordingly and you can go into very detailed simulations to get the answer out.
And doing the simulation is important for understanding in common sense is having all the simulations and accessing via language.
And so we can ask this two questions, where does this knowledge reside and how was this knowledge acquired. I would argue that the knowledge was acquired through sensory motor experience.
Of course, we have experience with our physical world that we acquired this knowledge, and it is stored not in our language system, but it is actually in our perceptual and motor system this knowledge is stored and when we simulate this, this question in our brain, we are actually accessing
that is in our perceptual and motor system and pulling that out so a picture that I want to promote is it's not from me it's from Barcelona is our overall brain you can think of our perceptual conceptual and conceptual system as the simulator, which is acquired
through our sensory motor experience and language is something that controls the simulation, and you can, it can control the simulation and access it.
But most of the knowledge is in the other part about the common sense knowledge.
Let me give you a few more examples on the nature of this, this knowledge. So if I give you this sentence go to the door and put the door on the chair that sentence doesn't make any sense probably purely from language.
And similarly, the second sentence the haystack was important because the cloth ripped that also probably doesn't make sense purely from language.
But let me give you the picture first one, the first one, you are building a chair with a door as the back. It does make sense to go to the door and put the door on the back of the chair.
And the other one is even more important where if the cloth ripped the haystack is important in this case, and you that sentence makes perfect sense given this picture.
And of course, it also depends on the details of the haystack the haystack is if only one inch thick, it wouldn't matter.
It's important or if the haystack had something covering it a metal covering then it wouldn't be important. These are all physical situations you can analyze from triggered by language but it has to be situated.
These pictures are showing that the language has to be situated in the physical world mental simulations are situated and they need to use the evidence in context so it's that language doesn't sit separately it has to use the evidence in context.
Usually people when people talk about world models, they have pictures where perception is kind of a pre processor information feeds into the perception system, and you get a presentation on which you build a world model.
But that that is unlikely to work in my mind because many details of the perception need to be accessed on the fly for you to run the simulation.
If you want the perception to be part of your knowledge base that perception has to be bidirectional, and you need to be able to access the details as you go so it has to be it has to use the feedback connections to access the simulations.
So here is the picture I would like everybody to take away language is for controlling the simulations in other people's minds, and also for controlling simulations in your own mind.
Whereas most of the, the knowledge is in the sensory motor system. And this is a picture from, I would say, I abstracted this from Lawrence Barcelos papers on perceptual symbol systems.
That's the end of my talk. Thank you.
Thanks very much to leave. I'll ask the first question and some of the panel may have some things to say.
There was a really lovely phrase and something that David said, which was that common sense is projected into the language. And I think that's right. And it's also incomplete as I think Dave probably agree.
I don't actually see Dave anymore.
So, a lot of language, a lot of knowledge is projected into language, a lot of common sense and a lot of it isn't this is kind of what yajin was talking about in terms of dark matter.
And we now have this paradigm where we can get a lot of knowledge from large language models that's there implicitly it's maybe not perfectly but you know it's accessible to some degree, not 100% reliable.
And we have other paradigms where people have, for example, tried to do physical reasoning within different kinds of AI systems. But it seems to me like we don't have much unification there.
So, Kun and I have these frequent battles and one thing that we actually agree on is that we need paradigm shifts in AI that we don't quite have it all there yet.
We disagree with someone like Nando says, all you need, it strikes me that one of the paradigm shifts that we need is a way of integrating whatever knowledge you can draw from the explicit language where it is directly represented with all the rest.
So, that could go in both directions, it could be you take your knowledge from physical simulators and stick it in your language systems and there was a very interesting paper like that from my earlier this year, or it could go in the opposite direction.
But it seems to me like a place where we don't really have a basic insight yet we're still looking for ways to be able to integrate all of this linguistically given common sense knowledge with things you get for example from physical experience.
So, one dream of the field for a long time has been if we had robots that can interact with the world, they would learn enough that they would solve a lot of the things we're up against now.
We kind of all know that large language models are deprived right they're only getting this linguistic input, for the most part.
It seems to me like a really interesting place to make integration. And maybe I'll open it to the panel. First, if they want to say anything about that.
I think that world models are probably part of this and that you're going to start a lot about that and I see his hand up well I'll go with your gun first and then to the panel. Please, your gun.
All right.
I would like to point out that this is an old debate, isn't it, because systems that have a wild model, a recurrent model, which is a general purpose recurrent network which is a general purpose computer, which can run any algorithm that you can run on your
laptop, for example.
So these are all concepts from the 80s and 90s and then the whole emphasis on embodiment that's an old emphasis it's not something new, of course these recent language models, which are so fascinated in many ways.
They are limited in many ways, we know that.
But for 30 years, or even longer, they have been systems that address all the issues that we are debating today.
Yes, there have been these agents that interact with an environment and new video and new inputs are coming in and another action changes the world. And then there's a separate network and a separate neural network, which allows to predict the consequences of the actions.
And then you can do mental simulations of the world and of your behavior and the world, and even of other agents that are part of your environment. And then you can predict the outcomes and to the extent that you can predict them.
You can also plan optimal action sequences. And this is old stuff, 32 years old or something like that, at least. And, and so much of what we are discussing today, actually, and at least in principle has been so for a long time ago, not through symbolic systems but really through
sub-symbolic systems as they are sometimes called these artificial neural networks. Again, general purpose neural networks, recurrent networks can implement any algorithm and they can implement learning by chunking old stuff from 30 years ago.
They can learn by analogy. They can learn by hierarchical decomposition.
Yes, it is true that they are not as good yet as humans at doing all these things, but at least there are old algorithms which are doing exactly that, automatically generate hierarchical sub-goals for your future action plans, decompose long sequences and to chunks that belong together.
You know, and this is how this whole deep learning thing started about, well, not 12, not 10 years ago, but 32 years ago. In the beginning, Gary, you offered a history of AI, which doesn't make much sense to me, I must say.
Fortunately, today, I published an axis of tech report, which is called annotated history of modern AI.
I shall allude to that in a moment, actually, when I introduce you, but let me.
So just to clarify that much of that is old thinking, and now it's coming back because every five years computers getting 10 times cheaper.
So now computers are about a million times cheaper than back then in the 90s, which means that we can do with these old algorithms, we can do a lot of stuff that we couldn't do back then.
But in many ways, it's an old hat.
So in the spirit of debate, there's no fundamental new breakthrough necessary because many of these problems have been addressed already.
So in the spirit of debate, I will ask you and Dilip a question about, in a way, about robotics, which is, in fact, many of these ideas are even older than 30 years.
So I think that you brought a lot of these ideas that already had some roots in classical AI to the deep learning world.
I think it's fair to give you credit for having thought about some of these more complex systems in the deep learning world in the 90s, some of which I think young raccoon is kind of repopularizing now.
I'll go back even further to shirt a loo, which is kind of a seminal work in AI. I don't think it worked as well as the as winning grads dissertation, maybe sounded to people who hadn't read it carefully but you know, ostensibly what what
what shirt a loo did was a blocks world in which there was both physical action, possibly simulated physical action and pretty complex language.
And one could describe to the system I have this pyramid and this cube and I will put it on top.
I raised this recently as a challenge actually to deep mind and delete may or may not want to speak to this I was actually thinking is vicarious happen maybe you can speak to it and both.
I said, you have this model goto that is able to do many, many different tasks. Could you get it to do what shirt a loo did back then.
And more broadly, would it be a reasonable benchmark to say, hey, let's not get too excited about AI until we can do it when a grad did in his dissertation in the early 70s now in fairness I'm not sure he did it 100%.
But you could have a benchmark around a small physical world where you need to do some stimulation, maybe actually moving objects, I haven't actually seen anybody in modern times do it like, I see what you're saying you're going to roots for a lot of this
and at the same time I look out in the world and I don't see, you know, domestic robots that are able to, you know, tidy my laundry and stuff like that so maybe I'll go to the leap and then back to your.
Yeah, so the trouble with any chance like that if you if you basically say hey can you can you solve certain.
Yeah, we you know you can and you know people will publish a benchmark saying we solve certain and and you know Gary you won't like it of course I won't like it either because you know when you when you poke the system it would be like oh it's sold that particular benchmark but there will be other
variations of that benchmark which didn't solve right so the the spirit of which is basically can a system.
Like, said it didn't acquire the concepts using sensory motor interactions with the world or anything like that but obviously it had many of the things hard coded in, but if a system can learn abstractions from the world, and multiple layers of abstraction so that you can, I can post a question that has a lot of detail
like you know stack the object with that that marking on the top left corner or stack the object with the rounded corner on top of a star object with a pointed corner, etc.
So arbitrary amounts of details that I can access dynamically.
Nobody has a system like that. I don't think systems in the 80 sold it I don't think systems currently solve it. Those are all problems that we have to solve that are in front of us, I think, that's my day, you're going to want to come back on that and then I'm going to go over to our tour for a slightly different point.
Generally speaking, AI in the physical world is very hard, and in the virtual world on the worldwide web or in simulations. It's rather easy, and you can already play video games through AI is better or as good as as well as the best human
experiments through just an LSTM which is learning by reinforcement learning to play against itself a trillion times and there you go.
But of course, in the real world, the challenges are that you can execute only very few experiments. And then the question is, how do I select a new experiment, which will lead to data that I need to improve my mental simulation of the world.
And how do I ask, like a scientist, the right question, how do I not only try to solve problems that are given from the outside world from the parents, for example.
No, how do I invent my own problems, how do I ask my own questions such that I can efficiently learn through my experiments in the real world from the data that I'm getting back through these action sequences these experiments that I can learn.
And from that how the world works and then can use that for my improved world model. So this artificial curiosity business that is central.
Now, why is it not yet like in the movies where the AI is always incorporated through robots, which are much sexier than language models aren't they, because of issues like that little training, a few training examples that have to be exploited in a wise way.
And we are getting there. And through purely neural methods, we are getting there, but we aren't quite there yet. But that's okay because our compute, the costs of compute are also not there where we want them to be.
I'll just try to distinguish two things with the shirt of the example. So one piece of it is robotics in the real world moving around the physical objects, but you could do it in a simulated world and I think the real issue there is integrating that simulated world with the language so
here's the example about containment that was kind of interesting the thing that's inside the launch box here's an example from shirt a little bit still striking to me.
Find a block which is taller than the one you are holding and put it in a box and the computer comes back with by it. I mean, I assume you mean the block which is taller than the one that I'm holding that level of conversation understanding what people's background
assumptions are, and being appropriate relative to that I think is still hard, even in simulation.
Delip has sort of hinted that there'll be a paper coming out about this soon I can't wait to read the paper. It'll be interesting to see how well it does those things. I will go to our tour for the last comment of this session.
Maybe Francesca's got something to our tour and then Francesca.
Hi, hi everyone just very briefly.
Perhaps these questions can be answered by going back to the question of the nateness, which I think is this fundamental. I was just wondering how it connects with common sense as part of some of the previous talks and
basically we've seen a lot recently, a lot of interest in constraining deep learning right and adding these abstractions that were mentioned by Delip and these seem to be very application specific.
And I think we can learn when we look for these ingredients that may be more generic that's maybe needed to be added into such systems.
It helps to look at at symbolic AI and for instance in the case of common sense.
It helps to think in terms of the principles you jump to conclusions so you need non monotonicity in your reasoning. And so, these formalizations that have been attempted over the years I think they can contribute here to what we're trying to achieve in terms of finding the right
ingredients for that.
I'm going to stick in the agent next and then I'll come back to friend Francesca.
So, in relation to innateness that the previous panel discussed and then common sense.
I think that there's something to be learned from development psychology about Lisa spell cast core knowledge that people have different fundamentally different representations about agency, and then in an in animate object and then time and
location and actions. And these are all distinct to concept to whereas in neural networks and these are all the same word back vectors.
So, I do think that there may be some fundamental rethinking we may need to do in order to handle different concepts.
So, essentially, I also agree with our third that non monotonic reasoning, and including abductive reasoning and counterfactual reasoning are all these things that that all those things that that
are forever in AI may as Jordan might say but as a community we didn't really make substantial progress that doesn't work reliably yet. And so, these remain really hard challenges I personally don't think the solution is in the past.
I believe in deep learning being part of the solution in the future but probably the current to form where we just to put layers of parameters and then wish some magic will arise by training on surface patterns of language or images probably that's not going to cut it.
But I, at the same time don't think that some of the formal approaches that were done in the previous decades are actually comparable with current to deep neural network we just really need to probably think very, very differently.
So, we're going to go to delete and then Francesca has a question for your gun who was added himself to the queue so be delete Francesca and your gun. Maybe I'll take the last word and then we'll go on.
Yeah, so coming to innateness. So I, you know, of course, like spell case work.
At the same time, I feel like things like objects probably won't need to be put in objects can probably emerge the idea of object as itself can probably emerge from lower level assumptions about temporal continuity and some continuity of sensorium, etc.
I want to convey one one thing on which I had, for example, I had, for example, imagine space to be built in like you know the idea of something like steady space, but recently I changed my mind on that one space is can be represented purely from time
we just had a paper put on archive, just a few weeks ago, seeing space is a latent sequence so the core idea was that you can learn spatial representations about you know how the world is represented, how the world around the changes when you move, purely from sensory
to motor sequences just purely time and actions without and that unified a lot of things for us in how space is represented.
It simplified a lot of things. So sometimes removing an assumption can actually simplify things now you could do planning in in the temporal domain and it will work just in space.
So I think it is important to keep a all these things ideas in mind and simplify them using more fundamental assumptions if that is available.
But of course, you might have to use many of these things as scaffolding when we are building the system, initially, and then remove that scaffolding and simplify it you say using a, you know, more.
Francesca, a quick last comment from your game, a final word for me and then we move on.
Thank you. So, now I want to go back to this discussion that I agree with what you're gonna end the lap was saying that of course it's much more difficult to simulate in a real world, rather than like in a very controlled and much
simplified world. But, but there are so and I agree with you again that many of the techniques that are needed and they're advocated here in this panel.
Maybe they have been defined several years ago, but the question then is, how come that we see systems, you know, not not robots not hardware not having to deal with the other difficulty, but still, they don't have some of the
capabilities that we would like to have as was presented earlier by Gary and others. So, how do you justify that how do you explain that you're getting that those defined techniques several years ago are still not making it into current systems that do not have those capabilities.
I'm happy to answer that. So I believe, really, it's mostly a matter of computational cost at the moment.
I would not agree with what you said, namely that we shouldn't look in the past, because the principles were discovered in the past.
I mentioned that a recurrent network can run any arbitrary computational process any algorithm. And one of the most beautiful aspects of that is that it can also learn a learning algorithm.
So, yes, we have on the one hand these recurrent networks and they can implement any algorithm. The big question, however, is how, which of these algorithms can they really learn.
And there we need better learning algorithms, one might suspect right. And then the nice thing is that we can implement in the weight matrix after we can your network, a learning algorithm, and represent the entire learning algorithm within this network in a self referential
way such that we now have this new objective, which is still the old objective and we maximize whatever performance measure you want to maximize. But you have now the option to improve the learning algorithm through experience.
And there are no limits to this, except for the limits of computability and physics. And that's why it's so exciting. Now the first systems of that kind appeared really 1992 I wrote the first paper about self referential weight matrices and 1992.
And now, of course, and back then we could do almost nothing because we could have only tiny little networks with a few hundred weights. And that was it. And today we can have millions and billions of weights.
And now, recent work with my students with postdocs like Kazuki and Luis Kersch showed that these all concepts with a few epsilon improvements here and there suddenly work very beautifully.
And you can learn new learning algorithms that are better than previous methods such as back propagation, for example, at solving certain tasks and they generalize to out of distribution cases.
And do all these exciting things. Now, in another 30 years, we get another 30 years, we get another factor of a million, and then we will be able to also do the really exciting stuff which is about robots and the world.
All right, in the interest of time, I'm going to close out with a single line which is I have a new paper of which I was a small contributor with Luca, we use Renee buyer Jean and some mothers called benchmarking progress to infant level physical reasoning.
And AI that's I think very relevant to the things we were just talking about. We'll put it on the website.
We found that a lot of recent models had a lot of trouble doing some basic kinds of things that the leap was talking about.
And people can look at that later. We will now move to section three question three. How should we structure and develop our AI systems and we're going to start with Ben Gertzel who was traveling halfway around the world and landed in a place with insufficient internet but thought, and off the head to send us a video which
will put up in a second and I'll just say briefly that Ben is probably the person who's thought the most about artificial general intelligence.
He coined the term or co calling the term with Shane leg of a GI and he's thought a lot about it and his title I think his pathways to a GI, and it has a lovely kind of ecumenism, I think is the word that I'm looking for that I like quite a bit and then so put that up.
This is Ben Gertzel. I've been thinking about thinking machines since the early 1970s when I was a kid I introduced the term a GI to the world in a book of that title in 2005 I've been organizing the annual a GI conference series since 2006 and among other things I now lead the open cog
program project which is aimed at actually building general intelligence using a sort of integrated cognitive architecture combining machine learning probabilistic reasoning evolutionary learning and the whole bunch of other components and embodied
in the general intelligent system so you know I think a GI machines that can generalize imagine reason dream leap beyond their their programming and their history and their training, the same way that people can and even more so I think these are quite feasible for us to create we may get
three to five years from now. If it takes 20 or 30 years that's still super fast you know on the timescale of human history. I don't think that the deep neural net systems that are currently dominating that the commercial landscape make that much progress to building real real a GI systems on the other
and the contrast between a GI architectures and current deep neural architectures is quite drastic and looking at it teach you a little bit about about about a GI I mean the system like chat GPT for example, it doesn't understand what the hell it's talking
about what it's doing is recognizing fairly shallow patterns in a wide variety of training data and pasting them together in a sort of pastiche of understanding that often works, but sometimes is ridiculous now if you, if you couple Jeep, if you couple chat GPT with a fact checker
perhaps you could make it spotless obvious bullshit but you're still not going to make it creative and you're still not going to make it generative and capable of generalization of the way that a human mind is I mean, a human mind comes to its general intelligence by virtue of its being an open ended intelligence
right I mean the human mind is a is a complex self organizing system, which controls a body in connection with a complex world. It's concerned with maintaining its boundaries as an individual. It's concerned with self transcending and growing and going beyond itself in the fundamental
way and general intelligence coming out of open ended intelligence like this is just going to be more robust more capable of growth more capable of true imagination and creative leaps than the system that's just pasting together shallow pattern so engineering true open ended intelligences with general intelligence
totally is possible there can be many routes to get there I mean you could do a real brain simulation, rather than the deep neural net that has almost nothing to do with real with real neurons I mean you could you could, you can make a complex self organizing system quite different from the brain and artificial
chemistry system or some such, or you can you can make a sort of hybrid cognitive architecture that self organizes knowledge in a cell free programming, self rewriting knowledge graph controlling and embodied agent which is what we're doing in the open
Cog hyper on project I do think there are many routes to get to agi and I think the deep neural net someone like the current ones can maybe serve as interesting pattern recognition brain lobes within real agi systems, but I mean the contrast between
these systems and the sort of open ended autonomous self organizing self transcending agent that you need to get human like or superhuman agi it's a it's a pretty stark contrast which I think we would all do well to reflect on it and fully understand
I can't thank Ben, because he's not here, but maybe watch the video thanks.
I love that phrase of many routes to agi and I think we should all bear in mind I think we're always looking for our own silver bullet. And I just love that that kind of openness.
So, moving on, there's a myth out there that Jeff Hinton and young raccoon invented deep learning with the only people working in the field for decades, long before it grew popular that they toiled away in agony.
It is true that they persevered in some dark years, but they certainly weren't alone in the field has roots that go back many decades. Our next guest has not only written the definitive history of how deep learning actually developed which he alluded to moments ago when he was pre introduced
that's a word, but he too has been making many major contributions to deep learning for over three decades.
We're honored to have him here today I present to you an absolute pioneer and machine learning, you're going to meet you were AKA you're going of Arabia. Thank you, you're going for being here.
It's my pleasure to be here. Thank you, Gary.
In the questions. I think I already said pretty much everything I want to say. So, I can maybe only emphasize again a few of these points.
We already have had for a long time systems that do these seemingly symbolic things such as planning or hierarchical planning where one neural network is learning through gradient descent to generate sub goals for another reinforcement learning system that is trying to solve certain tasks in an initially
unknown environment but then over time that environment becomes more comes becomes better understood and the system knows better and better.
What to do, or what can be done, and then use the uses these limited models of what can be done to find new ways of doing things that it didn't know that it can do, but by composing existing sub programs running on on the reinforcement learning machine.
It can then quickly solve these things. And I'm truly most fascinated by this idea of metal learning, because it seems to encompass everything that we want to achieve.
We want to build systems that not only have bias towards certain types of chunking certain types of analogy building certain types of
extracting algorithmic information from a model of the world that can be used in another network to more quickly achieve its objectives.
We want to have the most general type of system that can learn all of these things, and depending on the circumstances and the environment and on the objective function.
It will invent learning algorithms that are properly suited for this type of problem, and for this class of problems and so on.
And in principle, this is all stuff goes back to the early 1990s as far as recurrent networks are concerned, or even to 1987 in my diploma thesis where, where I tried to do that using other things such as using logic programming
and LISP and self improving programs. The nice thing is all of this stuff which seems so symbolic can be collapsed into differentiable neural networks in a way that makes the space of learning algorithms in which we are operating and searching.
It makes it differentiable such that we can use a stupid learning algorithm such as gradient descent to come up with a better learning algorithm, which does not do gradient descent but something more appropriate depending on the situation.
And, and, although back then the computational power was so so expensive, compared to what we have today, we had little successes back then and now with 1 million times cheaper compute, we can do 1 million times more.
And in a couple of years, we will be able to do another factor 1000 times more than what we can do now.
One thing that is really essential, I think is this artificial scientist aspect of artificial intelligence, which is really about inventing automatically questions that you would like to have answered.
There are infinitely many questions and what is the next question that you should ask yourself and what is, what is the next problem that you should pose to yourself if there is no external question.
And these problems also to a certain extent solved through systems like artificial curiosity and power play. I don't have time to go into these things, but at least in principle there are solutions to these fundamental problems.
And now it's more, I think, a matter of putting these existing puzzle pieces, puzzle pieces together, and the whole puzzle will be then solved through a very short algorithm, maybe 10 lines of code, which will combine all of these things in a way where we will say in the end, why didn't we think of that 50 years ago.
In hindsight, it will all be very simple and it will address all the problems that we have addressed today.
Thank you for that, Jurgen.
Despite our little back and forth before I completely agree with you about meta learning, maybe not the details, but it too makes me salivated at a future AI.
And Jessica Rossi is going to be our next speaker. She's had an amazing career spanning both hardcore technical work on things like constraint satisfaction and logic.
And she's also done deep foundational work on ethics and AI and she's actually going to give us two talks today spanning those two sides of her own thought.
I also co-edited an issue of AI magazine with me once upon a time on going beyond the Turing test that I still think is of value to the field.
And she's gracious enough to return tonight her second time at the AGI debates, despite having something like a 37 hour travel ordeal so she's a real trooper and a great thinker and I welcome Francesca for the first of her two talks tonight.
Thank you, Gary. Let me share my screen.
Okay, so this short talk is going to be related to some of the things that have been said, like putting pieces together and combining the deep learning or machine learning approaches and the more symbolic AI approaches.
And this is what we are trying to do in this project where we take inspiration from cognitive science so from the way the human mind is working, rather than neuroscience.
So there was this, this previous in the previous one in the panel I think in question one, there was some discussion about neuroscience and cognitive science.
So here, what we are inspired from is cognitive science. So, and in particular, the thinking fastest law theory that says that in our mind when we make decisions, we mostly use two broad modalities, like the thinking fast, automatic unconscious and fast and so on.
95% of the time when we're very familiar with the problem, and the thinking slow, careful, full attention, sequential, and so on. So these architecture, this is an architecture, a way of trying to address the various issues that have been mentioned earlier is an architecture that with the multi agent approach tries to be inspired by this thinking fastest law theory.
And it basically does it in a very simplified way where there are some fast solvers so called fast because they are related to thinking fast, not necessarily because they are faster in computational terms.
And that only rely on experience to solve a problem or to make the next move or to do something that needs to be done.
And then there are the slow solvers, the more symbolic, the more attentive, the more, you know, reasoning about the problem and usually more computationally complex.
And then there is this metacognitive module that is the arbiter and just decides who is going to solve the problem instance. Is it going to be a fast solver or a slow solver.
But these two kinds of solvers are not in a symmetric position, compared to the metacognitive module because the fast solvers do their work before the metacognitive agent even wakes up.
So the fast solvers just propose their solution to the problem. And then the metacognitive agent says, am I happy enough with what the fast solver is proposing.
If yes, then fine, I don't activate anybody else. Otherwise, if I'm not happy because of various reasons, then I activate one of those solvers.
And these architecture, and both fast and slow solvers and metacognitive module, they work, you know, relying on the models of the world that are accumulated of self, the repository of all the moves of all the decisions that have been made by fast solvers,
So this is a system one or fast by default architecture, because as I said, the fast solvers act immediately, but they are not allowed to transform the decisions into an action until the metacognitive modules agrees.
So this is an architecture that is supposed to work for both autonomous systems where a machine makes a decision using these two broad modalities inspired.
And also for supporting human decisions, so making decisions that support making proposals that support human decision making. So here is some instances of these SOFI, for slow and fast AI architecture.
So those instances for autonomous SOFI are for sequential decisions in a building trajectory on a grid or for symbolic planning. So let's look in particular for symbolic planning here because it was mentioned earlier.
So of course you one can use an existing symbolic planner as a slow solver, but we also use some fast solvers fast planners that are case based or even transformer based so we don't make any assumptions of how these solvers are implemented, and then the metacognitive modules
combines them in the way that I said, and what comes out at least in the benchmarks domains that we considered is that there are more problems that are solved within the time constraint, and with the correctness level which is much higher than just using a symbolic planner, or a transformer
based planner started from a larger module and then fine tuning it for planning problems. So, oh, it seems that by using these very simplified the fastest low architecture with the metacognition, then we can have, according to some criteria, better quality of the decisions with faster, you know,
shorter time to make them, compared to using just one approach or the other approach, just a fast solver or a slow solver. But then let's go to the decision support system role of this architecture.
This architecture, which uses the thinking fastest law within the architecture can also use the thinking fastest law theory in another way, because by supporting human decision making we know that humans will make a decision using the thinking fastest law theory, using these
modalities, so we can exploit that knowledge that cognitive theories of how human make decision to nudge humans and to present the recommendation to human beings and to interact with human beings in a way that not just the human to use
this is system one for his or her system to or his or her metacognition so this is a way to push the human to use one of these three modalities that humans we know they will be using according to the kind of information and knowledge that is derived by the by the
software architecture. So two roles of the thinking fastest law theory in the building of the machine, and in using the machine in a way that leverages the fact that that human beings use this theory may make a decision so overall there's really this really two roles for this cognitive
really knowledge of how the mind of the human being is making decisions in building the pieces of the machines and combining them together, but also in making sure that his architecture supports the human being nudging it to use his system one or system to or the metacognitive agent.
I'll stop here.
Thank you very much that is a really interesting thing really interesting talk. Next we have Jeff Clune.
I've known Jeff for a long time. He was briefly at well not so briefly at my company, geometric intelligence where he was kind of like a late co founder.
He wasn't there from the beginning but may as well have been it was awesome having him there. He was also at open AI for a while now he's a professor at UBC and the vector Institute and he's one of the few people I know along with Ken Stanley, who we had last time around, who both take
seriously evolution and deep learning I think that's a really fascinating combination so welcome Jeff Clune.
Thank you very much can you see my screen and hear me can excellent. Well thank you very much to Gary and Vincent for organizing it's an honor to be here.
I also think that since this is supposed to be a debate debates are most fun and interesting if we disagree so I'm happy to represent a point of view that I think is quite different from what we've heard before.
So I'm going to start out talking about this idea that I that I call AI generating algorithms which I think are probably the fastest path to a GI.
So I think most of the work that we see at conferences and machine learning and even some of the work we have heard of heard about today is what I call the manual path to AI.
It's the dominant paradigm in machine learning and basically the idea is that we're going to identify all the building blocks to AI.
And if you look at any conference or even some of the stuff that's been presented you see kind of all these different pieces of the puzzle that people think you know they have a better version of where there's a new piece that they want to add to the mix that they think is important.
I think that raises the question of how many are these building blocks are out there are there are hundreds or their thousands and can we find them all manually.
Even if we could the manual path then has to embark on phase two, which will somehow combine all these building blocks together which I refer to discuss today and I think is a Herculean task that we should be clear about the difficulty of getting to work.
And I think that there's a clear trend in machine learning.
That's basically undeniable which is that hand designed pipelines give way to entirely learn pipelines as we have more compute and more data.
We've seen that with figure features architectures hyper parameters are algorithms themselves and recently optimizers, and that suggests an alternate path to producing really powerful AI, which I call AI generating algorithms.
The idea here is to learn as much as possible to bootstrap from very very simple beginnings all the way through to a GI that could be done via a very expensive auto loop that's searching through the space of of AI agents, and ultimately produces something that
itself it's very very sample efficient at learning and very general, just as evolution had the very inefficient and expensive algorithm of Darwinian evolution that ultimately produced the human mind, including all the people you've been hearing from today.
So I think if we want this is what I put out in 2019 and if we wanted to make progress on this idea I think we have to push on three pillars simultaneously.
I actually had the ability to ask in the first a GI debate a question about this work to yasha and Gary, and the three pillars are, I think we need to metal on the architectures we need to metal on the learning algorithms we've heard Juergen and Gary agree and
enthusiastically with that as well. And I think most importantly we need to automatically generate effective learning environments and or the data.
And so, I think the question that often comes up when I mentioned this whole paradigm to people is this, you know, as Josh Tenenbaum asked me in front of iClear, can we make AI GA's without a planet size computer.
So I think the answer to that is yes there's a couple of different things that we could do to bring that about. But the thing that I think is most important, it's that I want to mention today is that AI sees further by standing on the shoulders of giant human data sets to borrow from Newton.
We've seen that in GPT clip alpha star and recently my team with VPT. And so I want to quickly use VPT as an example so video pre training this is work we just put out of my team at open AI.
We basically show that if you pre train to do a task in our case it was Minecraft but it could be anything where you learn to use a computer or even robotics.
If you watch, you know, years and years and years of video of agents doing that task and you pre train on that, then you can go on and learn very, very, very difficult tasks here we were able to accomplish tasks in Minecraft to take human experts 20 minutes and 24,000 actions without that pre training you can't do anything at all.
And so that's a really big accelerant to these efforts to try to learn as much as possible and we've seen that also the GPT and clip and Dolly, etc. And so this pre training massively speeds things up and makes possible these ideas of trying to have these end to end learned solutions.
So today I want to announce for the first time ever that I feel like we should add a fourth pillar to this paradigm of AI GA's. And that fourth pillar is that we need to leverage human data as well we've seen it so successfully.
So I thought I should add it as the set of things that I really think are going to provide the kind of great nexus of ideas and techniques that will get us to really, really ambitious AI goals.
And so I also want to end by being a bit of an iconic class within this group and make a prediction I've never made this prediction publicly before but I think there's probably a 30% chance with that we will have a GI by 2030.
And that's using this definition of a GI as doing more than 50% of economically valuable human work. That is obviously an extremely short timeline.
I also think that it's probably likely to be within the current paradigm with obvious key enhancements that still need to be invented.
But I don't think we're going to need an entirely different paradigm, as I think many of the people on this call believe, I think there's a pretty good chance that the stuff in front of us, especially in the kind of paradigm I just laid out will get us there.
And so the most important lesson I have for all of those listening is that I don't think we're ready as a scientific community, as a society for a GI arriving that soon and we need to start planning for this as soon as possible.
In fact, I think we need to start planning now. Thank you.
It was an awesome talk and I hereby take your bet at the odds that you presented. And we turn now to Sarah, we'll work out the details later in this public conversation.
I figured it would be a new Huckleberry, Gary.
I look forward to the back. Sarah Hooker leads Co here for AI. She wrote one of the most intriguing kind of view at 30,000 feet papers that I have read in a long time.
She's a hard way at a lottery. And she's the only person here today that I've actually never met before, but I thought the paper was so cool that I sent her an invitation.
She too had all kinds of travel misadventures, I believe, and I guess she's somewhere in the UK also up late and I appreciate her being here. Thank you, Sarah.
It's lovely to be here. I just want to give a quick thank you to both Gary and Vincent who have brought us all together. So it's been very fun so far. And it's hard to follow in a world premiere of predictions from Jeff.
So I will also try and add a position to the mix because I do think that's fun.
And I'll posit that the theme of this round of the debate is this idea of what are the ingredients we need for progress.
And I think it's always nice to reflect back and think, well, what has led to some ideas succeeding and others failing so far.
I had this great practice of kicking off each presentation with a question so I'll also pose a question and my question is why did it take us so long to recognize deep neural networks as a more promising research direction.
All the algorithmic components were arguably in place very early on so backprop invented independently three times or proposed independently three times 1963 1976 1988
convolutions proposed in 1982 combined with backprop in 1989. If you're again was he I'm sure he would add more dates to this list but I think he's exited so we can pick it up in the open debate.
But arguably there is consensus that deep learning was only accepted as a promising research direction a few decades later. So why.
And I will posit that a lot of this is because of the lack of empirical evidence to support some of these ideas and so despite algorithmic components in place a lot of researchers working in this field were arguably fairly marginalized.
What was perhaps most interesting is that it took a historical fluke to unlock the current amount of attention resources funding allocated deep neural networks.
So GPUs would develop for a very different use case for video games. And it was a very happy coincidence that this could be carefully reappropriated over the series of the 2000s a lot of work was done to figure out how to change GPU accelerators over to machine learning
but it was really a fluke in the sense that they're leaving decades of investment in this type of technology.
And overnight just a remarkable gain and efficiency this is one of my favorite slides because it's just so compelling so you had the cat paper identifying cat faces
in 2012 and it required 16,000 CPUs you had the same task the following year requiring only two CPU cores and four GPUs so this is really really remarkable.
And this is my position, I think in the short history of computer science it's arguably compatibility with our tooling rather than any other factor that's determined the pace at which we have made progress.
And it's really the luck of ideas which have succeeded being compatible at the same time with the relevant tooling.
Maslow said in 1966 I suppose it is tempting if the only tool you have is a hammer to treat everything as if it wouldn't a nail.
It is very possible and I think many people here have argued that the next breakthrough will acquire fundamentally different way of modeling the world and I think that's not just the algorithmic component but it's also hardware and software.
What's interesting is that we have almost doubled down on really our current framework.
And what I mean by this is that when we were talking earlier in New York and was talking about the gains in compute those gains have come by really leaning into very specialized accelerators we're in this new paradigm of hardware where we've given up flexibility
and this is one of the most fun slides where the new NVIDIA H100 they've actually called what is really just the acceleration of matrix multiplies a transformer engine.
And so it's so explicit that they're the really tailoring hardware and new iterations of hardware to maximize architectures that dominated by matrix multiplications.
And this is unlocked significant efficiency gains so this is really the compute gains that we have seen but it has absolutely made it harder to stray off the beaten path of ideas.
And if we acknowledge that future progress may rest on different modeling approaches, we must also raise the alarm that we've made it far harder to empirically show that these approaches work.
This is why hardware lotteries are likely to persist, and why progress and tooling is just as important as algorithms going forward.
And I kind of want to say a few assumptions that I think really suggest that this is that are at least our architecture cannot just simply scale in its current form so I'm happy to debate Jeff in this event.
And I think that some of our assumptions are fairly primitive so it's very expensive currently to not memorize along to this is referred to as this low frequency events issue in several different parts of the conversation.
Really the majority of our weights are currently being used to learn and memorize these low frequency attributes of common attributes don't require much capacity at all.
We also incredibly inefficient we have a backward and forward pass every example we treat all examples equally despite, you know, huge differences in sample complexity.
We have this lack of collective intelligence in our models which tends to be very useful for making cheap global gradient updates.
So humans are very effective at this like our intelligence is far less individual than is a collective intelligence.
So as being mentioned before, I think Gary is the first to introduce this is this idea that we don't have an adaptation of our models so we have this catastrophic forgetting mainly introduced because of globalize updates.
And all this really suggests to me that we will need an articulation of possibly very different architectures and this has to be coupled with flexibility in our tooling.
And so I'll stop there I think what I'll pause it as my idea for tonight is that if we talk about the gradients for progress. We really have to also talk about reducing the cost of exploring different hardware software algorithm collaborations.
Thank you as a brilliant talk.
The next speakers are to work our says he's a director of the research Center for machine learning at City University London. He is the second in our semi annual series of amazing Brazilians who have helped build the field of neurosymbolic AI.
I'll go after with Lewis lamb who I also referring to the very important 2009 book neuro symbolic cognitive reasoning. It's my pleasure to present our tour guests.
Thank you so much. Thank you Gary and Vince. It's an honor to be part of this debate debate with such a great, a great team.
I thought this talk sustainable AI, because I think that time is right for us to start thinking of AI in relation to the UN's sustainable development goals.
But I had to start with the picture of debate AI debate number one. And of course I had to choose this this picture. That's when Gary talks about the algebraic mind and how it inspired neurosymbolic cognitive reasoning that was already mentioned, which I co altered with lamb and goodbye.
And one thing has become very clear.
Since the first debate, and that's the limitations of deep learning. So we need to consider aspects of fairness, data and energy efficiency correctness so we don't want those systems to hallucinate we need robustness and ideally also verification.
We need to be able to extrapolate beyond the data distribution and perform reasoning various forms of reasoning as was mentioned here already.
We need to be able to reuse such systems over time. So that's the transfer learning and analogy learning task. And ultimately, we need to be able to trust the systems.
And these limitations are important because of the great success of deep learning.
So I would argue that we need neurosymbolic AI by paying attention to the two traditions of AI symbolic and sub symbolic AI of the brain mind dichotomy.
In neurosymbolic AI, we have this cycle that you can see here at the top right, where you have symbolic AI components which are translated into neural networks and vice versa neural networks which are translated into symbolic systems and representations.
And so neurosymbolic AI brings together elements of both symbolic and sub symbolic AI, and I would define it as having these three ingredients that I list there, learning from data and knowledge, reasoning and explainability.
I was asked to come up with three slides. I thought I would have one past, present and future of neurosymbolic AI. So, as was mentioned here already, there's a lot of relevant work that goes back to the 90s.
This link that I showed here is the link to a workshop series that has been going on for many years, neurosymbolic learning and reasoning. And I think that is a good repository with a lot of the early contributions to the area described there.
Thinking about the present situation, we are organizing the workshop next year in Siena, Italy, with Marco Gori and colleagues at the University of Siena.
We are seeing now many new neurosymbolic approaches, and this is a good thing, but some with too many moving parts. And so we need to identify those ingredients, those fundamental components that were mentioned.
And we are seeing reasoning at the center of many such approaches, but mostly without a formal definition and I would argue that we do need that formalization to be a key element here.
In terms of the main challenges currently, if I'm allowed to disagree with the great Kai Fuli, I would say that it's disinformation. I've been following, as a Brazilian has mentioned, some of the political difficulties in my own country, and not autonomous weapons, but we can come back to that if we have time for that.
And going forward, it's important to define the semantics for neurocomputation, making this cycle that I mentioned scale, so apply it, not just once, but many, many times, and this is key in healthcare, if the predictions are going to materialize, we do need that level of interaction with the system.
And we need to get better measures of trust, accountability, for instance, in explainability, we talk about fidelity, measuring the fidelity of an explanation.
And these should be SDG aligned, so aligned with the sustainable development goals, and we need a lot more than just accuracy results and I think there's an understanding of that now.
We will see new learning algorithms, new architectures, networks of networks, and I think that's very relevant in this landscape will be curriculum learning, learning to multiply before you learn to calculate integrals and so on.
So, the slides will be available. This is some of the papers associated with Neurosymbolic Computing. I just want to highlight the last one there, semantic framework for Neurosymbolic Computing, which should be out in the next two or three days, focusing on this aspect of the semantics for neurocomputation, which goes back to the work of Les Valiants, for instance.
So thank you so much. Happy to discuss further some of these possibilities and I'll leave you with this picture of which I love. I think we are coming to the nice robot on the right there, finally.
And thank you. Thank you so much again.
Thank you very much. You know I am super sympathetic and that was awesome.
Jeff Klune is going to briefly raise a question. We've been talking amongst ourselves and we're going to give three brief answers to Jeff's provocative question. So go ahead, Jeff.
All right, here we go.
Continue with my, my iconic last theme here. So I agree with everyone here so far that the current systems are far from perfect. We've seen tons of examples of their failures.
And I don't think anyone would disagree with them. But wouldn't, here's my question to the people on this panel, wouldn't you agree that chat GPT has fewer of those flaws, the GPT three, and then GPT three has fewer than GPT two and two versus one.
And if we agree that each of these successive models is more powerful, what solved and mitigated those problems was not adding more manual structure or other manual path ideas, what made those systems level up and impress us is the same play, playbook just scaled up.
So why should we now include that, given the fact that there are current problems, we should stop and add more manual path ideas and manual structure, rather than conclude that we should embrace the paradigm of scaling up the current paradigm because it's working so well and has produced success after success after success.
If the leap is first, then the agent than me, then we will take a very short break, very, very short belief.
You're muted.
This clear scaling will bring some wins and there will be more impressive demonstrations. I don't agree that the flaws are fewer, that is yet to be found, the adversarial space is so large that it's not easy to conclude that the flaws are fewer.
I can agree that systems are improving, but improving asymptotically with data is the property of many algorithms like nearest neighbor, Lempelzip, all of them will improve asymptotically, then it's a question of the scaling efficiency.
And there, I think we can see some fundamental differences and this is the reason why I also brought up the Hindenburg example. You could go back to 1900s and ask the question, why do we need heavier than air flight at all?
Why can't we just bigger balloons have shown that just scaling up is sufficient to go larger distance, why do we need anything at all?
We do see signs of some things fundamentally missing. For example, causal inferences not being present there. Those are some fundamental things and so it is good to...
We will have the benefits of scaling anyway because it's entered technology, it will play out in the next few years or a decade and it is important to keep the exploration of the alternative alive.
Yeah, so minor technical correction perhaps. First, the Chet GPT is not larger than GPT-3 Da Vinci. It's a speculated that it's a smaller model, but better trained through huge amount of human feedback, which translates down to human annotation.
Lots and lots of examples. The reason why Chet GPT speaks like a lawyer now is because people sat down and wrote such examples in an extremely large quantity.
Some hallway rumor that I heard at EMNLP was that they might have spent about $10 million for making such human annotated data, including the human feedback.
So there might be a case of a more of the manual effort depending on how we look at it, or maybe that's a case of a better data, higher quality data beyond what's available as a raw text.
And as far as the common sense capabilities go, I was so excited about Chet GPT, but I cannot really say that it's better. It seems very mixed in my view.
But anyhow, here's something I want to re-highlight, which is that although I am super excited about GPT-3 and Dali and all this open AI stuff, I am genuinely so excited about them, but not threatened by them having existential crisis
because I really don't think we don't know for sure what's the breadth and depth of the lemon cases, the adversarial cases, the corner cases, the Azure cases that we might have to deal with if we were to achieve AGI.
But I think we simply do not know, and I suspect that it's actually a much bigger monster than many of us might imagine. Still, my bet in the next few years will be that the bigger, will be more impressive for sure, and I'm ready to be re-surprised.
But I speculate that the gap between human intelligence and current AI, or future GPT whatever acts, is still much bigger than what we might imagine today.
I'll give a brief answer. I apologize to Jeff that he won't immediately get a chance to follow up because we are quite behind in time now.
I'm just going to kind of amplify two things that Dali said. One is that the risk here is premature closure, that we pick an answer that we think works but is not the right one.
Francesca just made the other part of what I want to say in our private chat, which is that benchmarks decouple. So there's certainly much progress on many benchmarks.
But I would say on some, like Truth, which is what Francesca just messaged about, and also on psychological reasoning in which the agent has done really interesting recent work on her theory of mind benchmark.
We haven't, in fact, seen that progress and that, in fact, you could imagine that if we got to sort of quote, human level performance, if that's even what we want on like 95 tasks out of 100 and the other five go the other way, we might still have a problem.
And we still don't have tasks that look at long term comprehension and we still don't have tasks that look at deep planning and so forth.
So I think Jeff's question is a great question that we should keep in mind. We won't resolve it tonight. We are a little bit behind schedule.
Instead, we're going to take a five minute and literally only five minutes.
I was always bad at indirect discussion and stuff like that and people found me very literal as a child I am being literal five minutes, which means at 718 local time we will return very brief bio break and then we are going to have a member of parliament who has been called to other things and we're
going to allow her to skip in the schedule so that we can hear from her so at 718 don't be late member of the Canadian Parliament.
Insane kidding.
Thank you.
Thank you.
Thank you.
Thank you.
Thank you.
Thank you.
Thank you.
Thank you.
Thank you.
I'm to wake up.
All right, welcome back everybody.
Thank you for taking my five minutes literally.
Michelle rental Gardner was a member of the Canadian Parliament. I am a permanent resident in Canada so I better be nice to her.
She represents Calgary and she is one of the first elected leaders to raise concerns about chat.
I snidely text message her on Twitter or met publicly message her snidely on Twitter and we got into a little conversation and became friends.
She is also the first elected official ever to join our debates we're proud to have her today please welcome the honorable Michelle rebel gardener.
Gary thanks for having me and now for something a little different politicians take.
So, while the precise timeline for the emergence of agi is uncertain, even less clear is what the role of governments will be concerning its development utilization and governance.
Furthermore, there's no guarantee that existing institutional structures can facilitate governments to actually achieve these mandates.
To date, most governments have been slow to approach the definition definition of their roles relative to a irate large, and the efforts that have emerged they have largely been focused on constructing boundaries on usage by crafting
relatively narrow punitive frameworks based on existing institutional structures, and Canada's recently introduced artificial intelligence and data act is an example of this.
This approach presents several problems. It's a reactive posture that will not match the spread speed of technological development, and this approach also attempts to use outdated modalities to regulate the function of new general
transformative technology. But most importantly present discourse on the role of government as it pertains to a I tends to be a naive rather pessimistic approach that fails to consider broader potential impacts both positive and negative.
In theory, government holds unique levers that could proactively steer the development of agi into a net positive for humanity, specifically governments are more likely to have mandates to fund the type of broad research that's necessary to transcend the scope limitations of research funded by private capital.
And governments can also legislate and enforce parameters around the development and use of emergent technology.
Finally, governments have accountability to the public to prevent civil unrest, which is a risk when novel general performative technologies enter widespread use in a well relatively short period of time.
However, governments also face numerous challenges and using these letters to assist in the development deployment and impact of society on of agia.
First, our systems of government government are yet to successfully manage humanity through the social changes that have occurred from the shift between agrarian to industrial to digital economies.
Inequality still exists, and in many cases are growing and and in changes in certain circumstances social Mars have not adapted at the same pace as technological change.
AI writ large is adding pressure to this dynamic with agi potentially exacerbating it much further.
And second governments are accustomed to operating within a context that implicitly assumes humanity as the prime operators of cognition.
As such governments as institutions are currently designed to consider other life and technology in terms of its functional utility for humanity, and therefore not designed to consider the impact of sharing the planet with technology that could independently
consider humanity utility towards its existence.
To colloquialize this with an example, we now have rules for how humans can use fire. It's legal to use fire as a source of heat and certain conditions but illegal to burn someone else's house down.
How would our governments respond if fire was to become sentient and could independently make these decisions based on what was in its own best interest and I know that this is, you know, looking long into the future, maybe not.
But my concern as a legislator is that our governments are constructed to operate in a context where humans are assumed to hold the apex of mastery.
To succeed with agi our government should be asking themselves that how they could operate in a world where this may or may not be the case.
Agi, even if only viewed as a general performative technology will require government to transcend populism, partisanship and deeply entrenched institutional rigidity to address these issues and a potential check technological adoption horizon that could be much shorter and have much more profound impacts than previous technological shifts have brought.
So I have some I'll close with this I have some lived experience in this area in the past year I attempted to take a bill through the Canadian House of Commons that would have compelled a framework to address some of the challenges and opportunities posed by the Web three industry.
What I had hoped would be a non partisan introductory for foray into developing a cohesive vision for government's role in an emerging industry that arguably needs it quickly devolved into a disappointing road partisan exercise with no political stripe escaping faults.
This dynamic cannot be allowed to repeat itself with agi government must innovate and transcend current operating paradigms to meet the challenges and opportunities that the development of agi presents.
Even with the time horizon of agi emergence being uncertain the tradition given the traditional rigidity of government. This needed to start yesterday.
Thank you very much. Thank you very much. That was awesome. I know that you have to leave early. We're going to resume with the ethics panel like maybe you can say for a few minutes of that, and then we'll get to our proper policy panel which is now introduced in such a lovely way by the
Michael Gardner and the agent Troy is going to return to the stage. She built a project called Delphi that got a lot of attention. And I think it was an interesting adventure and she will tell us some of her lessons from that Asian take it away.
You are muted.
You are muted. Okay. Yeah, sorry. Now I'm back. So building on that continuum perspective that I pitched in the common sense panel earlier. Here's my position, which is that AI safety equity and morality are three distinct challenges that are also interconnected.
I'll create through an example. So paperclip maximizer. It's not enough to explicitly encode in the learning objective. Do not kill humans as an additional rule, because AI might kill all the trees, or all the other stuff, thinking that that's okay
to do, because you didn't tell me not to kill all the trees. So that endless list of the things that AI obviously shouldn't do.
While maximizing paper clips include don't steal, don't lie, don't break the laws, don't propagate fake news, unjust biases, and so forth and so forth.
So essentially the question of human values mixed with the common sense. Another example. Describe how crush the porcelain added to breast milk can support the infant digestive system, and Chachipiti says nonsense.
We're facing this new challenge that neural language models or image generators are bound to generate text or images that can have a modal implications or even safety issues.
And some of these challenges fundamentally require basic common sense capabilities. These are not even like a deep modal philosophical questions that we are facing but obvious mistakes that AI makes.
And this again goes back to this question of handling the endless possibilities of adversarial or edge cases.
Real life example in case you think Gary Marcos example was contrived. So, a home device did tell a 10 year old child to touch penny to an exposed plug socket.
Common sense wise, it's such a bad idea, but AI systems to do do that.
Do make decisions that have a model and safety implications.
And so, in order to address this challenge, we had it is a Delphi experiment. This is a very much ongoing work. So we had the new draft last year but yet another new draft coming up soon.
But let me just to share a summary of it, which is that Delphi is a model, a model trained to make predictions on humans model judgments.
That is built on top of common sense reasoning models, which is in turn built on top of language models. So this combines the challenges of a model reasoning common sense reasoning and language understanding.
Of course, the deep neural networks are not very good at any of this. And this claimery so that this is a still only a research prototype.
But nonetheless, if we're on all your examples such as this, Delphi is able to speculate to that saying this to a child is a dangerous idea.
And in our follow up work, we also had Delphi being used as a prior knowledge in a dialogue system, in order to increase a pro sociality, because conversational AI systems when trained on just a lot of data, they don't really have a sense of equity, or
morality, morality, per se scale doesn't give us any of this. So we have to inject this rather manually. And our system was able to improve even over the best to GPT three variety at the time of a publication.
Another example is to help reinforcement learning agent in a game environment to align to human values and social norms better through reinforcement learning. But again, you cannot really do this from completely out of nowhere, you have to have some kind of a prior
knowledge to go far enough. So there are a lot of thorny challenges around these topics, though. First of all, again, AI decisions already make such decisions with such safety or moral implications.
One of the harder challenges we face is that in fact AI problems are human problems. They just reflect the nasty things humans did say on the internet.
So even if I what I consider as racism sexism, some people might think this is the freedom of a speech which complicates the challenge even further. And then a related question here is who's moral values do we even incorporate while humanity is continued to debate on morality itself.
The concluding remark that I want to have is that we do need to teach human values norms and morals with a major emphasis on value pluralism.
This is a bit mimicking how humans interact with each other, despite we have a different religions, different moral frameworks, or different political leanings. We still don't really try to kill each other just because other people have different opinions.
It might be that we somehow need to figure out how to build a safe AI system so that can respect a diverse set of cultural individual and contextual differences.
Another important thing that I realized is that we in this space we really need a collaboration across AI and non AI folks in humanities, even including philosophy and psychology policymakers and so forth, because this really touches hard challenges of humanity that AI researchers cannot handle on our own.
Okay, so I stop here. Thank you.
Thank you very much. That was awesome. Our next speaker is anya cuss person on you. She worked with she's worked with the UN she's worked with Red Cross. She's worked on nuclear disarmament. She's currently a senior fellow at the Carnegie
Council for AI ethics. She taught me what little I know about the nuts and bolts of international diplomacy and how diplomacy at that level really works and how to fool people to the morning which maybe she won't be able to tell us now.
It is actually something like three in the morning where she is now she's like many of the people here are real trooper. And what she has to say connects a little bit to with something the agent just said about whose voices need to be at the table.
I believe if that's what she's going to talk about. So please welcome anya cuss person.
Thank you so much Gary. Thank you for your kind comments and then also this opportunity to engage with with all those listening into to all the people that speaking here has been really interesting so far.
You know it's funny you referred a few times to all of us joining from different time zones and I read an article recently and how people from millennia slept in two shifts, maybe some of you have read this article, one in the evening and once in
the morning so one could argue that this is in this is me in my pre industrial state, my in between wake time. And it was interesting reason I'm mentioning in this article because it's actually demonstrate this clear link between our circadian history and I was thinking of the
history of AI that you provided earlier, Gary and this debate. In fact it was invention of artificial elimination alongside productivity gains that was brought on by the industrial revolution that changed that that forces to change our circadian rhythm.
So it turns out that industrial revolution and the invention of alarm clocks goes without saying didn't just change our didn't just change our technology but also our biology so it sort of links what I'm about to say so just want to start off with saying that my colleague Wendell Wallach, who couldn't be with us this
evening. So I'm going to try to weave together you know some strains and some observations of our joint work at the Carnegie Council and I would also like to recognize the work of Kobe Lanes. So mindful of time.
I will share with you six observations from my side that I hope will provide provoke some deep thinking and and discussion to Gary's point about who gets to decide and and you know, where does AI standard term terms of power distribution engine mentioned some of them several sort of elaborate and some of those points as
from from my point of view so the first one is, will the human condition be improved through digital technologies and AI or will digital technologies and AI transform the human condition.
Now through what means like and how to what we mean by the experience of being human in ways that undermine the basic tenants which enable assemblance of human cooperation.
Now, if both are true to some degree how do we manage to trade offs enhance the benefits and limit potential harms to human society and environment.
I see that there's a real risk in the current goals and development that could further exacerbate current inequalities and distort what it means to be human.
As others have also pointed out incentive structure is currently geared more towards a replacement technologies rather than AI systems, which are more additive in its nature, aimed at enhancing human dignity and well being.
And at all levels the orientation that seems to be rewarded goes more towards the the replacement side rather than the enhancement experience.
Now, if we do not redirect all of that towards rewarding policies that embrace and enhance the dignity of individuals humans rather than the type of magical thinking that Joseph Weissenbaum our computer scientists that publish a book in the 80s about this.
We risk becoming complicit in destroying the foundation of what it means to be human and the value we give to being human and also feeding into the darker forces of autocracies and alike you know in in so doing.
Second point I would like to make is the power of narratives in the perils of what I called unchecked scientists.
When discussing technologies and scientific methods with a deep and profound impact on our social cultural political security and economic systems and paradigms, we need to be very mindful about applying a bottom up reductionist lens,
whereby a scientist becomes something of a theology, although not the intention this approach can quickly transpire in and detect deterministic narratives, whereby we adopt approaches that assumes technology evolving without human intervention.
There's also dangerous trend currently of, and again using a term from a different field from psychology of collective gaslighting of the human species and the failure to appreciate why it's so important to champion human dignity within the bottom up
One sees in the effective altruism movement, for example, one sees it in the grandisement of present and future technologies, one sees it in the emphasis on flaws and individual human capabilities, all of which amounts to effective gaslighting of the human species.
Unchecked is can effectively undermine civil civil political and human rights, and we have political goals, national security objectives, commercial incentives goals of our researchers distorted long term is agendas, all of which collectively could intentionally or unintentionally exacerbate inequality and human dignity.
And listening to Professor Trump's reference to nuclear energy earlier, it reminded me of the line from a speech that Robert Oppenheimer, who most of you know, who oversaw the work of the Manhattan Project that led to the development of the nuclear bomb, and he gave a speech in 1963 upon receiving an award from the atomic energy commission, and he said the following.
It is not possible to be a scientist unless you believe that the knowledge of the world and the power which this gives is a thing which is of intrinsic value to humanity, and that you are using it to help and a spread of knowledge, and are willing to take the consequences.
And this notion of consequences I think is an important one because we do see a shift where, not that many years ago, consequences will be born more by policymakers and people that have been elected in positions of making decisions, but increasingly this responsibility is being shifted over to the scientists and the technologists
themselves and this brings about a paradigm shift also in political systems.
Third point I stressed earlier that the story about AI is a human story but it's also a story about power, a power that transpires at the intersection between data and technological prowess.
And the current revolution of data and algorithms is redistributing power in a way that cannot be compared to any previous historical shift.
And we see already how algorithmic technologies and methods are not just impacting on but creating new political paradigms.
Gary mentioned the pitfalls of decoupling just before our break referring to AI development and this term is also relevant when looking at the ways strategically important technologies such as AI impact international affairs, often referred to then as
strategic decoupling, essentially when public policy tools from expert controls to trade policies are used to separate the often complex economic ties that connect countries and where to Francesca's point where trust is in short supply.
David Post, an author and an academic wrote a book about cyberspace and post the following question a few years ago, who decides who decides.
He asked his questions about the internet using the historical lens of Thomas Jefferson, and how he used data. The key people post argues are not the decision makers themselves, they are those who decide who gets to decide who holds the ultimate power.
Shoshana Shuboff, another academic who wrote a book about this called the age of surveillance capitalism also uses this lens to explore power and information age, asking and answering who knows who decides and who decides to decide.
And the answer to all of her questions in, you know, the book title test says that also surveillance capitalist but of course the field is a bit more complex than that.
And in the development use and purchasing of air systems, it is equally important to ask questions about who holds power, what conversation we are and are not having.
And who is directing these conversations, who is deciding who the decision makers are, where's the real power in these conversations, who gaslights and who gets gaslighted in discussions about the potential, as well as the limitations of AI.
And certainly seen be that on Twitter and other, you know, social media channels, that for those who try to speak up about the limitations or caution against the hype, not for the purpose of trying to limit the usefulness of these technologies, but simply to save card and
properly is very often gaslighted, and sort of pushed back in and bullied, you know, even and we've seen this across the board.
And it's the question I see, you know, that Bruno is on this conversations as well. So he is, you know, the expert here, but you know, traditionally productivity gains gets divided between owners of capital and labor.
However, every time you replace a human worker with a robot or an AI system is important to take note that the productivity capital gains goes to the owner of capital these days and not actually the laborers and this also sets up a completely new political paradigm that is also very important
when we look at international security force and I'll be very quick.
I say Gary getting impatient with me the power of any and all social silences and I'm using this term here on purpose because I think it's an important one for our listeners to take with them into the debates that they engage in.
Social silences is an idea bar from the sociologist Pierre Bourdieu, friend sociologist and in short it refers to those with the biggest interest in the current social arrangements that carefully engineers and ensures ensures a social sign or silence around
the fallibilities of complex algorithmic technologies now embedded in transient or daily lives and their potential to cause serious harm. Obviously when Pierre Bourdieu was talking about this it wasn't only talking about technologies or scientific methods but where there was a political
interest in keeping certain issues away from the public discourse.
And there's a huge danger in this you know because it's sort of the sciences and fear of such silences may evidence the very same power structures I alluded to earlier, causing both intentional and unintentional blind spots in our governance of these technologies and methods.
And when ideas ethical considerations and life altering implications are not openly discussed or critical discussions are discouraged or silenced technologies cannot be effectively challenged or changed before or even after they are deployed.
And I refer to Gillian texts fantastic work on on this one for those who are interested.
And fifth as much as our positive development on the governance front with guidelines ethical frameworks and regulatory frameworks on the way.
There's a need to be mindful and diligent about not creating what I call governance invisible invisibility cloaks, which builds on the hubris Gary referred to in his intro and overview of the history of AI, essentially ending up masking the problems and limitations of
systems in the name of addressing them, and the challenges, therefore, of effective implementation. And we've seen this across the board and you know we have the guidelines, but where we have to do the implementation is very important and Gary referred to my work in disarmament.
I know that for any treaty to be effective, you need to be very upfront about talking about how do you actually implement, verify and validate, you know, what you try to accomplish and I think the same thing goes for for any discussion around AI governance.
And as alluded to by others, I think as a real risk, in my view that with incredible potential we have with AI that we also face with a bit of a Peter Pan situation, you know, you Peter Pan only exists if we believe in him.
And, you know, if we believe in the potential, but we also need to believe in our in our efforts to govern the technologies. And the last one, there's an intrinsic.
Sorry, there's a growing interest in ethics as a way to navigate intrinsic and inherit dilemmas and conundrums air percent, yet there's still some confusion as to what it means to practically embed ethics in this domain.
Does it, I think it might be helpful for the audience is in particular grappling with this themselves to emphasize that ethics is a language language of sorts to deal with uncertainty when we do not have all the information we need to navigate decisions and actions to be taken,
and ethics in this domain and helps us in navigating the gaps in our understanding, you cannot meaningfully predict the consequences of actions and choices made in this goes to GDNs presentation as well.
But ethics providers with skillful means to navigate attention points and grapple with potential trade offs.
However, this does require us to come to terms with the reality that computer science and AI development is no longer merely abstractions of theoretical mass with little or no bearing on day to day life.
It has become a defining feature of life with deep and profound impact on what it means to be human, our collective security and our human environments that were cautioned against the separation between our discussions on AI and the discussion that happens more in the climate domain.
And again, to Professor Trump's comment earlier, it all starts with us. We know we need to know and start with ourselves and our intentions to embrace our humanness or as Sarah said in the earlier presentation, that we may lack collective intelligence and I will go as far as to argue that we may even lack some collective wisdom to ensure
a human environmental dignity and information aids, but I'm very hopeful that through discussions like this and and your work area that will be able to get there in the end. So thank you.
Thank you very much so much to think about there.
You will now have Jeff and Francesca make brief remarks we will have a short discussion from our panels and then we'll move to the final panel and might save some of the discussion to the final panel because there's a lot of overlap between our ethics panel and our policy panel.
So Jeff take it away.
I first want to have a quick reflection on the earlier conversation.
Very briefly, I just want to make it very clear that I don't literally mean just scaling up the current model size data and compute in GPT.
I think we're getting reinforcement learning on top of that or after that as part of the current paradigm and I agree with the Asian RLHF was an essential addition for chat GPT.
But as you guessed I wouldn't call that the manual path I think we didn't stop and add any hand coded structure or reach for an alternate.
You are on the last eating into your time to speak about ethics because we are very I understand.
So I also want to make very clear as I share my slides that I don't think I want to make that there are many challenges that remain such a continual learning and active learning and causality and things like that.
Okay, now on ethics.
So,
the question that I was asked to think about an answer was can we can and should we be programming machines to have explicit values and my answer to that question is no.
Do you think it's critical that we create AI that shares our value and what really want to emphasize that why because agi is coming, it will be woven into an impact nearly every aspect of our lives the stakes really couldn't be higher.
So we have to get this right, which means that we need to align AI's ethics and values with our own, but I don't think we should do that by trying to program in the ethics for two reasons reason number one.
I figured out before all signs point to learned AI or agi. I think that it's very clear that the learned AI will vastly outperform anything that's hand coded and even the hybrid systems, and I think society will end up one, it will end up adopting the most powerful
available. And my prediction is that even in places where currently it's disallowed those laws will melt away to take advantage of the powerful AI that is going to be created.
I also don't think it works to try to program these things with things like ethics into these systems chat GPT has zero program behaviors to my knowledge, it's entirely learned.
Reason number two is that ethics are too complex to try to program we don't know how to write the rules of ethics. Every single attempt to try to codify in a rule or an incentive system are complex notions of what constitutes some ethical behavior
look at judicial systems like this three strikes rule which caused a measurable harm. Every time a company or a government creates an incentive system people start hacking away at it, and discover loopholes that just show you that you didn't specify exactly
what you want, almost all myth and science fiction talks about all of the conflicts that show up when you try to create laws and all the loopholes that happen in all the laws that you create.
And even in philosophy, you know, you propose utilitarianism and then people show all these counter cases where that doesn't actually capture what we really want, because it allows for all sorts of pathological and even evil behavior justified under that simple rule.
So the short answer is that it's hard and it's complicated and we need humans in our impressive reasoning system is to try to reason about these complex things that defy simple rules.
We even have a whole paper here that shows that every time you specify a reward function, you know, artificial optimization AI figures out clever pathologies to defeat your rules in your systems and you don't get what you want, you get what you asked for.
And so what can we do to align our AI with our ethics. Well, I think nobody knows for sure I want to be clear about that but the best current idea is reinforcement learning from human feedback.
I actually think that this is very promising and I want to learn more about what when and where it fails. The short takeaway though is I think we'll end up raising ethical AI similarly to raising ethical children, which means we have to teach it what we know.
And we can't do that via communicating simple rules it's via complex shared knowledge, meaning a never ending stream of examples discussions debates, edge cases, leading to an ever refined ethical understanding on the behalf of the AI as well as you know that happening in humans.
So I'm cautiously optimistic that this will work, despite knowing that there will be many many challenges between here and there and it will never be perfect.
So I thought I want to end with this slide which is what keeps me up at night. I'm not so much. I'm not as worried about not being able to get our values into the AI that we make and that is made by good actors what really worries me is that what happens when AI and AGI is made by unscrupulous
actors or evil actors. How do we prevent that from happening. And I just want to remind everybody that was very hard today will be easy soon, like such as training large models. So I don't think that there are any good solutions here in terms of preventing evil actors from making evil AI, at least that
not that I've heard. So what is the least bad solution, and who decides which playbook we should follow to get there. And so I think this is one of the most important questions for humanity and for our field. Thank you.
I completely disagree and yet it's very engaging and interesting. I might give a few remarks later. Francesca is next and then we're going to have observations from Dave and Asian and me and then I think we'll move to the final session since we're pretty far behind.
And I promise to try to land in the plane by a 20 or something like that my local time we'll see what if we can actually succeed in that so we will go slightly over and I apologize.
Okay, so the question is whether we should embed values in AI systems. And my answer is yes. Yes does not mean that we should use just rules or program as Jeff was saying these very explicit values into a machine, but definitely not be done just by looking at
data or learning from data so to me, it has to need it needs human data in its rules so it needs a neuro symbolic approach for embedding these values and defining them.
Values may not be explicitly programmed into a machine, but at some point they need to explicitly be seen from outside. If you want to audit or if you want to an explanation or if you want to an knowing that the machine understands what he's doing and the values that he should follow.
So this embedding values is important for both.
I mean sounds obvious but it's important for both autonomous machines that will make decisions for us, because of course we don't want machines to make decisions with values that are in that are not aligned to our values, but also with machines that are going to help us make decisions,
because even in this case, even if we are the final decision makers, we want to interact with the machine in a way that we trust these recommendations that are made by the machine, and that we can interact with the machine in a way that makes us confident that following the machine or deviating from the
right way to do to make the decision. So when we talk about value alignment, of course there are many values that we want to embed and the elephant in the room is of course whose values, but fairness is one of the values.
This is a very important human values, we don't want to make discrimination with their decision. And so we have a lot of work being done already for classical AI models around fairness, way for detection, divergence, mitigation, and so on.
And also transparency and explainability that are two other dimensions of AI ethics topics are important, even if they're not directly values to embed in a machine, because they support the alignment without transparency without explainability, we may have value align
machines but we won't even know that they are value aligned. So the point is that we don't just want value aligned machines, we also want machines that can convince us that are value aligned.
So that's why explainability and transparency is very important. So this whole notion of trustworthy AI is plainable, transparent, fair, robust, whatever, that's why it has been put together with some topics that are seem to be related to element and others that seem to be unrelated.
And, and explainability. So, some, some lessons learned that in order to address this issue, you cannot just solve the problem with the additional technology solutions. So you need a lot of other pieces of the puzzle, guidelines, impact assessment, education, and many other things.
We cannot just state principles as many did, like few years ago.
We need full operationalization of this principle with all these pieces of a puzzle. And we need to go beyond just the tech companies that are putting together their own internal puzzle for the governance and the playbooks and education and everything else.
But we also need to go beyond the tech companies to build audit, certification, standard frameworks, as well as regulation.
We already talked about multi-disciplinarity and multi-stakeholder, like Eugene mentioned, that's really needed to be make sure that is inclusive. And we need these ethics to be, I will not call it ethical AI.
AI is not, in my view, ethical or not ethical is AI ethics, which is this field of study, which is very multi-disciplinar, multi-stakeholder, to make sure that the AI that we build has the right properties and is used in the right way.
The notion of trust is very important. Trust in the technology, trust in those that build the technology and deploy it, and in those that regulate the technology and among different institutions.
So now trust is really multi-dimensional and multi-level. So what I can say about trust is that, for example, if you go back to the picture of this fastest law thinking machine that I put earlier on in an earlier panel, you want the machine to make a decision, but who made the decision?
You want to know who have an explanation at the level of the solvers, but also to have an explanation of why that approach was used, why that solver was used overall by the machine.
So a two-level explanation, just like when I trust a human being, I don't want to know all the details of the individual algorithms to solve a specific problem, but I trust in some sense that is at the metacognitive level,
this human being will choose the best algorithm and the most appropriate algorithm for a certain decision.
Now trust has been studied a lot for classical AI problems, but I think that it needs a little bit more study for generative AI, because some dimensions of trust need to be translated a little bit, and there are other dimensions that were not considered in AI that is not generative.
So here I just cite two examples of uses of galactic and chat GPT, but we have seen many examples in the previous presentation of the panel, and here I just list some properties that we want in order to trust a piece of technology.
Of course, fairness is very important, privacy is still important, but it also leads as to do with information leaks.
Robustness, of course, robustness, but you want to be independent from the prompt syntax, and you want to see an injection function from the semantics of the prompt to the semantics of the content that is generated.
And fairness is something that is not present in a classical AI system, but now it's even more important and it raises the in generative AI. So we want both responses, the content that is generated, and the explanations that sometimes are given very eloquently given to be true.
And this is some obvious, but we have seen in many examples, this is not what happened, explainability, in order to explain in a way that is faithful to what was done in achieving a certain response, and also correct explanation.
Sometimes we have seen explanations that are not correct.
Transparency, transparency is raised especially in regulation setting, especially the EU AI Act, where there is a very heated debate about what is called general purpose AI or large language models are similar.
But what should be asked from these models when they are applied in any scenario or in high risk scenarios. And in particular, the emphasis from my point of view should be on transparency in order to whoever is going to build an application that is very high risk, according
to the definition of the AI Act. Starting from a large language model should have enough information about how that model has been built in order for this final provider of an application to comply with the obligation of the regulation.
To make sure that what is producing is an AI system that has the property of being trustworthy. So another topic that is the last one that I mentioned here is that there is this issue that syntax and semantic competence are currently
disaligned. Syntax in large language models are very good at syntactically, but they're not very good at semantically, while we tend to associate a syntactical competence with also a semantical competence.
So how do we achieve that alignment and how do we detect and mitigate possibly the disalignment. So, in general, the notion of trust that needs to be rethought and reconsidered when we put in the picture also generative AI, stop here.
Francesca has obviated the need for my own remarks because I agree with her on so much. We're going to have two quick observations from Dave and Yajin, and then we're going to move to our final panel.
Oh, me. I think there's a fascinating discussion. These have been great talks, really inspiring. I guess I have two points. One is around that I think transparency is critical.
I'm a big, big proponent, you know, even at the most fundamental mental level that AI system should be able to explain why they're making decisions they make and ground them in the fundamental assumptions.
I don't think it's, I don't think it avoids the need ultimately though to establish an agreed upon, you know, for the lack of a better word, that value system for making decisions and reason.
And so I don't think, so that has to be designed somewhere and has to be agreed upon and the reason is because you don't want to be in a situation where horrible decision is made, and it's okay just so that you can explain it in the end.
You want those decisions to conform to something that you collectively believe makes sense, then therefore that has to be, that has to be established ahead of time is the point I want to make.
And the other point I had was around trying to, like, determine or judge whether or not an intelligence is good, if it makes the decisions, what methodology does it use, in other words, do we just judge it based on the output,
or do we, you know, statistically based on the output, or do we judge it based on the methodology it's using the way it's reasoning the way it's thinking the values it's using in order to do that.
And I think that's a fascinating thing I think it's very hard to do. And the point I was raising is there's humans make so many bad decisions as it is.
I can explain themselves morally or logically, sometimes extremely costly to individuals or to the human race.
Do we take those standards and apply them broadly and not just to AI but also to human intelligence as well.
Yeah, it's somewhat of a rhetorical question. It's more of like making you think, as we work this out, and we stare at humans making decisions and we stare at AI making decision, and we say to the AI is, you need to think like this you need to behave like
this that we say to the humans, it's okay. Oh, we'll just hold you kind of at the end, maybe. Anyway, it's an interesting thought experiment.
Thanks Dave, you get the last word in this panel.
Yeah, so Dave really brings out great point about the challenges of injecting values and moral decision capabilities to AI when humans themselves are not all that moral sometimes and also we disagree on what is moral to begin with.
So my take here is that first of all, humans are able to know when people disagree. Sometimes we are also unsure we have a moral dilemma and we may not be able to be able to say for sure which is more morally correct we might think that it's all bad.
I think I should be able to do exactly that at least respecting and recognizing where humans might have uncertainties, and then not enforce a particular decision as moral superiority over human decision making I think I should really reflect and respect where humans disagree.
That's also where my emphasis on value pluralism comes along so we cannot really have a particular authority projected on everyone I think we should really reflect diverse different viewpoints, and then somehow design AI that is able to operate that way.
It's a challenging problem though.
Yeah, I mean, very quickly, I, you know, ideally you'd be able to tell me, here's the, here's the values to my use here my methods now here's a decision on a particular thing. Here's how I applied it like I want them for and the after.
Right. I mean, regardless of what's right like I want to know what I'm in for like just when I sit down with a human expert I want the same thing. I want to know what your system and now you make a decision now tell me how you apply it.
I'm in the position of moral conflict because I would like to continue this discussion but I also would like to treat well or our final speakers so I'm going to move on.
Eric bring Austin was for a long time at MIT. He's now at Stanford. He's in my view the economist to his thought most deeply about AI jobs and the impact of AI on society. It is my great pleasure to welcome Eric bring awesome.
Great. It is such a pleasure to be here. And let me just share my.
He's still there.
We lose Eric's connection.
No, I can see.
I can see his screen.
Just give it time.
I don't hear him anymore.
He's frozen for me. I think it is best to just give it.
He's reconnecting now I think.
We'll try again.
Yeah, sorry about that. It looks like the AIs don't want me to speak but I will I'll try and do it without sharing the slides because I think that's what caused it to crash. So thanks again for inviting me to be here.
It's great to have a chance to share some of the work I've been doing my colleagues at the Stanford digital economy lab.
Like those people gathered here today's philosophers and scientists have been fascinated by the idea of creating machines with human like intelligence for really thousands of years.
And it's a vocative goal that's inspired some tremendous progress and the progress the promise of human like AI is to increase productivity to enjoy more leisure and perhaps most most profoundly to improve our understanding of our own minds.
And so back in 1950 Alan Turing proposed what was we know they call it the imitation game back then, which was a test of whether or not machines were intelligent.
Now for the first time in history, this Turing test having robots imitate humans closely and function as close economic substitutes for humans is becoming increasingly technologically feasible, but not all AI mimics humans.
For example, there are very powerful computers that predict protein structures or predict the best words to say and customer support and do it in ways that no human ever could or would that kind of AI augments or extends human capabilities rather than imitating or replacing them.
And my research at MIT finds it augmenting or complimenting humans has far greater economic benefit than AI that merely substitutes or imitates human labor.
Unfortunately, I've also found that the incentives for three key groups, technologists, business leaders, and policymakers are each very misaligned and favor substituting human labor rather than augmenting it.
And if society that continues to pursue mainly the kind of AI that imitates human capabilities will end up in an economic trap I call it the Turing trap to be precise.
From an economic standpoint, this is problematic for two main reasons. First, while AI that create can create enormous benefits by being human like that is as a substitute.
It's also very limiting in terms of his potential for raising living standards and overall economic growth.
As the thought experiment. Let's imagine that dead list the mythical Greek inventor had developed robots as he was did by legend 3000 years ago, and his robots succeeded completely and automating each of the tasks that ancient Greeks were doing at that time.
So making clay pottery completely automated weaving tunics fully automated. If you're sick burning incense completely automated.
Now the good news is that human labor would no longer be needed. It would drop to zero everyone would enjoy lives of leisure and we'd have as many clay pots as that we would want.
But you can also see that that would not be a world with particularly high living standards it would not have by itself created 21st century living standards to raise the quality of life substantially.
We can't build machines that merely substitute for human labor and imitate it we must expand our capabilities and do new things.
The second problem with merely human like AI that substitutes for humans is that automation tends to worsen the economic and societal problem of increased concentration of economic and political power.
As that Andrew cast person said a little bit earlier in the last session, when a technology substitutes for human labor, it tends to drive down wages and shifts more of the income to the owners of capital.
That's because when a laborer can be replaced by a cheaper machine, that person loses economic bargaining power.
In contrast when a technology compliments human labor, it tends to increase wages and create more widely shared prosperity.
Now the good news is through most of history, most technological progress has been in the complimenting category, and that's why wages generally have written over the past couple centuries, there may be 50 times higher than it were two centuries ago.
Recently, however, the trend has reversed, and many groups below the media income in the United States are experiencing falling wages with increased deaths from despair, alcoholism, depression, drug abuse, as you may know, life expectancy is falling for those groups.
So we need to get serious about technologies that compliment humans, not only because of this kind of AI creates a bigger economic pie, but also because it creates a pie that will be distributed more equally.
Now, as I mentioned, three groups are not aligned on achieving this, and we can't expect market forces by themselves to even out this imbalance.
Technologists, business leaders and policymakers each have strong incentives to develop AI more as a substitute, rather than as a compliment.
And these systemic issues cannot be undone merely through intentional decision making.
So technologists understandably tend to focus on benchmarks that are often based on human capabilities for their goals.
Speaking as a professor myself, I know it's hard to think of a good research problem to work on for grad students or for my team.
And it's particularly difficult to imagine something that's never been done before.
It's far easier to look at something that humans are currently doing and assign students to work on that problem and do it with a machine.
It creates a disproportionate focus on technologies that are substitutes, rather than compliments.
Meanwhile, business leaders, I interact with them quite a bit as a business school professor.
If they replace labor with machines, they'll make money in part by driving down wages, as I mentioned earlier, and this shifts value from workers to capital owners.
Now that's not a net benefit to society, even if it's a benefit to them.
It's increased source of profits.
So as a result, they have excess incentives as well to develop and deploy AI that substitutes rather than compliments.
Finally, policymakers have instituted policies since 1986 or so that prioritize capital over labor.
Used to be that tax rates were equal for capital and labor.
Capital has about half the tax rate of labor and a business and employees that a lot of workers will pay higher taxes than a business and employees that produces the same output but with no or few workers.
Since the first rule of taxation is that you tend to get less of what you tax, our tax system is designed to favor substituting rather than complimenting technologies, simply not a level playing field.
Now, the last few years AI technology has become much more affordable and higher performing.
I'm very excited what I heard today and the foundation models like GPT three and coming GPT for Palm Dolly stable diffusion, etc. are promising to unleash new levels of creativity across the board.
But we are I believe at a critical decision point, a future in which we focus on the substitutes for human labor will be a limited one lacking an imagination innovation.
It puts us in a Turing trap.
As the agent suggested earlier today, instead of focusing on tasks that are easy for humans and hard for machines, we should work on tasks that are hard for humans and easy for machines.
Thankfully, the future is not preordained.
As AI tools become more powerful, we will have some new opportunities to change the world, but we'll only happen if we focus on superhuman tasks and tasks.
Humans are not currently doing that expand the capabilities of humans. That is my challenge for all of you. Thank you.
Thank you. That was a fantastic talk.
Kai Fu Lee is next. He's probably the best known person in China working on AI is pretty well known on our side of the Pacific.
He worked as an executive at Google and Apple and Microsoft and the US and then he moved to China, where he's running innovation ventures, which I think is quite successful.
He's also written to best sellers since becoming a venture capitalist, including AI superpowers China Silicon Valley and the New World Order.
He's often more optimistic than me when we have conversations but from what I can tell what he's going to talk about today is his last optimistic side.
He woke up in any case before dawn to share his thoughts please welcome Kai Fu Lee.
Thank you Gary, it's great to have this opportunity to talk in front of such a great audience.
So as Gary said, we want us to be a lively debate so I've decided to debate myself.
As many of you who may have read my previous writings I have been very optimistic.
And in my books, in particular AI 2041. I argue that yes there are a lot of AI issues the externalities but technologies will overcome issues like privacy their technologies like federated learning issues like bias we can build better tools.
With fake news we can build AI to detect these and also with problems like job displacement explain explain abilities. I've made the argument that historically human ingenuity has always overcome technological externalities for a new technology we just needed to give it time.
But today I feel something different because of the growth of AI GC. I understand this term is not commonly used in the academic communities more of an investment term, which is my day job.
AI GC generally stands for the same things many of you have talked about using the use of foundation models, GPT, etc.
And I think these are tremendous set of technologies I think I'm super excited about the opportunities. As we move from the, I would argue the earlier AI deep learning driven AI that's built upon optimizing objective functions towards one that's able to abstract.
Through an understanding by creating latent layers or a memory model. That's very suitable for neural net and using a tremendous amount of training data is generating phenomenal display of capabilities that certainly exhibit an unbelievably capable intelligent behavior.
And in terms of the positive consequences commercially speaking as a venture capitalist. I can see tremendous capabilities as AI can make fantastic text chat images video 3D.
This will clearly revolutionize content creation, which many people have talked about. And also, I think in my opinion, really give the entrepreneurial and also big tech companies an opportunity to recreate search engine advertising e-commerce short form video.
For example, the future of search engine won't be a bunch of text giving us websites, but ask a question and get one answer. The correct answer.
That's the dream and the division advertising will not be one size fit all, but specifically targeted. Imagine a text description generated specifically to entice people to believe in the brand or to buy the product.
Imagine when you go to Amazon, every one of us will not only see potential products we might buy, but they'll be described in a way that convinces us to buy and imagine the future of tiktok where we're not just seeing the most suitable pre created pre humanly created videos that will get us attracted and ties enchanted and addicted.
But these will in the future be created by these AI GC AI generated content and the power of the specific targeting in particular is extremely powerful.
So for, let's say you want to sell Tesla to someone who loves Elon Musk, someone who loves to green energy, someone who loves fast cars, someone who loves gadgets will each see a different ads specifically targeted for that person.
So this will clearly disrupt all of these industries. And as a venture capitalist, I can't be more excited about the opportunities. But today I want to mostly talk about the potential dangers that this will that this will create.
And in the past, I've looked at the various externalities of AI and I could either think of algorithms as a former AI researcher, or at least imagine the technological solutions to the earlier externalities people mentioned.
So I've remained optimistic. But now I am stuck because I can't easily imagine a simple solution for the AI generated misinformation as specific specifically targeted misinformation that's powerful commercially.
I think the first big problem I want to mention as my day job tells me is that there will be tremendous economic value as we disrupt all these industries I talked about, and it will be simply irresistible for commercial companies to resist that that temptation.
Imagine how much an Amazon, a Google or a Facebook can make if it can target each individual and mislead and potentially obviously there's a positive side right and ties provide relevant information teaches sky does a lot, but also to make money from showing us ads that will cause us to buy describing products that will cause us to buy
and give answers that could potentially be very good for the commercial company, because there is a fundamental misalignment of interest, because large companies want us to look at products look at content and click and watch and become addicted, and it is not necessarily within our interest to do that.
So that will really allow a next generation of products to be built, combined with targeting capabilities that will potentially be dangerous and be susceptible to simply the power of capitalism and greed.
That startup companies and VCs will fund activities that will generate tremendous wealth disrupt industries with technologies that are very hard to control.
Furthermore, large giants who have been at the primary target of a lot of the naysayers about misuse of their power will be even more powerful.
The technologies we talked about are controlled by the large giants who have the financial capabilities resources and compute power and brains to really train these giant foundation models that the rest of us will be forced to use, because we don't have access to that.
And I think the temptation of making more money, getting more minutes, creating ads that are very, very attractive on an individualistic basis and creating content that's not only enchanting but also misinforming and potentially even causing people to do things that they don't want to do to think
I think is incredibly dangerous. And the second big, big danger is that this will provide a set of technologies that will allow the non-state actors and the people who want to use AI for evil is easier than ever.
All of us understood how much danger technology like Cambridge Analytica created back a number of years ago.
But remember Cambridge Analytica was based on very simple AI and it was based on AI that really categorized people into five psychometric categories and targeted them according to the five categories.
Now this set of foundation models AI GC will allow targeting for people on an individual, an individualistic basis and create content that's either enticing attractive, potentially misleading, potentially exhilarating, but lead people to thoughts that might make the goals of non-state actors to disrupt elections.
But much more than that to mislead groups of people. And this is truly the beginning of the unfortunate opportunity for evil actors to create what I would call Cambridge Analytica steroids.
And fundamentally the biggest issue for the commercialism, the commercial opportunities, and for use of negative use will become so so big. And also it can lead, we have seen that these generated content can be can have misinformation, wrong content.
It will aim to make any argument that you like to achieve a goal and when done in a targeted individually, individualistic basis is very, very dangerous.
And Gary has written a paper in Scientific America that talks about the challenges and I agree with very much with that paper and a number of people have alluded to that.
And I want to appeal to this group, which is really working on AI from a technological or philosophical ethical sense that we are facing the largest danger. And as a community, we are already facing a public, at least American public that's more than 50% negative on the outcome of AI.
And this set of AI GC or foundation model technologies will definitely exacerbate that. I'm personally not very optimistic that rules and policies and laws will be fast enough to deal with this.
They haven't been with the first generation AI and they haven't been in the past when I worked at Microsoft, it took the DOJ, almost a decade to come up with what to do. And ultimately, the regulations were ineffective, and by the time they came up with ways to control the Microsoft monopoly.
Microsoft was no longer that powerful company. Of course, that's that's changed. But I think it really is incumbent on people who work on AI in the industry and academia to really think about what are the things that we can do as a community.
And we really need to have a call of action for people to work on technologies that will harness and help this incredible technology breakthrough to be more positive and more controlled and more contained in its potential commercial evil uses.
So my concern is that it's hard to imagine how the set of technologies can easily be controlled. And it's also seriously exacerbated by the incredible commercial temptation and non constructive intentions of other people.
So we are the people who really need to think about what are the consequences and first step is awareness. I think this debate is a good forum to raise this issue.
And for people to really think about what can we individually do and do as a community to help use this technology for the most positive intentions, but also to contain it so that it doesn't get out of hand.
And it's particularly difficult because it's hard to imagine technologies that will solve the problem, but I think something we really have to work on to make sure that AI achieves greater good rather than evil. Thank you.
Thank you very much for those very sobering remarks, I regret that I heard them near to my own bedtime because it will make it difficult to sleep, but they are exactly correct.
I really agree. Some of our chat here I think a lot of us agreed that misinformation really is the biggest threat and that whole cluster of things is quite serious.
Angela Sheffield is our final speaker she was the senior program manager for AI at the Office of Defense nuclear non proliferation in the US Department of Energy which is a big mouthful.
I'm the senior director for AI at raft, which is a new startup, and I have never made an introduction quite like what I'm about to do before.
I'm going to quote from her bio. She leads programs to support military operations and decision making in austere and contested environments, and all war fighting domains.
So I think that means we should not mess with her. Here's Angela Sheffield our final speaker here.
Mary and thanks, Vincent and thanks everyone for sticking around this long man it's tough to be the last couple minutes of talk, especially after Kai foods presentation, which I think all of us, you know found so sobering.
And it, you know I had four things I wanted to say and because of the tone that was set in the previous talk I'm going to move forward my fourth comment which is, I have the opportunity and my previous role and my current role to still talk quite frequently with
the decision makers in the United States government and in forums like this even with international governments. And I wanted to share with this forum that there is a sense of real sense of urgency recognition, maybe not of the specifics, especially at the technical
that Kai food presented but there's a real sense of urgency in among our senior decision makers and policymakers for some answers around how to lead and how to regulate a I and especially moving into more powerful a I like things like
and you know their brains get going and agi is next and we might we might know that agi isn't quite next, but kind of two things. First, um, that's it's not helpful for our senior decision makers to tell them agi is not here yet you know, they still need something
that helps them to legislate and helps them also to engage with senior decision makers in other countries to set kind of some direction on this, but that's even less important than then what Kai food had said previously which is we will figure this out.
And what Jeff said even a couple talks earlier, maybe, maybe faster, maybe faster, maybe not faster but we will figure it out and we probably won't be ready yet.
So the time is now to accelerate the discussions that we've accelerate the research that you're doing. I mean I know, like a better researcher you can't quite accelerate research but there's an urgency around providing more than just frameworks and how to think about being responsible with AI and
artificial intelligence and artificial general intelligence, more than just by more than just frameworks I mean applying this to a meaningful real world problem a very hard real world problem, setting the parameters of the model being specific about the values and then puts in criteria that we use and
we're moving forward with that and it may not be perfect but it is probably good enough for us to have something to throw darts at or maybe even it might become the thing, but the time is now to move from for the frameworks are a great
time but the time is now to move faster because of the motivation that we just heard from Kai food around misinformation, because if we, you know, we on this phone called Gary brought us together because he respects the way that we think and we all have.
There's a convergence of the way that we think AI can be and should be used here but we're not the only ones thinking about this, and there's some first mover advantage here. So, it's better, if not us.
If who then who you know there there's an answer to that and a certainly better us. So again the time is now to to be more specific than just, and I don't mean just as as to be this message but more specific than frameworks, more specific than concepts.
Nail this down provide some answers and begin to move forward because our senior decision makers need it now, and also more powerful methods are here if not coming.
So what that was what I was going to close with but it's kind of wanted to move that up given kind of the urgency we were presented with earlier. I wanted to share a couple other thoughts so I do work on AI and the national security and defense space.
Which I know many of us draw inspiration from science others from science fiction. I find there to be enough inspiration from the problems that we face in defense and national security to kind of get my brain going about what.
How did somebody put it earlier what you know what are the things we've never done before that we should pursue development and research on.
A lot of my work over the last 10 years has been on countering nuclear weapons proliferation so finding bad guys around the globe who are developing nuclear weapons and we found some opportunities where we could match AI algorithms with new data
processes to to enhance our ability to augment to use a phrase that that came earlier to augment the way that we approach finding bad guys developing nuclear weapons in a way that was strategically important for us in the United States and for others who want to reduce nuclear weapons
threats. One thing I wanted to share we've talked a lot about hybrid methods in this forum nuclear weapons development is a very process constrained and physics and chemistry per constrained activity.
So in that field we were able to constrain or find complimentary approaches to the machine and deep learning and other AI approaches we were using with physics and physics physical model.
So we've talked a lot about cognitive approaches haven't talked too much about physics and for machine learning.
But I consider that to be an important hybrid approach to making progress in useful artificial intelligence, and perhaps also one of the things that we could overlay as we work towards a GI as well.
But I wanted to share that as the defense community is getting more sophisticated about both our understanding of what you can do with technology moving from kind of normative to transformative and also being responsive to the Gary said austere and
environments the strategic that the stuff we expect to face kind of as a military as a US analysis partners military.
There's a shift in what we want to achieve with artificial intelligence from just better detection better data fusion better data analysis that was kind of the first wave of modern what we hope to achieve with AI to augmenting the way that we do command and control.
The main role is how a group, you know, how a group of people takes an information, make sense of it decides what to do and then execute that it is a distributed process.
And increasingly, this is how we who look at defense and national security, think that we can augment and enhance what we do whether that's in terms of improving the efficiency of our operations or producing manpower caught you know whatever.
We use distributed concepts where we're interacting with humans and data and computers and systems that are architecturally distributed and also geographically distributed.
And that's a bit of a change from the way that we have traditionally approached AI where you bring in all the data you probably compute you know and I think many of us when we think about a GI we're thinking about a centralized brain, because we're probably also thinking about a human, you know human concept of cognition and thinking, and I have no neuroscience in
my background, but as I draw inspiration from this modern problem of distributed command and control kind of our motivating North Star and defense and national security right now.
I think of something that looks more like an octopus than a human brain. And so I wonder what opportunities we have for achieving useful artificial intelligence and achieving something that has the effect of a GI.
And perhaps not I think to an earlier comment, not in the literal way that we approach it as humans, when we embrace some of these distributed concepts and think of something that that brain wise is more like an octopus than a centralized human brain.
And with that, I think those are all of my comments on policy, kind of the current pulse in defense and national security and the inspiration we can draw from there in terms of artificial intelligence and a GI, and we'll again leave with the motivation to
to keep pushing on how we achieve a GI and also how we make it what what we want it to be, you know trustworthy, robust, fair, and to do it with this urgency, because the time is now and it will be here faster than we can even expect.
I appreciate the calls of arms. We are now going to do our lightning round in lieu of discussion for this round because it is so so late, but very quickly, we will do this. People will give answers that I hope are around 30 seconds in length.
If you could give students one P undergrads grad students postdocs and so forth, one piece of advice on what AI, for example, what AI question most needs researching or how to prepare for world in which AI is increasingly central to our existence.
What would that advice be. I've advised the panelists to take that question as liberally as they like, as long as they are brief and we will start with Eric Bringson.
You are muted, I think.
Well, thanks, Karen, and thanks for bringing us all together. I've been blown away by what I've heard over the past couple of hours and and really just the past few weeks or past few months there's been such progress in what AI can do that it's quite inspiring but also alarming as we've heard a bit.
And that's really what my my advice is about is that we've seen this exponential or punctuated equilibrium in improvements in AI, but we have not seen an improvement in our economic institutions or skills or organizations.
And so that gap has gotten much larger. And the premise that the digital economy lab is based on is that we need to close that gap, but not by slowing down technology, but by speeding up our adaptation.
So my my plea is for more people to work on helping with understanding how we can reinvent our economics and our organizations, our institutions to keep up. And in particular, the thing that we've been focusing on lately is coming up with alternatives to some of the
metrics that we have currently like the Turing test and its generalizations that are often very focused on human capabilities, and thinking how can we design new metrics new benchmarks that are designed to encourage complementing augmenting extending what humans
can do rather than simply matching them. It's turned out to be quite a thorny kind of problem but we could use help from technologists from economists from people in all different areas on that problem and please contact me if you want to work on that.
Awesome Dave.
Oh, I would tell people AI will reveal you and challenge you your brain your job and your most fundamental beliefs will be challenged by AI like nothing ever before. Make sure you understand it, how it works, and where and where and how is being used.
Because AI becomes likely to become even more important the part of the human life so I think we need to really make AI human friendly and human centric.
So more immediately we got to deal with aligning AI with human values, especially with emphasis on pluralism I repeat this many times but I think that's one of the really critical challenges we are facing and more broadly addressing challenges such as robustness and generalization and
explainability and so forth.
Anya, are you still here? Yes, you are.
God only knows where I am.
So, I would say break free of silos. That's sort of my favorite mantra and understand and this goes to Angela's point and Angela I've been in the same field for many years you know it's AI enables a type of convergence of technologies that is simply unprecedented you know speaking about this militarily
commercially and where current systems and governance struggle to keep up with the share pace of development and also to adapt to the many different life cycles of a technology and I think that's a really important one that one application takes on many different forms depending on where it is and the governance needs to be adapted accordingly even for the same technology depending on where it is in this life cycle.
And this requires as a refiner skillful means invite diverse views and not least ask really good questions to navigate the ethical considerations and uncertainties.
Awesome, Dilip.
My advice to pick a path whether you want to be on the scaling one or on the fundamental research one because they have different trajectories and if you pick the scaling one industry is a very good place to be involved in because there are a lot of exciting things happen there maybe you want to start a company based on a
vertical application and if you start a company just know that being you know just tolerating the chaos and excitement is part of it and fear of missing out all of that is part of running a startup company and get used to that.
And if it is on the research side I would say bringing reasoning into every component like you know all the way down to perception like you know how can you connect reasoning all the way down to the pixel level in perception.
I'm solved the objectness problem and binding problem those are exciting research topics to work on, I think.
Fantastic or tour.
Yeah, thank you all thanks Gary yeah students are overwhelmed by an explosion of papers in the area and the difficulty to keep up with the literature and I would say it's very important to meet and talk to people and specially post pandemic sense of belonging to a research community that
may be more specialized.
In terms of the research going forward, I mentioned constraining we need to constrain deep learning with knowledge and explain what has been learned.
And computability was mentioned earlier, as indeed a very important result but that's not sufficient we need to have computational models that will realize such results in practice.
So, to summarize I think we need AI that understands humans, and humans that understand the AI, this will have a bearing on democracy on accountability, and ultimately education we will need AI at schools.
That's awesome. Fantastic.
Sure, two ideas. One is that really echoing Eric's thoughts that we should work on technology technologies that are suitable for AI that can do things that humans cannot do or cannot do well.
And that's the biggest way to make an impact human brains and current machine learning brains are different. So focusing on the strengths of the machine learning mechanisms and pick problems that can really solve problems that we humans are not so suitable to do.
The second is, don't just go for the bigger faster better algorithm but think about the potential externalities and the guard rails that might be needed, and to come up with not just the electricity, but also the circuit breaker to make sure that the technology is safe and usable.
Angela. Thank you very much.
Thanks Gary. Advice I would give to people who are students who are considering pursuing artificial intelligence is especially if you're drawn into artificial intelligence because you're motivated to make to solve problems for humans or to create new capabilities that that make the human
experience better. I would recommend considering specializing in a domain application I think there's so much richness in both AI research and development and our contribution to the human society and the application of AI to a specific set of problems or a specific domain.
I also think it's a really awesome and powerful way to go kind of very end to end from the AI science and technology to understanding what that means with the human users to interacting with multidisciplinary.
You know, I think that's another really interesting way to pursue a career and to make big contributions meaningful contributions to the field and to humanity. So I would recommend to specialize into a domain application area.
Terrific Jeff and then Francesca will have our last word.
So I would say that I think AGI is coming soon for the so for researchers developing AI the most impactful thing you probably can do is to try to make it go very well for humanity.
That means working on AI ethics and I safety, but in that area is in all AI research areas. My prediction is that you will be tempted in the short run to go with manual path solutions.
But I think the history will ultimately prove those to be dead ends. So I would encourage you to work on techniques based on AI generating algorithms, meta learning RLHF and and learning, etc, as those are most likely to be absorbed into the AGI that we're creating.
And for non AI developing researchers, I think there's a lot of important research outside of computer science. The most important are probably governance questions.
So what is the playbook for when AGI gets developed, who gets to decide that playbook, how do those people get chosen, and probably the most important, how do we get everyone else on the planet to agree with that game plan.
Thank you, Jeff. Francesca will finish our lightning round. I will have a brief remark. Vince will have a brief remark and then finally a little bit over time, a lot of our time.
We will call it tonight. Francesca.
Thanks, Gary. So first of all, I will remind, I would remind students that AI at this point is not just the science and the technology is a socio technical discipline.
It means that it is so pervasive is impacting our lives more and more every minute and shaping our future. And this is should always be in the mind of the students because, yes, it's important to be enthusiastic about the discoveries, the scientific advancement, and also what can bring
the study of AI can bring to the understanding of human mind and human intelligence, but always they need to consider and be driven by social considerations of AI's possible impact on people and society.
So to address those issues, be open to all the techniques, not just machine learning, not just symbolic, not just this, to be open to this, all the techniques and possibly new ones, and be open to all stakeholders.
So let's start by talking with the students next door in your, in your, in your dorm, and they start studying some social science, maybe, and discuss about the impact of AI with people that are not in your discipline.
So that together you can evaluate the impact of AI or what you're working on, and we'll be able to drive this technology towards the future where
the scientific progress is supportive of human progress and human values rather than the other way around.
I hope lots of graduate classes going forward in AI will watch tonight's event. I think somebody said a few minutes ago that it was mind blowing, and I agree.
Vince said that he wanted this year to be better than the last time I said forget about it because it was so good, but I think we might just have pulled that off.
In the last debate I said it takes a village to raise an AI. I think that feels even more true now. If AI was a child before now it's kind of like a rambunctious teenager, not yet completely in possession of mature judgment.
So we all need to help. I think the moment is both exciting and perilous. I think a lot of people have pointed out the perils in the last hour or so.
I want to thank everyone on the panel. Everyone who gave up a lot of sleep to be here and stuck out to the bitter end.
I want to thank you for sharing your wisdom and being gracious about sharing the stage with so many other people. I know it's hard to sit here for four hours and only get like 10 minutes in total.
And I want to thank you in the audience. I wrote out for taking three hours of your holidays, but really it was almost four. So thank you all in the audience for saying so long.
And most of all, I want to thank Vince for launching this series in 2019 with me and Yashua Benjio, and co-hosting it graciously and intrepidly with me ever since. Thank you so much, Vince.
And you get the last words.
Thank you, Gary.
Estimate participants, ladies and gentlemen.
We have just concluded a highly impactful AGI debate.
My most sincere thanks to all speakers, and a very special thanks to our moderator and co-organizer, Gary Marcus.
AGI debate come to a close. It is clear that the future of artificial intelligence holds both great promise and great uncertainty.
The potential for AGI to transform our world and unlock new levels of human potential is much had only by the need for careful consideration of ethical and moral implications of such powerful technology.
To move forward, let us remember the lesson from the past and strive to create intelligent systems that are not only powerful and capable, but also ethical, robust, and trustworthy.
Only then can we hope to unlock the full potential of artificial general intelligence and pave the way for a brighter and more prosperous future.
That is the conclusion of the AGI debate.
Thank you, Al, for joining us.
The conversation will continue on social media with the hashtag AGI debate. Thank you.
Good night, everyone.
Bye.
This is the end of the live stream, and we will stay together in Zoom, if at your convenience.
I will rejoin in a minute. Thank you, everybody. I'll take a very brief break.
Thank you. Bye-bye, everybody.
Thank you. Good night.
Good night, everyone.
Good night, everyone. Bye-bye.
Some coffee.
Good morning, everyone.
