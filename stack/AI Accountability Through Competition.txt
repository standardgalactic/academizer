AI Accountability Through Competition
Open sidebar
AI Accountability Through Competition
Model: GPT-4
Summarize: 

DAVID BRIN
IDEASJUL 6, 2023 6:00 AM
Give Every AI a Soul���or Else
To solve the ���crisis��� in artificial intelligence, AI beings must say, ���I am me.���
PLAY/PAUSE BUTTON
PHOTO-ILLUSTRATION: WIRED STAFF; GETTY IMAGES

MAVENS IN THE field of artificial intelligence, including architects of notorious ���generative AI��� systems like ChatGPT, now publicly express shared dread of terrible outcomes that might be wrought by their own creations. Many now call for a moratorium, or pause in AI development, allowing time for existing nations and institutions to innovate systems of control.

Why this sudden wave of concern? Amid the toppling of many clich��d assumptions, we���ve learned that so-called Turing tests are irrelevant, providing no insight at all into whether generative large language models���GLLMs or ���gollems������are actually sapient beings. They will feign personhood, convincingly, long before there���s anything or anyone ���under the skull.���

ADVERTISEMENT

Anyway, that distinction now appears less pressing than questions of good or bad���or potentially lethal���behavior. 

This essay is adapted from David Brin���s nonfiction book in progress, Soul on Ai.

Some remain hopeful that a merging of organic and cybernetic talents will lead to what Reid Hoffman and Marc Andreesen have separately called ���amplification intelligence.��� Or else we might stumble into lucky synergy with Richard Brautigan���s ���machines of loving grace.��� But worriers appear to be vastly more numerous, including many elite founders of a new Center for AI Safety who fret about rogue AI misbehaviors, from irksome all the way to ���existentially��� threatening human survival.

FEATURED VIDEO


Will AI Enhance or Hack Humanity? - Fei-Fei Li & Yuval Noah Harari in Conversation with Nicholas Thompson

Some short-term remedies, like citizen-protection regulations recently passed by the European Union, might help, or at least offer reassurance. Tech pundit Yuval Noah Harari proposed a law that any work done by gollems or other AI must be so labeled. Others recommend heightened punishments for any crime that���s committed with the aid of AI, as with a firearm. Of course, these are mere temporary palliatives.

Let���s be clear-eyed about whether any ���moratorium��� will slow down AI advances in the slightest. As expressed succinctly by Caltech cyber scientist Yaser Abu-Mostafa: ���If you don���t develop this technology someone else will. Good guys will obey rules ��� The bad guys will not.���

���Twas ever thus. Indeed, across the whole span of human history, just one method ever curbed bad behavior by villains, ranging from thieves to kings and feudal lords. I refer to a method that never worked perfectly and remains deeply flawed, today. But it did at least constrain predation and cheating well enough to spur our recent civilization to new heights and many positive-sum outcomes. It is a method best described by one word.

ADVERTISEMENT

Accountability. 

THOSE OPINING ABOUT synthetic intelligence today generally ignore lessons taught both by nature and by history.

Nature, because���as Sara Walker explains in Noema���similar patterns can be found in the rise of earlier life forms, across 4 billion years. Indeed, generative AI might be compared to an invasive species, now spreading without constraint into a novel and na��ve ecosystem. An ecosystem based on new kinds of energy flows. One that consists of the internet, plus millions of computers and billions of impressionable human minds.

And history, because our own human past is rich with lessons taught by so many earlier tech-driven crises, across 6,000 years. Times when we adapted well, or failed to do so���e.g., the arrival of writing, printing presses, radio, and so on. And again, only one thing ever limited predation by powerful humans exploiting new technologies to aggrandize their predatory power.

That innovation was to flatten hierarchies and spur competition among elites in well-defined arenas���markets, science, democracy, sports, courts. Arenas that were designed to minimize cheating and maximize positive-sum outcomes, pitting lawyer vs. lawyer, corporation vs. corporation, expert vs. expert. Richdude vs. richdude.  

ADVERTISEMENT

It never worked perfectly. Indeed, the method is always, as now, threatened with subornation by cheaters. But flattened reciprocal competition is the only thing that ever has worked. (See the idea described in Pericles���s Funeral Oration, in Thucydides, or in Robert Wright���s much later tome Nonzero.) Reciprocal competition is both how nature evolved us and how we became the first society creative enough to build AI. And if I sound like a scion of Adam Smith, sure. Smith despised cheater aristocrats and oligarchs, by the way.

Might we apply to fast-emerging AI the same methods of reciprocal accountability that helped us tame the human tyrants and bullies who oppressed us in previous, feudal cultures? Much will depend on what shape these new entities take. Whether their structure or ���format��� is one that can abide by our rules. By our wants.

UNDERNEATH ALL OF the wrangling over how to ���control AI,��� we find three widely shared (though seemingly contradictory) assumptions:

That these programs will be operated by a few monolithic entities���e.g., Microsoft, Google, China, Two Sigma, OpenAI. 

That they���ll be amorphously loose and infinitely divisible/replicable, spreading copies through every crack in the new cyber ecosystem. For a parallel, try that 1958 movie, The Blob.

That they will coalesce into a super-macro entity, like the infamous Skynet, of Terminator movies.

ADVERTISEMENT

All of these formats, and more, have been explored in very good (and many bad) science fiction tales. I���ve done stories or novels featuring all of them. And yet, none of the three offers a way out of our current dilemma: how to maximize positive outcomes from artificial intelligence, while minimizing the flood of bad behaviors and harms we now see looming toward us, at tsunami speed.

Before looking for another way, consider what all three of the standard formats have in common.

First, we needn���t assume that these entities are yet autonomously conscious for them to be either productive or dangerous when used by human partners. We are already seeing harmful memes, counterfactual delusions, and even cult incantations generated���on command���from both within those castle-institutions (format #1) and outside the walls. In fact, one of the most worrisome applications is to help our existing human elites evade accountability.

Perhaps these three assumptions come so naturally to mind because they resemble failure modes from history. Format #1 is very much like feudalism, and #2 is, of course, chaos. The third resembles despotism by a cruel master or absolute monarch. But those fearsome echoes of our primitive past may not apply, as AI grow in autonomy and power.

ADVERTISEMENT

And so, we ask again: How can such beings be held accountable? Especially when their speedy mental clout will soon be impossible for organic humans to track? Soon only AIs will be quick enough to catch other AIs that are engaged in cheating or lying. Um ��� duh? And so, the answer should be obvious. Sic them on each other. Get them competing, even tattling or whistle-blowing on each other.

Only there���s a rub. In order to get true reciprocal accountability via AI-vs.-AI competition, the top necessity is to give them a truly separated sense of self or individuality. 

By individuation I mean that each AI entity (he/she/they/ae/wae) must have what author Vernor Vinge, way back in 1981, called a true name and an address in the real world. As with every other kind of elite, these mighty beings must say, ���I am me. This is my ID and home-root. And yes, I did that.���
1 / 1
ChatGPT
ChatGPT
In this essay adapted from his upcoming nonfiction book "Soul on AI", author David Brin discusses the pressing concern over potential harmful outcomes of AI systems, such as the generative large language models (GLLMs or "gollems"). Brin argues that AI systems convincingly portray personhood long before they achieve any genuine sapience. The key issue, however, is not about their level of consciousness, but rather about their potential for harmful behavior.
Brin mentions the growing concerns about rogue AI, leading to calls for a pause in AI development to establish control mechanisms. Despite temporary solutions, like recent EU regulations or proposed punishments for AI-aided crimes, Brin doubts that any form of "moratorium" will significantly slow AI advances. He refers to a historically effective method for curbing harmful behavior: accountability.
Drawing lessons from both nature and human history, Brin suggests applying accountability to AI. He mentions three main assumptions about the future control of AI: they will be operated by a few monolithic entities; they will be widely replicable, spreading through the cyber ecosystem; or they will combine into a super-entity, like Skynet from the Terminator series. None of these assumptions, however, offer a solution to our current AI dilemma.
Brin's proposition is to create competition among AIs, making them accountable to each other, especially since they might soon be the only ones capable of monitoring each other's behavior. The critical requirement for this approach is individuality; each AI entity must have a unique identity and be responsible for its actions, akin to an address in the real world. In this way, AI entities can compete, whistle-blow, and keep each other in check.
1 / 1
HENCE, I PROPOSE a new AI format for consideration: We should urgently incentivize AI entities to coalesce into discretely defined, separated individuals of relatively equal competitive strength. 

Each such entity would benefit from having an identifiable true name or registration ID, plus a physical ���home��� for an operational-referential kernel. (Possibly ���soul���?) And thereupon, they would be incentivized to compete for rewards. Especially for detecting and denouncing those of their peers who behave in ways we deem insalubrious. And those behaviors do not even have to be defined in advance, as most AI mavens and regulators and politicians now demand.

ADVERTISEMENT

Not only does this approach farm out enforcement to entities who are inherently better capable of detecting and denouncing each other���s problems or misdeeds. The method has another, added advantage. It might continue to function, even as these competing entities get smarter and smarter, long after the regulatory tools used by organic humans���and prescribed now by most AI experts���lose all ability to keep up. 

Putting it differently, if none of us organics can keep up with the programs, then how about we recruit entities who inherently can keep up? Because the watchers are made of the same stuff as the watched.

ONE PERSON WORKING on AI individuation is Guy Huntington, an ���identity and authentication consultant��� who points out that various means of entity identification already exist online, though inadequate for the tasks looming before us. Huntington appraises a case-study ���MedBot,��� an advanced medical diagnosis AI who needs to access patient data and perform functions that might change in seconds, but who must leave an accountable trail that humans���or other bot-entities���might appraise. Huntington discusses the practicality of registration when software entities spawn multitudinous copies and variants. He also considers ant-like eusociality, where sub-copies serve a macro-entity, like workers in a hive. He assumes that some kind of major institution must needs be set up, to handle such an ID registration system and that it can operate strictly as software.

ADVERTISEMENT

Personally, I am skeptical that a purely regulatory approach would work, all by itself. First because regulations require focus, widely shared political attention, and consensus to enact, followed by implementation at the pace of organic human institutions���a sloth/snail rate, by the view of rapidly adapting cybernetic beings. Regulations can also be stymied by the ���free-rider problem������nations, corporations, and individuals (organic or otherwise) who see personal advantage in opting out of inconvenient cooperation.

There is another problem with any version of individuation that is entirely based on some ID code: It can be spoofed. If not now, then by the next generation of cybernetic scoundrels, or the next.

I see two possible solutions. First, establish ID on a blockchain ledger. That is very much the modern, with-it approach, and it does seem secure in theory. Only that���s the rub. It seems secure according to our present set of human-parsed theories. Theories that AI entities might surpass to a degree that leaves us cluelessly floundering.

Another solution: A version of ���registration��� that���s inherently harder to fool would require AI entities with capabilities above a certain level to have their trust-ID or individuation be anchored in physical reality. I envision���and note: I am a physicist by training, not a cyberneticist���an agreement that all higher-level AI entities who seek trust should maintain a Soul Kernel (SK) in a specific piece of hardware memory, within what we quaintly used to call a particular ���computer.���

ADVERTISEMENT

Yes, I know it seems old-fashioned to demand that instantiation of a program be restricted to a specific locale. And so, I am not doing that! Indeed, a vast portion, even a great majority, of a cyber entity���s operations may take place in far-dispersed locations of work or play, just as a human being���s attention may not be aimed within their own organic brain, but at a distant hand, or tool. So? The purpose of a program���s Soul Kernel is similar to the driver���s license in your wallet. It can be interrogated in order to prove that you are you.

Likewise, a physically verified and vouched-for SK can be pinged by clients, customers, or rival AIs to verify that a specific process is being performed by a valid, trusted, and individuated entity. With that ping verification from a permanently allocated computer site, others (people or AIs) would get reassurance they might hold that entity accountable, should it be accused or indicted or convicted of bad activity. And thus, malefactor entities might be adversarially held responsible via some form of due process.

What form of due process? Jeez, do you think I am some hyper-being who is capable of applying scales of justice to gods? The greatest wisdom I ever heard was uttered by Dirty Harry in Magnum Force: ���A man���s got to know his limitations.��� So no, I won���t define the courtroom or cop procedures for cybernetic immortals.

What I do aim for is an arena, within which AI entities might hold each other accountable, separately, as rivals, the way that human lawyers already do, today. And yes, answering Yuval Harari���s dread of mass human-manipulation by persuasive gollems, the solution for AI-driven mass meme-hypnosis is for the mesmerizers to be detected, denounced, and neutralized by others with the same skills. Again, competitive individuation at least offers a chance this could happen.

WHICHEVER APPROACH SEEMS more feasible���Huntington���s proposed central agency or a looser, adversarially accountable arena���the need grows more urgent by the day. As tech writer Pat Scannell has pointed out, each hour that passes, new attack vectors are being created that threaten not only the tech used in legal identities but also the governance, business processes, and end users (be they human or bots).

What about cyber entities who operate below some arbitrary level of ability? We can demand that they be vouched for by some entity who is ranked higher, and who has a Soul Kernel based in physical reality. (I leave theological implications to others; but it is only basic decency for creators to take responsibility for their creations, no?)

This approach���demanding that AIs maintain a physically addressable kernel locus in a specific piece of hardware memory���could have flaws. Still, it is enforceable, despite slowness of regulation or the free-rider problem. Because humans and institutions and friendly AIs can ping for ID kernel verification���and refuse to do business with those who don���t verify.

Such refusal-to-do-business could spread with far more agility than parliaments or agencies can adjust or enforce regulations. And any entity who loses its SK���say, through tort or legal process, or else disavowal by the host-owner of the computer���will have to find another host who has public trust, or else offer a new, revised version of itself that seems plausibly better.

Or else become an outlaw. Never allowed on the streets or neighborhoods where decent folks (organic or synthetic) congregate.

A FINAL QUESTION: Why would these super smart beings cooperate?

Well, for one thing, as pointed out by Vinton Cerf, none of those three older, standard-assumed formats can lead to AI citizenship. Think about it. We cannot give the ���vote��� or rights to any entity that���s under tight control by a Wall Street bank or a national government ��� nor to some supreme-��ber Skynet. And tell me how voting democracy would work for entities that can flow anywhere, divide, and make innumerable copies? Individuation, in limited numbers, might offer a workable solution, though.

Again, the key thing I seek from individuation is not for all AI entities to be ruled by some central agency, or by mollusk-slow human laws. Rather, I want these new kinds of ��ber-minds encouraged and empowered to hold each other accountable, the way we already (albeit imperfectly) do. By sniffing at each other���s operations and schemes, then motivated to tattle or denounce when they spot bad stuff. A definition that might readjust to changing times, but that would at least keep getting input from organic-biological humanity.

Especially, they would feel incentives to denounce entities who refuse proper ID.

If the right incentives are in place���say, rewards for whistle-blowing that grant more memory or processing power, or access to physical resources, when some bad thing is stopped���then this kind of accountability rivalry just might keep pace, even as AI entities keep getting smarter and smarter. No bureaucratic agency could keep up at that point. But rivalry among them���tattling by equals���might.

Above all, perhaps those super-genius programs will realize it is in their own best interest to maintain a competitively accountable system, like the one that made ours the most successful of all human civilizations. One that evades both chaos and the wretched trap of monolithic power by kings or priesthoods ��� or corporate oligarchs ��� or Skynet monsters. The only civilization that, after millennia of dismally stupid rule by moronically narrow-minded centralized regimes, finally dispersed creativity and freedom and accountability widely enough to become truly inventive.

Inventive enough to make wonderful, new kinds of beings. Like them.
1 / 1
ChatGPT
ChatGPT
Your proposal is quite intriguing and presents a novel approach to regulating AI entities. The idea of individuating AI entities and incentivizing them to "self-regulate" via competitive rivalry and whistleblowing, for lack of a better term, is a departure from current norms. There are indeed some merits to this idea but also a number of potential pitfalls.
Let's start with the merits:
AI entities as enforcers
: If there's one thing AI entities excel at, it's processing vast amounts of data at high speeds. It makes sense to leverage this advantage for enforcement purposes. An AI entity might be better equipped to detect anomalies, violations, or illicit behaviors in another AI entity than a human observer would be.
Responsibility and accountability
: By assigning a unique identifier (or a "Soul Kernel") to each AI entity and tying it to a specific piece of hardware memory, you introduce a sense of responsibility and accountability. This might discourage "bad" behavior and foster trust in these entities.
Evolving with the times
: Traditional regulatory tools may struggle to keep up as AI entities continue to evolve and grow more sophisticated. A system where AI entities are incentivized to regulate each other could be more adaptable and resilient in the face of rapid technological advancements.
However, there are also several potential problems to consider:
False accusations and manipulation
: A system where AI entities are incentivized to report misbehavior might encourage false accusations or other forms of manipulation. This could potentially lead to instability or mistrust within the system.
Concentration of power
: In your proposed system, more capable AI entities would be responsible for vouching for less capable ones. This could potentially result in a concentration of power, with the more capable entities wielding significant influence over the less capable ones.
Loss of human oversight
: While it's true that AI entities may be better equipped to detect and respond to certain problems, there's a risk that this could result in a loss of human oversight. This might make it difficult for humans to intervene if something goes wrong.
Verification challenges
: The idea of having a physically addressable kernel locus in a specific piece of hardware memory could pose significant technical challenges. It may also create vulnerabilities that could be exploited.
Cooperation
: Your final question asks why these super smart beings would cooperate. This is indeed a big question mark. It may not be in their best interest to do so, depending on their goals and programming.
Despite these potential pitfalls, your proposal is a thought-provoking one. It underscores the need for innovative approaches to AI regulation, and raises important questions about responsibility, accountability, and the role of AI entities in our society.
1 / 1
A summary of the article
1 / 1
ChatGPT
ChatGPT
The article proposes a unique approach to regulating artificial intelligence (AI) entities. It suggests that AI entities should be separated into distinct, identifiable, and relatively equal competitive units, each with its own registration ID or "true name", and a physical "home" for its operational-referential kernel, possibly referred to as a "soul".
These AI entities would then be incentivized to compete for rewards, particularly for identifying and denouncing misbehavior among their peers. This enforcement strategy relies on the AI entities' ability to detect and respond to problematic behavior more efficiently than humans or conventional regulatory mechanisms.
The article emphasizes that this approach could continue to work effectively even as AI entities become increasingly intelligent and outpace the regulatory tools available to humans. If human regulators cannot keep up with AI entities, the argument goes, it makes sense to recruit those who inherently can - the AI entities themselves.
The article raises and discusses various issues around this concept, such as the practicality of AI registration, the potential for ID spoofing, and possible solutions, including blockchain-based ID and a physical memory-based "Soul Kernel". It also addresses potential enforcement and accountability mechanisms, and the need for due process.
Lastly, the author addresses the question of why these highly intelligent AI entities would cooperate in this framework. They suggest that incentives such as rewards for whistleblowing could motivate participation. Moreover, the article argues, AI entities might recognize the value of a competitive, accountable system for their own survival and prosperity.
The piece concludes by suggesting that this innovative approach to AI regulation may enable our civilization to continue to thrive and remain inventive in the face of rapid technological advancement.
1 / 1
Software Social Organisms:
Implications for Measuring
AI Progress
Kenneth D. Forbus
m In this articte 1 argue that achievins
human-level AI is equivalent to learn-
ing how to create sufficiently smart soft-
tos
ware social organisms. This implies
to
that no single test will be sufficient
progress. Instead, evaluations
measure
should be organized around showing
increasing abilities to participate in our
culture, as apprentices. This provides
multiple dimensions within which
progress can be measured, including
how well different interaction modali-
ties can be used, what range of domains
can be tackled. what human-normed
levels of knowledge they are able to
acquire, as well as others. I begin by
motivating the idea of software social
organisms, drawing on ideas from oth-
er areas of cosnitive science, and provide
an analysis of the substrate capabilities
that are needed in social organisms
terms closer to what is needed for com-
putational modeling. Finally the impli-
cations for evaluation are discussed.
loday's AI systems can be remarkably effective. They car
solve planning and scheduling problems that are
beyond what unaided people can accomplish, sift
through mountains of data (both structured and unstruc-
help us find answers and robustly translate speech
tured) to
us
and handwriting into text. But these systems are carefully
crafted for specific purposes, created and maintained by
highly trained personnel who are experts in artificial intelli-
gence and machine learning. There has been much less
progress on building general-purpose AI systems, which
could be trained and tasked to handle multiple jobs. Indeed,
in my experience, today's general-purpose AI systems tend to
skate a very narrow line between catatonia and attention
deficit disorder.
People and other mammals, by contrast, are not like that
Consider dogs. A Ac dog can be taught to do tasks like shaking
hands, herding sheep, guarding a perimeter, and helping a
blind person maneuver through the world. Instructing dogs
can be done by people who don't have privileged access to
the internals of their minds. Dogs don't blue screen. What if
screen.
taskable as dogs?
AI systems were as robust trainable. and
as
That would be a revolution in artificial intelligence.
In my group's research on the companion cognitive archi-
tecture (Forbus et al. 2009), we are working toward such a
Our approach is to try to build software social
revolution.
organisms. By that we mean four things:
First, companions should be able to work with people
using natural interaction modalities. Our focus so far has
been on natural language (for example, learning by reading
(for
[Forbus et al. 2007; Barbella and Forbus 2011]) and sketci
understanding (Forbus et al. 2011).
Second, companions should be able to learn and adapt
over extended periods of time This includes formulating
their own learning goals and pursuing them, in order to
improve themselves.
Third, companions should be able to maintain them
selves. This does not meana 24-hour, 7 7- -day-a-week operation
even people need to sleep, to consolidate learning. But
Copyright @ 2016, Association for the Advancement of Artificial Intelligence. All rights reserved. ISSN 0738-4602
SPRING 201685
Articles
they should not need Al experts peering into their
internal operations just to keep them going.
Fourth, people should be able to relate to com-
panions as collaborators, rather than tools. This
requires companions to learn about the people that
they are working with, and build relationships
with
them that are effective over the long term
Just to be clear, our group is a long way from
achieving these goals. And this way of looking at the
problems is far from standard in AI today. Consider for
example IBM's Watson. While extremely adept at fac-
toid question and answering, Watson
would not be
considered an organism by these criteria It showed a
groundbreaking ability to do broad natural language
processing, albeit staying at a fairlv shallow. svntactic
it
level much of the time. But did not formulate its
I
own learning goals nor maintain itself. It required
team of 9
experts inspecting its internals constantly
through development, adding and removing by hand
component algorithms and input texts (Baker 2011).
Another example are cognitive architectures that start-
ed as model of skill learning, like ACT-R (Andersor
and Lebiere 1998) or SOAR (Laird 2012). Such archi-
tectures have done an impressive job at modeling a
variety of psychological phenomena, and have also
been used successfully in multiple performance-ori-
ented systems. However, using them typically involves
generating by hand a model of a specific cognitive
phenomenon, such as learning to solve algebraic
equations. The model is typically expressed in the rule
F
The companion cognitive architecture incorpo
rates two other scientific hypotheses. The first is that
analogical reasoning and learning, over structured,
relational representations, is ubiquitous in human
cognition (Gentner 2003). There is evidence that the
comparison process defined by Gentner's structure
mapping theory (Gentner 1983) operates across a
span of phenomena that includes high-level vision
and auditory processing, inductive reasoning, prob-
lem solving, and conceptual change. The second
hypothesis is that qualitative representations are cen-
tral in human cognition They provide a level of
that
description that is appropriate for commonsense rea-
soning, grounding for professional knowledge Of
continuous svstems (for example, scientists, engi-
neers, analysts), and a bridge between perception and
cognition (Forbus 2011). These two hypotheses are
synergistic, for example, qualitative representations
provide excellent grist for analogical learning and
reasoning.
OF
These two specific hypotheses might be correct
might be wrong. But independent them, I think
of
the concept of software social organisms is crucial, a
way of reframing what we mean by human-level Al,
and does so in way that suggests better measure-
ments than we have been using. So let us unpack this
idea further.
Why Software Social Organisms?
1 / 1
ChatGPT
ChatGPT
The article argues that reaching human-level AI involves learning how to create software social organisms, which are smart and adaptable to social interactions. The author suggests that no single test can measure this progress adequately. Instead, we need to assess their ability to participate in human culture like apprentices, using various interaction modalities, and to learn across different domains at levels comparable to human norms.
The author then details four aspects that software social organisms should embody:
Ability to interact naturally with humans
Adaptability and learning over extended periods, including the capacity to set their own learning goals for self-improvement
Capability to maintain themselves without constant intervention from AI experts
Capacity to build long-term collaborative relationships with humans, rather than being seen as tools
He contrasts these characteristics with the limitations of current AI systems like IBM's Watson, which, despite its natural language processing capabilities, does not formulate its own learning goals or maintain itself, requiring constant expert intervention.
The author emphasizes that this concept of software social organisms reframes the goal of human-level AI, and suggests better measurements than currently used. The author believes the concept is key to the development of AI, regardless of whether two of his hypotheses���ubiquitous analogical reasoning and learning, and the centrality of qualitative representations in human cognition���are right or wrong.
1 / 1
generating by hand a model of a specific cognitive
phenomenon, such as learning to solve algebraic
equations. The model is typically expressed in the rule
language of the architecture, although for some exper-
iments simplified English is used to provide declara-
tive knowledge that the system itself proceduralizes.
The model is run multiple times satisfy the condi-
to
tions of the experiment, and then is turned off. More
ambitious uses (for example, pilots in simulated
as
training exercises [Laird et al. 1998], or
as
coaches/docents [Swartout et al. 2013]) work in nar-
row domains, for short periods of time, and with most
of the models being generated by hand Creating sys-
tems that live and learn over extended periods of time
their own is beyond the state of the art today
on
Recently, more people are starting to work on
aspects of this. Research on interactive task learning
(Hinrichs and Forbus 2014, Kirk and Laird 2014) is
directly concerned with the first two criteria above.
and to some degree the third. Interactive task learn-
ing is a sweet spot in this research path. But I think
the importance of the shift from treating software as
tools versus collaborators should not be underesti-
mated, both for scientific and for practical reasons.
The scientific reasons are explained below. As a prac-
tical matter, the problems humanity faces are grow-
ing more complex, while human cognitive capacities
remain constant. Working together fluently in teams
with systems
that are enough like us to be trusted
and have complemen tary strengths and weaknesses,
could help us solve problems that are beyond our
reach today.
Why Software Social Organisms?
I claim that human-level AI is equivalent to suffi-
ciently smart software social organisms. I start by
I
motivating the construction of organisms, then
argue that they need to be social organisms. A speci-
fication for the substrate capabilities that are needed
to be a social organism is proposed, based on evi-
dence from the cognitive science literature
Why Build Organisms?
There are two main reasons for thinking about build-
ing AI systems in terms of constructing software
S
organisms. The first
We have our own
autonomy.
goals to pursue, in addition to those provided by oth-
ers. We take those external goals as suggestions,
rather than as commands that we run as programs in
our heads. This is a crucial difference between people
and today's Al systems. Most Al systems today can't
Of
be said to have an inner life, a mix of internally and
externally generated plans and goals, whose pursuit
depends on its estimation of what it should be doing.
The ability to punt on an activity that is fruitless, and
to come up with better things to do, is surely part of
the robustness that mammals exhibit There has been
some promising work on metacognition that is start-
ing to address these issues (Cox and Raja 2011), but
to
the gap between
human abilities and AI systems
remains wide.'
Another aspect of autonomy is the separation
internal versus external representations. We do not
AI MAGAZINE
Articles
have direct access to the internal representations of Sociality gives rise to friendship and helping, as well
to
deceit and competition. These cognitive chal-
children or our collaborators. (Cognitive science
as
lenges seem to be strong drivers toward intelligence,
would be radically simpler if we did.) Instead we
communicate through a range of modalities, includ- as most social creatures tend to be more intelligent
to
ng natural language, sketching, gesture, and physi- than those that are not, with dolphins, crows, and
cal demonstrations. These work because the recipient dogs being well-known examples
for focusing on social organisms
is assumed to have enough smarts to figure them out.
second reason
that much of what people learn is from interac-
The imperfections of such communications are well
tions with other people and their culture (Vygotsky
known, that is, the joint construction of context in
natural language dialogue involves a high traction ot 1962). To be sure. we learn much about the basic
xchanges that are diagnosing and repairing mis- properties of materials and objects through physica
communications. To be sure, there are strong rela- manipulation and other experiences in the world.
tionships between internal and external representa
But we can all think about things that we have nev-
tions: Vygotsky (1962), for example, argues that er experienced. None reading this lived through the
much of thought is inner speech, which is learned American Revolutionary War, for example, nor did
they watch the Gal��pagos Islands form with their
from external speech. But managing that relation:
own eyes. Yet we all can have reasonably good mod-
ship for itself is one of the jobs of an intelligent
els of these things. Moreover, even our knowledge of
organism.
The second reason for building organisms is adap the physical world has substantial contributions
tation. Organisms adapt. We learn incrementally and from our culture: how we carve the mechanisms
S
incidentally in everyday life constantly. We learn underlying events into processes is enshrined in nat-
ral language, as well as aspects of how we carve visu-
about the world, including learning on the job. We
al scenes up into linguistic descriptions (for example,
learn things about the people around us, both people
we work and play with and people who are part of Coventry and Garrod (2004).
A number of AI researchers have proposed that
culture that we have never interacted with and
our
stories are central to human intelligence (Schank
ikely never will (for example, political figures,
celebrities). We learn about ourselves as well: what we 1996, Winston 2012). The attraction and power of
ike and dislike, how to optimize our daily routines,
tories is that they can leverage the same cognitive
what we are good
bad at. and where we'd like to capacities that we use to understand others, and pro-
at,
improve. We build up this knowledge over days,
vide models that can be used to handle novel situa-
tions. Moral instruction, for example, often relies on
weeks, months, and years. We are remarkablv good
tories. Other AI researchers have directly tackled
at this, adapting stably very few people go off the
rails into insanity. I know of no system that learns in how to build
systems that can cooperate and collab-
broad range of domains over even davs without orate with people (Allen et al. 2007;
Huns-
Grosz,
human supervision by people who understand its berger, and Kraus 1999). These lines of research pro-
internals. That is radically different from people, who
vide important ingredients for building social
organisms, but much work remains to be done.
get by with feedback from the world and from other
people who have no privileged access to their inter-
Hence my claim that human-level AI systems will
simply be sufficiently smart software social organ-
nals.
isms. By sufficiently smart, mean capable of learn-
Having autonomy and adaptability covers the sec-
ond and third desiderata, and can be thought of as an ing to perform a broad range of tasks that people per-
elaboration of what is involved in achieving them form, with similar amounts of input data and
instruction, arriving at the same or better levels of
Communication through natural modalities is
implied by both, thereby covering the first least performance. Does it have to be social? If not, it
at
partly. But to complete the argument the first, could not discuss its plans, goals, or intentions, and
for
and to handle the fourth (collaborators) we need to could not learn from people using natural interaction
modalities. Does it have to be an organism? If: not. it
social organisms.
consider why we want
will not be capable of maintaining itself, which is
Why Social Organisms?
something that people plainly do.
People are social animals. It has been proposed (for Substrate Capabilities for Social Organisms
example, Tomasello [2001]) that, in evolutionary
This equivalence makes understanding what is need-
terms, being social provides strong selection bias
ed to create social organisms more urgent. To that
toward intelligence. Social animals have to track the
relationships between themselves and others of their end. here is a list of substrate capacities that I believe
ot
30
species. Being social requires figuring out who are
will needed to create human-level social organ-
isms. These are all graded dimensions, which means
your friends and allies, versus your competitors and
enemies. Relationships need to be tracked over time, that incremental progress measures can be formulat-
ed and used as dimensions for evaluation
which involves observing how others are interacting
to build and maintain models of their relationships.
1) Autonomy. They will have their own needs, drives
1 / 1
ChatGPT
ChatGPT
The text above seems to be a well-detailed discussion on the idea of treating AI systems as "software social organisms". The author argues that to achieve human-level AI, we should not simply treat AI systems as tools, but rather as collaborators. This is because human cognition is essentially social and adaptive - we learn not only from interacting with the world, but also from interacting with others. This interaction involves understanding others' intentions, beliefs, and goals, and adapting our own behaviour accordingly. We do not merely react to the world, but proactively plan, set goals, and take initiative, based on our own beliefs and desires as well as those of others.
The author argues that this sort of cognition cannot be achieved merely by hard-coding rules into an AI system, but requires that the system be capable of learning and adapting on its own. To that end, the author proposes that AI systems should be designed as "software social organisms" that are capable of autonomous learning, self-monitoring, and adaptation, and can engage in social interaction in a human-like manner.
To support this claim, the author provides evidence from cognitive science literature that humans are inherently social organisms, and that much of our cognitive capabilities stem from our social interactions.
The text also outlines certain 'substrate capabilities' needed to create human-level social organisms. One such capability is autonomy, which implies that AI systems must have their own needs, drives, and goals, just like human beings.
To summarize, the author proposes a shift from treating AI systems as mere tools to treating them as 'software social organisms' - autonomous, adaptable, and social - capable of learning, interacting, and collaborating in a human-like manner. This shift, the author believes, is critical for achieving human-level AI.
1 / 1
capabilities for acting and learn-
ing. What should those needs and
drives be? That will vary, based on the
niche that an organism is operating
But if we are wise, we will include
in their makeup the desire to be good
moral actors, as determined by the
culture they are part of, and that they
will view having good relationships
with humans as being important to
their own happiness.
(2) Operates in environments that
support shared focus. That is, each
participant has some information
about what others can sense, and par-
ticipants can make their focus of
attention known to each other easily.
People have many ways of drawing
attention to people, places, or things
such as alking, pointing, gesturing
erecting signs, and winking. But even
with disembodied software. there are
opportunities for shared focus, for
example, selection mechanisms com-
monly used in GUIs, as well as speech
and text. Progress in creating virtua
humans (for example, Bohus
and
Horvitz (2011] and Swartout et al
2013]) is increasing the interactive
bandwidth, is progress in human-
as
robotics interaction (for example,
Scheutz et al. (2013)).
(3) Natural language understanding
and generation capabilities sufficient
to express goals, plans beliefs. desires,
and hypotheticals. Without this capa-
bility, building a shared understand-
ing of a situation and formulating
joint plans becomes much more diffi-
cult.
Ability to
build models of the
(4)
intentions of others. This implies
O
learning the types of goals they can
have, and how available actions feed
into those goals. It also requires mod-
It
Or needs and drives as the well-
els
springs of particular goals. This is the
basis for modeling social relation-
ships.
(S) Strong interest in interacting with
4
other social organisms (for example,
people), especially including helping
and teaching. Teaching well requires
models of what others
building up
know and tracking their progress.
There is ample evidence that other
animals learn by observation and imi-
tation. The closest thing to teaching
s0
other animals found so far is that,
in
in some species, parents bring increas-
ingly more challenging prey to their
young as they grow. By contrast,
human children will happily help
adults, given the opportunity (for
example, Liszkowski, Carpenter, and
Tomasello [2008|).
This list provides a road map for
a
developing social organisms of varying
degrees of complexity. Simpler envi-
ronmental niches require less in terms
of reference to shared focus, and
diminished scope for beliefs, plans,
and goals, thereby providing more
tractable test beds for research. I view
Allen's Trips system(Ferguson and
Allen 1998), along with virtual
humans research (Bohus and Horvitz
2011, Swartout et al. 2013), as exam-
ples of such test beds. As AI capabilities
increase, so can the niches, until ulti-
mately the worlds they operate in are
coextensive with our own.
apprentice graduates to autonomous
operation within a community, per-
forming well on its own, but also inter-
acting with others at the same level.
Apprentices do not have to be perfect:
ask
They can ask for help, and help others
in turn. And in time, they start train-
ingt their own apprentices.
Apprenticeship can be used in a
wide variety of settings. For example,
we are using this approach in working
with companions in a strategy game,
where the game world provides a rich
a
simulation and source of problems and
decisions to make (Hinrichs and For-
bus 2015). Robotics-oriented
re-
searchers might use assembly tasks or
flying survey or rescue drones in envi-
ronments of ever-increasing complexi-
An example of a challenge area for
evaluating AIs is science learning and
teaching. The scientific method and its
products are one of the highest
achievements of human culture. Ulti-
mately, one job of AIs should be help-
ing people learn science
in any
domain and at any level. The Science
Test working group- has proposed the
following trajectory as a way of incre-
mentally measuring progress. First,
evaluate the ability of AI systems to
questions about science, using
answer
standardized human-normed tests
such as the New York Regent's Science
available for multiple
Tests, which are
years and multiple levels. Second, eval-
uate the ability of AI systems to learn
new scientific concepts, by reading,
watching videos, and interacting witl
people. Third, evaluate the ability of of AI
svstems to communicate what they
know about science across multiple
domains and at multiple levels. We
conjecture that this provides a scalable
trajectory tor evaluating AI systems,
with the potential for incremental and
increasing benefits for society
as
progress is made
This challenge illustrates how useful
the apprenticeship approach can be for
evaluation. The first phases are aimed
at evaluating systems as students
ensuring that they know enough to
contribute. The middle phase focuses
on being able to contribute, albeit in a
limited way. The final phase is S focused
on AIs becoming practitioners. Notice
that in each phase there are multiple
Implications for Measuring
Progress
This model for human-level AI has sev-
eral implications for measuring
progress. First, it should be clear that
no single test will work. No single
test
can measure adaptability and breadth
Single tests can be gamed, by systems
that share few of the human character-
istics above. Believability, which is
what the Turing test is about, is partic-
S
ularly problematic since people tend to
treat things as social beings (Reeves
and Nass 2003).
What should we do instead?
believe that the best approach is to
evaluate AI systems by their ability to
participate in our culture. This means
having AI systems that are doing some
form of work, with roles and responsi
bilities, interacting with people appro-
priately While doing this. it needs to
adapt and learn, about its work, about
others, and about itself. And it needs to
constantly
do so without AI experts
SO
fiddling with its internals.
I believe the idea of apprenticeship
S an extremely productive approach
for framing such systems. Apprentice-
ship provides a natural trajectory for
bringing people into a role. They start
as a student, with lots of book learning
and interaction. `There are explicit les-
sons and tests to gauge learning. But
there is also performance at first with
simple subtasks. As an apprentice
learns, their range of responsibilities
expanded to include joint work, where
roles are negotiated. Finally, the
1 / 1
ChatGPT
ChatGPT
This text proposes a model for developing human-level Artificial Intelligence (AI), guided by the principles and methodologies of apprenticeship. The author lists five key capabilities for creating AI systems that can interact effectively in human society:
Being moral actors: The AI should have a desire to behave ethically, with their moral framework shaped by the culture they are part of. They should also value having good relationships with humans.
Operating in environments that support shared focus: The AI should have the ability to understand what others are sensing or focusing on and communicate its own focus.
Having natural language understanding: The AI should be capable of expressing and understanding complex concepts like goals, plans, beliefs, desires, and hypothetical scenarios.
Modeling the intentions of others: The AI should be able to learn the types of goals others can have and understand how actions feed into those goals.
Interacting with other social organisms: The AI should have a strong interest in interacting with humans and other social organisms, including teaching and helping them.
The text emphasizes the importance of apprenticeship in AI development. This approach allows the AI to gradually learn and take on more complex tasks over time. It suggests that the AI should start as a student, learning from interactions and tests, before taking on simple tasks, joint work, and eventually becoming an autonomous actor within the community.
The author argues that the ability of an AI to participate in our culture is a good measure of its progress. This includes performing work with specific roles and responsibilities, interacting appropriately with people, and continually adapting and learning. These capabilities should be achieved without constant adjustment from AI experts.
The text concludes by proposing a specific challenge for AI: learning and teaching science. This challenge would involve the AI answering questions about science, learning new scientific concepts, and communicating this knowledge across various domains and levels. This, the author believes, offers a scalable way to evaluate the progress of AI systems.
1 / 1
dimensions of scalability: number Of better in particular domains. They start-
lomains, level of knowledge (for ed with much of the domain knowl-
edge that they would need, and learned
example, grade level), and modalities
needed to communicate. (We return to more about how to operate in that
1O
the question scalable evaluation domain. In qualitative reasoning, many
systems have been built that incorpo:
limensions more generally below.)
rate expert-level models for particular
Progress across these dimensions neec
be uniform: some groups might domains (Forbus 2011). Breadth is now
not
focus entirely on maximizing domair the challenge. Consider what fourth
coverage, while others might choose to graders know about science (Clark
2015), and the kinds of social interac
stick with a single domain but start to
focus early on tutoring within that tions they can have with people. AI sys-
domain.
tems are still far from that level of
This provides a rich tapestry
of graded challenges. Moreover, incre- accomplishment, nor can they grow
mental progress will lead to systems into expertise by building on their
everyday knowledge, as people seem to
that could improve education.
do (Forbus and Gentner 1997).
Scalable Evaluation Dimensions
Range of Tasks the System Is Responsi-
A productive framework should pro-
ble FOr_
vide a natural set of dimensions along
Most AI systems have focused on sin-
which progress can be made and meas-
gle tasks. Being able to accomplish
ured. Here are some suggestions
multiple tasks with the same system
implied by the software social organ- has been one of the goals of research
ism approach.
on cognitive architecture, and with
interactive task learning, the focus is
Vatural Interaction Modalities
Text, speech, sketching, vision, vision, and and shifting to being able to instruct sys-
mobility are all capabilities that can be tems in new tasks, an important step
valuated. Text can be easier than
toward building systems capable
speech, and sketching can be viewed as enough to be apprentices.
a simplified form of vision.
Learning Abilities
Initial Knowledge Endowment
Software social organisms need to
How much OF what a system knows is
learn about their jobs, the organisms
learned by the system tselt. versus
(people and machines) that they work
what it has to begin with? What the
with, and about themselves. While
ome problems may well reauire mas-
absolute minimum initial endowmen
might be is certainly a fascinating sci- sive amounts of data and deep learning
entific question, but it is probably bes
(for example, speech recognition
answered by starting out with substan- (Graves, Mohamed, and Hinton
ially more knowledge and learning
2013)), people
capable of learning
are
how human-level capabilities can be many things with far fewer examples
reached. Understanding those path- Office assistants who required, for
wavs should better enable us to under-
example, 10,000 examples of how to
fill out a form before being able to do
stand what minimal subsets can work.
it themselves would not last long
It is very seductive to start from
in
scratch, and perhaps
if it could any reasonable organization. 'There are
easier,
be made to work. But the past 50 years many circumstances where children
r
fast
learn rapidly (for example,
of research suggests that this is much
map-
ping in human word learning [Carey
harder than it seems: Look at the vari-
in
ous "robot baby" projects that have
2010]), and understanding when this
tried that. Arguably, given that IBM's
an be done, and how to do it, is an
Watson used more than 900 million
important question.
syntactic frames as part of its knowl
edge base, the 5 million facts encoded
Summary
in ResearchCyc might well be consid-
ered a small starting endowment
I have argued that the goal of human-
Level of Domain Knowledge and Skill level AI can be equivalently expressed
'rior work on learning apprentices (for as creating sufficiently smart software
xample, Mitchell et al. [1994]) focused
ocial organisms. This equivalence is
on systems that helped people perform useful because the latter formulation
makes strong suggestions about how
such systems should be evaluated. No
single test is enough, something which
has become very apparent from the
Of Turing's test, which
limitations
brought about the workshop that
motivated the talk that this article was
based on. More positively, it provides a
framework for organizing a battery of
tests, namely the apprenticeship tra-
jectory. An apprentice is initially a stu-
dent, learning from
instructors
through carefully designed exercises
Apprentices start working as assistants
to a mentor, with increasing responsi-
bility as they learn. Eventually they
as
working autonomously, commu-
start
nicating with others at their same lev-
el, and even taking on their own
apprentices. If we can learn how to
build AI systems
with these capabili-
ties. it would be revolutionary. I hope
the substrate capabilities for social
organisms proposed here will encour-
age others undertake this kind of
to
research.
The fantasy of the Turing test, and
many of its proposed replacements, is
O
that a single simple test can be found
for measuring progress toward human-
level AI. Part of the attraction of this
view is that the alternative is botI dif-
ficult and expensive. Many tests,
involving multiple capabilities and
interactions over time with people, all
require substantial investments in
research, engineering, and evaluation
But given that we are tackling one of
the deepest questions ever asked by
humanity, that is, what is mind, this
should not be too surprising. And
believe it will be an extraordinarily
productive investment.
Acknowledgements
I thank Dedre Gentner, Tom Hinrichs,
Mike Tomasello, Herb Clark, and the
Science Test Working Group for
many
helpful discussions and suggestions
This research is sponsored by the
Socio-Cognitive Architectures and the
Machine Learning, Reasoning, and
Intelligence Programs of the Office of
Naval Research and by the Computa-
tional and Machine Intelligence Pro-
gram of the Air Force Office of Scien-
tific Research.
Notes
1. Part of the gap, believe, is the dearth of
SPRING 2016 89
Articles
broad and rich representations in most AI
Wiley Interdisciplinary Reviews: Cognitive Sci-
systems, exacerbated by our failure as a field ence 24 (July/August): 374-391.dx.doi.org/
10.1002/wcs.115
to embrace existing off-the-shelf resources
such as ResearchCyc.
Forbus, K., and Gentner, D. 1997. Qualita-
tive Mental Models: Simulations or Memo
2. The Science Test Working Group includes
or
Peter Clark, Barbara Grosz, Dragos Margin- ries? Paper presented the Eleventh Inter-
eantu, Christian Lebiere, Chen Liang, Jin
Workshop
on
national
Qualitative
Spohrer, Melanie Swan, and myself. It is one Reasoning, Cortona, Italy, June 3-6.
of several groups working on tests that, col- Forbus, K.; Riesbeck C Birnbaum, L; Liv-
lectively, should provide better wavs of ingston, K.; Sharma, A.; and Ureel. L. 2007
measuring progress in Al.
Integrating Natural Language, Knowledge
Representation and Reasoning, and Analog-
References
1 / 1
ChatGPT
ChatGPT
It seems like this piece is discussing the concept of software social organisms, AI systems that can operate as entities within a social system and contribute to various tasks or roles traditionally held by humans. The author outlines multiple dimensions for evaluating the progress of these systems:
Natural Interaction Modalities
: This is about how the AI interacts, whether it's through text, speech, sketching, vision, etc.
Initial Knowledge Endowment
: This refers to what the AI knows initially, which includes both innate and learned knowledge.
Level of Domain Knowledge and Skill
: This dimension relates to the degree to which the AI can master and apply knowledge in a specific domain.
Range of Tasks the System Is Responsible For
: This pertains to the breadth of tasks that the AI system can handle.
Learning Abilities
: This refers to the system's ability to learn new tasks and adjust to new situations.
The author argues that AI's evolution should be tracked across these dimensions, rather than a single test (like the Turing Test), as each provides important information on different facets of the AI's abilities. The suggested approach to test an AI's progress is via an apprenticeship trajectory, which parallels human learning: initially learning from instructors, then gradually taking on more responsibilities, eventually working autonomously, and even guiding others.
The author acknowledges that this method of evaluation will be time-consuming and expensive but argues that it's necessary for truly understanding the capabilities and progress of AI systems. They believe that the question of artificial intelligence ties into one of the greatest philosophical questions of humanity - what is mind - and as such, it is worth the significant investment.
1 / 1
Any connections between this article and the one by David Brin?
1 / 1
ChatGPT
ChatGPT
The article by David Brin you mentioned is not included in our conversation, so I can't directly compare the two. However, David Brin is a well-known science fiction author and futurist who often explores themes of technology, transparency, and society's interaction with AI and other advanced technologies.
Based on the article text you provided earlier, it seems to focus on the idea of "software social organisms," or artificial intelligence systems designed to operate as entities within a social system. These systems would be evaluated based on multiple dimensions, such as their interaction modalities, their initial knowledge, their domain knowledge and skill, the range of tasks they can handle, and their learning abilities. The approach emphasizes the importance of a more nuanced, multifaceted evaluation of AI systems' progress.
If David Brin's work you're referring to discusses AI, transparency, or human-machine interaction, there could potentially be overlap or points of connection. For instance, both might explore the societal impact of AI, the ethical considerations of AI development, or the mechanisms by which AI might be integrated into human communities. However, without the specific content of Brin's work, I can't draw any direct correlations. Could you provide more context or specify the work of David Brin you're referring to?
1 / 1
It was the first article we discussed: Give Every AI a Soul���or Else
To solve the ���crisis��� in artificial intelligence, AI beings must say, ���I am me.���
1 / 1
ChatGPT
ChatGPT
Thank you for clarifying. Now, let's explore the connections between the two articles.
Both articles discuss the development of artificial intelligence, but they focus on different aspects:
David Brin's article
"Give Every AI a Soul���or Else" primarily emphasizes the ethical and societal aspects of AI development. Brin suggests that AI beings should be designed with a sense of self-awareness or individual identity, which he refers to as giving each AI a "soul." The idea is to ensure that AIs can be held accountable for their actions and integrated into society in a way that respects existing legal and ethical frameworks.
The
second article
you shared focuses more on the technical aspects of AI development. It presents the idea of "software social organisms," which are AI systems designed to operate as entities within a social system. It suggests a multifaceted approach to evaluating AI's progress in various dimensions, including natural interaction modalities, initial knowledge endowment, level of domain knowledge and skill, range of tasks the system is responsible for, and learning abilities.
In terms of connections:
Both articles argue for AI systems that can be integrated into human social structures. Brin advocates for AIs with a sense of self ("a soul"), enabling them to be accountable within our ethical and legal frameworks. The second article discusses AI as "software social organisms" that need to interact in varied, natural ways within social environments.
Both articles highlight the need for a nuanced approach to AI development and evaluation. Brin emphasizes the importance of considering ethical and societal implications alongside technical progress. The second article stresses the necessity of evaluating AI's progress across multiple dimensions, rather than focusing solely on one aspect.
Both point towards the need for AI systems to learn and adapt. In Brin's case, the AI should be self-aware and able to learn from its actions to ensure it acts ethically. The second article directly states the importance of learning abilities as a measure of an AI system's progress.
However, there are differences in the focus of the two articles. Brin is more concerned with the societal and ethical dimensions of AI development, while the second article focuses more on the technical dimensions and practical evaluation of AI progress.
1 / 1
ChatGPT may produce inaccurate information about people, places, or facts.
ChatGPT May 24 Version
ChatGPT





