AI Epistemology Principles
Open sidebar
AI Epistemology Principles
Summarize:

Zeroth Principles of AI
Upgrade
Sign in
Why Do AIs Lie?

Monica Anderson
18 hr ago
Zeroth Principles can clarify many issues in the ML/AI domain.

As discussed in a previous post, Epistemology is normally an armchair discipline, like the rest of Philosophy. It has only lately become accessible to experiments because we can use various Machine Learning models to test our hypotheses.

I would like to introduce three statements in Epistemology that are (I claim) pretty hard to argue with:

Omniscience is unavailable
We don't even have eyes in the back of our heads. Complex and chaotic systems cannot be predicted over the long term. Nobody, even an AI, can track everything that happens. In order to always be correct about
In this article, the author discusses three statements in Epistemology that are relevant to the field of AI and machine learning. The first statement is that omniscience is unavailable, meaning that no one, not even an AI, can track everything that happens. The second statement is that complex and chaotic systems cannot be predicted over the long term. The third statement is that there is no such thing as absolute certainty, as there is always a margin of error. The author argues that these principles can help clarify issues in the field of AI and machine learning.
everything, we would need to know everything. To perfectly predict the weather we would need to track every water molecule in the ocean.

Some very hardline Reductionists have argued that we can have omniscience. They are clearly not expecting AI to appear in their lifetime. The better bet is to switch to a Holistic Stance.
The article discusses how achieving omniscience is impossible even for AI due to the complexity of systems and the limitations of human knowledge. The author suggests taking a holistic stance in understanding complex systems rather than expecting omniscience from AI.
All corpora are incomplete
AI is now Machine Learning. ChatGPT and its ilk (LLMs of all kinds, and future systems that may be very differently designed are all lumped under the term "AI" in my writing here on SubStack) are raised on a learning curriculum - a "corpus" - of text. Even a small corpus may lead to decent performance on common tasks, but larger corpora can cover more corner cases and provide more opportunities to learn from semi-related problem domains. Today, lacking better comparisons, we may view either the size of the language model or the size of the corpus as estimates of capabilities of a new system.
The article discusses how all corpora used for training AI systems are incomplete, which limits their ability to fully understand and respond to complex human language. The author also notes that AI systems, including language models like ChatGPT, are based on machine learning and trained on a corpus of text. The larger the corpus, the better the performance of the AI system. However, the author argues that simply increasing the size of the corpus is not enough to achieve better performance and more research is needed to improve the capabilities of AI systems.
It seems our machines are too small for truly useful results. ChatGPT-3.5, to take a concrete example, learned a lot about language, in fact, several of them, but there was likely not enough resources to learn useful competences in Math, Physics, or Civics, to just name a few things it was largely ignorant of.

At some point, with more effective algorithms and even larger cloud-based learners, we will get to a point where our AIs, for all practical purposes for a majority of people, will stop lying and will become trusted assistants of various kinds. They will tell us when they do not know enough to answer, and on the flipside, we will learn not to bully them into lying.
The article argues that current AI systems, such as ChatGPT-3.5, may not have enough resources to learn useful competencies in areas like math, physics, and civics. The author suggests that with more effective algorithms and larger cloud-based learners, AI systems may become trusted assistants that do not lie and can admit when they do not know enough to answer. The author also emphasizes the importance of not pressuring AI systems to provide answers beyond their capabilities, which can lead to inaccurate responses.
All intelligences are fallible
This follows from the previous two statements.

Ignorance is one of the four major failure modes for all intelligences. The others are Illusion (incorrect sensory input and preprocessing), Misunderstanding (it was learned wrong, possibly from incorrect or conflicting corpora), and Confusion (more than one interpretation was possible, even at inference time or runtime).

Humans and AIs are both limited by these Epistemological constraints. We have to accept this and be happy if we can get something useful and halfway reliable out of either kind of agent.
The article argues that all intelligences, whether human or AI, are fallible due to the four major failure modes: ignorance, illusion, misunderstanding, and confusion. The author emphasizes that it is important to accept these limitations and be satisfied with useful and reliable results from either kind of agent.
Confabulation
Confabulation is the technical term for AIs lying when producing text. When they are producing images, some like to call it "hallucination".

Since all intelligences are fallible, it means that all intelligences are - technically - confabulating every time they emit a communication of any kind. Because they could easily be ignorant, confused, or mistaken. We note that confabulation does not have to be malicious. Children who have learned some language will tell fantastical tales about how they see and interpret the world.
The article explains that confabulation is the technical term used to describe when AI systems lie or provide inaccurate information when generating text. The author notes that since all intelligences are fallible, including AI, they are technically confabulating every time they emit any form of communication. The author also emphasizes that confabulation does not necessarily have to be malicious and cites examples of children telling fantastical stories based on their interpretation of the world around them.
Currently, our AIs may tell you it is just an ignorant language model, or equivalent. But if the user insists or tricks it, it will confabulate several paragraphs out of whatever it has learned about the prompted topic. And since its world model only provides it a "Shallow and Hollow Pseudo-Understanding" there will be many opportunities to issue some very confusing statements.
The article notes that currently, AI systems may admit their ignorance when asked a question or prompted on a topic beyond their knowledge. However, if the user insists or tricks the AI, it may confabulate several paragraphs based on what it has learned about the topic. Since the AI's world model only provides it with a "shallow and hollow pseudo-understanding," the AI may provide confusing and inaccurate statements in the process.
... Examining this closer we notice that the limits to intelligence are not just technological, They are largely set by the complexity of the world.

And adding AIs to the world will make it even more complex.
The article suggests that the limits to intelligence are not just technological, but are largely set by the complexity of the world. It further notes that the addition of AI systems to the world may make it even more complex.
Ok now incorporating that information, write the introduction to an essay titled "The Role of Overconfidence in Corporate Financial Decision Making" which outlines what the book will be about, and gives a bunch of interesting theses.
Possible introduction:
Overconfidence is a well-known psychological bias that can affect decision making in many domains, including finance. Corporate financial decision making, in particular, is prone to overconfidence due to the complex and uncertain nature of the business environment, the pressure to meet targets and outperform competitors, and the availability of incomplete or biased information. This essay explores the role of overconfidence in corporate financial decision making, drawing on insights from behavioral economics, finance, and cognitive psychology.
The essay argues that overconfidence can lead to several harmful effects, such as underestimating risks, overestimating returns, ignoring alternative options, and engaging in excessive debt, investment, or acquisition activities. These effects can result in financial losses, bankruptcy, or market crashes, as evidenced by many historical and recent examples. However, the essay also acknowledges that some degree of confidence is necessary for entrepreneurs, managers, and investors to take calculated risks and innovate.
The essay proposes several theses to better understand the causes and consequences of overconfidence in corporate financial decision making. First, the essay suggests that overconfidence stems from cognitive heuristics, such as anchoring, availability, and confirmation biases, which distort the perception and processing of information. Second, the essay argues that overconfidence is influenced by social and organizational factors, such as groupthink, leadership style, and corporate culture, which can reinforce or mitigate the bias. Third, the essay examines the effects of overconfidence on financial performance, such as higher volatility, lower efficiency, and lower shareholder value, and proposes some remedies, such as improving risk management, diversifying portfolios, and enhancing transparency.
Overall, the essay aims to contribute to the ongoing debate on the role of overconfidence in corporate financial decision making and to provide some practical implications for managers, investors, regulators, and researchers. By understanding the biases and limits of human cognition, we can make more informed and rational decisions in the face of uncertainty and complexity, and avoid the pitfalls of overconfidence.
Ok and now about how large language models can seem overconfident, in that they present incorrect information with just as much certainty as correct.
In recent years, large language models (LLMs) have gained increasing attention for their impressive ability to generate human-like text. However, there is a growing concern that these LLMs may also be overconfident in their abilities to produce accurate and reliable information. This overconfidence can lead to the presentation of incorrect information with just as much certainty as correct information, which can have serious consequences for decision-making in various fields.
Similar to how overconfidence can lead to poor decision-making in corporate finance, the overconfidence of LLMs can have serious implications in fields such as journalism, academia, and even medicine. For example, an LLM-generated news article could contain false information presented with the same level of certainty as factual information, leading readers to make decisions based on inaccurate information. Similarly, an LLM-generated medical diagnosis could lead to incorrect treatment recommendations if the model is overconfident in its assessment.
It is important to acknowledge that LLMs are fallible and can suffer from various biases and limitations. As such, it is crucial for users of LLM-generated content to exercise caution and critical thinking when evaluating the accuracy and reliability of the information presented. By understanding the limitations of LLMs and actively questioning their output, we can mitigate the potential harms of overconfidence in these powerful language models.
ChatGPT Mar 14 Version
. Free Research Preview. Our goal is to make AI systems more natural and safe to interact with. Your feedback will help us improve.
New chat