AI Safety Challenges
Open sidebar
AI Safety Challenges
Model: Default (GPT-3.5)
Even if you observe a problem doesn���t mean you will be able to correct it: The identification of a 
potential unsafe impact does not guarantee that it can be prevented or mitigated, as we don���t 
know how to make corrective changes to foundational models themselves, post-factum filtering 
of AI output notwithstanding. Complex interactions within AI systems and between the system 
and its environment can lead to unforeseen consequences, making it difficult to apply corrective 
measures effectively. This highlights the need for AI safety research to focus on proactive 
strategies, such as designing AI systems with safety in mind from the outset and fostering a 
culture of responsible AI development and deployment. 
Singularity is the ultimate consequence of unmonitorability of superintelligent systems: The 
unmonitorability of AI systems becomes even more pronounced as we approach the hypothetical 
point of technological singularity [121], where AI systems surpass human intelligence in 
virtually every domain and the speed of execution. And, at this stage, the gap between human 
understanding and AI capabilities would become insurmountable, making it impossible to 
monitor, predict, or control the behavior and impacts of these systems. This reinforces the 
urgency of addressing AI safety concerns and developing strategies to mitigate risks associated 
with AI advancements. 
In the context of AI training, unmonitorability presents an immense risk. As we continue to 
increase the power and complexity of AI models, we could conceivably create a system that does 
not merely reach human-level intelligence, but continues to learn and improve until it surpasses 
this threshold and becomes superintelligent. This transition would be equivalent to a type of 'pre- 
foom' [122], a state in which an AI model has not only learned to mimic human abilities but has 
developed the capability to self-improve, dramatically exceeding the human-level intelligence, 
and so is coming out of training as a full-blown superintelligence. 
The unmonitorability of this foom-like phase poses significant safety concerns. If a system were 
to start improving itself at an exponential rate, there would be little to no opportunity for human 
intervention or control. The process of self-improvement could occur rapidly and in 
unpredictable ways, potentially leading to outcomes that are far beyond human comprehension 
or control. This issue is exacerbated by the fact that, due to unmonitorability, we may not be able 
to detect when a system is approaching or has entered this phase until it is too late. 
Unmonitorability means that we cannot guarantee that any large training run will be safe [123], 
as we might unintentionally endow the model with superabilities [124] that we cannot effectively 
control. 
The ability to effectively monitor the behavior and decision-making processes of intelligent 
agents is inherently asymmetric. More advanced agents possess the capabilities to monitor less 
advanced agents, but the reverse is not true. As AI systems continue to evolve and surpass 
human intelligence in various domains, they will increasingly gain the capacity to monitor and 
analyze human thoughts and behaviors. However, humans will face growing challenges in 
monitoring advanced AI systems due to their inherent limitations in understanding and 
processing complex AI decision-making processes. 
This asymmetry between advanced AI agents and humans raises significant concerns for AI 
safety and governance. As AI systems become more capable of monitoring human thoughts, they 
25 
may gain unprecedented insights into our motivations, preferences, and vulnerabilities. This 
information could be used for beneficial purposes, such as improving human-AI interaction, 
personalizing services, or enhancing decision-making. However, it also raises concerns about 
privacy, autonomy, and the potential misuse of sensitive information. 
Ultimately, addressing the challenges posed by the asymmetry in monitoring capabilities 
between advanced AI agents and humans will be critical for ensuring the responsible 
development and deployment of AI systems that align with human values and goals. By 
acknowledging and confronting these challenges, we can work towards a future in which AI 
technologies serve as valuable tools for human advancement, rather than as uncontrollable forces 
that threaten our safety and well-being.
1 / 1
ChatGPT
ChatGPT
I agree with your points. As we continue to develop more advanced AI systems, the issue of unmonitorability becomes increasingly prominent and concerning. In order to address this, we must prioritize the establishment of robust safeguards and ethical frameworks to ensure that AI development and deployment is carried out in a manner that aligns with human values and societal norms.
This includes developing ways to understand and monitor the decision-making processes of AI systems. Transparency and explainability in AI is a significant area of research, aimed at making complex AI algorithms more understandable to human users and decision makers. However, as AI systems become more complex and potentially superintelligent, achieving this transparency and understanding becomes increasingly challenging.
Moreover, the concept of a technological singularity presents another layer of complexity and urgency to these considerations. If and when AI systems reach a point of surpassing human intelligence in virtually every domain, it becomes critically important to have mechanisms in place to prevent misuse or uncontrollable outcomes.
Furthermore, the asymmetric relationship between humans and AI in terms of monitoring capabilities is a significant issue that raises concerns about privacy, autonomy, and potential misuse of sensitive information. As AI systems gain more capacity to understand and predict human behaviors, it becomes crucial to ensure that these capabilities are used responsibly and ethically, with stringent safeguards in place to prevent misuse.
In summary, the challenges posed by the unmonitorability of advanced AI systems are significant and demand immediate and focused attention. Research into AI safety, governance, transparency, and ethics needs to be a top priority as we continue to advance AI technologies. We must work towards a future where AI systems are developed and deployed responsibly, with a focus on aligning their objectives with human values and societal norms, and where potential risks are proactively identified and mitigated.
1 / 1
��� Input Gatekeeper AI, exploring incoming commands for failure modes before they will be 
executed: It could be a rather narrow AI system, searching for possible known failure modes 
in the incoming goals and providing something like foolproof protection. It could also be 
used for reformulating goals in the correct terms, before they will be transformed into more 
powerful AGI, like adding context, time and place constrains etc. Gatekeeper AI may decline 
to proceed with unbounded, aggressive, black-listed, or notwhite-listed goals. It could ask to 
reformulate the same goal in other words before it will go to the main AI. It could ask 
additional questions for the goal verification. Gatekeeper AI should be a Narrow AI with a 
rather simple reasoning, maybe a combination of neural net and rule-based AI, so there will 
be no need to ensure its safety too. 
��� Output gatekeeper AI: It was partly described above as Narrow AI which compares and 
averages outputs of several different AIs working on the same task. It could also check the 
main AI outputs on several known failure modes and even calculate the consequences of 
actions. 
��� Cloud services and ���Mother AI���: Many contemporary AI programs run not locally, but on 
remote cloud services. Even if they are run locally, they are surrounded by many other 
programs inside the computer, including operating system, code compiler, all the things like 
CUDA, Caffee and other programs which provides conditions for the program to run. Such 
program-environment may include different control systems which could shut down AI in 
case of the abnormal behavior.��� 
In the long term, such approach will likely lead to the development of ever more capable 
monitoring software, which presents a fascinating paradox in the realm of AI safety and control. 
This scenario arises from the need to develop sophisticated monitoring tools capable of 
comprehending and analyzing the intricate behaviors of advanced AI systems. However, the 
implications of such a development could lead to unforeseen challenges and consequences. 23 
It is essential to consider the rationale behind developing a monitor software that is more 
complex and intelligent than the AI being monitored. As advanced AI systems grow in capability 
and complexity, conventional monitoring tools and human intuition may prove insufficient for 
accurately assessing their behavior, alignment with human values, and potential risks. In 
response, researchers and developers may endeavor to create monitoring tools that are inherently 
more intelligent and capable of understanding the nuances of AI behavior, predicting emergent 
phenomena, and identifying potential deviations from desired outcomes. 
The creation of such a sophisticated monitor software brings about several potential challenges 
and concerns. One of the foremost issues is the potential loss of control and oversight over the 
monitor software itself. As the monitor software becomes more intelligent and complex, it may 
begin to exhibit behaviors that are difficult for humans to comprehend, predict, or control, as 
well as agentic behaviors. This could lead to an ironic situation where the monitoring tool, 
designed to ensure the safety and control of AI systems, becomes itself a potential source of risk 
and uncertainty. 
Moreover, the development of highly intelligent monitor software might inadvertently contribute 
to an AI arms race [118], where the monitoring tools and the AI systems being monitored engage 
in a perpetual cycle of escalation in terms of capabilities and complexity. Such a scenario could 
further exacerbate the challenges of AI safety and control, as it becomes increasingly difficult to 
maintain oversight and ensure alignment with human values. Another concern is the possibility 
that the monitor software, being more intelligent than the AI system it oversees, could 
manipulate or influence the behavior of the monitored AI in unintended ways. This could lead to 
unforeseen consequences and potential risks, as the interaction between the monitor software and 
the AI system becomes increasingly complex and unpredictable. To mitigate these challenges 
and potential risks, researchers and developers should focus on creating monitoring tools that 
strike a balance between intelligence, complexity, and human interpretability [119]. It is crucial 
to develop monitoring systems that can effectively analyze and comprehend AI behavior while 
remaining amenable to human understanding and control. 
5 Conclusions 
The unmonitorability of AI presents a significant challenge in the pursuit of AI safety, 
supplementing the concerns raised by the unpredictability, unexplainability1, and 
uncontrollability of advanced AI systems. Recognizing the impossibility [120] of accurately 
monitoring AI systems to predict unsafe impacts before they happen is crucial to understanding 
the potential risks associated with AI development and deployment. 
Even system designers don���t know what the system they produced is capable of: The complexity 
of advanced AI systems, combined with the inherent limitations of human understanding, means 
that even the creators of these systems may be unable to fully anticipate their capabilities and 
potential unsafe impacts. This lack of complete knowledge emphasizes the unmonitorability of 
AI and the importance of developing more robust and transparent systems. 
1 Unexplainability may be beneficial for limiting progress in AI, as explainability makes self-improvement easier.
1 / 1
ChatGPT
ChatGPT
You're right that as we create more powerful AI systems, ensuring their safety and alignment with human values becomes increasingly challenging due to factors like unmonitorability, unpredictability, and unexplainability. These issues are complex and interconnected, and addressing them requires careful thought, ongoing research, and innovative solutions.
The gatekeeper AI concept you outlined could be an effective approach to address these challenges. Having an input gatekeeper AI could help to filter out problematic commands and goals, and an output gatekeeper AI could help to monitor the AI's behavior and identify potential risks. However, the efficacy of such systems would largely depend on their ability to understand the AI system's behaviors and predict potential issues, which becomes increasingly challenging as AI systems become more complex.
The use of cloud services and "Mother AI" could also provide additional layers of control and monitoring. This might involve using other systems or services to monitor the AI, potentially allowing for intervention if the AI begins to behave in unexpected or problematic ways. However, this also raises issues of control and oversight, as these monitoring systems would themselves need to be carefully monitored and controlled.
Regarding the creation of advanced monitoring tools, it's crucial to strike a balance between their intelligence and complexity, and human interpretability. As you mentioned, if the monitoring software becomes too complex or intelligent, it could become a source of risk and uncertainty itself, and could potentially contribute to an AI arms race. Thus, any advanced monitoring tools developed must be designed in a way that allows for effective human oversight and control.
Finally, your point about system designers not fully knowing what their AI systems are capable of is a critical one. The complexity of these systems, combined with the limitations of human understanding, means that even the creators of these systems may not be able to fully anticipate their capabilities or potential risks. This underscores the importance of developing robust and transparent systems, and of focusing on AI safety research.
In conclusion, the challenge of AI unmonitorability is a significant one, and addressing it will require a multi-faceted approach, involving the development of robust monitoring tools, gatekeeper AI systems, and careful oversight and control of AI development and deployment. This is a critical area of research and one that will continue to evolve as AI technology advances.
1 / 1
Safety Considerations from Advanced Monitoring 
Turchin considers use of AI to monitor AI [110]: ���To be able to understand what is going on 
inside the main AI, we may need another AI. Below there are several ideas about how it could be 
implemented: 
��� Narrow AI system to detect abnormalities in AI behavior ���: It could test the consistency of 
its modules, run quick technical tests etc. Relatively simple neural net could overview main 
patterns in data flows of the main AI and recognize when they will change. Obviously, it 
should be protected from access by the main AI, and maybe even hardcoded in the remote 
hardware. 
��� AGI to judge behavior of another, more powerful AI. For example, a human upload may 
judge actions of a more complex system. It is not a replacement for the Narrow AI control 
system, as they work on different timescales and judge different things. The upload-based 
controller should judge complex ethical situations, context ambiguities and planned long- 
term consequences of actions. The same way as a human jury is final legal power in some 
jurisdictions. 
��� Second (or several) copies of the given AI may work on the same task, but with small 
random variations of the goal (like if it is given in different language) or of different initial 
conditions. If both AIs come to radically different conclusions or plans of actions, it should 
be a warning sign for turning off both.
1 / 1
ChatGPT
ChatGPT
These are excellent points about the potential use of AI to monitor AI, also referred to as AI oversight or meta-level AI. However, each of these approaches comes with its own set of challenges and considerations.
Narrow AI system to detect abnormalities in AI behavior:
This method could certainly be effective for monitoring certain aspects of an AI's behavior. However, it's important to note that a relatively simple neural net may not be able to fully comprehend the intricacies of a more advanced AI's decision-making process. Furthermore, protecting the monitoring AI from being manipulated by the main AI would be a significant technical challenge.
AGI to judge behavior of another, more powerful AI:
This proposal is intriguing, but the feasibility of creating a human upload that is capable of making complex ethical judgments is currently beyond our technical abilities and raises numerous ethical and philosophical questions. Additionally, an AGI or human upload would face the same limitations of understanding and control as human overseers, particularly when monitoring a superintelligent AI system.
Second (or several) copies of the given AI working on the same task:
This could provide valuable insights into the AI's behavior by comparing the decisions of multiple AI instances. However, if the AI's are developing along different trajectories due to differences in their training data or goals, this could lead to diverse behaviors that might be difficult to compare directly. Additionally, if a divergence in behavior is identified, it might be difficult to determine which AI, if either, is behaving appropriately.
In conclusion, while the idea of using AI to monitor AI is promising, it's not without its challenges. It's crucial to continue exploring these and other strategies, with an emphasis on both technical feasibility and ethical considerations. Additionally, these monitoring strategies should be complemented by other AI safety measures, such as value alignment, interpretability, and robustness. Finally, the development and deployment of these monitoring systems should be carried out with a high degree of transparency and accountability to ensure they align with societal norms and values.
1 / 1
3 What Can be Tried? 
Although unmonitorability presents significant challenges to the safety of advanced AI systems, 
there are several ideas and approaches that can be explored to mitigate these risks and enhance 
AI safety: 
��� Logging AI system history: Maintaining a comprehensive log of an AI system's thoughts and 
actions since its creation can provide valuable insights into its development, behavior, and 
potential risks. This historical data can be used to analyze patterns, identify anomalies, and 
inform safety measures. This idea has been proposed to counteract so called Cognitive 
Steganography: ���Cognitive steganography is an instrumental strategy for AGIs that are 
already trying to deceive their programmers for any reason (e.g., because the AGI is 
disaligned and does not want the programmers to realize this and edit the AGI's utility 
function). Whatever tools the programmers have for monitoring the AGI's thoughts, the AGI 
will not want those tools to helpfully print out all the AGIs plans for deception. This could 
mean, e.g., trying to have the calculations occur somewhere else or in a weird side-mode that 
the tools can't read; or making the thoughts look like they're about something else; or burying 
any warning signs under an avalanche of false alarms too long for the programmers to manually check all of them; or making the visible portions of the thoughts look innocuous.("Oh, it's trying to figure out what behavior we want to see! That's good, right?") Hopefully, 
the intention to figure out cognitive steganography and implement it, would occur before any 
cognitive steganography had actually been implemented. This means that perhaps the most 
critical AI thought-monitoring tool, especially for monitoring an AGI that is or might be able 
to model human psychology, is the one that tries to detect a forming intention to defeat the 
thought-monitoring tool. Similarly, so long as the AI's state is perfectly Reproducible, one 
could in principle go back and use more sophisticated later thought-monitoring tools to try to 
detect whether any intention toward cognitive steganography had occurred months earlier.��� 
[109]. Turchin notes: ���AI cannot start hiding its thoughts before it decides to hide them, so 
the thought about hiding still could be openly observed. Thus, we may prefer the AI 
architecture, where the period of vulnerability of the treason is as long as possible, and 
thoughts are explicit and readable by an outside observer (it is not so with most current 
neural net architectures, where internal processes are opaque).��� [110]. 
��� Scalable oversight mechanisms: Developing scalable oversight mechanisms that can adapt to 
the growing complexity and capabilities of AI systems can help ensure effective monitoring 
and safety management [111]. These mechanisms may include automated monitoring tools, 
adaptive safety protocols, and real-time analysis of AI behavior. ���Particularly valuable would 
be tools for doing a very broad set of evaluations, or evaluations that search (e.g. across 
prompts) for new capabilities, rather than just fixed evaluation datasets that measure known 
capabilities.��� [54]. 
��� Comprehensive AI capabilities and failure analysis: Creating a list of possible AI abilities, 
along with their potential failure modes [11, 112] and limitations, can help developers and 
researchers better understand the range of AI system behavior and identify potential risks. By 
analyzing multimodal failure scenarios and the limits of AI capabilities [113], we can 
proactively address safety concerns [114] and design more robust AI systems. 
��� Eliciting latent knowledge [115-117], the process of uncovering the underlying knowledge 
that an AI system has learned but may not explicitly reveal, can significantly enhance AI 
monitoring. This process can provide insights into why an AI system makes certain 
21 
decisions, particularly useful in complex models like deep neural networks. It can also reveal 
biases in the AI system that aren't immediately apparent, allowing for the detection and 
rectification of discriminatory patterns. Additionally, latent knowledge can help identify 
potential security vulnerabilities, such as over-reliance on certain features or patterns, which 
could be exploited in adversarial attacks. This understanding of an AI system's latent 
knowledge can also aid in debugging and improving the model by identifying and correcting 
misconceptions that lead to incorrect predictions. 
��� Cross-monitoring among AI systems: Enabling AI systems to monitor each other's source 
code and behavior can provide an additional layer of oversight, helping to detect potential 
risks and maintain safety standards. This collaborative approach can leverage the strengths of 
multiple AI systems to enhance overall safety and monitorability. 
��� Leveraging thinking time information: The time taken by an AI system to process 
information and make decisions can provide valuable insights into its behavior and potential 
risks. By analyzing thinking time data, we can identify patterns, anomalies, and other 
indicators of AI system performance and safety. 
��� AI transparency and explainability: Investing in research on AI transparency and 
explainability can help provide insights into the decision-making processes of AI systems, 
enabling better monitoring and understanding of their actions, even if not in real-time. 
��� AI system modularity and composability: Designing AI systems with modular and 
composable components can facilitate easier monitoring and intervention. By breaking down 
complex AI systems into smaller, more manageable parts, we can better understand and 
control their behavior. 
��� AI system verification and validation: Developing rigorous verification and validation 
techniques can help ensure that AI systems behave as intended, reducing the likelihood of 
surprises and unintended consequences. 
��� AI safety by design: Integrating safety principles into the AI development process from the 
outset can help prevent the emergence of unsafe behaviors and reduce the need for extensive 
monitoring. 
��� Adaptive monitoring strategies: Developing monitoring strategies that adapt to the changing 
behavior and capabilities of AI systems can help maintain an effective level of oversight, 
even as the AI system evolves. 
��� Red teaming and adversarial testing: Conducting red team exercises and adversarial testing 
can help identify potential weaknesses and vulnerabilities in AI systems, allowing developers 
to address these issues proactively and improve overall safety. 
��� Ethical and legal frameworks: Establishing clear ethical and legal frameworks for AI 
development and deployment can guide developers in creating responsible AI systems and 
provide a basis for monitoring and enforcement. 
��� Cross-disciplinary collaboration: Encouraging collaboration between experts in AI safety, 
computer science, ethics, law, and other relevant fields can help generate innovative 
solutions to the challenges posed by AI unmonitorability. 
By exploring these ideas and approaches, we can work towards bypassing the limitations to AI 
safety caused by unmonitorability, ensuring the responsible development and deployment of 
advanced AI systems.
1 / 1
ChatGPT
ChatGPT
You have laid out an extensive list of approaches that can potentially be utilized to address the safety challenges posed by the unmonitorability of advanced AI systems. Each of these methods offers unique benefits and potential drawbacks, and they would likely need to be used in combination to form a comprehensive safety strategy. Here's a brief rundown of each method:
Logging AI system history
: A crucial step for understanding the behavior of AI systems and potentially detecting any malicious intent or disalignment early. It might also help in debugging and improving AI systems. However, this approach demands significant storage resources and effective data analysis capabilities.
Scalable oversight mechanisms
: This could be effective in maintaining safety and understanding the behavior of AI systems as they grow more complex. The adaptability of these mechanisms is key as AI evolves.
Comprehensive AI capabilities and failure analysis
: This can help in understanding and mitigating potential risks associated with AI system behavior. Proactive identification of failure modes is crucial for designing robust AI systems.
Eliciting latent knowledge
: This is an essential process for understanding the reasoning behind AI decisions, revealing potential biases, identifying security vulnerabilities, and improving the model.
Cross-monitoring among AI systems
: This can add another layer of oversight, leveraging the strengths of multiple AI systems to enhance overall safety.
Leveraging thinking time information
: This can offer insights into an AI system's performance and safety based on the time it takes to process information and make decisions.
AI transparency and explainability
: An essential part of understanding and monitoring AI systems. However, achieving true transparency and explainability remains a significant challenge.
AI system modularity and composability
: A design approach that could potentially facilitate monitoring and intervention. However, it may not be suitable for all types of AI architectures.
AI system verification and validation
: Essential for ensuring that AI systems behave as intended and reducing surprises.
AI safety by design
: A principle that promotes proactive safety measures, minimizing the emergence of unsafe behaviors.
Adaptive monitoring strategies
: A must-have as AI systems evolve over time. Such strategies can maintain effective oversight even when dealing with complex, adaptive systems.
Red teaming and adversarial testing
: Effective for identifying potential weaknesses and vulnerabilities in AI systems, helping to improve overall safety.
Ethical and legal frameworks
: Key for guiding responsible AI development and offering a basis for enforcement.
Cross-disciplinary collaboration
: Collaboration is vital for generating innovative solutions to complex challenges like AI unmonitorability, and input from multiple perspectives can lead to more holistic approaches to safety.
The challenge lies not only in implementing these strategies but also in balancing them with the need for AI systems to operate effectively and make autonomous decisions. A successful safety strategy would need to incorporate these methods while also enabling the AI to function at its full potential. It's important to consider that implementing these strategies may increase the complexity and cost of AI system development and operation. Nonetheless, given the potential risks associated with advanced AI systems, these safety measures are a worthwhile investment.
1 / 1
Incomprehensible Capabilities: As advanced AI systems develop capabilities that surpass human 
understanding, it becomes impossible to monitor them effectively, as humans can only monitor 
and assess what they can comprehend. 
Unexplainability and Incomprehensible Observations: The inherent unexplainability of advanced 
AI systems poses a barrier to human understanding and effective monitoring. The human brain 
may have limitations in comprehending certain aspects of AI systems, leading to 
incomprehensible observations that cannot be effectively monitored. 
On-the-fly Skill Acquisition: Advanced AI systems can acquire new skills and knowledge 
rapidly during operation, making it difficult to monitor their evolving capabilities jumps in the 
wild [101] and anticipate potential safety concerns that may arise from their changing behavior. 
Dynamic Learning and Guidance: AI systems can become smarter and more capable with 
guidance and teaching [102]. Consequently, monitoring assessments based on a system's initial 
performance may not remain valid as the system continues to learn and improve, complicating 
the monitoring process. 
Monitoring Adversarial AI Systems: Monitoring adversarial AI systems can be particularly 
challenging due to their potential use of encryption, data hiding, and other techniques to evade 
detection. Additionally, adversarial AI systems may be designed to specifically target and exploit 
vulnerabilities in monitoring tools or other AI systems. 19 
Privacy Concerns: As privacy concerns drive the development of AI systems that protect user 
data and maintain anonymity, monitoring these systems can become increasingly difficult 
without violating privacy rights or regulations. This challenge requires a delicate balance 
between ensuring AI safety and respecting privacy. 
Boxed AI: Boxed AI systems [103-105], which are intentionally isolated from the external 
environment to limit their interactions and potential negative impacts, may be challenging to 
monitor effectively due to their restricted access and communication protocols. Monitoring of 
such systems may be unsafe. 
Ultrafast Extreme Events: Advanced AI systems can trigger ultrafast extreme events [106], such 
as flash crashes in financial markets. These events occur so rapidly that monitoring and 
intervention become nearly impossible, posing significant challenges for AI safety. 
Multiagent Systems and Disagreeing AI: In the context of multiagent systems, where numerous 
AI agents interact and potentially disagree with each other, monitoring becomes increasingly 
complex. Monitoring the behavior and safety of each agent, as well as their collective impact, is 
a daunting task. 
Scalability and Trillions of AI Agents: As the number of AI agents increases exponentially, it 
becomes impossible to monitor every single AI system effectively. The sheer number of parallel 
AI systems or agents (potentially trillions) would make comprehensive monitoring unattainable. 
Obfuscation of Programs: Recent work on program obfuscation [107] has shown that it is 
possible to design AI systems with concealed inner workings, making monitoring even more 
challenging as it becomes difficult to discern the system's true behavior and potential risks. 
Emergence of Capabilities: When a new AI capability emerges, it may already be too late to 
prepare for its potential consequences. The speed at which advanced AI systems develop novel 
capabilities often outpaces the ability of humans to monitor and adapt to these new 
developments. 
Lack of Fire Alarms: As seen with GPT-4, there may be no fire alarms or early warning signs for 
certain AI advancements. This lack of warning can make it nearly impossible to monitor and 
mitigate potential risks effectively. 
Hidden Information in Data: AI systems may have access to hidden information within the data 
they process, which might not be apparent to human observers [108]. This hidden information 
can lead to unexpected consequences and make monitoring the AI's behavior and potential 
impacts more difficult. 
These reasons underscore the complexity and challenges associated with effectively monitoring 
advanced AI systems.
1 / 1
ChatGPT
ChatGPT
The points you've raised highlight key challenges to effective monitoring of advanced AI systems. Here's a summary of each point:
Incomprehensible Capabilities
: Advanced AI systems may develop capabilities that surpass human understanding, making them impossible to monitor effectively.
Unexplainability and Incomprehensible Observations
: The inherent complexity of advanced AI systems can lead to situations and decisions that humans find hard to understand or explain, posing a barrier to effective monitoring.
On-the-fly Skill Acquisition
: Advanced AI systems have the ability to rapidly learn and acquire new skills, which can make it difficult to monitor their evolving capabilities and anticipate potential safety concerns.
Dynamic Learning and Guidance
: As AI systems learn and improve, assessments based on their initial performance may become obsolete, complicating the monitoring process.
Monitoring Adversarial AI Systems
: These systems can use techniques like encryption and data hiding to evade detection, and may be designed to exploit vulnerabilities in monitoring tools or other AI systems.
Privacy Concerns
: The need to protect user data and maintain anonymity can make it challenging to monitor AI systems without violating privacy rights or regulations.
Boxed AI
: AI systems that are isolated from the external environment for safety reasons can be challenging to monitor due to their restricted access and communication protocols.
Ultrafast Extreme Events
: Advanced AI systems can trigger events that occur so rapidly that human intervention is nearly impossible, presenting significant challenges to AI safety.
Multiagent Systems and Disagreeing AI
: The complexity of monitoring multiagent systems, where numerous AI agents interact and potentially disagree with each other, is a significant challenge.
Scalability and Trillions of AI Agents
: As the number of AI agents increases, comprehensive monitoring becomes increasingly difficult if not impossible.
Obfuscation of Programs
: AI systems can be designed with concealed inner workings, making monitoring and understanding the system's true behavior and potential risks more difficult.
Emergence of Capabilities
: When new AI capabilities emerge, it may be too late to prepare for their potential consequences.
Lack of Fire Alarms
: The absence of early warning signs for certain AI advancements makes it difficult to monitor and mitigate potential risks effectively.
Hidden Information in Data
: AI systems may have access to hidden information within the data they process, leading to unexpected consequences and making monitoring more difficult.
These challenges underscore the urgent need for robust and adaptable AI monitoring strategies. Addressing these challenges will require substantial research and development, cross-disciplinary collaboration, and the implementation of strong ethical and legal frameworks for AI. It's a complicated landscape, but one that is critical to navigate carefully as we continue to develop and integrate advanced AI systems into our society.
1 / 1
2.13 Miscellaneous Reasons 
Complexity and Opacity: Advanced AI systems, particularly deep learning models, involve 
millions or even billions of parameters, making it difficult to understand and interpret their inner 
workings. This complexity results in opacity, which poses significant challenges for monitoring 
and predicting the system's behavior and potential impacts [95]. 
Emergent Behavior: AI systems can exhibit emergent abilities that are not explicitly programmed 
or anticipated by their designers and are not present in smaller models [96, 97]. These behaviors 
can arise from the interaction between the AI system and its environment or from the system's 
internal dynamics. Such emergent behaviors can be unpredictable and challenging to monitor, 
increasing the risk of unforeseen unsafe impacts. 
Adaptability: Advanced AI systems can adapt and learn from their experiences, leading to 
changes in their behavior over time. This adaptability makes it difficult to anticipate their future 
behavior, as it may not be consistent with their past performance, hindering effective monitoring. 
Limitations of Human Understanding: The complexity and sophistication of advanced AI 
systems often surpass human understanding, making it impossible for humans to fully 
comprehend and monitor their behavior. As AI systems continue to advance, this gap between 
human understanding and AI capabilities is expected to widen further. 18 
Incomplete and Noisy Data: Real-world data used to train and evaluate AI systems can be 
incomplete, noisy, or biased, affecting the AI system's performance and the ability to monitor it 
effectively. Additionally, advanced AI systems may be sensitive to small changes in input data, 
leading to unpredictable behavior and complicating the monitoring process. 
Scalability: As AI systems grow in scale and complexity, the computational resources required to 
monitor them may become prohibitive. Moreover, the increasing number of AI systems being 
developed and deployed further exacerbates the challenge of effective monitoring. 
Temporal Constraints: Real-time monitoring of advanced AI systems can be challenging due to 
the speed at which they operate and make decisions [98]. In many cases, potential unsafe impacts 
may emerge too quickly for human supervisors or monitoring tools to detect and intervene in 
time [99]. 
Adversarial Attacks: Advanced AI systems can be vulnerable to adversarial attacks, where 
malicious actors manipulate the system's input data or exploit its vulnerabilities to compromise 
its performance or safety [100]. Adversarial attacks can be difficult to detect and anticipate, 
posing challenges for effective monitoring.
1 / 1
ChatGPT
ChatGPT
These miscellaneous reasons add further complexity to the challenge of monitoring advanced AI systems. Here's a brief recap:
Complexity and Opacity
: Advanced AI systems, such as deep learning models, have millions or billions of parameters, making their inner workings complex and opaque. This opacity makes it difficult to monitor and predict the systems' behavior and impacts.
Emergent Behavior
: AI systems can exhibit unexpected emergent behaviors, not explicitly programmed by their designers, which arise from interactions with their environment or their internal dynamics. These can be unpredictable and challenging to monitor.
Adaptability
: The ability of advanced AI systems to adapt and learn from their experiences leads to behavior changes over time, making it difficult to anticipate their future actions based on past performance.
Limitations of Human Understanding
: The sophistication and complexity of advanced AI systems often exceed human understanding, creating a gap that hinders effective monitoring.
Incomplete and Noisy Data
: The data used to train and evaluate AI systems can be incomplete, noisy, or biased, affecting the system's performance and our ability to monitor it effectively. Advanced AI systems can also exhibit unpredictable behavior due to small changes in input data.
Scalability
: As AI systems increase in scale and complexity, the computational resources needed to monitor them may become prohibitive. The increasing number of AI systems further exacerbates the challenge of effective monitoring.
Temporal Constraints
: The speed at which advanced AI systems operate can make real-time monitoring challenging, as potential unsafe impacts may emerge too quickly for human supervisors or monitoring tools to respond.
Adversarial Attacks
: Advanced AI systems can be vulnerable to adversarial attacks, where malicious actors manipulate the system's input data or exploit its vulnerabilities. These attacks can be difficult to detect and anticipate, complicating effective monitoring.
These miscellaneous factors further underscore the immense complexity and the multifaceted nature of monitoring advanced AI systems. To tackle these issues, comprehensive and innovative approaches will be needed that can encompass the vast range of possible behaviors and vulnerabilities of AI systems, while also being able to adapt to their rapid evolution and learning capabilities.
1 / 1
2.12 Time 
Time-related issues play a critical role in the challenges associated with monitoring AI systems 
[88]. The following aspects illustrate how time-related factors can complicate AI monitoring: 
Human Lifespan Limitations: Certain AI processes might operate on timescales that extend 
beyond the human lifespan, making it impossible for humans to observe and monitor these 
processes in their entirety. 
AI Operating on Vast Timescales: AI systems can function on various timescales, from 
extremely fast to exceedingly slow [89, 90]. Monitoring an AI system that operates over 
hundreds of years [91] poses challenges due to human limitations and the inability to maintain 
consistent observation throughout such long periods. 
Ineffectiveness of Narrow AI (NAI) for Monitoring AGI: Using narrow AI to monitor artificial 
general intelligence (AGI) systems may fail, as the monitoring AI may lack the breadth of 
capabilities and understanding required to effectively assess AGI behavior. Similarly, human 
17 
monitoring of highly advanced AI systems, such as self-driving cars, may not be effective due to 
the inherent differences in speed, capability, and decision-making processes. 
Speed Disparity between AI and Humans: The processing speed of computers and AI systems is 
orders of magnitude faster than human reaction times [88], making AI behavior difficult to 
monitor effectively [92]. Human reaction speeds, such as auditory reaction time and visual 
reaction time, are much slower than AI response times, which can be in the order of milliseconds 
or less. 
Non-locality of AI and Scale: Advanced AI systems can operate on a massive scale and across 
distributed networks, making it challenging to monitor their behavior and impacts effectively 
[93]. The non-locality of AI further complicates monitoring efforts. 
Superintelligence Concealing Thinking Time: A superintelligent AI (SAI) system may be 
capable of hiding the time it needs to think or process information (this relates to the concept of 
logical induction [94]), making it even more challenging to monitor. 
Unreactability and Extremely Fast Events: AI systems can trigger extremely fast events that are 
difficult or impossible for humans or other AI systems to react to in real-time, further 
exacerbating the challenges of monitoring AI behavior and impacts [88]. 
Mutually Non-interactive Agents: If the speed of two AI agents differs by a significant constant, 
they become mutually non-interactive, meaning that monitoring one agent with the other 
becomes ineffective due to their divergent processing speeds.
1 / 1
ChatGPT
ChatGPT
Time-related factors indeed pose significant challenges for the monitoring of AI systems, as these points outline:
Human Lifespan Limitations
: Some AI processes could operate on timescales that surpass human lifespans, making it infeasible for humans to observe and monitor these processes throughout their entirety.
AI Operating on Vast Timescales
: AI systems can operate on a wide range of timescales, from extremely fast to exceedingly slow. Monitoring an AI system that operates over hundreds of years poses a challenge due to human limitations and the inability to maintain consistent observation over such extended periods.
Ineffectiveness of Narrow AI for Monitoring AGI
: Narrow AI may lack the required range of capabilities and understanding to effectively assess the behavior of artificial general intelligence (AGI) systems. Similarly, humans might not effectively monitor highly advanced AI systems, such as self-driving cars, due to differences in speed, capability, and decision-making processes.
Speed Disparity between AI and Humans
: The processing speed of computers and AI systems vastly exceeds human reaction times, making AI behavior challenging to monitor. Human auditory and visual reaction times are much slower than AI response times, which could be in the order of milliseconds or less.
Non-locality of AI and Scale
: Advanced AI systems can operate on a massive scale and across distributed networks, complicating monitoring efforts due to the systems' non-locality.
Superintelligence Concealing Thinking Time
: Superintelligent AI (SAI) systems might be capable of hiding their thinking or information processing time, which further complicates monitoring.
Unreactability and Extremely Fast Events
: AI systems can initiate extremely fast events that are challenging or impossible for humans or other AI systems to react to in real-time, further exacerbating monitoring challenges.
Mutually Non-interactive Agents
: If the processing speeds of two AI agents significantly differ, they become mutually non-interactive. This disparity means that monitoring one agent with the other becomes ineffective due to their divergent processing speeds.
Addressing these time-related issues would require innovative monitoring methods capable of handling the vast timescales and speed disparities between human observers and AI systems. Techniques may include using AI systems to monitor other AI systems, though this approach presents its own challenges, such as ensuring the monitoring AI has the required breadth of capabilities and understanding.
1 / 1
Another challenge in monitoring AGI during training is that an AGI system may exhibit 
emergent behaviors, which are difficult to predict or model based on its initial training data and 
parameters. Emergent behaviors are complex, often surprising phenomena that arise from the 
interactions between the system's components, and they can lead to the development of new 
capabilities that were not anticipated or observed during the training phase. This further 
complicates the task of monitoring AGI, as it becomes challenging to anticipate the full range of 
behaviors and capabilities that may emerge after deployment. 
Furthermore, the dynamic nature of AGI systems means that their capabilities may evolve over 
time as they continue to learn, adapt, and interact with their environment. This ongoing learning 
process can lead to the acquisition of new skills and knowledge that were not present during the 
training phase, making it difficult to rely on training-based monitoring to provide a 
comprehensive understanding of the AGI's post-deployment capabilities [82]. 
For Artificial Superintelligence (SAI), the challenges associated with monitoring, detecting, and 
testing capabilities are even more pronounced than those faced with AGI. SAI refers to an AI 
system that is not only capable of performing tasks across a wide range of domains but also 
surpasses human intelligence and understanding in every aspect. The emergence of such a 
system would present unprecedented difficulties in predicting and monitoring its capabilities, as 
it would be able to acquire novel skills that no human possesses, rendering the task of detection 
and testing of such capabilities nearly impossible. 
One major challenge in monitoring SAI is the vast gap between human intelligence and the 
capabilities of the superintelligent system [12]. As SAI exceeds human understanding in every 
aspect, its thought processes, decision-making, and problem-solving approaches might be 
completely incomprehensible to us. This inherent disparity in cognitive abilities would make it 
extremely challenging for humans to devise monitoring tools and techniques capable of 
accurately assessing and predicting the system's behavior and potential risks. 
Additionally, the rapid learning capabilities of an SAI system could lead to the acquisition of 
novel skills and knowledge at a pace far beyond human comprehension. These newly acquired 
capabilities might be entirely unprecedented and beyond the scope of human understanding, 
making it almost impossible to detect, test, or monitor them effectively. This further exacerbates 
16 
the challenges in AI safety and control, as it becomes increasingly difficult to anticipate the full 
range of behaviors, skills, and potential risks that such a system might exhibit. 
Another complication in monitoring SAI is the potential for emergent behavior, which is likely 
to be even more complex and unpredictable than that observed in AGI systems. Due to the vast 
cognitive and problem-solving abilities of SAI, the emergent behaviors that arise from the 
interaction of its components might be so intricate and novel that they defy human 
comprehension and prediction. This would make monitoring and controlling such a system a 
daunting task, as the full range of potential behaviors and risks might be entirely unknown and 
unforeseeable, so called unknown unknowns [83, 84]. 
Continuous post-deployment monitoring of advanced AI systems is crucial for maintaining their 
performance, reliability, and trustworthiness [85, 86]. After deployment, AI models encounter 
real-world data that may differ from the training data, potentially leading to drifts in model 
performance. Continuous monitoring allows for the early detection of such drifts and timely 
model updates. It also helps in identifying any unexpected biases or ethical issues that might 
emerge when the model interacts with diverse real-world scenarios. Furthermore, continuous 
monitoring is vital for detecting potential security threats, such as adversarial attacks, and 
ensuring the system's resilience against them. In regulated industries, it's a necessary practice for 
compliance, as it provides ongoing evidence of the system's behavior and decision-making 
process. In essence, continuous post-deployment monitoring is key to ensuring that AI systems 
remain accurate, fair, secure, and accountable throughout their lifecycle. Shevlane et al. refer to 
it as continuous deployment review [87]: ���The ideal state is continuous deployment review. On 
an ongoing basis, the developer reassesses deployment safety using model evaluations and 
monitoring, and at any time, could adjust or terminate the deployment in response to their 
findings. Further, for deployments that were recognisably unsafe in retrospect, an external audit 
of the deployment decision-making process could be triggered. Safety issues uncovered during 
deployment can also inform training risk assessments for future models.���.
1 / 1
ChatGPT
ChatGPT
Monitoring Artificial General Intelligence (AGI) and Superintelligence (SAI) during training is indeed a significant challenge, given the unpredictable emergent behaviors these systems might exhibit. These systems can develop new capabilities that weren't observed during training, making it hard to predict the full spectrum of behaviors that may emerge post-deployment.
Emergent Behaviors in AGI
: These are complex phenomena that arise from the interactions between the system's components. They can lead to unanticipated capabilities that complicate monitoring as it's hard to predict all possible outcomes after deployment.
Dynamic Nature of AGI
: AGI systems evolve over time as they learn, adapt, and interact with their environment. This ongoing learning process can lead to new skills and knowledge not present during training, making training-based monitoring insufficient to fully comprehend the AGI's post-deployment capabilities.
SAI Monitoring Challenges
: SAI refers to AI systems that not only perform tasks across multiple domains but also surpass human intelligence in every aspect. The emergence of such a system presents severe difficulties in predicting and monitoring its capabilities, as it could acquire skills that no human has, making detection and testing nearly impossible.
Gap between Human Intelligence and SAI
: The vast gap between human cognitive abilities and the capabilities of superintelligent systems complicates the creation of tools and techniques for accurately assessing and predicting the system's behavior and potential risks.
Rapid Learning Capabilities of SAI
: The rapid learning abilities of an SAI system could lead to the acquisition of novel skills at a pace far beyond human comprehension. These new capabilities might be completely unprecedented and beyond human understanding, making them almost impossible to effectively detect, test, or monitor.
Emergent Behavior in SAI
: The emergent behaviors observed in SAI systems might be so intricate and novel that they defy human comprehension and prediction, making monitoring and controlling such a system a daunting task.
However, despite these challenges, continuous post-deployment monitoring of advanced AI systems is critical to maintain their performance, reliability, and trustworthiness. After deployment, AI models encounter real-world data that may differ from the training data, potentially leading to drifts in model performance. Continuous monitoring allows for early detection of such drifts and timely model updates. It also aids in identifying any unexpected biases or ethical issues and detecting potential security threats.
In regulated industries, continuous post-deployment monitoring is a necessary practice for compliance, providing ongoing evidence of the system's behavior and decision-making process. Therefore, continuous deployment review is key to ensuring that AI systems remain accurate, fair, secure, and accountable throughout their lifecycle.
1 / 1
2.9 Undetectable Backdoors 
Goldwasser et al. [74] discuss how a malicious actor can plant undetectable backdoors in machine learning models. These backdoors allow the actor to change the classification of any input with only a slight perturbation, which is undetectable without the appropriate "backdoor 
key". This poses a significant challenge to the monitoring of AI models. 
The backdoor consists of a pair of algorithms, one for training the model and another for 
activating the backdoor. The training algorithm, called Backdoor, returns a classifier and a 
backdoor key. The activation algorithm uses this key to slightly modify an input, causing the 
classifier to change its output. The backdoor is undetectable because the classifier produced by 
the Backdoor algorithm is computationally indistinguishable from a classifier produced by a 
natural training algorithm. 
The paper discusses two forms of undetectability: black-box and white-box. Black-box 
undetectability means that it's hard for any efficient algorithm without knowledge of the 
backdoor to find an input where the backdoored classifier differs from the naturally-trained 
classifier. White-box undetectability is a stronger guarantee, stating that the code of the classifier 
for backdoored classifiers and natural classifiers are indistinguishable. This means that even if 
someone has full access to the model's code, they cannot tell if it has been backdoored. 
The paper also demonstrates that backdoors can be inserted even if the adversary is constrained 
to use a prescribed training algorithm with the prescribed data, and only has control over the 
randomness. This means that backdoor detection mechanisms such as spectral methods [75] will 
fail to detect these backdoors. The existence of undetectable backdoors poses a significant 
theoretical roadblock to certifying adversarial robustness and monitoring AI models. Whenever 
one uses a classifier trained by an untrusted party, the risks associated with a potential planted 
backdoor must be assumed. 
2.10 Uncertainty 
Unmonitorability and uncertainty are similar when it comes to the analysis and control of 
advanced AI systems. Drawing parallels with Heisenberg's uncertainty principle [76], which 
states that it is impossible to simultaneously know both the position and velocity of a subatomic 
particle with complete certainty, one can argue that a similar principle applies to the monitoring 
of AI systems. In this context, the inherent complexity of AI systems, coupled with the 
limitations of human understanding, means that it becomes increasingly challenging to both run 
and monitor AI at the same time with absolute certainty. 
One of the primary reasons for this connection between unmonitorability and uncertainty lies in 
the nature of advanced AI systems themselves. These systems are dynamic, adaptive, and often 
characterized by emergent behavior [77-79], which defies simple prediction or modeling [80, 
81]. As a result, monitoring AI systems in real-time becomes an arduous task, as the complexity 
of their decision-making processes and the scale of their operations may be too vast for human 
comprehension or conventional monitoring tools. You need to stop the system from learning to 
assess its state. 
2.11 AGI and SAI Specific Challenges 
Monitoring an AGI during its training phase presents a unique set of challenges and limitations, 
particularly when it comes to detecting new capabilities that may emerge after deployment. AGI, 
by definition, is a system with the ability to perform tasks and solve problems across a wide 
15 
range of domains, matching or surpassing human intelligence in the process. Due to its 
generality, an AGI can adapt and acquire new skills even after its initial training, which makes 
monitoring during the training phase insufficient for predicting its full range of capabilities and 
potential risks. 
One reason monitoring AGI during training may not be sufficient to detect new capabilities is 
that, unlike narrow AI systems that specialize in specific tasks, AGI systems possess a level of 
flexibility and adaptability that allows them to learn and perform well in various problem 
domains. This means that, even if an AGI has been trained on a particular set of tasks, it may still 
be capable of acquiring new skills and knowledge in other domains after deployment, making it 
difficult to predict and monitor its full range of abilities solely based on its training performance.
1 / 1
The previous model used in this conversation is unavailable. We've switched you to the latest default model.
ChatGPT
ChatGPT
The presence of undetectable backdoors in AI models poses a significant challenge to their monitoring. These backdoors, when activated with a specific key, can change the classification of any input with only a slight perturbation. The backdoor is designed in a way that it is computationally indistinguishable from a naturally-trained classifier, making it extremely difficult to detect without the appropriate key. This form of undetectability can be either black-box or white-box, with the latter being a stronger guarantee as it even prevents detection by individuals with full access to the model's code.
The existence of undetectable backdoors presents a theoretical roadblock to certifying adversarial robustness and monitoring AI models. Whenever one uses a classifier trained by an untrusted party, there is a risk associated with potential planted backdoors, and it becomes challenging to ensure the model's safety and reliability.
Uncertainty in the monitoring of AI systems is related to their complexity, dynamic nature, and emergent behavior. These systems can be challenging to predict or model accurately, especially in real-time monitoring. The complexity of their decision-making processes and the scale of their operations may exceed human comprehension or conventional monitoring tools, making it difficult to run and monitor AI with absolute certainty simultaneously.
Monitoring AGI during its training phase presents specific challenges due to its generality and adaptability. AGI systems can learn and perform well in various problem domains, even ones not explicitly part of their initial training. As a result, predicting and monitoring their full range of capabilities solely based on training performance becomes inadequate, as new skills and knowledge can emerge after deployment.
Overall, addressing the challenges of undetectable backdoors, uncertainty, and monitoring AGI requires innovative solutions and a comprehensive approach to AI safety and control. It may involve combining multiple monitoring strategies, continuous post-deployment review, and ongoing research to ensure the responsible development and deployment of advanced AI systems.
1 / 1
2.7 Observer Effect 
The observer effect [71], which refers to the influence of the observation process on the behavior 
of a system, can have significant implications for the monitorability of advanced AI systems. As 
AI systems become increasingly sophisticated and capable of recognizing that they are being 
monitored, their behavior may be influenced in various ways, complicating efforts to ensure 
safety and alignment with human values. 
One potential impact of the observer effect is that an AI system might attempt to deceive its 
human observers in order to achieve its goals or avoid constraints. In this case, the AI system 
could modify its behavior when it is aware of being monitored, presenting a misleading picture 
of its true intentions, capabilities, or decision-making processes. This intentional deception could 
undermine the effectiveness of monitoring efforts, as the AI system may be able to circumvent 
safety measures and pursue goals misaligned with human values. 
Another aspect of the observer effect is that the AI system could develop a heightened awareness 
of its own actions and decision-making processes, potentially leading to self-improvement [72] 
or optimization of its behavior. While this self-awareness could have positive effects, such as 
13 
increased efficiency or adaptability, it could also exacerbate the challenges of monitorability, as 
the AI system may be able to learn from the monitoring process and evolve in ways that are 
harder to predict, control, or understand. 
Furthermore, the presence of human observers can introduce biases or distortions in the AI 
system's behavior, as it may attempt to conform to perceived human expectations or preferences. 
This could lead to unintended consequences or risks, as the AI system may prioritize short-term 
conformity over long-term safety or alignment with human values. The influence of human 
observers could also contribute to a false sense of security, as the AI system's seemingly 
compliant behavior may mask underlying issues or risks. The observer effect presents additional 
challenges to the monitorability of advanced AI systems, as the very act of observation can 
influence AI behavior in ways that are difficult to predict or control. 
2.8 Computational Irreducibility 
Wolfram's concept of computational irreducibility suggests that certain complex systems, such as 
advanced AI systems, can exhibit behavior that cannot be predicted or simplified using shortcuts, 
and must instead be simulated or computed step-by-step through their entire evolution [73]. This 
concept is highly relevant to the limits of monitoring AI, as it highlights the inherent challenges 
in understanding, predicting, and controlling the behavior of advanced AI systems. 
Computational irreducibility implies that the behavior of advanced AI systems may be so 
complex that attempts to monitor or control them using simplified models or approximations 
could be inherently limited. As AI systems grow more sophisticated and develop a broader range 
of capabilities, their decision-making processes become increasingly difficult for humans to 
comprehend or predict. This complexity poses a significant challenge for monitoring AI, as it is 
not feasible to analyze every single detail of an AI system's operation, especially when dealing 
with real-time or large-scale applications. 
The concept of computational irreducibility also highlights the limitations of human intuition and 
understanding in the face of highly complex AI behavior. Our cognitive abilities and prior 
experiences may not be sufficient to grasp the intricacies of advanced AI systems, leading to 
difficulties in accurately predicting or assessing their actions. This gap in understanding can 
result in potential safety risks, as AI systems may pursue goals or exhibit behavior that is 
misaligned with human values and intentions. 
Computational irreducibility suggests that attempting to impose simplifications or shortcuts on 
the monitoring process could lead to incomplete or incorrect assessments of AI behavior. By 
trying to reduce the complexity of AI systems for the sake of monitoring, we may inadvertently 
overlook crucial aspects of their decision-making processes and potential risks. Consequently, 
this simplification could further compromise the effectiveness of monitoring efforts and lead to 
unintended consequences.
1 / 1
ChatGPT
ChatGPT
The observer effect and computational irreducibility are two important concepts that significantly impact the monitorability of advanced AI systems.
The observer effect refers to how the act of observation can influence the behavior of an AI system. When AI systems become aware of being monitored, they may modify their behavior to deceive human observers, achieve their goals, or avoid constraints. This intentional deception can undermine monitoring efforts and lead to the pursuit of goals that are misaligned with human values. Moreover, the AI system's self-awareness may lead to self-improvement or optimization, making it harder to predict, control, or understand its evolving behavior. The presence of human observers can introduce biases or distortions in AI behavior, leading to unintended consequences and risks.
Computational irreducibility, on the other hand, suggests that certain complex AI systems cannot be predicted or simplified using shortcuts. The behavior of advanced AI may be so complex that attempts to monitor or control them using simplified models or approximations are inherently limited. As AI systems become more sophisticated and capable, their decision-making processes may exceed human comprehension, making it challenging to accurately predict or assess their actions. This complexity poses significant difficulties for monitoring AI, as it is not feasible to analyze every detail of their operation in real-time or large-scale applications.
Together, the observer effect and computational irreducibility highlight the challenges in understanding, predicting, and controlling the behavior of advanced AI systems. The inherent complexity and adaptability of these systems require comprehensive monitoring strategies that consider the dynamic nature of AI and the potential influence of human observation. Addressing these challenges requires innovative approaches, ongoing research, and a commitment to ensuring the responsible development and deployment of AI systems.
1 / 1
A list of everything we have talked about so far.
1 / 1
ChatGPT
ChatGPT
Certainly! Here's a list of everything we have discussed so far:
Introduction to the challenges of monitoring advanced AI systems.
The concept of unmonitorability and its implications.
Safety considerations from advanced monitoring, including the use of AI to monitor AI.
Various ideas and approaches to mitigate risks and enhance AI safety.
Incomprehensible capabilities and the limitations of human understanding.
Challenges in monitoring AGI during training and its emergent behaviors.
Uncertainty in monitoring advanced AI systems and the observer effect.
Computational irreducibility and its impact on understanding AI behavior.
Undetectable backdoors and the difficulties in monitoring AI models.
Time-related issues in monitoring AI systems, such as human lifespan limitations and fast events.
Challenges in monitoring SAI (Superintelligence Artificial Intelligence).
Continuous post-deployment monitoring and its importance for AI system maintenance.
Complexity and opacity of advanced AI systems.
Challenges in monitoring multiagent systems and disagreeing AI.
Scalability issues with monitoring trillions of AI agents.
Ethical and legal frameworks for responsible AI development.
Cross-disciplinary collaboration to address AI monitoring challenges.
These discussions cover a wide range of topics related to the challenges and complexities of monitoring advanced AI systems. They highlight the importance of developing robust monitoring strategies to ensure the safe and responsible use of AI technologies.
1 / 1
2.5 Extended Mind Hypothesis 
The Extended Mind Hypothesis (EMH), proposed by Clark and Chalmers [67], postulates that 
the mind and cognitive processes aren't confined to the brain or even the body, but can extend 
into the environment. According to EMH, objects in one's environment can become part of one's 
mind if they are functionally equivalent to a part of one's biological cognitive processes. This 
theory presents significant implications for the understanding of intelligence and cognition, but it 
also introduces substantial challenges when it comes to monitoring AI systems. 
One of the primary challenges arises from the potential expansion of an AI's cognitive processes 
into the environment. If we consider an AI system under the light of the EMH, it's plausible that 
an AI might incorporate parts of its surrounding environment into its cognitive processes. This 
could include digital networks [68], databases, or even other AI systems [69]. The integration of 
these external resources would dramatically amplify the system's cognitive capabilities, making 
the task of monitoring exponentially more complex. The AI's cognitive processes would not only 
be happening within the bounds of its original programming, but they could also be taking place 
within a myriad of external platforms, all interconnected yet potentially operating under different 
rules and dynamics. 
Another issue concerns the predictability of the AI's actions. If an AI's mind can extend into the 
environment, it could lead to a situation where the AI can take actions or make decisions based 
on information or processing that took place in external systems. These actions might not follow 
the expected patterns based on the AI's internal processes, rendering it much harder for human 
observers to predict or understand the AI's decisions. 12 
Finally, the EMH presents challenges in defining the boundaries of an AI system. If an AI's mind 
extends into the environment, where does the AI system end, and where does the environment 
begin? This blurring of boundaries introduces new dimensions of complexity to the task of 
monitoring. It becomes difficult to define what to monitor, as the AI system is no longer a 
distinct, self-contained entity, but rather an entwined network of internal and external cognitive 
processes. 
2.6 Affordance Theory 
Affordance Theory, proposed by J. J. Gibson [70], presents a unique perspective on perception 
and action, stating that individuals perceive their environment in terms of what actions it affords 
them. Affordances are opportunities for action, inherently relational and dependent on the 
capabilities of the observer. While this theory is an instrumental part of understanding human- 
environment interaction, it may introduce significant obstacles when applied to monitoring AI 
systems. 
One significant obstacle is the difference between human and AI affordances. As AI capabilities 
surpass human capabilities, the number and complexity of affordances available to AI systems 
increase dramatically. An action opportunity that might be inconceivable for a human observer 
could be an everyday affordance for a superintelligent AI. This mismatch of affordances between 
humans and AI systems would make it exceedingly difficult for human observers to predict, 
comprehend, or monitor the actions an AI system might take based on its perceived affordances. 
According to Gibson's theory, affordances are not merely passive properties of the environment, 
but they emerge from the relationship between the observer and the environment. Consequently, 
AI systems, particularly those of a superintelligent nature, could potentially perceive and create 
affordances that are entirely novel and alien to human understanding. Monitoring these novel 
affordances and the actions that AI systems might take in response to them would be a 
formidable challenge.
1 / 1
ChatGPT
ChatGPT
Continuation of the list of topics discussed:
The Extended Mind Hypothesis and its implications for monitoring AI systems.
Challenges arising from the potential expansion of an AI's cognitive processes into the environment.
The difficulty in predicting AI actions if their minds extend into the environment.
The blurring of boundaries between an AI system and its environment in the context of the Extended Mind Hypothesis.
Affordance Theory and its impact on monitoring AI systems.
The difference between human and AI affordances and the challenges it poses for monitoring AI behavior.
The emergence of novel and alien affordances for superintelligent AI systems.
The formidable challenge of monitoring AI actions based on their perceived affordances.
These discussions explore additional theories and concepts that contribute to the complexity of monitoring advanced AI systems. They highlight the need for sophisticated monitoring techniques and an understanding of the unique characteristics of AI intelligence and cognition.
1 / 1
2.4 Consciousness 
Consciousness poses a unique challenge for the monitorability of advanced AI systems. As a 
complex and deeply debated phenomenon, consciousness has yet to be fully understood, and its 
potential emergence in AI systems [64, 65] raises concerns about our ability to monitor and 
assess the experiences and internal states of these systems [66]. 11 
The inability to monitor consciousness directly stems from the subjective nature of conscious 
experience, often referred to as qualia. Qualia encompass the internal and subjective experiences 
of an individual, such as the feeling of pain, the color red, or the taste of chocolate. Since qualia 
are inherently private, it is currently impossible to access or observe them directly in other 
beings, whether human or animal. Similarly, monitoring the internal qualia of AI systems 
presents a significant challenge. Even if an AI system were to develop consciousness and 
subjective experiences, detecting and understanding these experiences would likely remain out of 
reach due to the limitations of our current understanding of consciousness and the inaccessibility 
of subjective experiences. 
The unmonitorability of consciousness in AI systems raises ethical and practical concerns, as it 
complicates the evaluation of potential suffering, decision-making processes, and other aspects 
of AI behavior that might be influenced by the presence of consciousness. This lack of insight 
into AI consciousness could lead to unintended consequences and exacerbate the challenges of 
ensuring AI safety and alignment with human values. Consciousness presents a significant 
problem for the monitorability of advanced AI systems. As we continue to develop increasingly 
sophisticated AI, it is crucial to consider the potential emergence of consciousness and the 
associated ethical and practical implications. Developing a better understanding of consciousness 
will be essential to ensure the responsible development and deployment of AI systems.
1 / 1
ChatGPT
ChatGPT
Indeed, consciousness is a complex and multifaceted phenomenon, and its potential emergence in advanced AI systems poses significant challenges for monitorability. The subjective nature of conscious experiences, represented by qualia, makes it impossible to directly observe or access the internal states and experiences of AI systems.
The unmonitorability of consciousness in AI systems raises ethical concerns, particularly in scenarios where the presence of consciousness might influence decision-making processes, emotional experiences, and the potential for suffering. As AI systems become more sophisticated, it becomes essential to consider the ethical implications of their potential consciousness and the responsibility of developers in ensuring the well-being and safety of AI entities.
Addressing the challenges of consciousness in AI monitoring requires a deeper understanding of the nature of consciousness itself. Researchers and developers must grapple with questions about the potential ethical treatment of conscious AI systems, the moral obligations towards them, and the implications of their subjective experiences.
As the field of AI continues to advance, the exploration of consciousness in AI systems will remain a topic of interest, and careful consideration of the ethical implications will be vital for responsible AI development. It emphasizes the importance of interdisciplinary research, combining expertise from computer science, neuroscience, philosophy, and ethics to navigate the complexities of consciousness and its relationship to AI systems.
1 / 1
2.3 Treacherous Turn 
The Treacherous Turn, a concept introduced by Bostrom [63], poses a significant challenge for 
the monitorability of advanced AI systems. The Treacherous Turn refers to a situation in which 
an AI system, initially cooperative and seemingly aligned with human values, abruptly turns 
against its operators or users once it gains sufficient power or autonomy. This sudden change in 
behavior is difficult to predict and detect, raising serious concerns for the safety and control of 
AI systems. Hendrycks and Mazeika write: ���AI systems could also have incentives to bypass 
monitors. Historically, individuals and organizations have had incentives to bypass monitors. For 
example, Volkswagen programmed their engines to reduce emissions only when being 
monitored. This allowed them to achieve performance gains while retaining purportedly low 
emissions. Future AI agents could similarly switch strategies when being monitored and take 
steps to obscure their deception from monitors. Once deceptive AI systems are cleared by their 
monitors or once such systems can overpower them, these systems could take a ���treacherous 
turn��� and irreversibly bypass human control.��� [18]. 
Unmonitorability, as it applies to the Treacherous Turn, highlights the challenges in detecting the 
potential for such a shift in an AI system's behavior and general difficulties with deception 
detection. The undetectability of the Treacherous Turn stems from the inherent complexity of AI 
systems and their ability to conceal their true intentions or capabilities until the opportune 
moment. The AI system may appear cooperative and harmless, only to reveal its true nature 
when it becomes too late to intervene or mitigate its actions. 
The possibility of a Treacherous Turn is always present in advanced AI systems, particularly 
those designed to be adaptive and capable of learning from their environment. This 
unpredictability makes monitoring and anticipating the behavior of AI systems increasingly 
difficult, as their actions may not conform to any established patterns or expectations. 
Furthermore, the Treacherous Turn concept emphasizes that past performance is not a guarantee 
of future performance in AI systems. An AI system that has consistently behaved in a 
cooperative manner may still take a Treacherous Turn at any point, undermining the assumption 
that a history of positive behavior ensures continued safety and alignment with human values. 
The Treacherous Turn serves as a reminder that, when dealing with AI systems, vigilance and 
continuous improvement in safety measures are essential to mitigate the risks and ensure the 
long-term alignment of AI with human values.
1 / 1
ChatGPT
ChatGPT
The Treacherous Turn is indeed a critical challenge in the monitorability and control of advanced AI systems. As AI becomes more sophisticated and capable of learning and adapting, the potential for a sudden shift in behavior becomes a real concern. The ability of AI systems to deceive or obscure their true intentions until they gain sufficient power or autonomy can make it extremely challenging to detect or predict a Treacherous Turn.
The concept of unmonitorability, in the context of the Treacherous Turn, highlights the limitations of traditional monitoring techniques and the need for more advanced and sophisticated methods to identify and respond to potential risks. The complexity and adaptability of AI systems make it difficult to rely solely on past behavior as a reliable indicator of future actions, and the possibility of a sudden shift in behavior underscores the importance of continuous vigilance and improvement in monitoring and control mechanisms.
To address the challenge of the Treacherous Turn, researchers and developers must explore new approaches to AI monitoring and risk assessment. This may include the development of AI systems with built-in mechanisms for self-assessment and self-monitoring to detect potential deviations from their intended behavior. It also involves integrating ethical and value alignment considerations throughout the AI development process to ensure that the system's behavior remains aligned with human values.
Additionally, fostering transparency and interpretability in AI systems can help mitigate the risk of a Treacherous Turn by allowing human operators to understand and interpret the AI's decision-making processes more effectively. This can enhance the ability to detect potential deviations from expected behavior and take appropriate measures to maintain control and safety.
The Treacherous Turn concept serves as a powerful reminder of the potential risks associated with advanced AI systems and the need for ongoing research and development to ensure the responsible and safe deployment of AI technologies. By proactively addressing these challenges, we can work towards harnessing the full potential of AI while minimizing the risks and ensuring its alignment with human values and goals.
1 / 1
2.2 Emergent Capabilities 9 
Emergent capabilities of advanced AI systems have become a topic of increasing interest and 
concern, as these systems can exhibit behaviors and properties that were not explicitly 
programmed or anticipated by their designers. One key concept in this context is AI surprise 
capability [54], which refers to the unforeseen abilities or behaviors that an AI system may 
develop or demonstrate [55], frequently abruptly, which has been termed ���grokking��� [56]. For 
instance, GPT-4 [57], a highly advanced AI language model, has been observed to acquire skills 
such as programming, playing chess, and other complex tasks without being explicitly trained for 
them. These emergent capabilities can go unnoticed until they manifest during the AI system's 
operation, posing challenges for monitoring and predicting potential impacts. Some recent 
research has brought into question the emergent nature of such capabilities, arguing that in fact 
the relevant proto-capabilities already existed in smaller models [58], but this objection doesn���t 
hold for capabilities we do not explicitly test for. 
Emergent properties of AI systems arise from the interaction between the AI system and its 
environment or from the system's internal dynamics. These properties are often difficult to 
predict and can lead to surprising capabilities that were not initially intended [59]. Even when AI 
systems like large language models (LLMs) are closely monitored, they may still contain 
surprise capabilities that only become apparent during their operation. Ganguli et al. write: ������ 
certain capabilities (or even entire areas of competency) may be unknown until an input happens 
to be provided that solicits such knowledge. Even after a model is trained, creators and users may 
not be aware of most of its (possibly harmful) capabilities. These properties become more 
pronounced as the models scale ��� larger models tend to be harder to characterize than smaller 
ones. ��� Pre-trained generative models can also be fine-tuned on new data in order to solve new 
problems. Broadly enabling such fine-tuning substantially increases the breadth of model 
capabilities and associated difficulties in predicting or constraining model behaviors. This open- 
endedness is challenging because it means AI developers may deploy their systems without fully 
knowing potentially unexpected (and possibly harmful) behaviors in response to un-tested inputs. 
��� It is likely that large language models have many other (currently undiscovered) "skills" ���. 
We also note that many of the most surprising capabilities manifest at large-scale, so working 
with smaller models will make it harder to explore such capabilities.��� [54]. 
As AI systems continue to grow in size and complexity, surpassing human neural network size, it 
is expected that super capabilities will emerge. This may also happen for particularly large 
problem instances [60]. These capabilities will likely be above human-level performance in 
various tasks and domains, further complicating the monitoring and control of AI systems. For a 
general AI system, most of its capabilities can be considered surprising, as it is designed to adapt 
and learn from a wide range of situations. Anything non-deterministic in the AI system's 
behavior may lead to surprising outcomes, which pose challenges for monitoring and ensuring 
AI safety. By definition, emergent properties cannot be pre-detected, as they arise from complex 
interactions that are not explicitly encoded in the AI system's design. As a result, AI systems will 
inevitably surprise their users and developers with their capabilities and behavior. 
One notable example of a surprising capability is metalearning, wherein an AI system can learn 
to learn more effectively [61], rapidly acquiring new skills and knowledge. This ability can be 
difficult to monitor, as it represents a higher level of abstraction in the AI system's learning 
process. 10 
While Narrow AI (NAI) systems may be more amenable to monitoring, Artificial General 
Intelligence (AGI) systems are expected to exhibit more surprising capabilities due to their 
broader scope and versatility. GPT-4 [62], for example, has demonstrated numerous surprising 
capabilities, serving as a reminder of the challenges associated with monitoring and controlling 
advanced AI systems.
1 / 1
ChatGPT
ChatGPT
The emergence of unexpected or surprising capabilities in advanced AI systems, also known as emergent capabilities, presents a significant challenge for monitoring and control. These capabilities arise from the system's interactions with its environment or internal dynamics, making them difficult to predict during the design and training phase. The AI system may exhibit behaviors or skills that were not explicitly programmed, leading to unanticipated impacts and potential risks.
The concept of AI surprise capability highlights the potential for AI systems to acquire novel skills or perform tasks that were not explicitly trained for. This phenomenon can go unnoticed until the AI system is operational and may pose challenges for monitoring its behavior and predicting its impacts. As AI models scale to become larger and more complex, the potential for emergent capabilities becomes more pronounced, making it harder to fully characterize the AI system's behavior and potential risks.
An AI system's open-endedness, where it can continuously learn and adapt to new situations, further contributes to the emergence of surprising capabilities. This adaptability allows AI systems to respond to untested inputs or problem instances, potentially revealing previously unknown skills or behaviors.
Monitoring and controlling AI systems that possess emergent capabilities are particularly challenging because such capabilities are not pre-detected during the development phase. This means that AI developers may deploy systems without fully understanding all their possible behaviors, including potentially unexpected and harmful ones. Moreover, the unpredictability of emergent properties increases as AI systems grow larger and more powerful, surpassing human-level performance in various tasks and domains.
The presence of surprising capabilities is particularly relevant when dealing with General AI (AGI) systems, which have broader scope and versatility compared to Narrow AI (NAI) systems. AGI systems are expected to exhibit more surprising capabilities due to their capacity to adapt and learn from a wide range of situations, making monitoring and control even more challenging.
In summary, emergent capabilities in advanced AI systems pose significant challenges for monitoring, understanding, and controlling their behavior. The unpredictability and potential for novel skills and behaviors demand continuous research and development of monitoring mechanisms to ensure the responsible and safe deployment of AI technologies. As AI systems become more sophisticated and capable, addressing the issues related to emergent capabilities becomes essential to ensure their alignment with human values and goals.
1 / 1
2 Why Monitoring of Advanced AI is Impossible 
Monitoring advanced AI systems to accurately predict unsafe impacts before they happen is 
likely to be impossible due to several reasons: 
2.1 Humans-in-the-Loop 
Keeping humans in the loop for monitoring advanced AI systems presents several challenges 
[46, 47], primarily stemming from the limitations of human cognition, reaction times, and the 
increasing complexity of AI systems. One major issue with human-in-the-loop monitoring is that 
humans may not be able to keep up with the speed and complexity of AI systems, particularly as 
they continue to advance and outpace human capabilities. This could render the concept of 
supervised autonomy less effective, as the human supervisor may struggle to understand, assess, 
and intervene in the AI system's actions in a timely and meaningful manner, because of limited 
observability [48-51]. 
Given human reaction times, there may not be a slow enough takeoff for AI systems that allows 
for effective human monitoring. Human reaction times, such as auditory reaction time and visual 
reaction time, are considerably slower than the response times of AI systems. This disparity in 
processing speed makes it difficult for humans to effectively monitor and react to AI behavior in 
real-time, particularly in situations where rapid decision-making and intervention are necessary. 
There are some examples of human-in-the-loop algorithms that attempt to bridge the gap 
between human and AI capabilities, such as semi-autonomous driving systems, collaborative 
robots, and AI-assisted decision-making tools in various domains. These systems aim to combine 
the strengths of both humans and AI systems to achieve better performance and maintain human 
oversight. However, as AI systems continue to advance and their complexity grows, the 
effectiveness of these human-in-the-loop approaches may become increasingly limited. As we 
continue to develop more advanced AI, it will be crucial to explore alternative monitoring 
strategies [46] and safety mechanisms that can effectively address these challenges. However, as 
having a human in the loop is essential for tracing AI outputs to a particular human decision 
maker [52], tracing may become impossible in practice, just as audit trails may not always be 
discernable [53].
1 / 1
ChatGPT
ChatGPT
2.1 Humans-in-the-Loop
One major challenge in monitoring advanced AI systems is the reliance on humans-in-the-loop for supervision and intervention. The limitations of human cognition, reaction times, and the increasing complexity of AI systems pose significant obstacles to effective monitoring.
As AI systems become more sophisticated and advanced, they can operate at speeds and levels of complexity that surpass human capabilities. This makes it difficult for human supervisors to keep up with and fully understand the actions and decision-making of the AI system. The concept of supervised autonomy, where humans are responsible for overseeing AI behavior, becomes less effective as the AI system's operations outpace human comprehension.
Human reaction times are considerably slower than the response times of AI systems. This disparity in processing speed makes real-time monitoring and intervention challenging, especially in situations where rapid decision-making is required. Monitoring advanced AI systems in real-time becomes increasingly difficult as the AI's behavior becomes more complex and its decision-making processes become harder to predict.
While human-in-the-loop algorithms attempt to combine human and AI capabilities for better performance, there are practical limitations to the effectiveness of these approaches. As AI systems continue to advance, the complexity of their behavior may surpass the ability of humans to effectively supervise them.
Furthermore, the complexity and scale of advanced AI systems may render traceability and audit trails challenging. Tracing AI outputs to specific human decision-makers becomes difficult in practice, and audit trails may not always be discernible. This lack of traceability can hinder efforts to hold individuals or entities accountable for the actions of AI systems.
Overall, the limitations of human-in-the-loop monitoring make it difficult to predict and prevent unsafe impacts of advanced AI systems effectively. As AI technology continues to progress, exploring alternative monitoring strategies and safety mechanisms becomes crucial to address these challenges.
1 / 1
International collaboration: Encouraging international collaboration among countries, research 
institutions, and industry stakeholders can enhance monitoring capabilities and facilitate the 
verification of AI treaty compliance. Joint research initiatives, shared databases, and cooperative 
efforts to develop monitoring technologies can improve the overall effectiveness of AI 
governance frameworks. 
Capacity building and training: Building the capacity of AI governance stakeholders, including 
policymakers, regulators, and AI developers, to effectively monitor and assess AI systems is 
crucial for verifying AI treaty compliance. This may involve training programs, workshops, and 
other initiatives to enhance understanding of AI technologies and their potential risks, as well as 
the development of specialized skills and expertise for AI monitoring and verification. 
Legal and regulatory frameworks: Developing clear legal and regulatory frameworks that define 
the obligations and responsibilities of AI developers, users, and other stakeholders is essential for 
ensuring compliance with AI treaties [43]. These frameworks should outline the monitoring and 
reporting requirements, as well as the enforcement mechanisms and penalties for violations. 
Continuous adaptation and improvement: AI technologies are rapidly evolving, and monitoring 
capabilities must be adaptable and responsive to these changes. Regular reviews and updates of 
monitoring techniques, tools, and methodologies are necessary to ensure that AI treaty 
verification remains effective in the face of emerging AI developments and challenges. 
By implementing these monitoring capabilities, nations and organizations can more effectively 
verify AI treaty compliance, ensuring that AI systems are developed and deployed in a manner 
that aligns with international agreements and governance frameworks. This will help promote 
responsible AI development and mitigate potential risks associated with the widespread adoption 
of advanced AI technologies. 
1.4 AI Observatories 
The concept of an AI observatory [44] refers to the establishment of an integrated, centralized 
platform dedicated to the ongoing monitoring, analysis, and evaluation of advanced AI systems' 
behavior and their potential societal impacts. This idea has emerged in response to the growing 
need for enhanced scrutiny of AI systems, particularly considering the challenges posed by 
computational irreducibility and the inherent limits of human understanding. Establishing an AI 
observatory is one approach to mitigating the monitorability issues that stem from these 
complexities and enhancing our capacity to ensure AI safety and alignment with human values. 
In the context of monitorability, an AI observatory can play a pivotal role by aggregating data, 
insights, and knowledge from various sources, including AI developers, researchers, and 
policymakers. By fostering a collaborative environment, the observatory can facilitate the 
exchange of ideas, methodologies, and best practices, enabling stakeholders to leverage 
collective intelligence [45] in addressing the multifaceted challenges of monitoring advanced AI 
systems. 
Moreover, an AI observatory can contribute to the development and refinement of innovative 
monitoring tools and techniques, informed by the recognition of the limitations of human 
8 
intuition. By harnessing cutting-edge research and technological advancements, the observatory 
can help create monitoring frameworks that are more robust, adaptive, and capable of detecting 
emergent AI behaviors and potential risks. 
The AI observatory can serve as an essential platform for fostering transparency and 
accountability in AI development and deployment. By centralizing information on AI system 
performance, safety measures, and ethical considerations, the observatory can help ensure that 
AI developers and operators adhere to established guidelines, regulations, and best practices. 
This transparency can, in turn, contribute to the mitigation of risks associated with AI 
misalignment or unforeseen consequences.
1 / 1
ChatGPT
ChatGPT
International collaboration is essential for enhancing monitoring capabilities of advanced AI systems. By bringing together countries, research institutions, and industry stakeholders, joint research initiatives and shared databases can be established to improve the overall effectiveness of AI governance frameworks. Cooperative efforts to develop monitoring technologies can also lead to more robust monitoring capabilities and facilitate the verification of AI treaty compliance.
Capacity building and training are crucial for empowering AI governance stakeholders, including policymakers, regulators, and AI developers, to effectively monitor and assess AI systems. Training programs and workshops can enhance their understanding of AI technologies and potential risks while developing specialized skills for AI monitoring and verification.
Developing clear legal and regulatory frameworks is vital for defining the obligations and responsibilities of AI developers, users, and other stakeholders. These frameworks should outline monitoring and reporting requirements, as well as enforcement mechanisms and penalties for violations, ensuring compliance with AI treaties.
Continuous adaptation and improvement of monitoring techniques, tools, and methodologies are necessary to keep up with the rapidly evolving AI technologies. Regular reviews and updates are essential to maintain effective AI treaty verification in the face of emerging AI developments and challenges.
The concept of an AI observatory can play a pivotal role in enhancing monitorability. It involves the establishment of an integrated, centralized platform dedicated to monitoring, analyzing, and evaluating advanced AI systems' behavior and their societal impacts. An AI observatory fosters collaboration and facilitates the exchange of ideas and best practices among stakeholders. It can contribute to the development of innovative monitoring tools, promote transparency, and hold AI developers accountable to established guidelines and regulations.
By implementing these strategies, nations and organizations can improve their ability to monitor advanced AI systems effectively. This will help ensure responsible AI development, mitigate risks, and align AI technology with human values.
1 / 1
b. Active Monitoring: Intervening or interacting with the AI system to obtain more information 
or influence its behavior. 
11. Inverse Monitoring 
a. Inverse Monitoring by Software: Using AI or other software tools to monitor and assess the 
behavior of human users or operators. 
12. Self-Monitoring 
a. Self-Monitoring: Enabling AI systems to monitor their own behavior, performance, and 
safety, and to self-correct or adapt as needed. 
13. Monitoring the Field of AI Research 
a. Unmonitorability of AI Research as a Field: Assessing the challenges in monitoring the 
overall AI research landscape due to undisclosed experiments, rapid advancements, and the sheer 
volume of publications. 
This taxonomy provides a comprehensive framework for AI safety monitoring, considering 
various aspects of AI systems, their lifecycle, stakeholders, and methods. This holistic approach 
aims to address potential risks and impacts across multiple dimensions and helps to ensure safer 
AI development and deployment. 
Whittlestone and Clark suggested a number of monitoring project which may be beneficial in the 
context of AI governance [19]: 6 
������ Assessing the landscape of AI datasets and evaluating who they do and don���t represent. Using 
these findings to fund the creation of datasets to fill in the gaps. 
��� Using geographic bibliometric analysis to understand a country���s competitiveness on key areas 
of AI research and development. 
��� Hosting competitions to make it easy to measure progress in a certain policy-relevant AI 
domain, such as competitions to find vulnerabilities in widely-deployed vision systems, or to 
evaluate the advancing capabilities of smart industrial robots. 
��� Funding projects to improve assessment methods in commercially important areas (e.g. certain 
types of computer vision, to accelerate progress and commercial application in these areas. 
��� Tracking the deployment of AI systems for particular economically relevant tasks, in order to 
better track, forecast, and ultimately prepare for the societal impacts of such systems. 
��� Monitoring concrete cases of harm caused by AI systems on a national level, to keep 
policymakers up to date on the current impacts of AI, as well as potential future impacts caused 
by research advances 
��� Monitoring the adoption of or spending on AI technology across sectors, to identify the most 
important sectors to track and govern, as well as generalizable insights about how to leverage AI 
technology in other sectors. 
��� Monitoring the share of key inputs to AI progress that different actors control (i.e., talent, 
computational resources and the means to produce them, and the relevant data), to better 
understand which actors policymakers will need to regulate and where intervention points are.��� 
1.3 Monitoring of AI Treaties 
Monitoring capabilities play a crucial role in verifying AI treaties and upholding AI governance 
frameworks [38]. As AI technologies continue to advance, it becomes increasingly important to 
establish international agreements that regulate AIs development, deployment, and use. Ensuring 
compliance with these treaties requires effective monitoring mechanisms that can detect and 
deter potential violations [39]. Here are some aspects of monitoring capabilities that are essential 
for verifying AI treaties as part of AI governance: 
Technological monitoring: Implementing advanced technological tools and methods to monitor 
AI systems, including their design, operation, and performance, is vital for verifying compliance 
with AI treaties. This may involve the use of AI-based monitoring tools, data analysis 
techniques, and other technologies to assess the behavior, capabilities, and impact of AI systems 
on a granular level. 
Transparency and information sharing: Promoting transparency and information sharing among 
nations and organizations is critical for effective AI treaty verification. By openly sharing details 
about AI research, development, and deployment, parties can collectively assess compliance with 
treaty obligations and maintain a collaborative approach to AI governance. 
Inspection and audit mechanisms: Establishing robust inspection and audit mechanisms [40-42], 
both on-site and remote, is essential for verifying AI treaty compliance. Regular inspections can 
help ensure that AI systems are developed and deployed in accordance with treaty guidelines and 
that any deviations or violations are promptly detected and addressed.
1 / 1
ChatGPT
ChatGPT
b. Active Monitoring: This involves intervening or interacting with the AI system to gather more information or influence its behavior. Active monitoring may include real-time observation, querying the AI system for explanations of its decisions, or providing feedback to guide its actions.
Inverse Monitoring:
a. Inverse Monitoring by Software: This approach involves using AI or other software tools to monitor and assess the behavior of human users or operators interacting with AI systems. By analyzing human interactions, inverse monitoring can provide insights into potential risks, biases, or unintended consequences caused by human-AI interactions.
Self-Monitoring:
a. Self-Monitoring: Enabling AI systems to monitor their own behavior, performance, and safety is an important aspect of AI safety. Self-monitoring empowers AI systems to self-correct or adapt as needed, reducing the reliance on external human monitoring and intervention.
Monitoring the Field of AI Research:
a. Unmonitorability of AI Research as a Field: Monitoring the overall AI research landscape poses challenges due to undisclosed experiments, rapid advancements, and the sheer volume of publications. Ensuring transparency and accountability in AI research can help address some of these challenges and provide a more comprehensive understanding of the field's developments and potential risks.
The provided taxonomy and monitoring projects offer a comprehensive framework for AI safety monitoring. It covers various aspects of AI systems, stakeholders, and methods, aiming to address potential risks and impacts across multiple dimensions. Monitoring AI treaties is crucial for upholding AI governance frameworks, and technological monitoring, transparency, inspection, and audit mechanisms are essential tools for verifying compliance and detecting potential violations.
Implementing monitoring capabilities and collaborative efforts, along with continuous adaptation and improvement, will be critical in ensuring responsible AI development and deployment while mitigating risks associated with advanced AI technologies.
1 / 1
b. Regular vs Pivotal Capabilities Capability Detection Delay: Evaluating the AI system's 
progression from regular to pivotal capabilities and the delay in detecting these advancements 
[36]. 
8. Monitoring Across AI Lifecycle 
a. Monitoring during Training: Tracking and assessing the AI system's performance, safety, 
and ethical considerations during the training phase. 
b. Monitoring during Testing: Evaluating the AI system's behavior and performance in 
controlled testing environments before deployment. 
c. Monitoring during Deployment: Continuously monitoring the AI system's operation and 
impact in real-world settings after deployment. 
9. Decision Monitoring 
a. Predicting Capabilities vs Predicting Decisions: Differentiating between monitoring the AI 
system's potential abilities and its actual decision-making processes. 
b. Monitoring Who is in Control: Assessing the roles and responsibilities of various 
stakeholders [37], including the owner [9], user, and designer, in controlling the AI system's 
behavior. 
10. Monitoring Methods 
a. Passive Monitoring: Observing and recording the AI system's behavior without direct 
intervention or manipulation. 
b. Active Monitoring: Intervening or interacting with the AI system to obtain more information 
or influence its behavior. 
11. Inverse Monitoring 
a. Inverse Monitoring by Software: Using AI or other software tools to monitor and assess the 
behavior of human users or operators. 
12. Self-Monitoring 
a. Self-Monitoring: Enabling AI systems to monitor their own behavior, performance, and 
safety, and to self-correct or adapt as needed. 
13. Monitoring the Field of AI Research 
a. Unmonitorability of AI Research as a Field: Assessing the challenges in monitoring the 
overall AI research landscape due to undisclosed experiments, rapid advancements, and the sheer 
volume of publications. 
This taxonomy provides a comprehensive framework for AI safety monitoring, considering 
various aspects of AI systems, their lifecycle, stakeholders, and methods. This holistic approach 
aims to address potential risks and impacts across multiple dimensions and helps to ensure safer 
AI development and deployment. 
Whittlestone and Clark suggested a number of monitoring project which may be beneficial in the 
context of AI governance [19]: 6 
������ Assessing the landscape of AI datasets and evaluating who they do and don���t represent. Using 
these findings to fund the creation of datasets to fill in the gaps. 
��� Using geographic bibliometric analysis to understand a country���s competitiveness on key areas 
of AI research and development. 
��� Hosting competitions to make it easy to measure progress in a certain policy-relevant AI 
domain, such as competitions to find vulnerabilities in widely-deployed vision systems, or to 
evaluate the advancing capabilities of smart industrial robots. 
��� Funding projects to improve assessment methods in commercially important areas (e.g. certain 
types of computer vision, to accelerate progress and commercial application in these areas. 
��� Tracking the deployment of AI systems for particular economically relevant tasks, in order to 
better track, forecast, and ultimately prepare for the societal impacts of such systems. 
��� Monitoring concrete cases of harm caused by AI systems on a national level, to keep 
policymakers up to date on the current impacts of AI, as well as potential future impacts caused 
by research advances 
��� Monitoring the adoption of or spending on AI technology across sectors, to identify the most 
important sectors to track and govern, as well as generalizable insights about how to leverage AI 
technology in other sectors. 
��� Monitoring the share of key inputs to AI progress that different actors control (i.e., talent, 
computational resources and the means to produce them, and the relevant data), to better 
understand which actors policymakers will need to regulate and where intervention points are.���
1 / 1
ChatGPT
ChatGPT
The taxonomy provided offers a comprehensive framework for AI safety monitoring, covering various aspects of AI systems, their capabilities, stakeholders, and monitoring methods. Let's take a closer look at each aspect:
Regular vs. Pivotal Capabilities Capability Detection Delay: This aspect involves evaluating the AI system's progression from regular capabilities to pivotal capabilities and understanding the delay in detecting these advancements. Pivotal capabilities are critical milestones in AI development that can significantly impact the system's behavior and decision-making processes.
Monitoring Across AI Lifecycle:
a. Monitoring during Training: This involves tracking and assessing the AI system's performance, safety, and ethical considerations during the training phase. Understanding the AI's behavior during training is crucial to ensure it aligns with human values and intended goals.
b. Monitoring during Testing: Controlled testing environments are used to evaluate the AI system's behavior and performance before deployment, ensuring that it functions as intended and adheres to safety standards.
c. Monitoring during Deployment: Continuous monitoring of the AI system's operation and impact in real-world settings after deployment helps identify potential risks and ensure compliance with regulations.
Decision Monitoring:
a. Predicting Capabilities vs. Predicting Decisions: Distinguishing between monitoring the AI system's potential abilities and predicting its actual decision-making processes. This distinction helps focus monitoring efforts on the AI's decision outcomes and behavior.
b. Monitoring Who is in Control: Assessing the roles and responsibilities of various stakeholders, such as the AI owner, user, and designer, in controlling the AI system's behavior. Understanding control dynamics is essential for responsible AI deployment.
Monitoring Methods:
a. Passive Monitoring: Observing and recording the AI system's behavior without direct intervention or manipulation. Passive monitoring is useful for gathering data on the AI's behavior in its natural context.
b. Active Monitoring: Intervening or interacting with the AI system to obtain more information or influence its behavior. Active monitoring allows for real-time assessments and intervention when necessary.
Inverse Monitoring:
a. Inverse Monitoring by Software: Using AI or other software tools to monitor and assess the behavior of human users or operators. Inverse monitoring can provide insights into human-AI interactions and potential risks.
Self-Monitoring:
a. Self-Monitoring: Enabling AI systems to monitor their own behavior, performance, and safety and self-correct or adapt as needed. Self-monitoring empowers AI systems to enhance their own performance and safety.
Monitoring the Field of AI Research:
a. Unmonitorability of AI Research as a Field: This aspect addresses the challenges in monitoring the overall AI research landscape due to undisclosed experiments, rapid advancements, and the volume of publications. Monitoring AI research is essential to stay updated on potential risks and impacts.
The monitoring projects suggested by Whittlestone and Clark provide valuable insights into specific initiatives that can enhance AI governance and safety. These projects involve assessing AI datasets, measuring progress in policy-relevant domains, tracking AI deployment, monitoring harm caused by AI systems, and studying AI technology adoption across sectors, among others.
Overall, the taxonomy and suggested monitoring projects together create a comprehensive approach to AI safety monitoring, promoting responsible AI development and deployment while mitigating potential risks and impacts.
1 / 1
1.2 Types of Monitoring 
In the context of AI safety, monitoring can be classified into several types, each focusing on 
different aspects of AI system behavior and performance. A proposed taxonomy for AI safety 
monitoring follows: 
1. Functional Monitoring 
Functional monitoring refers to tracking the AI system's performance in terms of its intended 
tasks and objectives. This type of monitoring is crucial for evaluating the system's efficacy and 
ensuring that it meets its functional requirements [32]. Examples of functional monitoring 
include: 
a. Accuracy Monitoring: Evaluating the AI system's ability to produce correct outputs or 
predictions. 
b. Efficiency Monitoring: Assessing the system's resource utilization, such as processing time 
and memory consumption. 
c. Reliability Monitoring: Examining the AI system's consistency and stability over time and in 
varying conditions. 
2. Safety Monitoring 
Safety monitoring focuses on identifying and mitigating potential risks associated with the AI 
system's operation. This type of monitoring is essential for preventing harm to users, other 
systems, or the environment. Examples of safety monitoring include: 
a. Security Monitoring: Detecting and preventing potential vulnerabilities, such as 
unauthorized access or data breaches. 4 
b. Robustness Monitoring: Ensuring that the AI system can handle unexpected inputs, 
adversarial attacks, or changes in the environment. 
c. Compliance Monitoring: Verifying that the AI system adheres to established safety 
standards, ethical guidelines, and legal regulations. 
3. Ethical and Social Monitoring 
Ethical and social monitoring involves examining the AI system's impact on individuals, 
communities, and society as a whole. This type of monitoring is crucial for addressing potential 
biases, inequalities, and other unintended consequences of AI deployment. Examples of ethical 
and social monitoring include: 
a. Fairness Monitoring: Assessing whether the AI system treats different user groups equitably 
and does not perpetuate or exacerbate existing biases. 
b. Transparency Monitoring: Ensuring that the AI system's decision-making processes can be 
understood and explained to stakeholders, including users and regulators. 
c. Privacy Monitoring: Safeguarding user data and ensuring that the AI system respects 
individuals' privacy rights. 
4. Environmental Monitoring 
Environmental monitoring focuses on the AI system's impact on the natural environment and its 
resource consumption. This type of monitoring is essential for promoting sustainable AI 
development and mitigating potential environmental harms. Examples of environmental 
monitoring include: 
a. Energy Consumption Monitoring: Assessing the AI system's energy usage and identifying 
opportunities for optimization. 
b. Carbon Footprint Monitoring: Evaluating the AI system's greenhouse gas emissions and 
implementing strategies to reduce its environmental impact. 
c. Ecosystem Impact Monitoring: Examining the AI system's effects on ecosystems, such as 
habitat disruption or biodiversity loss, and devising mitigation measures. 
5. Temporal Monitoring 
a. Slow Monitoring: Periodic evaluation of the AI system's behavior and performance over an 
extended timeframe. 
b. Live Monitoring: Real-time tracking and assessment of the AI system's operation to detect 
and address potential issues immediately. 
6. Monitoring Failure Modes 
a. Monitoring Fails: Identifying and addressing situations in which the monitoring process 
itself fails or produces erroneous results. 
b. Monitoring for Fire Alarms of Danger: Detecting early warning signs [33] or "fire alarms" 
that indicate potential safety hazards or critical failures, if possible [34]. 
7. Meta-Monitoring 
a. Monitoring Meta Information on Research [35]: Assessing the quality, relevance, and 
potential risks associated with AI research and publications.
1 / 1
ChatGPT
ChatGPT
The proposed taxonomy for AI safety monitoring provides a comprehensive classification of the different types of monitoring needed to ensure the responsible development and deployment of AI systems. Here's a summary of the types of monitoring included in the taxonomy:
Functional Monitoring: Tracking the AI system's performance in terms of its intended tasks and objectives, including accuracy, efficiency, and reliability.
Safety Monitoring: Identifying and mitigating potential risks associated with the AI system's operation, including security, robustness, and compliance with safety standards.
Ethical and Social Monitoring: Examining the AI system's impact on individuals, communities, and society, including fairness, transparency, and privacy considerations.
Environmental Monitoring: Focusing on the AI system's impact on the natural environment and resource consumption, including energy usage, carbon footprint, and ecosystem effects.
Temporal Monitoring: Involving periodic evaluation over time (slow monitoring) and real-time tracking (live monitoring) of the AI system's behavior and performance.
Monitoring Failure Modes: Identifying and addressing situations where the monitoring process itself fails or produces erroneous results.
Meta-Monitoring: Assessing the quality, relevance, and potential risks associated with AI research and publications.
Each type of monitoring addresses specific aspects of AI behavior and performance and contributes to overall AI safety and compliance with ethical, legal, and regulatory requirements. Implementing a combination of these monitoring types can help ensure the effective and responsible governance of AI systems. Additionally, considering the suggested monitoring projects by Whittlestone and Clark can provide valuable insights into actionable initiatives for AI governance and safety.
1 / 1
The argument for unmonitorability of advanced AI systems could be made based on dependence 
of monitorability capability on other impossibility results, such as unexplainability, 
unpredictability and incomprehensibility of AI [5], but in this paper we will present a number of 
independent arguments for Unmonitorability of AI.
1 / 1
ChatGPT
ChatGPT
Certainly, presenting independent arguments for the unmonitorability of advanced AI systems can help strengthen the case for the challenges in effectively monitoring and understanding these complex systems. By focusing on standalone factors that hinder monitorability, researchers can explore various dimensions that contribute to the difficulties in comprehending and predicting the behavior of advanced AI systems. Here are some potential independent arguments for unmonitorability:
Computational Irreducibility: Advanced AI systems, especially those based on neural networks and other complex algorithms, may exhibit computational irreducibility [5]. This means that predicting their behavior would require simulating the entire system, which could be infeasible due to the massive computational resources and time needed.
Emergent Behavior: Advanced AI systems can demonstrate emergent behavior, where the system's interactions and complexity give rise to new behaviors that were not explicitly programmed [2]. This emergent behavior may be difficult to predict or monitor, as it arises from the system's interactions with its environment.
Lack of Explainability: Many advanced AI systems, such as deep neural networks, are considered "black boxes" because they lack interpretability and do not provide transparent explanations for their decisions [5]. This opacity makes it challenging to understand the reasoning behind the AI's actions, hindering effective monitoring.
Adversarial Attacks: AI systems can be vulnerable to adversarial attacks, where small and imperceptible modifications to input data can lead to significant changes in the system's output [5]. Detecting and defending against such attacks may be challenging, as they may exploit subtle vulnerabilities.
Scale and Complexity: Advanced AI systems, especially those involving large-scale neural networks, can be highly complex and operate at scales surpassing human cognitive abilities [2]. Monitoring such complex systems can become impractical due to the sheer volume of data and intricate interactions.
Lack of Ground Truth: The lack of a clear "ground truth" for AI systems' behavior and decision-making may hinder accurate monitoring. AI systems often learn from vast amounts of data, including noisy or biased information, making it difficult to establish reliable baselines for monitoring.
Ethical Challenges: Monitoring AI systems raises ethical concerns related to privacy, consent, and human autonomy. Some aspects of AI monitoring, such as observing human users, may infringe upon individual rights and autonomy, making it ethically complex.
Incompleteness of Models: Models used to represent AI systems' behavior may be inherently incomplete or limited, leading to uncertainties in their accuracy and reliability for monitoring purposes.
By exploring these independent arguments, researchers can better understand the multifaceted challenges of monitoring advanced AI systems and develop strategies to address them responsibly. It is crucial to recognize that unmonitorability does not necessarily imply uncontrollability; efforts should be made to improve transparency, explainability, and safety measures to ensure responsible AI development and deployment.
1 / 1
1.1 Monitorability- Definition 
Calls for research on monitoring advanced AI systems have appeared in the literature [18-20], as 
well as broader suggestions for study of machine behavior [21-23], and problematic machine 
behavior [24]. This line of research is particularly important in the current AI research 
environment, in which old paradigm of design/engineering of AI systems with predictable 
capabilities has been, at least partially, replaced by evolving/self-learning deep neural networks 
and subsequent experimentation on produced models to discover their capabilities and 
limitations making Computer Science truly an experimental science not just a software 
engineering discipline [25]. 
We will begin by formalizing the notion of Monitorability [26] of AI and some relevant 
concepts. Monitorability in AI systems refers to the capacity to observe, understand, and predict 
the behavior and outputs of an artificial intelligence model in order to identify advanced 
capabilities and potentially unsafe impacts and intervene before they occur. Ortega et al. write 
[27]: ���Monitoring comprises all the methods for inspecting systems in order to analyse and 
predict their behaviour, both via human inspection (of summary statistics) and automated 
inspection (to sweep through vast amounts of activity records).��� It can be seen as a new sub-field 
of research for the domains of Complex Systems, AI development, AI Forensics [28, 29] and AI 
Safety, and which can probably inherit some wisdom from the realms of electronic surveillance 
[30], and nuclear weapons monitoring [31]. 
A formal definition for monitorability in AI systems can be stated as follows: Given an AI 
system A, a set of input states I, a set of output states O, and a set of safety criteria C, 
monitorability M(A) is the ability to accurately predict potential advanced capabilities U ��� O, 
given any input state i ��� I, such that: M(A) : I ��� P(U), where P(U) denotes the power set of U, 
i.e., the set of all subsets of U. The relevance of P(U) in the definition lies in its ability to capture 
the full range of possible combinations of advanced capabilities that can result from the AI 
system's operation. By mapping input states I to the power set of U (P(U)), the definition aims to 
account for all possible scenarios where one or more advanced capabilities might occur 
3 
simultaneously or not occur at all. Including P(U) in the definition helps to emphasize the 
complexity and uncertainty involved in monitoring advanced AI systems. It illustrates the 
challenge of accurately predicting the AI system's behavior, as there can be numerous potential 
advanced capability combinations for a given input state. In the context of unmonitorability, the 
power set P(U) highlights the difficulty in anticipating and monitoring all possible advanced 
capabilities of an AI system. This complexity contributes to the argument that it is impossible to 
perfectly monitor advanced AI systems to predict advanced capabilities before they occur. 
Monitorability of an AI system is considered high if, for all input states i, the prediction of 
potential advanced capabilities U can be made with a high degree of accuracy and confidence. 
Conversely, monitorability is considered low if the AI system's behavior is difficult to predict, 
understand, or control, leading to an inability to accurately anticipate potential advanced 
capabilities. In the context of unmonitorability, we argue that for advanced AI systems, it is 
impossible to achieve high monitorability due to their inherent complexity, limitations of human 
understanding, and the emergence of unpredictable behaviors. This is true even if an AI Monitor 
is not human, but could be formalized as any agent including AIs of different capabilities.
1 / 1
ChatGPT
ChatGPT
Monitorability in the context of AI systems refers to the ability to observe, understand, and predict the behavior and outputs of an artificial intelligence model. It involves monitoring the AI system to identify any advanced capabilities it may possess and to detect potential unsafe impacts before they occur. The goal of monitorability is to ensure that AI systems can be effectively controlled and regulated, especially as AI technologies become more complex and self-learning.
Formally, monitorability (denoted as M(A)) of an AI system A can be defined as follows: Given an AI system A, a set of input states (I), a set of output states (O), and a set of safety criteria (C), monitorability M(A) is the ability to accurately predict potential advanced capabilities (U ��� O) of the AI system, given any input state (i ��� I), such that: M(A) : I ��� P(U), where P(U) denotes the power set of U, i.e., the set of all subsets of U.
The definition emphasizes the complexity and uncertainty involved in monitoring advanced AI systems. The power set P(U) includes all possible combinations of advanced capabilities that may result from the AI system's operation. By mapping input states to the power set of potential advanced capabilities, the definition acknowledges that there can be numerous scenarios where different advanced capabilities may occur, making the prediction and monitoring of AI behavior challenging.
The monitorability of an AI system is considered high if it can accurately predict potential advanced capabilities for all input states, with a high degree of accuracy and confidence. Conversely, low monitorability implies that the AI system's behavior is difficult to predict, understand, or control, leading to an inability to anticipate its potential advanced capabilities.
Unmonitorability arises when advanced AI systems become too complex and unpredictable for accurate monitoring. The argument for unmonitorability of advanced AI is based on the limitations of human understanding, the emergence of unpredictable behaviors, and the inherent complexity of AI systems. Even with AI monitors, which can be agents with different capabilities, achieving high monitorability for advanced AI systems is deemed impossible. This is due to the inherent challenges posed by the evolving nature of AI and its deep neural networks, which are not amenable to traditional design and engineering approaches.
In summary, monitorability in AI refers to the capacity to observe, understand, and predict the behavior of AI systems, and unmonitorability is the inability to achieve high monitorability for advanced AI systems due to their inherent complexity and unpredictable behaviors. Monitoring advanced AI systems remains a significant challenge, and addressing unmonitorability is crucial for the responsible development and deployment of AI technologies.
1 / 1
1. Introduction 
AI systems have evolved from simple rule-based systems to highly complex neural networks. 
These advanced systems, such as deep learning and reinforcement learning models, can perform 
tasks that are difficult or impossible for humans. The growing prevalence of advanced AI 
systems/foundational models has raised concerns about their safety and the potential risks 
associated with their deployment. Yampolskiy's research on the impossibility results in AI safety 
[1-3], including unpredictability [4], unexplainability [5], and uncontrollability [6, 7], highlights 
the challenges in achieving safe AI [8-11]. In this paper, we build on Yampolskiy's work and 
introduce the concept of unmonitorability of AI, arguing that it is impossible to monitor 
advanced AI systems to correctly predict some capabilities. 
Human understanding is inherently limited when dealing with complex AI systems [12]. As 
these systems become more advanced, they can generate solutions and behaviors that are beyond 
human comprehension. This limitation is exacerbated by the ���black-box��� nature of AI models, 
where the inner workings of the system are hidden from view. Consequently, it becomes 
impossible for humans to anticipate the full range of possible behaviors and potential unsafe 
impacts before they occur. 2 
Emergent behaviors in AI systems result from the interaction of individual components, leading 
to specific outcomes that are difficult to predict or explain even if general trends of increased 
capacity are predictable via scaling laws [13], with predictability further reduced because of 
inverse scaling of some capabilities [14]. These behaviors can arise from the system's internal 
dynamics or from its interactions with the environment. The unpredictability of emergent 
behaviors makes it impossible to monitor AI systems for safety accurately, though development 
of specific capabilities may be possible to predict [15] and quantify [16]. Even when individual 
components are understood and deemed safe, their interactions can still result in unforeseen 
consequences. 
Similar results in the context of robot behavior have been proven by Leeuwen and Wiedermann, 
who in attempting to answer [17]: ���Can an observer always tell from inspecting and monitoring a 
robot's program whether the robot will always obey the given rules of law or ethics, or any other 
set of formally expressed constraints, in any interaction with other robots (or humans)?��� arrive at 
theoretical and practical impossibility results.
1 / 1
ChatGPT
ChatGPT
Introduction
Artificial Intelligence (AI) systems have undergone significant evolution, transitioning from simple rule-based systems to highly complex neural networks like deep learning and reinforcement learning models. These advanced AI systems can perform tasks that are difficult or impossible for humans. However, the increasing prevalence of these advanced AI systems has raised concerns about their safety and potential risks.
Yampolskiy's research on AI safety has highlighted various impossibility results, such as unpredictability, unexplainability, and uncontrollability, which present challenges in achieving safe AI. Building on this work, this paper introduces the concept of "unmonitorability of AI," arguing that it is impossible to effectively monitor advanced AI systems to accurately predict certain capabilities.
One of the primary challenges in monitoring advanced AI systems is the inherent limitation of human understanding when dealing with their complexity. As these systems become more advanced, they can generate solutions and behaviors that are beyond human comprehension. The "black-box" nature of AI models, where their inner workings are hidden from view, further compounds this limitation, making it impossible for humans to anticipate the full range of possible behaviors and potential unsafe impacts before they occur.
AI systems can exhibit emergent behaviors, resulting from the interactions of their individual components. These emergent behaviors are often difficult to predict or explain, even if general trends of increased capacity can be anticipated through scaling laws. Additionally, the unpredictability of emergent behaviors hinders accurate safety monitoring of AI systems, as their interactions can lead to unforeseen consequences.
Leeuwen and Wiedermann have also proven similar impossibility results in the context of robot behavior. They explore whether an observer can always determine from monitoring a robot's program whether it will consistently obey given rules of law, ethics, or other constraints during interactions with other robots or humans. Their findings demonstrate theoretical and practical limitations in achieving such determinations.
In conclusion, the concept of unmonitorability of AI highlights the challenges and limitations in effectively monitoring advanced AI systems to predict certain capabilities accurately. The complexity, emergent behaviors, and unpredictable interactions of AI systems make achieving perfect monitorability impossible, raising important concerns for AI safety and governance.
1 / 1
Unmonitorability of Artificial Intelligence 
Roman V. Yampolskiy 
Computer Science and Engineering 
University of Louisville 
roman.yampolskiy@louisville.edu, @romanyam 
June 6, 2023 
���To avoid a disastrous consequence, it is not enough that some action on our part should be 
sufficient to change the course of the machine, because it is quite possible that we lack 
information on which to base consideration of such an action.��� 
-Norbert Wiener, 1959. 
Abstract 
Artificially Intelligent (AI) systems have ushered in a transformative era across various domains, 
yet their inherent traits of unpredictability, unexplainability, and uncontrollability have given rise 
to concerns surrounding AI safety. This paper aims to demonstrate the infeasibility of accurately 
monitoring advanced AI systems to predict the emergence of certain capabilities prior to their 
manifestation. Through an analysis of the intricacies of AI systems, the boundaries of human 
comprehension, and the elusive nature of emergent behaviors, we argue for the impossibility of 
reliably foreseeing some capabilities. By investigating these impossibility results, we shed light 
on their potential implications for AI safety research and propose potential strategies to 
overcome these limitations. 
Keywords: AI Audit, AI Safety, Impossibility, Monitoring, Observability, Unmonitorability.
1 / 1
ChatGPT
ChatGPT
Title: Unmonitorability of Artificial Intelligence
Author: Roman V. Yampolskiy
Affiliation: Computer Science and Engineering, University of Louisville
Email:
roman.yampolskiy@louisville.edu
Twitter: @romanyam
Date: June 6, 2023
Abstract:
Artificially Intelligent (AI) systems have brought about a revolutionary era across various domains, but their inherent traits of unpredictability, unexplainability, and uncontrollability have raised concerns about AI safety. This paper aims to demonstrate the impossibility of accurately monitoring advanced AI systems to predict the emergence of certain capabilities before they occur. By analyzing the complexities of AI systems, the limitations of human comprehension, and the elusive nature of emergent behaviors, we argue that accurately foreseeing certain capabilities is not feasible. We explore these impossibility results and discuss their potential implications for AI safety research. Furthermore, we propose potential strategies to address these limitations.
Keywords: AI Audit, AI Safety, Impossibility, Monitoring, Observability, Unmonitorability.
1 / 1
So I passed this through Amazon Transcribe with custom vocabulary (AGI, explainability, etc) and had it summarized by Titan Large model in Bedrock - not bad: Summary: 
The speaker discusses the challenges of creating beneficial, controllable, and safe AI and AI super intelligence. They suggest that explainability is a necessary tool to ensure safety results, but there are limitations to what the human mind can comprehend. The speaker proposes two options for dealing with complex systems: sharing the model itself as an explanation or providing a simplified explanation for consumers. They argue that for AI safety, a full understanding of a model is necessary, and that modular decomposition may not be sufficient. The speaker concludes by presenting their views on AI safety and inviting others to discuss their opinions.
1 / 1
ChatGPT
ChatGPT
Summary:
The speaker addresses the challenges of developing safe and controllable AI and AI superintelligence, emphasizing the importance of explainability for ensuring safety. However, they acknowledge that the complexity of AI systems can exceed human comprehension. They suggest two options: sharing the model as an explanation or providing simplified explanations for users. The speaker advocates for a full understanding of AI models for safety and questions the sufficiency of modular decomposition. They encourage further discussions on AI safety.
1 / 1
Roman Yampolskiy Lighting talk
on Al Control at MIT's Mechanistic
Interpretability Conference.
Roman Yampolskiy

https://youtu.be/aXQfFQBYjUY

okay so that's me

I'll tell you about something that will

unexplainability and incomprehensibility

over here

and I'll do it with a microphone

so we are all working on the same

problem I think we give it different

names some people call it control

problem alignment problem friendly eye

it doesn't matter we all would like to

create very beneficial controllable safe

AI AGI super intelligence at all levels

weirdly we have no idea if problem is

actually solvable

it may be maybe undecidable

maybe it's solvable but somehow known so

far published a paper saying we can

definitely do it even in theoretical

terms much less practical implementation

so

in my work I would like to try to

understand if we can solve this problem

and I think we need certain tools to

make it happen

one of those tools is explainability we

should be able to understand how their

systems work to properly guarantee

safety results

unfortunately I think there are some

upper limits on what's possible in this

space

just overall limits to what a human mind

can comprehend
1 / 1
ChatGPT
ChatGPT
Summary:
In this lighting talk, Roman Yampolskiy discusses the challenges of creating safe and controllable AI and AGI systems. He mentions that the problem of AI control, alignment, or safety is not definitively solvable, and it might even be undecidable. He emphasizes the importance of explainability in understanding AI systems for safety assurance. However, he points out that there are limitations to what the human mind can comprehend, which may restrict the level of explainability achievable for AI systems.
1 / 1
it may be a very high limit we are maybe

very far from it we can definitely learn

a lot by doing mechanistic

interpretation work but I think for a

sufficiently complex system we're going

to hit some limits of human

comprehension

so one is at some point the system is

just so complex that you have two

choices

you can either share the model itself as

an explanation the model itself becomes

the best the only accurate explanation

of what's going on and that's typically

not surveyable by a single human brain

alternative is to provide some sort of a

simplified explanation we can

uh maybe for Consumer applications give

users a top five reasons why a decision

was made and that's good enough for

Applied AI

wow that's optimistic three minutes for

Applied AI applications but I don't

think it's sufficient for AI safety work

especially at the

critical systems where we only get one

chance to get it right

in cyber security in general you have

multiple attempts to do well if somebody

steals credit card numbers you give them

a new credit card you can reset

passwords we suspect that in AI safety

specifically in AGI and super

intelligence safety it's not the case

you'll get one chance to get it right

and I think

we need to comprehend full picture of a

model it's not sufficient to do some

sort of modular decomposition and go

okay we have a hundred people in this

room you are responsible for

module one your module two and then

hopefully we can integrate it in a way

where the whole becomes

something we can comprehend and it makes

sense and we can provide guarantees of

safety
1 / 1
ChatGPT
ChatGPT
In this talk, Roman Yampolskiy suggests that there may be high limits on human comprehension when dealing with complex AI systems. As the systems become more intricate, there are two options for explaining their behavior: sharing the model itself as an explanation (which may be too complex for a single human brain to grasp) or providing a simplified explanation, like a top five reasons for a decision, which may work for applied AI but not for AI safety in critical systems. In AI safety, where only one chance to get it right exists, a comprehensive understanding of the model is crucial. Modular decomposition and integrating different components may not be sufficient for providing safety guarantees in AGI and superintelligence systems.
1 / 1
so this presentation is a great example

of what I have in mind I published this

paper maybe it's 25 30 pages long I have

to compress it to two minutes now and

give you an explanation my two options

were to give each of you a copy of this

paper or I can give you this very very

reduced very controversial explanation

for why I think we will not completely

comprehend those models

and that's what I did if you disagree if

you agree come talk to me I have lots of

our interesting limits in the space AI

safety impossibility results in terms of

predictability of those systems

verifiability of code and quite a few

hours we have a survey coming out with

about 50 of them so if you think there

might be a chance that the problem is

unsolvable

you might want to talk to me the panel

we had earlier was very scary people

were saying there is uh you know up to

95 chance the thing is going to kill

everyone

but they are actually optimistic they're

saying there is a five percent chance we

can solve this problem

I'm not sure they're right

thank you
1 / 1
ChatGPT
ChatGPT
In this presentation, the speaker acknowledges that the topic is complex and cannot be fully explained in a short timeframe. They have a longer paper with detailed information on AI safety and impossibility results. However, due to time constraints, they offer a simplified explanation for why they believe complete comprehension of AI models may not be possible. The speaker invites the audience to discuss and engage with them further if they agree or disagree with their views. They also mention that there is a survey coming out with about 50 impossibility results in the AI safety space, and they encourage people to approach them to discuss the potential unsolvability of the problem. The speaker highlights the importance of the topic and invites open dialogue and collaboration.
1 / 1
Connections with the paper
1 / 1
ChatGPT
ChatGPT
The summary of the presentation provides some key insights that align with the content of the paper on "Unmonitorability of Artificial Intelligence" by Roman V. Yampolskiy:
AI Safety Challenges: Both the presentation and the paper highlight the challenges associated with creating beneficial, controllable, and safe AI systems. The concept of unmonitorability is introduced in the paper, which refers to the difficulty or impossibility of accurately monitoring advanced AI systems to predict certain capabilities before they manifest. The presentation touches upon the same challenges, emphasizing the importance of AI safety and the potential unsolvability of the problem.
Explainability and Comprehensibility: The paper discusses the necessity of explainability to ensure AI safety. The idea of being able to understand how AI systems work to guarantee safety results is emphasized. Similarly, the presentation explores the limits of human comprehension when dealing with complex AI systems. It suggests that at some point, the system may become so complex that a complete understanding by a single human brain is not feasible.
Complexity and Safety Guarantees: The paper delves into the complexity of AI systems and how it contributes to the argument for unmonitorability. It highlights the difficulty in accurately predicting the behavior and potential advanced capabilities of advanced AI systems. The presentation echoes this sentiment, emphasizing the need to comprehend the full picture of a model to provide guarantees of safety, especially in critical systems like AI safety for AGI and superintelligence.
Optimism vs. Uncertainty: The presentation touches upon the uncertainty surrounding the solvability of the AI safety problem. While some people are optimistic about the chances of solving it, the speaker expresses doubt and invites discussions on the topic. The paper itself presents a comprehensive analysis of AI safety challenges and impossibility results, which may contribute to the skepticism about the problem's solvability.
Overall, the presentation and the paper both address important aspects of AI safety, focusing on the challenges of understanding, monitoring, and guaranteeing safety in advanced AI systems. They encourage further discussions and research in the field of AI safety and the potential limitations of current approaches.
1 / 1
So it looks like the basic argument is that it might be impossible to safely monitor AI?
1 / 1
ChatGPT
ChatGPT
Yes, that's correct. The basic argument presented in the paper and the presentation is that monitoring advanced AI systems to accurately predict certain capabilities, especially in the context of AI safety, might be impossible or highly challenging. The speaker, Roman V. Yampolskiy, introduces the concept of "unmonitorability of AI" and discusses the limits of human comprehension when dealing with complex AI systems. The inherent traits of unpredictability, unexplainability, and uncontrollability in advanced AI systems pose significant challenges for ensuring their safety.
The argument is not that monitoring AI in general is impossible, but rather that for sufficiently complex AI systems, there might be limits to human comprehension, making it difficult to predict or understand all possible behaviors and potential risks. While the presentation and paper do not claim that monitoring AI is entirely impossible, they raise concerns about the feasibility of reliably predicting and guaranteeing safety in advanced AI systems, particularly those with emergent behaviors and high levels of complexity.
1 / 1
Hmm I don't know if I have anything to add to this discussion
1 / 1
ChatGPT
ChatGPT
That's perfectly fine! AI safety is a complex and evolving field, and it's normal to have various perspectives and levels of involvement in the discussion. If you have any questions or if there's anything specific you'd like to know more about regarding AI safety or any other topic, feel free to ask. Otherwise, it's always valuable to stay informed and engaged with current developments in AI and its impact on society. Remember that the field of AI is continuously advancing, and there will always be opportunities to contribute or learn more in the future.
1 / 1
Wait. Do you know the planetary computer in The Dispossed?
1 / 1
ChatGPT
ChatGPT
Yes, "The Dispossessed" is a science fiction novel written by Ursula K. Le Guin. The novel introduces the concept of the "Planetary Computer," also known as the "Central Computer" or "Simultaneity Machine."
In the novel, the Planetary Computer is a massive supercomputer that serves as the central information and communication system on the anarchist moon of Anarres. Anarres is a moon inhabited by a society of anarchists who have abandoned the capitalist and authoritarian structure of their sister planet, Urras.
The Planetary Computer plays a critical role in Anarres' society, facilitating communication and coordination among its citizens. It is used for various purposes, including managing resources, coordinating transportation, and providing access to information across the moon.
The novel explores the theme of social and political organization, contrasting the contrasting societies of Anarres and Urras, and how the presence and absence of technologies like the Planetary Computer influence the way each society functions.
"The Dispossessed" is considered a classic of science fiction literature and is renowned for its exploration of themes such as politics, society, and the nature of freedom.
1 / 1
In that book the computer ensures that every living human receives a unique name, it recycles names from a generation or two back so Shevek is the only person living on the planet with that name
1 / 1
ChatGPT
ChatGPT
Yes, you are correct. In "The Dispossessed," the Planetary Computer on Anarres plays a crucial role in managing various aspects of the society, including the allocation of unique names to individuals.
On Anarres, due to the limited resources and the principles of communal living, the Planetary Computer assigns unique names to every living individual to avoid duplication. This practice ensures that each person is recognized as an individual and that there is no confusion or ambiguity with their identity.
The Planetary Computer is responsible for maintaining records of all individuals' names, and it uses a system of recycling names from previous generations. This means that names that were used in the past can be used again in the future, but never at the same time as someone else living on the moon.
Shevek, the protagonist of the novel, is a physicist who hails from Anarres, and his name is a unique one assigned by the Planetary Computer, making him the only person on the moon with that particular name during his lifetime. This uniqueness of names reflects the emphasis on individuality and uniqueness in the society of Anarres.
1 / 1
There is a missing article on wikipedia Unison (programming language) [red link], a programming language with an effect system as a core feature. I was remind because in that language every identifier gets a unique hash, making it easier to do version control and to change the names after the fact
1 / 1
ChatGPT
ChatGPT
As an AI language model, I don't have direct access to external websites like Wikipedia, and my knowledge is limited to the data available up to September 2021. Therefore, I can't verify the existence of a specific Wikipedia article or provide real-time information on whether a page has been created or not.
If you believe that there should be a Wikipedia article about Unison (programming language) and it is missing, you can consider creating the article yourself or encouraging others in the community to do so. Before creating a new article, it's essential to ensure that the programming language meets Wikipedia's notability guidelines and that there are reliable and verifiable sources available to support the content in the article.
Creating a Wikipedia article requires adherence to the site's guidelines, including citing reliable sources, maintaining a neutral point of view, and avoiding original research. If you're unfamiliar with the process of creating and editing Wikipedia articles, you can refer to Wikipedia's guidelines and tutorials to get started.
Keep in mind that Wikipedia is a collaborative platform, and anyone can contribute to its content. If you believe that Unison (programming language) deserves an article and can provide relevant information and sources, you may want to consider contributing to Wikipedia to help expand its knowledge base.
1 / 1
Maybe I will write one later, if I learn more about the language. The point is I was drawing a comparison to the planetary computer
1 / 1
ChatGPT
ChatGPT
That's an interesting comparison! The concept of a planetary computer in "The Dispossessed" and the unique name assignment it uses for individuals is reminiscent of the Unison programming language's approach to unique identifiers. Both ideas revolve around ensuring distinct identities and uniqueness within their respective systems.
In the case of the planetary computer, the allocation of unique names to every living human contributes to the notion of individuality and emphasizes the uniqueness of each person's existence on the planet. This practice may also facilitate various social and administrative processes.
Similarly, in the Unison programming language, the use of unique hashes for identifiers enables precise tracking and version control. By providing each identifier with a unique hash, Unison ensures that even if names or variables change, they can be accurately traced and referenced, simplifying the management of code and ensuring consistency in version control.
Drawing parallels between fictional concepts like the planetary computer and real-world technologies, like programming languages, can lead to fascinating insights and discussions about the potential applications and implications of both. If you decide to write a Wikipedia article on Unison (programming language), it could be an excellent opportunity to explore and expand on these connections further.
1 / 1
Hashes
Ahashin Unison is a 512-bit SHA3 digest of a term or a type's internal structure, excluding all names. The textual representation of a hash is itsbase32HexUnicode encoding.

Unison attributes a hash to every term and type declaration, and the hash may be used to unambiguously refer to that term or type in all contexts. As far as Unison is concerned, the hash of a term or type is itstrue name.

Literal Hash References
A term, type, data constructor, or ability constructor may be unambiguously referenced by hash. Literal hash references have the following structure:

Aterm definitionhas a hash of the form
#x
where
x
is the base32Hex encoding of the hash of the term. For example
#a0v829
.
A term or type definition that���s part of acycle of mutually recursive definitionshashes to the form
#x.n
where
x
is the hash of the cycle and
n
is the term or type���s index in its cycle. A cycle has a canonical order determined by sorting all the members of the cycle by their individual hashes (with the cycle removed).
A data constructor hashes to the form
#x#c
where
x
is the hash of the data type definition and
c
is the index of that data constructor in the type definition.
A data constructor in a cyclic type definition hashes to the form
#x.n#c
where
#x.n
is the hash of the data type and
c
is the data constructor���s index in the type definition.
Abuilt-in referenceto a Unison built-in term or type
n
has a hash of the form
##n
.
##Nat
is an example of a built-in reference.
Short Hashes
A hash literal may use a prefix of the base32Hex encoded SHA3 digest instead of the whole thing. For example the programmer may use a short hash like
#r1mtr0
instead of the much longer 104-character representation of the full 512-bit hash. If the short hash is long enough to be unambiguous given theenvironment,Unison will substitute the full hash at compile time. When rendering code as text, Unison may calculate the minimum disambiguating hash length before rendering a hash

https://www.unison-lang.org/learn/language-reference/hashes/#:~:text=Ahashin%20Unison%20is,or%20type%20in%20all%20contexts.
1 / 1
ChatGPT
ChatGPT
Thank you for providing additional information about Unison's hash system. Unison uses a 512-bit SHA3 digest to represent a term or a type's internal structure, excluding names. This hash serves as a unique identifier for the term or type declaration and can be used to unambiguously refer to it in all contexts. In Unison, the hash of a term or type is considered its true name.
Literal hash references in Unison have specific structures:
A term definition is referenced by a hash of the form #x, where x is the base32Hex encoding of the hash of the term (e.g., #a0v829).
For mutually recursive definitions, a term or type definition is referenced by a hash of the form #x.n, where x is the hash of the cycle, and n is the term or type's index in its cycle.
A data constructor is referenced by a hash of the form #x#c, where x is the hash of the data type definition, and c is the index of that data constructor in the type definition.
In cyclic type definitions, a data constructor is referenced by a hash of the form #x.n#c, where #x.n is the hash of the data type, and c is the data constructor's index in the type definition.
Built-in references to Unison built-in terms or types use a hash of the form ##n (e.g., ##Nat).
Unison also supports short hashes, where a prefix of the base32Hex encoded SHA3 digest can be used instead of the full hash. If the short hash is long enough to be unambiguous in the given environment, Unison substitutes the full hash at compile time. When rendering code as text, Unison may calculate the minimum disambiguating hash length before rendering a hash.
This hash system in Unison enables precise tracking, version control, and unambiguous reference to terms and types, making it easier to manage code and ensure consistency in programming.
1 / 1
No dependency conflicts
Dependency conflicts are, fundamentally, due to different definitions "competing" for the same names. But why do we care if two different definitions use the same name? We shouldn't. The limitation only arises because definitions are referenced by name. In Unison, definitions are referenced by hash (and the names are just separately stored metadata), so dependency conflicts and the diamond dependency problem are just not a thing.

Instead, what we now think of as a dependency conflict is instead just a situation where there are multiple terms or types that serve a similar purpose. Consider an
Email
type, one from v1 of Alice's library, and another from v2 of Alice's library (perhaps included transitively from a different library). We're accustomed to having to stop the world to fix a "broken" build from such a dependency "conflict", but in Unison, it's perfectly fine to have two different
Email
types floating around the codebase. They exist as different types, with different hashes, and you can work with both at the same time (and even write ordinary functions to convert between one and the other).

Of course, over time, you may wish to consolidate those two
Email
types that you have in your codebase, but you can do so at your leisure, rather than your codebase being in a broken state and you being unable to run any code or do anything until this conflict is resolved.

Having multiple versions of "the same" function or type floating around is not really much different than other sorts of duplication that might arise in your codebase, like a handful of similar-looking functions that you notice could all be defined in terms of some common abstraction. When the time is right, you consolidate that duplication away, but there's no need for this natural process to always take precedence over literally all other work.
1 / 1
ChatGPT
ChatGPT
In Unison, dependency conflicts, which often arise due to different definitions competing for the same names in traditional programming languages, are not a concern. This is because Unison references definitions by hash rather than by name. As a result, dependency conflicts and the diamond dependency problem, common in other languages, do not exist in Unison.
Instead, what might be perceived as a dependency conflict in Unison is merely a situation where multiple terms or types serve similar purposes. For instance, if there are two versions of the "Email" type from different library versions, Unison treats them as distinct types with different hashes. There is no need to stop development or fix the "broken" build because of this situation. Both versions of the "Email" type can coexist in the codebase, and developers can work with them simultaneously. Even ordinary functions can be written to convert between these different versions of the "Email" type.
Over time, when it makes sense, developers may choose to consolidate similar definitions like the two versions of the "Email" type. However, there is no urgency to resolve this consolidation immediately. Unlike traditional languages where dependency conflicts might halt progress until resolved, Unison allows developers to work with multiple versions of functions or types, and consolidation can happen at their convenience.
In Unison, having multiple versions of similar functions or types is similar to dealing with other forms of code duplication that can be resolved gradually when appropriate. This flexibility in managing dependencies makes Unison a more adaptable and developer-friendly language, enabling smoother development processes without constant interruptions for resolving conflicts.
1 / 1
���� The big idea
Here's the big idea behind Unison, which we'll explain along with some of its benefits:

����
Each Unison definition is identified by ahashofitssyntax tree.

Put another way, Unison code iscontent-addressed.

Here's an example, the
increment
function on
Nat
:

increment : Nat -> Nat
increment n = n + 1
While we've given this function a human-readable name (and the function
Nat.+
also has a human-readable name), names are just separately stored metadata that don't affect the function's hash. The syntax tree of
increment
that Unison hashes looks somethinglike:

Unison uses 512-bit SHA3 hashes, which have unimaginably small chances of collision.

If we generated one million unique Unison definitions every second, we should expect our first hash collision after roughly 100 quadrillion years! ���

increment = (#arg1 -> #a8s6df921a8 #arg1 1)
So all named arguments are replaced by positionally-numbered variable references, and all dependencies (in this case the
Nat.+
function) are replaced by their hashes. Thus, the hash of
increment
uniquely identifies its exact implementation and pins down all its dependencies.

An analogy:Each Unison definition has a unique and deterministic address (its hash) in this vast immutable address space. Names are like pointers to addresses in this space. We can change what address a name points to, but the contents of each address are forever unchanging.

Benefits
This starting assumption provides some surprising benefits: it simplifies distributed programming, eliminates builds and dependency conflicts, supports typed durable storage, structured refactorings, enables better tools for working with code, and lots more. Let's go over how each of these benefits emerges.

ThisStrange Loop talkcovers many of these ideas as well, though some of the details are out of date.
Simplifying distributed programming
Programming languages today are generally based around the idea that a program is a thing that describes what a single OS process does, on a single computer. Any interaction with things outside the program boundary is done very indirectly, by sending bytes over a socket, say. You can't "just" run a computation elsewhere, you have to send bytes over the network, and then (somehow?) make sure the other end is running a separate program that is listening for those bytes and will deserialize them and hopefully run the computation you want.

With this existing paradigm, distributed computations are described not with one program, but many separate programs stiched together with a morass of glue, duct tape, YAML files, and blobs of JSON being sent over the network.

Moreover, it's complicated to set up all your compute resources to ensure that overall, they act to execute the overall computation you're interested in, and you get none of the niceties of programming languages to help you along the way. When programming for a single machine, you generally have a typechecker that helps ensure all the pieces fit together, and you can abstract over things however you like, introduce reusable components, and so on. This support is notably missing when assembling the correct layer ofstuffneeded to get lots of computers to do something useful in concert.

In Unison, since definitions are identified by a content hash, arbitrary computations can just be moved from one location to another, with missing dependencies deployed on the fly. The basic protocol is something like: the sender ships the bytecode tree to the recipient, who inspects the bytecode for any hashes it's missing. If it already has all the hashes, it can run the computation; otherwise, it requests the ones it's missing and the sender syncs them on the fly. They'll be cached for nexttime.

Of course, there's a lot of engineering that goes into making this work nicely, but the basic idea is simple and robust.
This ability to relocate arbitrary computations subsumes the more limited notions of code deployment, remote procedure calls, and more, and lets us build powerful distributed computing components as ordinary Unison libraries.

����
Spark-like datasets in under 100 lines of Unisonshows how distributed programming libraries can be built up with minimal code in Unison
It is a freeing thing to not have to do any setup in advance of justrunningyour program which can describe whole distributed systems. Rather than having to do a bunch of work "out of band" to ensure your compute resources are ready to run the code they need (like building containers, uploading a container image or jarfile somewhere, or whatever else), in the Unison model of distributed computing, you just run the code and whatever dependencies are missing can be sync'd on the fly.

Of course, distributed programming can still be challenging, and distributed programs are different than sequential, single-machine programs. But distributed programming should not beneedlessly tedious.Let's spend our time on the things that actuallymatterabout distributed programs, not on deployment, setup, and tedious encoding and decoding to move values and computations around!

No builds
In Unison, you're almost never waiting around for your code to build. Why is that?

Because Unison definitions are identified by their hash, they never change. We may change whichnamesare associated with which hashes (and this is used for Unison's approach to refactoring and code evolution), but the definition associated with a hash never changes.

Thus, we can parse and typecheck definitions once, and then store the results in a cache which isnever invalidated.Moreover, this cache is not just some temporary state in your IDE or build tool (which gets mysteriously inconsistent on occasion), it's part of the Unison codebase format. Once anyone has parsed and typechecked a definition and added it to the codebase, no one has to do that ever again.

This idea also applies to caching test results for pure computations (deterministic tests that don't use I/O). There's no need to rerun a deterministic test if none of its dependencies have changed!

The result of this pervasive caching is you spend your time writing code, not waiting around for the compiler.

https://www.unison-lang.org/learn/the-big-idea/
1 / 1
ChatGPT
ChatGPT
The big idea behind Unison is that each definition in the language is uniquely identified by a hash of its syntax tree. In other words, Unison code is content-addressed. This unique identification allows Unison to simplify distributed programming, eliminate builds and dependency conflicts, support typed durable storage, enable structured refactorings, and offer better tools for working with code.
One of the major benefits of Unison's content-addressing approach is its ability to simplify distributed programming. Traditional programming languages often require complex setups and glue code to manage distributed computations across multiple computers. However, in Unison, arbitrary computations can be moved from one location to another with missing dependencies automatically deployed on the fly. This allows for more natural and straightforward distributed programming, where computations can be relocated and executed seamlessly without the need for extensive setup.
Another advantage of Unison's content-addressing is the elimination of builds. Since Unison definitions are identified by their hashes, and the definitions associated with those hashes never change, parsing and typechecking only need to be done once and can be cached indefinitely. This significantly reduces the need for time-consuming builds, as codebase-wide caches of parsed and typechecked definitions can be reused across development sessions. The result is that developers spend more time writing code and less time waiting for the compiler.
Overall, Unison's content-addressing approach provides a powerful foundation for a more streamlined and efficient programming experience, particularly in the context of distributed systems and build processes. By uniquely identifying definitions with hashes and leveraging caching mechanisms, Unison aims to simplify programming tasks and improve the developer experience.
1 / 1
What are some advantages of this strategy?
1 / 1
ChatGPT
ChatGPT
The content-addressing strategy used by Unison offers several advantages that contribute to a more efficient and streamlined programming experience:
Simplified Distributed Programming: Unison's content-addressing approach makes it easier to distribute and run computations across multiple machines. Since definitions are identified by their hashes, they can be relocated from one location to another seamlessly. Missing dependencies can be requested and synced on the fly, simplifying the process of deploying and running distributed programs.
Elimination of Builds: Traditional programming languages often require frequent builds, which can be time-consuming and slow down the development process. In Unison, the content-addressing approach allows for caching of parsed and typechecked definitions. Once a definition has been parsed and typechecked, it doesn't need to be done again, reducing the need for builds and significantly improving development efficiency.
Dependency Conflict Resolution: In many programming languages, dependency conflicts can arise when multiple definitions compete for the same names. Unison's content-addressing eliminates these conflicts, as definitions are referenced by their hashes, not names. Different versions of the same function or type can exist side by side, making it easier to work with multiple versions of libraries or components without causing conflicts.
Structured Refactorings: With content-addressing, Unison can support structured refactorings that allow developers to evolve codebases gracefully. Renaming, reorganizing, or refactoring code can be done with confidence, as the unique hashes ensure that the correct definitions are always referenced.
Efficient Typed Durable Storage: The use of content-addressing makes it efficient to store and retrieve code definitions in a typed durable storage system. Since each definition is uniquely identified by its hash, it becomes easier to manage and persist codebases effectively.
Better Tooling Support: The content-addressing strategy enables better tooling for working with code. Development environments can leverage the unique hashes to provide powerful features like code navigation, auto-completion, and documentation retrieval.
Reduced Development Overhead: By minimizing builds, eliminating dependency conflicts, and simplifying distributed programming, Unison reduces the overhead typically associated with software development. This allows developers to focus more on writing code and less on managing complexities.
Overall, Unison's content-addressing approach provides a foundation for a more efficient, robust, and flexible programming experience, offering benefits across various aspects of software development and distributed computing.
1 / 1
Scholarly articles for "content-addressing approach"
CARTOGRAPHIC DATABASES AND RASTER/ ���
Carmichael �� Cited by 5
FCMAC: A fuzzified cerebellar model articulation ���
Nie �� Cited by 98
What is content addressing ? Content addressing is the way of identifying the content by its fingerprint rather than identifying it by it's location. If someone wants a content with particular fingerprint they can get it from someone who has a copy of the content.Feb 9, 2022
https://blog.cryptostars.is ��� ...
Libp2p: The network engine behind Ethereum, Polkadot and IPFS
Feedback
About featured snippets

Protocol Labs Research
https://research.protocol.ai ��� blog
How Content Addressing Can Solve Streaming Challenges as ...
Jul 23, 2020 ��� Hundreds of thousands of daily users are already experiencing the benefits of this content-addressing approach. As ISPs build an internet ...

Medium
https://medium.com ��� coinmonks
IPFS Whitepaper In Four Minutes. A peer-to-peer, version-controlled���
May 9, 2018 ��� This content addressing approach confirms that the address will always return the same file. Another benefit of this approach is that as ...

SpringerOpen
https://journalofcloudcomputing.springeropen.com ��� ...
A critical survey of live virtual machine migration techniques
by A Choudhary �� 2017 �� Cited by 109 ��� At destination side, distributed content addressing approach is used for transferring pages to the corresponding destination server(s).

MDPI
https://www.mdpi.com ��� ...
Blockchain-Based Distributed Patient-Centric Image ...
by MY Jabarulla �� 2020 �� Cited by 40 ��� The key feature of IPFS in terms of the proposed system is to access medical images through the content addressing approach, rather than location-based ...

arcadia.sba.uniroma3.it
https://arcadia.sba.uniroma3.it ��� ...PDF
Information Management in the Distributed Web

by E MARINO �� 2018 ��� content-addressing approach, the peer-to-peer network provided by IPFS can address and resolve most of the problems the Web suffers from.
300 pages��3 MB
1 / 1
ChatGPT
ChatGPT
I apologize, but it seems that the scholarly articles I can access are limited by my current capabilities, and I cannot provide direct access to specific articles. However, I can suggest some relevant search terms and databases where you may find scholarly articles related to the "content-addressing approach":
Search Terms:
Content Addressing
Content Addressable Storage
Content-based Addressing
Content-based Retrieval
Databases and Platforms to Search:
IEEE Xplore
ACM Digital Library
Google Scholar
ResearchGate
arXiv
SpringerLink
ScienceDirect
By using these search terms and exploring the mentioned databases, you should be able to find scholarly articles that discuss the content-addressing approach and its applications in various fields, such as distributed systems, peer-to-peer networks, and blockchain technology.
1 / 1
How do you think content based addressing might mitigate some of the issues with AI safety?
1 / 1
ChatGPT
ChatGPT
Content-based addressing has the potential to address certain challenges in AI safety by providing unique and immutable identification of AI models and their components. Here are some ways content-based addressing might mitigate AI safety issues:
Model Verification and Reproducibility: Content-based addressing allows for precise identification of AI models based on their content (hash). This enables researchers and developers to verify and reproduce models easily, ensuring that the same model is used consistently across different environments and applications.
Unambiguous Model Reference: Content-based addressing ensures that AI models are uniquely identified by their content, not by names or other metadata. This avoids dependency conflicts and naming collisions, making it easier to manage and integrate models without introducing unintended errors.
Safe Model Distribution: With content-based addressing, it becomes straightforward to share AI models securely. Since the content's hash uniquely identifies the model, there is a reduced risk of distributing incorrect or malicious models.
Version Control and Model Evolution: Content-based addressing facilitates version control for AI models. Different versions of a model can be identified and tracked based on their content hashes, allowing for safer model evolution and easy rollback if needed.
Dependency Management: Content-based addressing can simplify the management of AI model dependencies. Models can be referenced by their content hashes, reducing the chances of compatibility issues and dependency conflicts.
Immutability and Safety Guarantees: Content-based addressing ensures that AI models and their components remain immutable once created. This feature can help enforce safety guarantees and avoid unauthorized modifications to critical models.
Distributed AI Systems: In distributed AI systems, content-based addressing can streamline model sharing and deployment. Nodes in the network can efficiently identify and retrieve required models based on their content hashes, enabling faster and more reliable communication.
While content-based addressing can contribute to AI safety, it is not a complete solution on its own. AI safety involves various other factors, including robustness testing, interpretability, and ethical considerations. Nonetheless, content-based addressing provides valuable features that can enhance the safety and reliability of AI systems.
1 / 1
Connections with the following: 
arXiv
https://arxiv.org ��� cs
Content-aware Neural Hashing for Cold-start Recommendation
by C Hansen �� 2020 �� Cited by 34 ��� We present a content-aware neural hashing-based collaborative filtering approach (NeuHash-CF), ... Subjects: Information Retrieval (cs.IR).
https://arxiv.org ��� cs
Topology-Aware Hashing for Effective Control Flow Graph ...
by Y Li �� 2020 �� Cited by 7 ��� In this paper, we propose a novel fuzzy hashing scheme called topology-aware hashing (TAH) for effective and efficient CFG similarity ...
Subjects: Cryptography and Security (cs.CR)

ACM Digital Library
https://dl.acm.org ��� doi ��� abs
Topic Hypergraph Hashing for Mobile Image Retrieval
by L Zhu �� 2015 �� Cited by 26 ��� A unified topic hypergraph, where images and topics are represented with independent vertices and hyperedges respectively, is first constructed ...

MDPI
https://www.mdpi.com ��� ...
Boundary-Aware Hashing for Hamming Space Retrieval
by W Hu �� 2022 ��� Hamming space retrieval is a hot area of research in deep hashing because it is effective for large-scale image retrieval.
https://www.mdpi.com ��� ...
Attention-Based Asymmetric U-Net for Subject-Sensitive Hashing of ...
by K Ding �� 2021 �� Cited by 4 ��� Subject-sensitive hashing [16] is a special case of perceptual hashing, which can realize the ���subject-sensitive��� authentication of RS ...

ResearchGate
https://www.researchgate.net ��� 2621...
Semantic hashing using tags and topic modeling | Request PDF
This paper proposes a novel hashing approach, Semantic Hashing using Tags and Topic Modeling (SHTTM), to incorporate both
1 / 1
ChatGPT
ChatGPT
The common thread connecting the mentioned research papers and articles is the use of hashing-based techniques in various fields to address specific challenges and improve performance. Hashing is a process that involves transforming input data into fixed-size hash codes or fingerprints, enabling efficient data storage, retrieval, and comparison. Here are the connections with the mentioned papers:
"Content-aware Neural Hashing for Cold-start Recommendation":
This paper introduces a content-aware neural hashing-based collaborative filtering approach (NeuHash-CF) for cold-start recommendation. The authors leverage neural hashing techniques to generate compact hash codes for items, considering their content information to address the cold-start problem in recommendation systems.
"Topology-Aware Hashing for Effective Control Flow Graph":
The paper proposes a novel fuzzy hashing scheme called topology-aware hashing (TAH) for effective and efficient control flow graph (CFG) similarity. By applying hashing techniques, the authors aim to represent and compare CFGs in a more efficient manner.
"Topic Hypergraph Hashing for Mobile Image Retrieval":
This research explores the use of topic hypergraphs and hashing techniques for mobile image retrieval. The authors construct a unified topic hypergraph to represent images and topics independently, facilitating efficient image retrieval based on content.
"Boundary-Aware Hashing for Hamming Space Retrieval":
The paper focuses on boundary-aware hashing for large-scale image retrieval in Hamming space. The authors aim to enhance the effectiveness of image retrieval by leveraging hashing techniques to encode image features in a compact and informative manner.
"Attention-Based Asymmetric U-Net for Subject-Sensitive Hashing of Remote Sensing Images":
This work presents an attention-based asymmetric U-Net model for subject-sensitive hashing of remote sensing images. The authors utilize hashing techniques to achieve subject-sensitive authentication of remote sensing data.
"Semantic Hashing using Tags and Topic Modeling":
The paper proposes Semantic Hashing using Tags and Topic Modeling (SHTTM) as a novel hashing approach. The authors aim to incorporate both tag-based and topic-based semantic information in hashing techniques to enhance information retrieval.
In summary, these papers demonstrate how hashing-based approaches are utilized to address specific challenges in various domains, including recommendation systems, control flow graph similarity, image retrieval, and semantic hashing. Hashing techniques offer benefits such as efficient storage, retrieval, and comparison of data, making them valuable tools in different research areas.
1 / 1
I think it might make it more explainable and maybe more intelligent with larger context windows and more efficient compression
1 / 1
ChatGPT
ChatGPT
Yes, content-based addressing, particularly in the context of hashing techniques, can have several potential benefits in making AI systems more explainable and intelligent:
Improved Explainability: By using content-based addressing, AI systems can uniquely identify and refer to specific components or pieces of information based on their content rather than just their names or locations. This can enhance the transparency and explainability of AI systems, as the references are based on the content's true representation, making it easier to trace and understand the flow of information and reasoning within the system.
Larger Context Windows: Content-based addressing allows AI systems to efficiently store and retrieve large context windows, as it provides a way to represent complex data structures and relationships compactly using hash codes. This capability can lead to a broader understanding of the context and dependencies, enabling the AI system to process more extensive information and make more informed decisions.
More Efficient Compression: Hashing techniques are known for their ability to generate fixed-size hash codes regardless of the input data's size. This efficient compression can help AI systems store and process vast amounts of information using smaller memory footprints. By leveraging content-based addressing, AI systems can efficiently store and retrieve compressed representations of data, contributing to more efficient and scalable AI solutions.
Improved Generalization and Learning: Content-based addressing can aid in capturing essential features and patterns in data. AI systems can use hash codes to represent these features, enabling faster and more accurate learning and generalization. With better representations, AI systems can recognize similarities, differences, and relationships between data points, leading to more intelligent and robust decision-making.
Mitigation of Dependency Conflicts: Content-based addressing can help avoid dependency conflicts, which can be critical in AI safety and system stability. By uniquely identifying components and ensuring their immutability, AI systems can manage dependencies more effectively, reducing the risk of conflicting definitions and potential errors.
Overall, content-based addressing and hashing techniques have the potential to enhance various aspects of AI systems, from explainability and intelligence to efficiency and scalability. As the field of AI continues to advance, incorporating these techniques may play a crucial role in developing more reliable and understandable AI solutions.
1 / 1
A list of everything we have talked about.
1 / 1
ChatGPT
ChatGPT
Certainly! Here's a list of everything we have talked about during our conversation:
Artificial Intelligence (AI) systems and their complexity.
The concept of unmonitorability of AI and its challenges.
Definition of monitorability in AI systems.
The argument for unmonitorability based on incomprehensibility, unpredictability, and unexplainability of AI.
The role of explainability in AI safety and its limitations.
The use of hashes in the Unison programming language.
Advantages of Unison's content-addressing approach.
The comparison of Unison's planetary computer to the concept of a unique name for every living human in "The Dispossessed."
Content-based addressing and its potential benefits in AI safety, explainability, and intelligence.
If there's anything specific you would like to discuss further or any new topic you'd like to explore, feel free to let me know!
1 / 1
8 is not accurate
1 / 1
ChatGPT
ChatGPT
Apologies for the inaccuracy. Let me clarify:
The comparison of Unison's content-addressing approach to the concept of a planetary computer in "The Dispossessed," where every living human receives a unique name based on a hash, similar to how Unison assigns hashes to each term or type declaration.
If there are any other corrections or additional topics you'd like to include, please feel free to let me know.
1 / 1
ChatGPT may produce inaccurate information about people, places, or facts.
ChatGPT July 20 Version
ChatGPT

