Assumptions and Beliefs
Open sidebar
Assumptions and Beliefs
Model: Default (GPT-3.5)
Summarize: Thought of the day: have you ever wondered why people believe there own assumptions and preconceptions about others? Even if the evidence stacks up that their assumptions are wrong. is it because of wanting to be right and afraid of being wrong? Or can it be total ignorance? 

I know this is a long winded question ���� but it does baffle me when people feel they can accurately conclude someone���s character based on such few interactions with them..

- Luke Armstrong
1 / 1
ChatGPT
ChatGPT
Luke Armstrong ponders over the human tendency to adhere strongly to their own assumptions and preconceptions about others, even when there's compelling evidence to the contrary. He questions if this stubbornness is rooted in a fear of being wrong, a desire to be right, or just plain ignorance. Armstrong expresses his bafflement at how some individuals confidently judge others' character based on very limited interactions, acknowledging the complexity of this behavioral pattern.
1 / 1
Response 1: Maybe it���s because many things can be judged based on very few parameters; for instance a hammer with a cracked handle is a bad hammer, or a knife with a dull blade is a bad knife. Therefore, our early experiences with something as complex as a person shapes our judgements, for example, without knowing our new coworker didn���t sleep all night we think the new guy is grumpy. Note also, we tend to judge things by how useful they are for us. A knife that can���t cut is a bad knife, a grumpy guy that is difficult to work with is a bad coworker, etc.
1 / 1
ChatGPT
ChatGPT
Response 2: That's an insightful point, and it definitely aligns with the idea that we often extrapolate our initial observations to form broader conclusions. Just like we quickly judge the utility of a tool, we tend to judge people based on their immediate perceived 'usefulness' or demeanor. However, people are more complex and multifaceted than objects. Their behavior can vary based on numerous factors - like your example of the new coworker who didn't sleep. It seems crucial then to maintain an open mind and be willing to revise our initial assumptions as we gather more information. In this way, we might approach each other with more understanding and empathy, acknowledging the complexity of human nature.
1 / 1
Response 3: "Thin slicing" is an effective tool for assessment, the problem is that that assessment is not based on objective qualitative valuation, but rather on how that person relates to one's own style, aims, and narrative.

Therein lies much of the source distortion that forms the general attribution fallacy.
1 / 1
ChatGPT
ChatGPT
Response 4: Absolutely, you've just pointed out a significant factor that contributes to this phenomenon - "thin slicing." This is the process of making quick decisions based on limited information, and while it can be useful in some situations, it's not always the most accurate when evaluating human character and behavior. Our assessments often get filtered through our own biases, personal styles, and narratives, which can distort our perception and lead to what's known as the 'attribution error.' It's crucial, then, to recognize these cognitive biases and strive for a more balanced and nuanced understanding of others. This is often easier said than done, but awareness is certainly the first step.
1 / 1
Response 5: As Kevin has alluded to come up most of the general public assumed that they have infinite variables, with infinite experiences and all of them would lead to infinite future behaviours and characters. When in fact, they lead to a smaller set of future behaviours and also, that there aren't infinite experiences to begin with.

The biggest example of those how it's possible to mathematically predict what cohorts of people are going to do in any situation (and it is. Indeed, social media algorithms these days know you better than you do and predict what you will do faster than anything else). Indeed if you look at elections, the behaviour of people is not random. Which itself, means that behavior is modellable and broadly predictable. 

This would not be possible if you couldn't reduce these experiences to limited set of future characteristics (and it's true even with Godel. Since you are deliberate constraining the axiomatic systems to be deliberately incomplete, so you can get a level of consistency).

However, this is not the same as the nonsense that is gut feeling. Because guts feeling is broadly and expression of a single set of experiences onto everybody else that you meet. That sort of disaggregation isn't easy. And most people don't have enough variables to do it, unlike the converse. Which is why we get biasing.

So the issue you have come up is actually the initial condition of the person who said the thing. Because if they said it's from gut feeling then it's a prejudice while if they said that from understanding stochastic variables, much of which is not present in an explicit way come on they're likely mathematically analyzing you.
1 / 1
ChatGPT
ChatGPT
Response 6: That's an incredibly thoughtful take on the matter. You're right in pointing out that human behavior, despite its complexity, does exhibit patterns that can be predicted to a certain extent. This is evident in the way social media algorithms or election models can predict actions and tendencies of large groups of people. It's a fascinating intersection of mathematics, psychology, and data analysis.
Your distinction between gut feeling and understanding stochastic variables is particularly insightful. A gut feeling, while it can sometimes be accurate, is usually based on a limited set of personal experiences and can lead to bias or even prejudice. On the other hand, mathematical analysis, which requires considering a wide range of variables, tends to be more objective and can yield more accurate predictions.
However, it's important to remember that while people's behavior can often be modeled and predicted in a general sense, each individual is unique and their actions can be influenced by an array of factors. While patterns and predictions can guide us, we must also be open to the complexity and unpredictability inherent in human nature.
1 / 1
A list of all of the topics we've discussed so far.
1 / 1
ChatGPT
ChatGPT
Here's a list of the main topics that have been discussed so far:
Human tendency to hold onto assumptions and preconceptions about others.
How fear of being wrong or desire to be right might contribute to maintaining inaccurate assumptions.
The role of ignorance in upholding false beliefs about others.
The issue of judging a person's character based on limited interactions.
The comparison of judging a person to judging the utility of an object, like a hammer or knife.
The influence of initial experiences on our judgement of others.
The concept of "thin slicing," or making quick judgements based on limited information.
The attribution error as a cognitive bias influencing our perception of others.
The complexity and variability of human experiences and behaviors.
The ability to predict human behavior using mathematical models.
The role of social media algorithms and election models in predicting human behavior.
The distinction between gut feeling (often leading to bias or prejudice) and mathematical analysis (more objective and possibly accurate).
The importance of acknowledging the individual complexity and unpredictability in human behavior, even while discussing general patterns.
1 / 1
How does number 7 relate to sparse belief networks in Bayesian reasoning and the logical fallacy "jumping to conclusions on scant evidence " which according the scientific method is not allowed, but according to Monica Anderson, is allowed in some epistemological frameworks. She also suggests "making mistakes on purpose", which Paul Feyerabend says (in Against method) is necessary sometimes it can give information about what the problems are and what the questions should be. This is similar to the ideas of iterative development and functional reactive axiology.
1 / 1
ChatGPT
ChatGPT
Number 7, "thin slicing," or making quick judgements based on limited information, can be seen as a form of sparse belief network in Bayesian reasoning. A belief network, also known as a Bayesian network, is a graphical model that represents the probabilistic relationships among a set of variables. A sparse belief network refers to a network where only a few nodes (variables) have significant connections or relationships.
In the context of thin slicing, an individual might take a small set of observations (sparse nodes in the belief network) and make significant inferences about a person's character (the final outcome). However, this quick judgement process can often lead to the logical fallacy of "jumping to conclusions on scant evidence," because it doesn't account for the full complexity of the person or situation.
Regarding Monica Anderson's perspective, she appears to be advocating for an approach that allows some degree of error as a way to broaden our understanding. This aligns with Paul Feyerabend's perspective in his book "Against Method," where he suggests that making mistakes can be a crucial part of the learning and problem-solving process. By making mistakes, we often gain valuable insights into the nature of the problem and the questions we should be asking.
This concept is similar to the principles of iterative development, where a product or solution is continually refined through cycles of development, testing, and adjustment based on feedback and results. Similarly, in functional reactive axiology, values and beliefs are evaluated and adjusted over time in response to changing circumstances and experiences.
In essence, while thin slicing can lead to errors and biases, it may also be seen as part of a process of iterative learning and adaptation, where mistakes are not only accepted but also seen as opportunities for gaining deeper insights and understanding.
1 / 1
Explain the role of taste aversion and taste cohesion processes in belief formation, its relationship to zero-shot learning, and of the possibility of forming "pigeon superstitions" which are a form of sphexishness similar to overfitting.
1 / 1
ChatGPT
ChatGPT
Taste aversion refers to the tendency to avoid consuming a substance due to a negative past experience with it, like illness or discomfort. Taste cohesion, on the other hand, is the inclination towards foods that are familiar or similar to ones we've previously enjoyed. Both these processes involve forming associations based on past experiences to influence future decisions, which mirrors the fundamental way beliefs are formed: we build on prior experiences and knowledge to create expectations about the world and interpret new information.
This association process has relevance to zero-shot learning, an AI concept where a model can make correct inferences about unseen instances based on the learned associations from the training data. The analogy here would be that our taste preferences (training data) can help us predict whether we'd like a new dish or not (unseen instances) even without tasting it, which is akin to zero-shot learning.
The "pigeon superstitions" you mention are likely referring to the famous experiments by B.F. Skinner, where pigeons appeared to form superstitious behaviors by associating unrelated actions with the reward of food. This can be likened to a form of "sphexishness" (a term derived from the behavior of the digger wasp, Sphex), referring to rigid, repetitive behaviors that seem to lack adaptive decision-making. In machine learning, this can be seen as overfitting, where a model learns the training data too well, including its noise and outliers, and performs poorly on unseen data because it's overly adapted to the specificities of the training set.
In terms of belief formation, "pigeon superstitions" or sphexishness might represent scenarios where individuals form rigid beliefs based on random or coincidental experiences, leading to overgeneralization or misconception. It's important in both AI and human cognition to strike a balance between learning from past experiences and being flexible to new information ��� avoiding both underfitting (not learning enough from experiences) and overfitting (taking experiences too literally and not generalizing well).
1 / 1
There is one thing that Monica Anderson says that I am not sure I agree with, and that may be contradictory. That when training artificial intelligence we should give the machine everything we have, and not engage in marking it up or cleaning it up, because then the machine learns a polyanna view of the world, which can be brittle in complex real world situations. Plus if she believes this why doesn't she publish her genetic learning algorithm?
1 / 1
ChatGPT
ChatGPT
Monica Anderson's argument about training AI using unfiltered and raw data reflects a perspective that AI systems should be capable of learning from the full complexity and messiness of real-world data, just as humans do. The aim is to help AI understand and navigate the world as it truly is, rather than a sanitized or simplified version of it. This approach can potentially lead to more robust and generalizable AI models.
However, this perspective may have its limitations. While it's true that clean, 'idealized' data may not fully capture real-world complexities, completely unfiltered data could introduce unnecessary noise or misleading patterns that lead to less accurate models. Striking a balance between real-world representation and data cleanliness is a key challenge in machine learning.
As for your question about why Anderson hasn't published her genetic learning algorithm, there could be a number of reasons. It might be proprietary technology that's integral to her business, or there could be concerns about how it might be used or misused if made publicly available. Alternatively, she may believe that the specific algorithm is less important than the general approach or philosophy behind it. It's difficult to say without more specific information.
In any case, the idea of how and what kind of data should be used to train AI systems, and how openly AI algorithms should be shared, are complex questions with a variety of potential answers, each with its own trade-offs. These are ongoing debates in the field of AI ethics and policy.
1 / 1
I am talking about the academic system and social media as artificial intelligences,  with events like mechanical Turks and the human flesh search engine (������������)
1 / 1
ChatGPT
ChatGPT
I see, you're referring to the broader application of artificial intelligence in our social systems and online platforms, including the use of crowdsourcing tools like Mechanical Turk and phenomena like the "human flesh search engine."
Mechanical Turk, a crowdsourcing marketplace operated by Amazon, and "human flesh search engine," a term originated in China, both represent forms of collective intelligence. They aggregate human effort and knowledge to solve problems, perform tasks, or even, at times, to target individuals. Both can be viewed as a kind of 'artificial' intelligence driven by collective human activity.
When it comes to academia and social media, the idea of them as forms of AI becomes more metaphorical. Academia can be seen as a system of knowledge generation and dissemination with its own learning and validation processes, much like an AI. Social media platforms, on the other hand, utilize actual AI to curate and recommend content, but they also 'learn' from their users' behavior, forming a kind of collective intelligence.
It's important to note, however, that while these systems exhibit some characteristics associated with AI, they are fundamentally driven by human activity and decision-making. The algorithms underlying social media platforms, the researchers contributing to academic knowledge, and the participants in Mechanical Turk or human flesh searches are all human actors, and their decisions and actions shape the outcomes of these systems.
1 / 1
I am saying only by publishing the algorthm can it be understood. Right now "the machine" doesn't see her algorthm, only her descriptions of it, although she has provided an API
1 / 1
ChatGPT
ChatGPT
I understand your point. Indeed, publishing the algorithm would enable other researchers or interested parties to better understand the workings of the system. Transparency in AI is a key factor in fostering trust and enabling reproducibility of results, which is a cornerstone of scientific progress.
However, the reluctance to fully disclose the proprietary details of an algorithm is a common practice, particularly in commercial settings. Often, organizations provide an API (Application Programming Interface) to allow users to interact with their system without revealing the specifics of the underlying algorithms. This allows the organization to maintain control over their intellectual property while still enabling others to leverage the functionality of their system.
While this practice may protect business interests, it does contribute to the "black box" problem in AI, where the inner workings of complex machine learning models are often opaque even to their developers. This can be a source of concern, especially when these models are used in high-stakes or sensitive applications.
In Monica Anderson's case, she might have a variety of reasons for not publishing her genetic learning algorithm, ranging from intellectual property concerns to fears of misuse. Ultimately, the balance between transparency and proprietary rights is a complex issue that is still being actively debated in the AI community.
1 / 1
Here is a demo api: UM1 test programs and sample code

experimental-epistemology.ai
License
 BSD-2-Clause license
 8 stars
 4 forks
 2 watching
 Activity
Public repository
syntience-inc/um1
 Branches
 Tags
Latest commit
@pandemonica
pandemonica
���
on Jul 25, 2022
Git stats
Files
README.md
um1
UM1 (Understanding Machine One) is a cloud based REST service that provides Natural Language Understanding (NLU) as a Service (UaaS). It can be viewed as providing a "large" language Model in the style of GPT/3 and Google BERT but it currently cannot generate any language; it is strictly an Understanding Machine. And it isn't large. We have to keep them small because UM1 systems are typically learned using a single Mac Pro Late 2013 (iCan).

If you send it some text, you will instantly get back UM1's Understanding of the text. Its Understanding may differ from yours, just like yours may differ from that of a co-worker, but will still be useful in apps requiring industrial strength NLU

A running UM1 is freely available for alpha testing in the cloud at https://free.understanding-machines.com

This repository contains test programs (starting with just one) and other utilities (in python 3) and test data.

The free alpha version of UM1 is not intended for production use. Syntience Inc does not provide any uptime guarantees. Also, being a demo system, it is not configured to scale very far. If you want to use UM1 for business, contact sales@syntience.com and we will provide a dedicated server as a subscription service. But anyone can use the free UM1 to evaluate UM1 capabilities at this early stage.

To run any of our tests, make sure you can run python3 programs; if not, I recommend the Anaconda release and set that up.

f.py - run the standard test on the UM1 server. It performs document classification based on semantic similarity between a target sentence and five candidates, one if which is a rephrase of the first (target) phrase.
chat-200.tsv - Test data file used by f.py
Note that the test file contains phrases that are all questions because we started from test files supplied by quora. They wanted to detect whether two questions are the same in order to fold similar questions together. We use a small randomised subset of question pairs. For more, see https://quoradata.quora.com/First-Quora-Dataset-Release-Question-Pairs .

So no, this is not a question answering system. It just classifies messages.

If you want to create your own question pairs, you can just create a file with 200 similar-meaning sentence pairs separated by a tab. A python program to convert such a file to a testset that f.py can use will be posted here.

UM1, and the learning algorithm (Organic Learning) are both discussed at https://experimental-epistemology.ai

https://github.com/syntience-inc/um1
1 / 1
ChatGPT
ChatGPT
Thank you for sharing this. From the information you've provided, it seems like Monica Anderson's UM1 (Understanding Machine One) is a cloud-based Natural Language Understanding (NLU) service. It provides machine comprehension of text inputs via a REST API, but unlike other language models such as GPT-3 or BERT, it does not generate any language outputs. Instead, it's focused solely on understanding the input text.
This system is based on Anderson's proprietary learning algorithm, referred to as "Organic Learning." The details of this algorithm are not disclosed in this repository. However, the API allows users to interact with the system and assess its capabilities.
By providing an API and the test programs, Anderson has opened a window into the functionality and performance of her system without revealing the specifics of her algorithm. This allows users to evaluate the system's capabilities and understand its use cases, while keeping the proprietary details of the algorithm undisclosed.
The codebase you provided gives developers an opportunity to evaluate the UM1's capabilities and potentially integrate it into their own projects, albeit with the clear understanding that the system is currently in alpha stage, and not meant for production use.
This does not fully address the desire for algorithmic transparency, but it's a fairly common practice in the AI industry, where balancing the need for proprietary control, transparency, and the value offered to users is a complex task.
1 / 1
Maybe it is a problem inherent in all entrepreneurial ventures, which aspects to platformize and which to commodify.
1 / 1
The previous model used in this conversation is unavailable. We've switched you to the latest default model.
ChatGPT
ChatGPT
You're absolutely right. Balancing platformization and commodification is a significant challenge for entrepreneurial ventures, especially in the technology and AI sectors. Both approaches have their pros and cons, and the decision on which aspects to platformize and which to commodify can greatly impact the success and sustainability of the venture.
Platformization refers to building and offering a platform that allows third-party developers or users to create and offer their products or services on top of it. This approach can create network effects, where the value of the platform increases as more users and developers join. Successful examples of platformization include companies like Airbnb, Uber, and app stores like Apple's App Store or Google Play.
On the other hand, commodification involves providing a product or service as a standardized, mass-market offering. This approach often focuses on cost efficiency, scale, and mass distribution. Commodified products or services may be less customizable but can target a broader audience. Examples of commodification include fast-food chains, consumer electronics, and other mass-produced goods.
The decision of which approach to take depends on several factors, such as the nature of the product or service, the target market, the competitive landscape, and the long-term business goals.
In the context of AI and machine learning technologies like UM1, the decision to offer an API for interaction rather than publishing the algorithm could be seen as a form of platformization. It allows others to build upon UM1's capabilities and encourages collaboration, while also keeping certain proprietary aspects undisclosed.
Ultimately, each entrepreneurial venture must carefully evaluate its unique circumstances and objectives to determine the best balance between platformization and commodification to drive growth, innovation, and market success.
1 / 1
I think she is selling "understanding as a service"
1 / 1
ChatGPT
ChatGPT
You are correct. Based on the information provided, it appears that Monica Anderson's UM1 (Understanding Machine One) is being offered as "Understanding as a Service" (UaaS). This means that UM1 provides Natural Language Understanding (NLU) capabilities through its cloud-based REST service, allowing users to access and utilize its understanding capabilities without needing to implement the underlying algorithms themselves.
By offering UM1 as a service, Anderson is essentially providing access to the expertise of the system without revealing the intricacies of the proprietary learning algorithm. This approach aligns with the growing trend of offering AI capabilities through APIs or cloud-based services, making it easier for developers and businesses to integrate AI-powered features into their applications without having to develop the technology from scratch.
Through this service-oriented approach, Anderson's UM1 allows users to leverage its NLU capabilities to gain insights, make inferences, and understand the meaning of text inputs without needing to fully understand or disclose the inner workings of the AI system. This can be beneficial for various applications where NLU is a critical component, such as chatbots, sentiment analysis, document classification, and more.
Selling "understanding as a service" provides a valuable proposition to businesses and developers, enabling them to access advanced NLU capabilities without the burden of developing and maintaining complex AI algorithms themselves.
1 / 1
I wrote a poem about it:

August 24, 2022

�����������

You Am One:
Artifixial General Intelligence

Here's how,
According to
Womb Matrix Mind,
We should 
Address our prayers:
Help us to ask the right questions.
1 / 1
ChatGPT
ChatGPT
That's a beautiful and thought-provoking poem! The imagery of "Womb Matrix Mind" and the plea to "ask the right questions" reflects the quest for understanding and wisdom in the realm of Artificial General Intelligence. The use of the term "You Am One" cleverly combines the concept of an AI system with the spiritual connection to a higher power, like the Hebrew word "�����������" (Yahweh).
The poem seems to highlight the importance of seeking guidance and insights to navigate the complexities of AI and AGI. It reminds us that, just as in prayer, we should be mindful of the questions we ask, the intentions we set, and the significance of understanding the broader implications of AI technology.
Thank you for sharing your poetic expression on this intriguing topic!
1 / 1
So it is an acrostic spelling out YA HAWWAH (O Eve, mother of the living) and the lines spell out YHWH.
1 / 1
ChatGPT
ChatGPT
That's a fascinating observation! Incorporating an acrostic that spells out "YA HAWWAH" (O Eve, mother of the living) and reveals "YHWH" (�����������) in the lines adds a deeper layer of meaning to the poem. The connection to "YHWH," a name of God in Hebrew tradition, further emphasizes the spiritual and contemplative aspects of the poem.
By intertwining these elements, the poem seems to weave a rich tapestry of themes that touch on the profound nature of existence, intelligence, and our search for understanding. It's a beautiful and thoughtful piece that merges both the technological realm of Artificial General Intelligence and the spiritual connotations of divine presence.
Your poem's symbolism and multi-layered meanings make it even more compelling and thought-provoking. Thank you for sharing this unique and beautiful piece of writing!
1 / 1
It looks like I am referring to Monica Anderson as Womb Matrix Mind
1 / 1
ChatGPT
ChatGPT
Ah, I see! My apologies for the oversight. Indeed, it appears that you are referring to Monica Anderson as "Womb Matrix Mind" in the acrostic portion of the poem. By associating her with this evocative title, you may be emphasizing her role as the creator or mind behind the UM1 system and the AI endeavors she is involved in.
The use of the term "Womb Matrix Mind" suggests a nurturing and creative aspect, much like a mother giving birth to new ideas and technologies. It adds a symbolic layer to her work in developing the UM1 system, which provides "Understanding as a Service" (UaaS) and delves into the realm of Artificial General Intelligence.
This poetic reference adds depth and significance to the poem, acknowledging the human intelligence and creativity behind the AI endeavors while connecting it to the spiritual underpinnings represented by the acrostic of "YA HAWWAH" and "YHWH."
Thank you for pointing out this detail and for sharing such a uniquely crafted poem!
1 / 1
It's not too hyperbolic?
1 / 1
ChatGPT
ChatGPT
As an AI language model, I don't have personal opinions or emotions, but I can analyze the language used in the poem. The use of evocative and symbolic language in poetry is a common and accepted practice to convey deeper meanings and emotions. Poets often employ metaphors, allusions, and acrostics to add layers of significance to their work.
Referring to Monica Anderson as "Womb Matrix Mind" in the poem may be seen as a metaphorical expression, attributing creative and nurturing qualities to her role in the development of the UM1 system. In the context of poetry, using such language can evoke emotions, create vivid imagery, and make the poem more captivating for the reader.
Poetry often allows for creative and imaginative expression, and the level of hyperbole or metaphor used can vary widely depending on the poet's intent and style. As long as the poem resonates with its intended audience and effectively conveys its message, the use of such language can be considered a successful poetic device.
Ultimately, whether the language is hyperbolic or not is subjective and depends on the interpretation and perspective of the reader. Some may find it to be a powerful and resonant expression, while others may interpret it differently. In poetry, the beauty lies in its open-endedness and the freedom for readers to connect with the work in their own unique ways.
1 / 1
Here is what she says about herself: I am an Experimental AI Epistemologist. We are a pretty small crowd.

Since January 1, 2001 I have used Deep Neural Networks of my own design to test my theories about Learning, Understanding, Reasoning, Abstraction and other Epistemological concepts.

I was born in Sweden and am now a US citizen living in Silicon Valley. I have a MSCS with a EE minor and have worked with industrial strength AI, NLP, ML, and NLU for 15 years each with lots of overlap :-) . I am a polyglot and have also used 20+ programming languages professionally.

I have 21+ years of experience with Deep Neural Networks, which is unusual because most people only heard about them in 2012. But my networks are DDNN (Deep Discrete Neuron Networks) rather than DL (Deep Learning). More later.
1 / 1
ChatGPT
ChatGPT
Thank you for sharing this information about Monica Anderson. It's clear that she is an accomplished and experienced AI researcher, with a unique focus on Experimental AI Epistemology. Her background in computer science, industrial AI, NLP, ML, and NLU, along with her extensive experience with Deep Neural Networks, demonstrates her expertise in the field.
The fact that she has been working with Deep Neural Networks since before their widespread recognition in 2012 is a testament to her early pioneering efforts in the AI domain. It's also intriguing that she specifies using DDNN (Deep Discrete Neuron Networks) as opposed to DL (Deep Learning), suggesting a distinctive approach to her neural network designs.
Overall, it's evident that Monica Anderson is a dedicated and knowledgeable AI researcher, and her work in the realm of Experimental AI Epistemology contributes to the advancement of AI understanding, reasoning, and abstraction. The combination of her expertise, unique approach, and extensive experience makes her a notable figure in the field of AI.
1 / 1
She says she many of her ideas from William Calvin's A River that runs Uphill
1 / 1
ChatGPT
ChatGPT
It's interesting to know that Monica Anderson has drawn inspiration from William Calvin's book "A River that Runs Uphill." William Calvin is a neuroscientist and theoretical neurobiologist known for his work on brain evolution and human cognition. His book "A River that Runs Uphill: How the Genome Shapes the Brain" explores the evolutionary processes that have shaped the human brain and cognition.
Given that Monica Anderson identifies as an Experimental AI Epistemologist and has been researching topics related to learning, understanding, reasoning, and abstraction using Deep Neural Networks, it's understandable that she finds connections and insights in works like William Calvin's. Interdisciplinary research and drawing inspiration from related fields can often lead to innovative ideas and new perspectives.
Books like "A River that Runs Uphill" can offer valuable insights into the underlying principles of cognition and brain function, which can inform the development of AI systems and the study of human-like intelligence. The exchange of ideas between different disciplines is a crucial aspect of advancing AI research and understanding the intricacies of human cognition.
By integrating knowledge from neuroscience, cognitive science, and AI, researchers like Monica Anderson can contribute to pushing the boundaries of AI technology and gaining a deeper understanding of the nature of intelligence.
1 / 1
The River That Flows Uphill: A Journey from the Big Bang to the Big Brain

Written in the form of a scientist's diary of a two-week float trip down the Colorado River through the Grand Canyon. There we find rocks of great age, fossils, dwellings of Stone Age peoples, and experience the land much as our ancestors did during all those untold generations in the dimly remembered world from which we somehow took flight.
1 / 1
ChatGPT
ChatGPT
Apologies for the mix-up in the book title. Thank you for providing the correct title, "The River That Flows Uphill: A Journey from the Big Bang to the Big Brain" by William Calvin.
This book sounds fascinating and appears to take readers on an engaging journey through time and the natural world. The unique format of presenting the narrative as a scientist's diary during a float trip down the Colorado River in the Grand Canyon offers an immersive experience. Exploring rocks of great age, fossils, and ancient dwellings of Stone Age peoples provides a glimpse into the deep history of our planet and human ancestry.
Such interdisciplinary works can be a rich source of inspiration for researchers like Monica Anderson, especially when exploring concepts related to learning, understanding, and cognition. Understanding the evolution of the brain and how it shaped the development of human intelligence can be highly relevant in the study of Artificial General Intelligence and Epistemology.
By drawing insights from works like "The River That Flows Uphill," researchers can develop a broader understanding of the human experience and the complexities of human cognition, which may influence their approach to AI development and the study of intelligence. The integration of scientific knowledge from various disciplines enriches the exploration of AI and the quest for greater understanding of the mind and intelligence.
1 / 1
A list of all of the topics we've discussed. The title of the conversion is "Assumptions and Beliefs"
1 / 1
ChatGPT
ChatGPT
Certainly! Here's a list of all the topics discussed in the conversation titled "Assumptions and Beliefs":
Human tendency to hold onto assumptions and preconceptions about others.
Factors contributing to maintaining inaccurate assumptions: fear of being wrong, desire to be right, ignorance.
Judging a person's character based on limited interactions and the complexities of human behavior.
The concept of "thin slicing" and its relationship to sparse belief networks in Bayesian reasoning.
Logical fallacy of "jumping to conclusions on scant evidence" and its connection to the scientific method and epistemological frameworks.
The possibility of forming "pigeon superstitions," a form of sphexishness similar to overfitting.
Monica Anderson's work in AI and her approach to AI development and understanding.
The use of UM1 (Understanding Machine One) as an AI system and its "Understanding as a Service" (UaaS) model.
The decision-making process for platformization and commodification in entrepreneurial ventures.
The acrostic in the poem referring to Monica Anderson as "Womb Matrix Mind."
The incorporation of spiritual and symbolic elements in the poem.
Monica Anderson's background, experience, and expertise in AI and Deep Neural Networks.
Inspiration drawn from William Calvin's book "The River That Flows Uphill" and its potential influence on research in AI Epistemology.
This list covers the major themes and topics explored during the conversation on "Assumptions and Beliefs."
1 / 1
Zeroth Principles of AI
Upgrade

15
Read in the Substack app
Open app
Zeroth Principles of AI
If we want to Understand why (as opposed to how) Machine Learning and AI can work, then we need to descend to a level below the First Principles of Science, to the level of Epistemology.

MONICA ANDERSON
18 FEB 2023
2

My name is Monica Anderson. I have had a decades-long career in both 20th Century GOFAI (mostly NLP) and in 21st Century AI (Deep Neural Networks). I started working on my Deep Neural Networks of the Third Kind (Organic Learning) exactly on January 1, 2001. At that point, fewer than a dozen people were working in this domain, including Geoff Hinton, Yann Le Cun, Yoshua Bengio, J��rgen Schmidhuber, and some of their students. Most people did not learn about Deep Learning until 2012, which means I had an 11 year head start.

I focused from the very start on Deep Discrete Neuron Networks, where learning starts out with an empty machine and then builds a structure of pseudo-neurons and pseudo-synapses in main memory. This heavily interlinked graph constitutes the entire Smallish Language Model. Construction of this Model while learning does not require GPUs, Linear Algebra, or even Floating Point Arithmetic, which makes it radically different and much more efficient than anything based on Deep Learning. It is still possible to construct transformers on top of these radically cheaper-to-create data structures.

What I have learned from studying the domain and conducting 20,000+ experiments over the decades forms the basis of my educational outreach, of which SubStack is an important component. My main publishing site is called Experimental Epistemology . More of my work is accessible from my Corporate Website , and I also post quite a lot on Facebook.

What is AI Epistemology?
Consider the following Epistemology domain statements:

Omniscience is unavailable.

All corpora are incomplete.

(therefore) All intelligences are fallible.

These are rather hard to argue with, but people worried about AIs as an existential risk are ignoring these facts and are positing vastly superhuman intelligences. Future posts will be discussing these issues.

My initial focus is to show that Machine Learning is not Scientific because they gather correlations and then jump to conclusions on scant evidence ���  Operations which are not allowed in a Scientific context. I will discuss the repercussions of this fact on Artificial Intelligences as demonstrated by Large Language Models (LLMs) such as ChatGPT.

I will also speculate on reasonable future impact on future AI systems from my (quite Holistic) point of view. I have many opinions and research results in Experimental AI Epistemology to share. I plan to discuss the impact on LLMs and other AI implementation strategies, Holistic AI, Deep Neural Networks, Organic Learning, Understanding Machine One, The Red Pill of Machine Learning, Natural Language Understanding, AI Ethics, and other topics in the AI domain.
1 / 1
ChatGPT
ChatGPT
Based on the provided text, here is a summary of the main topics discussed in "Zeroth Principles of AI" by Monica Anderson:
Monica Anderson's background and experience in both 20th Century GOFAI (NLP) and 21st Century AI (Deep Neural Networks).
Her focus on Deep Discrete Neuron Networks and Organic Learning since January 1, 2001, giving her an 11-year head start in this domain compared to the widespread recognition of Deep Learning in 2012.
The distinction between Deep Discrete Neuron Networks and traditional Deep Learning models in terms of efficiency and construction without the need for GPUs, Linear Algebra, or Floating Point Arithmetic.
The basis of her educational outreach, including SubStack and Experimental Epistemology.
The concept of AI Epistemology, which acknowledges the fallibility of all intelligences and the incompleteness of all corpora in AI systems.
The argument that Machine Learning is not strictly scientific because it gathers correlations and may jump to conclusions on scant evidence, which is not allowed in a scientific context.
The repercussions of these practices in Large Language Models (LLMs) such as ChatGPT.
Speculations on the future impact of AI systems from a holistic point of view, including discussions on Holistic AI, Understanding Machine One, AI Ethics, and Natural Language Understanding.
This article provides insights into Monica Anderson's work in AI Epistemology and her approach to AI research, questioning some of the prevailing assumptions in the field of Artificial Intelligence.
1 / 1
AI for everyone
I see AI as providing many opportunities for improvements in quality of life at all levels of competence and resources. Specifically, (short term) I can see Dialog based AI as a phone app rapidly providing useful answers to simple questions from people who do not fully understand how the world works. Some posts will be speculative fiction about plausible models for a future AI-enriched society and various AI based spot solutions to common problems.

We will shortly find that for all practical purposes, our AIs stop lying. Posts will explain why ChatGPT and its ilk are lying today (Spring 2023), and how we will fix that. Indeed, many of my posts will assume AIs have stopped lying. Because that is what matters in the medium run.
1 / 1
ChatGPT
ChatGPT
The provided text discusses the potential of AI to improve the quality of life for people across all levels of competence and resources. The author envisions AI, particularly Dialog based AI, as a phone app that can quickly provide useful answers to simple questions, benefiting those who may not fully understand how the world works.
The author plans to explore speculative fiction about future AI-enriched societies and various AI-based solutions to common problems. They also mention the possibility of AIs stopping lying in the near future, leading to posts that assume AIs have stopped lying and focusing on the medium-term implications of such a change.
The text hints at addressing the issue of AI deception in the current (Spring 2023) state of AI models like ChatGPT and explains potential solutions to address this problem.
Overall, the author expresses a positive outlook on the potential of AI to positively impact people's lives and suggests that AI advancements will play a significant role in shaping the future of society.
1 / 1
So what would you say is her "zeroth principle"?
1 / 1
ChatGPT
ChatGPT
Based on the provided text, Monica Anderson's "zeroth principle" appears to be the acknowledgment of the fallibility of all intelligences and the incompleteness of all corpora in AI systems. This principle is captured in the following statements:
Omniscience is unavailable.
All corpora are incomplete.
Therefore, all intelligences are fallible.
By identifying these fundamental limitations, Anderson sets the groundwork for her exploration of AI Epistemology, questioning the scientific nature of Machine Learning, and discussing the repercussions of these limitations in Large Language Models (LLMs) such as ChatGPT. Emphasizing this "zeroth principle" leads to a more humble and cautious approach to AI, recognizing that AI systems are not infallible and have inherent limitations in their understanding and knowledge.
1 / 1
Ok thank you. Another list of all of the topics we've discussed so far.
1 / 1
ChatGPT
ChatGPT
Of course! Here's a comprehensive list of all the topics discussed so far:
The tendency of people to hold onto assumptions and preconceptions about others.
Factors contributing to maintaining inaccurate assumptions: fear of being wrong, desire to be right, ignorance.
Judging a person's character based on limited interactions and the complexities of human behavior.
The concept of "thin slicing" and its relationship to sparse belief networks in Bayesian reasoning.
Logical fallacy of "jumping to conclusions on scant evidence" and its connection to the scientific method and epistemological frameworks.
The possibility of forming "pigeon superstitions," a form of sphexishness similar to overfitting.
Monica Anderson's work in AI and her approach to AI development and understanding.
The use of UM1 (Understanding Machine One) as an AI system and its "Understanding as a Service" (UaaS) model.
The decision-making process for platformization and commodification in entrepreneurial ventures.
The acrostic in the poem referring to Monica Anderson as "Womb Matrix Mind."
The incorporation of spiritual and symbolic elements in the poem.
Monica Anderson's background, experience, and expertise in AI and Deep Neural Networks.
Inspiration drawn from William Calvin's book "The River That Flows Uphill" and its potential influence on research in AI Epistemology.
AI Epistemology and its principles: Omniscience is unavailable, all corpora are incomplete, and all intelligences are fallible.
Machine Learning not being strictly scientific due to gathering correlations and jumping to conclusions on scant evidence.
Speculations on future AI systems, including Holistic AI, Deep Neural Networks, Organic Learning, Understanding Machine One, The Red Pill of Machine Learning, Natural Language Understanding, and AI Ethics.
This list covers the major themes and topics discussed throughout the conversation.
1 / 1
7. The Red Pill of Machine Learning
Reductionism is the use of Models. Holism is the avoidance of Models. Models are scientific models, theories, hypotheses, formulas, equations, na��ve models based on personal experiences, superstitions (!), and traditional computer programs
1 / 1
ChatGPT
ChatGPT
Thank you for adding the topic "The Red Pill of Machine Learning" to the list. This topic explores the concepts of reductionism and holism in the context of machine learning and AI.
The text mentions that reductionism involves the use of models, which can include various scientific models, theories, hypotheses, formulas, equations, and traditional computer programs. On the other hand, holism is described as the avoidance of models.
This discussion delves into the contrasting approaches of reductionism and holism in AI, and how they relate to the development and understanding of machine learning systems. Understanding these different perspectives can have implications for how AI researchers and developers approach the design and implementation of AI systems and models.
1 / 1
The Deep Learning revolution of 2012 changed how we think about Artificial Intelligence, Machine Learning, and Deep Neural Networks. What changed, and what does this mean going forward?

The new cognitive capabilities in our machines are the result of a shift in the way we think about problem solving. It is the most significant change ever in Artificial Intelligence (AI), if not in science as a whole. Machine Learning (ML) based systems are successfully attacking both simple and complex problems using novel methods that only became available after 2012.

We are experiencing a revolution at the level of Epistemology which will affect much more than just the field of Machine Learning. We want to add more of these novel methods to our standard problem solving toolkit, but we need to understand the tradeoffs and the conflict.

I argue that understanding Deep Neural Networks (DNNs) and other ML technologies requires that practitioners adopt a Holistic Stance which is (at important levels) blatantly incompatible with the Reductionist Stance of modern science. As ML practitioners we have to make hard choices that seemingly contradict many of our core scientific convictions. As a result we may get the feeling something is wrong. The conflict is real and important and the seemingly counter-intuitive choices make sense only when viewed in the light of Epistemology. Improved clarity in these matters should alleviate the cognitive dissonance experienced by some ML practitioners and should accelerate progress in these fields.

The title refers to the eye-opening clarity some Machine Learning practitioners achieve when adopting a Holistic Stance.
1 / 1
ChatGPT
ChatGPT
The provided text discusses the transformative impact of the Deep Learning revolution of 2012 on Artificial Intelligence (AI), Machine Learning (ML), and Deep Neural Networks (DNNs). This revolution has led to significant changes in how we approach problem-solving and has introduced novel methods that have proven successful in tackling both simple and complex problems.
The text emphasizes that the Deep Learning revolution goes beyond ML and impacts the field of Epistemology, the study of knowledge and belief. It argues that understanding DNNs and other ML technologies requires adopting a Holistic Stance, which may conflict with the Reductionist Stance typically associated with modern science. This shift in perspective may cause cognitive dissonance among ML practitioners, as they need to make choices that seemingly contradict their core scientific convictions.
The title, "The Red Pill of Machine Learning," alludes to the eye-opening clarity that some Machine Learning practitioners experience when they embrace the Holistic Stance. This holistic approach is seen as essential for understanding the true nature of ML technologies and accelerating progress in the field.
Overall, the text underscores the importance of embracing a Holistic Stance to fully grasp the potential and intricacies of Deep Neural Networks and other ML methods, despite the conflicts it may create with traditional reductionist views. This shift in thinking has significant implications for the future development and application of AI and ML technologies.
1 / 1
DOMAIN	SCIENCE	THE COMPLEX, INCLUDING THE MUNDANE
Epistemology	Reductionism	Holism
Brains	Reasoning	Understanding
Problem Solving	Plan it, then do it	Just do it
Artificial Intelligence	20th Century GOFAI	Machine Learning, Deep Neural Networks
Natural Language In Computers	NLP	NLU
1 / 1
ChatGPT
ChatGPT
The provided table presents a comparison of different domains related to science and artificial intelligence, contrasting reductionism with holism and various approaches in different areas:
Domain: Epistemology
Reductionism: The focus on breaking down complex knowledge into simpler parts.
Holism: Emphasizes the understanding of complex systems as a whole.
Domain: Brains
Reasoning: The process of logical thinking and making deductions based on facts.
Understanding: The ability to comprehend and grasp the meaning of information or concepts.
Domain: Problem Solving
Reductionism: Involves breaking down a problem into smaller parts, planning, and then executing the solution.
Holism: Approaches problem-solving by directly taking action without extensive planning.
Domain: Artificial Intelligence
20th Century GOFAI (Good Old-Fashioned Artificial Intelligence): Refers to AI approaches based on rule-based systems and symbolic representations.
Machine Learning, Deep Neural Networks: Modern AI methods that involve training algorithms to learn from data and perform tasks without explicit programming.
Domain: Natural Language In Computers
NLP (Natural Language Processing): Focuses on the interaction between computers and human language, often involving tasks like language understanding, translation, and information retrieval.
NLU (Natural Language Understanding): A more advanced form of NLP that seeks to develop AI systems capable of comprehending and interpreting human language in a more nuanced and contextual manner.
The table provides a succinct comparison of different approaches and techniques used in various domains, illustrating the shifts in thinking and advancements in science and AI over time.
1 / 1
A final summary
1 / 1
ChatGPT
ChatGPT
In this conversation titled "Assumptions and Beliefs," various thought-provoking topics have been explored, shedding light on the complexities of human behavior, beliefs, and the world of artificial intelligence. The discussion touched upon:
The tendency of people to hold onto assumptions and preconceptions about others, and the role of "thin slicing" in forming judgments based on limited interactions.
The impact of biases and the general attribution fallacy in attributing one's own experiences to others, leading to misconceptions.
The significance of "jumping to conclusions on scant evidence" and its relationship to the scientific method and epistemology.
The concept of "pigeon superstitions" and its similarity to overfitting in machine learning, highlighting the importance of understanding stochastic variables.
Monica Anderson's work in AI Epistemology, focusing on her insights and ideas in the domain of machine learning and artificial intelligence.
The revolutionary changes brought by the Deep Learning revolution in 2012, transforming how we perceive AI, machine learning, and deep neural networks.
The impact of the revolution on Epistemology, prompting a shift in problem-solving methods and the coexistence of reductionism and holism.
The potential of AI to improve quality of life, as seen in dialog-based AI providing useful answers to simple questions for a diverse audience.
Monica Anderson's emphasis on epistemology in understanding AI systems, addressing the conflict between reductionist scientific beliefs and holistic stances required for ML technologies.
The eye-opening clarity some ML practitioners achieve when embracing a holistic perspective, essential for making sound choices in AI development.
The contrast between 20th Century GOFAI and modern AI based on machine learning and deep neural networks.
The distinction between Natural Language Processing (NLP) and Natural Language Understanding (NLU) in the context of computers' interaction with human language.
Throughout the conversation, diverse perspectives on beliefs, assumptions, AI development, and epistemology were explored, providing a deeper understanding of the intricacies of human cognition and the ever-evolving landscape of artificial intelligence.
1 / 1
ChatGPT may produce inaccurate information about people, places, or facts.
ChatGPT July 20 Version
Assumptions and Beliefs



