Nobody knows for sure when we're going to get the artificial general intelligence that
can outsmart us across the board, but the tipping point is now, because once we get
too close to it, we'll already have lost a bit too much control, I think, as a species
to be able to change the direction.
Now is a time when we are still in charge as humans, if we get this right.
AI can unlock the intelligence inherent in the cosmos and make it work for us.
As long as we humans have worked the earth, we've wondered about how stuff works and gradually
we figured out a lot about how the external stuff works, the physics, chemistry, and so
on.
Then we started to figure out that we are all the kind of machines, we've built machines
that could do things faster and with more power than us and got the industrial revolution.
We could never figure out quite this intelligence stuff, so artificial intelligence is simply
non-biological intelligence.
So what's intelligence?
Intelligence is the ability to accomplish goals.
The more difficult the goals are, the higher the intelligence, traditionally we've had
AI that could only accomplish really narrow goals like playing chess or multiplying numbers.
Now we're getting ever more broad intelligence that can do a wide range of tasks, more like
a human.
Artificial general intelligence means AI that can do all the tasks basically as well
as human, take all our jobs effectively.
And then there's artificial super-intelligence, which is dramatically beyond the human capability.
It might be as far beyond our intelligence as human intelligence is beyond ladybug say.
Early AI researchers totally underestimated how clever evolution had been in designing
our brain.
You have in your brain about 100 terabytes.
It was only pretty recently that you could put that in a computer, but the exponential
growth of our computer technology has totally caught up with that now.
And in addition, as the computers got bigger and we got more data, and we started also
realizing that we could invent algorithms which were a little bit brain-inspired where
the machine learns like a child from data, that ushered in this possibility that the
machines can now get much smarter than their creators, just like a kid can outsmart their
parents because they can learn.
The early AI was all stuff where we humans had to program in the intelligence.
What we call the good old-fashioned AI go-fi is when humans who know stuff program in
instructions for how to do it into a machine, and it sometimes beats humans like Gadi Kasparov
and chess just because it can do more operations per second.
The more modern approach machine learning is where you don't program in the intelligence,
you just give it a lot of data to learn from, and it's actually pretty easy to understand
how you do it.
You build something called an artificial neural network, which is basically like a black box
with a whole bunch of knobs on it.
You put in some data and something comes out depending on what the settings are of these
knobs or parameters as we call them, and then you teach the computer what good means.
Good means when you win the chess game, or good means when it's an accurate translation,
and you just tell it, okay, maximize the goodness, so the computer itself will just go twiddling
all these knobs now and constantly make the thing better and better and low and behold.
If you have a lot of data to train on and a lot of knobs, it often gets incredibly good.
A large language model is simply a kind of artificial neural network, which is trained
on massive amounts of language.
GPT-4, for example, is crudely speaking, read all the text on the internet.
That would take me about 10,000 years to do if I didn't pause for sleeping, and it's
trained that the good just means an accuracy of predicting the next word, which seems super lame.
But it's turned out that just to be really, really good at predicting the next word,
it has to teach itself also all the different languages on earth.
It has to teach itself to kind of model the humans that are writing the words to see,
are they Republican or Democrat, etc.
It's been quite shocking to see how such a simple definition of good that the computer
can train itself on has given these quite broad general intelligent capabilities.
And once you have that basic architecture, depending on what you're training on,
it gets incredibly good at a really wide range of tasks.
I think the media perceives as if something dramatic has just happened all of a sudden,
whereas in fact, as a researcher, what I've seen is just quite steady progress year after year
is just that when it suddenly crosses a threshold where it becomes commercially viable
and everybody starts using it, people outside our little research bubble are like,
you know, something happened.
But it's exactly because of this steady progress that we can feel so confident also
predicting that there's going to be a whole bunch more, even more wild things happening soon.
Since AGI can by definition do all the jobs as well as humans,
that includes the job of AI development.
So as soon as you get to AGI, you can replace all the human AI researchers like me
by AIs that can do it way faster and better.
And that can shorten the R&D cycle for the next generation from the human timescale
of maybe a year to a machine timescale like a week or an hour.
And then if you repeat that many, many, many times over,
you get this exponential runaway, which is often called an intelligence explosion.
Which could leave human intelligence as far behind the machine intelligence
as a cockroach intelligence is behind human intelligence.
Of course, no explosion can go on forever.
And an intelligence explosion will end ultimately when you bump up against the laws of physics.
But there's been some really nice work on that showing that the laws of physics
limit you only to things that are no more than about a million, million,
million, million, million times more powerful than today.
So there's a lot of room for intelligence
that's just dramatically beyond human intelligence.
I've been concerned about and excited about the possibility of advanced AI
for as long as I've known about AI.
Because it's kind of obvious that all the cool stuff
and all the horrible stuff that we humans do, we do with intelligence.
What's really changed is that the timelines have gotten so much shorter.
Five years ago, most of my colleagues thought we would have to wait decades
to pass the Turing test, have machines that could talk so well
that they could fool us into thinking they were human.
And oops, DPT4, just check that box.
And now a lot of very serious people think we might get the AGI
next year, two years from now, or at least very soon.
It's not GPT4 that gives me nightmares.
It's what it represents, namely that we're no longer far away from the things that really scare me.
When Enrico Fermi managed to build the first nuclear reactor
under the Chicago football stadium in the 40s, the big deal wasn't that.
The big deal was that that made them realize that they could build nuclear weapons.
And similarly, the fact that we have been able to now master human language with AI
makes us realize that, you know, we're probably going to be able to master the rest too pretty soon.
It's not really a leap where there's like nothing, nothing, nothing, and then,
kaboom, we all die.
It's a trend that you can already start to see
where we are delegating ever more power to machines.
We already have machines today rejecting some job applications, making bail decisions,
making important military decisions.
And it's so convenient for those in charge, right, to improve the efficiency of this,
improve the efficiency of that, they kind of don't see the forest for all the trees,
that we're gradually giving up power.
And the ultimate source of all the power that we humans have,
the reason we are the most powerful species on the planet, is our intelligence.
We have more power than tigers, not because we have a bigger bicep to sharper claws,
but because you're smarter.
And it's pretty obvious that if we create new entities that are way smarter than us,
there's no guarantee that we're not going to totally lose power to them.
The biggest risk is obviously human extinction, that every single person on the planet is dead.
It's not very far-fetched that we would all go extinct if there's a more intelligent
set of entities on the planet, because it's happened before.
We humans have already driven about half of the other species extinct.
And similarly, if we lose control over our planet, maybe they'll change the environment
so that we can't survive it anymore.
If we protest too much, they'll view us as pests and decide to get rid of us.
It's all about control.
The reason why other species that we wiped out, like the woolly mammoth and the anorthol,
couldn't stop it, because they had lost control over the planet.
They were no longer the smartest.
And similarly, once we abdicate control,
destiny is no longer in our hands.
It doesn't have to be a disaster to be in the presence of more intelligent beings.
You and I have both done this when we were kids.
They were mommy and daddy.
And it worked out for us because their goals were aligned with ours.
The big risk is that the more intelligent beings we're creating now
might have goals that are not aligned with ours.
That's exactly what went wrong for the woolly mammoth, the neanderthals,
and all the other species that we wiped out.
Some people dismiss extinction risk by arguing that when you're more intelligent,
you become less power-seeking and generally sort of more nice.
Whereas I think it's pretty clear from human history that intelligence
is not something that makes you morally good or morally bad.
It just makes you more capable of doing whatever it was that your goals were.
If Hitler had been more intelligent,
they actually think we would have been worse off rather than better off as a species.
And that means it's not enough to just build smart machines.
You have to actually make sure that you give them goals that are aligned with human flourishing.
Just like when you're a parent, you don't want to just teach your child to use
powerful tools and weapons.
You want to really also make sure that they have good values.
I see two main challenges for a good future with AI.
One is figuring out how to make machines actually do what their owners want them to do.
That's often called AI safety.
You don't want your safe self-driving car to suddenly go off a cliff.
The other one is to make sure that society gives the right incentives to all players
so that you don't get some human who wants to take over the world
or wipe out a certain ethnic group or whatever to do that with the aid of AI.
If you just solve the technical problem,
it's not even clear that that's a net positive,
because now someone who wants to do horrible things has more ability to do so.
The truth is that we have really no better understanding of how
a large language model works inside than we have about how our own brain does what it does.
When a large language model convinces this Belgian man to commit suicide
or tried to convince a journalist to leave his wife,
it's not because some engineers were like,
oh, let's mess with these people and put this feature in.
They had no idea that it was going to do those things.
So, mechanistic interpretability is the nerdy name for this quest to change that.
To say, let's look inside the black box, figure out how it's actually doing,
and so that we can figure out whether we should trust it
and maybe even change it to make it worthy of our trust.
Some of my colleagues think we're doomed because the West and China somehow are never
going to cooperate. I think that's too pessimistic, because it doesn't matter if you're American or
Chinese once you're extinct, and the Chinese government is no more interested in losing
control over this technology than the American government is, so they actually have an aligned
interest now that extinction is something they think about and take seriously.
Building ever more powerful AI is not a race that the West can win and China lose or the other way
around. I think the only really realistic outcomes are either that we all get screwed,
all humans, or that we all win in the sense that suddenly Americans are 100 times richer,
the Chinese are 100 times richer, nobody dies of cancer anymore.
It's not a zero-sum game in any way, and the real enemy, therefore, is not any one of the
countries or any one of the tech companies. The enemy is just this dynamic that makes us fight
against each other and fail to collaborate and create a future where we all get way, way, way better off.
Let's remember that many of these challenges we've succeeded in facing before. We've successfully
regulated very powerful tech before in biology. We've successfully managed to find ways of collaborating
with others that we were incredibly distrustful of. When we invented money, for example,
enabled people to start doing mutually beneficial transactions with people they didn't trust at
all. Even when the US really wasn't getting along with Stalin, they still managed to collaborate
enough that they didn't blow each other up. We can totally do this. If we do, it's going to be so
much more rewarding for all parties than the nuclear arms race where it was either a poof
or nothing. Here, either we go extinct or we can have this most incredible future blowing
away even the most optimistic fantasies of sci-fi writers. The space of possible artificial minds
is vastly larger than the space of evolved minds. When we make artificial minds, we have a huge
freedom. What if we can just amplify our intelligence now to cure all those diseases that
take away our loved ones, to eliminate poverty? We humans are actually at this fork on the road
where if we take the wrong turn here, we're going to lose control and probably get wiped out.
I actually think the most likely cause of death for you and me is AI related, not cancer or any
of the old stuff. It's now, it's happening. This is not a lost battle. The way we win it
is to talk calmly about it and do the right things. We know what we need to do already,
we just need to get out there and do it.
