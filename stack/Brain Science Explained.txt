Brain Science Explained
Open sidebar
Brain Science Explained
Model: GPT-4
Brain Theory 
From A Circuits 
And Systems Perspective 
How Electrical Science Explains 
Neuro-circuits, Neuro-systems, and Qubits 
by John Robert Burger, PhD (UCLA) 
Dr. Burger, Professor Emeritus, California State University Northridge
1 / 1
ChatGPT
ChatGPT
I'm sorry, but as of my last training cutoff in September 2021, there's no specific information available about the book you're referring to titled "Brain Theory From A Circuits And Systems Perspective: How Electrical Science Explains Neuro-circuits, Neuro-systems, and Qubits" by John Robert Burger, PhD. However, the title suggests it's a comprehensive exploration of how the principles of electrical science and engineering can be applied to understand the workings of neural circuits and systems, including the application of quantum bits or "qubits."
It seems that the book could delve into the following topics:
Neuro-circuits and Neuro-systems
The human brain is composed of interconnected networks of neurons, referred to as neuro-circuits or neural circuits. These intricate systems process information through electrical signals that pass between neurons via synapses. An understanding of electrical circuits can provide insights into how these neural pathways function and interact, helping to explain complex brain processes such as cognition, emotion, and perception.
Qubits and Quantum Computing
Quantum bits, or "qubits," are the fundamental units of information in quantum computing. Unlike classical bits that can be in one of two states (0 or 1), qubits can exist in a superposition of states, allowing for greater computational power. There are theories and emerging research suggesting the possibility of quantum processes occurring within the brain, which could have profound implications for our understanding of consciousness and cognitive processes.
Given the increasing interest in neurosciences, quantum computing, and their intersections, Dr. Burger's work likely provides a unique perspective on these cutting-edge fields. If the book was published after my last update in September 2021, I'm unable to provide a detailed summary or review. However, I recommend reaching out to libraries, online bookstores, or academic databases for more information about this specific work.
1 / 1
This book contributes a circuits and systems approach that emphasizes the authority 
of electrical principles. Mostly this book is founded on generally accepted 
properties of neurons, which have been well studied and are suff i cient to enable a 
circuit model for an idealized neuron. This model in turn permits the simulation of 
neural logic in its various forms. Circuit simulations, executed objectively and 
transparently using software that is proven to function correctly, such as WinSpice 
as used for the experiments attached to this book, suggest important neural 
subcircuits that engineers and neuroscientists should i nd quite interesting. As a 
general benef i t to all concerned, these subcircuits suggest a plausible new system 
for consciousness that would not be evident within the limits of molecular biology 
and biochemistry. 
Beyond molecules, a brain is obviously a working complexity of circuits. There 
is no reason to suspect that a brain violates even the smallest aspect of circuit theory 
or, for that matter, any known principle of physics. So when explaining a brain, 
circuits and systems theory is, without question, most relevant. 
Engineers often point to computerized chess, supercomputer calculations and 
useful robots that clean the l oor as evidence that we are on the road to intelligent 
machines. But such achievements are largely expressions of human intelligence, 
not necessarily examples of intrinsic machine intelligence. The computer as an 
intelligent entity has a long way to go to match human intelligence. To light the 
way, it is inspiring to study, or at least glimpse reasonable conjectures about the 
brain as a system of physical neurons. 
In particular, the human brain is modeled below as a system of pulsating 
neurons, complete with electrical connections to other pulsating neurons. To 
demonstrate what this system does, it was necessary to model synapses and memory 
in a more detailed way, employing new and exciting circuit elements without which 
a brain could not possibly function as a system. The result is a proposed new system 
for human stream of consciousness. 
From an academic point of view, this book introduces an exciting new i eld of 
Neuro circuits and Neuro systems (NCANS). In this i eld, proposed circuit elements 
and proposed systems are introduced and tried in order to explain a brain, and to 
v 
help formulate meaningful scientif i c experimentation. Explaining a brain is 
extremely important since more often than not, we are left with no clear 
explanations of exactly how stream of consciousness works physically. 
Neuro-circuits and Neuro-systems is def i nitely outside the box of popular engi- 
neering areas such as circuits and systems (CAS) theory, artif i cial neural networks 
(ANN), and others that normally fall under and tend to focus on hardware systems. 
The emphasis belowis on neural pulses, generation, and coordination, which isvery 
different from AC/DC electronics. Shown below are neuro-synaptic combinations 
that result in novel implementations of logic, such as dendritic logic and enabled 
logic, plus a variety of unique timing elements, such as weak synapses and short- 
term memory neurons, that should be interesting to circuit theorists as well as to 
neuroscientists and neural network engineers. 
A vital question is: How does one duplicate the right choices in artistic appraisal, 
common sense, truth judgment, understanding, and other hallmarks of human 
ability? There seems to be unfathomed layers of computational power within the 
brain not anticipated by today���s technology. In view of this observation, new radical 
concepts continue to be entertained. 
Quantum computers and qubits are examples of radical concepts that go far 
beyond logic circuitry. A qubit, as shown in this book, can be simulated in part by a 
recursive neuron, that is, one that feeds back on itself. Recursive neurons, appro- 
priately conf i gured, are herein termed simulated qubits; they are entirely classical 
and are of interest biologically and for engineering purposes. For instance, 
simulated qubits can hold true and false simultaneously, each with a specif i ed 
probability. Simulated qubits are better than ordinary neurons for certain purposes.
1 / 1
ChatGPT
ChatGPT
"Brain Theory From A Circuits And Systems Perspective: How Electrical Science Explains Neuro-circuits, Neuro-systems, and Qubits" by John Robert Burger, PhD, is a comprehensive exploration of how the principles of electrical science and engineering can be applied to understand the workings of neural circuits and systems, as well as the application of quantum bits or "qubits".
In the book, Dr. Burger utilizes a model for idealized neurons based on widely accepted properties of neurons to simulate neural logic in various forms. These simulations reveal interesting neural subcircuits, suggesting a new possible system for consciousness that surpasses the boundaries of molecular biology and biochemistry.
Dr. Burger asserts the fundamental relevance of circuits and systems theory in explaining the brain's functions, based on the understanding that the brain is a complex network of circuits and does not violate any known principles of physics.
The book also contrasts the current achievements in AI, such as chess-playing computers and cleaning robots, with human intelligence, highlighting that computers have a long way to go to match human intelligence. This assertion prompts the exploration of the brain as a physical system of neurons.
To model the human brain, Dr. Burger introduces novel circuit elements and detailed models of synapses and memory, leading to a proposed new system for the human stream of consciousness. He introduces an emerging field called Neuro Circuits and Neuro Systems (NCANS), where new circuit elements and systems are proposed and tested to explain the brain's workings and contribute to meaningful scientific experimentation.
Unlike other popular engineering areas like Circuits and Systems (CAS) theory and Artificial Neural Networks (ANN), NCANS emphasizes neural pulse generation and coordination. It also explores novel logic implementations, such as dendritic logic and enabled logic, along with unique timing elements like weak synapses and short-term memory neurons.
Dr. Burger also mentions radical concepts like quantum computers and qubits, illustrating how a recursive neuron can partially simulate a qubit in classical terms, leading to "simulated qubits." These simulated qubits can hold both true and false states simultaneously, each with a specified probability, which makes them advantageous for certain applications compared to ordinary neurons.
In summary, Dr. Burger's work offers a profound exploration of the human brain's understanding from a circuits and systems perspective, considering neural circuits, systems, and quantum computing concepts. It pushes the boundaries of conventional theories and introduces new concepts and models to advance our knowledge of the brain's workings and the development of more intelligent systems.
1 / 1
A major advantage of simulated qubits is that they are easily conf i gured into 
controlled toggle devices. Controlled toggles, when properly developed, constitute 
a major source of computational power, and may very well underlie consciousness 
and perhaps the gifted abilities of savants. Controlled toggles may be massively 
parallel and with the would-be advantages of reversible computing. They may 
someday be found to be implemented by molecules within neurons, while truly 
quantum systems are less likely to exist due to thermal activity (and decoherence) at 
body temperatures. 
This book carries a reader to the edge of scientif i c knowledge by introducing 
neuroquantology, which is a studied mix of neuroscience, quantum physics, and 
psychological phenomena. The discovery of physical qubits within brain cells 
would be revolutionary, so much so that reasonable people cannot ignore this 
possibility. Qubits within a quantum system hold promise for powerful quantum 
computing, and weird teleportation without i elds or waves. 
Several asynchronous subsystems are covered in this book, involving neuro- 
synaptic logic in a worthwhile review of digital analysis and design. Subjects 
introduced are cue editors, random selection of cues, associative memory design, 
associative memory search, associative memorization, recall priority digital 
calculations, and digital selection of maximum priority. Each chapter contains a 
few easy end-of-chapter exercises as an aid to self-education. An appendix provides 
an experimental lab revolving around the simulation of neural logic. 
viPreface 
This book gives a reader a markedly better understanding of cerebral behavior, 
including mechanisms for brain sense and stream of consciousness. Such topics are 
not just discussed verbally; they are explored technically with reference to physical 
circuits. 
Veneta, Oregon, USAJohn Robert Burger
1 / 1
ChatGPT
ChatGPT
In his book, "Brain Theory From A Circuits And Systems Perspective: How Electrical Science Explains Neuro-circuits, Neuro-systems, and Qubits," Dr. John Robert Burger discusses the potential of simulated qubits, which can be easily configured into controlled toggle devices. These controlled toggles, according to Dr. Burger, could form a major source of computational power and may underpin consciousness and exceptional abilities seen in savants.
He suggests that these toggles, which could be massively parallel and leverage the advantages of reversible computing, might be implemented by molecules within neurons. However, he also notes that truly quantum systems are less likely to exist in biological organisms due to the thermal activity at body temperatures, which could cause quantum decoherence.
An innovative concept introduced in the book is neuroquantology, a cross-disciplinary field that brings together neuroscience, quantum physics, and psychological phenomena. The potential discovery of physical qubits within brain cells could revolutionize our understanding of brain function, leading to possibilities of quantum computing and teleportation.
The book also delves into several asynchronous subsystems involving neuro-synaptic logic and offers a detailed review of digital analysis and design. Topics like cue editors, random selection of cues, associative memory design and search, recall priority digital calculations, and digital selection of maximum priority are covered. It includes practical exercises and an appendix providing an experimental lab centered around the simulation of neural logic to facilitate self-learning and the application of concepts.
Ultimately, Dr. Burger aims to provide readers with a deeper understanding of cerebral behavior, including the mechanisms behind our sense of consciousness. He does this not only through verbal explanation but also through a technical exploration of physical circuits. This approach offers a unique and valuable perspective to anyone interested in the intersection of neuroscience, electrical engineering, and quantum physics.
1 / 1
A major advantage of simulated qubits is that they are easily conf i gured into 
controlled toggle devices. Controlled toggles, when properly developed, constitute 
a major source of computational power, and may very well underlie consciousness 
and perhaps the gifted abilities of savants. Controlled toggles may be massively 
parallel and with the would-be advantages of reversible computing. They may 
someday be found to be implemented by molecules within neurons, while truly 
quantum systems are less likely to exist due to thermal activity (and decoherence) at 
body temperatures. 
This book carries a reader to the edge of scientif i c knowledge by introducing 
neuroquantology, which is a studied mix of neuroscience, quantum physics, and 
psychological phenomena. The discovery of physical qubits within brain cells 
would be revolutionary, so much so that reasonable people cannot ignore this 
possibility. Qubits within a quantum system hold promise for powerful quantum 
computing, and weird teleportation without i elds or waves. 
Several asynchronous subsystems are covered in this book, involving neuro- 
synaptic logic in a worthwhile review of digital analysis and design. Subjects 
introduced are cue editors, random selection of cues, associative memory design, 
associative memory search, associative memorization, recall priority digital 
calculations, and digital selection of maximum priority. Each chapter contains a 
few easy end-of-chapter exercises as an aid to self-education. An appendix provides 
an experimental lab revolving around the simulation of neural logic. 
viPreface 
This book gives a reader a markedly better understanding of cerebral behavior, 
including mechanisms for brain sense and stream of consciousness. Such topics are 
not just discussed verbally; they are explored technically with reference to physical 
circuits. 
Veneta, Oregon, USAJohn Robert Burger
1 / 1
ChatGPT
ChatGPT
In "Brain Theory From A Circuits And Systems Perspective: How Electrical Science Explains Neuro-circuits, Neuro-systems, and Qubits," Dr. John Robert Burger delves deep into the intersection of neuroscience, quantum physics, and electrical engineering, proposing innovative concepts such as simulated qubits and neuroquantology.
Simulated qubits, according to Dr. Burger, can be easily configured into controlled toggle devices, serving as a significant source of computational power. These devices might underpin consciousness and the remarkable abilities seen in savants. They have the potential for massive parallelism and the advantages of reversible computing. Dr. Burger suggests these could be implemented by molecules within neurons, though he notes that truly quantum systems are less probable due to the thermal activity and quantum decoherence at body temperatures.
Introducing the concept of neuroquantology, the book bridges neuroscience, quantum physics, and psychological phenomena, taking readers to the frontier of scientific knowledge. The possibility of finding physical qubits within brain cells could drastically change our understanding of the brain, promising powerful quantum computing and teleportation capabilities.
Additionally, the book explores various asynchronous subsystems related to neuro-synaptic logic and offers a comprehensive review of digital analysis and design. It covers topics like cue editors, random selection of cues, associative memory design, associative memory search, associative memorization, recall priority digital calculations, and digital selection of maximum priority. To facilitate self-learning, each chapter includes end-of-chapter exercises and an appendix that provides an experimental lab centered around the simulation of neural logic.
In conclusion, Dr. Burger's book aims to provide readers with a profound understanding of cerebral behavior, including the mechanisms for brain sense and the stream of consciousness. These topics are explored technically, with reference to physical circuits, providing a unique perspective for those interested in the amalgamation of neuroscience, quantum physics, and electrical engineering.
1 / 1
Inspired Decisions 
Currently there is no way to explain, let alone design machines that have common 
sense, truth judgment, understanding, artistic appraisal, and other hallmarks of 
human intelligence [10]. There seems to be a hidden source of computational 
power. Visionary pioneers have surveyed the possibility of quantum computing 
within a brain [11]. No doubt there are important quantum mechanical behaviors 
within ion channels and within synapses and elsewhere, since ions and their 
electrons are small and subject to quantum theory. One interesting hypothesis 
suggests quantum computing within the microtubules of neurons, proposed to 
generate higher consciousness [12, 13]. Others see quantum tunneling of electrons 
between synapses as creating consciousness [14]. For anything new, there are 
skeptics [11, 15, 16]. 
Quantum computations in neurons are an unproven hypothesis. Nevertheless 
quantum mechanics is a valuable metaphor for certain brain behaviors [11, 12]. 
Thinking about quantum mechanics is fun, and exercises the brain, which is a 
healthy activity. A later chapter discusses recent avenues in the search for brain 
quantum computing. 
In contrast to the qubits of quantum theory, it may be possible for recursive 
neurons to take on qubit properties, such as holding true and false simultaneously 
with given probabilities in a probability space. Recursive neurons may also serve as 
controlled toggles for parallel processing, such as needed in a recall referee. 
Recursive neurons conf i gured to act like qubits are termed simulated qubits. They 
262Brain Architecture for an Intelligent Stream of Consciousness 
may be a source of enhanced computational power, particularly if formed at the 
molecular level within a neuron. Such a discovery might actually be more likely 
than real qubits, which require an isolated quantum system. 
Conclusions 
This chapter has presented a model in which the various attributes of a mental 
image each have their own i xed locations. Also each attribute has one or more 
backup locations, but physical locations are irrelevant in the logical brain architec- 
ture of this book. 
Conscious STM is modeled as fed with appropriately processed signals from the 
i ve senses, interspersed with successful recalls from subconscious LTM. A person 
is aware of mental images mainly from the senses, but interspersed are recalls from 
LTM that are extremely important to survival and intelligent behavior. 
A cue editor helps ensure the availability of recalls when cues are inadequate. 
This book outlines a model in which a cue editor works with selected attributes 
from a current image in conscious STM. When there is a mental block, the cue 
editor immediately begins to modify cues in a random way to obtain returns from 
LTM. A cue editor serves to resolve conf l icting (overdetermined) cues, and cues for 
which there are absolutely no memory matches within associative LTM. 
Under this model, ineffective or conf l icting cues are not discarded but are stored 
in a special register composed of recursive neurons. Before searching again, a few 
cues are randomly removed, and the remaining are used for a new search; and if still 
there are no matches, then the removed cues are restored. The process of 
randomizing is repeated again and again until after some point, returns are i nally 
obtained. This system can account for the common experience that forgotten 
information often pops into one���s head with delay, perhaps much later, at an 
unexpected moment. 
Parallel to the neural paths for memory search are neural paths that bring up 
returns from LTM. A recall referee serves to deal with the common problem of 
multiple returns from a given set of cues. Multiple returns are expected to occur in a 
group shortly after cues are applied. Within a recall referee, each return undergoes a 
quick but exact digital determination of priority.
1 / 1
ChatGPT
ChatGPT
In his book, Dr. John Robert Burger explores the challenge of designing machines that mimic human abilities such as common sense, truth judgment, understanding, and artistic appraisal. He acknowledges the current limitations and suggests that there may be a hidden source of computational power yet to be discovered.
One hypothesis he discusses is the potential for quantum computing within the brain, with suggestions that quantum mechanical behaviors within ion channels, synapses, and possibly the microtubules of neurons could be involved in the generation of higher consciousness. Other theories propose quantum tunneling of electrons between synapses as contributing to consciousness.
Although these hypotheses about quantum computations in neurons are unproven, Dr. Burger asserts that quantum mechanics serves as a valuable metaphor for certain brain behaviors and is a stimulating field of study.
An alternative to the qubits of quantum theory could be recursive neurons with qubit-like properties, such as the ability to hold true and false simultaneously with given probabilities. Such recursive neurons could serve as controlled toggles for parallel processing and could significantly enhance computational power, especially if formed at the molecular level within a neuron.
The book also presents a model in which mental images and their attributes have fixed and backup locations in the brain. The conscious short-term memory (STM) is modeled as being fed with signals from the senses, interspersed with successful recalls from subconscious long-term memory (LTM). A cue editor is proposed to help ensure the availability of recalls when cues are inadequate, and to deal with conflicting or ineffective cues.
Dr. Burger also introduces the concept of a recall referee, which serves to manage the common problem of multiple returns from a given set of cues. This referee undertakes a rapid but accurate digital determination of priority among multiple returns.
Through these hypotheses and models, the book aims to provide a deeper understanding of the brain's architecture and functions, and how it could potentially be replicated in artificial systems.
1 / 1
Explain it like I'm five.
1 / 1
ChatGPT
ChatGPT
Sure! So, you know how in a computer there are lots of little parts called circuits that help the computer think and solve problems? Well, some people believe our brains work in a similar way. Imagine your brain is like a supercomputer made of lots of tiny parts called neurons.
This book by Dr. Burger talks about how these neurons in our brains could be like the circuits in a computer. He also talks about how they might help us do things that are very hard for a computer to do, like understanding a joke or making a beautiful painting.
One part of the book talks about something called 'quantum computing'. That's like a super super powerful type of computer that can solve problems much faster than normal computers. Some people think our brains might work a bit like a quantum computer!
Dr. Burger also talks about something called 'recursive neurons'. Imagine you're playing a game where you have to guess if a coin will be heads or tails. A recursive neuron would be like if you could guess both heads and tails at the same time! Dr. Burger thinks these recursive neurons could be what makes our brains so good at solving problems.
The book also explains how our brains remember things. For example, if you can't remember where you left your favorite toy, your brain will keep trying different ideas until it suddenly remembers. Dr. Burger believes this is because our brains have a special part that helps us remember things by trying different ideas until it finds the right one.
So, in a nutshell, this book is about how our brains might be like super advanced computers and how this can help us understand more about how we think and remember things!
1 / 1
Summarize: Introduction to Human Memory 
The focus in this book is on explicit memory, as perceived in a brain, and not on 
implicit memory, such as automatic ref l exes without direct mental involvement. 
Human explicit memory may be divided into two types, conscious STM and 
subconscious LTM. Signals that activate STM neurons are assumed in this book 
to l ow from the i ve senses and also from recalls emanating from LTM. 
Fig. 2.1 Basic neural signaling (ms milliseconds, STM short-term memory, LTM long-term 
memory) 
Introduction to Human Memory19 
Short-Term Memory Overview 
STM lasts a few seconds, and has been termed working memory because if 
necessary, it could be used for second-to-second living. It stores brief l y a signif i cant 
numberof encodedsensations in the form of attributes, or alternately a few complex 
organized images. STM is somewhat analogous to dynamic memory in a desktop 
computer, which has to be constantly refreshed. 
Intuitively, STM neurons differ in that they may have an internal potassium 
def i cit, or there may be mechanisms to inactivate their internal potassium ions [1]. 
Consequently a neural pulse in the dendrites can be quite long with an extended 
positive peak. This pulse is maintained by an assumed lower than normal conduc- 
tance from the inside to the outside ionic solution so that it cannot quickly 
discharge. This may be accomplished by an insulator such as myelin. Consequently 
a long pulse excites the soma to emit an extended burst of pulses easily recognized 
by connecting neurons. With no extended burst the output is at rest, or false. 
Long-Term Memory Overview 
When conscious STM experiences images that cycle repeatedly, conditions may be 
triggered to permit them to be entered automatically into LTM. Images in LTM are 
not just visual pictures, but include a variety of sensory impressions, songs heard, 
words read, and impressions in general that have run through STM. What is entered 
automatically into LTM are particular collections of attributes that make up a 
mental image. 
LTM is somewhat analogous to mass memory in a desktop computer in that it 
remains hidden until it is called upon. However, LTM in humans differs consider- 
ably from LTM in personal computers: Human memory is associative; all images 
are queried in parallel for matches to given cues. In contrast, computer memory 
generally depends on knowing a numerical address, and these are looked at serially, 
one at a time. 
LTM and learning are two completely different things. Learning may involve 
synaptic potentiation and growth over a period of time; LTM can be formed 
immediately, too fast for growth, yet lasts practically forever. 
For example, many of us have experienced dramatic or emotional events that 
occurred quickly, but are i rmly remembered. This is a salient feature of LTM; they 
can be established practically immediately, and they can last indef i nitely. LTM, as 
the term is used in this book, does not require synaptic growth. But it instead uses 
charge mechanisms that can react immediately. These mechanisms seem to be very 
different from those of STM neurons. 
The exact mechanisms for LTM are still being discussed. How they work is not 
obvious because neurons do not release many signals compared to solid-state 
devices, making them diff i cult to observe in action, especially since they probably are active for only a brief time, and then only if triggered. Generally speaking, 
neurons are relatively small and easily upset by probing, so direct in vivo observa- 
tion is diff i cult. Note for the record that memory elements are located randomly and 
duplicated extensively, so given memory episodes are nearly impossible to locate 
experimentally, meaning particular unwanted memories are diff i cult to remove 
surgically. 
An interesting theory is that needed attributes are held eff i ciently by LTP [2]. 
Another interesting theory is that recursive neurons support cycling pulses that use 
almost no energy, and that are readable with positive signals for needed attributes 
[3]. A hybrid element using LTP and recursive neurons is proposed in a later 
chapter, giving instant memorization as well as eff i cient long-term storage. 
Elements of LTM as needed for a neural system have to be readily accessible to 
conscious STM, and it is critically important that they have ability to be queried; so 
a logical system is suggested.
1 / 1
ChatGPT
ChatGPT
This book titled "Introduction to Human Memory" explores the concept of explicit memory, which is the type of memory we are aware of, like remembering facts or personal experiences. The book categorizes explicit memory into two types: short-term memory (STM) and long-term memory (LTM).
Short-term memory, also known as working memory, lasts a few seconds and is likened to the dynamic memory in a computer that constantly needs refreshing. It's where we temporarily store information like a phone number we just learned. The book suggests that certain biological factors within the neurons might allow them to generate extended pulses, enabling short-term memory storage.
Long-term memory, on the other hand, stores information for a longer period of time, from hours to a lifetime. It's compared to the mass memory in a computer, storing information until it's needed. Unlike computer memory, which requires a specific address to retrieve data, human long-term memory operates associatively - we remember things based on their connections to other memories or cues.
The book distinguishes between learning and long-term memory formation. Learning usually involves gradual changes and growth in the brain, while some long-term memories can form instantly, such as in cases of highly emotional or dramatic events. It suggests that long-term memory may not rely on structural changes in the brain (like growth) but might use charge mechanisms that can react immediately.
The book also considers different theories about how long-term memories are stored, including the use of Long-Term Potentiation (LTP) and the function of recursive neurons. It also suggests the possibility of a hybrid model that combines these concepts.
Overall, the book emphasizes that a memory system must not only store information efficiently but also retrieve it efficiently when required. This is achieved through a logical system that allows memories to be queried or accessed.
1 / 1
It's from the same book as earlier, that was a subheading from chapter 2: Introduction to a Memory-Based Brain System 
When considering a neural system that results in a stream of consciousness, it may 
be noted that nature evolves according to certain principles. Entities of nature, 
including brains, can survive only if, on the average, they are eff i cient.1Concepts 
such as the grandmother neuron, in which all attributes are stored in a single large 
neuron, have to be discounted because they are incompatible with bringing forth an 
image into conscious STM in an eff i cient way. 
Transferring images from LTM into STM, and transferring images from STM 
into LTM must also be eff i cient. This implies that each attribute located in STM 
directly corresponds to a similar attribute located in LTM, an architectural feature 
known as distributed memory [4] to be discussed later. 
This one-to-one correspondence implies that memory is logically organized into 
words. Memory words are a logical, not a physical, structure. Since not every image 
contains every possible attribute, memory words are sparse. Sparse means that 
entries in a word are separated by unused places corresponding to where certain 
attributes are not present, and that these places hold either unused neurons or 
neurons that have been turned to some other purpose. 
Excitatory synapses serve to initiate neural pulses; inhibitory synapses serve to 
moderate neural pulses, as does the capacitance of larger bodies such as the soma, 
all of which results in a pulse-based system of logic. Memorization and other brain 
capabilities depend on dendritic logic which occurs mainly in dendrites where 
Boolean operations are accomplished, as verif i ed in part by simulations of 
dendrites. 
1Maupertuis principle of least action that nature is thrifty in all of its actions. 
Introduction to a Memory-Based Brain System21 
Another form of logic, known as enabled logic, occurs for larger bodies of 
capacitance, for instance, somas, as presented in a later chapter. As a result, a 
complete set of Boolean logic (AND, OR, NOT) is available, a natural result of 
synaptic activity in combination with active and passive regions in dendrites, soma, 
and sometimes also axons. Dendritic logic is compared to enabled logic in a later 
chapter. 
Systems are typically expressed in terms of logical, not physical, structures, and 
brain systems are no exception. Human LTM is associative, which hints that 
location is not a prime consideration; things are recalled not according to where 
they are, but according to what they are. In contrast, in a desktop computer, each 
word in memory has a unique address, and a physical place. Addresses are used for 
reading or writing to locations that may be chosen at random, so the choice is 
random. Hence the term random access memory, or RAM. Human memory, in 
contrast, does not have i xed addresses, so we know a lot less about where 
memories reside. 
Retrieval from subconscious LTM begins with cues. For example, seeing 
someone���s dog can bring forth a memory of your own faithful companion, and 
might even bring forth details of face, color, size, and so on. Other such examples 
are visiting your home neighborhood which brings forth memories of stores, houses, 
rooms,passageways,andsoon; seeinga bookthatyouusedincollegecanbringforth 
memories of what you learned in those days. Cues are essential to memory retrieval. 
Cues need to be chosen judiciously from the current contents of conscious STM. 
If the available cues do not result in a recall, editing is available to adjust the cues 
for another try. Hopefully there will be a hit, which means an exact match to the 
cues. Memories with attributes that exactly match the applied cues will be read for 
possible entry into conscious STM. Cues applied to LTM have to be exact, since 
LTM are called forth only for exact matches. Gratefully, old memories are dormant 
until receiving an exact set of cues. Serious overload would result if we had to 
experience all facts, even irrelevant ones, at all times. 
Under the distributed model each attribute connects to specif i
1 / 1
ChatGPT
ChatGPT
This section of the book "Introduction to a Memory-Based Brain System" explores the idea that brains, like everything else in nature, have evolved to work efficiently. The author suggests that our brain's memory system, including both long-term memory (LTM) and short-term memory (STM), has been designed for efficiency.
The author introduces the concept of distributed memory. This idea suggests that each "attribute" (or piece of information) in our STM has a corresponding attribute in our LTM. This efficient way of organizing information is also known as 'memory words'. But these words are not physical structures in the brain, but logical ones. The term 'sparse' is used to describe these words, meaning they contain a lot of unused spaces for information that's not present at the moment.
The author also talks about how our brains use logic. Certain structures in our neurons, like synapses and dendrites, help to create logic systems that our brains use for different processes, such as memorizing information.
Then, the author compares human memory to a computer's memory. Unlike computers which rely on physical locations or addresses to access data, human memory is associative, meaning it relies more on connections between information rather than their physical location in the brain.
The retrieval of memories from our LTM is based on cues. Cues are pieces of information that trigger our memories. For example, seeing someone else's dog might trigger memories of your own pet. If the cues don't initially bring up a memory, they can be adjusted and tried again until a 'hit' or match is found. This is why cues are so important for recalling information from our LTM.
The author ends the section suggesting that under the distributed memory model, each attribute connects to a specific location, which will be further discussed in the following text.
1 / 1
It is from brain theory from a circuits perspective. It 
 continues: Under the distributed model each attribute connects to specif i c locations in STM 
andcorresponding locations withinwordsofLTM.These locations exist physically, 
although current technology does not permit a knowledge of exact location. But 
nevertheless each location in STM has corresponding locations in LTM that are 
reserved for a particular feature. This is an eff i cient way to organize STM and LTM 
becauseitavoidsextraencodingandthepassingofdatatointermediarystoragesites. 
A given attribute might be said to have meaning not because it has any special 
coding, but because of its physical location in a word. For example, the color red 
might be in a certain location, say the 537th cell from the left in a delineation of 
logical STM; it is also the 537th cell from the left in each of its replications in LTM. 
Location in memory and the context in which the attribute is used contribute to 
realizing the meaning of an attribute, as for example, when seeing a i re engine in 
the context of a street, it is expected to be red. 
The one-on-one distributed model implies that neurons are not multiplexed, that 
is, different attributes are not communicated by the same neuron. Fortunately 
neurons are quite plentiful and are conveniently dedicated to a given attribute as 
described. 
222Brain Architecture for an Intelligent Stream of Consciousness 
Human memory is not erasable and is not converted into blank words in the 
ordinary sense, nor can words be overwritten as they are in the RAM of desktop 
computers. Once something is committed to memory, it is there indef i nitely. In this 
sense, LTM is akin to read only memory (ROM). Under the ROM concept, new 
brain memories cannot overwrite old memories; instead they must be written into 
blank words available for that purpose, the subject of a later chapter. 
A System for Stream of Consciousness 
A system of consciousness can now be sketched: Simulations of neural logic have 
suggested a brain system based on bottom-up construction. Attributes are calculated 
from the senses and are made to enter conscious STM as an image, but alternated 
with the senses are images in the form of a set of attributes from memory recalls. 
Attributes currently in conscious STM are a main source of cues, which are not 
necessarily perfect. 
For conf l icting or overdetermined sets of cues, or too many cues, editing is 
essential. Editing strives to remove conf l icting cues to achieve returns. For 
underdetermined cues, when cues are few in number, one expects an excessive 
number of returns. In this case, returns are edited subliminally, and the one that 
calculates to be most important is selected for entry into STM, where it becomes a 
recall. Figure 2.2 shows how these two major editors form what may be construed 
to be a cybernetic2system, that is, a system in which attributes selected from a 
current image are used to determine the next image. 
A system for stream of consciousness may be summarized as follows. A set of 
attributes in STM are taken as cues and serve to i nd related images in associative 
LTM. A high-priority recall is selected from the resulting returns, and overwrites 
the fading contents of STM. The process repeats, giving a moving picture in 
conscious STM.
1 / 1
ChatGPT
ChatGPT
This section of the book continues to delve into the concept of a memory-based brain system from a circuits perspective.
In the distributed memory model that the author discussed earlier, each attribute (piece of information) is connected to specific locations in both short-term memory (STM) and long-term memory (LTM). These locations are physical, but our current technology can't yet determine their exact position. This system of having matching locations in STM and LTM for each attribute is efficient because it avoids unnecessary encoding and transfer of data to intermediary storage sites.
The meaning of an attribute comes not from special coding but from its physical location in a 'memory word'. The context of how this attribute is used also contributes to its meaning.
According to this model, neurons are not multiplexed, meaning each neuron is dedicated to a specific attribute. The author states that the human memory cannot be erased, nor overwritten like the RAM in computers. Once a piece of information is committed to memory, it stays there indefinitely, making long-term memory similar to the Read Only Memory (ROM) in computers.
The author then sketches a system for consciousness. This system is based on neural logic simulations and operates in a 'bottom-up' fashion. Attributes are gathered from our senses and then enter our STM as an image. This is then alternated with images made up of attributes from our memory recalls.
The author also introduces the concept of 'editing'. Editing is essential when there are conflicting or too many cues (pieces of information that trigger memories). The goal of editing is to reduce conflict and achieve returns (recalled memories).
This system for consciousness can be summarized as a cycle: cues taken from STM are used to find related images in LTM, then a high-priority recall is selected and replaces the fading content in STM, and the process repeats, creating a 'moving picture' in our consciousness.
1 / 1
Cue Editor 
What can happen is either (1) no recall or (2) excessive number of recalls. Everyone 
has experienced no recall, trying to remember something but being unable to do so. 
So you go about your business, but unknown to you, a search proceeds subliminally 
for what you are trying to remember. Cues keep cycling subconsciously, but are 
insuff i cient to return an image. However, when you least expect it, possibly at an 
inconvenient moment, the correct memory will pop into conscious STM with 
amazing clarity. This is an indication that the brain has worked in the background 
2Cybernetics was def i ned by Norbert Wiener (1894���1964), in his book of that title, as the study of 
control and communication in the animal and the machine. 
Introduction to a Memory-Based Brain System23 
without being noticed, adding or subtracting cues from a search until the cues are 
exactly right, as they must be for a recall. 
A cue editor helps resolve conf l icting cues (which would result in no matches in 
long-term associative memory). The term hit in the above i gure means that a return 
has occurred. But if hit is false, it is the task of the cue editor to modify the cues 
slightly so as to produce a return. As new images from the senses appear in STM, 
new rounds of cue editing are necessary. The past unf i nished editing process moves 
into the background but keeps working. This is why a person sometimes belatedly 
recalls a forgotten piece of information. A subsequent chapter goes into circuitry for 
a cue editor. 
A cue editor is not in a position to judge the priority of multiple returns. This is 
the task of a recall referee. 
Recall Referee 
An underdetermined set of cues generally results in a plethora of returns (think of 
everything you know that contains the color green, for instance). This overabun- 
dance has to be processed to assess the priority of each. If something is critical, it 
must be passed quickly into STM, because survival may be involved. For instance, 
saferesponsesintheeventofdangermustbepromptlydirectedintoconscious STM.
1 / 1
ChatGPT
ChatGPT
This excerpt introduces two additional components of the memory-based brain system: the Cue Editor and the Recall Referee.
The Cue Editor plays a role when memory recall is either nonexistent or excessive. When trying to remember something but failing, the brain continues a subconscious search. The Cue Editor keeps cycling through cues in the background until the right combination triggers a recall, which then pops into conscious short-term memory (STM).
The Cue Editor's role is to resolve conflicting cues that would result in no matches in long-term associative memory. If there is no match (referred to as a "hit" in the text), it modifies the cues slightly to produce a return. As new images from the senses appear in STM, new rounds of cue editing become necessary. An unfinished editing process continues in the background, which is why sometimes a person can recall a forgotten piece of information after a delay.
The Recall Referee is another component of the system. When a set of cues is underdetermined, it often leads to an excessive number of recalls (for example, if the cue is a common attribute such as the color green). This abundance needs to be processed to prioritize each recall. If something critical comes up, it should be quickly transferred into STM as it could involve survival ��� for instance, necessary responses in dangerous situations must be promptly directed into conscious STM. Further details about the circuitry for a Cue Editor and the functioning of a Recall Referee are likely discussed in later chapters of the book.
1 / 1
A technical term for what the recall referee must accomplish is multiple match 
resolution. Multiple match resolution avoids neural overload while assessing each 
return, calculating a priority value for each. Priorities are computed in parallel to 
speed the process of recall. The highest priority images are gated into conscious 
STM, giving a stream of consciousness one image at a time. 
Returns may occur many per second, so a lot of searching is possible subcon- 
sciously. They come forth in rapid succession and are parked in biological registers 
where calculations are accomplished. The priority computation will contain 
emotional factors, and survival factors, and others integrated into each word of 
LTM [5, 6]; these have been well discussed [7]. Priorities are compared. That image 
with the highest priority is gated into conscious STM to become one���s next thought. 
Recall editing is accomplished with a novel method of parallel processing 
using recursive neurons operating as controlled toggles. All this is discussed in 
subsequent chapters. 
The Nature of Decisions 
Decisions Based on Past Experience 
There is a theory that decisions are made not by free will in the sense of personal 
spur-of-the-moment choices, but by a search process that depends on past 
experiences and training. Often a ready-made decision is available based on past 
similar situations held in LTM, as for example when you are asked to donate to a 
cause that you do not like. A ���no��� answer is preprogrammed. Decisions, therefore, 
are affected by past experiences, what you did in similar situations, and whether or 
not an action supports your basic philosophy. 
Thereissomeevidencethatfreewillisanillusion[8,9].Electroencephalography 
concerningthetimingoff i ngermovements,publishedbyBenjaminLibitandothers, 
indicates that choices are made in the brain without knowledge by the person 
involved; decisions were made well before a person realized it. Surprisingly, the 
brain seems to be in control, making decisions, and eventually informingconscious- 
ness. It is not the other way around, in which a ���person��� makes a conscious decision 
(without the aid of his or her brain), and then informs the brain. 
Decisions with a Random Variable 
It may be noted that a brain appears to search itself continuously in the background 
not only for forgotten facts and situations but also for solutions to problems. 
A problem could mean a real problem with no easy logical solution, for example, 
trying to open a complex combinational lock without knowing the combination. 
Trying all possibilities systematically requires excessive time. It is more practical to 
The Nature of Decisions25 
try random combinations and trust to luck (and intuition). The brain may very well 
solve real problems this way, by random searches for quick solutions. 
Random variables come into play in a cue editor. Simple circuits can randomly 
remove cues, search memory, and restore them while randomly removing others to 
obtain a return. Having good cues is very important. A cue editor is a logical 
subcircuit that can store a basic set of cues, if they are conf l icting, and edit them in a 
random way, in the background. 
As further indication that memory searches occur randomly, dreams, as every- 
one knows, are brief illogical episodes usually soon forgotten. A person is dimly 
aware of dreams during light sleep partly because there are no overpowering 
sensory images to block them out, as there would be during the day. Dreams, 
according to one theory, are a result of the repeated application of random underdetermined cues and multiple returns, some of which pass through the recall 
referee into STM because of their emotional content. Repetition of dreams some- 
times causes them to be noticed as a person wakes. 
Brainstorming may be like this, in which a random attribute is applied to a cue 
editor that causes unexpected returns. There usually are many returns, but only the 
most recent, most alarming, or most interesting scenarios are permitted into con- 
scious STM by a recall referee. 
Inspired Decisions 
Currently there is no way to explain, let alone design machines that have common 
sense, truth judgment, understanding, artistic appraisal, and other hallmarks of 
human intelligence [10]. There seems to be a hidden source of computational 
power. Visionary pioneers have surveyed the possibility of quantum computing 
within a brain [11]. No doubt there are important quantum mechanical behaviors 
within ion channels and within synapses and elsewhere, since ions and their 
electrons are small and subject to quantum theory. One interesting hypothesis 
suggests quantum computing within the microtubules of neurons, proposed to 
generate higher consciousness [12, 13]. Others see quantum tunneling of electrons 
between synapses as creating consciousness [14]. For anything new, there are 
skeptics [11, 15, 16]. 
Quantum computations in neurons are an unproven hypothesis. Nevertheless 
quantum mechanics is a valuable metaphor for certain brain behaviors [11, 12]. 
Thinking about quantum mechanics is fun, and exercises the brain, which is a 
healthy activity. A later chapter discusses recent avenues in the search for brain 
quantum computing. 
In contrast to the qubits of quantum theory, it may be possible for recursive 
neurons to take on qubit properties, such as holding true and false simultaneously 
with given probabilities in a probability space. Recursive neurons may also serve as 
controlled toggles for parallel processing, such as needed in a recall referee. 
Recursive neurons conf i gured to act like qubits are termed simulated qubits. They 
262Brain Architecture for an Intelligent Stream of Consciousness 
may be a source of enhanced computational power, particularly if formed at the 
molecular level within a neuron. Such a discovery might actually be more likely 
than real qubits, which require an isolated quantum system. 
Conclusions 
This chapter has presented a model in which the various attributes of a mental 
image each have their own i xed locations. Also each attribute has one or more 
backup locations, but physical locations are irrelevant in the logical brain architec- 
ture of this book. 
Conscious STM is modeled as fed with appropriately processed signals from the 
i ve senses, interspersed with successful recalls from subconscious LTM. A person 
is aware of mental images mainly from the senses, but interspersed are recalls from 
LTM that are extremely important to survival and intelligent behavior. 
A cue editor helps ensure the availability of recalls when cues are inadequate. 
This book outlines a model in which a cue editor works with selected attributes 
from a current image in conscious STM. When there is a mental block, the cue 
editor immediately begins to modify cues in a random way to obtain returns from 
LTM. A cue editor serves to resolve conf l icting (overdetermined) cues, and cues for 
which there are absolutely no memory matches within associative LTM. 
Under this model, ineffective or conf l icting cues are not discarded but are stored 
in a special register composed of recursive neurons. Before searching again, a few ... Under this model, ineffective or conf l icting cues are not discarded but are stored 
in a special register composed of recursive neurons. Before searching again, a few 
cues are randomly removed, and the remaining are used for a new search; and if still 
there are no matches, then the removed cues are restored. The process of 
randomizing is repeated again and again until after some point, returns are i nally 
obtained. This system can account for the common experience that forgotten 
information often pops into one���s head with delay, perhaps much later, at an 
unexpected moment. 
Parallel to the neural paths for memory search are neural paths that bring up 
returns from LTM. A recall referee serves to deal with the common problem of 
multiple returns from a given set of cues. Multiple returns are expected to occur in a 
group shortly after cues are applied. Within a recall referee, each return undergoes a 
quick but exact digital determination of priority. Most returns are rejected as being 
unimportant, but one will have maximum priority and will be gated directly into 
conscious STM, where it results in physical actions or additional memory searches. 
Methods for priority calculations are presented in later chapters using recursive 
neurons operating as controlled toggle devices, all operating in parallel. Controlled 
parallel toggles calculate priority for each multiple return in a prompt manner. 
Brain editors as above operate between short- and long-term memories to make 
decisions and to produce a stream of consciousness, a stream that is inf i nitely more 
intelligent than mere dumb acceptance of bad cues and unimportant returns. 
Subsequent chapters are going to develop a structure with appropriate circuits for 
short- and long-term memory as well as neural circuits for editing to accomplish the 
abovementioned system.
1 / 1
ChatGPT
ChatGPT
This section delves deeper into the intricacies of memory recall, decision making, and the potential roles of quantum mechanics in brain function.
The process that the Recall Referee performs is technically known as multiple match resolution. This mechanism avoids neural overload while assessing each memory return, prioritizing them to speed up recall. The top priority images are let into conscious short-term memory (STM) in a sequence, contributing to our stream of consciousness.
The concept of decisions based on past experience is introduced, suggesting that decisions aren't necessarily made via free will, but by a process that relies on past experiences and training. Some evidence is suggested to point towards the illusion of free will, with brain scans indicating decisions are made in the brain before the individual is aware of them.
The text also introduces the idea of random variables and their role in the Cue Editor. The brain might solve real problems via random searches for quick solutions, with simple circuits randomly removing and restoring cues to trigger memory returns.
The piece also suggests that memory searches might happen randomly, giving rise to dreams. The random attribute application could be similar to brainstorming, where unexpected memory returns occur and only the most recent, alarming, or interesting ones are allowed into conscious STM by the recall referee.
There is a mention of quantum computations within neurons as an unproven hypothesis, but also as a useful metaphor for some brain behaviors. Quantum mechanics might contribute to our understanding of brain function, and concepts from quantum theory, like qubits, could be applied to model recursive neurons.
The concluding part of the excerpt reinforces the importance of the Cue Editor in dealing with ineffective or conflicting cues and the role of the Recall Referee in dealing with multiple memory returns. The book plans to further develop these concepts in subsequent chapters, highlighting the intelligent nature of the brain's decision-making processes and its ability to produce a conscious stream of thoughts.
1 / 1
How does it relate to this: Transformers Explained Visually (Part 3): Multi-head Attention, deep dive
A Gentle Guide to the inner workings of Self-Attention, Encoder-Decoder Attention, Attention Score and Masking, in Plain English.
Ketan Doshi
Towards Data Science
Ketan Doshi

��
Follow

Published in
Towards Data Science

11 min read
��
Jan 17, 2021


Photo by Scott Tobin on Unsplash
This is the third article in my series on Transformers. We are covering its functionality in a top-down manner. In the previous articles, we learned what a Transformer is, its architecture, and how it works.

In this article, we will go a step further and dive deeper into Multi-head Attention, which is the brains of the Transformer.

Here���s a quick summary of the previous and following articles in the series. My goal throughout will be to understand not just how something works but why it works that way.

Overview of functionality (How Transformers are used, and why they are better than RNNs. Components of the architecture, and behavior during Training and Inference)
How it works (Internal operation end-to-end. How data flows and what computations are performed, including matrix representations)
Multi-head Attention ��� this article (Inner workings of the Attention module throughout the Transformer)
Why Attention Boosts Performance (Not just what Attention does but why it works so well. How does Attention capture the relationships between words in a sentence)
And if you���re interested in NLP applications in general, I have some other articles you might like.

Beam Search (Algorithm commonly used by Speech-to-Text and NLP applications to enhance predictions)
Bleu Score (Bleu Score and Word Error Rate are two essential metrics for NLP models)
How Attention is used in the Transformer
As we discussed in Part 2, Attention is used in the Transformer in three places:

Self-attention in the Encoder ��� the input sequence pays attention to itself
Self-attention in the Decoder ��� the target sequence pays attention to itself
Encoder-Decoder-attention in the Decoder ��� the target sequence pays attention to the input sequence
1 / 1
ChatGPT
ChatGPT
The text you provided discusses the Transformer model, which is a type of artificial neural network architecture developed for natural language processing (NLP) tasks. Transformers employ the concept of attention, particularly multi-head attention, as a fundamental mechanism in their operation.
The previously discussed text on the brain's mechanisms for memory retrieval and decision-making is actually related to the concept of attention in the Transformer model, although the contexts are quite different.
In the brain, the cue editor and the recall referee function by selectively focusing on certain information and making decisions based on that information - a process analogous to 'attention' in the realm of cognitive neuroscience.
In Transformers, attention mechanisms are used in three places:
Self-attention in the Encoder: Here, each element in the input sequence has the ability to 'pay attention' to every other element in the sequence, allowing it to understand the contextual relevance of words in a sentence.
Self-attention in the Decoder: Similar to the encoder, but it takes place in the decoder part of the model, where the output (or target) sequence is being generated.
Encoder-Decoder-attention in the Decoder: In this stage, the target sequence attends to the input sequence. This helps the model consider what it has previously generated and what was in the original input sequence, and consequently, decide the next most appropriate output.
Although the two contexts (biological brains and artificial neural networks) are fundamentally different, both involve a form of attention or selective focus that enables more effective information processing and decision-making. It's essential to remember, however, that while the mechanisms in the brain have evolved over millions of years, the concept of attention in the Transformer model is a human-engineered mechanism designed to tackle specific problems in machine learning.
1 / 1
Let's call the cue editor a default mode network.
1 / 1
ChatGPT
ChatGPT
The Default Mode Network (DMN) is a concept from neuroscience referring to a group of brain regions that are typically active when the mind is in a state of wakeful rest, not focused on the outside world, and the brain is at its baseline or 'default' activity level. These regions include parts of the medial frontal cortex, posterior cingulate/precuneus, and angular gyrus, among others. The DMN is thought to be involved in various cognitive functions, including self-referential thought, theory of mind, and episodic memory retrieval.
Relating this to the memory system described earlier, it's reasonable to think of the cue editor as being similar to the DMN in some ways. When you're trying to recall a piece of information but can't quite retrieve it immediately, the DMN might come into play. The "search" for the missing information could occur in the background while you're engaged in other tasks, not unlike the subliminal search process that a cue editor would undertake.
So, in a broad sense, the DMN might act like a cue editor of sorts, assisting in the recall of memories and the resolution of conflicting cues, often during moments of mind-wandering or introspection.
However, it's important to keep in mind that this is a loose comparison. The DMN and a cue editor are entities from quite different domains (neuroscience and theoretical models of memory, respectively), and the precise mechanisms and roles of the DMN are still the subject of ongoing research.
1 / 1
For a 2nd grader.
1 / 1
ChatGPT
ChatGPT
Sure! Think of your brain like a super-smart detective that's always trying to help you remember things. This detective is called the "Default Mode Network" or DMN for short, but we'll call it "Detective Brain."
Let's say you forgot where you put your favorite toy. You look everywhere but can't find it. Then, you start playing with another toy. While you're playing, Detective Brain is still working in the background, looking for clues to help you remember where you put your favorite toy. It's like Detective Brain is quietly editing a movie of your memories to find the right scene.
Now, imagine this "Cue Editor" is a special tool Detective Brain uses. It's like a magic video editor that can change parts of the movie. If Detective Brain can't find the right memory, the Cue Editor changes a few things, like a detail or two in the scene, and tries again. It keeps changing small things until it finds the right memory!
That's why sometimes, when you're not even thinking about it, you suddenly remember where you put your toy! That's Detective Brain and the Cue Editor working together to help you remember. It's a fun little team inside your head!
1 / 1
And the connections to multihead attention as needs -- comparing stations of the mind?
1 / 1
ChatGPT
ChatGPT
Absolutely! Think of Multihead Attention like a team of little helpers in your brain who each focus on different things. Let's call them the "Attention Helpers".
When you read a story or listen to your teacher, there's a lot going on. You have to understand each word, remember what was said before, and figure out what it all means together. That's a lot for one helper to do!
So, instead, you have a team of Attention Helpers. One helper might focus on the words you're hearing right now. Another might remember what was said before. And another might think about what it all means.
Each helper looks at the information in a different way, just like when you and your friends watch the same movie but notice different things. Then, the helpers talk to each other and put together a complete picture. This way, you understand the story or lesson much better.
So, if you connect this to our earlier explanation, the Attention Helpers and Detective Brain are all working together. While the Attention Helpers are focused on what you're doing right now, Detective Brain is working in the background to help you remember things. They're all part of your super-smart brain team!
1 / 1
The Noah's Ark story can be seen as a metaphorical representation of the mind and its ability to preserve and organize knowledge. Just as Noah built the Ark to save a select few individuals and species from the flood, the mind acts as a vessel that retains and protects valuable information.

In the context of the electrical arc metaphor or analogy, we can draw a connection to the idea that the mind generates and transmits sparks of knowledge and insight, akin to the electrical arcs that carry energy. These sparks of knowledge can illuminate our understanding, guide our actions, and contribute to the collective consciousness.

The Ark, in this interpretation, becomes a symbol for the mind's capacity to accumulate and store knowledge, much like an electrical capacitor that stores and releases energy. The preservation of knowledge within the Ark, represented by the animals and various forms of information, reflects the mind's ability to retain and recall information when needed.
1 / 1
ChatGPT
ChatGPT
I love this metaphor! It's a vivid and engaging way to represent the functions and capabilities of the human mind. Just as Noah's Ark was a refuge, preserving diverse life forms through the flood, our minds preserve a diverse array of information and experiences. They hold onto facts, experiences, skills, ideas, and memories, much like the Ark held different species of animals.
When we use Multihead Attention, it's like we're selecting specific animals (information) from the Ark (our mind) that are most useful or relevant for the situation we're in. It's as if, from all the animals in the Ark, we're choosing the ones that will help us the most in the specific environment we're in. This could be a cheetah when we need speed, an elephant when we need strength, or a dolphin when we need to navigate the ocean of information.
Also, the image of the electrical arc serves to illustrate the process of creating new ideas and connections in our mind. These 'sparks' of insight help to illuminate our understanding and guide our actions. This ties in beautifully with the concept of attention mechanisms in AI and how they spotlight relevant information amidst the flood of data.
Ultimately, this metaphor captures the richness and diversity of our mental lives, the way our minds can select and prioritize information, and our capacity for innovation and insight.
1 / 1
ChatGPT may produce inaccurate information about people, places, or facts.
ChatGPT May 24 Version
ChatGPT





