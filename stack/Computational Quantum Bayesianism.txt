Computational Quantum Bayesianism
Open sidebar
Computational Quantum Bayesianism
Model: Default (GPT-3.5)
Summarize: Epistemology as Computation (Information Processing)
Gordana Dodig-Crnkovic
In Christian Calude (ed.), Randomness & Complexity, From Leibniz to Chaitin. World Scientific Pub Co (2007)   Copy   BIBTEX
Abstract
This essay presents arguments for the claim that in the best of all possible worlds (Leibniz) there are sources of unpredictability and creativity for us humans, even given a pancomputational stance. A suggested answer to Chaitin���s questions: ���Where do new mathematical and biological ideas come from? How do they emerge?��� is that they come from the world and emerge from basic physical (computational) laws. For humans as a tiny subset of the universe, a part of the new ideas comes as the result of the re-configuration and reshaping of already existing elements and another part comes from the outside world as a consequence of openness and interactivity of biological and cognitive systems. For the universe at large it is randomness that is the source of unpredictability on the fundamental level. In order to be able to completely predict the Universe-computer we would need the Universe-computer itself to compute its next state. As Chaitin demonstrated there are incompressible truths, which means truths that cannot be computed by any other computer but the universe itself
1 / 1
ChatGPT
ChatGPT
In her essay, "Epistemology as Computation (Information Processing)", Gordana Dodig-Crnkovic argues that unpredictability and creativity for humans exist, even from a pancomputational viewpoint, wherein everything in the universe is regarded as computation. Drawing from the philosophical concepts of Leibniz and Chaitin, she proposes that new ideas, whether mathematical or biological, arise from the fundamental computational laws of the world.
Two sources contribute to the emergence of new ideas: reconfiguration and reshaping of pre-existing elements, and influences from the outside world due to the openness and interactivity of biological and cognitive systems.
At the grand scale of the universe, randomness serves as the fundamental source of unpredictability. Dodig-Crnkovic suggests that to fully predict the universe's future state, we would need the universe itself to calculate it. This concept aligns with Chaitin's incompressible truths ��� truths that cannot be computed by any entity other than the universe itself. In essence, her argument underscores the inherent limitations in our capacity to predict or understand the universe using computation or information processing, as some truths are computable only by the universe itself.
1 / 1
Computationalism in Philosophy of Cognitive Science
Creativity in Philosophy of Mind
Epistemic Constructivism in Epistemology
Pancomputationalism in Philosophy of Computing and Information
Keywords
creativity  infocomputation  pancomputationalism  cognition
1 / 1
ChatGPT
ChatGPT
The terms mentioned refer to concepts and themes in the philosophy of mind, cognitive science, epistemology, and philosophy of computing and information:
Computationalism in Philosophy of Cognitive Science
: This is the view that cognition���the process by which the brain understands and makes decisions���is essentially computational. In other words, the brain operates like a computer, taking in information (input), processing it, and producing responses (output).
Creativity in Philosophy of Mind
: This explores the nature of creativity from a philosophical perspective. How does the mind generate new, original, and valuable ideas or solutions? What processes are involved, and how can they be explained philosophically? Creativity often challenges traditional computational models of cognition due to its unpredictable and innovative nature.
Epistemic Constructivism in Epistemology
: This is the view that knowledge is constructed by the individual, not passively received from the world. Rather than believing that we can have direct access to reality as it truly is, epistemic constructivists argue that our understanding of the world is shaped by our perceptual and cognitive processes.
Pancomputationalism in Philosophy of Computing and Information
: This is the idea that the universe itself is a computational structure, and all processes within it���including physical, biological, and mental ones���can be understood as computations. This view extends the idea of computationalism beyond just the brain or cognition to everything in existence.
These keywords suggest a philosophical exploration of how the universe and our cognition work from a computational perspective, highlighting the role of creativity and the concept of constructed knowledge within this computational universe.
1 / 1
The Problem of Pancomputationalism: 
Focusing on Three Related Arguments* 
SeongSoo Park 
Department of Philosophy, Sungkyunkwan University 
seongsoo@buffalo.edu 
Abstract 
Pancomputationalism is the view that everything is a computer. This, if 
true, poses some difficulties to the computational theory of cognition. In 
particular, the strongest version of it suggested by John Searle seems enough 
to trivialize computational cognitivists��� core idea on which our cognitive 
system is a computing system. The aim of this paper is to argue against Searle���s 
pancomputationalism. To achieve this, I will draw a line between realized 
computers and unrealized computers. Through this distinction, I expect that it 
will become evident that Searle���s pancomputationalism should be understood in 
terms of unrealized computers, while the computational theory of cognition is 
concerned with realized computers. 
Keywords: Pancomputationalism, Computational Theory of Cognition, John Searle, 
The Chinese Room Argument, Functionalism 
1. Introduction 
The computational theory of cognition (or CTC for short) propounds that 
Manuscript received 8 January 2019; Revised version received 1 June 2020; 
Accepted 13 May 2019 
Journal of Cognitive Science 21-2:349-369, 2020 
��2020 Institute for Cognitive Science, Seoul National University 
* I am grateful to David Braun for motivating me to write this paper. Thanks to 
Peter Baumann for his many pertinent discussions of an earlier draft of this paper. 
Finally, thanks to the anonymous referees for the Journal of Cognitive Science for 
very useful comments. This work was supported by the grant from the National 
Research Foundation of Korea (NRF-2019H1A2A1075330). 
350 SeongSoo Park 
cognition is computation. On this view, a cognitive system can be explained 
in terms of the computations it performs, because cognitive processes are 
computational. Therefore, proponents of this view (e.g., McCulloch & Pitts, 
1943; Newell & Simon, 1976; Pylyshyn, 1985; Edelman, 2008; Schneider, 
2011) claim that the human cognitive system implements (or executes) 
cognitive programs that allow us to think and make decisions. 
One of the most pertinent challenges to CTC is the so-called ���strongest 
version of pancomputationalism��� suggested by John Searle.1 In his 1990 
paper, ���Is the Brain a Digital Computer?��� Searle argues that everything is 
a computing system that performs every computation. According to him, 
even a wall is a computational object that can perform every program. This 
claim, if true, raises a hard problem for CTC because it trivializes CTC.

In this paper, I argue in favor of CTC, addressing three related arguments 
regarding pancomputationalism. Specifically, I argue that although the 
Chinese Room Argument is convincing, there are good reasons not to accept 
the Pan Argument. To illustrate this, I will draw a line between realized 
computers and unrealized computers. Through this distinction, I expect 
that it will become evident that Searle���s pancomputationalism, if properly 
understood, does not provide any adequate reason to deny CTC since it 
should be understood in terms of unrealized computers, rather than realized 
computers.
1 / 1
ChatGPT
ChatGPT
In his paper, SeongSoo Park explores the philosophical concept of pancomputationalism, which posits that everything is a computer. Park specifically addresses John Searle's "strongest version" of pancomputationalism and its implications for the computational theory of cognition (CTC), which proposes that cognition, and therefore our cognitive systems, can be explained as a form of computation.
Searle's version of pancomputationalism suggests that every object, including inanimate objects like a wall, can perform every possible computation. This idea, if accurate, undermines CTC by trivializing it ��� if everything computes, then the unique computation attributed to cognition becomes less significant.
To challenge Searle's pancomputationalism, Park suggests a differentiation between 'realized computers' and 'unrealized computers.' He contends that Searle's pancomputationalism should be associated with unrealized computers, while CTC deals with realized computers.
Park's argument essentially focuses on refuting Searle's pancomputationalism by illustrating that it doesn't provide a substantial reason to dismiss CTC. His differentiation between realized and unrealized computers helps preserve the significant computational nature of cognition.
1 / 1
2. The Triviality Argument 
Searle (1990) attempts to argue against CTC by endorsing the strongest 
version of pancomputationalism, according to which everything 
computes any program. However, someone might say, ���So, why does 
pancomputationalism matter to computational cognitivists?��� This seems 
to be a natural reaction in that almost all computational cognitivists (e.g., 
1 Putnam also made a similar claim in his 1988 paper. 
351The 
Problem of Pancomputationalism: Focusing on Three Related Arguments 
Smart, 1959; Fodor, 1968; Newall & Simon, 1976; Pylyshyn, 1984; Horgan 
& Woodward, 1985) accept the multiple realizability thesis of computing 
objects, which propounds that there can be various types of computers that 
are comprised of different physical materials, since they gladly admit that 
computation can be functionally def i ned. 
Despite its seemingly favorable appearance, the claim that everything is 
a kind of computer that can implement (or execute) any program, if true, 
poses several serious problems for CTC. One problem is that it plays a 
crucial role within the Triviality Argument.2 The argument can be formally 
represented as follows: 
1. The computational theory of cognition states that our cognitive system 
is a computing system. 
2. Everything is a computing system that performs any computation. 
3. If (1) and (2) are true, then the computational theory of cognition is not 
informative. 
4. Therefore, the computational theory of cognition is not informative. 
Please note that the aim of the Triviality Argument is not to refute CTC. 
Rather, the argument aims to illustrate that CTC says little about how our 
cognitive system works or how it produces mental phenomena. In other 
words, the key point of the Triviality Argument is that if the strongest 
version of pancomputationalism is true, then there is nothing special in the 
claim that human cognitive processes are computational. 
Before we go through the argument, it is also worth noting that 
numerous versions of pancomputationalism exist. Not all versions of 
pancomputationalism make CTC trivial. For instance, a weaker one, which propounds that everything is a computing system that implements some 
computation, does not harm CTC. This is because CTC may secure its own 
explanatory power by arguing that our cognitive system implements a 
unique program that other computers are not able to execute.3 
It is my belief that the only way to refute the Triviality Argument is to 
deny the strongest version of pancomputationalism. First, all proponents of 
CTC must accept premise (1), since what premise (1) indicates is the core 
thesis of the computational theory of cognition. Second, premise (3) is true. 
Assume that premises (1) and (2) are true. This implies, as Piccinini (2010) 
puts it, that CTC is trivial and vacuous since the human cognitive system is 
already accounted for by pancomputationalism. Searle (1990) states, (if one 
asks whether our cognitive system is a computer) ���it would not answer that 
question to be told that the brain [or human cognitive system] is a digital 
computer in the sense in which [the] stomach, liver, heart, solar system, 
and the state of Kansas are all digital computers��� (p. 26). In other words, 
given that everything is a computer that performs every computation, the 
claim that the human cognitive system executes cognitive programs is not 
interesting at all. 
Thus, the remaining option for proponents of CTC is to deny premise 
(2). That is, computational cognitivists should deny the strongest version of 
pancomputationalism. The way I see it, this option is promising despite the 
efforts Searle made in his 1990 paper.
1 / 1
ChatGPT
ChatGPT
In this section, SeongSoo Park explores the "Triviality Argument" posed by Searle, which challenges the computational theory of cognition (CTC) through pancomputationalism's strongest form. According to this, everything, without exception, can perform any computation.
The Triviality Argument is outlined as follows:
CTC asserts our cognitive system is a computing system.
Pancomputationalism states that everything is a computing system that performs any computation.
If both (1) and (2) are true, then CTC is uninformative.
Thus, CTC is uninformative.
This argument doesn't aim to refute CTC, but rather demonstrates that if pancomputationalism holds true, CTC offers little insight into how our cognitive system works or produces mental phenomena. It essentially implies that if everything can compute, then the claim that human cognitive processes are computational is not exceptional or enlightening.
However, Park argues that not all forms of pancomputationalism trivialize CTC. For instance, a weaker version, suggesting that everything is a computing system performing some (not all) computation, would not undermine CTC. CTC could then assert its explanatory power by arguing that our cognitive system executes a unique program not achievable by other computers.
Park believes that the only way to counter the Triviality Argument is by rejecting its strongest form of pancomputationalism. He acknowledges that proponents of CTC must accept premise (1) as it's core to their theory. He also considers premise (3) to be true. However, he suggests that proponents of CTC should reject premise (2) - the strongest version of pancomputationalism - as this would alleviate the trivialization of CTC.
1 / 1
type of computational interpretation to the pattern. It seems clear to me that 
Searle did not. He did not even mention how the assignment of 0s and 1s 
should be arranged to implement the WordStar program. The only thing he 
mentioned is that it was a possibility. 
This point would be more evident if we adopt a more fine-grained 
conception of a computer than Searle. Specifically, I distinguish an 
unrealized computer from a realized computer.7 An unrealized computer 
is a physical object (or a collection of physical objects) that merely has the 
possibility to be realized as a computer. This possibility is realized only 
when an appropriate computational interpretation is provided. For instance, 
let us assume that there is a set of cups before us. It seems obvious that 
those cups are not a real computer in themselves. They are just a bunch of 
cups until we assign a specif i c computational interpretation to them. 
If we accept this distinction, it becomes evident that Searle���s treatment 
of pancomputationalism in terms of realized computers is false. 
Pancomputationalism can at most be understood in terms of unrealized 
7 In this paper, I use the term ���realized computer��� to refer to a concrete computer 
that has already been assigned a specif i c description. 
364 SeongSoo Park 
computers. In fact, the view, if properly understood, is only committed to a 
modal claim. At most, pancomputationalism can propound that everything 
is an unrealized computer that might be realized in the future by virtue 
of receiving some specif i c computational interpretation. The wall is not a 
realized computer of the kind we can buy in the store in the sense that it 
has not been assigned any specif i c type of computational interpretation. If 
so, the original Triviality Argument should also be understood in a modal 
way as follows. I will call this modif i ed version of it the Modal Triviality 
Argument. 
1. The computational theory of cognition states that our cognitive system is 
a computing system. 
2. Everything could be a computing system that performs any 
computation. 
3. If (1) and (2), then the computational theory of cognition is not 
informative. 
4. Therefore, the computational theory of cognition is not informative. 
I do not think that this is the result that Searle wanted to argue for. 
Above all, CTC is not refuted by this argument. As we can see in premise 
(2), the Modal Triviality Argument adopts a modal understanding of 
pancomputationalism. According to this understanding, everything is not a 
computer performing any program, but everything could be a computing 
system that performs any computation. However, this modal understanding 
of pancomputationalism is also false, since not all physical objects are 
unrealized computers. This can be argued as follows: 
First, some objects (or collections) might be too simple to be
1 / 1
ChatGPT
ChatGPT
In this part of his work, SeongSoo Park brings up the distinction between unrealized and realized computers to counter Searle's understanding of pancomputationalism. An unrealized computer is a physical object (or a collection of physical objects) that merely has the potential to be realized as a computer if a suitable computational interpretation is provided. On the other hand, a realized computer is a concrete computer that has already been assigned a specific computational interpretation.
Park argues that Searle's conception of pancomputationalism, which seems to consider everything as realized computers, is incorrect. He posits that pancomputationalism should be understood in terms of unrealized computers. That is, pancomputationalism merely asserts a modal claim: everything is an unrealized computer that could potentially be realized in the future through a specific computational interpretation.
Based on this distinction, Park modifies the original Triviality Argument into the "Modal Triviality Argument" as follows:
The computational theory of cognition states that our cognitive system is a computing system.
Everything could be a computing system that performs any computation.
If (1) and (2), then the computational theory of cognition is not informative.
Therefore, the computational theory of cognition is not informative.
In this argument, pancomputationalism is understood in a modal sense, suggesting that everything could potentially be a computing system, not that everything already is. Park argues that this modal understanding of pancomputationalism is also incorrect since not all physical objects are unrealized computers - some may be too simple to realize as computers.
1 / 1
operates over language-like in the mind. If so, the computational theory of 
cognition should be understood in terms of a realized computer.9 However, 
premise (2) of the Restricted Modal Triviality Argument does not relate 
to the notion of a realized computer. Hence, the argument does not refute 
CTC. 
Moreover, premise (2) is also false. For example, let us suppose that some 
unrealized computers are only complex enough to perform an addition 
program. If so, they cannot be a computer that implements a complex 
program. That is, they are complex enough to be computers, but they are 
still too simple to implement certain kinds of programs. This implies that 
only some physical objects within type X could qualify as computers 
that can perform cognitive programs. The point is that CTC is not refuted 
by the fact that some objects within type X could potentially be realized 
as computers that run cognitive programs. Indeed, this is the claim that 
9 The claim that our cognitive system is a realized computer can be made in 
different ways. Piccinini is one of philosophers who discusses this. See Piccinini 
2012. 
367The 
Problem of Pancomputationalism: Focusing on Three Related Arguments 
proponents of CTC want to highlight. According to computational theorists 
of cognition, our cognitive system, in its nature, is not that different from 
sufficiently complex computers. This is why they attempt to explain our 
cognitive system in terms of computation. Therefore, as long as it does not 
trivialize CTC, computational theorists of cognition can embrace the fact 
that some physical objects within type X could potentially be realized as 
computers that can implement cognitive programs. 
7. Conclusion 
This completes my defense of the computational theory of cognition. 
I conclude that the Triviality Argument is unsound because the strongest 
version of pancomputationalism is false. In showing this, I did not directly 
engage with the Chinese Room Argument. I have nothing to say on the 
syntactic conception of computation. However, I have many words to say 
about the Pan Argument. The observer-relativity of computation does not 
guarantee that everything is a realized computer. Any object cannot become 
a computer before a specific computational interpretation is actually 
given to it. Hence, the strongest version of pancomputationalism is false. 
Furthermore, a modal understanding of it is also unconvincing, since not all 
physical objects are unrealized computers. Thus, the most legitimate move 
of pancomputationalism is merely that some physical objects belonging to 
the category of unrealized computer could be a computer that performs any 
program. This peculiar version does not threaten the computational theory 
of cognition. Rather, this is the core claim that computational cognitivists, if 
they adopt the syntactic conception of computation, should embrace.
1 / 1
ChatGPT
ChatGPT
In this concluding part of his work, SeongSoo Park reiterates his defense of the computational theory of cognition (CTC) against the Triviality Argument. He argues that CTC should be understood in terms of a realized computer, not an unrealized one. Moreover, he says that the Triviality Argument's premise, which suggests that any physical object could be a computer that performs any computation, does not apply to a realized computer, hence it cannot refute CTC.
Park further argues that not all unrealized computers have the capacity to perform complex programs due to their simplicity. Therefore, the notion that any physical object can execute any computation is incorrect. Only some physical objects, due to their complexity, can potentially be realized as computers running cognitive programs. He emphasizes that this potential realization doesn't trivialize CTC but actually underscores what computational theorists of cognition aim to highlight - that our cognitive system, in its essence, is not vastly different from sufficiently complex computers.
Finally, he concludes that the strongest version of pancomputationalism is false because it posits that everything is a realized computer. He asserts that any object cannot become a computer until a specific computational interpretation is given to it. Furthermore, the modal understanding of pancomputationalism, which suggests that everything could potentially be a computing system, is also not convincing because not all physical objects are unrealized computers. According to him, the most legitimate form of pancomputationalism is that some physical objects belonging to the category of unrealized computers could potentially become computers performing any program. This claim, he suggests, does not threaten the computational theory of cognition, but should actually be embraced by computational cognitivists.
1 / 1
Pancomputationalism: Theory or metaphor?
Vincent C. M��ller
In Ruth Hagengruber & Uwe Riss (eds.), Philosophy, computing and information science. Pickering & Chattoo. pp. 213-221 (2014)   Copy   BIBTEX
Abstract
The theory that all processes in the universe are computational is attractive in its promise to provide an understandable theory of everything. I want to suggest here that this pancomputationalism is not sufficiently clear on which problem it is trying to solve, and how. I propose two interpretations of pancomputationalism as a theory: I) the world is a computer and II) the world can be described as a computer. The first implies a thesis of supervenience of the physical over computation and is thus reduced ad absurdum. The second is underdetermined by the world, and thus equally unsuccessful as theory. Finally, I suggest that pancomputationalism as metaphor can be useful. ��� At the Paderborn workshop in 2008, this paper was presented as a commentary to the relevant paper by Gordana Dodig-Crnkovic " Info-Computationalism and Philosophical Aspects of Research in Information Sciences".
1 / 1
ChatGPT
ChatGPT
In this abstract, Vincent C. M��ller discusses pancomputationalism, the theory that all processes in the universe are computational. While pancomputationalism has a certain allure, promising an understandable theory of everything, M��ller expresses skepticism about the clarity of its aim and its methodology.
M��ller offers two possible interpretations of pancomputationalism as a theory:
I) The world is a computer: This interpretation suggests that everything in the world is a computational process. This could imply that physical processes are essentially computational, leading to a form of supervenience of the physical over computation. M��ller suggests this interpretation is untenable, and reduces itself to absurdity, as it does not align with our understanding of the physical world.
II) The world can be described as a computer: In this interpretation, the world, while not necessarily being a computer, can be described or modeled in terms of computation. This interpretation, however, is argued to be underdetermined by the world, meaning there are not enough constraints in our observations of the world to fully specify this computational model. Thus, this interpretation is also unsuccessful as a theory.
Despite his criticisms of pancomputationalism as a theory, M��ller suggests that pancomputationalism can still be useful as a metaphor. As a metaphor, it could help in conceptualizing complex physical processes and phenomena. This paper was presented as a commentary to a paper by Gordana Dodig-Crnkovic on "Info-Computationalism and Philosophical Aspects of Research in Information Sciences" at a workshop in Paderborn in 2008.
1 / 1
Representation, Analytic Pragmatism and AI
Raffaela Giovagnoli 
Chapter
981 Accesses

3 Citations

Part of the Studies in Applied Philosophy, Epistemology and Rational Ethics book series (SAPERE,volume 7)

Abstract
Our contribution aims at individuating a valid philosophical strategy for a fruitful confrontation between human and artificial representation. The ground for this theoretical option resides in the necessity to find a solution that overcomes, on the one side, strong AI (i.e. Haugeland) and, on the other side, the view that rules out AI as explanation of human capacities (i.e. Dreyfus). We try to argue for Analytic Pragmatism (AP) as a valid strategy to present arguments for a form of weak AI and to explain a notion of representation common to human and artificial agents
1 / 1
ChatGPT
ChatGPT
In this abstract, Raffaela Giovagnoli discusses the relationship between human and artificial representation. Giovagnoli's goal is to find a valid philosophical strategy that allows for productive comparisons and interactions between human and artificial intelligence. This requires a theoretical approach that avoids two extremes:
Strong AI, represented by the work of philosopher John Haugeland, which proposes that artificial intelligence (AI) can replicate or surpass human intelligence.
The view articulated by philosopher Hubert Dreyfus that categorically rejects AI as an explanation for human capacities.
Giovagnoli proposes Analytic Pragmatism (AP) as a potential solution. AP combines the clarity and precision of analytic philosophy with the practical and action-oriented focus of pragmatism. She argues that AP is a valid strategy for advocating for a form of weak AI, which holds that AI can simulate certain aspects of human intelligence but does not replicate the totality of human cognition.
Using AP, Giovagnoli aims to explain a notion of representation that can be applicable to both human and artificial agents. Representation here refers to the ability of both humans and AI systems to create mental or digital models of the world that guide their actions and decision-making. By adopting the AP strategy, Giovagnoli hopes to find a middle ground that recognizes the potential of AI without overstating its capabilities or underestimating the complexity of human cognition.
1 / 1
Connections between the articles
1 / 2
ChatGPT
ChatGPT
The two articles "The Problem of Pancomputationalism: Focusing on Three Related Arguments" by Seongsoo Park and "Representation, Analytic Pragmatism and AI" by Raffaela Giovagnoli, both delve into philosophical aspects of artificial intelligence (AI), specifically in relation to human cognition and the nature of computation.
Pancomputationalism vs. Info-Computationalism
: Pancomputationalism, as discussed by Park, suggests that everything in the universe is a computational process. It challenges the computational theory of cognition (CTC), which posits that our cognitive system works like a computational system, by arguing that if everything is computational, then CTC becomes trivial. Vincent C. M��ller's "Pancomputationalism: Theory or metaphor?" discussed by Park, provides further context, questioning the validity of pancomputationalism as a comprehensive theory. On the other hand, Dodig-Crnkovic's info-computationalism, which is mentioned in M��ller's paper, posits that computation combined with information forms the fabric of reality, a position that seeks to reconcile universal computation with the uniqueness of cognitive processes.
Understanding of AI and Human cognition
: Giovagnoli's approach in "Representation, Analytic Pragmatism and AI" also deals with the understanding of AI and human cognition. She proposes analytic pragmatism as a way to avoid extremes (i.e., strong AI and AI denial), aiming for a weak AI position that recognizes AI as simulating aspects of human intelligence but not fully replicating it.
Representation
: Both Park and Giovagnoli delve into the concept of representation in cognition. Park's arguments revolve around the idea that computational interpretation, essentially a form of representation, is necessary for something to be considered a computing system. Similarly, Giovagnoli's focus is on finding a common ground between human and artificial representation.
In summary, both articles grapple with the philosophical implications of viewing cognition and the universe through a computational lens. They offer different perspectives on how AI and human cognition relate, how to conceptualize the universe's computational nature, and how representations play a crucial role in these discussions.
1 / 1
Info���Computationalism and Philosophical Aspects of Research   
in Information Sciences  
Gordana Dodig���Crnkovic  
M��lardalen University, Sweden,   
http://www.idt.mdh.se/personal/gdc   
19th November, 2008 

The historical development has lead to the decay of Natural Philoso��� 
phy which until 19th century included all of our knowledge about the  
physical world into the growing multitude of specialized sciences,  
within ���The Classical Model of Science���. The focus on the in���depth  
enquiry disentangled from its broad context lead to the problem of  
loss of common world���view and impossibility of communication be��� 
tween specialist research fields because of different languages they  
developed in isolation. The need for a new unifying framework is  
becoming  increasingly  apparent  with  the  information  technology  
enabling and intensifying the communication between different re��� 
search fields, knowledge communities and information sources. This  
time, not only natural sciences, but also all of human knowledge is  
being integrated in a global network such as Internet with its diverse  
knowledge and language communities.   
Info���computationalism (ICON) as a synthesis of pancomputational��� 
ism  and  paninformationalism  presents  a  unifying  framework  for  
understanding  of  natural  phenomena  including  living  beings  and  
their cognition, their ways of processing information and producing  
knowledge. Within ICON physical universe is understood as a net��� 
work of computational processes on an informational structure. The  2/16 
matter/energy in this model is replaced by information/computation  
where information is the structure, whose dynamics are identified as  
natural computation.   
ICON is an example of philosophical framework in a direct connec��� 
tion with the related scientific fields, and the process is mutual ex��� 
change: scientific findings influence philosophical thinking and vice  
versa. Research is going on in Natural computing on modeling natu��� 
ral  phenomena  including  living  organisms  as  info���computational  
agents and implementing natural computation principles into tech��� 
nological artifacts. Lessons learned from the design and implementa��� 
tion  of  our  understanding  of  living  natural  computational  agents  
leads to artifacts increasingly capable of simulating essential abilities  
of  living  organisms  to  process  and  structure  information.  Among  
others, ICON supports scientific understanding of the mind (percep��� 
tion,  thinking,  reasoning,  will,  feelings,  memory,  etc.)  providing  
computational naturalist models of cognition.
1 / 1
ChatGPT
ChatGPT
In the article, "Info���Computationalism and Philosophical Aspects of Research in Information Sciences," Gordana Dodig���Crnkovic explores the concept of info-computationalism, or ICON, as a unifying philosophical framework that interprets all natural phenomena, including cognition and life processes, as computational processes on an informational structure.
This theory presents an interesting merger of pancomputationalism (everything is computation) and paninformationalism (everything is information), suggesting that the physical universe is essentially a network of computational processes on an informational structure.
According to Dodig���Crnkovic, the matter/energy paradigm in traditional physics is replaced by an information/computation paradigm in ICON, where information represents structure and natural computation represents dynamics. This perspective is not confined to natural sciences but extends to all forms of human knowledge, embracing the global interconnectedness facilitated by information technology.
The implications for artificial intelligence (AI) and cognitive science are profound. It suggests that living organisms can be modeled as info-computational agents, with their biological processes interpreted as computation on information. Moreover, it indicates that technology can, in principle, simulate the essential abilities of living organisms to process and structure information.
ICON also provides a framework for understanding cognitive functions such as perception, thinking, reasoning, will, feelings, and memory, offering a computational naturalist model of cognition.
Importantly, Dodig���Crnkovic highlights the bidirectional relationship between philosophical thinking and scientific findings. Philosophy guides how we interpret and understand scientific results, while scientific discoveries, in turn, inspire and inform philosophical speculation. This dynamic exchange enriches both domains, pushing the boundaries of our knowledge and understanding.
1 / 1
1. Science and Philosophy  
���Our best efforts are directed at finding out why things are as they are or why the  
events around us occur as they do. This is so in all disciplines and philosophy is  
no exception in this regard; what sets philosophy apart is that it probes deeper as  
well as being more general. It queries the presuppositions other disciplines leave  
untouched, and in trying to clarify such presuppositions and trace their inter��� 
connections it seeks to find out how the world is put together and how it works.���  
(Piv��evi��, 2007)  
Knowledge, both propositional and non���propositional is the basic constituent of  
science produced by research process. Nevertheless, often knowledge is consi��� 
dered as identical with propositional knowledge and science is identified with a  
search for truth about the world; truth considered to be in a propositional form.  
Though History, Sociology and Philosophy of Science all offer good reasons for  
seeing  science  as  a  goal���driven  human  activity  aimed  at  production  of  models  
3/16 
which enable us to predict, correlate and structure (compress, according to Chai��� 
tin) relevant information about the world.   
The role of science as information compression mechanism (in a sense of Chaitin���s  
algorithmic information, Chaitin 2007) is especially visible in its search for simple  
and universal laws, which especially characterize physical sciences. Historically  
however the idea of natural law has significantly evolved. The ancient idea of  
deterministic order of the universe was closely related to the principle of causality  
that  denotes  a  necessary  relationship  between  one  event  (cause)  and  another  
event  (effect)  which  is  the  direct  consequence  (result)  of  the  first.  However,  
indeterminism  of  quantum  physics  induced  new  elements  into  the  picture  of  
natural laws. Later on disorder was found even in rule���governed systems, show��� 
ing that deterministic functions can generate unpredictable results.   
What is crucial to the scientific knowledge is not its certainty, otherwise not much  
would  qualify  as  knowledge  in  the  history  of  science.  Even  understanding  of  
fundamental scientific ideas such as time, space, mass, and trajectory have been  
successively  historically  revised.  What  has  constantly  been  characterizing  
sciences is the rationality of their approaches, presuppositions and aims. Science is  
primarily an explanatory and predictive tool of making sense (Piv��evi��).  
Nowadays, complexity of real���world problems becomes a focus of sciences, in the  
first place thanks to computational capabilities of ICT. Instead of static, symme��� 
tric  and  steady���state,  eternal  static  order,  a  new  dynamic  picture  emerges  in  
which everything changes, and order is created as a pattern over layers of pulsat��� 
ing underlying physical processes, and very simple basic rules can lead to evolution of complex systems.
1 / 1
ChatGPT
ChatGPT
This excerpt from "Info���Computationalism and Philosophical Aspects of Research in Information Sciences" by Gordana Dodig���Crnkovic explores the interconnected roles of philosophy and science.
According to the author, while all disciplines seek to understand why things are as they are, philosophy distinguishes itself by probing deeper and questioning the presuppositions that other disciplines leave unexamined. In doing so, it seeks to understand how the world is put together and operates.
Science, for its part, is driven by the pursuit of knowledge, both propositional (expressed in declarative sentences or indicative propositions) and non-propositional. Often, knowledge is equated with propositional knowledge, and science is seen as the pursuit of propositional truth about the world. However, Dodig���Crnkovic emphasizes that science can also be seen as a goal-driven human activity aimed at creating models that allow us to predict, correlate, and structure information about the world.
She then discusses the evolution of the concept of natural law from deterministic order based on the principle of causality to the recognition of indeterminism in quantum physics and the unpredictability even within deterministic systems. She emphasizes that what is crucial to scientific knowledge is not its certainty (as concepts like time, space, mass, and trajectory have been successively revised), but the rationality of its approaches, presuppositions, and aims.
With the advent of powerful computational technologies, the complexity of real-world problems has come into focus, challenging the traditional view of a static, symmetrical, steady-state order. Instead, a dynamic picture emerges, where everything changes and order is seen as a pattern over pulsating underlying physical processes. Simple basic rules can lead to the evolution of complex systems, a perspective that complements info-computationalism and the pancomputationalist view of the world.
1 / 1
3. Info���Computationalism as a Framework for Unity of Knowledge  
Info���computationalism is a view according to which the physical universe on a  
fundamental level can be understood as an informational structure which dynam��� 
ics is computational process. The matter/energy in this model is replaced by in��� 
formation/computation; matter (structure) corresponds to information while the  
dynamics ��� constant changes in the informational structure ��� are computational  
processes. In this view the universe is a huge computer network which by physi��� 
cal laws ���computes��� its own next state (Chaitin, pancomputationalism). Informa��� 
tion is the fabric of the universe. An instantaneous ���snap��� of the universe reveals  
the structure. Changes are computational processes. Computation is simply in��� 
formation processing.   
ICON  unites  pan���computationalism  (natural  computationalism)  with  pan��� 
informationalism. In short, the Info���Computationalism is a dual���aspect approach  
based on two fundamental concepts: information and computation. On this view,  
as many computationalists already have declared (Zuse, Fredkin, Wolfram, Chai��� 
tin, Lloyd, ..) the universe is a huge computing system (or a network of compu��� 
ting processes) which by means of physical laws compute its own next state. It  
must be pointed out that the computing universe is not identical with (or reducible to)  
today���s computers. Rather, computing is what the whole of the universe does while  
processing its own information by simply following natural laws. (���Natural com��� 
puting��� in its different forms. [How much of ���natural laws��� are orderly and how  
much is random is a separate discussion, see Chaitin���s work].  
One might suspect that the computationalist idea is vacuous, and if everything is  
info���computational, then it says nothing about the world. The computationalist  
claim however should be understood as similar to the claim that universe is made  
of atoms. Atom is a very useful concept which helps understanding the world in  
many fields. So is the info���computational approach. Universe is NOT ���nothing  
7/16 
but atoms���, but on some view (level of organization, level of abstraction) may be  
seen as atoms.  
Unlike many other (pan)computationalists I do not presuppose that computatio��� 
nalism necessarily implies digital computing. As Seth Lloyd points out, on the  
basic quantum���mechanical level discrete and analogue, digital and continuous  
computing is going on. See more about the question of digital/analog universe in  
(Dodig���Crnkovic, 2006)  
As already emphasized, physical reality can be addressed at many different levels  
of organization. Life and intelligence are the phenomena especially characterized  
by info���computational structures and processes. Living systems have the ability  
to act autonomously and store information, retrieve information (remember), an��� 
ticipate  future  behavior  in  the  environment  with  help  of  information  stored
1 / 1
ChatGPT
ChatGPT
This segment from the same text by Gordana Dodig���Crnkovic introduces Info-Computationalism, a perspective that views the universe at a fundamental level as an informational structure whose dynamics are computational processes. In other words, the universe is seen as a vast computer network, with physical laws dictating how it "computes" its future state.
According to the Info-Computationalism view, information serves as the fabric of the universe, with an instantaneous snapshot of the universe revealing its structure. Changes within this structure are seen as computational processes or information processing.
The author merges two ideas into this framework: pan-computationalism (the idea that everything is computational in nature) and pan-informationalism (the idea that everything is informational). She likens this perspective to viewing the universe as composed of atoms. Just as atoms help us understand the world across many fields, so too does the Info-Computationalism approach.
The author also addresses the debate between digital and analog computing. Unlike many pancomputationalists, she does not necessarily view computation as exclusively digital. She echoes Seth Lloyd's sentiment that both digital and continuous computing occur on the quantum-mechanical level.
Finally, she suggests that this Info-Computationalist perspective can be used to examine physical reality at different levels of organization. In particular, she points to life and intelligence as phenomena characterized by Info-Computational structures and processes, noting how living systems can act autonomously, store information, retrieve it, and anticipate future behavior based on stored information.
1 / 1
5. Knowledge as a Complex Informational Architecture. The Necessity of a  
Multidisciplinary Dialogue  
Why is it important to develop a multi���disciplinary discourse which will present  
the departure from the monolithic ���Classical Model of Science��� caused by diver��� 
sity of the domains, methods and levels of organization/levels of abstraction? The  
main reason  is  epistemological  ���  multidisciplinarity  provides  the  fundamental  
framework  suitable  for  common  understanding  and  communication  between  
presently disparate fields. This argument builds on a view of knowledge as in��� 
formational construction. According to Stonier (1997), data is a series of discon��� 
nected facts and observations, which is converted into information by analyzing,  
cross���referring, selecting, sorting and summarizing the data. Patterns of informa��� 
tion, in turn, can be worked up into knowledge which consists of an organized  
body  of  information.  Stonier���s  constructivist  view  emphasizes  two  important  
facts:   
��� going from data to information to knowledge involves, at each step, an input  
of work, and   
��� at each step, this input of work leads to an increase in organization, thereby  
producing a hierarchy of organization.   10/16 
Research into complex phenomena (Mainzer 2004) has led to an insight that re��� 
search problems have many different facets which may be approached differently  
at different levels of abstraction and that every knowledge field has a specific  
domain of validity.   
This new understanding of a multidimensional many���layered knowledge space  
of phenomena have among others resulted in an vision of an ecumenical conclu��� 
sion of science wars by recognition of the necessity of an inclusive and complex  
knowledge architecture which recognizes importance of a variety of approaches  
and  types  of  knowledge.  [See  for  example  Smith  and  Jenks,  2006.]  Based  on  
sources in philosophy, sociology, complexity theory, systems theory, cognitive  
science, evolutionary biology and fuzzy logic, Smith and Jenks present a new in��� 
terdisciplinary perspective on the self���organizing complex structures. They analyze  
the  relationship  between  the  process  of  self���organization  and  its  environ��� 
ment/ecology. Two central factors are the role of information in the building of  
complex structures and the development of topologies of possible outcome spac��� 
es. The authors argue for a continuous development from emergent complex or��� 
ders  in  physical  systems  to  cognitive  capacities  of  living  organisms,  complex  
structures of human thought and to cultures. This is a new understanding of uni��� 
ty  of  interdisciplinary  knowledge,  unity  in  structured  diversity,  also  found  in  
(Mainzer 2004).   
In a complex informational architecture of knowledge, logic, mathematics, quan��� 
tum mechanics, thermodynamics, chaos theory, cosmology, complexity, the ori��� 
gin  of  life,  evolution,  cognition,  adaptive  systems,  intelligence,  consciousness,  
societies of minds  and their production of knowledge and other artifacts ��� there  
are two basic phenomena in common: information and computation which provide  
a framework for those jigsaw puzzle pieces of knowledge to be put together into  
a complex and dynamic unified info���computational view.  
The body of knowledge and practices in computing and information sciences, as  
a new research field, has grown around an artifact ��� a computer.
1 / 1
ChatGPT
ChatGPT
This section discusses the importance of developing a multi-disciplinary approach to depart from the monolithic "Classical Model of Science", due to the diversity of domains, methods, and levels of abstraction or organization in knowledge. The author argues that this approach is critical for mutual understanding and communication across different fields.
The segment introduces knowledge as an informational construction. It explains that data is a series of disconnected facts and observations, which is turned into information through analysis, cross-referencing, selecting, sorting, and summarizing. Patterns of information can then be processed into knowledge, which consists of an organized body of information.
The work involved in transitioning from data to information and finally to knowledge leads to an increase in organization at each step, forming a hierarchy of organization. Research into complex phenomena has shown that problems have many different facets which can be approached differently at various levels of abstraction, with each field of knowledge having its specific domain of validity.
This new understanding of a multi-dimensional, many-layered knowledge space has led to the recognition of the need for an inclusive and complex knowledge architecture that recognizes the importance of a variety of approaches and types of knowledge. The author refers to a work by Smith and Jenks which presents a new interdisciplinary perspective on self-organizing complex structures, analyzing the relationship between the process of self-organization and its environment/ecology.
In a complex informational architecture of knowledge, various areas such as logic, mathematics, quantum mechanics, thermodynamics, chaos theory, cosmology, complexity, the origin of life, evolution, cognition, adaptive systems, intelligence, consciousness, societies of minds, etc., share two common phenomena: information and computation. These provide a framework for assembling the disparate pieces of knowledge into a complex and dynamic unified info-computational view.
Lastly, the author points out that the field of computing and information sciences has grown around an artifact ��� the computer. It suggests that this artifact has significantly influenced our understanding of knowledge and practices in these fields.
1 / 1
sary and in the long run, even in the education system, research ethics, compu��� 
ting ethics, information ethic and other specific branches of applied ethics accom��� 
panying specialized sciences and technologies should become an integral part of  
education and research system.  
7. Answering some Usual Kinds of Criticism of Info���Computationalism  
Info-Computationalist view may be interpreted as a claim that the whole of the 
world is ���nothing but a (computational) machine��� and that we humans are essen- 
tially machines with no free will or feelings. That is obviously not the case. The 
view that the universe is an info-computational phenomenon means that the un- 
iverse as it is may be understood and modeled in info-computational language. 
Feelings, qualia and other mental phenomena are emergent properties of the 
physical world which is info-computational.   
The role of different paradigms in our understanding of the universe can be ana- 
lyzed on historical examples. In the past, several major paradigm shifts occurred: 
from mytho-poetic to mechanistic to the emerging computationalist understand- 
ing of the universe. Consequently, we can ask the same question about mytho- 
poetic and mechanist universe: Was that understanding of the universe true? Was 
it real or merely metaphoric? For the mytho-poetic universe, the answer is simple 
��� it was a metaphor. Even though mechanicism was primarily the outlook at the 
inanimate matter, and mechanistic approaches to robotics (mechanical quasi- 
humans) did not work for any other purpose but the entertainment, mechanistic 
worldview nevertheless helped us to learn a lot about the universe, in the first 
place inanimate but even many basic facts about living world (such as that there is 
no ��lan vital but the same physical mechanistic laws govern the whole of the 
physical universe, living as well as non-living).   
The parallel development goes on in the course of computationalism now again. 
We will learn about informational and computational resources and capabilities, 
14/16 
of the universe and we will develop even more powerful ways of learning, via 
intelligent systems we will successively improve.   
Knowing that biological organisms (including humans) are information- 
processing ���machines��� does not make them less fascinating. In the same way as 
knowing that all of us are made of atoms does not mean that we do not have free 
will, imagination and real feelings. Understanding fundamental level processes 
does not make music, arts and philosophy obsolete.   
Info-computationalism helps us both by supplying the tools for knowledge and 
artifact production and even tools for understanding of the natural phenomena 
and artifacts on many different levels. That is also why philosophy is coming 
back to sciences based on info-computational knowledge. A holistic, high level of 
abstraction view is necessary as a self-reflective process of knowledge.   
In sum: Info-Computationalism is by no means the final answer to ���Life!��� ���The Un- 
iverse!��� and ���Everything!��� (which is 42, as we learned from Adams as quoted by 
Vincent M��ller in this volume), but a learning tool which will help us again reach 
a unity of knowledge on one specific level of abstraction: info-computational. 
So the answer (Info-Computationalism) is not The final answer, but all the same 
seems to be a reasonable answer to a reasonable question: how to get common 
language for disparate specialist fields which can enable mutual understanding 
and building of new knowledge?
1 / 1
ChatGPT
ChatGPT
This section argues for the necessity of including research ethics, computing ethics, and other specific branches of applied ethics in both education and research systems as they relate to specialized sciences and technologies.
The section then addresses criticisms of Info-Computationalism, stating that it does not claim that everything, including humans, is "nothing but a (computational) machine", devoid of feelings or free will. Rather, Info-Computationalism offers a framework for understanding and modeling the universe, with mental phenomena such as feelings being emergent properties of the info-computational physical world.
Historical paradigm shifts are mentioned, from mytho-poetic to mechanistic to the emerging computationalist understanding of the universe. The mechanistic worldview, while primarily applied to inanimate matter, provided valuable insights into the universe, including the realization that the same physical laws govern both living and non-living matter.
The author suggests that the rise of computationalism will enhance our understanding of informational and computational resources and capabilities. The knowledge that biological organisms are information-processing "machines" does not diminish their complexity or fascination; similar to how understanding that we are made of atoms does not negate our free will, imagination, or feelings. Understanding fundamental processes does not render music, arts, or philosophy obsolete.
Info-Computationalism is seen as a tool that aids in the creation of knowledge and understanding of natural phenomena at various levels. It is viewed as an important factor in the reintegration of philosophy into the sciences, offering a high level of abstraction necessary for a self-reflective process of knowledge.
The author concludes that Info-Computationalism is not the final answer to the universe's deepest questions. However, it's a useful learning tool to understand knowledge at a particular level of abstraction. The approach offers a common language that facilitates mutual understanding and the creation of new knowledge across diverse specialist fields.
1 / 1
Connections with the earlier discussion.
1 / 1
ChatGPT
ChatGPT
The earlier discussions revolve around the idea of Info-Computationalism and the need for a multidisciplinary approach to understanding and modeling the universe. The core idea is that knowledge is constructed from data through an organizational process, leading to a hierarchy of organization. This philosophy hinges on the understanding that our universe can be seen as a complex, many-layered knowledge space, which needs a multidisciplinary dialogue to fully comprehend.
The arguments here connect back to the initial discussion about the importance of multidisciplinary dialogue. Different fields of study offer unique perspectives, methodologies, and levels of organization or abstraction. Together, these distinct yet interconnected disciplines can enhance our overall understanding of the universe and its phenomena.
The reference to Stonier���s constructivist view emphasizes the idea of knowledge as a structured organization of information, produced from raw data through an input of work at each stage. This aligns with the earlier discussion, providing a structure for understanding the transition from data to information to knowledge.
The reference to Smith and Jenks' work illustrates an application of this idea. They present an interdisciplinary perspective on self-organizing complex structures, highlighting the role of information in the construction of these structures and the development of possible outcomes.
In the later part of the discussion, addressing criticisms of Info-Computationalism, the philosophy is defended as a tool to provide a unified view of diverse knowledge fields rather than a definitive answer to all questions. This connects with the initial discussions by reinforcing the importance of Info-Computationalism as a philosophical approach to knowledge and understanding, rather than an all-encompassing answer to the mysteries of the universe.
1 / 1
Connections with the following: The Computational Theory of Cognition (CTC): We discussed the theory which posits that human cognitive processes are computational in nature. It proposes that our minds function similarly to a computer, processing information and making decisions based on certain rules and algorithms.

Pancomputationalism: We discussed this philosophical position which holds that every part of the universe is a computational system or can be modeled as such.

The Triviality Argument: We explored this argument which asserts that if everything is a computing system that performs any computation (as per pancomputationalism), then the Computational Theory of Cognition becomes uninformative and trivial.

Searle's Objections to Pancomputationalism: We discussed philosopher John Searle's criticism of pancomputationalism and his arguments against it, including his critique that pancomputationalism does not clarify how computation is assigned or understood within a system.

Representation in AI and Humans: We examined the topic of representation within both human cognition and artificial intelligence. We discussed how the theory of Analytic Pragmatism could serve as a framework for understanding and comparing representation in both humans and AI systems.

Strong AI vs. Weak AI: We touched upon the debate between strong AI (the belief that AI can fully replicate human intelligence) and weak AI (the belief that AI can only simulate certain aspects of human cognition).
1 / 1
ChatGPT
ChatGPT
Info-Computationalism, as discussed earlier, has strong connections to all these topics:
The Computational Theory of Cognition (CTC): Info-Computationalism and CTC both view cognitive processes as computational in nature. Both theories propose that cognition, whether human or artificial, involves the processing of information, similar to how a computer operates.
Pancomputationalism: Info-Computationalism can be seen as a variant of pancomputationalism in that it proposes the universe as an informational-computational system. However, Info-Computationalism emphasizes the transformation of data into information and knowledge, rather than simply positing everything as a computational system.
The Triviality Argument: This argument posits that if everything is a computational system (as per pancomputationalism), theories like CTC or Info-Computationalism become trivial. However, the emphasis on a hierarchy of organization from data to information to knowledge in Info-Computationalism counters this triviality, providing a framework for understanding the complexity of such systems.
Searle's Objections to Pancomputationalism: Info-Computationalism may offer a response to Searle's objections by providing a more structured and less broad view of the universe as an informational-computational system. It distinguishes between data, information, and knowledge, and emphasizes the necessity of work to transform one into the other.
Representation in AI and Humans: In Info-Computationalism, the representation of knowledge can be understood as the organization of information within a system. It could provide a philosophical grounding for understanding representation in both humans and AI.
Strong AI vs. Weak AI: Info-Computationalism doesn't necessarily take a side in this debate. However, it provides a theoretical framework in which the progression from Weak AI (which could be likened to handling 'data') to Strong AI (which would involve the manipulation and understanding of 'knowledge') could be explored and understood.
1 / 1
It seems that pancomputationalism is often called Digital physics, or even Mathematical universe hypothesis.
1 / 1
ChatGPT
ChatGPT
Yes, pancomputationalism does share a conceptual affinity with concepts such as digital physics and the mathematical universe hypothesis. These ideas posit a fundamentally computational or mathematical substrate to reality.
Digital Physics
: This is the notion that the entire history of our universe is computable and can be thought of as the evolution of a cellular automaton or similar digital computing device. It proposes that reality is essentially informational, and physics, at its core, can be described entirely in terms of information theory.
Mathematical Universe Hypothesis (MUH)
: This is a more radical proposition put forth by physicist Max Tegmark. It suggests that the universe is not merely described by mathematics, but that it
is
a mathematical structure in its entirety. Every object, including you and me, is an abstract mathematical object.
While they all have similarities, it's crucial to distinguish these concepts. Pancomputationalism generally asserts that everything can be modeled as a computation. In contrast, digital physics goes a step further to suggest that the universe is fundamentally digital and informational. The MUH takes an even more extreme position, positing the universe itself as a purely mathematical structure. Each of these perspectives offers a unique lens through which to understand reality, and each comes with its own set of philosophical implications and debates.
1 / 1
Ok any other debate theories?
1 / 1
ChatGPT
ChatGPT
Yes, there are several other theories and debates within the philosophy of mind and cognitive science that relate to the computational understanding of the world and consciousness. Here are a few:
Functionalism
: This is the theory that mental states are defined by their functional roles rather than their intrinsic properties. It argues that what makes something a mental state is not its internal constitution but its role in the overall psychological system.
Connectionism
: This theory emphasizes the idea that human cognition and learning are best understood in terms of network structures (like neural networks) and their interrelated properties. It is often contrasted with computational approaches that rely on symbolic reasoning.
Embodied Cognition
: This is the idea that cognitive processes are deeply rooted in the body's interactions with the world. It challenges the computational model of cognition that views the mind as a disembodied system.
Extended Mind Thesis
: This concept suggests that the tools and technologies we interact with can become part of our cognitive process, effectively extending our mind into the world.
Chinese Room Argument
: Proposed by John Searle, this thought experiment is designed to argue against the notion of "strong AI," or the idea that machines can truly understand and have consciousness.
Turing Test and its criticisms
: Proposed by Alan Turing, the Turing Test is a measure of a machine's ability to exhibit intelligent behavior equivalent to, or indistinguishable from, human behavior. There have been many criticisms of this test and what it actually demonstrates.
Physicalism vs Dualism
: This is a classic debate about the nature of consciousness. Physicalism asserts that all mental states are physical states, while Dualism holds that mental states are distinct and separate from physical states.
Phenomenal Consciousness and Access Consciousness
: Proposed by Ned Block, this is a distinction between two types of consciousness. Phenomenal consciousness is about experience and sensation, while access consciousness is about the use of mental content in reasoning and behavior.
These are just a few of the many debates and theories that arise when considering the nature of mind and consciousness, many of which intersect with questions about the role of computation and information in understanding the nature of cognition.
1 / 1
Sorry I didn't mean to say debated, I meant "related" like about the nature of reality being digital or computational. I suppose the free energy principle might be connected.
1 / 1
ChatGPT
ChatGPT
Certainly, there are many theories and concepts in philosophy and physics that postulate a digital or computational nature to reality. Here are a few:
Digital Physics
: As mentioned earlier, this perspective argues that the universe operates like a digital computer or a cellular automaton. It's a form of pancomputationalism that views the universe as a vast computational structure.
Holographic Principle
: This is a principle derived from string theories and quantum gravity, suggesting that all the information contained within a volume of space can be described by data on the boundary of that space, much like a 3D image encoded on a 2D surface.
Quantum Information Theory
: This field merges quantum mechanics and information theory, often viewing quantum states and transformations as information processing units.
Mathematical Universe Hypothesis (MUH)
: Proposed by physicist Max Tegmark, MUH posits that the universe is a mathematical structure and that our perceived physical reality is a derivative of underlying mathematical relationships.
The Church-Turing Thesis
: While not strictly a theory about the nature of reality, this thesis posits that a Turing machine can simulate any process of logical deduction. This thesis has been extrapolated by some to suggest a computational basis for the universe.
Simulation Hypothesis
: This speculative hypothesis proposes that reality is a simulation or a complex computer program, and that we as conscious beings are part of that program.
Free Energy Principle
: In the context of cognitive and brain sciences, the free energy principle posits that all adaptive living systems minimize their free energy ��� a measure of surprise or prediction error ��� to model their environment and adapt. While it doesn't necessarily imply that reality is computational or digital, it does frame cognition and life processes in terms of information theory and computation.
Each of these theories presents a different angle on the idea that reality could be fundamentally digital or computational, either through direct computation (like cellular automata), informational qualities (like the holographic principle), or mathematical descriptions (like the Mathematical Universe Hypothesis).
1 / 1
A free energy principle for generic quantum systems
Author links open overlay panelChris Fields a, Karl Friston b, James F. Glazebrook c d, Michael Levin e
Show more
Add to Mendeley
Share
Cite
https://doi.org/10.1016/j.pbiomolbio.2022.05.006
Get rights and content
Abstract
The Free Energy Principle (FEP) states that under suitable conditions of weak coupling, random dynamical systems with sufficient degrees of freedom will behave so as to minimize an upper bound, formalized as a variational free energy, on surprisal (a.k.a., self-information). This upper bound can be read as a Bayesian prediction error. Equivalently, its negative is a lower bound on Bayesian model evidence (a.k.a., marginal likelihood). In short, certain random dynamical systems evince a kind of self-evidencing. Here, we reformulate the FEP in the formal setting of spacetime-background free, scale-free quantum information theory. We show how generic quantum systems can be regarded as observers, which with the standard freedom of choice assumption become agents capable of assigning semantics to observational outcomes. We show how such agents minimize Bayesian prediction error in environments characterized by uncertainty, insufficient learning, and quantum contextuality. We show that in its quantum-theoretic formulation, the FEP is asymptotically equivalent to the Principle of Unitarity. Based on these results, we suggest that biological systems employ quantum coherence as a computational resource and ��� implicitly ��� as a communication resource. We summarize a number of problems for future research, particularly involving the resources required for classical communication and for detecting and responding to quantum context switches.

Introduction
Since its introduction as a theory of brain function (Friston, 2005, 2010; Friston et al., 2006; Friston and Stephan, 2007), the variational Free Energy Principle (FEP) has been extended into an explanatory framework for living systems at all scales (Friston, 2013; Friston et al., 2017a; Ramstead et al., 2018, 2019; Kuchling et al., 2020), and shown to characterize, in its most general form, all random dynamical systems that remain measurable, and hence identifiable as persistent and separable entities, over macroscopic times (Friston, 2019). To summarize, it is shown in (Friston, 2019) that any system that has a non-equilibrium steady state (NESS) solution to its density dynamics i) possesses an internal dynamics that is conditionally independent of the dynamics of its environment, and ii) will continuously ���self-evidence��� by returning its state to (the vicinity of) its NESS. The FEP is the statement that any measurable, i.e. bounded and macroscopically persistent, system will behave so as to satisfy these requirements. In particular, the classical FEP as developed in (Friston, 2019) considers the joint environment-agent system as a random dynamical system that possesses an attracting set. By placing particular constraints on the coupling among systemic states (e.g., with sparse coupling in flow operators or stochastic differential equations), one can partition the joint state space into external, internal and Markov blanket (MB; see (Clark, 2017) for an informal review) states. In turn, the blanket states are partitioned into sensory states that mediate the influence of external states on internal states and active states that mediate the influence of internal states on external states. Crucially, this particular partition imposes conditional independence
1 / 1
ChatGPT
ChatGPT
The Free Energy Principle (FEP) for quantum systems is an extension of the original FEP as applied in neuroscience, but extended to quantum systems. FEP was first developed by Karl Friston as a framework to explain the brain's function, and has since been extended to explain the behaviors of all living systems.
In simple terms, FEP suggests that our brains (and, by extension, all adaptive living systems) are constantly predicting the state of the environment and then minimizing the difference between the prediction and the actual sensory input. This is formalized as a variational free energy, which acts as an upper bound on the prediction error (surprisal or self-information). When the free energy is minimized, the system is in a state of least surprise, indicating that its model of the world is accurate.
When applied to quantum systems, the Free Energy Principle suggests that these systems also behave in a way to minimize Bayesian prediction error in environments characterized by uncertainty, insufficient learning, and quantum contextuality.
Quantum contextuality refers to the idea that the outcome of a measurement on a quantum system can depend on the specific context in which the measurement is made (such as the entire set of measurements that are performed together). So in this setting, quantum systems are considered to behave as if they were agents, assigning semantics to observational outcomes and minimizing prediction errors in environments with these specific characteristics.
Furthermore, the paper suggests that the quantum version of FEP is asymptotically equivalent to the Principle of Unitarity, a key principle in quantum mechanics that states that the evolution of a quantum system is deterministic and reversible.
Finally, the paper makes an intriguing suggestion that biological systems may be using quantum coherence as a computational and communication resource, and proposes a number of areas for future research.
In essence, the Free Energy Principle for quantum systems is suggesting a form of quantum "self-evidencing", whereby the system continuously acts to maintain a model of the world that minimizes prediction error.
1 / 1
Self-organization, the FEP tells us, is not a rare, special case, but a ubiquitous feature of physical systems with sufficient dynamical stability to be called ���things.��� All ���things,��� in particular, self-organize MBs comprising ���sensory��� states that encode incoming information and thus mediate the influence of external states on internal states, and ���active��� states that encode outgoing information and thus mediate the influence of internal states on external states. This partitioning of ���things��� into internal and MB states means that every ���thing��� can be construed as a certain kind of ���particle��� ��� a particle that is in open exchange with external states via its MB. In short, the MB of any such ���particle��� underwrites conditional independence between its internal states and the external states of its environment by localizing and thereby restricting information exchange; hence, the MB can be viewed as separating internal from external states, while mediating their exchange.

As noted in (Friston, 2019), generalizing the FEP to characterize the behavior of all ���things��� substantially weakens the traditional distinction between ���cognition��� and merely ���physical��� dynamics, and hence weakens the even deeper, pretheoretical (Scholl and Tremoulet, 2000) distinction between ���agents��� and mere ���objects��� (Fields et al., 2021a). Treating physical interaction as information exchange ��� ���observation��� of the environment followed by ���action��� upon it ��� redescribes the ���mechanical��� process of returning to the NESS ��� as a random global (a.k.a., pullback) attractor ��� in terms of inference. This can be read as ���active inference��� (Friston, 2010, 2013, 2019; Friston et al., 2017a) in which the existential integrity of the MB, and hence of the ���self��� ��� ���world��� distinction (Levin, 2020), can be maintained in the face of environmental fluctuations by changing internal states (via sensory states: c.f., perception) or changing external states (via active states: c.f., action). It refocuses the discussion, in other words, from abstract trajectories (or flows) in state space to the MB itself as a concrete locus of ���identity��� in the form of persistent measurability, and hence of ���self-evidencing��� via active inference to maintain that identity.

It is worth emphasizing that the FEP is a theory of what can be termed, in the language of cognitive or computer science (Pylyshyn, 1984), the functional architectures of physical systems. It describes the information processing functions that physical systems execute, but is agnostic about their implementation. As first established by Church (1936) and Turing (1936), information processing functions are implementation-independent in principle; provided the generic capabilities of a Turing machine (or of AND and NOT gates), choice of implementation determines resource requirements including time, space and free energy, but does not affect what can be computed. Implementation independence ��� also sometimes termed ���multiple realizability��� (Fodor, 1968, Putnam) ��� underlies the black box methodology of classical cybernetics (Ashby, 1956), in particular Moore's theorem (Moore, 1956) showing that the internal processes of a black box cannot be determined by finite observation. Indeed an MB can be viewed as implementing the input/output (I/O) interface of a black box; in more general terms, it can be viewed as implementing the application programming interface (API) of a modularized computing system (Booch et al., 2007). An MB can, therefore, be viewed as encapsulating, in the sense of (Pylyshyn, 1984) or of computer science generally, information about implementation. In the language of (Laughlin and Pines, 2000), an MB implements a ���quantum protectorate��� of the system it encloses.

The idea that all physical systems, including the environment at large, can be considered ���observers��� that also act on their surroundings to ���prepare��� them for subsequent observations has become commonplace in quantum theory, largely replacing the ���wave-function collapse��� postulate of traditional quantum mechanics (Von Neumann, 1955) with interaction-induced decoherence (i.e., dissipation of quantum coherence) as the generator of classical information (Omn��s, 1992; Zurek, 1998; Zurek, 2003; Schlosshauer, 2003; Schlosshauer, 2007).1 Indeed while quantum theory was originally developed ��� and is still widely regarded ��� as a theory specifically applicable at the atomic scale and below, since the pioneering work of Wheeler (1983), Feynman (1982), and Deutsch (1985), it has, over the past few decades, been reformulated as a scale-free information theory (Nielsen and Chuang, 2000; Hardy, 2001; Fuchs, 2003; Brassard, 2005; Chiribella et al., 2011;
1 / 1
ChatGPT
ChatGPT
The text you've shared describes the generalization of the Free Energy Principle (FEP) to all physical systems, not just cognitive or biological ones. Essentially, the FEP applies to any system that maintains its existence over time in a changing environment. This wide applicability blurs the traditional distinction between cognition and mere physical dynamics, or between agents and mere objects.
The Free Energy Principle, in this broader context, can be understood as a kind of 'active inference', where a system maintains its integrity in the face of environmental fluctuations. This can be achieved either by adjusting its internal state (which is akin to perception) or by modifying its external environment (which is similar to action).
The FEP is framed as a theory of the functional architectures of physical systems, describing the information processing functions that these systems execute. It's agnostic about the specifics of their implementation. This notion of implementation independence is an essential principle in computer science and cognitive science, reflecting the idea that a given function (like a computation) can be realized in multiple ways.
The text also suggests a view of all physical systems as observers that interact with their surroundings. This idea is common in quantum theory, replacing the traditional idea of wave-function collapse in quantum mechanics with the concept of interaction-induced decoherence as the source of classical information. In other words, systems don't passively receive information from their environment, but actively engage with it, generating new information through the process of interaction.
Lastly, it hints towards the idea that quantum theory, which was originally developed for very small scales, has been reformulated as a scale-free information theory. This suggests that the principles of quantum mechanics might apply at all scales, not just the microscopic, which would further blur the lines between traditionally separate domains of physics and information theory.
1 / 1
Ok I thought this related to our discussions of pancomputationalism, digital physics, and the mathematical universe hypothesis.
1 / 1
ChatGPT
ChatGPT
Yes, the extension of the Free Energy Principle (FEP) to all physical systems indeed connects with the themes of pancomputationalism, digital physics, and the Mathematical Universe Hypothesis. Let's look at these connections:
Pancomputationalism
: Pancomputationalism posits that the universe as a whole is a computational system. The FEP applied to all physical systems could be seen as a form of pancomputationalism, as it models all systems as engaging in a sort of computation or information processing to maintain their existence. The principle describes how these systems act like computational agents, adjusting their internal states or acting on their environments based on the information they receive.
Digital Physics
: Digital physics argues that the universe is, at its core, describable by information, and is therefore computable. The idea presented in the article���that all physical systems, including quantum systems, can be thought of as observers that participate in information exchange���is reminiscent of this. The notion of the universe as a complex information processing system aligns with the core tenets of digital physics.
Mathematical Universe Hypothesis (MUH)
: The MUH suggests that the universe is a mathematical structure. By extending the FEP to all physical systems and hinting at quantum mechanics as a scale-free information theory, the discussion moves towards a universe fundamentally describable by mathematical laws. This aligns with the MUH's proposition that reality is a mathematical structure.
Overall, these connections highlight a growing perspective in theoretical physics and philosophy of mind that sees the universe and its constituents���whether cognitive or physical���as fundamentally informational and computational in nature.
1 / 1
Connections with this: Quantum Physics
[Submitted on 1 Feb 2011 (v1), last revised 29 Nov 2011 (this version, v7)]
The Sheaf-Theoretic Structure Of Non-Locality and Contextuality
Samson Abramsky, Adam Brandenburger
We use the mathematical language of sheaf theory to give a unified treatment of non-locality and contextuality, in a setting which generalizes the familiar probability tables used in non-locality theory to arbitrary measurement covers; this includes Kochen-Specker configurations and more. We show that contextuality, and non-locality as a special case, correspond exactly to obstructions to the existence of global sections. We describe a linear algebraic approach to computing these obstructions, which allows a systematic treatment of arguments for non-locality and contextuality. We distinguish a proper hierarchy of strengths of no-go theorems, and show that three leading examples --- due to Bell, Hardy, and Greenberger, Horne and Zeilinger, respectively --- occupy successively higher levels of this hierarchy. A general correspondence is shown between the existence of local hidden-variable realizations using negative probabilities, and no-signalling; this is based on a result showing that the linear subspaces generated by the non-contextual and no-signalling models, over an arbitrary measurement cover, coincide. Maximal non-locality is generalized to maximal contextuality, and characterized in purely qualitative terms, as the non-existence of global sections in the support. A general setting is developed for Kochen-Specker type results, as generic, model-independent proofs of maximal contextuality, and a new combinatorial condition is given, which generalizes the `parity proofs' commonly found in the literature. We also show how our abstract setting can be represented in quantum mechanics. This leads to a strengthening of the usual no-signalling theorem, which shows that quantum mechanics obeys no-signalling for arbitrary families of commuting observables, not just those represented on different factors of a tensor product.
1 / 1
ChatGPT
ChatGPT
The paper you mentioned, "The Sheaf-Theoretic Structure Of Non-Locality and Contextuality," discusses how sheaf theory���a branch of mathematics���can provide a framework for understanding phenomena such as non-locality and contextuality in quantum physics. Non-locality refers to the phenomenon in quantum mechanics where changes to a particle can instantaneously affect another particle, regardless of the distance between them (e.g., entangled particles). Contextuality, on the other hand, refers to the fact that the outcome of a measurement on a quantum system can depend on the context in which it is measured.
This paper seems to offer a mathematical framework for these strange quantum phenomena that fits into our broader discussion in the following ways:
Digital Physics & Mathematical Universe Hypothesis (MUH):
This paper presents a mathematical model for understanding quantum phenomena, which could be interpreted as additional support for these viewpoints. Both Digital Physics and MUH posit that the universe can fundamentally be understood via mathematical structures or computations. The use of sheaf theory in modeling quantum mechanics is in line with this perspective.
Pancomputationalism:
Pancomputationalism argues that everything in the universe can be modeled as a computational system. The paper describes quantum phenomena like non-locality and contextuality using sheaf theory, essentially showing a 'computation' or 'information processing' happening at the quantum level. This could be seen as supporting the view of the universe as fundamentally computational.
Free Energy Principle:
The paper doesn't directly connect with the Free Energy Principle, but one could imagine a point of intersection in future work. If quantum systems can be modeled as engaging in information processing (as suggested by the paper), and if the Free Energy Principle is extended to all physical systems (including quantum systems), then there might be a connection in terms of understanding quantum systems' 'behaviors' or 'dynamics' from an information theory perspective.
However, it's essential to note that these interpretations and connections are at a conceptual and speculative level, and the specific details might involve complex and technical nuances that may challenge these connections. For instance, the nature of 'information processing' in quantum systems might be vastly different from how we traditionally understand it in classical computational systems or cognitive systems.
1 / 1
Classical formulations
The FEP applies to physical systems ��� what Friston (2019) calls ���things��� or ���particles��� ��� that are persistent, in the sense of having a well-defined state space, over some time period of interest. Adopting the notation of Friston (2019), if the internal states �� of such a system S are conditionally independent of the states �� of its environment E, it (by definition) possesses an MB, defined as the set b of states on which the dependence of �� on �� is conditioned. This MB condition will hold, in

Physical interactions are informationally symmetric
As discussed in connection with Eq. (1), and as clear from the S-E exchange symmetry of Eq. (3) or Fig. 1, the FEP applies equally to any system and its environment (see also the discussion of this point in Fields et al. (2022)). The interaction HSE as defined by Eq. (3), and its implementation on the inter-system boundary  are, in particular, informationally symmetric. Each ���cycle��� of interaction between S and E consists, in the representation of Fig. 1, in the encoding by E of an N-bit

The ���environment��� is an active agent
It is well known that simple physical systems can self-organize complex structures when subjected to mechanical and thermodynamic forcing; thunderstorms provide the most familiar example. Turing (1953) introduced such processes to the study of morphogenesis; see Kondo and Miura (2010); Morelli et al. (2012) for reviews. In practice, such mechanisms are generally conceptualized as occurring inside the system of interest, e.g. inside a cell, or inside a developing organism. By considering the

Conclusion
We have seen here how the FEP provides a generic model of MCAs that applies equally to natural and engineered systems and equally to short and long timescales. Indeed the FEP erases the distinction between natural and engineered systems. Because the FEP characterizes the environment of any system of interest as an agent, the environment can always be regarded as ���training��� or ���engineering��� the system. The ubiquitous role of the environment in providing the parts required for replication
1 / 1
ChatGPT
ChatGPT
The provided text explores some aspects of the Free Energy Principle (FEP) and its implications.
Physical systems and Markov blanket (MB)
: The FEP applies to persistent physical systems with a well-defined state space. If a system's internal states are conditionally independent of its environment's states, the system possesses a Markov blanket. This blanket, which is essentially a boundary, conditions the dependence of internal states on external environmental states.
Informational symmetry of physical interactions
: The principle sees physical interactions as symmetric in terms of information exchange. A cycle of interaction between a system (S) and its environment (E) involves encoding information by E, perceived by S through its sensory states, which then mediates the effect of E on S's internal states. This symmetric view applies equally to any system and its environment.
Environment as an active agent
: Rather than considering environmental processes as external to a system, the FEP suggests the environment is an active agent that interacts with and influences the system. This viewpoint removes the distinction between natural and engineered systems, positing that the environment always plays a role in training or engineering a system.
Implications for understanding cognition and agency
: The FEP broadens the definition of 'cognition' and 'agency', beyond traditional conceptions which are often tied to biological organisms or complex machines. By considering all persistent systems as possessing a form of agency mediated by their Markov blanket, we can think about cognition in terms of information processing and adaptive response to the environment. This extends to everything from cells, to human brains, to artificial intelligence systems, and potentially even to non-living systems like rivers or weather patterns.
The FEP is thus a theoretical framework for understanding how any system���be it biological, physical, or artificial���interacts with its environment. It suggests a form of universal 'cognition' or 'agency' that all systems exhibit, providing a potential common ground for understanding diverse phenomena in the natural world.
1 / 1
Sorry, I forgot to mention - that was from this article: Elsevier
Biosystems
Volume 229, July 2023, 104927
Biosystems
Regulative development as a model for origin of life and artificial life studies
Author links open overlay panelChris Fields a, Michael Levin a b
Show more
Add to Mendeley
Share
Cite
https://doi.org/10.1016/j.biosystems.2023.104927
Get rights and content
Abstract
Using the formal framework of the Free Energy Principle, we show how generic thermodynamic requirements on bidirectional information exchange between a system and its environment can generate complexity. This leads to the emergence of hierarchical computational architectures in systems that operate sufficiently far from thermal equilibrium. In this setting, the environment of any system increases its ability to predict system behavior by ���engineering��� the system towards increased morphological complexity and hence larger-scale, more macroscopic behaviors. When seen in this light, regulative development becomes an environmentally-driven process in which ���parts��� are assembled to produce a system with predictable behavior. We suggest on this basis that life is thermodynamically favorable and that, when designing artificial living systems, human engineers are acting like a generic ���environment���.

Introduction
Living systems are hierarchical arrangements of components that, critically, exhibit a multiscale competency architecture (MCA; Levin, 2022; Clawson and Levin, 2022). In an MCA, components at each scale are competent to perform the functions appropriate to that scale without explicit top-down instructions. Human cells, for example, do not have to be told how to divide by the brain, or by any larger-scale system; they are competent to divide on their own. At any scale j in an MCA, the behaviors of the components at the lower scale j ��� 1 provide enabling mechanisms for j-appropriate behaviors, while the behaviors of the components at the higher scale j + 1 impose boundary conditions. As the boundary conditions effectively constrain the possibility space at scale j, state changes at scale j + 1 can modulate or deform the possibility space at scale j, providing a source of top-down novelty. Cell division, for example, is enabled by gene expression within the cell, and constrained by signaling from, or transmitted via, the surrounding multicellular tissue. Top-down ���instructions��� from the surrounding, or even distal (McMillan et al., 2021), tissue can increase or decrease the rate of cell division, e.g. during wound healing; other examples include the modulation of cancer cell properties by the microenvironment (Bissell et al., 2002; Ingber, 2008; Bizzarri and Cucina, 2014), control of stem cell fate by large-scale axial patterning cues during regeneration (Durant et al., 2017), and local remodeling of a structure based on its larger anatomical context (Farinella-Ferruzza, 1956)
1 / 1
ChatGPT
ChatGPT
The excerpt you provided is from a Biosystems journal article titled "Regulative development as a model for origin of life and artificial life studies" by Chris Fields and Michael Levin.
This study uses the Free Energy Principle (FEP) to argue that the thermodynamic requirements for bidirectional information exchange between a system and its environment can spur the creation of complexity, leading to the formation of hierarchical computational architectures in systems operating sufficiently far from thermal equilibrium. In this context, the environment is seen as a player in the system's morphological complexity, nudging it towards more extensive, macroscopic behaviors.
A concept at the heart of this work is the Multi-scale Competency Architecture (MCA), which is an integral part of living systems. In an MCA, each component level is competent to perform functions pertinent to that scale without needing explicit top-down instructions. The article's authors argue that this independence at each scale, combined with influence from the environment, can shape and drive complex behaviors in both biological and artificial systems.
This perspective reshapes our understanding of life and living systems, suggesting that life's origin and existence could be thermodynamically favorable, and that the environment plays a significant role in engineering life's complexity. The authors extend this idea to the design of artificial life systems, proposing that human engineers, in essence, are acting like a generic "environment" when creating such systems.
In other words, this approach to understanding living systems is rooted in the principles of thermodynamics and information theory, presenting life and cognition as fundamentally intertwined with the environment and the broader thermodynamic context. This provides a new lens through which to consider questions about the origins of life, the nature of cognition, and the design of artificial life.
1 / 1
By providing a scale-free, effectively thermodynamic definition of competence, the FEP enables a scale-free biology that treats evolution and development as manifestations (albeit at different levels of organization) of a single process ��� VFE minimization (Fields and Levin, 2020a, 2020b). It is natural to extend this conception of biology to encompass biologically-relevant prebiotic, abiotic, or exobiotic processes, including those invoked in origin of life and artificial life scenarios. To investigate the definition of ���competence��� provided by the FEP in this larger context, we will focus, in particular, on two kinds of biological self-organization:
Regulative development: Using information available from their surrounding environment(s), some number of cells self-organize into a multicellular organism. In both embryonic development and regeneration, such regulated self-organization enables multicellular systems to reach ���normal��� target morphologies despite significant perturbations or altered starting conditions (Birnbaum and Alvarado, 2008; Levin, 2011; Vandenberg et al., 2012; Lobo et al., 2014; Pezzulo and Levin, 2016; Pinet and McLaughlin, 2019; Fields and Levin, 2020b). Such processes are also employed by heterogeneous, facultative multicellular systems like microbial mats.

Ab initio self organization: Some number of molecules self-organize into a cell. The result could also be a self-sustaining proto-cell as in some origin-of-life models.


Regulative development has been studied for over two centuries, and many model systems are now understood at a substantial level of detail including, in several cases, cell-specific gene expression profiles (Tintori et al., 2016; Farnsworth et al., 2020; Wang et al., 2022). Examples of ab initio self-organization, on the other hand, remain hypothetical. While a variety of models have been proposed, mostly in an origin-of-life context, none have been fully implemented experimentally and there is little consensus even about biological plausibility. The goal of any scale-free explanation is to tell exactly the same kind of story about these two examples. Constructing such an explanation by requiring examples of ab initio self-organization to be both formal and mechanistic analogs of regulative development raises a number of issues, mainly concerning the structure and role of the ���environment��� in ab initio self-organization, that have been relatively neglected by previous approaches.

In particular, we will see that treating ab initio self-organization as an analog of regulative development challenges two deeply-entrenched ideas. First, it questions the near-universal assumption that any ab initio model must result in a self-replicating system. Evolutionary models from Darwin onwards have strongly coupled variation with inheritance via either meiotic or mitotic cell division (Monod, 1972; Szathm��ry and Maynard Smith, 1995; Michod, 1999). Natural selection acts on variants, and hence depends on Darwinian models of reproduction. Such models suggest ab initio processes that involve self-replicating molecules immediately. We will suggest that models in which the needed ���parts��� are generated by environmental processes that are at most weakly coupled to the systems of interest are also worth consideration. Variation generated by weakly coupled processes is consistent with evolution at the global scale, but does not depend on natural selection at any single local scale; it thus provides a ���Non-Darwinian��� source of order. Such weak coupling is exemplified by situations involving self-organizing systems that include engineered and manufactured components; we suggest that weak-coupling models may be realistic in other settings as well.

Second, treating ab initio self-organization as an analog of regulative development challenges the very idea of self-organization. In its purest form, the idea of self-organization suggests that the information needed for organization is present in the self. This immediately raises the essentially unanswerable question of how this critical information got there to begin with. Directly relating ab initio self-organization to regulative development forces us to ask, at each step of the process, what systems count as ���selves,��� how the environment of each ���self��� is defined, and how, in each case, the exchange of information between ���self��� and environment is implemented (Levin, 2021).
1 / 1
ChatGPT
ChatGPT
In this part of the paper, the authors present an approach to understanding biological organization and complexity that treats evolution and development as manifestations of a single process under the Free Energy Principle (FEP) - the minimization of Variational Free Energy (VFE). This approach extends beyond the biological domain to encompass prebiotic, abiotic, and exobiotic processes, which are particularly relevant for theories of the origin of life and artificial life.
The authors highlight two forms of biological self-organization:
Regulative development
: Cells use information from their surroundings to self-organize into a multicellular organism. This process enables multicellular systems to reach normal target morphologies despite significant disturbances or altered starting conditions.
Ab initio self-organization
: Molecules self-organize to form a cell or a self-sustaining proto-cell, a concept often invoked in origin-of-life models.
The authors note that while regulative development is well studied, instances of ab initio self-organization remain largely hypothetical.
Taking ab initio self-organization as analogous to regulative development, they challenge two deep-seated ideas:
The assumption that any ab initio model must result in a self-replicating system. Instead, the authors suggest considering models where the necessary "parts" are generated by environmental processes that are at most weakly coupled to the systems of interest.
The very notion of self-organization. The authors propose that the concept of self-organization, in its pure form, where information needed for organization is present within the system itself, raises the question of how such information originated. They recommend analyzing how the exchange of information between the "self" and the environment occurs at each stage of the process.
These discussions provide a novel perspective on the emergence of complex systems and life itself, suggesting that the environment plays a more active role in this process than traditionally thought. They also underscore the importance of considering interactions at various scales, from individual cells to whole organisms, and the dynamic, ongoing exchanges of information that occur between these entities and their environments.
1 / 1
Our analysis suggests two broad conclusions:
1.
Reproduction via cell division is an efficiency hack ��� a cheap heuristic ��� which evolution ���froze in��� by selecting strong self/other recognition systems starting in bacteria. Decoupling replication from self-organization generalizes the systems of interest to developmental studies toward ���egalitarian��� (Strassmann and Queller, 2010) assemblages of unrelated, or in the evolved case, only distantly related, components that ���just happen��� to work well together. This allows simpler origin of life stories. It acknowledges the possibility of kinematic replication in biological systems, as observed experimentally in ���xenobots��� constructed from dissociated Xenopus laevis skin cells (Kriegman et al., 2021). It suggests that symbiotic systems such as microbial mats are ���canonical��� to at least the same extent as obligately sexual multicellulars. The focus of both ab initio and developmental studies becomes, in this case, how components interact once they are placed in mutual proximity, independently of their origins.

2.
���Self-organization��� is always environment-dependent, so we can view it as at least in part environment-directed: some of the instructive information is initially in the environment, not in the self that assembles. From the perspective of physical interaction (Fields and Marcian��, 2020; Fields, Glazebrook and Marcian��, 2021) or of the FEP (Friston, 2019; Ramstead et al., 2022; Fields et al., 2022), this is obvious. It is also obvious in many origin of life models, in microbial mat assembly, and in embryogenesis, even in ���European plan��� organisms like C. elegans (Barri��re and Bertrand, 2020). It is, nonetheless, often neglected or de-emphasized, particularly in theoretical work. As discussed in Fields et al. (2022), the FEP applies to the environment of any system of interest, rendering it an uncertainty-minimizing agent as discussed in ��4 below. The environment-dependence of self-organization is a consequence of the environment acting as a Bayesian agent.


In what follows, we first review in ��2 both the classical development of the FEP and its quantum information-theoretic generalization. As the latter provides the most convenient formalism for describing the system-environment interaction, we adopt it in what follows. We then discuss in ��3 a fundamental symmetry of the FEP that is often neglected: the FEP requires the environment of any system to also be a VFE-minimizing agent. While any system-environment interaction is informationally symmetric ��� equal quantities of information flow in both directions ��� what the two parties do with the information they receive may be radically different. Symmetric information flows, in other words, are consistent with ���cognitive light cones��� (Levin, 2021, 2022) of different widths and depths, and hence different active inference capabilities, on the two sides of the system-environment boundary. We turn in ��4 to an explicit comparison between regulative development and ab initio self-organization, focusing first on characterizing the environments of each active component as information sources, and hence as themselves active agents, at each step in the process. We investigate, in particular, how both regulative development and ab initio self-organization decrease the VFE detected by the environment, making them thermodynamically-driven processes under the FEP. The environment of any system, in other words, can be expected to act so as to increase that system's complexity, with the likely outcome being a complexity-increasing ���arms race��� between the system and its environment if both have sufficient computational resources. We conclude by outlining a ���mix and match��� experimental strategy suggested by these results in ��5.
1 / 1
ChatGPT
ChatGPT
The authors draw two primary conclusions from their analysis:
The process of reproduction through cell division, which is a common feature in life as we know it, is seen as an "efficiency hack" that evolution selected. The authors argue that by separating replication from self-organization, it allows the investigation of systems composed of unrelated components that function well together. This shift in perspective simplifies origin-of-life scenarios, highlights potential for kinematic replication (as seen in experiments with 'xenobots'), and posits that symbiotic systems such as microbial mats are equally 'canonical' as obligately sexual multicellular organisms.
The authors argue that "self-organization" is always dependent on the environment, and can be viewed as partly directed by the environment. Some of the information required for the assembly of the "self" is initially found in the environment. This concept is clear in physical interaction models, the Free Energy Principle (FEP), many origin-of-life models, and embryogenesis. The authors suggest that the environment acts as a Bayesian agent in the process of self-organization.
Following these conclusions, the authors plan to delve deeper into the classical development of the Free Energy Principle and its quantum information-theoretic generalization. They discuss a critical symmetry of the FEP, asserting that the FEP requires the environment of any system to also minimize Variational Free Energy (VFE).
The authors then plan to explore an explicit comparison between regulative development and ab initio self-organization, focusing on characterizing the environments of each active component as information sources, and hence as themselves active agents, at each step in the process. This approach reveals that both regulative development and ab initio self-organization reduce the VFE detected by the environment, rendering them thermodynamically driven processes under the FEP.
The authors propose that the environment of any system is likely to act in a way that increases the system's complexity, leading to a potential "arms race" in complexity between the system and its environment if both have sufficient computational resources.
Finally, the authors propose a "mix and match" experimental strategy suggested by these results. This suggests they plan to outline a method for exploring these ideas further in an experimental setting.
1 / 1
Biosystems
Volume 219, September 2022, 104714
Biosystems
Neurons as hierarchies of quantum reference frames
Author links open overlay panelChris Fields a, James F. Glazebrook b c, Michael Levin d
Show more
Add to Mendeley
Share
Cite
https://doi.org/10.1016/j.biosystems.2022.104714
Get rights and content
Abstract
Conceptual and mathematical models of neurons have lagged behind empirical understanding for decades. Here we extend previous work in modeling biological systems with fully scale-independent quantum information-theoretic tools to develop a uniform, scalable representation of synapses, dendritic and axonal processes, neurons, and local networks of neurons. In this representation, hierarchies of quantum reference frames act as hierarchical active-inference systems. The resulting model enables specific predictions of correlations between synaptic activity, dendritic remodeling, and trophic reward. We summarize how the model may be generalized to nonneural cells and tissues in developmental and regenerative contexts.

Introduction
Neurons are canonical biological information processors. Theoretical and, particularly, conceptual models of neural information processing, however, lag increasingly far behind our developing empirical understanding of neurons as electrically-excitable cells. Experimental work over the past two decades has, for example, firmly established that dendrites undergo activity-dependent remodeling (Butz et al., 2009, Carulli et al., 2011, Hogan et al., 2020), particularly alterations of spine location, density, and function (Runge et al., 2020), even in adult humans. This ontogenic process is functionally analogous to the evolution of structural and positional diversity of dendrites as they have adapted to a spectrum of functional roles (Wittenberg and Wang, 2016), e.g. the implementation of deep learning via synaptic plasticity (Guerguiev et al., 2017, Sardi et al., 2018). Neurons are not, therefore, static structures, but rather can be regarded as undergoing continuous development throughout the life cycle. This dynamic process has significant consequences for both neuron-level and organism-level function. For example, in organisms (such as caterpillars transitioning into butterflies or moths) that exhibit drastic remodeling and reconstruction of their brains, some of their learned memories remain and survive the process (Blackiston et al., 2015). In other contexts, memories can be imprinted on newly regenerating brains from other tissues (Shomrat and Levin, 2013, McConnell, 1967), underscoring the plasticity of large-scale neural structure and of the information stored within it. These effects of remodeling are not just an issue for so-called lower animals, as applications in regenerative medicine are likely to soon produce human patients in whom some portion of the brain has been replaced by the progeny of na��ve stem cells to treat degenerative disease or brain damage.
1 / 2
ChatGPT
ChatGPT
The authors present a novel approach to modeling neurons using quantum information-theoretic tools. This approach aims to create a uniform, scalable representation of synapses, dendritic and axonal processes, neurons, and local networks of neurons, in which hierarchies of quantum reference frames function as hierarchical active-inference systems. The model can make specific predictions about correlations between synaptic activity, dendritic remodeling, and trophic reward. Moreover, the authors hint at the potential to generalize this model to non-neural cells and tissues in the context of development and regeneration.
In the introduction, the authors argue that theoretical and conceptual models of neural information processing are lagging behind empirical understanding. They highlight recent experimental findings, which demonstrate that dendrites (the branched extensions of neurons that receive signals from other neurons) undergo activity-dependent remodeling. This means that neurons aren't static structures but undergo continuous development throughout their lifecycle.
The authors point out several implications of this perspective. They note that in organisms that undergo drastic remodeling of their brains, such as caterpillars transitioning into butterflies or moths, some learned memories can survive the process. This underscores the plasticity of large-scale neural structure and the information stored within it. Additionally, they mention instances where memories can be imprinted on newly regenerating brains from other tissues.
Finally, the authors argue that this topic isn't just of interest in studying "lower animals". They suggest that the understanding of these processes will be important for future applications in regenerative medicine, where human patients may have portions of their brain replaced with progeny of na��ve stem cells to treat degenerative diseases or brain damage.
1 / 1
It has also been shown that subnetworks of dendrites can compute local logical operations including exclusive-or (XOR) (Gidon et al., 2020). Nonetheless, the idea that dendritic trees effectively compute weighted sums, dating back to 1943 (McCulloch and Pitts, 1943), continues not only to dominate artificial neural network (ANN) development, but also to guide biological thinking. Explicit cable-theory modeling can reproduce time-dependent signal propagation and processing in simplified representations of dendritic trees (Segev and London, 2000), but rapidly becomes intractable with increasing geometric complexity. Increasingly sophisticated hardware models of neurons allow exploration of phase coding, frequency modulation, and other forms of hybrid analog���digital computing (Schuman et al., 2017, Tang et al., 2019), but are not readily mapped to biological neural networks and are not standard components of a theoretical neuroscientist���s toolkit.

Conceptualizing neurons as biological wires ��� even functionally sophisticated wires ��� provides, moreover, no insight into the fundamental question of cognitive neuroscience: the question of how information present in the external world is encoded, via some combination of evolution, development, remodeling, and learning, into the functional architecture of a nervous system. Brains are not constructed, as ANNs are, in the absence of functional activity, nor do they ���begin learning��� from a default initial state of uniform connectivity. Brains are not ���wired up��� into some starting configuration, after which they are turned on and exposed to the world. Brains instead develop from neural primordia that are already functionally complex multicellular microenvironments. The function of the nervous system, from its earliest phylogenetic and ontogenetic role in directing morphogenesis (Fields et al., 2020) to adult cognition, depends on the ability of individual neurons to negotiate this microenvironment, both structurally and functionally. The local microenvironment, with its diverse cell types, biochemical and (micro)anatomical complexity, and network of biomolecular and bioelectric signaling, is the ���world��� with which each individual neuron interacts, and is each neuron���s sole source of nongenomic information. There are many interesting scenarios supporting this viewpoint; some are of the ���socialization��� and decision making type. For instance, assortative neural networks demonstrate collective resilience, whereby nodes of a certain degree have an affinity to team up with those of the same topological type, and thus often contribute to formation of a small world network structure (Barrat et al., 2008, Rubinov and Sporns, 2010). Neural network modeling of independent tasks in the prefrontal cortex in Latham and Dayan (2005), for example, reveals how seemingly separate neuronal groups engaged in their respective tasks may sometimes be drawn into a coalition, casting votes for a stimulus, and then enacting a committee-like decision.
1 / 1
ChatGPT
ChatGPT
This part of the paper further discusses the complexity of neural processes and how current models fall short in capturing these dynamics. For example, dendritic trees perform complex computations, but are often simplistically modeled as just computing weighted sums, a perspective that dates back to 1943. Although there are sophisticated hardware models of neurons that explore advanced concepts such as phase coding and frequency modulation, they don't easily map to biological neural networks and are not standard tools in theoretical neuroscience.
The authors critique the view of neurons as merely biological wires, stating that it provides no insight into the fundamental question of cognitive neuroscience: how is information from the external world encoded into the functional architecture of a nervous system through processes such as evolution, development, remodeling, and learning? They argue that brains are not constructed in the absence of functional activity, nor do they start learning from a state of uniform connectivity. Rather, brains develop from already functionally complex neural primordia.
The authors also highlight that the function of the nervous system depends on the ability of individual neurons to negotiate with their microenvironment, both structurally and functionally. They stress that the local microenvironment is the only source of nongenomic information for each neuron.
Additionally, the authors discuss the idea of neural networks showing collective resilience and the formation of small-world network structures. They also mention a study which reveals how separate neuronal groups may sometimes form coalitions, providing an example of the intricate dynamics within neural networks.
1 / 1
In line with the Free Energy Principle (FEP) and the idea of Bayesian active inference (Friston, 2010, Friston, 2013, Friston and Stephan, 2007, Ramstead et al., 2020, Friston, 2019), individual neurons can be considered cognitive agents that minimize aggregate uncertainty about their future states (variational free energy or VFE) by actively exploring and developing predictive models of their local microenvironments. It is the joint activity of hundreds (in C. elegans Varshney et al. (2011)) to tens of billions (in primates Herculano-Houzel (2011)) of neurons within this jointly-constructed environment that encodes information sourced from the external world. Hence an adequate conceptual model of neurons as biological systems must explain how each neuron���s construction of a predictive model of its local microenvironment results in the joint encoding of a predictive model of the external environment at the scale of the entire brain. The utility of FEP based models of cell sorting in morphogenesis (Friston et al., 2015a, Kuchling et al., 2020), cortical minicolumn and local-circuit organization (Kiebel and Friston, 2011, Bastos et al., 2012, Shipp et al., 2013, Kanai et al., 2015, Adams et al., 2015), and network and whole-brain function (Friston, 2013, Clark, 2013, Hohwy, 2013, Seth, 2013, Friston et al., 2015b) suggests that a scale-free conceptual framework is the right way to approach this question of functional coherence between microenvironment models constructed by individual neurons and external environment models constructed by brains. The FEP is, however, fundamentally a statement about error minimization; it is an implementation-independent principle of thermodynamics (Friston, 2019). It does not tell us anything specific about neuronal functional architecture, or about how neurons interact with either each other or the non-neuronal components of their microenvironments.
1 / 1
ChatGPT
ChatGPT
The authors discuss the Free Energy Principle (FEP) and the idea of Bayesian active inference, which suggest that individual neurons can be seen as cognitive agents that minimize uncertainty about their future states by actively exploring and developing predictive models of their local microenvironments. The collective activity of neurons within this jointly constructed environment encodes information sourced from the external world.
Therefore, an adequate conceptual model of neurons must explain how each neuron's construction of a predictive model of its local environment results in the joint encoding of a predictive model of the external environment at the scale of the entire brain. They point out that FEP based models have been useful in various contexts, such as cell sorting in morphogenesis, organization of cortical minicolumns, local circuits, and network and whole-brain function.
The authors also highlight a limitation of the FEP: it is fundamentally a principle about error minimization and does not provide specific information about the functional architecture of neurons, or about how neurons interact with each other or the non-neuronal components of their microenvironments.
1 / 1
Here we suggest that the idea of a quantum reference frame (QRF) developed within quantum information theory (Aharonov and Kaufherr, 1984, Bartlett et al., 2007) is usefully applied to characterize neurons both architecturally and functionally. While quantum information theory employs the formalism of standard quantum theory, it makes no scale-dependent assumptions and is applicable from sub-microscopic to cosmological scales. Both the concept of a QRF and the accompanying formalism are, in particular, applicable at any scale. A QRF is a physical system that assigns units of measurement, and hence an operational semantics, to each observational outcome, enabling outcomes obtained at different times or places, or with different QRFs, to be compared. Macroscopic systems such as meter sticks, clocks, gyroscopes, and the Earth with its gravitational and magnetic fields are canonical examples of QRFs (Bartlett et al., 2007). While the behavior of such macroscopic systems can be adequately described for practical purposes using classical physics, quantum theory introduces a subtle but important difference when they are reconceptualized as QRFs. Physical systems employed as reference frames, e.g. clocks or meter sticks, are typically thought of as both functionally exchangable and fully-specifiable as abstractions in classical physics; however, neither is the case when they are redescribed using quantum theory. The quantum-theoretic description of any physical system includes unmeasurable but functionally relevant quantum phase information; it is this phase information that generates observable phenomena such as quantum interference (Feynman et al., 1964/1966) and entanglement (Mermin, 1985). Every QRF is, therefore, in an important sense a unique physical system that can neither be functionally exchanged nor fully specified. A QRF cannot, in particular, be fully specified either structurally or functionally by any finite bit string; it is ���nonfungible��� in the terminology of Bartlett et al. (2007). Alice can, in this case, share a QRF with a distant observer Bob only by physically transferring the QRF to Bob; sending any finite description is provably insufficient. Alice can, moreover, transfer her QRF to Bob successfully only if Bob already possesses a functionally equivalent QRF (Fields and Marcian��, 2019b); Alice cannot, for example, successfully communicate the meaning of ���1 meter��� to Bob unless Bob already has a QRF that effectively functions as a meter stick. From an information processing point of view, a QRF is a quantum computer: it outputs an outcome value that is reproducible within some fixed resolution, and that is assigned standardized units that confer an operational semantics, when given a ���raw��� measurement, or simply a physical interaction, as input.

We propose here that neurons are usefully regarded as hierarchies of QRFs. Neurons are, in other words, hierarchies of measurement devices, each one of which samples its local microenvironment at its own scale and encodes scale-specific expectations about how its microenvironment will behave. At the smallest scale, that of an individual patch of membrane, the relevant microenvironment is predominately extracellular; what is measured at this scale is primarily an array of local ion concentrations. At each larger scale, the relevant microenvironment is predominately intracellular; what is measured are primarily signals generated by multiple smaller-scale QRFs, with the fan-in determined by the three-dimensional (3d) configuration of the neuron at the relevant scale. These incoming signals encode, effectively, prediction errors and hence VFE generated at smaller scales. The output of an individual neuron encodes, for each cell that receives it, a convolution of its lowest-level local measurements with its particular hierarchy of higher-level constraints, constraints determined both by its time-varying state and its unique 3d geometry. As the functionally relevant time scale increases with spatial scale, from the  timescales of ion transport across membrane patches (Chung and Kuyucak, 2002) to the tens of ms timescales of dendrite-scale postsynaptic potentials, this convolution process implements temporal as well as spatial coarse-graining. The resulting whole-neuron scale convolved representations are encoded in the ���units of measurement��� defined by the sending neuron���s particular combination of QRFs. These large scale, effective units impose operational semantic constraints on all downstream processing. These constraints are inherited by, and give a particular operational meaning to, the activity patterns of the neural system as a whole.
1 / 1
ChatGPT
ChatGPT
In this segment, the authors propose that the idea of a quantum reference frame (QRF), developed within quantum information theory, is a useful way to characterize neurons both structurally and functionally. A QRF is a physical system that assigns units of measurement, enabling outcomes obtained at different times or places to be compared. This concept applies at all scales, from sub-microscopic to cosmological.
The authors argue that while traditional physical systems used as reference frames can be described by classical physics, when these systems are reconceptualized as QRFs, quantum theory introduces additional information. Every QRF includes unmeasurable but functionally relevant quantum phase information, which produces observable phenomena such as quantum interference and entanglement.
The authors propose that neurons can be seen as hierarchies of QRFs, where each level of the hierarchy measures its local microenvironment at its own scale and encodes expectations about its future behavior. At the smallest scale, this environment is primarily extracellular and measures an array of local ion concentrations. At larger scales, the microenvironment is mostly intracellular and measures signals generated by smaller-scale QRFs. These signals represent prediction errors and variances in free energy from smaller scales. The output of an individual neuron represents a convolution of its lowest-level local measurements with its hierarchy of higher-level constraints, determined by its current state and unique three-dimensional geometry. This convolution process implements both spatial and temporal coarse-graining, and the resulting whole-neuron scale convolved representations impose semantic constraints on all downstream processing. These constraints shape the operational meaning of the activity patterns of the entire neural system.
1 / 1
It is worth emphasizing that our goal in describing neurons as hierarchies of QRFs is to provide a uniform, scale-free formalism for specifying both what is measured and how measurement outcomes are processed at each scale of the hierarchy. We have, in related work, recently shown that the methods employed here enable a formulation of the FEP applicable to generic quantum systems (Fields et al., 2021a); hence describing neurons as hierarchies of QRFs is also, via a well-defined formal mapping, describing them as hierarchies of VFE-minimizing Bayesian agents. This is a functional description, and is hence implementation-independent; however, it places strong constraints on the information-processing capabilities of any implementation. It provides, in particular, strong first-principles constraints on the computational capabilities of biological structures at each scale of the hierarchy. In this it contrasts with quantum biology approaches that focus on quantum effects in, and hence quantum computational capabilities of, particular molecules or cellular structures, e.g. Hameroff and Penrose, 1996, Fisher, 2015 and Smolin (2020); see Marais et al. (2018) and Cao et al. (2020) for recent reviews.

We begin in Section 2 by briefly reviewing the implementation of QRFs by generic quantum systems (Fields et al., 2021a, Fields and Marcian��, 2019a, Fields and Glazebrook, 2020b, Fields et al., 2021c) and their representations by CCCDs. We then turn to biological systems, reviewing the representation of simple homeostatic setpoints as QRFs (Fields and Levin, 2020, Fields et al., 2021b) and extending this treatment to perisynaptic processes in neurons in Section 3. This allows us in Section 4 to represent the hierarchical structures of dendrites as hierarchical QRFs that detect the states ��� activity patterns ��� of particular ���objects��� ��� spatially-organized aggregates of presynaptic axon terminals ��� in their microenvironments. We then extend this representation to neurons in Section 5, showing how the integration of signals from multiple dendritic branches implements an effectively tomographic computation. This view of neurons as state identifiers scales upwards to functional networks, the global activity of which provide coarse-grained representations of particular components of the external environment. We then briefly review in Section 6 results from a variety of systems showing that with suitable spatial and temporal scaling, this model applies to electrically-excitable cells generally. We consider simulation modeling and experimental approaches suggested by this framework in Section 7.
1 / 1
ChatGPT
ChatGPT
In this paragraph, the authors reiterate that their primary aim is to describe neurons as hierarchies of Quantum Reference Frames (QRFs) to provide a uniform, scale-free formalism for defining both what is measured and how these measurements are processed at each scale of the hierarchy. This method allows them to characterize neurons as hierarchies of Variational Free Energy (VFE) minimizing Bayesian agents, providing a functional and implementation-independent description. This places strong constraints on the information-processing capabilities of any implementation and provides first-principles constraints on the computational capabilities of biological structures at each hierarchical level.
The authors distinguish their approach from quantum biology approaches that focus on quantum effects in specific molecules or cellular structures, suggesting that their approach offers more comprehensive insight into the function and capabilities of biological structures.
They then provide an overview of the structure of their work. They start by briefly reviewing the implementation of QRFs by generic quantum systems and their representations. They then turn to biological systems, reviewing the representation of simple homeostatic setpoints as QRFs and extending this to perisynaptic processes in neurons. Following this, they represent the hierarchical structures of dendrites as hierarchical QRFs. They then extend this representation to neurons, showing how the integration of signals from multiple dendritic branches carries out an effectively tomographic computation. They then review results from a variety of systems showing that this model applies to electrically-excitable cells generally. Lastly, they discuss simulation modeling and experimental approaches suggested by this framework.
1 / 1
Section snippets
QRFs as generic representations of measurement
Information processing in biological systems has traditionally been represented as classical; despite theoretical arguments from a variety of perspectives (Hameroff and Penrose, 1996, Schr��dinger, 1944, Bordonaro and Ogryzko, 2013, Tononi and Koch, 2015, Georgiev, 2020) and experimental evidence for the functional relevance of quantum coherence in photoreception and magnetoreception (Arndt et al., 2009, Lambert et al., 2012, Melkikh and Khrennikov, 2015), quantum biology remains in its infancy (

QRFs at the macromolecular scale
We now turn from generalities to the specific case of mammalian cortical neurons, and show how these can be conceptualized as hierarchies of QRFs, i.e. hierarchies of physically-implemented computations, each of which can be regarded as a measurement, at a defined scale and with respect to defined units, of some aspect of its local environment. We then generalize this treatment to arbitrary cells in Section 6.

Dendrites as spatially-organized QRFs
Dendritic branches are traditionally viewed as lossy but otherwise passive integrators of relatively slow (tens of ms) postsynaptic potentials (PSPs), though faster spike-like activations and backwards propagation of APs are also possible (Spruston, 2008); see Eyal et al. (2018) for detailed dendritic spike models. This passive view of dendritic activity renders the execution of complex logical operations like XOR (Gidon et al., 2020) surprising and activity-dependent remodeling (Carulli et

Dendritic geometry imposes salience
As the ���cut��� and hence the boundary imposed to render a branch ���terminal��� is moved toward the soma, the branch accumulates junctions separating sub-branches, and object-identity information is replaced by coincidence information. At each such sub-branch junction, the post-junction sector of the dendrite is faced with the task of reducing VFE by distinguishing ���true��� from ���random��� coincidences. Learning and remodeling driven by trophic reward remain the tools available for amplifying the former

Generalizing neural computation to cellular computation
The prior discussion was framed in the context of neurons; however, it becomes more generally applicable when we note that neurons did not appear de novo but in fact evolved slowly from other cell types which already shared many of their features (reviewed in Fields et al. (2020)). Not only are the basic molecular components of neurons (ion channels, electrical synapses, neurotransmitter machinery) already present in most cells including unicellulars, but all cells produce bioelectric gradients 

Conclusions and future directions
Neurons, networks of neurons, and biological structures generally exchange information with their environments via physical interactions interpretable in terms of measurement (or the gathering of sensory input) and manipulative action. This interpretation of biological activity forms the basis of the active-inference principle (Friston, 2010, Friston, 2013) and has achieved wide currency in the biophysics, evo-devo, and neuroscience communities. We have shown here how to formulate this view of
1 / 1
ChatGPT
ChatGPT
In these sections, the authors delve into their conceptualization of neurons as hierarchies of Quantum Reference Frames (QRFs), breaking it down into several components.
QRFs as generic representations of measurement:
The authors note that traditionally, information processing in biological systems has been represented as classical, with quantum biology remaining a relatively nascent field. However, they underscore that Quantum Reference Frames can provide a new perspective for understanding the information processing in biological systems.
QRFs at the macromolecular scale:
The authors discuss how cortical neurons can be conceptualized as hierarchies of QRFs. Each QRF in the hierarchy represents a computation or a measurement of some aspect of its local environment at a specific scale and in relation to defined units.
Dendrites as spatially-organized QRFs:
Here, the authors challenge the traditional view of dendritic branches as passive integrators of postsynaptic potentials. They suggest that dendrites could instead be considered as spatially-organized QRFs that conduct complex logical operations.
Dendritic geometry imposes salience:
In this section, the authors discuss how the geometry of dendrites can influence information processing, particularly in the context of reducing Variational Free Energy (VFE). The learning and remodeling driven by trophic reward is seen as a tool for amplifying specific information.
Generalizing neural computation to cellular computation:
The authors extend their theory beyond neurons, arguing that the computational concepts and tools present in neurons are also found in other cell types. They suggest that the concept of QRFs can be applied to all cells that produce bioelectric gradients.
Conclusions and future directions:
The authors conclude by emphasizing the importance of understanding the interaction between biological structures and their environments in terms of measurement and manipulative action. They reiterate that their formulation of this view using Quantum Reference Frames provides a promising new perspective for the fields of biophysics, evolutionary developmental biology (evo-devo), and neuroscience. They suggest that their conceptualization of neurons as hierarchies of QRFs offers a novel approach to understanding the fundamental mechanisms of biological information processing.
1 / 1
A list of all of the topics we've discussed so far.
1 / 1
ChatGPT
ChatGPT
Here's a list of the topics we've discussed so far in our conversation:
Quantum Reference Frames (QRFs) and their applicability in quantum information theory.
Concept of QRFs being applicable at all scales, from sub-microscopic to cosmological.
Quantum phase information being intrinsic to the quantum-theoretic description of physical systems, leading to phenomena such as quantum interference and entanglement.
The concept of neurons as hierarchies of QRFs, and their role in information processing.
The application of the Free Energy Principle (FEP) and Variational Free Energy (VFE) in the context of QRFs and neurons.
The application of QRFs at different scales, from the macromolecular to the whole-neuron scale.
The impact of dendritic geometry on information processing and salience in the context of neurons as QRFs.
The generalization of the concept of neurons as QRFs to all electrically-excitable cells.
The exploration of experimental and modeling approaches suggested by the concept of neurons as QRFs.
The discussion of the relevance and future directions of considering neurons as hierarchies of QRFs, and its implications for the fields of biophysics, evolutionary developmental biology (evo-devo), and neuroscience.
1 / 1
Connections with the following topics, we discussed earlier: Regulative development: We discussed this concept in relation to how it contributes to the origin of life and artificial life studies. It's a biological process where cells self-organize into multicellular organisms, based on information from their environment. The process allows these systems to achieve their usual morphologies despite substantial perturbations or modified starting conditions.

Ab initio self-organization: This concept deals with the self-organization of molecules into a cell, as seen in some models of the origin of life. This process remains largely hypothetical, with no consensus on its biological plausibility.

Free Energy Principle (FEP): We discussed the application of the FEP to biological systems, where it provides a thermodynamic definition of competence that is scale-free, enabling a biology that treats evolution and development as manifestations of a single process: Variational Free Energy (VFE) minimization.

Challenge to traditional concepts: We explored how treating ab initio self-organization as an analog of regulative development challenges some entrenched ideas, including the necessity for self-replication in ab initio models and the concept of self-organization itself.

Environment's role in self-organization: The authors suggest that the information needed for self-organization may be present in the environment, which directs part of the process. This perspective treats the environment as a Bayesian agent that minimizes uncertainty.

Decoupling replication from self-organization: The authors propose that cell division is an "efficiency hack" evolution selected and that separating replication from self-organization can lead to simpler origin-of-life scenarios. They emphasize the potential for self-organization amongst unrelated components that function well together.

The environment as an active agent: The authors suggest that the environment acts to increase a system's complexity, leading to a potential complexity-increasing "arms race" between the system and its environment.

Mix and match experimental strategy: The authors propose an experimental strategy that could be used to investigate these theories further.

These topics contribute to an ongoing exploration of life's origins, the mechanisms behind biological development, and possible strategies for the creation of artificial life.
1 / 1
ChatGPT
ChatGPT
The connections between the earlier discussions and the recent topics of Quantum Reference Frames (QRFs) and neurons are intricate and lend themselves to a comprehensive understanding of biological systems. Here are some of the connections:
Regulative development and QRFs
: Both concepts are centered around self-organization and information processing. In regulative development, cells self-organize based on environmental information. In the case of neurons as QRFs, neurons are considered to self-organize into a hierarchy based on their local microenvironments.
Ab initio self-organization and QRFs
: Both themes focus on the self-organization of a system, albeit at different scales. Ab initio self-organization is a macro-level concept involving the self-organization of molecules into a cell, while QRFs apply at micro and macro scales, highlighting the capability of neurons to self-organize and process information.
Free Energy Principle (FEP) and QRFs
: FEP was discussed as a unifying principle for biological systems. In the context of QRFs, FEP is used to describe neurons as hierarchies of Variational Free Energy (VFE)-minimizing Bayesian agents.
Challenge to traditional concepts
: Both discussions question traditional notions���in the first case, concepts of self-replication and self-organization, and in the latter, the classical view of neurons and information processing in biological systems.
Environment's role in self-organization and QRFs
: Both narratives emphasize the role of the environment in directing the self-organization process. Neurons as QRFs propose that neurons are hierarchies of measurement devices, each one sampling its local microenvironment.
Decoupling replication from self-organization and QRFs
: The concept of neurons as QRFs decouples the functional aspects of neurons (information processing) from their physical replication, similar to how ab initio self-organization separates self-replication from self-organization.
The environment as an active agent
: The concept of QRFs places a neuron's local microenvironment in an active role, similar to the environment's role as a Bayesian agent in the earlier discussions.
Mix and match experimental strategy
: Both discussions point to the need for innovative experimental approaches to validate the theoretical constructs proposed.
These connections serve to highlight the holistic nature of biological systems, bridging quantum physics, information theory, and biological self-organization to form a unified understanding of life processes.
1 / 1
And before that we discussed pancomputationalism, digital physics, and the mathematical universe hypothesis.
1 / 1
ChatGPT
ChatGPT
Sure, let's add those topics to our list and connect them with the discussions we've just had:
Pancomputationalism
: This idea posits that the universe is a computational machine, or a network of computational processes which, following fundamental physical laws, computes its own next state from the current one. When considering neurons as QRFs, the computation becomes more nuanced - they're not just passive computation units, but they actively engage with their local microenvironment, applying computations to minimize variational free energy. This idea extends pancomputationalism to a more interactive, dynamic level.
Digital Physics
: Digital Physics proposes that the universe is, at its core, describable by information, and that all its processes are computational. The concept of QRFs takes this idea and applies it at the level of neurons, seeing them as information processors that detect, measure, and respond to their local environment.
Mathematical Universe Hypothesis (MUH)
: MUH suggests that our physical reality is not merely described by mathematics, but that it is a mathematical structure. When considering neurons as QRFs, each neuron's state and the resulting network dynamics can be represented mathematically. Similarly, the application of the FEP to biological systems and the representation of environments as Bayesian agents brings a mathematical framework to the understanding of biology, supporting the MUH.
By integrating these topics, we move towards a broader perspective that encompasses not just the physical and biological domains, but also the realm of information and computation. This perspective could provide new insights into our understanding of life, intelligence, and the very nature of reality itself.
1 / 1
Ok now another list of the topics discussed.
1 / 1
ChatGPT
ChatGPT
Here's a comprehensive list of all the concepts we've discussed so far:
Regulative Development
: The biological process where cells self-organize into multicellular organisms, responding to environmental cues.
Ab Initio Self-Organization
: Theoretical self-organization of molecules into a cell in origin-of-life models.
Free Energy Principle (FEP)
: A thermodynamic principle applied to biological systems, treating evolution and development as manifestations of a single process of variational free energy (VFE) minimization.
Environment's Role in Self-Organization
: Treating the environment as a Bayesian agent that provides information needed for self-organization and uncertainty minimization.
Decoupling of Replication from Self-Organization
: A novel approach that separates replication from self-organization, emphasizing the potential for self-organization among unrelated components.
Environment as an Active Agent
: The proposition that the environment acts to increase a system's complexity, leading to a potential complexity-increasing "arms race" between the system and its environment.
Mix and Match Experimental Strategy
: A proposed experimental strategy to further investigate these theories of life's origins and development.
Pancomputationalism
: The view that the universe is a computational machine, or a network of computational processes.
Digital Physics
: A perspective proposing that the universe is fundamentally describable by information, and that all its processes are computational.
Mathematical Universe Hypothesis (MUH)
: The idea that our physical reality is not merely described by mathematics, but that it is a mathematical structure.
Quantum Reference Frames (QRFs)
: A method of representing neurons as information processors that detect, measure, and respond to their local environment.
Each of these concepts contributes to a broader, interdisciplinary understanding that spans physics, biology, and information theory, and could potentially offer novel insights into the origins of life, biological development, artificial life, consciousness, and the fundamental nature of reality.
1 / 1
I don't think I understand the mathematical universe hypothesis
1 / 1
ChatGPT
ChatGPT
The Mathematical Universe Hypothesis (MUH) is a philosophical proposal put forth by cosmologist Max Tegmark. The hypothesis suggests that the universe and everything in it, including all physical events, structures, and entities, are not just described by mathematics, but are fundamentally mathematical in nature.
In other words, according to the MUH, mathematics doesn't just describe reality ��� it is reality. All structures that exist mathematically also exist physically, and this is true for all conceivable structures.
Tegmark's hypothesis is divided into four levels:
External Reality Hypothesis (ERH)
: The idea that a reality exists independent of human observers.
Theory of Everything (TOE)
: This assumes that all physical phenomena, including consciousness, have a unified theoretical basis that is fundamentally mathematical.
Mathematical Universe Hypothesis (MUH)
: The proposal that our universe is one of many in a multiverse, and that all universes in this multiverse correspond to different mathematical structures.
Ultimate Ensemble
: The notion that the multiverse encompasses every mathematical structure, thus incorporating every possible universe.
It's important to note that while this hypothesis is thought-provoking, it remains controversial and is not widely accepted in the scientific community. The primary criticism is that it is not empirically testable, at least not with our current understanding and technological capabilities, and thus it might be more of a philosophical or metaphysical position than a scientific one.
1 / 1
Ok I accept 1, but two seems a bit speculative and having a theoretical basis that is mathematical does not necessarily imply that the universe *is* mathematical, I'm not sure exactly what that implies. 3 seems unlikely to me, because it seems to be based on an assumption that all actions in the universe fork reality, with the counterfactual world existing in some other world, when the history of the universe seems to be a continuous curve in its relative configuration space. 4 is just ludicrous, there is no way of knowing what parameters vary across vast (trans particle horizon distances) the universe may be constrained by parameters which disallow  certain configurations. Stars may not form before a certain period for example, there is no observed phenomena that implies that all mathematical possible laws of physics have to be tried somewhere. I think possibility space allowed by mathematics and geometry is far larger than the ones actually that are physically possible.
1 / 1
ChatGPT
ChatGPT
You're raising several thoughtful points. Let's address each of them:
Theory of Everything (TOE)
: You're correct that a theoretical mathematical basis doesn't necessarily imply that the universe itself is mathematical in nature. The main idea here is not that a mathematical description proves the universe is itself mathematical, but rather that if a unified mathematical model can be found to describe all physical phenomena, it would suggest that the universe's behavior might fundamentally be mathematical in nature.
Mathematical Universe Hypothesis (MUH)
: This indeed rests on the assumption of a multiverse, which is controversial and speculative. The idea of a continuous curve in the universe's configuration space is an alternate viewpoint that does not assume a multiverse. These are two different interpretations and currently, there is no decisive empirical evidence supporting one over the other.
Ultimate Ensemble
: Tegmark's fourth level is indeed the most speculative, and many find it hard to accept. It does seem to assume a kind of "maximalism", where all mathematical structures must be realized physically somewhere in the multiverse. But as you point out, there may well be constraints or principles at work that limit what's physically possible.
Many physicists share your skepticism and view the MUH more as an interesting philosophical perspective than a scientifically testable hypothesis. It's a thought-provoking idea that raises intriguing questions about the nature of reality and our understanding of it, but it's crucial to bear in mind the distinction between speculation and established scientific theory.
1 / 1
I can imagine mathematical structures like boltzman brains out of stars or galaxies, but they might take trillions of years to form and it is possible that no part of the universe has had sufficient expansion time.  Of course gasbrain boltzman brains might be more likely, but still be constrained by inflation or heat death to not be physically  before or after a certain point. It is possible that randomness prevents highly complicated structures from surviving long enough in a has cloud to achieve any kind of homeostasis, so it seems perfectly possible to have a universe with no boltzman brains.
1 / 1
ChatGPT
ChatGPT
You're right in noting that the likelihood of complex structures like Boltzmann brains forming spontaneously due to random fluctuations depends greatly on specific conditions and circumstances within the universe.
A Boltzmann brain is a hypothetical self-aware entity that arises due to random fluctuations out of a state of chaos. It's named after the physicist Ludwig Boltzmann (1844���1906), who advanced an early theory about the relationships between entropy, probability, and the nature of physical states.
For a Boltzmann brain to form, the exact configuration of particles that would create such a structure would have to occur spontaneously, which would be exceedingly unlikely due to the sheer number of particles and the specific configuration required. Even in an infinitely old or large universe, the probability of such a fluctuation occurring within any given volume of space or period of time could be extremely small, possibly so small that it would never happen.
Moreover, conditions in the universe might indeed make the formation of such structures unlikely or impossible. For example, the universe's ongoing expansion, resulting in the dispersal of matter and energy, could prevent the formation of complex structures. The potential heat death of the universe, in which all usable energy is expended and everything approaches a state of thermodynamic equilibrium, could also preclude the formation of Boltzmann brains.
Therefore, while it's a fascinating concept in the context of statistical mechanics and cosmology, the actual existence of Boltzmann brains remains purely theoretical and highly speculative. The constraints you mention (inflation, heat death, or simply the extreme unlikelihood of the necessary conditions arising) can indeed make a universe with no Boltzmann brains perfectly possible.
1 / 1
Anyways, I don't know why I have an opinion on the matter but a multiverse seem like they multiply entities unnecessarily, if there was a multiverse that wasn't detectable is even worse.
1 / 1
ChatGPT
ChatGPT
Your skepticism about the multiverse is shared by many, and there are indeed serious debates about it in the scientific community.
The principle of parsimony, often called Occam's Razor, states that one should not multiply entities beyond necessity. This principle encourages us to choose the simplest explanation that fits the evidence.
The multiverse is a hypothesis that invokes an extremely large, perhaps infinite, number of universes to explain observations about our own universe. Critics argue that it's an overly complex explanation, making it less preferable according to Occam's Razor. If there's a simpler, single-universe model that can explain our observations just as well, then according to this principle, we should prefer that model.
Furthermore, the multiverse, by its very nature, may not be empirically testable. If we can't interact with or observe these other universes, then we can't gather evidence about them. In this sense, the multiverse theory may be more of a metaphysical than a scientific hypothesis, since it might not be falsifiable. This is another reason why some scientists and philosophers are skeptical about the multiverse.
However, it's also worth noting that there are several different multiverse hypotheses, and some of them do make testable predictions. For example, the "Many-Worlds" interpretation of quantum mechanics suggests that all possible quantum events occur in some universe, and this has implications for what we should expect to see in our own universe.
In summary, while the idea of a multiverse is an intriguing one that can potentially explain some puzzling features of our universe, it's also a highly controversial idea that goes beyond our current ability to test or confirm. As such, it's entirely reasonable to be skeptical about it.
1 / 1
Yeah what is a one world interpretation  called?
1 / 1
ChatGPT
ChatGPT
A "one-world" interpretation of quantum mechanics is often referred to as the "Copenhagen interpretation." The Copenhagen interpretation, developed by Niels Bohr and Werner Heisenberg, is one of the earliest and most commonly taught interpretations of quantum mechanics.
According to the Copenhagen interpretation, the act of measurement causes the wave function (which describes the state of a quantum system) to "collapse" into a definite state. Prior to measurement, a quantum system is in a superposition of multiple states, but once a measurement is made, the system is found in only one of these states. In other words, reality, at the quantum level, only comes into being when a measurement is made.
This is contrasted with the "Many-Worlds" interpretation, which suggests that all possible outcomes of a quantum measurement actually occur, each in a separate, parallel universe. In this interpretation, the wave function never collapses - instead, the universe itself "splits" into multiple branches.
There are also other interpretations of quantum mechanics, such as the de Broglie-Bohm "pilot-wave" theory, and the "quantum Bayesianism" or "QBism" interpretation, each with their own unique approach to the puzzling aspects of quantum mechanics.
1 / 1
What is the everettian no-collapse theory?
1 / 1
ChatGPT
ChatGPT
The Everettian no-collapse theory, also known as the Many-Worlds Interpretation (MWI), is an interpretation of quantum mechanics that was first proposed by Hugh Everett III in 1957.
The MWI rejects the idea of wavefunction collapse, a concept prevalent in the traditional Copenhagen interpretation of quantum mechanics. In the Copenhagen interpretation, a quantum system exists in a superposition of states until it is measured, at which point the superposition collapses to a single state.
The MWI, however, posits that all possible outcomes of quantum measurements are physically realized in some "world" or universe. When a measurement is made on a quantum system, rather than the wavefunction collapsing, the universe is supposed to "split" or "branch" into a set of mutually unobservable but equally real universes, each one corresponding to a different outcome of the measurement.
In this theory, every quantum event is a branching point and results in the creation of parallel universes. So, rather than a single, unique reality, there is a vast and constantly proliferating network of diverging, parallel realities.
Everett's interpretation is considered quite radical, as it implies an infinite number of universes. Despite being controversial, the MWI has been gaining popularity among physicists and philosophers due to its ability to explain quantum phenomena without introducing additional axioms like "wave function collapse", and it provides a clear mechanism for quantum decoherence.
Please note that while the MWI is a well-known interpretation, it is just one of many, and there is currently no consensus among physicists about which interpretation is the "correct" one. Interpretations of quantum mechanics are designed to make sense of the mathematical formalism and the empirical observations, and different interpretations often simply offer different philosophical perspectives on what the math and the experiments are saying about the nature of reality. As of my knowledge cut-off in 2021, the nature of quantum reality remains an open and much-debated question in physics.
1 / 1
I think I much prefer quantum Bayesianism.
1 / 1
ChatGPT
ChatGPT
Quantum Bayesianism, also known as QBism, is another interpretation of quantum mechanics that integrates Bayesian probability theory with the formalism of quantum mechanics. The central idea in QBism is that the wavefunction or quantum state doesn't represent an objective reality. Instead, it represents the subjective beliefs and expectations of the observer about the possible outcomes of measurements.
In QBism, the act of measurement is viewed as an interaction between the observer and the system, causing an update of the observer's beliefs, much in the way a Bayesian update of a probability distribution works.
Some key points of QBism include:
Subjectivity: The quantum state is not a property of an individual system, nor does it provide an objective description of a quantum system. Instead, it is a tool used by an agent to place bets on the outcomes of experiments. It is a representation of the observer's beliefs about the world, not a description of the world itself.
Quantum probabilities represent beliefs: In QBism, the probabilities that appear in the quantum formalism are not objective chances but are instead degrees of belief. Quantum probabilities are used by an agent to quantify their uncertainty about the outcome of a future measurement.
Measurement outcomes are personal to agents: A measurement outcome becomes a fact only once it is registered by a conscious observer. Different agents may have different information, and hence different views on what is and isn't a fact.
QBism seeks to resolve some of the paradoxes and controversies of quantum mechanics by removing the objective reality from quantum states and instead emphasizing the role of the observer. This interpretation, while removing some issues, introduces others, such as the nature and role of the "observer". However, it is another active area of research and discussion in the foundations of quantum mechanics.
1 / 1
2.3 Is QBist Quantum Theory Explanatory?
Realists often appeal to scientific explanation when arguing against instrumentalists. Quantum theory is generally acknowledged to provide us with a wide variety of successful explanations of phenomena we can���t explain without it. Timpson (followed by Brown 2019) objects that QBists cannot account for its explanatory success.

��� think of the question of why some solids conduct and some insulate; why yet others are in between, while they all contain electrons, sometimes in quite similar densities���. Ultimately we are not interested in agents��� expectation that matter structured like sodium would conduct; we are interested in why it in fact does so. (Timpson 2008: 600)

QBists face two problems here. In their view a user of quantum theory can���t appeal to a description of objective, physical quantum states in explaining the phenomena; and quantum theory���s Born rule outputs subjective probabilities for each user independently that bear not on what is objectively likely to happen but only on what (s)he should expect to experience, given her prior beliefs and experiences.

Fuchs and Schack (2015) reply that explanations offered by quantum theory have a similar character to explanations offered by probability theory and give examples. This does not address the first problem. But QBists could rationalize biting that bullet by pointing to long-standing problems of measurement and non-locality faced by interpretations that take quantum states to be physically real that don���t arise in their view. To respond to the second problem they could try to develop a subjectivist view of scientific explanation as ultimately a matter of making an economical and effective unity out of all an agent���s beliefs and expectations.

2.4 Is the Born Rule a New Bayesian Norm?
Bacciagaluppi (2014) has raised an objection against the claim that the Born rule as formulated in Equation 
(
2
)
 states an empirically motivated normative addition to Bayesian coherence conditions. His basic objection is that QBism assumes the probability 
q
(
j
)
 of an actual measurement outcome (as also the probability 
p
(
j
)
 of a hypothetical measurement outcome) is independent of the procedure by which this measurement is performed. That this is so follows from the usual formulation of the Born Rule relating Born probabilities of measurement outcomes to quantum state assignments. But QBism cannot justify the procedure-independence of 
q
(
j
)
 and 
p
(
j
)
 in this way because it considers the Born Rule in the form of Equation 
(
2
)
 to be primitive, and so incapable of empirical support from the relation between quantum states and outcomes of laboratory procedures.

There are also technical problems with Equation 
(
2
)
, which assumes the existence of SICs in the relevant Hilbert space. But infinite as well as finite-dimensional Hilbert spaces are used in quantum theory, and SICs have not (yet) been shown to exist in every finite dimension.[4] Informationally-complete (but not necessarily symmetric) POVMs do exist in all finite dimensional spaces. Fuchs and Schack (2015) give a schematic alternative to Equation 
(
2
)
 that does not require symmetry of an informationally-complete POVM representing a hypothetical fiducial measurement.

2.5 Is QBism too Subjective?
The QBist approach to quantum theory is often criticized as too subjective in its treatment of quantum states, measurement outcomes, and probabilities.

Many people assume a wave-function or state vector represents a physical quantum state. On this assumption a quantum state is ontic���a fundamental element of reality obeying the quantum dynamics that underlies classical dynamical laws. Bacciagaluppi (2014) urges QBists to accept this approach to dynamics even while maintaining a subjectivist or pragmatist interpretation of probability. But doing so would undercut the QBist account of discontinuous change of quantum state on measurement as simply corresponding to epistemic updating.

Most people take it for granted that a competently performed quantum measurement procedure has a unique, objective outcome. QBists deny this, assimilating a measurement outcome to an agent���s personal experience���including her experience of another agent���s verbal report of his outcome. QBists take a measurement outcome to be personal to the agent whose action elicited it. This tenet is key both to QBist denial that quantum phenomena involve any nonlocal influence (Fuchs, Mermin and Schack 2014) and to the QBist (DeBrota, Fuchs and Schack 2020) resolution of the paradox of Wigner���s friend (see the entry on Everett���s relative-state formulation of quantum mechanics). But their notions of experience and agency are broad enough to encompass personal experiences of agents other than individual, conscious humans.

By rejecting the objective authority of observation reports QBists challenge what many have considered a presupposition of the scientific method. This rejection also threatens to undercut the standard personalist argument (see entry on Bayesian epistemology, ��6.2.F) that the opinions of agents with very different prior degrees of belief will converge after they have accumulated sufficient common evidence.

QBists consider a subjective view of quantum probability a core commitment of the view, even when that probability is 1 (Caves, Fuchs and Schack 2007). But Stairs (2011) and others have argued that QBist strategies for resolving conceptual problems associated with non-locality may be co-opted by a qualified objectivist about quantum probabilities. https://plato.stanford.edu/entries/quantum-bayesian/
1 / 1
ChatGPT
ChatGPT
You've cited an excellent section from the Stanford Encyclopedia of Philosophy on Quantum Bayesianism (QBism). Here's a summary and some clarifications:
2.3 Is QBist Quantum Theory Explanatory?
Realists often argue that QBism cannot account for quantum theory's explanatory success because it doesn't appeal to a description of objective, physical quantum states in explaining the phenomena. However, QBists argue that quantum theory offers explanations similar to those offered by probability theory. To address the criticism, they suggest developing a subjectivist view of scientific explanation, which integrates an agent's beliefs and expectations.
2.4 Is the Born Rule a New Bayesian Norm?
The Born rule, as interpreted by QBists, is argued to be an empirically motivated normative addition to Bayesian coherence conditions. But some argue QBism cannot justify the Born rule as it assumes the probability of a measurement outcome is independent of the procedure by which the measurement is performed.
2.5 Is QBism too Subjective?
Critics suggest that QBism is overly subjective in its treatment of quantum states, measurement outcomes, and probabilities. They argue that it rejects the objective authority of observation reports, which many consider a prerequisite for the scientific method. This rejection also threatens to undercut the standard personalist argument for the convergence of opinions among agents with different initial beliefs after accumulating sufficient common evidence.
However, QBists consider a subjective view of quantum probability to be a core commitment of their view. They extend the notion of agency and experience to include more than individual, conscious humans, thereby accommodating diverse agents' personal experiences.
It's important to note that QBism remains a contentious topic within the philosophy of quantum mechanics and is a subject of ongoing discussion and research. It seeks to reconcile quantum mechanics' strange features with a coherent philosophical framework but faces its own challenges and criticisms. As with all interpretations of quantum mechanics, its value lies in the novel perspectives it offers on the issues inherent to quantum theory.
1 / 1
Measurements are actions
Just like "observer", the term "measurement" can be misleading because it suggests a pre-existing property that is revealed by the measurement. Instead, a measurement should be thought of as an action an agent takes to elicit a response from the world. A measurement is an act of creation that brings something entirely new into the world, an outcome that is shared between the agent and the agent's external world.

Quantum mechanics is often depicted as "weird" and hard, or indeed impossible, to understand. As a matter of fact, the weirdness of quantum mechanics is an artifact of looking at it the wrong way. Once the two main QBist insights���that the quantum rules are guides to action and that measurements do not reveal pre-existing properties���are taken on board, all quantum paradoxes disappear.

Take Schr��dinger's cat, for example. In the usual formulation, the unfortunate animal is described by a "quantum state" taken to be a part of reality and implying that the cat is neither dead nor alive.

The QBist, by contrast, does not regard the quantum state as a part of reality. The quantum state a QBist agent might assign has no bearing on whether the cat is alive or dead. All it expresses is the agent's expectations concerning the consequences of possible actions they might take on the cat. Unlike most interpretations of quantum mechanics, QBism respects the fundamental autonomy of the cat.

Or take quantum teleportation. According to a common way of presenting this operation, a particle's quantum state, again regarded as a part of reality, disappears at one place (A) and mysteriously reappears at another (B)���quite literally as in a transporter in the Star Trek science fiction series.

For a QBist, however, nothing real is transported from A to B. All that happens in quantum teleportation is that an agent's belief about the particle at A becomes, after the operation, the same agent's belief about a particle at B. The quantum state that expresses the agent's belief about the particle at A initially is mathematically identical to the quantum state that expresses that same agent's belief about the particle at B after the operation. Quantum teleportation is a powerful tool used in applications such as quantum computing, but in QBism there is nothing counter-intuitive or weird about it.

QBism is an ongoing project. It spells out clearly the meaning of all mathematical objects in the theory and is thus a fully developed interpretation of quantum mechanics. Yet, QBism is also a program for developing new physics and has already yielded deep insights even if it is still a work in progress.

QBism has also led to a fruitful dialogue with the kindred philosophical schools of thought of pragmatism and phenomenology. Its vision of the world is one in which agents possess genuine freedom and respect each other's autonomy. I like to think that this is what quantum mechanics has been trying to tell us about reality all along.

https://theconversation.com/qbism-quantum-mechanics-is-not-a-description-of-objective-reality-it-reveals-a-world-of-genuine-free-will-200487
1 / 1
ChatGPT
ChatGPT
Thank you for sharing this passage from The Conversation, which provides a clear and relatable overview of Quantum Bayesianism (QBism).
The passage does a great job of describing how QBism reframes our understanding of quantum mechanics, stressing that measurements are actions and the quantum state is not a description of objective reality, but rather an agent's expectations about potential actions' outcomes. This perspective allows us to view phenomena such as Schr��dinger's cat and quantum teleportation differently, eliminating the "weirdness" often associated with quantum mechanics.
As the passage emphasizes, QBism is an ongoing project, a new perspective on the interpretation of quantum mechanics that also holds potential for generating new physics. It leads to a more pragmatic and phenomenological understanding of the world, suggesting a universe where agents possess true freedom and each other's autonomy is respected. It's indeed a fascinating way of looking at quantum phenomena, even if it challenges some conventional wisdom about the nature of reality.
Just remember, while QBism offers a promising approach to interpreting quantum mechanics, it's not the only one and is still a subject of ongoing debate among physicists and philosophers. As with all theories in science, our understanding continues to evolve over time as new information and perspectives come to light.
1 / 1
Another list of the topics we have discussed
1 / 1
ChatGPT
ChatGPT
Sure, here's an updated list of the topics we've discussed:
The research paper by Egbert and Barandiaran (2023): A detailed discussion of the ideas presented in the research paper, focusing on concepts such as regulative development, ab initio self-organization, the application of the Free Energy Principle (FEP), the role of the environment in self-organization, the decoupling of replication from self-organization, and an experimental strategy for studying these theories.
Pancomputationalism: A philosophical view that the universe is a computational machine, or similar to a computational process.
Digital physics: A collection of theoretical perspectives based on the premise that the universe is describable by information and computation.
Mathematical universe hypothesis: A speculative theory suggesting that the universe is a mathematical structure, and if it exists, it must have a mathematical structure.
Boltzmann brains: A concept in cosmology referring to a self-aware entity which arises due to random fluctuations out of a state of chaos.
Multiverse theories: We discussed skepticism towards multiverse theories and the multiplication of entities they seem to require.
Everettian no-collapse theory: This theory suggests that all possible outcomes of quantum measurements are physically realized in some "world" or universe within a larger multiverse.
Quantum Bayesianism (QBism): A particular interpretation of quantum mechanics that applies Bayesian probability theory to the quantum state.
These discussions covered a wide range of topics from quantum mechanics, the philosophy of science, computational theory, cosmology, and the origins of life.
1 / 1
Have you heard of this? Chapter 1 
Introduction 
1.1Basic Ideas of TGD 
The basic physical picture behind TGD was formed as a fusion of two rather disparate approaches: 
namely TGD is as a Poincare invariant theory of gravitation and TGD as a generalization of the 
old-fashioned string model. 
1.1.1Background 
T(opological) G(eometro)D(ynamics) is one of the many attempts to i nd a unif i ed description of basic 
interactions. The development of the basic ideas of TGD to a relatively stable form took time of about 
half decade [K1]. The great challenge is to construct a mathematical theory around these physically 
very attractive ideas and I have devoted the last twenty-three years for the realization of this dream 
and this has resulted in seven online books about TGD and eight online books about TGD inspired 
theory of consciousness and of quantum biology. 
Quantum T(opological) G(eometro)D(ynamics) as a classical spinor geometry for inf i nite-dimensional 
conf i guration space, p-adic numbers and quantum TGD, and TGD inspired theory of consciousness 
and of quantum biology have been for last decade of the second millenium the basic three strongly 
interacting threads in the tapestry of quantum TGD. 
For few years ago the discussions with Tony Smith initiated a fourth thread which deserves the 
name ���TGD as a generalized number theory���. The basic observation was that classical number i elds 
might allow a deeper formulation of quantum TGD. The work with Riemann hypothesis made time 
ripe for realization that the notion of inf i nite primes could provide, not only a reformulation, but a 
deep generalization of quantum TGD. This led to a thorough and extremely fruitful revision of the 
basic views about what the i nal form and physical content of quantum TGD might be. Together with 
the vision about the fusion of p-adic and real physics to a larger coherent structure these sub-threads 
fused to the ���physics as generalized number theory��� th 
A further thread emerged from the realization that by quantum classical correspondence TGD 
predicts an inf i nite hierarchy of macroscopic quantum systems with increasing sizes, that it is not at 
all clear whether standard quantum mechanics can accommodate this hierarchy, and that a dynam- 
ical quantized Planck constant might be necessary and certainly possible in TGD framework. The 
identif i cation of hierarchy of Planck constants whose values TGD ���predicts��� in terms of dark matter 
hierarchy would be natural. This also led to a solution of a long standing puzzle: what is the proper 
interpretation of the predicted fractal hierarchy of long ranged classical electro-weak and color gauge 
i elds. Quantum classical correspondences allows only single answer: there is inf i nite hierarchy of p- 
adically scaled up variants of standard model physics and for each of them also dark hierarchy. Thus 
TGD Universe would be fractal in very abstract and deep sense. 
Every updating of the books makes me frustrated as I see how badly the structure of the repre- 
sentation ref l ects my bird���s eye of view as it is at the moment of updating. At this time I realized 
that the chronology based identif i cation of the threads is quite natural but not logical and it is much 
more logical to see p-adic physics, the ideas related to classical number i elds, and inf i nite primes 
as sub-threads of a thread which might be called ���physics as a generalized number theory���. In the 
1 
2Chapter 1. Introduction 
following I adopt this view. This reduces the number of threads to four! I am not even sure about 
the number of threads! Be patient! 
TGD forces the generalization of physics to a quantum theory of consciousness, and represent 
TGD as a generalized number theory vision leads naturally to the emergence of p-adic physics as 
physics of cognitive representations. The seven online books [K88, K67, K54, K48, K68, K77, K76] 
about TGD and eight online books about TGD inspired theory of consciousness and of quantum 
biology [K81, K9, K60, K7, K30, K38, K42, K75] are warmly recommended to the interested reader. 
1.1.2TGD as a Poincare invariant theory of gravitation 
The i rst approach was born as an attempt to construct a Poincare invariant theory of gravitation. 
Space-time, rather than being an abstract manifold endowed with a pseudo-Riemannian structure, is 
regarded as a surface in the 8-dimensional space H = M4 ��CP2, 
where M 4 denotes Minkowski space and 
CP2= SU(3)/U(2) is the complex projective space of two complex dimensions [A25, A7, A17, A5]. 
The identif i cation of the space-time as a submanifold [A4, A22] of M4�� CP2leads 
to an ex- 
act Poincare invariance and solves the conceptual dif i culties related to the def i nition of the energy- 
momentum in General Relativity. 
It soon however turned out that submanifold geometry, being considerably richer in structure 
than the abstract manifold geometry, leads to a geometrization of all basic interactions. First, the 
geometrization of the elementary particle quantum numbers is achieved. The geometry of CP2explains 
electro-weak and color quantum numbers. The dif f erent H-chiralities of H-spinors correspond to the 
conserved baryon and lepton numbers. Secondly, the geometrization of the i eld concept results. The 
projections of the CP2spinor connection, Killing vector i elds of CP2and of H-metric to four-surface 
def i ne classical electro-weak, color gauge i elds and metric in X4. 
1.1.3TGD as a generalization of the hadronic string model 
The second approach was based on the generalization of the mesonic string model describing mesons 
as strings with quarks attached to the ends of the string.In the 3-dimensional generalization 3- 
surfaces correspond to free particles and the boundaries of the 3- surface correspond to partons in 
the sense that the quantum numbers of the elementary particles reside on the boundaries. Various 
boundary topologies (number of handles) correspond to various fermion families so that one obtains 
an explanation for the known elementary particle quantum numbers. This approach leads also to a
1 / 1
The previous model used in this conversation is unavailable. We've switched you to the latest default model.
ChatGPT
ChatGPT
Based on the provided text, here are the topics discussed:
Basic Ideas of TGD (Topological GeometroDynamics): An overview of TGD as a Poincare invariant theory of gravitation and a generalization of the old-fashioned string model.
Background of TGD: The historical development of TGD's basic ideas and the challenges faced in constructing a mathematical theory around them.
Threads of TGD: Different aspects of TGD, including Quantum TGD as a classical spinor geometry, p-adic numbers, TGD-inspired theory of consciousness, and quantum biology.
TGD as a Generalized Number Theory: The realization that classical number fields and the notion of infinite primes provide a deeper formulation and generalization of quantum TGD.
Fractal Nature of TGD Universe: The hierarchy of macroscopic quantum systems with increasing sizes and the prediction of an infinite hierarchy of p-adically scaled up variants of the standard model physics and dark hierarchy.
TGD as a Poincare Invariant Theory of Gravitation: TGD's approach to construct a Poincare invariant theory of gravitation, with space-time regarded as a surface in 8-dimensional space.
TGD as a Generalization of the Hadronic String Model: Generalizing the mesonic string model to describe particles as strings with quarks attached to the ends and 3-dimensional generalization with 3-surfaces corresponding to free particles and boundaries corresponding to partons.
It seems that the text provides an introduction to the basic ideas and background of TGD, as well as its connections to various mathematical and physical theories.
1 / 1
Euclidian regions would give at wormhole throats the same contribution apart from coef i cients 
and in Minkowskian regions the ���g 4factor would be imaginary so that one would obtain sum of 
real term identif i able as K�� ahler function and imaginary term identif i able as the ordinary action 
giving rise to interference ef f ects and stationary phase approximation central in both classical 
and quantum i eld theory. Imaginary contribution- the presence of which I realized only after 
33 years of TGD- could also havetopological interpretation as a Morse function. On physical 
side the emergence of Euclidian space-time regions is something completely new and leads to a 
dramatic modif i cation of the ideas about black hole interior. 
2. The manner to achieve the reduction to Chern-Simons terms is simple. The vanishing of Coulom- 
bic contribution to K�� ahler action is required and is true for all known extremals if one makes a 
general ansatz about the form of classical conserved currents. The so called weak form of electric- 
magnetic duality def i nes a boundary condition reducing the resulting 3-D terms to Chern-Simons 
terms. In this manner almost topological QFT results. But only ���almost��� since the Lagrange 
multiplier term forcing electric-magnetic duality implies that Chern-Simons action for preferred 
extremals depends on metric. 
3. A further quite recent hypothesis inspired by ef f ective 2-dimensionality is that Chern-Simons 
terms reduce to a sum of two 2-dimensional terms. An imaginary term proportional to the total 
area of Minkowskian string world sheets and a real tem proportional to the total area of partonic 
2-surfaces or equivalently strings world sheets in Euclidian space-time regions. Also the equality 
of the total areas of strings world sheets and partonic 2-surfaces is highly suggestive and would 
realize a duality between these two kinds of objects. String world sheets indeed emerge naturally 
for the proposed ansatz def i ning preferred extremals. Therefore K�� ahler action would have very 
stringy character apart from ef f ects due to the failure of the strict determinism meaning that 
radiative corrections break the ef f ective 2-dimensionality. 
1.2.2TGD as a generalized number theory 
Quantum T(opological)D(ynamics) as a classical spinor geometry for inf i nite-dimensional conf i gu- 
ration space, p-adic numbers and quantum TGD, and TGD inspired theory of consciousness, have 
been for last ten years the basic three strongly interacting threads in the tapestry of quantum TGD. 
The fourth thread deserves the name ���TGD as a generalized number theory���. It involves three sep- 
arate threads: the fusion of real and various p-adic physics to a single coherent whole by requiring 
number theoretic universality discussed already, the formulation of quantum TGD in terms of hyper- 
counterparts of classical number i elds identif i ed as sub-spaces of complexif i ed classical number i elds 
with Minkowskian signature of the metric def i ned by the complexif i ed inner product, and the notion 
of inf i nite prime. 
p-Adic TGD and fusion of real and p-adic physics to single coherent whole 
The p-adic thread emerged for roughly ten years ago as a dim hunch that p-adic numbers might be 
important for TGD. Experimentation with p-adic numbers led to the notion of canonical identif i cation 
mapping reals to p-adics and vice versa. The breakthrough came with the successful p-adic mass 
calculations using p-adic thermodynamics for Super-Virasoro representations with the super-Kac- 
Moody algebra associated with a Lie-group containing standard model gauge group. Although the 
details of the calculations have varied from year to year, it was clear that p-adic physics reduces not 
only the ratio of proton and Planck mass, the great mystery number of physics, but all elementary 
particle mass scales, to number theory if one assumes that primes near prime powers of two are in a 
physically favored position. Why this is the case, became one of the key puzzless and led to a number 
6Chapter 1. Introduction 
of arguments with a common gist: evolution is present already at the elementary particle level and 
the primes allowed by the p-adic length scale hypothesis are the i ttest ones. 
It became very soon clear that p-adic topology is not something emerging in Planck length scale 
as often believed, but that there is an inf i nite hierarchy of p-adic physics characterized by p-adic 
length scales varying to even cosmological length scales. The idea about the connection of p-adics 
with cognition motivated already the i rst attempts to understand the role of the p-adics and inspired 
���Universe as Computer��� vision but time was not ripe to develop this idea to anything concrete (p-adic 
numbers are however in a central role in TGD inspired theory of consciousness). It became however 
obvious that the p-adic length scale hierarchy somehow corresponds to a hierarchy of intelligences and 
that p-adic prime serves as a kind of intelligence quotient. Ironically, the almost obvious idea about 
p-adic regions as cognitive regions of space-time providing cognitive representations for real regions 
had to wait for almost a decade for the access into my consciousness. 
There were many interpretational and technical questions crying for a def i nite answer. 
1. What is the relationship of p-adic non-determinism to the classical non-determinism of the 
basic i eld equations of TGD? Are the p-adic space-time region genuinely p-adic or does p-adic 
topology only serve as an ef f ective topology? If p-adic physics is direct image of real physics, 
how the mapping relating them is constructed so that it respects various symmetries? Is the 
basic physics p-adic or real (also real TGD seems to be free of divergences) or both? If it is both, 
how should one glue the physics in dif f erent number i eld together to get The Physics? Should 
one perform p-adicization also at the level of the conf i guration space of 3-surfaces? Certainly 
the p-adicization at the level of super-conformal representation is necessary for the p-adic mass 
calculations. 
2. Perhaps the most basic and most irritating technical problem was how to precisely def i ne p-adic 
def i nite integral which is a crucial element of any variational principle based formulation of the 
i eld equations. Here the frustration was not due to the lack of solution but due to the too large 
number of solutions to the problem, a clear symptom for the sad fact that clever inventions 
rather than real discoveries might be in question. Quite recently I however learned that the 
problem of making sense about p-adic integration has been for decades central problem in the 
frontier of mathematics and a lot of profound work has been done along same intuitive lines 
as I have proceeded in TGD framework. The basic idea is certainly the notion of algebraic 
continuation from the world of rationals belonging to the intersection of real world and various 
p-adic worlds. 
Despite these frustrating uncertainties, the number of the applications of the poorly def i ned p-adic 
physics growed steadily and the applications turned out to be relatively stable so that it was clear 
that the solution to these problems must exist. It became only gradually clear that the
1 / 1
ChatGPT
ChatGPT
Based on the provided text, here are the topics discussed:
Euclidean and Minkowskian Regions: The contributions and characteristics of Euclidean and Minkowskian regions in TGD, and their implications for wormhole throats and black hole interior.
Reduction to Chern-Simons Terms: The method to achieve the reduction of terms in TGD to Chern-Simons terms and the implications for quantum field theory.
TGD as a Generalized Number Theory: The fusion of real and p-adic physics to form a single coherent whole, the formulation of quantum TGD in terms of hypercounterparts of classical number fields, and the notion of infinite primes.
p-Adic TGD and Fusion of Real and p-adic Physics: The emergence and importance of p-adic physics in TGD, and the hierarchy of p-adic physics from Planck length scales to cosmological scales.
Relationship Between p-adic and Classical Physics: The relationship between p-adic non-determinism and classical non-determinism in TGD, and the challenges in defining p-adic integration.
Applications of p-adic Physics: The growing number of applications of p-adic physics in TGD and its significance in various areas.
The text provides an overview of the development and implications of TGD as a unified theory of interactions, incorporating ideas from different areas such as Poincare invariant theory of gravitation, string models, and p-adic physics.
1 / 1
7Quantum Model of Memory343 
7.1Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .343 
7.1.1Geometric and subjective memories. . . . . . . . . . . . . . . . . . . . . . . .343 
7.1.2p-Adic physics as physics of intentionality . . . . . . . . . . . . . . . . . . . . .343 
7.1.3Spin glass model of memories . . . . . . . . . . . . . . . . . . . . . . . . . . . .343 
7.1.4Mirror mechanism . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .344 
7.1.5Third person aspects of memory. . . . . . . . . . . . . . . . . . . . . . . . . .344 
7.1.6Symbolic and cognitive representations of memories. . . . . . . . . . . . . . .345 
7.1.7Biosupercomputers and memories . . . . . . . . . . . . . . . . . . . . . . . . . .345 
7.2Dif f erent types of memories . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .346 
7.2.1Geometric and subjective memories. . . . . . . . . . . . . . . . . . . . . . . .346 
7.2.2Habits, skills, associations . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .349 
7.2.3Spin glass model of learning and long term memories . . . . . . . . . . . . . . .351 
7.2.4Long term memories . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .352 
7.2.5Implicit memories. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .356 
7.2.6Procedural memories . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .357 
7.3Model for long term memories . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .357 
7.3.1General ideas . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .358 
7.3.2Could gravitation have something to do with long term memories? . . . . . . .361 
7.3.3Is the right brain hemisphere the quantum entangler?. . . . . . . . . . . . . .363 
7.3.4Going to the neuronal level . . . . . . . . . . . . . . . . . . . . . . . . . . . . .365 
7.3.5Hippocampus and long term memories . . . . . . . . . . . . . . . . . . . . . . .368 
7.3.6Microtubuli and long term memory . . . . . . . . . . . . . . . . . . . . . . . . .372 
7.4Hyper-f i nite factors of type II1, dark matter hierarchy, and long term memories . . . .376 
7.4.1Hyper-f i nite factors of type II1and quantization of Planck constant. . . . . .377 
7.4.2Dark matter hierarchy . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .377 
7.4.3Dark matter hierarchy and the notion of self. . . . . . . . . . . . . . . . . . .379 
7.4.4The time span of long term memories as signature for the level of dark matter 
hierarchy. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .379 
7.4.5Remote metabolism, long term memory, and zero energy ontology. . . . . . .380 
7.4.6Applying computer analogy to the model for long term memories . . . . . . . .382 
7.5A proposal for memory code . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .385 
7.5.1Basic ideas of the model of memorycode . . . . . . . . . . . . . . . . . . . . . .386 
7.5.2TGD view about the situation. . . . . . . . . . . . . . . . . . . . . . . . . . .388 
7.6Updates since 2012 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .390 
7.6.1How memories are represented and recalled?. . . . . . . . . . . . . . . . . . .391 
7.6.2Could interaction free measurement be used to deduce information about self 
model?. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .391 
7.6.3Realization of memory representations in terms of braided l ux tubes . . . . . .394 
8About the Nature of Time397 
8.1Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .397 
8.2The most recent vision about zero energy ontology and p-adicization . . . . . . . . . .399 
8.2.1Zero energy ontology brief l y . . . . . . . . . . . . . . . . . . . . . . . . . . . . .399 
8.2.2WCW spinor i elds . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .400 
8.2.3Def i nition of energy in zero energy ontology . . . . . . . . . . . . . . . . . . . .401 
8.2.4p-Adic variants of the imbedding space. . . . . . . . . . . . . . . . . . . . . .401 
xivCONTENTS 
8.3Zero energy ontology, self hierarchy, and the notion of time. . . . . . . . . . . . . . .403 
8.3.1Space-time and imbedding space correlates for selves . . . . . . . . . . . . . . .403 
8.3.2Why sensory experience is about so short time interval? . . . . . . . . . . . . .403 
8.3.3Arrow of time . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .404 
8.3.4The mechanism of self reference . . . . . . . . . . . . . . . . . . . . . . . . . . .405 
8.3.5Can selves interact and evolve? . . . . . . . . . . . . . . . . . . . . . . . . . . .407 
8.4What arrow of time means at the level of quantum states. . . . . . . . . . . . . . . .407 
8.4.1Arrow of time as an inherent property of zero energy states . . . . . . . . . . .407 
8.4.2Does state function-state preparation sequence correspond to alternating arrow 
of geometric time? . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .408 
8.4.3The arrow of geometric time and the arrow of logical implication . . . . . . . .409 
8.4.4How experienced time and the geometric time of physicist relate to each other?: 
the most recent approach . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .409 
8.4.5Quantum dynamics for the moduli of CDs and the arrow of geometric time . .413 
8.5Time for time . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .416 
8.5.1Flash-lag ef f ect and its modif i cation. . . . . . . . . . . . . . . . . . . . . . . .417 
8.5.2We live in the past: but in what sense?. . . . . . . . . . . . . . . . . . . . . .418 
8.5.3Kublai Khan���s problem and three more surprises . . . . . . . . . . . . . . . . .419 
8.6Updates since 2012 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .421 
8.6.1The roles of sensory perception and motor action in TGD framework . . . . . .421 
8.6.2Trying to understand the relationship between subjective and geometric time .424 
IIIINTELLIGENCE, INFORMATION, AND COGNITION427 
9Conscious Information and Intelligence429 
9.1Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .429 
9.1.1Magnetic body as intentional agent and experiencer. . . . . . . . . . . . . . .429 
9.1.2What is conscious intelligence . . . . . . . . . . . . . . . . . . . . . . . . . . . .430 
9.1.3The meanings of sensory, cognitive, symbolic. . . . . . . . . . . . . . . . . . .433 
9.1.4Topics of the chapter . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .433 
9.2How to def i ne measures for the information content of consciousness? . . . . . . . . . .433 
9.2.1Various kinds of information measures . . . . . . . . . . . . . . . . . . . . . . .436 
9.2.2Information concept at space-time level. . . . . . . . . . . . . . . . . . . . . .437 
9.2.3Information theoretic interpretation of K�� ahler function . . . . . . . . . .
1 / 1
ChatGPT
ChatGPT
The provided text discusses various aspects of memory, the nature of time, and intelligence. Here are the main topics discussed in each section:
Chapter 7: Quantum Model of Memory
Introduction: Introduces the concept of memory and its different types, such as geometric and subjective memories. It also discusses the role of p-adic physics in memory and the mirror mechanism.
Different Types of Memories: Discusses various types of memories, including habits, skills, associations, long-term memories, implicit memories, and procedural memories.
Model for Long-Term Memories: Explores the concept of long-term memories and their potential connection to gravitation and brain structures like the hippocampus and microtubules.
Hyper-finite Factors of Type II1, Dark Matter Hierarchy, and Long-Term Memories: Discusses the role of hyper-finite factors of type II1 and dark matter hierarchy in understanding long-term memories.
A Proposal for Memory Code: Proposes a model for memory codes and their representation.
Updates since 2012: Presents updates and advancements in the understanding of memory representations, interaction-free measurements, and memory codes.
Chapter 8: About the Nature of Time
Introduction: Introduces the concept of time and its relationship with zero-energy ontology and p-adicization.
Zero Energy Ontology, Self Hierarchy, and the Notion of Time: Discusses the role of zero-energy ontology in understanding time, the arrow of time, and the mechanism of self-reference.
What Arrow of Time Means at the Level of Quantum States: Explores the concept of the arrow of time in the context of quantum states and logical implications.
How Experienced Time and Geometric Time of Physicist Relate to Each Other?: Discusses the relationship between experienced time and the geometric time of a physicist.
Quantum Dynamics for the Moduli of CDs and the Arrow of Geometric Time: Examines the dynamics of moduli of causal diamonds and their connection to the arrow of geometric time.
Time for Time: Presents various phenomena related to time perception, including the flash-lag effect and Kublai Khan's problem.
Chapter 9: Conscious Information and Intelligence
Introduction: Introduces the concept of conscious intelligence and the magnetic body as an intentional agent and experiencer.
How to Define Measures for the Information Content of Consciousness?: Discusses different measures for the information content of consciousness.
Information Concept at Space-Time Level: Explores the information concept at the space-time level and its connection to the K��hler function.
Topics of the Chapter: Lists the main topics covered in the chapter.
Overall, these chapters delve into the fascinating topics of memory, time, consciousness, and intelligence, and how they relate to the theoretical framework of TGD (Topological Geometrodynamics).
1 / 1
TGD INSPIRED THEORY OF CONSCIOUSNESS 
Matti Pitk�� anen 
K�� oydenpunojankatu D 11, 10900, Hanko, Finland
1 / 1
ChatGPT
ChatGPT
The TGD (Topological Geometrodynamics) inspired theory of consciousness, developed by Matti Pitk��nen, explores the relationship between consciousness and the fundamental physics of the universe. The theory is based on the idea that consciousness is a fundamental aspect of reality and cannot be reduced to mere neural processes in the brain. Instead, it proposes that consciousness is intricately connected to the structure of space-time and the dynamics of the physical world.
Key aspects and ideas of the TGD inspired theory of consciousness include:
Consciousness as Fundamental
: In this theory, consciousness is considered a fundamental aspect of the universe, akin to space, time, and matter. It is not seen as an emergent property arising from complex neural networks, but rather as an intrinsic aspect of reality.
Geometric Approach
: The theory employs a geometric approach to understanding consciousness, where the structure of space-time and its geometry play a central role in shaping conscious experiences.
Quantum Mechanical Basis
: Quantum mechanics is a foundational element in the theory, and it is believed that the fundamental processes of quantum mechanics are intimately linked to the emergence of consciousness.
Self-Referentiality
: The theory suggests that self-referentiality is a key feature of consciousness. It posits that the ability of a system to represent itself and have self-awareness is crucial in understanding consciousness.
Hierarchy of Consciousness
: The theory proposes a hierarchy of consciousness, where different levels of consciousness exist, from the simplest to the most complex. It suggests that even elementary particles may have a primitive form of consciousness.
Connection to Cosmology
: The TGD inspired theory of consciousness connects consciousness to the cosmological evolution of the universe. It suggests that consciousness has played a role in shaping the structure and evolution of the cosmos.
Interactions between Levels
: The theory explores the interactions between different levels of consciousness, from the microscale of particles to the macroscale of living organisms and the universe as a whole.
Non-locality and Entanglement
: Non-locality and quantum entanglement, which are fundamental features of quantum mechanics, are considered important for understanding how information is processed in conscious systems.
Relationship to Brain and Body
: While the theory sees consciousness as not reducible to brain activity, it acknowledges the crucial role of the brain and the body in providing a physical substrate for consciousness to manifest in the world of classical experiences.
It's important to note that the TGD inspired theory of consciousness is still a work in progress, and like any other theory in the field of consciousness studies, it remains a subject of ongoing research and discussion. As of my last knowledge update in September 2021, this theory might have further evolved or been refined since then.
1 / 1
Modern notions of geometrodynamics
More recently, Christopher Isham, Jeremy Butterfield, and their students have continued to develop quantum geometrodynamics to take account of recent work toward a quantum theory of gravity and further developments in the very extensive mathematical theory of initial value formulations of general relativity. Some of Wheeler's original goals remain important for this work, particularly the hope of laying a solid foundation for quantum gravity. The philosophical program also continues to motivate several prominent contributors.

Topological ideas in the realm of gravity date back to Riemann, Clifford, and Weyl and found a more concrete realization in the wormholes of Wheeler characterized by the Euler-Poincar�� invariant. They result from attaching handles to black holes.

Observationally, Albert Einstein's general relativity (GR) is rather well established for the solar system and double pulsars. However, in GR the metric plays a double role: Measuring distances in spacetime and serving as a gravitational potential for the Christoffel connection. This dichotomy seems to be one of the main obstacles for quantizing gravity. Arthur Stanley Eddington suggested already in 1924 in his book The Mathematical Theory of Relativity (2nd Edition) to regard the connection as the basic field and the metric merely as a derived concept.

Consequently, the primordial action in four dimensions should be constructed from a metric-free topological action such as the Pontryagin invariant of the corresponding gauge connection. Similarly as in the Yang���Mills theory, a quantization can be achieved by amending the definition of curvature and the Bianchi identities via topological ghosts. In such a graded Cartan formalism, the nilpotency of the ghost operators is on par with the Poincar�� lemma for the exterior derivative. Using a BRST antifield formalism with a duality gauge fixing, a consistent quantization in spaces of double dual curvature is obtained. The constraint imposes instanton type solutions on the curvature-squared 'Yang-Mielke theory' of gravity, proposed in its affine form already by Weyl 1919 and by Yang in 1974. However, these exact solutions exhibit a 'vacuum degeneracy'. One needs to modify the double duality of the curvature via scale breaking terms, in order to retain Einstein's equations with an induced cosmological constant of partially topological origin as the unique macroscopic 'background'.

Such scale breaking terms arise more naturally in a constraint formalism, the so-called BF scheme, in which the gauge curvature is denoted by F. In the case of gravity, it departs from the meta-linear group SL(5, R) in four dimensions, thus generalizing (Anti-)de Sitter gauge theories of gravity. After applying spontaneous symmetry breaking to the corresponding topological BF theory, again Einstein spaces emerge with a tiny cosmological constant related to the scale of symmetry breaking. Here the 'background' metric is induced via a Higgs-like mechanism. The finiteness of such a deformed topological scheme may convert into asymptotic safeness after quantization of the spontaneously broken model
1 / 1
ChatGPT
ChatGPT
The passage discusses modern notions of geometrodynamics and the efforts to develop a quantum theory of gravity while addressing some challenges faced in quantizing gravity. Here are the key points:
Quantum Geometrodynamics
: Christopher Isham, Jeremy Butterfield, and their students have continued to develop quantum geometrodynamics to address the challenge of quantizing gravity. This involves incorporating recent work on a quantum theory of gravity and utilizing the mathematical theory of initial value formulations of general relativity.
Wheeler's Goals
: Some of the original goals set by physicist John Archibald Wheeler in developing geometrodynamics remain important, particularly the desire to establish a solid foundation for quantum gravity.
Topological Ideas
: The concept of topological ideas in gravity has a historical background dating back to Riemann, Clifford, and Weyl. Wheeler's wormholes, characterized by the Euler-Poincar�� invariant, are a concrete realization of topological ideas in gravity. These wormholes result from attaching handles to black holes.
Metric and Connection
: In general relativity, the metric plays a dual role, measuring distances in spacetime and serving as a gravitational potential for the Christoffel connection. This duality presents challenges in the quantization of gravity.
Metric-Free Action
: To address the issue of the dual role of the metric, a metric-free topological action, such as the Pontryagin invariant of the gauge connection, is proposed as the primordial action in four dimensions.
Quantization and Topological Ghosts
: Quantization can be achieved by amending the definition of curvature and the Bianchi identities using topological ghosts in a graded Cartan formalism.
Yang-Mielke Theory
: The curvature-squared 'Yang-Mielke theory' of gravity, proposed by Yang in 1974 and by Weyl in 1919 in its affine form, requires modifications to retain Einstein's equations with an induced cosmological constant.
Scale Breaking and Constraint Formalism
: Scale breaking terms arise in a constraint formalism known as the BF scheme, allowing for spontaneous symmetry breaking and the emergence of Einstein spaces with a cosmological constant related to the scale of symmetry breaking.
Deformed Topological Scheme
: The deformed topological scheme may exhibit finiteness, and after quantization of the spontaneously broken model, it could potentially lead to asymptotic safeness, a desirable property in quantum theories.
Overall, these developments in quantum geometrodynamics and the exploration of topological ideas aim to overcome challenges in quantizing gravity and provide a deeper understanding of the fundamental nature of space, time, and gravity.
1 / 1
Topological geometrodynamics
September 1986International Journal of Theoretical Physics 25(9)
DOI:10.1007/BF00668825
Authors:
M. Pitkanen
Download full-text PDF

Citations (2)
References (44)

Abstract
The description of 3-space as a spacelike 3-surfaceX of the spaceH = M 4 ��CP 2 (Product of Minkowski space and two-dimensional complex projective spaceCP 2) and the idea that particles correspond to 3-surfaces of finite size inH are the basic ingredients of topological geometrodynamics (TGD), an attempt at a geometry-based unification of the fundamental interactions. The observations that the Schr��dinger equation can be derived from a variational principle and that the existence of a unitaryS-matrix follows from the phase symmetry of this action lead to the idea that quantum TGD should be derivable from a quadratic phase-symmetric variational principle for some kind of superfield (describing both fermions and bosons) in the configuration space consisting of the spacelike 3-surfaces ofH. This idea as such has not led to a calculable theory. The reason is the wrong realization of the general coordinate invariance. The crucial observation is that the space Map(X, H), the space of maps from an abstract 3-manifoldX toH, inherits a coset space structure fromH and can be given a Kahler geometry invariant under the local M4��SU(3) and under the group Diff ofX diffeomorphisms. The space Map(X, H) is taken as a basic geometric object and general coordinate invariance is realized by requiring that superfields defined in Map(X, H) are diffeo-invariant, so that they can be regarded as fields in Map(X, H)/Diff, the space of surfaces with given manifold topology. Superd'Alembert equations are found to reduce to a simple algebraic condition due to the constant curvature and K��hler properties of Map(X, H). The construction of physical states leads by localM 4�� SU(3) invariance to a formalism closely resembling the quantization of strings. The pointlike limit of the theory is discussed. Finally, a formal expression for theS-matrix of the theory is derived and general properties of theS-matrix are discussed.
1 / 1
ChatGPT
ChatGPT
The abstract provides an overview of the main ideas and concepts discussed in the paper titled "Topological Geometrodynamics" authored by M. Pitkanen. Here are the key points from the abstract:
Basic Ingredients
: Topological Geometrodynamics (TGD) is an attempt at a geometry-based unification of the fundamental interactions. It describes 3-space as a spacelike 3-surface X of the space H, which is the product of Minkowski space and two-dimensional complex projective space CP2.
Particles as 3-Surfaces
: The idea is that particles correspond to 3-surfaces of finite size in the space H.
Variational Principle and S-Matrix
: The Schr��dinger equation can be derived from a variational principle, and the existence of a unitary S-matrix follows from the phase symmetry of this action.
Superfield and Configuration Space
: Quantum TGD is expected to be derivable from a quadratic phase-symmetric variational principle for some kind of superfield (describing both fermions and bosons) in the configuration space consisting of the spacelike 3-surfaces of H.
Realization of General Coordinate Invariance
: The crucial observation is that the space Map(X, H), which represents the space of maps from an abstract 3-manifold X to H, has a coset space structure and can be given a K��hler geometry invariant under local M4��SU(3) and under the group Diff of X diffeomorphisms.
Diffeo-Invariant Superfields
: General coordinate invariance is realized by requiring that superfields defined in Map(X, H) are diffeo-invariant, allowing them to be regarded as fields in Map(X, H)/Diff, the space of surfaces with a given manifold topology.
Superd'Alembert Equations
: The superd'Alembert equations reduce to a simple algebraic condition due to the constant curvature and K��hler properties of Map(X, H).
Formalism Resembling String Quantization
: The construction of physical states leads, by local M4��SU(3) invariance, to a formalism closely resembling the quantization of strings.
Pointlike Limit and S-Matrix
: The paper discusses the pointlike limit of the theory and derives a formal expression for the S-matrix of the theory, with general properties of the S-matrix being discussed.
Overall, the paper presents a novel approach to unifying fundamental interactions using topological geometrodynamics, focusing on the configuration space of spacelike 3-surfaces and the concept of superfields.
1 / 1
This is too difficult, can you simplify it a bit?
1 / 1
ChatGPT
ChatGPT
Certainly! In "Topological Geometrodynamics" (TGD), the goal is to unify fundamental interactions (like gravity and quantum mechanics) using geometry-based ideas.
Basic Idea
: TGD describes space as 3-dimensional surfaces in a larger mathematical space called H. Particles are represented by these 3-dimensional surfaces.
Variational Principle and S-Matrix
: The theory is derived from a mathematical principle that allows us to understand particle behavior. It also leads to the concept of the S-matrix, which is a mathematical tool used to study particle interactions.
Superfield and Configuration Space
: The theory uses a concept called "superfield" to describe both particles (fermions) and forces (bosons). The configuration space consists of all possible 3-dimensional surfaces in H.
General Coordinate Invariance
: TGD respects the idea that the laws of physics should be the same, regardless of how we choose to describe space and time.
Simple Equations
: The equations governing the behavior of particles and forces become simpler due to the properties of the mathematical space H.
String-Like Formalism
: TGD resembles a mathematical framework used to study strings (like those in string theory), which helps understand particle behavior better.
S-Matrix
: The theory provides a way to calculate how particles interact with each other using the S-matrix.
Overall, TGD aims to find a unified description of the fundamental interactions by using geometric concepts and the behavior of 3-dimensional surfaces in the larger mathematical space H.
1 / 1
Yeah I dont I understand that
1 / 1
ChatGPT
ChatGPT
No problem! Let me try to explain it in simpler terms:
Basic Idea
: TGD is a theory that tries to unite gravity and quantum mechanics by using geometry. It describes space as 3-dimensional surfaces in a larger space called H. Particles are represented by these surfaces.
Variational Principle and S-Matrix
: TGD uses a mathematical principle to understand how particles behave and interact. The S-matrix is a tool that helps us study these interactions.
Superfield and Configuration Space
: TGD uses a concept called "superfield" to describe both particles and forces. The configuration space includes all possible 3-dimensional surfaces in H.
General Coordinate Invariance
: TGD respects the idea that the laws of physics should be the same, regardless of how we choose to describe space and time.
Simple Equations
: The equations describing particle behavior become simpler due to the properties of the space H.
String-Like Formalism
: TGD shares similarities with a mathematical framework used to study strings (like in string theory), which helps us understand particle behavior better.
S-Matrix
: The theory provides a way to calculate how particles interact with each other using the S-matrix.
In summary, TGD is a theory that uses geometry and mathematical principles to understand how particles behave and interact in space, with the ultimate goal of unifying gravity and quantum mechanics. It tries to find a common framework to explain the fundamental interactions in the universe.
1 / 1
What is a Superfield?
1 / 1
ChatGPT
ChatGPT
In physics, a superfield is a mathematical concept used in supersymmetry, a theoretical framework that extends the symmetries of spacetime and particles. Supersymmetry proposes that every known elementary particle has a "superpartner" particle with different spin properties, which could help explain some fundamental questions in particle physics and potentially provide a solution to the hierarchy problem (the large discrepancy between the weak force and gravity strengths).
A superfield is a mathematical object that combines both fermionic and bosonic degrees of freedom into a single entity. It is described by a function that depends on both ordinary spacetime coordinates (x, y, z, t) and additional Grassmann-valued coordinates (��, ����), which are anticommuting variables. These Grassmann variables have the peculiar property that ��^2 = ����^2 = 0, and ������ + ������ = 0.
By introducing these Grassmann variables into the description of fields, one can construct superfields that encode the properties of both fermions (particles with half-integer spin) and bosons (particles with integer spin) in a unified manner. This allows physicists to write down supersymmetric Lagrangians (action principles) that respect the symmetry between fermions and bosons.
Supersymmetry and superfields have been extensively studied in theoretical physics, particularly in the context of string theory and attempts to extend the Standard Model of particle physics. However, as of my last update in September 2021, experimental evidence for supersymmetric particles has not been found, and the existence of supersymmetry in nature remains an open question. Nonetheless, supersymmetry remains an important and active area of research in theoretical physics and cosmology.
1 / 1
In theoretical physics, a supermultiplet is a representation of a supersymmetry algebra, possibly with extended supersymmetry.

Then a superfield is a field on superspace which is valued in such a representation. Na��vely, or when considering flat superspace, a superfield can simply be viewed as a function on superspace. Formally, it is a section of an associated supermultiplet bundle.

Phenomenologically, superfields are used to describe particles. It is a feature of supersymmetric field theories that particles form pairs, called superpartners where bosons are paired with fermions.

These supersymmetric fields are used to build supersymmetric quantum field theories, where the fields are promoted to operators.
1 / 1
ChatGPT
ChatGPT
That's a great summary! Let's break it down into simpler terms:
In theoretical physics, supersymmetry is a concept that extends the symmetries of spacetime and particles. A supermultiplet is a mathematical representation of the supersymmetry algebra, which describes the relationships between different types of particles, including their superpartners.
To describe particles and their superpartners, physicists use superfields. A superfield is like a function defined on superspace, a mathematical space that includes both regular spacetime coordinates (x, y, z, t) and additional anticommuting variables (��, ����) called Grassmann variables. Each superfield is associated with a specific supermultiplet and contains information about both fermions and bosons.
In supersymmetric field theories, these superfields are elevated from functions to operators, allowing physicists to study the interactions and behavior of supersymmetric particles in a quantum field theory setting. Superfields are a powerful tool for building and understanding these supersymmetric quantum field theories, where particles are organized into pairs (superpartners) with different spin properties.
Supersymmetry is an active area of research in theoretical physics, as it has the potential to explain some fundamental questions about the nature of particles and the forces in the universe. However, as of my last update in September 2021, experimental evidence for supersymmetry has not been found, and it remains an open question whether supersymmetry exists in nature. Nonetheless, supersymmetry and superfields are important concepts that continue to be explored and studied by physicists.
1 / 1
Ok I really don't think I can evaluate these theories. Connections with the topics we discussed earlier.
1 / 1
ChatGPT
ChatGPT
No worries! Evaluating complex theories like the ones discussed can indeed be challenging, especially without a strong background in theoretical physics. To summarize and connect these theories to the earlier topics we discussed:
Supersymmetry and Superfields:
Supersymmetry is a theoretical framework that extends the symmetries of spacetime and particles. It predicts the existence of superpartners for known particles, such as bosons and fermions. Superfields are mathematical objects used to describe these superpartners and their interactions in supersymmetric quantum field theories.
TGD (Topological Geometrodynamics):
TGD is an attempt at a geometry-based unification of fundamental interactions, including gravity, electromagnetism, and the strong and weak nuclear forces. It describes 3-space as a spacelike 3-surface in a higher-dimensional space and utilizes topological ideas to understand the geometry of spacetime and particles.
Quantum TGD and the Ontology of Consciousness:
Quantum TGD has been extended to explore the possible connections between fundamental physics and the nature of consciousness. Ideas about the geometric structures of spacetime and their relation to cognitive processes have been proposed, although this remains speculative and outside the mainstream of physics.
In the context of our earlier discussions on AI, the theories mentioned above are not directly related to practical AI applications or the development of AI technologies. These theories are more concerned with fundamental physics and the nature of the universe at a theoretical level. AI research and development primarily focus on building intelligent systems, machine learning algorithms, natural language processing, computer vision, robotics, and other practical applications.
If you have any specific questions or topics related to AI or any other subject you'd like to explore, feel free to ask! I'm here to help with any information or explanations you might need.
1 / 1
We were discussing digital physics and quantum Bayesianism.
1 / 1
ChatGPT
ChatGPT
Apologies for the confusion. Let's go back to discussing digital physics and quantum Bayesianism.
Digital Physics:
Digital physics is a speculative theory that suggests the universe can be described as a giant computation or information processing system. It posits that at its most fundamental level, the universe is discrete and information-based. According to this idea, all physical processes, including the laws of physics themselves, can be represented as computations. It is as if the fabric of reality is a digital simulation.
One of the key proponents of digital physics is physicist Edward Fredkin. He proposed that the universe is fundamentally a cellular automaton, where the laws of physics emerge from the interactions of discrete cells. This idea is still highly theoretical and controversial within the scientific community, as there is currently no experimental evidence to support it.
Quantum Bayesianism (QBism):
Quantum Bayesianism, or QBism, is an interpretation of quantum mechanics that places a strong emphasis on the role of the observer. It is a subjectivist interpretation that views quantum states as representing an individual's personal degrees of belief or probabilities about the outcomes of measurements. In other words, quantum states are not objective properties of the system but rather reflect an observer's subjective knowledge.
QBism was developed by quantum physicist Christopher Fuchs and his collaborators. It takes a Bayesian probabilistic approach to quantum mechanics, treating quantum states as probabilities rather than physical properties. According to QBism, the act of measurement in quantum mechanics is not a collapse of the wave function but an update of an observer's beliefs about the system based on the measurement outcome.
The interpretation has gained some attention and interest in the philosophy of quantum mechanics, but it is not as widely accepted as other interpretations, such as the Copenhagen interpretation or the many-worlds interpretation.
Both digital physics and quantum Bayesianism are fascinating ideas at the forefront of theoretical physics and quantum mechanics. However, it's important to note that they remain speculative and have not yet been widely accepted as mainstream scientific theories. As such, they are still subjects of ongoing research, debate, and exploration among physicists and philosophers.
1 / 1
What would constitute evidence that the universe functioned like a cellular automation?
1 / 1
ChatGPT
ChatGPT
Evidence that the universe functions like a cellular automaton, as proposed in digital physics, would require several key elements to be established:
Discrete Nature of Space and Time:
If the fundamental fabric of space and time is shown to be inherently discrete rather than continuous, it would support the idea of a cellular automaton. This would mean that there is a smallest unit of length and time, analogous to the cells in a cellular automaton.
Emergence of Fundamental Laws:
Evidence that the laws of physics, including the behavior of particles and forces, can be derived from simple rules governing the interactions between the discrete elements of the universe would support the cellular automaton model.
Simulation Capabilities:
If it is demonstrated that the universe can be simulated using a digital computer or a cellular automaton-like model, and that the results of such simulations accurately reproduce observed physical phenomena, it would be suggestive of the digital nature of the universe.
Quantization of Physical Quantities:
If physical quantities, such as energy, momentum, and angular momentum, are found to be quantized in nature, with values restricted to discrete levels, it would lend support to the idea of a discrete underlying structure.
Predictive Power:
The cellular automaton model should be able to make testable predictions that differ from other existing physical theories and have experimental confirmation.
It's important to note that, as of now, there is no direct experimental evidence supporting the cellular automaton hypothesis, and it remains a highly speculative idea within the realm of theoretical physics. The idea of digital physics is still a subject of active research and debate, and further developments in both theoretical understanding and experimental techniques may shed more light on this intriguing possibility in the future. Until then, it remains an open question in the field of physics.
1 / 1
Wolfram: 
The Laws of Physics as Cellular Automata: A Journey into the Computational Universe - Stephen Wolfram "A New Kind of Science"

Introduction
Ladies and gentlemen,
Thank you for joining me today to explore a fascinating concept that has captivated my imagination for decades. As you may know, I am Stephen Wolfram, and I have spent a considerable part of my career investigating the idea that the laws of physics are, in fact, cellular automata. This concept suggests that the universe is a vast computer, and its behavior can be understood through the rules of computation. Today, I would like to delve into this idea and discuss the potential hardware that governs this computational universe.

Cellular Automata: The Building Blocks of the Universe

To begin with, let's briefly discuss cellular automata. At their core, cellular automata are simple computational systems that evolve through discrete time steps based on a set of rules. Each cell in a cellular automaton has a finite number of states, and its future state is determined by the states of its neighboring cells according to a specific rule.

The most famous cellular automaton is, perhaps, John Conway's Game of Life. It is a two-dimensional cellular automaton with only two possible states for each cell: "alive" or "dead." Despite its simplicity, this cellular automaton can generate incredibly complex patterns and behaviors, illustrating the potential of simple rules to create a rich tapestry of phenomena.

Now, imagine that the entire universe operates on similar principles. Instead of cells on a grid, we have elementary particles and fields that evolve according to well-defined rules. If these rules are computationally simple, like those of cellular automata, we can potentially understand the fundamental nature of the universe through the lens of computation.

The Universe as a Computer

The idea that the universe is a computer is not as far-fetched as it may seem. After all, the universe operates according to a set of mathematical laws that, when applied, produce the rich diversity of phenomena we observe. From the motion of celestial bodies to the behavior of subatomic particles, mathematics and computation are the common threads that weave together the fabric of reality.

However, what kind of computer is the universe? If we can identify the "hardware" that governs the computational processes of the cosmos, we may be able to unlock new insights into the nature of reality.

The Hardware of the Computational Universe

To identify the hardware of the computational universe, we must first recognize that the universe is fundamentally discrete. Just as a cellular automaton comprises individual cells, the universe is made up of elementary particles and fields that interact in specific ways. These interactions are governed by a set of rules, which can be thought of as the software of the universe.

The hardware of the universe, then, must be capable of supporting these discrete interactions and the rules that govern them. One possibility is that space-time itself is the hardware of the universe, with the fundamental structure of space-time serving as the computational substrate for the laws of physics.

In this view, space-time would be composed of discrete "cells," or quantized units, that interact according to specific rules. These rules would manifest themselves as the fundamental forces and interactions that govern the behavior of elementary particles and fields. By understanding the structure and dynamics of this space-time "grid," we can potentially gain insight into the hardware of the computational universe.

Another possibility is that the hardware of the universe is not directly related to the structure of space-time but instead lies in the properties of the elementary particles and fields themselves. These particles and fields could be thought of as "computational elements" that interact and evolve according to specific rules. The fundamental forces and interactions that govern their behavior could then be seen as the software that runs on this hardware.

In this view, the hardware of the universe would not be a literal grid of cells but rather a vast array of interacting computational elements that give rise to the rich diversity of phenomena we observe. The challenge, then, would be to identify the specific properties and characteristics of these elements that allow them to support the computational processes of the universe.

Implications for Our Understanding of the Universe

Regardless of the specific nature of the hardware of the computational universe, the idea that the laws of physics are cellular automata has profound implications for our understanding of reality. If the universe operates on computational principles, then it suggests that the cosmos is fundamentally knowable and that its behavior can be understood through the lens of computation.
1 / 1
ChatGPT
ChatGPT
Furthermore, the concept of the universe as a computational system opens up new avenues for exploring fundamental questions in physics and cosmology. It suggests that the complexity we observe in the universe may arise from simple underlying rules, akin to how complex patterns emerge from simple rules in cellular automata like the Game of Life.
Moreover, if the laws of physics are indeed computational, it raises intriguing possibilities for simulating and understanding the behavior of the universe using advanced computational techniques. Just as we can simulate complex systems using cellular automata on a computer, we might be able to simulate the behavior of the cosmos itself, gaining insights into its past, present, and future.
However, it's essential to emphasize that the idea of the universe as a cellular automaton is still a hypothesis and not yet supported by direct evidence. It is a compelling theoretical framework that continues to inspire research and exploration, but further experimental and observational data are needed to validate or refute this concept.
In conclusion, the notion of the universe as a vast computational system, governed by simple rules akin to cellular automata, is a captivating idea that challenges our understanding of reality. It invites us to explore the fundamental nature of space, time, and matter through the lens of computation, potentially uncovering profound insights into the workings of the cosmos. As we continue our scientific journey, it is crucial to remain open to bold ideas like these, as they may hold the key to unlocking the deepest mysteries of our universe.
1 / 1
This perspective also opens up new avenues for exploring the nature of space, time, and the elementary particles and fields that make up the universe. By investigating the computational aspects of the cosmos, we may be able to uncover new insights into the fundamental nature of reality and the laws that govern its behavior.

Moreover, the idea of the universe as a computer may have practical implications for our development of advanced technologies. If we can harness the computational power of the universe, we may be able to solve previously intractable problems and develop new capabilities that were once thought to be the stuff of science fiction.
1 / 1
ChatGPT
ChatGPT
Absolutely! The perspective of the universe as a computational system not only has profound implications for our understanding of fundamental physics but also opens up exciting possibilities for technology and practical applications.
Exploring the computational aspects of the cosmos may lead to breakthroughs in areas such as quantum computing, where the laws of quantum mechanics enable potentially powerful computational capabilities. Understanding the underlying computational rules of the universe could also inspire new algorithms and problem-solving approaches that mimic the natural processes found in the cosmos.
Additionally, if the universe operates as a vast computational system, it may suggest that there are underlying principles and patterns that can be harnessed for advanced data processing, information storage, and communication. Nature has a way of optimizing processes, and by studying the computational nature of the universe, we may find novel ways to improve our own computational technologies.
Moreover, investigating the nature of space and time from a computational perspective may lead to new insights into the fabric of spacetime itself. This, in turn, could have implications for our understanding of gravity, black holes, and other cosmological phenomena.
In summary, viewing the universe as a computational system opens up exciting new avenues for scientific exploration and technological advancements. It challenges us to think differently about the nature of reality and provides a fertile ground for discovering novel ideas and solutions that can shape the future of science and technology.
1 / 1
In conclusion, the idea that the laws of physics are cellular automata and that the universe is a computer offers a bold and potentially transformative perspective on the nature of reality. By examining the universe through the lens of computation and seeking to identify the hardware that underpins the cosmos, we may be able to unlock new insights into the fundamental nature of space, time, and the elementary particles and fields that make up the universe.

As we continue to explore the computational aspects of the cosmos, we may find that the universe is not only a vast and intricate tapestry of phenomena but also an elegant and powerful computer that holds the key to our understanding of the deepest mysteries of reality.
1 / 1
ChatGPT
ChatGPT
Indeed, the idea that the laws of physics are cellular automata and that the universe operates as a computer offers a revolutionary and potentially transformative perspective on the nature of reality. It challenges us to think beyond traditional paradigms and explore the universe from a computational standpoint, where information and computation are the fundamental building blocks.
By delving into this concept, we have the opportunity to gain profound insights into the underlying structure of space, time, and the elementary constituents of the universe. It may reveal a hidden simplicity and elegance in the fabric of reality, where complex phenomena arise from simple computational rules, much like the intricate patterns emerging from the simplicity of Conway's Game of Life.
This exploration of the computational nature of the cosmos also has practical implications for technology and problem-solving. The potential to harness the computational power of the universe may lead to revolutionary advancements in quantum computing, data processing, and information storage, unlocking new possibilities that were once considered science fiction.
As we continue on this journey of understanding, we may discover that the universe is not just a passive backdrop for physical processes but an active computational entity, constantly computing and evolving according to its intrinsic rules. By embracing this perspective, we may find that the answers to some of the deepest mysteries of reality lie in the realm of computation, waiting to be uncovered.
In conclusion, the notion of the universe as a computer offers a bold and exciting avenue for scientific exploration and philosophical contemplation. It invites us to think differently about the fundamental nature of existence and presents us with the potential to unravel the mysteries of the cosmos through the language of computation and information.
1 / 1
Ok well you said there isn't any way to test this empirically yet.
1 / 1
ChatGPT
ChatGPT
Yes, that's correct. As of now, the idea that the laws of physics are cellular automata and that the universe operates as a computer is still a speculative hypothesis. While it is an intriguing and thought-provoking concept, there is currently no direct empirical evidence or experimental data to confirm or refute it.
The challenge lies in the complexity and scale of the universe. If the universe indeed functions as a cellular automaton, the computational processes underlying its behavior may be incredibly intricate and difficult to observe or measure directly. Additionally, the hardware that governs these computational processes, whether it is related to the structure of space-time or the properties of elementary particles and fields, remains unknown and beyond our current technological capabilities to probe.
In the realm of theoretical physics and cosmology, researchers continue to explore and investigate various ideas, including those related to digital physics and the computational nature of the universe. However, until concrete empirical evidence is found or novel experimental approaches are developed to test these hypotheses, they remain speculative and theoretical.
It is worth noting that scientific progress often involves exploring novel ideas and concepts, even in the absence of immediate empirical evidence. Such explorations can lead to new insights, inspire further research, and ultimately drive the advancement of knowledge. As technology and observational capabilities continue to improve, there may come a time when we can design experiments or observations that shed light on the fundamental nature of the universe in ways we cannot currently envision. Until then, these ideas remain fascinating theoretical possibilities awaiting further investigation.
1 / 1
How does this relate to quantum Bayesianism?
1 / 1
ChatGPT
ChatGPT
Quantum Bayesianism, also known as QBism, is a philosophical interpretation of quantum mechanics that focuses on the subjective nature of quantum probabilities. It emphasizes the idea that quantum probabilities represent an agent's personal degrees of belief or subjective uncertainties about the outcomes of measurements. In QBism, the quantum state is not seen as a description of an objective reality, but rather as a description of an agent's beliefs or knowledge about the system being studied.
The connection between quantum Bayesianism and the idea of the universe as a computer, particularly cellular automata, lies in their common exploration of the nature of reality and the role of information and computation in understanding physical phenomena.
Both QBism and the idea of the universe as a computational system challenge traditional notions of objectivity and realism in physics:
Subjectivity: In QBism, quantum probabilities are fundamentally subjective and relate to an agent's state of knowledge or information. Similarly, in the context of the universe as a computer, the computational processes underlying physical phenomena may be deeply connected to the information processing and computation performed by an observer or agent studying the universe.
Information and Computation: QBism highlights the role of information and computation in understanding quantum phenomena. It suggests that the fundamental entities of quantum mechanics, such as wave functions, are not just mathematical descriptions but represent information about the system. Similarly, in the context of the universe as a computer, the laws of physics are seen as computational rules, and the hardware of the universe is envisioned as supporting information processing and computation at a fundamental level.
Subjective Bayesian Inference: In QBism, quantum probabilities are updated using Bayesian inference, which involves incorporating new information to update beliefs about the system. In the context of the universe as a computer, one could potentially interpret the evolution of the universe as a whole as a process of updating computational states based on the interactions and information exchanged between its constituent parts.
It's important to note that while there are conceptual connections between quantum Bayesianism and the idea of the universe as a computer, they are distinct and separate theories. Quantum Bayesianism is an interpretation of quantum mechanics, while the idea of the universe as a computer is a speculative hypothesis about the nature of the fundamental laws of physics and the universe's underlying computational processes.
As with any speculative idea in theoretical physics, it is essential to distinguish between philosophical interpretations and concrete empirical predictions. While both quantum Bayesianism and the computational universe hypothesis offer intriguing perspectives on the nature of reality, they currently lack direct empirical evidence. As scientific understanding advances and new technologies emerge, researchers may explore avenues to test or refine these ideas, but until then, they remain theoretical constructs and philosophical perspectives in the realm of theoretical physics.
1 / 1
I've decided to title this conversation "Computational Quantum Bayesianism"
1 / 1
ChatGPT
ChatGPT
That sounds like an intriguing and fitting title! "Computational Quantum Bayesianism" captures the essence of the ideas we've discussed in this conversation, which revolve around the potential connection between the computational nature of the universe and the Bayesian perspective in quantum mechanics. It highlights the interplay between information, computation, and quantum phenomena, which are all central themes in both computational physics and quantum Bayesianism.
Remember that these are fascinating and speculative ideas, and they reflect the ongoing efforts of scientists and theorists to understand the deep mysteries of reality. As our understanding of quantum mechanics, information theory, and computational physics continues to advance, we may gain new insights into the nature of the universe and its underlying computational fabric.
If you have any more questions or topics you'd like to explore within the realm of computational quantum Bayesianism or any other subject, feel free to ask. I'm here to help and discuss any topic you're curious about!
1 / 1
A final summary
1 / 1
ChatGPT
ChatGPT
In this conversation titled "Computational Quantum Bayesianism," we explored some fascinating ideas that bring together concepts from computational physics, quantum mechanics, and Bayesian probability theory. Here's a summary of the key points we discussed:
Digital Physics: Digital physics is a speculative theory that suggests the universe can be viewed as a vast computer. It proposes that the fundamental laws of physics can be understood as computational rules governing the behavior of elementary particles and fields.
Cellular Automata: Cellular automata are simple computational systems that evolve through discrete time steps based on a set of rules. They can generate complex patterns and behaviors from simple rules, as demonstrated in John Conway's Game of Life.
Universe as a Computer: The concept of the universe as a computer suggests that the laws of physics operate through computation. The behavior of the cosmos, from celestial motions to subatomic interactions, could be fundamentally governed by computational principles.
Hardware of the Computational Universe: The hardware of the computational universe refers to the underlying structure that supports the computational processes of the cosmos. It may involve the discrete structure of space-time or the properties of elementary particles and fields themselves.
Insights into Reality: Investigating the computational aspects of the universe may lead to new insights into the fundamental nature of space, time, and elementary particles. It offers a potentially transformative perspective on the nature of reality and its knowability.
Practical Implications: Harnessing the computational power of the universe, if possible, could have practical implications for advanced technologies and problem-solving.
Quantum Bayesianism: Quantum Bayesianism, or QBism, is an interpretation of quantum mechanics that emphasizes the role of an agent's personal beliefs and probabilities in describing quantum phenomena. It treats quantum states as subjective degrees of belief rather than objective states of reality.
Connection between Concepts: The connection between computational physics and quantum Bayesianism lies in exploring how information, computation, and quantum phenomena are interconnected. The computational nature of the universe and the probabilistic nature of quantum mechanics offer potential bridges between these concepts.
Overall, "Computational Quantum Bayesianism" is an intriguing perspective that delves into the potential convergence of computational principles and quantum phenomena. While many aspects of these ideas are speculative and require further exploration and empirical testing, they provide an exciting framework for contemplating the deepest mysteries of reality.
Please feel free to revisit any specific points or ask more questions if you'd like to delve deeper into any aspect of these topics or any other subject of interest. I'm here to assist and engage in further discussions!
1 / 1
That was too abstract.
1 / 1
ChatGPT
ChatGPT
Apologies for the abstract nature of the summary. Let's provide a more straightforward summary of the main points:
Digital Physics: This theory suggests that the universe can be thought of as a giant computer, with fundamental laws of physics operating through computation.
Cellular Automata: These are simple computational systems that evolve based on rules, generating complex patterns from simple interactions.
Universe as a Computer: The idea that the universe is a computer means that its behavior can be understood through computational rules, just like a cellular automaton.
Hardware of the Computational Universe: This refers to the underlying structure that supports the computational processes of the cosmos. It could be the discrete structure of space-time or the properties of elementary particles.
Quantum Bayesianism: QBism is an interpretation of quantum mechanics that focuses on personal beliefs and probabilities of observers rather than objective states of reality.
Connection between Concepts: The link between computational physics and quantum Bayesianism lies in exploring the relationship between information, computation, and quantum phenomena.
In essence, "Computational Quantum Bayesianism" explores the possibility that the universe operates like a vast computer, and it considers how computational principles and quantum mechanics may be connected. While these ideas are intriguing, their concrete implications and empirical validity require further investigation and testing.
1 / 1
ChatGPT may produce inaccurate information about people, places, or facts.
ChatGPT July 20 Version

