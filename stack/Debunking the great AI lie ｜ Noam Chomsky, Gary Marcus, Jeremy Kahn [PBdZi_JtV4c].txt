Oh my word, there's so many people in here for our final two acts.
In a moment, I'll introduce Noam Chomsky and two amazing people to speak alongside him.
And then we're going to close out as his tradition with the pretty incredible president of Portugal.
It's been an incredible four days.
I hope everybody has had.
Have you had a good time?
It's been remarkable.
The father of modern linguistics, Noam Chomsky, joins scientist, author and entrepreneur Gary
Marcus for a wide-ranging discussion on why the myths surrounding artificial intelligence
are so dangerous and why we should not rely on AI to save us all.
Please welcome to the stage Noam Chomsky and Gary Marcus of Robust.ai in conversation with
Jeremy Kahn from Fortune.
Hi, Noam.
Thanks for joining us.
And Gary, thanks for joining us.
A lot of people think we're living in a golden age of AI.
There's a lot of hype around generative AI, the systems like Dolly and stable diffusion
that can create amazing images just from text prompts.
There's also a lot of discussion about GPT-3, a system created by open AI, that can create
long passages of coherent text, again, just from a small human-written prompt.
A lot of you are very excited about where this technology is going.
But no, my understanding is that you are not, that you are actually disappointed in where
we are with this technology.
Why is that?
Oh, he's, I don't know if he can hear us.
There we go.
Yeah, I am indeed, yes.
Be efficient.
One way is it's not strong enough, fails to do certain things.
The other way is it's too strong.
It does what it shouldn't do.
Well, my own interests happen to be language and cognition, language specifically.
So take GPT, there's Gary Marcus, others have found lots of ways in which the system's
deficient, this system and others, doesn't do certain things.
There's the other, that can, in principle, at least be fixed.
You add another trillion parameters, double the number of terabytes, maybe do better.
When a system is too strong, it's unfixable, typically.
And that's the problem with GPT and the other systems.
So if you give a database to the GPT system, which happens to be from an impossible language,
one that violates the rules of language, it'll do just as well, often better, because the
rules can be simpler.
For example, one of the fundamental properties of the way language works, there's good reasons
for it, is that the rules, the core rules do not ignore linear order of words.
They ignore everything that you hear.
They attend only to abstract structures that the mind creates.
So it's very easy to construct impossible languages, which use very simple procedures
involving linear order of words.
Trouble is, that's not language, but GPT will do just fine with them.
So it's kind of as if somebody were to propose, say, a revised version of the periodic table,
which included all the elements, all the possible elements, and all the impossible elements,
and didn't make any distinction between them.
They wouldn't tell us anything about elements.
And if a system works just as well for impossible languages as for possible ones, by definition,
not telling us anything about language, and that's the way these systems work.
It generalizes to other systems, too.
So the problem is, the deep problem that concerns me is too much strength.
I don't see any conceivable way to remedy that.
Interesting.
Gary, you've examined how some of these systems fail.
Can you talk a little bit about what those sort of modes of failure are that you've discovered?
Sure.
Usually, also, Nome and I have had this interchange over the last few months where he tells me
that I'm too nice when I'm negative about AI.
In this case, there's a certain way in which I thought he was too nice.
So he said that in the ways in which these systems are too weak, just adding more parameters,
maybe that will help.
And there, I actually take a darker view, maybe, than Nome does.
Some examples of the kinds of problems these systems have are Dolly.
If you tell it to draw a blue cube on top of a red cube,
it might just give you a red cube on top of a blue cube.
So one of the most basic things about language is that we put together meanings from the orders
of words, an idea that goes back to Frege and even further.
These systems don't understand the relation between the orders of words
and their underlying meanings.
Another version of this system, you say something like draw a picture of a man chasing a woman
or draw a picture of a woman chasing a man.
And the system is basically a chance.
It really can't tell the difference.
And it's not clear that just adding in more of the same,
what people call scaling today, is actually going to help.
I think that there's something fundamental missing from the systems,
which is the understanding of the world, how objects work, what objects it's describing.
There was another article that just came out on Theory of Mind.
And whether these systems can understand what you believe about other people.
And there was failure on those kinds of things.
There's failure on a system to understand something like, if I want some grapes,
or sorry, I ask you if you have something, did you touch this glass?
I can't remember the exact example.
And you say, well, I had gloves on, so my finger, implying your fingerprints aren't there.
And the system doesn't understand it.
That was another benchmark that came out this week.
So when Dolly came out, Sam Altman of OpenAI said, AGI is going to be wild.
Artificial general intelligence is going to be here,
and implied that these systems are closed artificial intelligence.
And scientists like me weren't given access.
And in the last couple of months, we've been given access,
and these things fall apart left and right.
So yes, they can draw pretty pictures, but no,
they really have a very shallow understanding of language.
And then to Noam's point, if they have a shallow understanding of language,
they're not really helping with the question that Noam has worked on.
All his career, which is, why is human language the way that it is?
And what he's saying is, these systems could learn computer languages,
or languages that aren't like humans.
They could learn anything.
They don't do any of it perfectly, and they don't really tell much about why we
are the special creatures that we are.
Interesting.
Noam, I know you've sort of said that these systems are maybe good engineering,
but not very good science at all.
Can you explain what you meant by that, and why you think the science here
isn't really valid?
Well, let's take engineering achievements.
So I happen to be part of hearing, so I can't hear most of what you're saying.
I'm reading life transcription captions.
It's pretty helpful.
I'm all in favor of it.
It's achieved by brute force, no scientific interest, but it's fine.
I'm happy with it.
I'm happy with a snow plow that clears the streets.
You don't have to do it by hand.
I don't see anything.
I have no criticism of that.
I think it's great.
Sometimes the engineering achievements can contribute to science.
So a telescope, for example, allowed Galileo to discover things he couldn't have
discovered without it.
Well, that's fine.
There are apparently some engineering achievements with the deep learning
approaches that have been like a telescope.
So they've apparently produced some helpful results with regard to protein
folding, massive computation beyond hand computation.
So that's good.
But science is a different concern.
You're trying to understand what the world is like.
How can I make something useful?
Nothing wrong with making useful things.
But the project of trying to understand what the world is like is different.
So, turning to Gary's point, I'm perfectly willing to concede that I've been
too nice in thinking that if you add a thousand parameters, maybe you'll get
somewhere, maybe you won't.
But my concern is different.
If the system doesn't distinguish what's the actual world from non-actual
world, it's not telling us anything.
Just as in the case of the souped-up periodic table, the GPT systems and others
can find their professional regularities and astronomical amounts of data
and produce something that looks more like what their data was.
But it can do exactly the same thing, even better often, with data that
violates all the principles of language and cognition.
So they're first telling us nothing about that.
You might ask whether it's making an engineering contribution.
Actually, in this case, I don't really see any.
I don't see what point there is to GPT, except maybe helping some
student fake an exam or something.
But so it doesn't seem, it has no scientific contribution.
It doesn't seem to have any engineering contribution.
So as far as I can see, it's basically a way of wasting a lot
of the energy in California and inspiring them.
That's a good point.
I want to bring Gary in.
What do you think the dangers are here?
I mean, if these are tools that are in some ways useful, is there a danger,
though, in thinking that they're more capable than they are?
Is there a danger in the hype?
Gary, what do you have to say about that?
I think there's two kinds of dangers.
One kind of danger here is that I think we're following the wrong path
in AI, and the right path to AI I think is going to involve looking
more at humans and doing the science that Noam is talking about.
And so we're in this moment where we're at a local maximum.
We have something that looks good, but isn't really as deep as we need it.
And it's sucking the oxygen away from the field of cognitive science,
looking at how humans understand language, how humans understand the world,
because it's so much fun to play with these toys, and it's fine for people
to play with the toys, but it takes away a lot of the energy.
Young graduate students, if they have a choice between going into linguistics
or a choice between working on GPT-3 and getting paid a lot of money,
they're probably going to work on GPT-3.
But I think it's short-sighted that we wind up with a technology that actually
has a kind of short road ahead, isn't going to last as long as people think.
And you look at the history of AI, there have been fads over and over again,
expert systems and support vector machines that people in this crowd
probably haven't even heard of that were super popular for a while.
And we need to have something that's more stable, and one more fad's probably
not helping us.
Then there's another kind of danger, which is the systems we have now,
what they do is they perpetuate past data.
They don't really understand the world.
So for example, that means that they're sexist and they're racist.
They're not built to be that way, but because they just copy the data
that's there and don't have values about equality, for example,
they perpetuate past bias.
Another example is because they don't have models of the world,
they kind of lie indiscriminately.
Now, they don't do that literally because they have no intention,
there's no malice, but they produce misinformation,
and it's going to completely change the world in the next couple of years.
The amount of misinformation that troll farms are going to be able to produce
and the cost of that is really, I think, going to be devastating to the
democratic process.
And so the fact that you have a system that kind of looks good
can make sentences that are grammatical, but make up stuff like
why is it good to eat socks after meditating, and then say some experts
believe that it's good to eat socks after meditating because such and such.
It all sounds plausible.
You read it on a website when you're like falling asleep and you believe it.
Well, now you can do this at scale.
You can get your own custom version of GPT-3 for less than half a million dollars.
So if you're a troll farm, that's a great investment, but it's terrible for society.
So there's some specific dangers around it.
Then there are lesser dangers.
A lot of people spend a lot of money on driverless cars,
and it doesn't really look like it's working out.
Maybe it'll happen in 20 years, but we're not going to have routine level five
self-driving where you can type in a destination and go anywhere
the way Musk has been promising it for six or seven years.
That's just not going to happen.
There's been $100 billion put into that industry, and the money is wasted.
I'm more concerned by the threat of democracy than investors wasting their money.
But it is another kind of side product of all of this.
Is there a danger also in sort of not understanding modes of failure?
I mean, some of those things you've discovered recently,
when you have had access to these systems about the way in which they do not understand language,
is there a danger in sort of believing the hype and thinking these things are more capable than they actually are?
Well, I actually have a prediction.
I think it comes out tomorrow in Wired for the year 2023.
I predict that it's going to be the first year in which there's a death attributable to one of these systems,
either because it tells somebody to commit suicide or somebody falls in love with one of these systems,
but the system doesn't actually love them back and they abandon it,
or because it tells them to mix Clorax with Drano and they drink it all.
So people trust these things because they appear plausible,
but it's kind of like a magic trick.
What they're really doing, we haven't said this, is basically a version of autocomplete.
Like in your phone, you predict the next word.
GPT is really just autocomplete on steroids, but it gives this illusion that it's more than that,
and if people get sucked in, then they can take bad advice from these systems,
and I think we will see a lot of that next year as these things become widely distributed and cheap.
Interesting.
No, I know you said you don't think there's been much contribution here.
Do you think there's been sort of any contribution from these large language models
to the actual understanding of linguistics, which is what you've devoted your career to?
I can't think of one single thing.
I mean, I can give you an example of the kind of stuff that comes out just a couple of days ago.
I don't follow the literature on this closely.
I'm not like Gary, but somebody sent me a friend sent me an article from something called,
I think the Federation of Associations of Behavioral and Computer Science or something like that.
It was a study, a massive study in which the guy had, which published in their journal,
he had all of Reddit and a ton of other things in his database,
all kind of supercomputers studying what you can learn about the frequency of words
and how important it could be for language understanding.
And they gave one example that was really the chief achievement.
They discovered that the word occasion is used much more frequently than the word molecule.
And the reason they explained it is because it's used in many domains,
whereas molecules only use in a few domains.
Who would have guessed it?
That kind of contribution that comes out, if there's anything else, I've missed it.
I mean, these things can be useful.
Like I say, basically what lies behind the life transcription that I'm using.
So again, I don't have anything against snow clouds.
Happy to have them around.
But they shouldn't mislead people indicating it for Gary's reasons and others,
that they're making some kind of contribution to science.
They're not helping us understand the world and they can't, in principle,
because they're too strong irremediably.
No, I think you've said, when we were speaking before this,
that looking back at the history of AI,
that the purpose of the field was not simply to make better snow clouds,
that there was this idea of trying to actually understand human intelligence
and then to build on that.
Can you talk a little bit about, do you feel like the whole field's kind of gone awry here
in focusing so much on these deep learning systems?
Well, I was around when AI was being developed and no many other people.
The original goal was to use the capacities that had been developed
in computer science, both technical and intellectual,
because there's a major intellectual breakthrough,
the theory of computation developed by Turing and others.
The original idea was, let's use this technical intellectual contribution
to see if we can understand something about how humans think.
You go back to the paper that initiated the field.
Alan Turing's famous paper on 1950 paper on cold can machines think.
A machine, of course, means program, not the object.
Well, he was interested in seeing if we can understand what thinking is.
You go to early pioneers in the field,
Simon, Mark Menzky, others.
That was their goal as well.
Can we learn something about cognition thinking?
Well, that's by now considered old-fashioned AI.
There's various disparities.
And it was gradually replaced over the years by what Gary was just describing,
playing with fancy toys.
And it's understandable.
I mean, I've spent most of my life at MIT.
I take teenage kids and smart teenage kids,
hand them fancy computers.
You can do all sorts of things, massive programming systems.
It's going to be a lot of fun.
When I got to MIT in the 50s,
one of the ways that students used to have fun was by playing with a very elaborate
railways, which you could do all kind of complicated things with.
Well, 10 years later in the 60s, when they started having these toys,
no more electric railways.
This was a lot more fun.
You could do all sorts of things.
Figure out a way to make the elevator run when nobody was running it.
But it's running a program.
You can imagine what it's like.
It's such exciting.
You can play with the toys.
You can do a lot of complicated things.
It does get intellectually interesting when you work into how the programming
works, how the statistical analysis works, and so on.
It's just not going anywhere.
It may produce useful things like live transcription,
but that's not very exciting.
I mean, it's helpful, but not a real contribution.
Understanding the world.
As far as understanding the nature of language or any other cognitive process,
I literally can't think of anything that's been contributed.
And Gary, I know you've thought that there's a lot of things we do know about
cognitive science that is being ignored by the current enthusiasm for deep learning
and ever larger deep learning systems.
Yeah, I look at what people are doing now.
And I think, have they ever taken a class in linguistics?
So in linguistics, you learn about the relationship between syntax and semantics,
for example, and between those and pragmatics.
And if you build a language production system, for example, in the classic way,
you start with a meaning that you want to express and you translate that into words
or in language comprehension.
You start with a sentence that you want to understand and you translate that into a meaning.
Well, GPT-3 doesn't do that, and people don't even notice that it doesn't do that.
What GPT-3 does is it hears a sequence of words and it predicts the next word.
But let's say you want to talk to a user and you know what you want to say.
Well, GPT is like a wild bucking bronco.
It's very powerful.
It might produce something grammatically interesting,
but whether it winds up giving you what you want is an entirely different matter.
And if you want to extract from it a model of the world, it doesn't really do that.
There's this paper called Palm Seitan, I think, by Google where they put GPT-3 in a robot.
And it works like three-quarters of the time and it's amazing,
but a quarter of the time it doesn't work.
Now imagine that you want to put your grandpa in bed and you want to tell the robot to do that
and three-quarters of the time it does that and one quarter of the time it drops your grandpa.
Like this is not good.
We don't want to drop grandpas around.
So cognitive science tells us that we want, for example, it's just a basic thing,
but you want to map a meaning onto a sentence or the other way around.
And people are like, I've got all this data.
I don't need to do that.
Well, no, you really do still need to do that.
Another example is we have lots of knowledge.
You go to college and read books or when you're five you read books.
You absorb verbal symbolic knowledge from the world.
And there's this funny politics that goes back 50 years about using neural networks versus a knowledge-based approach.
It turns out the knowledge-based approach has some value, but it had some trouble 30 years ago.
So now we're doing all this stuff without knowledge, which means you have all the knowledge in Wikipedia
and nobody really knows how to put it into these systems.
And because there's statistical mimics, you ask a question like, who's the president of the United States?
And they might say Donald Trump because in their data set there are more examples of Donald Trump than Joe Biden.
Whereas you want to be able to reason and say, well, Biden is now the president, Trump was the president before.
You want to use your understanding of sequences of events and time.
And cognitive scientists think about this stuff.
They talk about things like discourse models, putting together what it is that we're talking about and how we're talking about it.
So there are lots of ideas.
Another one that we didn't talk about yet, but which I learned from Noam and which is really foundational,
I know he learned it from Plato, is innateness.
The idea that something is built into the mind.
There's lots of reason to think that when we learn language, we're not starting from a blank slate.
Noam has made this argument for years and years.
I think it's still true.
And the failures that we're seeing, for example, it's Dali, and I'll tell you about one another,
are showing you that if you start with a blank slate that just accumulates statistics, you don't really understand language.
The paper I have yesterday, which I sent to both of you guys, shows the same thing in vision.
So there's some models that label activities.
So it can say, you're nodding your head.
Those people are sitting out there.
It can do some visual labeling.
And the myth is always the right things will emerge, which is kind of magic.
It's like a Latinate word for magic.
It will emerge when you give enough data to these systems that they will understand the world.
So we built a benchmark based on what 10-month-olds do in the lab,
or four-month-olds for some of the experiments, understanding basic physics, like things drop,
or if they're hidden, you can still find them eventually.
And these systems don't understand at all.
Allen AI Institute and some developmental psychologists from Illinois and I published this paper yesterday.
It's a complete failure of the empiricist hypothesis that if we just give a lot of data, the cognition will emerge.
And developmental psychologists have done lots of work, Liz Belke in particular,
but many people, Renee Byers-Jean, one of our collaborators on this paper,
have done lots of work showing that children seem to have probably an innate sense of how objects exist in the world.
We don't start with nothing.
Kant said we start with time and space and causality.
He's probably right. We probably start with those things.
And over and over, people keep pursuing this hypothesis that ignores the cognitive science around that
and says, we'll just use all the data.
And because it works like 75% of the time, they think they're making progress.
Sometimes you make progress 75% of the way it's not enough.
We saw that in the driverless car industry, right?
You know, getting close doesn't really seem to solve that problem.
And that's what we're seeing here.
And Gary, I know you have some ideas about how you would go forward.
You know, if today's AI, despite all the hype around it, is not delivering what we want, you know, what would?
What would it be a path forward?
So I have an article called Next Decade in AI, which people can read later in archive.
But it points to four things.
One is what we call neurosymbolic AI, which is putting together this tradition of neural networks that's popular now
with the old-fashioned AI, trying to find some kind of synthesis between the two.
Because they each have some value. The neural networks are good at learning from data, but they're not very good at abstract knowledge.
And the old symbolic stuff is good with abstract knowledge, but not learning from data.
So that's step one.
Step two is we need to have a large database of machine interpretable knowledge.
So all of these things kind of fake their way through, but knowledge is never rich enough and abstract enough.
So like, you know that if I have a bottle, you can't see it, but if you saw me bring it up, if I knock it over, there's probably going to be water on the stage.
Even if you haven't seen this particular bottle in this configuration, you have very general abstract knowledge.
And that's crucial, I think, to any real general intelligence.
Then we can reason about things. We can make inferences about them.
And we have cognitive models of the world.
So you know right now that I've mentioned it that this bottle is here even if you can't see it.
And so you're maintaining an internal representation of the stuff that's out there.
It might be imperfect, but you have a representation in your head of the things out there in the world, and you reason over it.
These are foundational ideas in cognitive science, and they're not represented in current AI, and we're seeing the cost of that.
Interesting. Noam, do you think that what Gary is saying is probably the way forward?
Did some of these ideas about innateness should be incorporated into the way we think about AI systems?
I think those are good ways to proceed, but I also think that if AI wants to proceed, it should take into account what has come to be understood about the basis for acquiring their cognitive capacities,
which of course do involve fundamental built-in innate properties.
We now have some understanding of them, and even some understanding of why they're the kind that evolution would provide.
We should recall that back in the early days, the days that I happened to favor myself,
AI was basically not distinct from cognitive science.
It was just one of the ways of approaching the problems of cognitive science using achievements, great achievements that had been made both intellectually and technologically with the development of the theory of
computability and the physical devices that were able to implement it.
This was a great way to advance cognitive science as long as you were part of science, not when you're playing with toys or trying to impress science reporters and journals.
Like when you're pursuing the methods of science, in the cases that Gary was discussing, taking into account the kind of discoveries of the Spelke, Renee Byersong, and others about the innate basis for our ability to maneuver in the world.
If you're working on language and thought, which are virtually indistinguishable, then you look at what has been discovered about the innate basis for the acquisition of language on the basis of virtually no evidence.
As has been shown by now in experimental work, a lot of it by the late Lilac Lightman, others, which show that a child just acquires language the way it walks.
He's just sitting there waiting to be triggered by some stimulation and it all comes out. Well, you want to study introducing such understanding into the methods of AI could lead to carrying forward the earlier programs, which I thought were on the right track of using the achievements of
intellectual achievements, technological achievements as a way of carrying cognitive science forward in its effort to understand what the world is like in this case, whatever minds are like.
Great. Well, we're out of time, but I want to thank Noam Chopsky and Gary Marcus for coming here and debunking a bit of the hype around today's AI and also pointing forward to some ways we might be able to take the technology forward.
Thank you very much and thank you for listening.
