Deep Learning Cognition Overview
Open sidebar
Deep Learning Cognition Overview
Some questions: What cognitive capacities, if any, do current deep learning systems possess?
What cognitive capacities might future deep learning systems possess?
What kind of representations can we ascribe to artificial neural networks?
Could a large language model genuinely understand language?
What do deep learning systems tell us about human cognition, and vice versa?
Current deep learning systems possess a wide range of cognitive capacities, including the ability to recognize patterns and make predictions, classify images and text, and generate natural language.
In the future, deep learning systems may be able to perform more complex tasks such as multi-modal reasoning, planning and decision-making, and even general intelligence.
Artificial neural networks can be thought of as a way to represent and process information, often in the form of a hierarchy of features or concepts.
While large language models like GPT-3 can generate human-like text and answer questions, they do not have a true understanding of language or the world, but rather rely on patterns in the data they were trained on.
Deep learning systems can provide insights into the nature of human cognition and the way we process information, but it is important to keep in mind that these models are not identical to the human brain and may have limitations or biases.
How can we develop a theoretical understanding of deep learning systems?
How do deep learning systems bear on philosophical debates such as rationalism vs empiricism and classical vs. nonclassical views of cognition.
What are the key obstacles on the path from current deep learning systems to human-level cognition?
Developing a theoretical understanding of deep learning systems requires a combination of techniques from mathematics, computer science, and cognitive science. Researchers use formal models, such as computational graphs, to describe the functions and representations of neural networks, and mathematical analysis to understand their properties and behavior. Empirical methods, such as experimentation and data analysis, are also crucial for validating and improving these models.
Deep learning systems bear on philosophical debates such as rationalism vs empiricism in that they rely heavily on data and learning through experience, aligning them more with empiricism. However, they also involve the use of formal models and mathematical reasoning, which aligns them with rationalism.
The key obstacles on the path from current deep learning systems to human-level cognition include the need for more sophisticated architectures that can handle more complex tasks, more robust and generalizable learning algorithms, and the ability to incorporate prior knowledge and context. Additionally, the ability to understand and explain the decisions made by the models, and to deal with the ethical and societal issues raised by the technology is also important.
Classical views of cognition tend to focus on the symbolic manipulation of information, whereas non-classical views focus on distributed representations and sub-symbolic processing, which is more similar to the way deep learning systems work.
Humans absorb these representations using background knowledge, "fleshing out" missing details based on common sense. Shorn of background knowledge, logical contents — a single word or phrase, a few notes on a music score, the markings in an equation or sentence — typically represent only what philosophers call "discrete facts": objects and properties, musical phrases or the relation of numbers in an equation.  -- Jacob Brown
The quote you've provided is discussing the idea that humans use background knowledge to understand and interpret information, which allows us to make inferences and fill in missing details. This is in contrast to deep learning systems, which typically rely on patterns in the data they were trained on rather than background knowledge. The quote also suggests that without background knowledge, logical contents such as a single word or phrase, only represent discrete facts. This highlights one of the key differences between human cognition and current deep learning systems, and emphasizes the importance of incorporating background knowledge and context in order to build more sophisticated AI systems that can truly understand and reason about the world like humans do.
He continues: By contrast, iconic representations — images, maps, music recordings or videos — involve elements that only make sense in relation to each other: shapes in a picture, the location of a mountain range or the various positions and movements of actors in a movie. Iconic representations depend on the relationship between elements and their locations, like how a black-and-white photograph represents certain wavelengths of light at different locations.
The quote you've provided is discussing the difference between discrete representations, such as words and phrases, and iconic representations, such as images, maps, and videos. Discrete representations are made up of individual elements that can be understood separately, while iconic representations depend on the relationship between the elements and their location. Iconic representations, like black-and-white photographs, capture certain wavelengths of light at different locations, and the meaning is inferred by the relationship between those elements. The quote highlights the importance of understanding that different types of representations have different properties and are processed differently in human cognition and AI systems. It also suggests that iconic representations might be a good starting point to develop AI systems that can understand and reason about the world like humans do.
And finally: Both kinds of representation are expressive, but logical representations cannot capture relations between elements without adding more information, whereas iconic representations cannot depict elements non-relationally. 

Neither of these forms of representation reflects how we experience them. Musicians looking at a familiar musical score — a logical representation — will instantly imagine their favorite recording of the piece: an iconic representation. But this is the work of our background familiarity with both kinds of representation.
The quote you've provided is emphasizing the limitations of different types of representations, and how they differ in their ability to capture information. Logical representations, such as words and phrases, can only capture relations between elements with additional information, while iconic representations, such as images and videos, cannot depict elements non-relationally. This suggests that neither representation fully captures the way we experience the world and that it's important to use multiple types of representations to capture all the necessary information.
The quote also mentions that our background familiarity with different types of representations allows us to easily switch between them and relate them to each other. For example, a musician can look at a musical score, a logical representation, and instantly imagine a recording of the piece, an iconic representation. This highlights the importance of background knowledge and context in understanding and interpreting information, both for humans and AI systems.
"DALL-E and CLIP recognize and reproduce not just skeletal content but also flesh it out."
The quote you've provided is referring to the capabilities of two specific deep learning systems, DALL-E and CLIP. It suggests that these systems have the ability to not only recognize and reproduce skeletal content, but also to add additional information and "flesh it out." This implies that DALL-E and CLIP are able to make inferences and add context to the information they are processing, which is a more advanced capability than simply recognizing and reproducing content.
DALL-E is a deep learning model developed by OpenAI, it is capable of generating images from text descriptions, and CLIP is a neural network model also developed by OpenAI, which is able to recognize and generate images, videos, and audio. Both models are trained on large amounts of data and demonstrate an ability to recognize patterns in the data and generate new information.
Now summariZe: The Opposite of Reduction

We also need to be able to move in the opposite direction, from Models to Reality. Or at least from more abstract Partial Models to Partial Models closer to reality.

When an actor is given a screenplay they know it only contains rough directions for what to do and what lines to say.

The actor's job is to "give a little of themselves" to flesh out the screenplay to actual actions, including creating — synthesizing — the appropriate display of emotions, tone of voice, and body language.

They use their experience as people and as actors; they use elements of their past lives and skills they have acquired by training to create something people in the audience might relate to. 

For example, they may re-purpose a personal experience ("be as sad as when my hamster died"), things they learned in drama school (speaking, singing, dancing, swordplay), from other actors, from fiction, from other movies and plays, etc.

-- Monica Anderson
The quote you've provided is discussing the idea of moving from models to reality, or from more abstract partial models to partial models that are closer to reality. It is comparing the process of an actor interpreting a screenplay and adding their own elements, such as emotions, tone of voice, and body language, to flesh out the script, to how AI systems can be improved by incorporating background knowledge, context, and human-like understanding to create more sophisticated models. The quote suggests that the process of an actor requires using personal experiences, skills acquired through training, and elements from other sources to create something relatable to the audience. Similarly, AI systems can also be improved by incorporating background knowledge, context, and human-like understanding to create more sophisticated models that can understand and reason about the world like humans do.
What is the relation between Anderson's quote and the one by Brown, about "fleshing out" an abstract "skeletal content"?
Both Anderson's and Brown's quotes are discussing the idea of "fleshing out" or adding additional information to abstract or "skeletal" content. Anderson's quote uses the analogy of an actor interpreting a screenplay and adding their own elements, such as emotions, tone of voice, and body language, to flesh out the script. Similarly, Brown's quote is discussing the way that humans use background knowledge to "flesh out" or make inferences about abstract or "skeletal" information, such as a single word or phrase. Both quotes are emphasizing the importance of adding context, background knowledge, and human-like understanding to create more sophisticated models that can understand and reason about the world like humans do.
ChatGPT Mar 14 Version
. Free Research Preview. Our goal is to make AI systems more natural and safe to interact with. Your feedback will help us improve.
New chat

