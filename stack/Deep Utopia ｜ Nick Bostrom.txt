Welcome to the Predictive World series brought to you by Sales Choice,
which specializes in ending revenue uncertainty for human advantage.
I'm your host, Jen Yim.
I'm the Director of Product and Marketing here at Sales Choice,
and we bring three services powered by our AI platform.
There's Sales Insights,
which helps sales professionals with guided insights,
Mood Insights, which helps connect frontline voices to leadership,
and third, AI and data science as a service,
where we design, build, and maintain AI models.
With a new focus on generative AI.
Over the years, we've been hosting numerous experts and thought leaders
on AI and revenue growth,
and we've built a trove of podcasts, webinars, white papers, and more.
And I invite our listeners to go to saleschoice.com slash resources to access that.
Today, we have a very esteemed guest with us.
It's Nick Bostrom, author of Deep Utopia.
Nick, welcome and pleasure to meet you.
And could you tell us a bit about yourself?
Hi, yeah, I'm happy to be with you.
Well, I'm a philosopher, I guess, if you have to use a label,
although I have a background in a variety of disciplines,
and I was, until recently, a professor at Oxford University,
where I headed the Future of Humanity Institute.
As a multidisciplinary group, it was that had as its mandate
to try to think carefully about the really big picture questions
for human civilization.
And in particular, the way that technologies
might transform the human condition in some fundamental way.
Very cool.
And to start our conversation,
I'm going to turn it over to our Dr. Cindy Gordon to kick it off.
Nick, it's such a pleasure being with you today.
And I was really intrigued to read your new book and congratulations.
You know, just scanning through Monday, Tuesday, Wednesday,
Thursday, Friday, Saturday.
I thought that was a very original approach.
And I love the dialogue.
But what surprised me, Nick, it was you think your first book
that you wrote on superintelligence, which was really a reflective
perspective on what are the risks, what the dangers, strategies,
and trying to unlock, you know, that deeper critical thinking.
This book was very positive in many regards,
in the sense that just extrapolating the human condition out
and looking at how deep utopia explores life and meaning in a solved world.
And I felt it was a very spiritual book as well, this one,
which was very intriguing for me.
Obviously, your philosophical roots always shine through
and everything you write and the theoretical aspects also come through.
But there was definitely a deep thought process to
not blend dystopia with utopia.
And I would just love to hear why you wrote this book.
And it really does inspire more hope.
I don't know if I'll be here when this all comes together.
But, you know, it was an interesting intellectual process
to go through it in reflection.
Well, thank you.
I mean, you might well be given the timelines have started to look
a lot shorter recently, right?
The AI is just galloping ahead.
And I think the next few years will be really interesting.
Yeah, it's the other side of the coin, you might say.
So whereas the earlier book, Superintelligence, came out in 2014
and had been in the works for six years prior to that,
focused primarily on what could go wrong with AI.
So that the view was if we could more clearly see where the pitfalls lay,
we might have a better chance of steering clear of them.
And this new book, Deep Utopia, looks at what happens if things go right.
And the condition that we will then end up with where you have,
you know, very advanced AIs and robots and other technical advances.
And what the role of us humans might be in such a quote unquote solved world
that we might have done achieve.
Yeah, to me, the underpinning also was just the focus on the meaning
of life and humanity and that sort of deeper reflection
that can come about with more abundance of time in many regards.
Jen, do you want to go to the next slide?
Yeah, so I think this really focuses on the next question
is what are the big questions that you really want organizations,
leaders, board directors to really think hard about that they need to really confront
and plan for, much to your earlier point, that this is moving at a faster trajectory.
But what are your, what do you have to say that the big questions you want people to really
think hard about?
What are they?
Well, we first need to make sure we get there to this deep utopia condition.
And I think we are still very far away from that.
There are some huge challenges between where we are now and this hypothetical future state
of a solved world.
I tend to divide them broadly into three categories, but I should say I'll print that
in addition to these three broad categories of sort of ultimate challenges.
There are also many more here and now challenges that we need to address.
But if we are thinking more about this kind of fundamental transformation to machine
intelligence era, I think there is first and perhaps foremost the alignment problem.
This was a big focus of superintelligence.
And the idea there is that if we do figure out at some point how to create machine intellects
that can do everything that the human brain can do.
And then, you know, presumably I would say shortly after that superintelligence that
can do all of that, but much better and faster.
How then can we make sure to align these sort of arbitrarily powerful AI systems to the goals
of their creators?
And this is a technical challenge.
There has been a big shift in that.
When I wrote superintelligence, and this was like the main reason for writing it,
this alignment problem was very much neglected.
Now, on the other hand, all the frontier AI labs have research groups specifically trying
to figure out some solutions to this.
But we still don't probably have a solution that would work.
So it remains an open question.
We will solve this problem of scalable alignment.
But that's the first challenge.
Then I think we have a broad category, which you might refer to as sort of the governance
challenge, which is assuming we technically are able to sort of steer these powerful AI systems.
As we wish, how can we then make sure that we humans use this powerful technology predominantly
for positive purposes?
So not to wage war against each other or to oppress each other or for other nefarious ends,
but like broadly speaking to create prosperity and health and happiness and so on.
This is like, this is a broad category, but it's more a political challenge than a technical
challenge.
But we need to do at least reasonably well on that, I think, to have anything that could be
called a utopian future.
And then a third fundamental challenge has so far received much less attention.
But I think ultimately it's really important as well, which is not just how can we make
sure the AI's don't harm us, and not just how can we make sure we don't harm each other
using AI tools, but also how can we make sure we don't harm the AI's.
I think we will build increasingly sophisticated digital minds, and some of those might attain
various levels of moral patience, moral status, so that it matters also how well the future
goes for these potentially sentient minds that we're building.
So those are three big challenges.
Now, I think obviously for like sort of the corporate leader, these are more sort of
existential challenges for humanity, rather than something that on a day-to-day basis
is relevant in a corporate setting.
And I think the more near-term challenges we have with intellectual property, with
like access to these AI systems with a reliability, I think soon we will have
increasingly sophisticated social companion bots and a whole sort of ethical issues arising from
that, making sure that they don't enable users to do nefarious things like creating, you know,
biological weapons or cyber crime tools and such are kind of issues that already arise with the
current generation of AI systems.
Yeah, the, you know, just building on what you're saying, you know, from a corporate context,
one of the foundations is, you know, to have that deeper dialogue on the purpose and the circular
economy, and really what they want to achieve, what they want to be known for, that's one of
the areas that the gap is probably not having that time reflection, that purposeful discussion.
And you discussed purpose a lot in your book, which was really great to see. And I think that
kind of dialogue is really what's needed in the board rooms to really look at the use cases and
are we building a better world because of these efforts and the design practices. And, you know,
so often it's, you know, wanting to get on the, wanting to get on underway with some initiatives,
and then all of a sudden these initiatives blossom, and people haven't had that, those
deeper conversations. And the key word I think you mentioned was an alignment, which is really,
really key. We don't have the alignment efficiency structures yet, operationally moving in most
of the organizations that we see, whether they're large global enterprises, you know,
they're still wrestling a lot with data complexity issues and machine learning ops, and obviously
now with the new EU Act coming in and influencing other structures, people are starting to intensify
their audit controls, right, and making sure that, you know, things are more buttoned down for what's
to come. Let's go on and continue our discussion, but thank you for sharing the, you know, those
underpinnings. One of the things that I was quite intrigued by your book was you introduced a three
factor model. And I really love the structure of that model. And can you explain a little bit
about what you were trying to do as a result of that framework? Yeah, so this is not my original
model, but in growth economics, it's common to distinguish on the one hand, labor as an input
to economic production, and then capital, these are sort of the factories and machine tools,
et cetera. And then a third, what is referred to as land, but really encompasses any resource that
we can't make more of. And so those three broad categories of inputs then combine in the economy
to enable the outputs that we see. And so one can then contemplate in this context where we have
increasingly advanced AIs, what happens to economic growth in these scenarios. And in
effect, what you will have is these machine minds becoming to an increasing degrees substitute for
labor. Like in the limit, you will be able to do everything that human workers can do, but do
it cheaper and better through AIs and robots. So the value of labor then declines, right? And then
initially, that might be a scarcity of certain forms of capital, in particular, the hardware to
run all of these AIs, the data centers, the Nvidia chips, we're already seeing a kind of
shortage of Nvidia chips and reflected in the market cap. But ultimately, we can make more of
those. And then in the longer run, the constraint will be land or these resources that you can't
make more of. And yeah, so it's in the context of this discussion of what will happen to average
income and labor productivity as this transition to the machine intelligence era unfolds and
the different dynamics that can impact that. Yeah, Nick, also in a lot of your thinking,
you know, to me, it's getting people to really think about the scenarios, right? So you've been so
focused on advising people on the job impacts, right? And the sheer magnitude and you think you
stretch out and all of a sudden, you know, if we get to the point where 90% of, you know, I don't
know if we can get to Nirvana, you know, I think our legal system will probably prevent full Nirvana
because you can't necessarily sue an algorithm, right? Although, you know, who knows when you think
about, you know, this incredible, incredible sentient intelligence. It really does make you think,
right, around the elasticity and the balancing that's required at an international level. And,
you know, we continually have good actors and obviously bad actors, right? Next slide, Jen.
Let's move along. Just one of the things that is, I think, underpinning your book is
to get to the deeper utopia as you talk about the technological maturity across a number of
different technology capabilities, right? And it was interesting. I had a conversation not too long
ago in a session that was done where you, sorry, the chief data scientist from META was there.
And he said, you know, we're still, even though we're maturing, AI is still, you know,
very similar to the intelligence of a cat. You know, so we do get these variances, you know,
in terms of precision of thinking on different views from different experts. How long do you think
it's going to take? Do you have a sense in your own intuition, Nick, and also what you're seeing
broader in the marketplace of when these technology advances are going to get to the point where,
you know, we're getting moving into incremental improvements, right? Which is what you highlighted
in the book, right? You kind of reach a point and then the magnitude of the growth of those
investments start to slow down. Yeah, I think there is a lot of headroom still, space above us in
the space of cognitive capability that machine intelligence will be able to explore. I don't
think we are anywhere near the sort of the maximally efficient way of processing information
that is physically possible in our universe. And even if we were, it would only be within the limits
of a human skull. And in principle, you could have much larger computational systems.
I do think in the long run, and this becomes relevant when one is thinking about sort of the
ultimate outcomes and the deep utopia, et cetera. At some point in the future, if things go well,
our civilization will start to approximate a condition of a technological maturity,
which would be a state where you have already developed basically all general technologies
that are useful, and that are physically possible. And I think we might never quite reach there,
but I think we will get close. And possibly this might happen not that long after we have
superintelligence, because once we have superintelligence, it would then be doing the further
research technological development, right, but on digital time scales, rather than on the biological
time scales that we operate on. So you might have, with the development of superintelligence,
telescoping of the future, where all those inventions that we might have been able to make
in the fullness of time, in thousands of years of continued work on this, they might happen
quite quickly after you have superintelligence. And then you might have this approximation to
technological maturity. It's interesting, you mentioned like, well, you said maybe our AISRS
intelligent as a cat, I think as we move closer to AGI, artificial general intelligence,
it starts to look less like a point, and more like a richer multi dimensional space. So right
now we have, I mean, I don't know what cats, you know, but the ones that I have encountered
have not really been that good at writing Python scripts and doing these other things that our
current large language models can do. So in some sense, I think current systems are
like superintelligent compared to cats. On the other hand, I think cats still have better sort
of motor dexterity and ability to like survive in a natural environment than our current sort of
robot incarnated AI systems have. It's also interesting to think in terms of just from a
kind of processing power perspective, it might well be that the largest neural networks we
currently have that undergird the performance of such EPT4, et cetera, are roughly comparable in
terms of the number of parameters to the number of synapses in a cat brain, you know, give or
take some order of magnitude. But it does look like we've seen the performance of these AI systems
scale roughly in proportion to sort of the parameter count and the amount of compute used
in training and running these, which is one of the reasons for suspecting that we might see
continued rapid improvement over the next few years as we keep scaling these systems up.
If their performance, if their intelligence grows correspondingly, we were kind of soon
approaching full human equivalence. Yeah, it's, you know, I'm sure you're following Dr. Jeffrey
Hinton's concerns about what we've unleashed. And, you know, what you're trying to, I think,
also communicate is we will have incredible power about how we use it is going to be what's
really, really key, right? It's interesting to see, you know, him really speak out a lot more about
the risks that we have. Yeah. Yeah, so he's one of the, like, the, I guess, the fathers of
deep learning revolution. And there has been this quite remarkable shift back when Super
Intelligence was published, all these questions about potential existential risks from AI and
alignment that were completely ignored, like nobody in academia was taking them seriously.
There were like a handful of people on the internet who were trying to start to think about them,
but basically people dismissed it as sort of future is a more science fiction. And in the
intervening years, there has been this complete reversal of that where now it's a very, very
mainstream view that AI alignment is hard and could potentially post existential risks and that
the whole AI transition will be transformative. And people like internal coming out and then
expressing concern and other, like Joshua Benjo is like another one of the sort of fathers of
deep learning, similarly, and many other leading AI leaders. So it's been quite interesting just
as a kind of participant in this space, just to see how ideas that at one point are like
completely outside the overturned window and regarded as very fringy or a kind of speculative
or silly sort of just over a few short years become completely mainstream. Yeah, it's a very
good observation. But it's good, you know, I guess part of the question is, can we move fast enough?
Right? You know, you look at the accelerated cybersecurity innovations that are going on
right now and deep fakes and, you know, can we curb some of these activities? Lots and lots
of concerns just on the incredible growth of the dark web as well. If we go on to the next
question, Jen, if you don't mind, this one was interesting. I suspect that this was your formula
logic. Nick, I don't know what the roots were, you know, here, but, you know, I love the fact that
you were putting together a structure, you know, in terms of purpose and the meaning of life and,
you know, sort of the correlations and, you know, putting that structure together, you were really
trying to communicate something I think that was pretty profound. So I just love to hear the story
because I'm always intrigued when people start putting some structure, mathematical structure
together and to guide. So just to get a voiceover would be great.
Yeah, so the context here is an exploration of the values that we could realize
in this condition of technological maturity and in the solved world. What human values are possible,
what kind of lives would be actually make sense in this transformed condition? Because a lot of
the things that currently sort of furnish our lives would become completely unnecessary in a
solved world. Like a lot of our hours now are taken up by doing economic labor. Like, you know,
we go into work while some of us, I guess, work from home. But, you know, in order to get the
paycheck and then even in the non-economic part of our lives, there's a lot of stuff we do because
we have to, you know, we brush our teeth because we don't want to have tooth decay, you do a laundry
because you need clean clothes, you drive the kids to school because they need to get there,
like all kinds of stuff. That occupies a very large fraction of our waking hours. But if
machines could do all of these tasks better, then there is the question of what remains.
Now, some plausible human values could definitely be realized to a very high degree in this
condition of a solved world. And it's kind of trivial that they could be. So, for example,
pleasure and enjoyment are like affordances that where there is no incompatibility with living
in a solved world. And in fact, you would have much better means of inducing pleasure and enjoyment
through like some like super drugs or direct brain manipulation, et cetera. But certain other human
values look like they might potentially be undermined. And in particular, certain values
having to do with purpose and significance, interestingness and meaning. And so, the book
goes into more depth in trying to analyze for each of those values, whether they actually
could be instantiated in this condition of technological maturity, where in some sense,
human labor is not needed for anything, like our instrumental efforts more generally
is unnecessary. And then what gives us purpose. And so, one of these concepts is the concept
of meaning in life. And that's what this particular bit pertains to, where I have a lot of conceptual
analysis of this notion of meaning in life. That is, yeah, it's kind of summarized there. I
will probably take us too long to fully explicate that. But it's like a little analysis of what we
might be referring to when we speak of a life being meaningful or lacking meaning or the value
of having meaning in your life. As distinct from just sort of feeling subjectively a sense of purpose,
which is something that you would be able to induce in this condition of technical maturity,
like you up your dopamine levels or whatever, and you all feel kind of supremely engaged and
motivated. But some philosophers think that there's more to meaning than that. It's not
just a subjective state. It's also like have more objective elements. And this little
explanation is an attempt to get clarity on that so that one can then ask the question of whether
this value of meaning would be possible in a solid world. Maybe you think ahead and you think
of our educational systems, right? And the need to build deeper foundations.
You know, there's lots of concerns on the decline of critical thinking, right? The detention deficit
realities, right? Always being plugged in. And do we have a dialogue to strengthen the human condition
and the point about maybe there is a cornerstone that has to flow through everything, you know,
in terms of how to parent, you know, the dialogue to have all the little ones and all this, you know,
because there'll still be learning going on even in this perfect world. But what the dialogue is
about and stimulating the deeper thinking that that's, you know, humans are good at. I liked
in some ways that it's a bit scary. And I think about this multi-dimensional, all-knowing,
intelligent being, right? Which is theoretically very possible, right? Because the more and more
we feed these AI models, we know how the leaps that they can take forward, right? And as we amplify
multiple methods, which we've seen, right? You know, and so when we were talking about
trillions and trillions of parameters, there's no way that humans will be able to outperform,
right? Just at the speed that the trajectory is moving. But getting people to understand
and have a vision for their own life and how they might want to live it is,
you know, I guess philosophy needs to be mainstream. You know, maybe that's where
everything will go, right? Into the philosophical domain and the dialogues that are needed.
So we kind of curtail, you know, when I look at some of the declining human comprehension rates,
you know, if you get into the deep math skills, right? And the general aptitude of the general
public on math is an example. Anyways, that fascinating conversation, you know, I think that
the unifying point for our viewers is purpose and meaning might be the most significant foundations
to preserve humanity. Yeah, or at least one part of it that we, yeah, want to keep track of.
Yeah, very, very true. And Jen, you can go to the last question. I found this quite interesting,
where you declare in the book, I think it was a few times that you think you're a pessimist.
And for me, when I read this book, I really felt I don't think of myself as a pessimist. I don't
think I declare. It's okay. I'll find the right page. But yeah, maybe you have caught me out.
It says I am a pessimist, but that's okay. You know, just, you know, let's just focus in on
the real question was, do you think humanity will really achieve utopia versus slip into a
dystopian reality? Or do you think we're going to, you know, see a bit of both of these realities
before there's a equilibrium, you know, I think through generations of humans, there's always
been the good, there's been the bad, and there's been the real ugly, and those forces are kind
of universal dynamics. I think the jury is still out. Whereas so far, it's kind of
been a mixed bag through human history. I mean, a lot of bad stuff, and a decent amount of good
stuff as well. But like, the human life is what it is. It's not like, but I think it's likely to
or somewhat likely to shake out so that we'll get like either something extremely good or
something very bad, and less likely the longer the time scale you're considering that we will keep
have this mixed bag. But which way it will shake out, I think is very much an open question.
Yeah, that's a bit alarming, right? Which way the pendulum will really, really swing.
Do you think the legislation is going to help? Or do you think, you know, it's going to take
every, you know, large technology organization to really help lead us because they have so much
power. They're developing these capabilities, right?
It's hard to say. I think there certainly will be places where regulation will be needed,
but figuring out exactly what the right form of that is and the right points during the development
at which to introduce it is a really complex question. And I don't really have all the answers
to that. I am thinking about it, but there are so many considerations on different sides of this.
It's a very strategically difficult issue to consider. I think that broadly speaking,
it would seem desirable if there were the opportunity at some critical point in the
development, like say just when we are figuring out how to create superintelligence.
If it were possible for whoever is actually doing this, whether it's some particular lab or
some national project or whatever, if they had the ability to go a little bit slower during those
critical stages, maybe to, you know, pause for six months or to rather than immediately
cranking all the knobs up to 11, like doing it incrementally. And so you could observe what
happens and like check that the safeguards are working as intended. I think that would seem
preferable to a situation where you have, you know, maybe 15 different competitors all racing to
get there first. And whoever decides to be a little bit cautious just immediately fall behind
and becomes irrelevant. Like that, that would seem to be a sort of risk maximizing approach
to developing superintelligence. So I think having some ability for frontier leading edge
developers to coordinate at the relevant time to sort of possibly slow down a little bit
would be valuable. On the other hand, I'm also a little bit concerned about us ending up on some
trajectory where we closed doors on AI forever. It doesn't look likely given the current situation,
but it looks more likely than it did just two years ago, given how much the conversation has
changed. And there are now these sort of AI doomers who are advocating for AI pauses or
bands and regulations. And I think that we probably need more regulation and oversight than
we currently have. But I would want it to sort of stop at the optimal point, whereas I think
there is a risk that once you get into this game, it kind of overshoots the target.
And you could get the runaway stigmatization of AI that results in some sort of permaban,
low probability, but increasing probability. So it's like an avalanche, like it's easy,
you can trigger an avalanche, right? But once it gets going, you can't really stop it,
like it runs its course. So some of these questions are kind of weighing on my mind,
and I haven't fully made up my mind yet where I think things should be pushed.
Yeah, I can't imagine the technology leaders slowing things down. It would be other forces,
right? And you see the drama that's gone on with open AI. And obviously, you've got some
innovation with Mistral, which is, you know, actually a really good performing large language
model. We've been doing a fair bit of analysis across the different LLMs. One of the things
that Amazon's done with their bedrock infrastructure, it makes it very easy
to plug in different data sets and then look at how the different models are performing.
And, you know, sometimes bigger isn't necessarily better, right? And the number of
parameters, you know, these things are very use case specific. I like the fact of
people coming together, right? It goes back to the old, you know, intellectual property
discussions, right, of how much people are willing to share to really do the deeper thinking
and create their safeguards, you know, that I think these are very significant board director
dialogues. We do see the governance structures, you know, shifting in terms of director
liability insurance, right? So you can, the insurance and the reinsurance
organizations are shifting and building new, shifting their augmenting additional policies
to protect their clients. Because I think we can see the court cases, you know, are going to ramp
and how these impacts will be with all the courts way on open AI right now. I think has
a lot of people wondering kind of to your point, will that, you know, start to really create an edge,
a rougher edge and then declining willingness to work with certain organizations, you know.
I think that will be like increasing pressures from different directions being brought to bear
on leading AI developers. Like there are big stakes and different people have different
interests in this and will try to push them from various sides to change their behaviors. And I
think it will become a fraught area with like no easy path that avoids controversy. And so,
yeah, it's got to be a hard job to be the leader of any one of these AI labs in the coming years.
Yeah, I almost wish that Nick, all of the scientists and the CTOs have a mandatory course
with you to really get into their rewire some of their psyche. I think, you know,
in it known, I mean, here that in many of the courses that CIOs or CTOs or chief data scientists
take, you know, they take a basic course in AI ethics, but it's not a real deep, deep, deep,
deep, deep discovery philosophical foundation. And they, you know, it's hard to keep up with
other practices, but how important it is because they're really shaping the future for all of us
in many, many dimensions. Thank you so much, Nick, for joining us today. It's just been a real pleasure.
I know our viewers will enjoy reading Deep Utopia. I can hardly wait to write up my article on it.
And just thank you so much for all of your generous community, you know, your generous
contributions to making our world a better place, not being afraid to expose the real
questions that we really need to think deeply about and to create really a world that's got
more purpose and, to use your words, far more meaning. And it is in reach, but what we do and
on all aspects of so many integrated disciplines to come together is really,
it's really a deep, deep conversations. And I think the word you chose deep was a very
appropriate title in front of Utopia. It's aspirational, at least. Very aspirational. But,
you know, it's better to paint a world that has a possibility than a world that,
use your words, is all gloom and doom, right? Yeah, yeah. I mean, I've enjoyed the conversations
in there. I mean, I feel we're still very much like, I feel like a little child trying to understand
this much bigger world that we've never seen before with the AIs coming online and the fundamental
transformations that will lead to. Yeah, I don't think anybody has all the answers here,
but at least we can start to try to search for what the right questions are to ask.
Very right. It is the right questions. And that begins the exploration. And maybe that's one of
the most important things that humans need to really protect is not being afraid to think hard
and asking the right questions, right? That leads us to make a change or to think about
doing something different. Thank you so much, Jen. Thank you. Yeah, thanks, Nick. Pleasure to connect
and discuss all of this together. And thanks to our listeners for joining us on the Predictive
World Series by Sales Choice. I invite them to go to saleschoice.com slash resources for more and
join us next time.
