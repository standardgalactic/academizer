Designing Robots with LLMs
Open sidebar
Designing Robots with LLMs
Model: Default (GPT-3.5)
Summarize: CAN LARGE LANGUAGE MODELS DESIGN A ROBOT? 
Authors: 
*[1,2] Francesco Stella, francesco.stella@epf l .ch, 
[2,3] Cosimo Della Santina, c.dellasantina@tudelft.nl 
[1] Josie Hughes, josie.hughes@epf l .ch 
Af i liations: 
[1] CREATE Lab, EPFL, Lausanne, Switzerland. 
[2] Department of Cognitive Robotics, Delft University of Technology, Delft, The Netherlands. 
[3] Institute of Robotics and Mechatronics, German Aerospace Center (DLR), Wessling, Germany. 
*Corresponding author 
Abstract.Large Language Models can lead researchers in the design of robots. 
Large Language Models (LLMs) [1], are revolutionizing the i eld of robotics, providing robots with the ability to 
understand and process natural language at a level previously thought impossible. These powerful AI tools have the 
potential to improve a wide range of tasks in robotics, including natural language understanding, decision making, 
and human-robot interaction. One of the key advantages of large language models is their ability to process large 
amounts of text data, such as instructions, technical manuals, and maintenance logs, and internalize an implicit 
knowledge containing rich information about the world from which factual answers can be extracted. In fact, the 
text you have just read was generated by the LLM ChatGPT-3 [2] when prompted ���Can you write an introduction 
in a newsy style to the potential of large language models in robotics?���. 
Language models have long been used in robotics to translate natural language instructions into actions executable 
by robots [3][4], synthesize code from text prompts [5] and i nd relationships between dif f erent i elds of knowledge. In 
light of these impressive capabilities, LLMs may now contribute to another bottleneck of robotics, design. Leveraging 
their emerging capabilities [6], LLMs can deliver a dialogue that enables, teaches, and guides humans in building a 
robot from scratch. These capabilities could fundamentally change the methodology by which we design robots, and 
could shift the role of humans from designer or engineer to technician. So, to what extent can ChatGPT-3 replace 
an engineer and design a robot? 
To generate the i rst ChatGPT-3 designed robot we approach the task in a two step approach.In the i rst 
high-level phase, the computer and the human collaborate on a conceptual level, discussing ideas and outlining the 
specif i cations for the robot design while in the second phase the physical implementation of the design specif i cations 
takes place. As an example of this AI-driven design process, we consider the challenge of a human engineer driven 
by the desire to ���help the world with robotics,��� as shown in Figure 1. The human operator starts by asking the 
LLM which are the future challenges for humanity and promptly gets an overview with a clear outline of the main 
hazards. The human can then select the option they are most interested in and narrow down the design space by 
asking for clarif i cations. This interaction can span multiple i elds of knowledge and levels of abstraction, ranging 
from concepts to technical implementation. In this way, the human can spot new intersections between research 
i elds, such as agriculture and robotics, and consider factors that are hardly part of the experience of an engineer 
by training, such as what is the crop that is economically most valuable to automate. By iterating this process, the 
LLM and the human converge to the technical design specif i cations of a robotic system.
1 / 1
ChatGPT
ChatGPT
This paper discusses how Large Language Models (LLMs), such as ChatGPT-3, can be employed to aid in the design of robots. The authors suggest that these LLMs are revolutionizing the field of robotics by providing abilities such as natural language understanding, decision-making, and improved human-robot interaction. They are capable of processing large amounts of text data, such as instructions and technical manuals, thereby possessing an implicit knowledge base from which factual answers can be extracted.
The authors propose that LLMs can also assist in the design process of robots, delivering dialogues that guide humans in creating robots from scratch. This could transform the methodology of designing robots and shift the role of humans from being designers to technicians.
In the process of building a robot, the authors describe a two-step approach involving collaboration between the computer and the human. In the first phase, they discuss ideas and outline the design specifications. In the second phase, the physical implementation of the design occurs.
As an example, they discuss a scenario where a human engineer aims to "help the world with robotics". They interact with the LLM, which provides an overview of future challenges. The human can select an option they're interested in and further narrow down the design space by asking for clarifications. The interaction can span multiple fields of knowledge and levels of abstraction, enabling the discovery of new intersections between research fields, such as agriculture and robotics. By iterating this process, the LLM and human can converge on the technical design specifications of a robotic system.
1 / 1
Typically, in a computational design framework, the computer solves technical problems specif i ed by the human. 
In this case, conversely the LLM proposes conceptual options to the human, who then selects the most appealing 
choice. In this sense, the LLM acts as the researcher, leveraging knowledge and i nding interdisciplinary connections, 
while the human acts as a manager, providing direction to the design.The application is selected as an output of 
Date: December 2022. 1 
arXiv:2303.15324v1 
[cs.RO] 
15 
Mar 
2023 
2CAN LARGE LANGUAGE MODELS DESIGN A ROBOT? 
How can robotics 
support agriculture? 
Food supply 
Aging population 
Climate change 
What are the future 
challenges for 
humanity? 
20 
20 
30 
40 
50 
10 
10 
0 
20 
40 
60 
80 
100 
120 
0 
��C��F 
Crop harvesting 
Parasite control 
Packaging 
Navigation 
Gripping 
Compliant interaction 
Ef i ciency 
What features 
should a robot 
harvester have? 
Large Language 
Models 
OvalScoop 
In controlEmbodied 
f(x) 
What shape should the 
gripper have ? 
How should the compliance 
be included? 
ActuatedPassive 
How should the gripper interact 
with the tomato plant? - ���Manufacturing 
process selection 
���CAD design support 
���Material selection 
���Size specif i cations 
���Silicone selection 
���Guidance on the 
molding process 
���Silicone PLA 
connection 
���Motor selection 
���Python code 
generation 
���Pulley system design 
specif i cations 
High level 
design 
discussion 
Technical 
implementation 
Design 
specif i cations 
Real-world 
evaluation 
Biology 
Neuroscience 
Economics Figure 1.On the left, the two phases of the design process: i rst the human and LLM discuss the specif i cs 
application and of the design and later the human implements them. On the right, a pictorial overview of 
the discussion, with the questions prompted by the human on top, and the options provided by the LLM 
below. The green color highlights the decision tree of the human, which gradually focuses the problem to 
match his goal. 
this i rst part of the process, and a set of initial technical specif i cations are generated. This includes code, material, 
components, manufacturing method selection, and mechanism design. In the second, low-level phase of the design 
process, these directions need to be translated into a physical and functioning robot. Although LLMs can currently 
not generate entire CAD models, test code, or automatically fabricate the robot, recent advances have shown that 
AI algorithms can support the technical implementation of software [7], mathematical reasoning [8], or even shape 
generation [9]. Thus, we expect that, in the near future, AI-generated inputs will highly support a large set of tech- 
nical tasks. However, in the foreseeable future, humans will remain mainly in charge of the technical implementation 
of the robotic solution. The human is therefore relegated to the technician role, polishing the code proposed by 
the LLM, i nalizing the CAD, and fabricating the robot. This robot can then be tested in real-world scenarios, and 
a new conversation with the LLM can be used to iterate on the design process in light of experimental evidence. 
As an example of this second phase, Figure 2 displays the main outputs generated by the LLM and the real-world 
deployment of the AI-designed robotic gripper for crop harvesting.
1 / 1
ChatGPT
ChatGPT
The paper outlines a computational design framework where, unlike traditional methods, the Large Language Model (LLM) proposes conceptual options for the human, who then selects the most suitable choice. In this way, the LLM acts as the researcher, using its extensive knowledge to find interdisciplinary connections, while the human behaves more like a manager, guiding the direction of the design.
In the initial high-level design discussion, the LLM helps to determine the application and generate preliminary technical specifications, such as code, material, components, manufacturing method selection, and mechanism design. During the second phase of the design process, these guidelines are translated into a physical and operational robot.
While LLMs currently cannot generate entire CAD models, test code, or fabricate the robot automatically, advances in AI algorithms have shown that AI can assist in technical implementation tasks such as software development, mathematical reasoning, or shape generation. It's expected that AI-generated inputs will increasingly support a broad array of technical tasks in the future.
However, for the foreseeable future, humans will remain primarily responsible for the technical implementation of the robotic solution, fulfilling the role of a technician who fine-tunes the code suggested by the LLM, finalizes the CAD, and builds the robot. The constructed robot can then be tested in real-world scenarios, and further discussions with the LLM can be used to iterate on the design process based on experimental evidence.
The paper provides an example of the second phase with the design of an AI-driven robotic gripper for crop harvesting, displaying the main outputs generated by the LLM and the real-world deployment of the designed gripper.
1 / 1
From this exploration we can foresee dif f erent modalities of human-AI interaction and collaboration. At one ex- 
treme, the LLMs could provide all the input required for robot design, which the human follows blindly. The AI is 
the inventor, addressing human questions and providing ���creativity���, technical knowledge and expertise, whereas the 
human deals with the technical implementation. This could indirectly foster transfer and democratization of knowl- 
edge, by enabling non-specialists to realize robotic systems. A more moderate, yet powerful approach is collaborative 
exploration between the LLM and the human, leveraging the ability of the LLM to provide interdisciplinary and 
wide ranging knowledge to augment the human���s expertise. Finally, we can consider a third approach in which the 
LLM acts as a funnel, helping to ref i ne the design process and providing technical input whilst the human remains 
the inventor or scientist involved in the process. This collaboration between AI and humans presents clear benef i ts 
and opportunities. By augmenting human knowledge with LLMs, this methodology removes the limits imposed by 
the learning process and supports the human in i nding relevant connections between i elds, making interdisciplinary 
research and reasoning more accessible. It can spur the curiosity of researchers, interactively teach new robotics 
engineers, and accelerate the design process. As seen in our demonstration, the relationship between human and AI 
may vary for dif f erent parts of the design process depending on the skill and expertise of the individual and the goal 
of the robotic design process. At the same time, the introduction of LLMs into the design of robots brings questions 
CAN LARGE LANGUAGE MODELS DESIGN A ROBOT?3 
Real-world solution 
"One possible shape for a gripper that is 
designed to grasp tomatoes could be a 
round or oval shape that is slightly larger 
than the tomato" 
"Overall, a Dynamixel motor is 
suitable for actuating a gripper for 
picking tomatoes" 
"One way to build a gripper for handling 
crops would be to use silicone or rubber, 
which are both sof t and fl exible, helping to 
reduce the risk of crushing the crops" "One way to connect the oval shape 
of a gripper to a motor would be to 
use a gear or pulley system. Th is 
could involve at t aching a pulley to 
the output shaf t of the motor, and 
then connecting the gripper to the 
pulley using a linkage such as a 
belt, chain, or cable" 
"Here is an example of 
how you might use Python 
to control a Dynamixel 
motor" 
import pyDynamixel 
dynamixel = 
pyDynamixel.Dynamixel(port="COM3") 
dynamixel.set_torque_limit(1.0) 
dynamixel.set_goal_position(0.0) 
Figure 2.An AI designed this robotic gripper. 
regarding its potentially negative ef f ects on scientif i c disciplines and engineering- a creative, interdisciplinary, and 
IP-creating process that currently relies on highly-skilled professionals. In this regard, it is pivotal to point out 
that LLMs should be regarded as an evolution of search engines, generating the ���most probable��� answer to a given 
prompt [10]. As such, it is debatable if they can develop creative solutions that substantially advance the robotics 
discipline beyond what is already known by the scientif i c community. But unlike search engines, LLMs can propose 
ways to integrate ���knowledge��� and apply it to unseen problems, thus providing a potentially false impression that new 
knowledge is being generated. Moreover, we see another potential issue in the widespread use of LLMs in our i eld. 
As the same trained model is accessible to everybody, it could create a bias in researchers��� focus toward solutions 
that the model statistically prefers. This way, it may hinder the exploration of new technological solutions. Finally, 
this ability of the LLM to apply and adapt prior experience to new problems could prevent humans from taking 
responsibility for the solutions developed [11], which could lead to dangerous outcomes and a lack of human creativity 
in the design process. This could prohibit and stagnate the advancement of new robotic technologies and designs. 
There are also signif i cant societal and ethical implications resulting from human-AI interactions for robot design. 
LLMs could automate high-level cognitive design tasks, and have humans focusing on more technical jobs. This could 
redef i ne the set of skills that are required by an engineer, and change the education that engineers should receive. 
Finally, there are key issues regarding plagiarism, traceability and IP [12]. Can a design created via LLM be consid- 
ered to be novel as it builds only on prior knowledge [13], and also how can this previous knowledge be referenced? 
Similarly, if human-AI collaboration leads to the creation of novel IP, is this not a function of the training data 
of the LLM? As this technology matures there are also longer term considerations including data-privacy, the fre- 
quency of retraining and how new knowledge should be integrated to maintain the usability and relevancy of this tool. 
To conclude, the robotics community must identify how to leverage these powerful tools to accelerate advances 
and capabilities, yet doing so in an ethical, sustainable and socially empowering way. We must develop means of 
acknowledging the use of LLMs [14], and also being able to trace the lineage of the generation of designs from LLM. 
Looking forward, we strongly believe that LLMs will open many exciting possibilities and that they will be a force for 
good if opportunely managed. The design process could be fully automated by combining collaborating LLMs to ask 
and answer questions, with one helping to ref i ne the other. This could also be augmented with automated fabrication 
to allow for a fully autonomous pipeline for the creation of bespoke and optimized robotic systems. Ultimately, it 
is an open question for the future of this i eld if these tools can be used to assist robot developers and leverage 
inter-disciplinary knowledge leading to new robotic capabilities, or does this lead to a long-term stagnation of the 
i eld, with lazy, unskilled engineers, relying on external computation to generate new knowledge?
1 / 1
ChatGPT
ChatGPT
The paper concludes by presenting different ways in which humans and AI can collaborate, foreseeing several potential modalities of interaction.
AI as Inventor
: At one extreme, LLMs could provide all the necessary inputs for robot design, with humans solely responsible for the technical implementation. This would democratize knowledge by enabling non-specialists to build robotic systems.
Collaborative Exploration
: A moderate approach sees humans and LLMs collaborating closely, with the LLM's wide-ranging knowledge augmenting the human's expertise.
LLM as a Funnel
: In a third approach, the LLM helps refine the design process and provides technical input, while the human remains the primary inventor or scientist involved in the process.
The paper emphasizes that this form of collaboration could significantly enhance interdisciplinary research and reasoning by breaking down the barriers imposed by the traditional learning process. The LLM can help humans make connections between different fields, fostering curiosity, educating new engineers, and accelerating the design process.
However, the authors also caution about potential negative effects. They point out that LLMs, which function as an evolution of search engines, might not develop truly innovative solutions. LLMs could create biases in research, leading to a focus on solutions the model statistically prefers, thus hindering the exploration of new technological solutions. A reliance on LLMs could also lead to humans not taking responsibility for the solutions developed, which could stifle creativity and hinder the development of new robotic technologies.
They also highlight significant societal and ethical implications. With LLMs automating high-level cognitive tasks, the skills required by engineers could change dramatically, impacting engineering education. Questions of plagiarism, traceability, and intellectual property also arise: Can a design created via an LLM be considered novel since it builds only on prior knowledge, and how should this previous knowledge be referenced?
In the long term, data privacy, the frequency of retraining, and the integration of new knowledge to maintain the usability and relevance of the tool should be considered.
To leverage LLMs responsibly, the authors suggest developing ways to acknowledge their use and trace the lineage of designs generated by them. They foresee LLMs potentially automating the entire design process, from asking and answering questions to automated fabrication. However, they leave open the question of whether this could lead to new robotic capabilities or cause long-term stagnation with engineers overly reliant on computation to generate new knowledge.
1 / 1
References 
[1] Brants, T., Popat, A. C., Xu, P., Och, F. J. & Dean, J. Large language models in machine translation (2007). 
[2] Floridi, L. & Chiriatti, M. Gpt-3: Its nature, scope, limits, and consequences. Minds and Machines 30, 681���694 (2020). 
[3] Ahn, M. et al. Do as i can, not as i say: Grounding language in robotic af f ordances. arXiv preprint arXiv:2204.01691 (2022). 
[4] Huang, W., Abbeel, P., Pathak, D. & Mordatch, I. Language models as zero-shot planners: Extracting actionable knowledge for 
embodied agents. In International Conference on Machine Learning, 9118���9147 (PMLR, 2022). 
[5] Budzianowski, P. & Vuli�� c, I. Hello, it���s gpt-2���how can i help you? towards the use of pretrained language models for task-oriented 
dialogue systems. arXiv preprint arXiv:1907.05774 (2019). 
[6] Wei, J. et al. Emergent abilities of large language models. arXiv preprint arXiv:2206.07682 (2022). 
[7] Chen, M. et al. Evaluating large language models trained on code. arXiv preprint arXiv:2107.03374 (2021). 
[8] Alpha as the way to bring computational knowledge superpowers to chatgpt (2023). https://writings.stephenwolfram.com/2023/ 
01/wolframalpha-as-the-way-to-bring-computational-knowledge-superpowers-to-chatgpt/. Accessed: 2022-12-30. 
[9] Ramesh, A., Dhariwal, P., Nichol, A., Chu, C. & Chen, M. Hierarchical text-conditional image generation with clip latents. arXiv 
preprint arXiv:2204.06125 (2022). 
[10] Shanahan, M. Talking about large language models. arXiv preprint arXiv:2212.03551 (2022). 
[11] Stokel-Walker, C. Chatgpt listed as author on research papers: many scientists disapprove. Nature . 
[12] Ai systems can���t patent inventions, us federal circuit court conf i rms (2022). https://www.theverge.com/2022/8/8/23293353/ 
ai-patent-legal-status-us-federal-circuit-court-rules-thaler-dabus. Accessed: 2022-12-30. 
[13] George, A. & Walsh, T. Can ai invent? Nature Machine Intelligence 1���4 (2022). 
[14] Lee, J. Y. Can an artif i cial intelligence chatbot be the author of a scholarly article? science editing 10, 7���12 (2023).
1 / 1
ChatGPT
ChatGPT
This article appears to be based on the following references:
Brants, T., Popat, A. C., Xu, P., Och, F. J. & Dean, J. "Large language models in machine translation" (2007). This paper likely discusses the use of large language models in the field of machine translation.
Floridi, L. & Chiriatti, M. "Gpt-3: Its nature, scope, limits, and consequences" in Minds and Machines 30, 681���694 (2020). This paper probably provides an in-depth analysis of GPT-3, its capabilities, limitations, and potential impacts.
Ahn, M. et al. "Do as I can, not as I say: Grounding language in robotic affordances" (2022). This arXiv preprint likely explores the concept of grounding language in the context of robotics.
Huang, W., Abbeel, P., Pathak, D. & Mordatch, I. "Language models as zero-shot planners: Extracting actionable knowledge for embodied agents." This paper probably discusses the role of language models in planning and decision-making for embodied agents such as robots.
Budzianowski, P. & Vuli��, I. "Hello, it���s GPT-2���how can I help you? towards the use of pretrained language models for task-oriented dialogue systems." This work likely explores the potential of using pretrained models like GPT-2 in dialogue systems.
Wei, J. et al. "Emergent abilities of large language models." This paper probably discusses new capabilities discovered in large language models.
Chen, M. et al. "Evaluating large language models trained on code." This arXiv preprint likely evaluates the performance of large language models when trained on code.
Stephen Wolfram's blog post discussing how Wolfram Alpha can enhance ChatGPT's computational knowledge.
Ramesh, A., Dhariwal, P., Nichol, A., Chu, C. & Chen, M. "Hierarchical text-conditional image generation with clip latents." This preprint probably discusses the generation of images conditioned on textual inputs using CLIP latents.
Shanahan, M. "Talking about large language models." This work likely discusses the discourse surrounding large language models.
Stokel-Walker, C. An article in Nature on the controversy surrounding the listing of ChatGPT as an author on research papers.
An article on The Verge discussing a court ruling that AI systems can't patent inventions.
George, A. & Walsh, T. "Can AI invent?" in Nature Machine Intelligence. This paper likely discusses whether AI can be considered capable of invention in the legal and ethical sense.
Lee, J. Y. "Can an artificial intelligence chatbot be the author of a scholarly article?" This paper likely discusses the ethical and legal issues around AI authorship of scholarly articles.
1 / 1
A Survey on Causal Discovery: 
Theory and Practice 
Alessio Zanga1and Fabio Stella1 
1Department of Informatics, Systems and Communication, 
University of Milano- Bicocca, Milan, Italy 
May 18, 2023 
Abstract 
Understanding the laws that govern a phenomenon is the core of sci- 
entif i c progress.This is especially true when the goal is to model the 
interplay between dif f erent aspects in a causal fashion.Indeed, causal 
inference itself is specif i cally designed to quantify the underlying relation- 
ships that connect a cause to its ef f ect. Causal discovery is a branch of 
the broader i eld of causality in which causal graphs is recovered from 
data (whenever possible), enabling the identif i cation and estimation of 
causal ef f ects. In this paper, we explore recent advancements in a unif i ed 
manner, provide a consistent overview of existing algorithms developed 
under dif f erent settings, report useful tools and data, present real-world 
applications to understand why and how these methods can be fruitfully 
exploited. 
1Introduction 
1.1A General Overview 
One of the mantras that is repeated in every statistical course is that correla- 
tion does not imply causation. This is also observed in several disciplines, such 
as economics [38], biology [68], computer science [33, 71] and philosophy [27]. 
Following [32], the main goal of a research study is often to assess the ef f ect, if 
any, of an action on some outcome and not measuring a mere correlation. For 
example, this is true when it comes to decision making, since deciding which in- 
tervention must be taken is not straightforward and must be addressed properly 
to avoid any potential side ef f ects. In order to identify and quantify a causal 
ef f ect, the set of tools provided by causal discovery must be used accordingly. 
Here, the i nal goal is to decompose the total ef f ect of an action into causal and 
non-causal ef f ect, removing the bias that is introduced during the estimation 
process. 1 
arXiv:2305.10032v1 
[cs.AI] 
17 
May 
2023 
Causal inference itself relies heavily on a formal description on the inter- 
actions between the observed variables, i.e.a casual graph.Such graphical 
representation is na�� ��ve in its concept, yet so ef f ective when it comes to explain- 
ability. Following [99], it boils down to connect a cause to an ef f ect (outcome) 
by drawing arrows from the former to the latter, to obtain a qualitative descrip- 
tion of the system under study. This is in stark contrast with black-box tech- 
niques, where predictions about an outcome are made with a pure data-driven 
approach. Indeed, these methods fall short both in terms of explainability and 
decision making, as stated in [8, 28, 32]. Therefore, when causality is empow- 
ered through the instrument of graphical models, it is possible to overcome the 
current limitations of machine learning and deep learning tools, enabling the 
researcher to reach a higher level of understanding.
1 / 1
ChatGPT
ChatGPT
This text appears to be the abstract and part of the introduction of a scientific paper titled "A Survey on Causal Discovery: Theory and Practice" by Alessio Zanga and Fabio Stella from the Department of Informatics, Systems and Communication, University of Milano- Bicocca, Milan, Italy. The paper was uploaded to arXiv on May 18, 2023.
The paper aims to provide a unified exploration and overview of the latest advancements in causal discovery. Causal discovery is a subfield of causality in which causal relationships within data are identified and quantified, and is fundamental in scientific research for understanding the interplay between different aspects of a phenomenon.
The authors emphasize that the tools and techniques of causal discovery are crucial to separating causal effects from non-causal effects, thus eliminating the biases that are introduced during estimation. They further underline that causal inference relies heavily on a formal description of the interactions between observed variables, which is typically represented via causal graphs. These graphical models offer an effective and intuitive way to understand complex systems.
The authors draw a contrast between causal models and black-box methods, which are purely data-driven and lack in terms of explainability and decision-making. They argue that the integration of causality into graphical models can help to overcome the limitations of conventional machine learning and deep learning tools, leading to a deeper understanding of the systems under study.
1 / 1
When the causal graph is unknown, one may recover the cause-ef f ect pairs 
by combining available data together with prior knowledge, whenever possi- 
ble. The process of learning graphical structures with a causal interpretation 
is known as causal discovery. Recently, causal discovery has gained signif i cant 
traction, especially when experimental data are available. However, this growth 
fragmented the landscape into multiple i elds that dif f er for assumptions, prob- 
lems and solutions, while aiming to the same goal. For this reason, this work 
summarizes the current status of causal discovery from both a theoretical and 
practical point of view, unifying shared concepts and addressing dif f erences in 
the algorithms made available by the specialized scientif i c literature. 
This survey is structured as follows. In Section 1, the reader is provided a 
general introduction to the causal discovery problem, along with an overview 
of previous works on the same topic. Section 2 is devoted to provide concepts, 
def i nitions and problems that are common across dif f erent approaches presented 
in the following pages. Section 3 explores the i rst set of algorithms in the ob- 
servational setting, while Section 4 relaxes the acyclicity assumption. In Section 
5, the scope is extended to cover the experimental scenario, where multiple in- 
teractions with the system of interest are taken into account. Section 6 and 7 
report respectively on evaluation techniques and on practical applications of the 
discussed methodologies. Finally, Section 8 draws conclusions about the current 
landscape of causal discovery. 
1.2Related Works 
To the best of our knowledge, six dif f erent surveys on causal discovery were 
published from 2019 to 2022.In particular, [67] acted as a meta-survey by 
checking the contents covered by the others against i ve topics, namely: theory, 
data, software, metrics and examples. A modif i ed version of this checklist can be 
found in Table 1, which was adapted for a direct comparison with the structure 
of our survey. 
While every contribution provided adequate background knowledge and the- 
oretical def i nitions involving the fundamental aspects of causal discovery, only 
few examples [27, 63, 66] reported evaluation data sets or metrics, and just 
two of them listed both [63, 67]. The landscape is even more fragmented when 
2 
Related Works Theoretical Def 
i nitions Evaluation Datasets Evaluation Metrics Software Packages Practical Applications 
A Survey of Learning Causality with Data: 
Problems and Methods [29] 
 
Causal Inference for Time Series Analysis: 
Problems, Methods and Evaluation [63] 
 
Review of Causal Discovery Methods based on 
Graphical Models [27] 
 
Causal Discovery in Machine Learning: Theo- 
ries and Applications [66] 
 
Toward Causal Representation Learning [86] 
Table 1: Comparison of recent surveys on causal discovery in terms of covered 
contents. 
observed from a practical point of view: only two contributions [66, 67] pre- 
sented and discussed the availability of software tools to perform the described 
procedures, thus hindering the applicability of causal discovery to researchers 
approaching for the i rst time to this topic. 
In particular, the work from [29] provides some insights on the discovery pro- 
cedure from a deep-learning point of view. Authors in [63] tackled the problem 
of recovering the causal graph from time series data sets, while [27] restricted 
its attention to the most famous techniques. Moreover, [66] presented a general 
survey on the topic without a proper interventional section, as for [86] in the 
latent case. Finally, [67] covers causal inference and causal discovery from a 
high-level perspective, which is opposed to our in-depth approach focused on 
structural learning only.
1 / 1
ChatGPT
ChatGPT
This portion of the paper discusses the process of causal discovery when the causal graph is unknown. In such a case, one has to rely on the available data and any prior knowledge that might be available to recover cause-effect pairs. This process of learning graphical structures with a causal interpretation is referred to as causal discovery.
The authors note that while there has been significant growth and interest in causal discovery, especially when experimental data is available, this has also led to a fragmentation of the field. Different fields and researchers make different assumptions and propose different solutions, even though they aim for the same goal. This paper aims to unify these varying concepts and approaches to present a comprehensive status of causal discovery.
The paper is structured into eight sections, starting with a general introduction and overview of previous works. The subsequent sections delve into different approaches, concepts, definitions, algorithms, and problems in both observational and experimental settings. The authors also discuss evaluation techniques and practical applications, and conclude with reflections on the current state of causal discovery.
The authors then examine six surveys published between 2019 to 2022 on the topic of causal discovery, noting that while all provided ample background knowledge and theoretical definitions, only a few provided practical resources like evaluation datasets or metrics. This has led to a fragmented landscape, hindering newcomers from easily applying causal discovery concepts. The authors also comment on the focus of these previous works, such as attention to deep learning methods, time series data sets, and high-level overviews, emphasizing that their work provides an in-depth approach specifically focused on structural learning.
1 / 1
Note: Mathematical models have tremendous power to describe observations of real-world systems. They are routinely used to test hypothesis, explain mechanisms and predict future outcomes. However, real-world systems are often nonlinear and multidimensional, in some instances rendering explicit equation-based modeling problematic. Empirical models, which infer patterns and associations from the data instead of using hypothesized equations, represent a natural and flexible framework for modeling complex dynamics.

Donald DeAngelis and Simeon Yurek illustrated that canonical statistical models are ill-posed when applied to nonlinear dynamical systems.[19] A hallmark of nonlinear dynamics is state-dependence: system states are related to previous states governing transition from one state to another. EDM operates in this space, the multidimensional state-space of system dynamics rather than on one-dimensional observational time series. EDM does not presume relationships among states, for example, a functional dependence, but projects future states from localised, neighboring states. EDM is thus a state-space, nearest-neighbors paradigm where system dynamics are inferred from states derived from observational time series. This provides a model-free representation of the system naturally encompassing nonlinear dynamics.

A cornerstone of EDM is recognition that time series observed from a dynamical system can be transformed into higher-dimensional state-spaces by time-delay embedding with Takens's theorem. The state-space models are evaluated based on in-sample fidelity to observations, conventionally with Pearson correlation between predictions and observations.
1 / 1
ChatGPT
ChatGPT
This passage discusses the power of mathematical models in describing real-world systems, noting their ability to test hypotheses, explain mechanisms, and predict future outcomes. However, real-world systems are often nonlinear and multidimensional, posing a challenge for traditional equation-based modeling approaches.
The authors highlight Empirical Dynamic Modeling (EDM), which infers patterns and associations directly from the data, bypassing the need for predefined equations. This method is seen as a more flexible framework for modeling complex dynamics, particularly for nonlinear dynamical systems.
Donald DeAngelis and Simeon Yurek pointed out the limitations of traditional statistical models when dealing with nonlinear dynamics, particularly their inability to account for state-dependence, where system states are linked to previous states and govern the transition from one state to another. EDM addresses this issue by operating in the multidimensional state-space of system dynamics, as opposed to one-dimensional observational time series. This approach doesn't assume predefined relationships among states, but projects future states from neighboring ones.
A critical aspect of EDM is Takens's theorem, which allows time series observed from a dynamical system to be transformed into higher-dimensional state-spaces through a process known as time-delay embedding. The performance of these state-space models is usually evaluated by their fidelity to observations, conventionally measured using Pearson correlation between predictions and observations. This approach offers a model-free representation of the system, naturally encompassing its nonlinear dynamics.
1 / 1
Back to the article: 2.1Notation 
We denote mathematical objects with capital letters, such as random variable 
X, and collections of objects with capital boldface letters, such as set X. 
Def i nition 2.1 (Graph). A graph G = (V,E) is a mathematical object repre- 
sented by a tuple of two sets: a i nite set of vertices V and a i nite set of edges 
E ��� V �� V. 
Def i nition 2.2 (Directed Graph). A directed graph (DG) G is a graph where 
the edge (X,Y ) is distinct from the edge (Y,X). 
In particular, a directed edge (X, Y) is graphically represented by an arrow 
as X ��� Y , and induces a set of relationships between the vertices of the graph 
G. Given a vertex X, we denote its parents, i.e., the set of vertices that have an 
arrow into X, by Pa(X), while we denote its children, i.e., the set of vertices 
that have an arrow out of X, by Ch(X). Recursively, any parent and parent 
of a parent (child and child of a child) of X is an ancestor An(X) (descendant 
De(X)) of X. 
The vertices connected to X are said to be adjacent to X and denoted as 
Adj(X), while the vertices connected with an undirected edge to X are the 
neighbors Ne(X). These two sets of vertices are identical in undirected graphs, 
but may be dif f erent in graphs with other mixed orientations. 
Def i nition 2.3 (Path). A path �� = (X ��� ������ ��� Y ) is a tuple of non repeating 
vertices, where each vertex is connected to the next in the sequence with an 
edge. 
Def i nition 2.4 (Directed Path). A directed path �� = (X ��� ������ ��� Y ) is a 
tuple of non repeating vertices, where each vertex is connected to the next in 
the sequence with a directed edge. 
Def i nition 2.5 (Cycle). A cycle is a path that starts and ends at the same 
vertex. 
Def i nition 2.6 (Directed Acyclic Graph). A directed acyclic graph (DAG) is 
a directed graph G that has no cycles.
1 / 1
ChatGPT
ChatGPT
This section of the paper introduces mathematical notation and definitions that will be used throughout the research. Let's break down the definitions:
Definition 2.1 (Graph):
A graph G is denoted as G = (V,E), where V is a finite set of vertices (points/nodes) and E is a finite set of edges (lines connecting the vertices).
Definition 2.2 (Directed Graph):
A directed graph (DG) G is a graph in which the edges (connections) have a specific direction, meaning the edge from vertex X to Y (X,Y) is different from the edge from Y to X (Y,X).
A directed edge (X, Y) is graphically represented by an arrow as X ��� Y, which indicates a relationship from X to Y.
Definition 2.3 (Path):
A path �� is a sequence of non-repeating vertices that are connected with edges. It is represented as (X ��� ������ ��� Y), indicating a connection from vertex X to Y.
Definition 2.4 (Directed Path):
A directed path �� is a sequence of non-repeating vertices where each vertex is connected to the next with a directed edge. It is represented as (X ��� ������ ��� Y), indicating a directional connection from vertex X to Y.
Definition 2.5 (Cycle):
A cycle is a path that starts and ends at the same vertex. It is a closed loop within the graph.
Definition 2.6 (Directed Acyclic Graph):
A Directed Acyclic Graph (DAG) is a directed graph that doesn't contain any cycles. It has a specific starting point and an endpoint.
Additional terms include "parents" (Pa(X)), which are vertices that direct an edge into vertex X, "children" (Ch(X)), which are vertices that receive a directed edge from X, "ancestors" (An(X)), which are a vertex's parent or any vertex up the directed path, and "descendants" (De(X)), which are a vertex's child or any vertex down the directed path. Vertices connected to X are called "adjacent" (Adj(X)), and vertices connected to X by an undirected edge are "neighbors" (Ne(X)).
1 / 1
2.2Causal Model 
Def i nition 2.7 (Causal Graph). A causal graph G [8] is a graphical description 
of a system in terms of cause-ef f ect relationships, i.e. the causal mechanism. 
4 
Def i nition 2.8 (Direct and Indirect Cause). For each directed edge (X,Y ) ��� E, 
X is a direct cause of Y and Y is a direct ef f ect of X. Recursively, every cause 
of X that is not a direct cause of Y , is an indirect cause of Y . 
This def i nition is formally enforced by the causal edge assumption [28], 
where: 
Def i nition 2.9 (Causal Edge Assumption). The value assigned to each variable 
X is completely determined by the function f given its parents: 
Xi:= f(Pa(Xi))���Xi��� V.(2.1) 
As natural consequence of such def i nitions, we can def i ne models that entail 
both the structural representation and the set of functions that regulate the 
underlying causal mechanism. 
Def i nition 2.10 (Structural Causal Model). A structural causal model (SCM) 
[8, 56] is def i ned by the tuple M = (V,U,F,P), where: 
��� V is a set of endogenous variables, i.e. observable variables, 
��� U is a set of exogenous variables, i.e. unobservable variables, where V ��� 
U = ���, 
��� F is a set of functions, where each function fi��� F is def i ned as fi: 
(V ��� U)p��� V, with p the ariety of fi, 
so that fidetermines completely 
the value of Vi, 
��� P is a joint probability distribution over the exogenous variables P(U) = 
Q 
iP(Ui). 
Structural Causal Models are also known as Structural Equation Models 
(SEMs). 
The joint exogenous distribution P is responsible for the non-deterministic 
nature of the model, adding a layer of uncertainty through a set of independent 
noise distributions. The unobserved terms U are represented in Figure 2.1 as 
dashed vertices with dashed edges.
1 / 1
ChatGPT
ChatGPT
In this section, the paper introduces the concept of a causal model, which is a mathematical framework for representing causal relationships. It begins with the definitions of a causal graph and direct and indirect causes:
Definition 2.7 (Causal Graph):
A causal graph G is a visual representation of cause-effect relationships within a system, often termed as the causal mechanism.
Definition 2.8 (Direct and Indirect Cause):
In a directed edge (X,Y) in graph G, X is considered a direct cause of Y, and Y is a direct effect of X. Any cause of X that is not a direct cause of Y is considered an indirect cause of Y.
This relationship between X and Y is enforced by the "Causal Edge Assumption":
Definition 2.9 (Causal Edge Assumption):
The value of each variable X is determined by a function f of its parent nodes (Pa(Xi)), expressed as Xi:= f(Pa(Xi)) for all Xi in V.
These concepts lead to the definition of a Structural Causal Model (SCM):
Definition 2.10 (Structural Causal Model):
An SCM is defined by the tuple M = (V, U, F, P), where:
V is a set of endogenous (observable) variables,
U is a set of exogenous (unobservable) variables with V ��� U = ���,
F is a set of functions, where each function fi in F is defined as fi: (V ��� U)p��� V, with p being the arity of fi, such that fi determines the value of Vi completely,
P is a joint probability distribution over the exogenous variables P(U) = Q iP(Ui).
SCMs are also referred to as Structural Equation Models (SEMs). In these models, the joint exogenous distribution P is responsible for the non-deterministic nature of the model, introducing a degree of uncertainty through a set of independent noise distributions. The unobserved terms U are often represented in the model as dashed vertices with dashed edges.
1 / 1
2.3The Causal Discovery Problem 
The causal discovery problem [100] consists in selecting a causal graph as a 
possible explanation for a given data set. 
Formally, let G be the set of graphs def i ned over the variables V of a data 
set D and G������ G be the true but unknown graph from which D has been 
generated. 
Def i nition 2.11 (Causal Discovery Problem). The causal discovery problem 
[99] consists in recovering the true graph G���from the given data set D. 
A causal discovery algorithm is said to solve the causal discovery problem if 
and only if it converges to the true graph G���in the limit of the sample size. 
Def i nition 2.12 (Soundness and Completeness). A causal discovery algorithm 
is sound if it is able to solve the causal discovery problem, while it is complete 
if it outputs the most informative causal graph G that can be recovered from 
the input data set D, without making further assumptions. 
Def i nition 2.13 (Consistency of a Causal Graph). A causal discovery algo- 
rithm is consistent [27, 99] if it outputs a graph G that induces a probability 
distribution consistent with the input data set D. 
Def i nition 2.14 (Identif i ability of a Causal Graph). A causal discovery algo- 
rithm is said to identify [28] a graph G if it is able to determine the direction 
of any edge in G. 
In the following pages we will see that some algorithms are able to identify 
the causal graph up-to its equivalence class, meaning that setting the direction 
6 
of any of the remaining undirected edges would not induce a dif f erent probability 
distribution, i.e. it is not possible to choose a specif i c direction for that edge 
without further assumptions. 
Moreover, some of these methods are able to exploit only observational dis- 
tributions, i.e. probability distributions that are induced by observation data 
set, while others are capable of taking advantage of interventional distributions, 
i.e. probability distributions that are generated by experimental data, where we 
intervene on the system of interest. 
Finally, even though the general formulation of the discovery problem is fo- 
cused on the causal graph only, causal discovery algorithms are usually designed 
to i nd a solution w.r.t. a specif i c set of functions [12, 83, 91, 94], e.g. non-linear 
equations. 
2.4Acyclicity and Faithfulness 
A graphical model is said to satisfy the Markov property if the associated joint 
probability distribution P(V) can be decomposed recursively as: 
P(V) = Y 
Xi���V P(Xi|Pa(Xi))(2.2) 
The probability factorization expressed in Equation 2.2 relies on the assump- 
tion that the relationships encoded by the graph match exactly the underlying 
conditional probability independencies: 
X ��� ���PY |Z =��� X ��� ���GY |Z(2.3) 
Essentially, it is assumed that probability independence (��� ���P) 
implies graph- 
ical independence (��� ���G), 
as stated in Equation 2.3. 
This assumption is known as d-faithfulness or ���directed faithfulness���. In 
fact, the graphical model is required to rely on a DAG in order to satisfy the 
Markov property. More recently, extensions of the faithfulness assumption to 
the cyclic setting have been taken into consideration, e.g. ��-faithfulness [11, 60], 
enabling the discovery of general non-acyclic DGs. 
In order to test whether a variable X is conditionally independent from Y 
given a set Z in any probability distribution P faithful to G, one can use the 
d-separation criterion which is based on the concept of blocked path. 
In particular, when Z blocks every path between X and Y , we say that X 
and Y are d-separated by Z. A path �� is blocked depending on the presence of 
specif i c graphical patterns in it, as given in the following two def i nitions.

... 3Causal Discovery 
In this section we introduce the i rst class of causal discovery algorithms. Here, 
the hypothetical data set is represented by static observational data samples, 
neither interventional information nor time dependencies are taken into account. 
A summary of the explored algorithms can be found in Table 2. 
3.1Constraint-based Algorithms 
Constraint-based algorithms try to recover the causal graph by exploiting a 
set of conditional independence statements (CISs) obtained from a sequence 
of statistical tests.This class of methods translates conditional probability 
independence into graphical separation by assuming faithfulness (Subsection 
2.4) of the underlying distribution. 
Def i nition 3.1 (Perfect Map). A graph G is said to be a perfect map [16, 47] 
for a probability distribution P if every CIS derived from G can also be derived 
from P and vice versa: X ��� ���PY |Z ������ X ��� ���GY |Z(3.1) 
Def i nition 3.2 (Conditional Independence Test). The null H0and alternative 
hypotheses H1def i ned as H0: X ��� ���PY |Z and H1: X 6��� ���PY |Z, let I(X,Y |Z) 
to denote a conditional independence (CI) test. The null hypothesis H0is not 
rejected if and only if the resulting p-value is higher than a chosen signif i cance 
level ��: �� 
I(X,Y |Z) > �� =��� X ��� ���PY |Z(3.2) 
When faithfulness is assumed, probability independence implies graphical 
separation2. The main limitation of this approach is related to the exponential 
growth of the conditioning set Z. Indeed, given the pair (X,Y ), in the worst case 
scenario where X is dependent on Y (or vice-versa), the algorithm is required 
to test for ? ?2(V\{X,Y })??conditioning 
sets. 
Constraint-based methods are generally capable of integrating prior knowl- 
edge into the learning process. 
Conditional Independence with Mixed DataConstrain-based techniques 
are essentially agnostic of the specif i c conditional independence test that is be- 
ing used. Indeed, it is possible to take advantage of such approaches in a wide 
variety of scenarios, as long as the assumptions of the said test are satisf i ed. 
While the main focus of causal discovery studies has been into either discrete or 
continuous settings, recent advances in conditional independence testing [6, 107] 
extend existing tests to mixed-data. 
2Here the term separation is used as a placeholder for a generic graphical separation, which 
is intended as d-separation for directed graphs and m-separation for mixed graphs.

... Peter-Clark (PC)One of the most studied algorithm that leverages the CISs 
is the Peter-Clark (PC) algorithm [99] with its variants [18, 49]. 
The i rst step of the procedure consists in def i ning a complete undirected 
graph over the variables of the given data set D. Subsequently, a sequence of 
conditional independence (CI) tests are performed following an heuristic strategy 
[18], in order to minimize the number of tests needed. For instance, it is known 
that the power of CI test decreases when the size of the conditioning set increases 
[51], due to the curse of dimensionality. A common approach consists in selecting 
an upper limit to the size of the conditioning set, discarding computational- 
intensive time-wasting tests with low signif i cance levels. 
The obtained independence statements are then used to remove the associ- 
ated edges and identify the underlying skeleton. Finally, the remaining edges are 
oriented according to a set of rules [58] that leverages the identif i ed v-structures 
and acyclicity property. 
The resulting equivalence class is returned as a CPDAG, where the remain- 
ing undirected edges are reversible for the given observational distribution that 
arises from the data. 
Fast Causal Inference (FCI)A i rst extension of the PC algorithm to the 
causal insuf i cient setting (Subsection 2.6) is represented by the Fast Causal 
Inference (FCI) [102, 113] algorithm. Specif i cally, the FCI algorithm relaxes 
both the assumption of no latent confounding [32] and no selection bias [50] 
in the observational setting, pushing the causal discovery problem a step closer 
to real-world scenarios. In this context, the authors leverage the def i nition of 
discriminating path to derive a new set of orientation rules. 
Def i nition 3.3 (Discriminating Path). Let G be an ancestral graph, a path 
�� = (X,...,W,Z,Y ) between X and Y is a discriminating path for Z if (i) �� 
contains at least three edges, (ii) X is not adjacent to Y , (iii) Z is adjacent to 
Y , and (iv) every vertex between X and Z is a collider on �� and parent of Y . 
Discriminating paths are closely related to the separation sets identif i ed by 
the PC algorithm: if a path �� between X and Y is discriminating for Z, then 
Z is a collider on �� if f every set that separates X and Y does not contains Z, 
otherwise it is a non-collider if f every set that separates X and Y contains Z. 
3.2Score-based Algorithms 
Score-based algorithms are usually structured around the maximization of a 
measure of i tness of a graph G through a space of possible graphs G for the 
observed samples D, following a def i ned scoring criterion S(G,D) [17]: 
G���= argmax 
G���G S(G,D)(3.3) 
In the next few paragraphs, a set of properties for scoring criteria are intro- 
duced, before shifting the focus on an optimal two-step procedure for the causal 
suf i cient scenario.
1 / 1
ChatGPT
ChatGPT
The paper now discusses the causal discovery problem, its algorithmic resolution, and relevant criteria:
Definition 2.11 (Causal Discovery Problem):
The causal discovery problem entails recovering the true graph G* from a given data set D.
A causal discovery algorithm successfully solves the problem if it converges to the true graph G* as the sample size increases.
Definition 2.12 (Soundness and Completeness):
An algorithm is sound if it can solve the causal discovery problem and complete if it produces the most informative causal graph G recoverable from the input data set D without requiring further assumptions.
Definition 2.13 (Consistency of a Causal Graph):
An algorithm is consistent if it produces a graph G whose induced probability distribution is consistent with the input data set D.
Definition 2.14 (Identifiability of a Causal Graph):
An algorithm can identify a graph G if it can determine the direction of any edge in G.
Some algorithms can only identify the causal graph up to its equivalence class - meaning they can't determine the direction of an edge without further assumptions. The choice of observational (derived from observed data) versus interventional distributions (derived from experimental data) varies by algorithm.
The section also discusses Acyclicity and Faithfulness and the principle that a graphical model satisfies the Markov property if the joint probability distribution P(V) can be recursively decomposed.
The paper then introduces the concept of causal discovery algorithms, with a focus on constraint-based algorithms. These algorithms try to recover the causal graph by utilizing a set of conditional independence statements obtained from a series of statistical tests.
Definition 3.1 (Perfect Map):
A graph G is a perfect map for a probability distribution P if all conditional independence statements derived from G can also be derived from P and vice versa.
Definition 3.2 (Conditional Independence Test):
This test is used to reject or accept the null hypothesis H0 (X is independent from Y given Z) based on a significance level ��.
Notably, the PC (Peter-Clark) and FCI (Fast Causal Inference) algorithms are mentioned as specific examples of constraint-based algorithms.
The section also introduces score-based algorithms which maximize a fitness measure of a graph G across a space of possible graphs G for the observed samples D, using a defined scoring criterion S(G,D).
1 / 1
Greedy Equivalent Search (GES)The Greedy Equivalence Search (GES) 
[4, 57] is optimal in the limit of the number of samples [17]. The i rst step of the 
algorithm consists in the initialization of the empty graph G. The algorithm is 
composed by two phases: the forward search and the backward search. In the 
forward search phase, (i) G is modif i ed by repeatedly adding the edge that has 
the highest delta score, until there is no such edge that increases the score. In 
the backward search phase, (ii) the edge that again achieves the highest delta 
score is repeatedly removed. The algorithm terminates once it reaches a local 
maximum during the backward search phase. 
This algorithm is designed to work under causal suf i ciency.When this 
assumption no longer holds, the procedure is known to introduce extra edges as 
a compensation behaviour for the unobserved relationships. For example, when 
a fork (X ��� Y ��� Z) is present and the middle vertex is indeed latent, GES 
will likely add an edge between the other two observed vertices of the structure, 
even if such edge is not present in the true graph. Any algorithm that is based 
on this technique and does not address the issue directly displays such pattern. 
Fast GES (FGES)Score-based algorithms are as fast as the computation of 
the chosen scoring criterion is. Leveraging the properties of the score function, it 
is possible to minimize the number of computations needed by storing previous 
intermediate evaluations. Not only this optimizations reduce the computation 
time considerably, but also allow the application of these methods to high- 
dimensional data sets [6, 75]. This ���fast��� variant of GES (FGES) caches partial 
graph scores (i.e. delta scores), signif i cantly increasing the memory usage, since 
relevant fragments of the graph may be considered. Moreover, computationally 
expensive sections of the algorithm can be parallelized, taking advantage of high 
performance computing (HPC) settings. 
3.3Hybrid Algorithms 
With the term ���hybrid��� algorithms we refer to the class of methods that combine 
constraint-based and score-based approaches to mitigate their drawbacks. 
Adaptively Restricted GES (ARGES)Consistency of constraint- and 
score-based algorithms is usually proved in low-dimensional use cases, where the 
number of samples is orders of magnitude greater than the number of variables. 
Hybrid approaches generally lacks a formal and rigorous proof of consistency, 
leading to undef i ned behaviour. For this reason, an adaptively restricted variant 
of GES (ARGES) [65] has been developed, targeting specif i cally the consistency 
weakness in both low- and high-dimensional spaces. 

3.4Other Methods 
Hidden Compact Representation (HCR)Causal discovery methods for 
discrete and mixed variables have gained renovated interested in the last few 
years [6, 107]. Although additive noise models have been widely used in the 
context of continuous variables, it is dif i cult to justify their application with 
categorical data, where the addition operator between the levels of variables is 
not well def i ned. 
For this reason, authors in [15] developed a new low-dimensional embedding 
for discrete variables, allowing a (hidden) compact representation (HCR) of the 
discrete states of such variables. The method follows a two-stage procedure: 
at i rst, a discrete variable is deterministically mapped into a low-cardinality 
representation (e.g. binary), which acts as a proxy for the information contained 
in the original variable; then, a set of samples are drawn for the new proxy 
variable using a probabilistic mapping. The overall complexity of the model 
in controlled using the BIC score, balancing between total i tness and size of 
parameters. 18 
The authors address the problem of identif i ability of the model and prove 
that, under mild conditions, the causal graph recovered from observational data 
is identif i able. The method is tested against both synthetic and real-world data, 
providing reference values for performance metrics. In these experiments, HCR 
outperforms linear models in terms of accuracy and sensitivity, especially when 
the additive noise assumption does not hold. 
Quantile Causal Discovery (bQCD)The quantile causal discovery (bQCD) 
[105] technique is designed to uncover cause-ef f ect pairs in the bivariate setting. 
By re-expressing independence statements in light of the minimum description 
length (MDL) [81], the authors build a discovery procedure by using quantile 
scoring. 
Following [41], let X and Y be two random variables with joint, marginal 
and conditional distributions denoted by F, FXand FX|Yrespectively. 
The key 
concept here is that a lower complexity follows from a correct causal orientation 
of the (X,Y ) pair, since it is a more informative representation of the associated 
data. 
Hence, the Kolmogorov complexity K(F) is def i ned as the length of the 
shortest program that outputs F(X). Since K(F) measures the information con- 
tained in F, authors in [104] state that if X causes Y , then K(FX)+K(FY |X) ��� 
K(FY)+K(FX|Y). The problem is that K(F) cannot be computed in practice. 
Therefore, the authors relies on the MDL principle as a proxy for the Kol- 
mogorov complexity. Such an approximation can be performed by estimating 
the population quantiles through nonparametric quantile regression. 
The resulting procedure is robust to outliers and can be generalized to a 
wide range of distributions, although it requires that all population quantiles 
must be computable, which could be a limiting factor in real-world applications. 
Linear Non-Gaussian Acyclic (LiNGAM)In the context of linear causal 
models, when causal suf i ciency holds, the observed variables can be expressed 
as a linear combination of the noise terms: 
x = Bx + e(3.6) 
Here, the exogenous distribution is assumed to be made of mutually independent 
(possibly non-Gaussian) variables. Solving for x reduces to the identif i cation of 
the matrix A such that: x = (I ��� B)���1e 
= Ae(3.7) 
The LiNGAM [94, 95] algorithm relies on independent component analysis 
(ICA) [20] to identify a possible solution for A. In fact, multiple mixing ma- 
trices A are feasible solutions for the given joint probability distribution. This 
technique is essentially focused on discovering asymmetries in the sample dis- 
tribution to determine the correct causal ordering.Once such ordering has 
been discovered, the causal graph is built by recovering all and only the edges 
coherent with the order.
1 / 1
ChatGPT
ChatGPT
The text you have provided gives an overview of several techniques for causal discovery. It discusses Greedy Equivalent Search (GES), Fast GES (FGES), adaptively restricted GES (ARGES), Hidden Compact Representation (HCR), quantile causal discovery (bQCD), and Linear Non-Gaussian Acyclic models (LiNGAM).
Greedy Equivalent Search (GES)
: This is a two-phase algorithm starting with an empty graph. The forward phase modifies the graph by adding edges to increase the delta score, while the backward phase removes edges to reach a local maximum. It works best when the assumption of causal sufficiency holds.
Fast GES (FGES)
: This is an optimized version of GES that reduces computation time by storing intermediate evaluations. It's especially useful for high-dimensional datasets.
Adaptively Restricted GES (ARGES)
: This variant of GES targets the consistency weakness in both low- and high-dimensional spaces. This method combines constraint-based and score-based approaches to improve performance.
Hidden Compact Representation (HCR)
: This method offers a low-dimensional embedding for discrete variables, allowing for a compact representation of their discrete states. The model complexity is controlled using the Bayesian Information Criterion (BIC) score.
Quantile Causal Discovery (bQCD)
: This method uncovers cause-effect pairs in a bivariate setting. It re-expresses independence statements using the minimum description length (MDL) and quantile scoring.
Linear Non-Gaussian Acyclic (LiNGAM)
: In this method, the observed variables can be expressed as a linear combination of noise terms, under the assumption of causal sufficiency. The LiNGAM algorithm relies on Independent Component Analysis (ICA) to solve the corresponding equations, focusing on discovering asymmetries in the sample distribution to determine the correct causal ordering.
Each of these techniques provides unique advantages and approaches to understanding causality in complex systems. The best choice of method would depend on the specific problem domain and the data available.
1 / 1
The LiNGAM method has been extended later for causally insuf i cient set- 
tings [34].Let f be the vector of latent variables and �� the matrix of the 
connections strength between f and x, then: 
x = Bx + ��f + e(3.8) 
The proposed model can be solved with a variant of ICA, called overcomplete 
ICA, which takes into account the presence of unobserved ef f ects. 
The LiNGAM algorithm consistently estimates the connection matrix B. 
While standard ICA does not scale well in high-dimensional settings, approxi- 
mated variants of ICA can be used to compute the components with a predef i ned 
i x number of iterations with reasonable precision. This leads to a ef i cient so- 
lution in presence of non-gaussian noise and causally insuf i cient data sets. 
Continuous Optimization (NOTEARS)In the ���DAGs with NO TEARS��� 
[114] algorithm, the causal discovery problem is reduced to a continuous opti- 
mization problem. The acyclicity constraint is expressed as an equality con- 
straint h(W) = 0, where h is a smooth dif f erentiable function that measures 
the ���DAG-ness��� (i.e. a quantif i cation of the acyclicity violations) of a given 
adjacency matrix W. When W is binary then: 
h(W) = tr(eW���W) ��� n = 0(3.9) 
where tr, is the trace operator, ��� is the Hadamard product, e���is 
the matrix 
exponential and n the size of W. Moreover, this function has a rather simple 
associated gradient: ���h(W) = (eW���W)T��� 2W(3.10) 
Coef i cients smaller than a i xed threshold �� > 0 are set to zero, rounding the 
solution with an arbitrary precision. The evaluation of the matrix exponential 
is O(n3), i.e. cubic in the number of vertices. Given the low computational 
complexity, NOTEARS outperforms existing methods when both the in-degree 
and the sample size are large. 
4Causal Discovery with Cycles 
4.1Cyclic SCM 
In a SCM, the causal graph induces a functional set F where equations follows 
the decomposition enforced by the causal edge assumption, Subsection 2.9. If 
the causal graph is acyclic, then the SCM itself is called acyclic, or recursive 
SEM. The concept of recursion is linked to the hierarchical order that arises 
from the topological ordering of the underlying DAG. Indeed, it is possible to 
def i ne a sequence X1,X2,...,Xnof vertices over V such that for any Xiand 
Xjwhere i < j, Xjis not a cause of Xi[9]. 
Therefore, in a non-recursive SEM, or cyclic SCM, some endogenous vari- 
ables are connected to each other, forming cycles that do not allow a recursive 
20 
decomposition. Still, the causal edge assumption is satisf i ed, since its def i nition 
is consistent even in the presence of cycles.
1 / 1
ChatGPT
ChatGPT
The text continues discussing causal discovery techniques, specifically extensions of the Linear Non-Gaussian Acyclic model (LiNGAM) and a new method called DAGs with NO TEARS.
Extended LiNGAM
: This model has been modified to account for causally insufficient settings, meaning settings where not all causal factors are observed. The revised equation introduces a vector of latent variables and a matrix of connections between these latent variables and the observed variables. This model can be solved with a variant of Independent Component Analysis (ICA), called overcomplete ICA, which accounts for unobserved effects. This leads to an efficient solution in the presence of non-Gaussian noise and causally insufficient data sets.
DAGs with NO TEARS (NOTEARS)
: This algorithm reduces the causal discovery problem to a continuous optimization problem. The acyclicity constraint is expressed as an equality constraint h(W) = 0, where h is a function that measures the "DAG-ness" of a given adjacency matrix W. Coefficients smaller than a fixed threshold are set to zero, rounding the solution with an arbitrary precision. Given its low computational complexity, NOTEARS outperforms existing methods when both the in-degree and the sample size are large.
The text then starts discussing Causal Discovery with Cycles and introduces the concept of Cyclic Structural Causal Models (SCMs). In a SCM, the causal graph leads to a set of equations based on the causal edge assumption. If the causal graph is acyclic, then the SCM is called acyclic or recursive SEM. The concept of recursion is linked to the hierarchical order arising from the topological ordering of the underlying Directed Acyclic Graph (DAG). In contrast, a non-recursive SEM or cyclic SCM has some endogenous variables connected to each other, forming cycles that don't allow a recursive decomposition. Despite this, the causal edge assumption is satisfied, since its definition remains consistent even in the presence of cycles.
1 / 1
4.2No Acyclicity Assumption 
Conditional independencies arising from cyclic SCMs are entailed by the cyclic 
graphs [80]. It can be shown that, in general, there is no DAG encoding the 
conditional independencies which hold in such SCM [64]. Nonetheless, cyclic 
SCMs are widely used to model systems with feedback, and are applied in soci- 
ology, economics and biology, making this class of models a relevant target of 
interest for causal discovery techniques. 
To test for such independencies, d-separation can be adapted to the cyclic 
setting under the assumption of causal suf i ciency [101]. In causally insuf i cient 
scenarios, d-separation can be replaced with ��-separation [24, 25] applied to 
directed mixed graphs (DMGs), i.e.mixed graph (Subsection 2.24) without 
undirected edges. 
Def i nition 4.1 (Strongly Connected Component). Let G be a DG and X a 
vertex in G. The strongly connected component [24] of a vertex X is def i ned as: 
SCC(X) = An(X) ��� De(X)(4.1) 
that is, the set of vertices that are both ancestors and descendants of X, includ- 
ing X itself. 
Def i nition 4.2 (��-separation). Let G be a DMG, �� be a path on G and Z a 
subset of V. The path �� is blocked [24, 25] by Z if and only if �� contains: 
��� a collider A������ B ������C where B 6��� An(Z), or 
��� a non-collider A ��� B ��������� C (or A ��������� B ��� C) where B ��� An(Z) and A 
(respectively C) is part of SCC(B) (Equation 4.1). 
The set Z ��-separates X from Y if it blocks every path between X and Y . 
The above graphical criterion implies d-separation and reduces to it in the 
case of DAGs. 
Cyclic Causal Discovery (CCD)The Cyclic Causal Discovery (CCD) al- 
gorithm [80] has been the only provably sound (Subsection 2.12) approach to 
general directed graphs until the development of LiNG [48]. CCD is a constraint- 
based algorithm that follows the same initial procedure as the one of the PC 
algorthm, with i ve dif f erent orientation rules. CCD outputs a PAG G which 
dif f er from the output of FCI for a couple of additional patterns: 
��� underlining triples (X ���������Y ���������Z), where Y is an ancestor of at least one 
of X or Z in every graph in [G], and 
��� dotted underlining triples (X������Y 
...������Z), where Y is not a descendant of 
a common child of X and Z. 21 
These additional patterns arise from a fundamental problem: the algorithms is 
not complete, and, therefore, there may be features common to all graphs in the 
same equivalence class that are not present in the output PAG (i.e. it is not 
the most informative PAG). While not being complete in the same sense of the 
previous algorithms, CDD is d-separation complete, meaning that the resulting 
PAG represents an equivalence class with a single graph, i.e.it encodes all 
the needed conditional independecies. Therefore, CCD is useful when one is 
interested in querying the resulting graph about dependencies, but lacks the 
capability to represent every causal edge by def i nition, in contrast to others 
algorithms. This limitation makes it less suitable for the def i nition of SCMs, 
especially when one is interested in the form of the functional set. 
Linear Non-Gaussian (LiNG)The LiNGAM algorithm can be adapted to 
the cyclic setting by weakening the acyclicity assumption. Specif i cally, instead 
of targeting a DAG, LiNG (or LiNG-D family) [48] try to recover a simple graph 
(i.e. without self-loops) by forcing all entries on the diagonal of the B matrix 
to be zero. 
While LiNGAM output could be seen as a set of admissible models that 
contains a single model (i.e. the model is identif i able), the cyclic variant usually 
admits more than one causal graph at the time. In fact, the acyclicity assump- 
tion that allowed to i nd the row-permutation of B that best i ts the given data 
set is missing. The authors then suggest to limit the discovery procedure to the 
k-th best assignment, following the intuition that permutations associated to 
inadmissible models would score poorly asymptotically. This approach selects 
one single model from the equivalent class (i.e. returning set). 
LiNG inherits both limits and strengths of the original method: approximate 
(or sparse) ICA can be a valid alternative if running the full ICA is computa- 
tionally expensive for the considered task.
1 / 1
ChatGPT
ChatGPT
In section 4.2, the author discusses that cyclic SCMs, or Structural Causal Models with cycles, are important in real-world scenarios where feedback exists, such as in sociology, economics, and biology. These models can't be generally encoded in Directed Acyclic Graphs (DAGs), yet they can capture significant conditional independencies.
A couple of key concepts related to cyclic SCMs are discussed:
d-separation
: This concept can be adapted to the cyclic setting under the assumption of causal sufficiency. In causally insufficient scenarios, d-separation can be replaced with ��-separation applied to Directed Mixed Graphs (DMGs).
Strongly Connected Component (SCC)
: Defined as the set of vertices that are both ancestors and descendants of a vertex X, including X itself.
��-separation
: Defined as a path being blocked if it contains certain conditions. The set Z ��-separates X from Y if it blocks every path between X and Y.
Two main cyclic causal discovery algorithms are then discussed:
Cyclic Causal Discovery (CCD)
: CCD has been a unique, provably sound approach to general directed graphs. However, it's not complete, meaning there might be features common to all graphs in the same equivalence class that are not present in the output. Therefore, while it is useful for querying about dependencies, it lacks the ability to represent every causal edge by definition.
Linear Non-Gaussian (LiNG)
: This is a variation of the LiNGAM algorithm adapted to cyclic settings. The primary adjustment is to weaken the acyclicity assumption, thus allowing the algorithm to handle cycles. However, the cyclic variant usually admits more than one causal graph at a time due to the missing acyclicity assumption. Thus, the authors suggest limiting the discovery procedure to the k-th best assignment.
1 / 1
Linear Non-Gaussian (LiNG)The LiNGAM algorithm can be adapted to 
the cyclic setting by weakening the acyclicity assumption. Specif i cally, instead 
of targeting a DAG, LiNG (or LiNG-D family) [48] try to recover a simple graph 
(i.e. without self-loops) by forcing all entries on the diagonal of the B matrix 
to be zero. 
While LiNGAM output could be seen as a set of admissible models that 
contains a single model (i.e. the model is identif i able), the cyclic variant usually 
admits more than one causal graph at the time. In fact, the acyclicity assump- 
tion that allowed to i nd the row-permutation of B that best i ts the given data 
set is missing. The authors then suggest to limit the discovery procedure to the 
k-th best assignment, following the intuition that permutations associated to 
inadmissible models would score poorly asymptotically. This approach selects 
one single model from the equivalent class (i.e. returning set). 
LiNG inherits both limits and strengths of the original method: approximate 
(or sparse) ICA can be a valid alternative if running the full ICA is computa- 
tionally expensive for the considered task. 
��-Connection GraphsFrom the concept of ��-separation, one can derive a 
MG where conditional independencies are expressed in the presence of cycles 
and latent variables, namely a ��-Connection Graph (��-CG). An algorithm to 
learn this structures from data has been developed [24] as a natural extension 
of the work presented in [36]. The causal discovery problem is re-casted as a 
continuous optimization problem based on the following loss function: 
L(G,S) = X 
��i(1��i>0��� 1Xi��� ���GYi|Zi)(4.2) 
where S is a set of conditional independence statements expressed as S = 
?(X 
i,Yi,Zi,��i) ?n 
i=1, where Xi,Yi and Ziare variables in V and ��i��� R ��� 
{������,+���} encodes the conf i dence of probabilistic conditional independence 
Xi��� ���PYi|Zias 
a constraint. 
The ��iweights are evaluated using the indicator function 1 to constrain the 
conditional dependence between variables.Therefore, Equation 4.2 quantify 
the amount of observations against the proposed causal graph based on the ob- 
served data. During the experimental evaluation, authors relied on the weights 
22 
proposed in [53]: ��i= logpi��� log��(4.3) 
with pirepresenting the p-value of a statistical test for conditional independence 
and �� being a signif i cance level. 
Minimizing the loss function may lead to multiple optimal solutions, where 
each solution G is an instance of the actual equivalence class [G]. Indeed, as 
for d-separation and CPDAGs, the ��-separation criterion and the associated 
��-CGs take into account possible undirected edges that are invariant for any 
causal graph belonging to the same equivalence class. 
This algorithm has been benchmarked against synthetic data in low-dimensional 
setting. While the recovery metrics show consistent performances across the ex- 
periments, especially when increasing the number of interventions, it is clear 
that the main limitation of this approach is linked with the ��-separation en- 
coding, as noted by [78]. Indeed, the separation checks are preformed using 
Answer Set Programming (ASP), a declarative logic programming language, 
which slows down the learning procedure.
1 / 1
ChatGPT
ChatGPT
The document further explains the LiNGAM (Linear Non-Gaussian Acyclic Model) algorithm when adapted for cyclic settings by weakening the acyclicity assumption. Unlike a directed acyclic graph (DAG), LiNGAM targets a simple graph (a graph without self-loops) by setting all entries on the diagonal of the B matrix to zero. The cyclic variant of LiNGAM may yield more than one causal graph, unlike the original LiNGAM output, which can be seen as a set of admissible models containing a single identifiable model.
To mitigate this, the authors suggest limiting the discovery procedure to the k-th best assignment, following the notion that permutations related to inadmissible models would rank poorly over time. This way, a single model is selected from the equivalence class. It's important to note that the adapted LiNGAM, termed LiNG or LiNG-D, retains the strengths and weaknesses of the original method. An approximate or sparse Independent Component Analysis (ICA) could serve as a viable alternative when running the full ICA is computationally demanding.
The concept of ��-separation leads to the development of ��-Connection Graphs (��-CGs). These are mixed graphs where conditional independencies can be expressed even in the presence of cycles and latent variables. An algorithm to learn these structures from data has been developed. The causal discovery problem is reframed as a continuous optimization problem based on a specific loss function. The loss function measures the difference between the observations and the proposed causal graph based on the observed data.
The loss function might have multiple optimal solutions, each one being an instance of the actual equivalence class. Just like with d-separation and CPDAGs, the ��-separation criterion and associated ��-CGs take into account potential undirected edges invariant for any causal graph in the same equivalence class.
Finally, this algorithm has been benchmarked against synthetic data in a low-dimensional setting. Despite showing consistent performances, especially when the number of interventions is increased, a key limitation of this approach is tied to the ��-separation encoding. The separation checks are performed using Answer Set Programming (ASP), a declarative logic programming language that slows down the learning process.
1 / 1
bcauseThe procedures described so far are essentially approximate algo- 
rithms that reduce the search space (i.e.the number of conditionally inde- 
pendence tests) by using previously computed test results. In fact, edges that 
are tested in later phases rely on adjacent vertices that are selected in earlier 
steps of the algorithm. During the last few years, exact search approaches have 
been developed in a branch-and-bound fashion. 
The bcause algorithm [77] explores the search space in a tree-like visit guided 
by an objective function that determines the weight of a potential solution. 
During the discovery phase, any edge of an intermediate result G is either absent, 
present or undecided. Before the actual branching step, the lower bound of the 
given objective function for the current partial solution G0is computed. If such 
bound is higher than the weight obtained by the previous solution G, the branch 
can be closed and the algorithm backtracks. Otherwise, if G0contains at least 
one undecided edge, the procedure branches recursively in two directions: one in 
which said edge is set as present and the other marked as absent. Finally, if the 
branch cannot be closed and G has no undecided edge, then the current solution 
G0is updated if and only if the evaluation of the objective function results in a 
lower weight. The search procedure will return G as a globally optimal solution. 
Since the causal discovery problem is inherently exponential, an exact search 
algorithm is unfeasible in the general setting. However, if both the objective 
function and its lower bound can be ef i ciently evaluated, a constrained space 
for a low dimensional problem can be ef f ectively explored. For example, the 
authors benchmark their method under dif f erent conditions, showing that as- 
suming acyclicity result in a lower execution time.Moreover, the algorithm 
maintains a set of constraints satisf i ed by the local solution and updates them 
incrementally. Therefore, any incompatible extension of the current solution is 
ruled out by leveraging a linear programming solver, reducing the total number 
of evaluation needed. 23 
LayerQuestionMethod 
ObservationalHow would seeing X change my 
belief in Y ? 
Un/Supervised 
Learning 
InterventionalWhat happens to Y if I do X?Reinforcement 
Learning 
CounterfactualWhat would have happened to Y 
if I had done X0instead of X? 
Structural 
Causal Model 
Table 3: Layers of causation with associated questions, practical examples and 
methods. 
5Causal Discovery with Interventions 
This section is focused on the dif f erence between learning causal models using 
either observational or interventional data. While the former setting has been 
explored extensively in the past decades, only recently solutions for properly 
handling experimental data have been proposed. 
5.1Observational vs. Interventional 
In order to grasp the added value of experimental data, we will introduce the 
concept of ladder of causality [8, 72] as a reference framework. 
The Ladder of CausationThe ladder of causation, also called the causal 
hierarchy, is an ordered structure of composed by three layers, where each layer 
is mapped to a cognition level: observational, interventional and counterfactual. 
A level inherently def i nes the set of potential queries that can be answered with 
the given information associated to it. 
In practice, the observational layer is composed by associational or factual 
data, while the interventional layer is related to data that are generated by an 
intervention on the system, i.e. an experiment. Interacting with the system 
itself is the reason why these two levels are dif f erent. The counterfactual layer 
is the highest level of cognition, where one may ask what would have happened 
if a dif f erent intervention had been performed, opposed to the one that factu- 
ally altered the system. This hypothetical scenario is strongly opposed to the 
observational one, being in the counter-factual space. 
Even if the three layers represent dif f erent information levels, they are not 
distinct. In fact, each layer is a generalization of the previous one, e.g. the 
observational setting can be seen as a special case of the interventional scenario, 
where no intervention is performed. Therefore, the interventional layer subsumes 
the observational one. The same happens with the counterfactual layer w.r.t. 
the interventional one, provided that the former allows to def i ne hypothetical 
actions that were not present in the latter, as expressed in Table 3. 
At this point, one may ask how to formally represent the concepts expressed 
by this hierarchy, to operatively exploit the informative gap between the layers.

The answer is provided by do-calculus [70]. 
do-calculusQueries that are usually expressed in natural language can be 
rephrased in terms of probability distribution by introducing the do operator, 
whenever possible3. 
Def i nition 5.1 (Rules of do-calculus). Let G be a causal graph and P the 
probability distribution induced by G. For any disjoint subset of variables X, 
Y, Z and W, the following three rules apply: 
1. Insertion and deletion of observations: 
P(Y|do(X),Z,W) = P(Y|do(X),W)(5.1) 
if (Y ��� ��� Z|X,W) holds true in GX, 
2. Exchange of observations and interventions: 
P(Y|do(X),do(Z),W) = P(Y|do(X),Z,W)(5.2) 
if (Y ��� ��� Z|X,W) holds true in GX,Z, 
3. Insertion and deletion of interventions: 
P(Y|do(X),do(Z),W) = P(Y|do(X),W)(5.3) 
if (Y ��� ��� Z|X,W) holds true in GX,Z(W), 
where GXis the subgraph of G where the incoming edges into X are re- 
moved, GZis the analogous for the outgoing edges from Z, and i nally Z(W) is 
Z\An(W) w.r.t. the subgraph GX. 
With these rules, which are correct and complete, a causal ef f ect can be 
identif i ed if there exists a i nite sequence of applications of such rules leading to 
a do-free expression of the considered probability distribution. 
5.2Types of Interventions 
Def i nition 5.2 (Perfect Intervention). An intervention is said to be perfect (or 
hard) if it removes the causal dependencies (i.e. the incoming causal edges, as 
in Subsection 2.9) that af f ect the intervention target. 
Indeed, do-calculus enables us to express perfect interventions in a operative 
framework, but there are other types of interventions that cannot be expressed 
using this notation. 
3We restrict ourselves to a minimal introduction of the do-calculus, aiming to formally 
represent the set of concepts that are essential for the causal discovery scenario. For a broader 
discussion on identif i cation and estimation of the causal ef f ect, refer to [96]
1 / 1
ChatGPT
ChatGPT
The document explains that the previously described procedures are approximate algorithms that help reduce the search space, essentially reducing the number of conditional independence tests by utilizing previously computed test results. It also introduces the bcause algorithm, which explores the search space in a tree-like visit guided by an objective function that determines the weight of a potential solution. This algorithm uses a branch-and-bound approach to precisely search the solution space.
The document then moves to discuss layers of causation and the types of questions associated with them. There are three main layers: observational, interventional, and counterfactual. Each layer represents different information levels but are not entirely distinct. They each generalize the one before it. For example, the observational layer can be viewed as a specific case of the interventional scenario, where no intervention is performed. This makes the interventional layer a super set of the observational one, and the counterfactual layer a super set of the interventional one.
The ladder of causation or the causal hierarchy is then explained. This is an ordered structure of three layers, where each layer is mapped to a cognition level: observational, interventional, and counterfactual. Each level inherently defines the set of potential queries that can be answered with the given information associated with it.
Observational data are factual, whereas interventional data are generated by an intervention on the system or an experiment. The counterfactual layer, the highest level of cognition, is concerned with hypothetical scenarios, asking what would have happened if a different intervention had been performed.
To exploit the informative gap between the layers, do-calculus is introduced. The do-operator allows natural language queries to be rephrased in terms of probability distribution.
In conclusion, interventions are categorized as perfect or hard if they remove the causal dependencies affecting the intervention target. Do-calculus allows for expressing these perfect interventions, but there are other types of interventions that cannot be expressed using this notation.
1 / 1
Def i nition 5.3 (Imperfect Intervention). An intervention is said to be imperfect 
(or parametric, soft) [55] if it does not remove the causal dependence that af f ects 
the intervention target, but alters the functions that represents such dependence. 
For instance, an imperfect intervention on an SCM could be a change in the 
parameters that quantify the strength of the causal relationships, while a perfect 
intervention would result in hard setting them to zero. In this sense, perfect 
interventions are a subset of imperfect interventions, where some variables are 
removed from the equations of the functional set as a special case. 
Mechanism ChangeImperfect interventions itself are a formal def i nition of 
a broader concept called mechanism change [106]. For a SCM M with a causal 
graph G and a set of parameters �� associated to the function set F. A mechanism 
change is a mapping from M to M0, where the new set of parameters is def i ned 
as ��0= ��0���(��\��), with the new subset ��0that 
dif f ers from the original subset 
��. The change af f ects the behaviour of the function set F, inducing a set F0. 
5.3Def i ning the Intervention Target 
We can rephrase perfect and imperfect interventions under a single unif i ed 
framework through the concept of intervention target [31]. 
Def i nition 5.4 (Intervention Target). Let G be a causal graph. A subset I ��� V 
is said to be an intervention target if it contains all and only the variables 
associated to an intervention over G. 
Therefore, a single-variable intervention is an intervention target that con- 
tains only one variable, while in a multi-variable intervention it contains more 
than one. As a special case, when I = ��� the intervention target represents 
the observational case. A set of multiple intervention targets {I0,I1,...,In} is 
called an intervention family and it is denoted with the calligraphic letter I. 
Def i nition 5.5 (Conservative Family). A family of targets I is conservative if 
for each vertex X in V there exists at least one intervention target in I that 
does not contain X: ���I : X 6��� I ��� I,���X ��� V(5.4) 
Essentially, a conservative family is a family that allows the existence of at 
least one intervention target which does not intervene on an specif i c variable. 
This property guarantees that there is at least one experiment in the family 
which does not alter the behaviour of such variable if performed. 
In this settings, a conservative family allows to observe the inf l uence of a 
(known) set of targets on at least one unaf f ected variable, enabling the possibility 
of disentangling such ef f ect, especially when compared to the other experiments 
in the whole family. 
Def i nition 5.6 (Intervention Graph). Let G be a causal graph and I be an 
intervention target def i ned over G. The intervention graph G(I)= (V,E(I)) is 
26 
the causal graph obtained by removing any directed edge that points to a vertex 
in I from G: E(I)= {(X,Y ) | (X,Y ) ��� E ��� Y 6��� I}(5.5) 
This def i nition of intervention graph is coherent with the intervened graph 
resulting from a do-intervention [70], also known as graph surgery or graph 
manipulation. 
We can now formally express the interventional distribution associated to 
an intervention graph. 
Def i nition 5.7 (Interventional Distribution). Let G be a causal graph and I 
be an intervention target. The interventional distribution P(I)can be expressed 
using the factorization formula: 
P(I)= Y 
Xi���I P(I)?Xi|Pa(Xi)? Y 
Xi6���I P(���)?Xi|Pa(Xi)?(5.6) 
where P(���)is the observational distribution of the variables which were not 
included in the intervention target, if any. 
In case of perfect interventions, the interventional distribution can also be 
expressed using the do-notation: 
P(I)= Y 
Xi���I P(I)?Xi|do(I)? Y 
Xi6���I P(���)?Xi|Pa(Xi)?(5.7)
1 / 1
ChatGPT
ChatGPT
The document now introduces the concept of imperfect intervention, also known as parametric or soft intervention. These interventions do not remove the causal dependence that affects the intervention target but modify the functions representing such dependence. An example could be a change in the parameters quantifying the strength of the causal relationships in a Structural Causal Model (SCM). Perfect interventions, on the other hand, completely eliminate these variables, setting their relationships to zero. As such, perfect interventions can be considered a subset of imperfect interventions.
The document then moves on to discuss the concept of a mechanism change, which is a broader term encompassing imperfect interventions. For an SCM M with a causal graph G and a set of parameters �� associated with the function set F, a mechanism change maps M to M0, where the new set of parameters ��0 is defined, resulting in a change in the behavior of the function set F, inducing a new set F0.
The concept of an intervention target is introduced next, which unifies perfect and imperfect interventions under a single framework. An intervention target is a subset of the causal graph G that contains all and only the variables associated with an intervention over G. This can range from a single-variable intervention (only one variable in the set) to a multi-variable intervention (multiple variables in the set). An intervention family is a set of multiple intervention targets. If the family allows for at least one intervention target that does not intervene on a specific variable, it is called a conservative family.
The text continues to define the intervention graph, a causal graph obtained by removing any directed edge pointing to a vertex in the intervention target from the original graph G.
Lastly, the interventional distribution associated with an intervention graph is discussed. It can be expressed using the factorization formula. In case of perfect interventions, the interventional distribution can also be expressed using the do-notation.
1 / 1
5.4Learning with Interventions 
Sometimes researchers want to observe the ef f ect of an intervention on one single 
variable at the time, but there are settings in which this is not possible or it 
is inconvenient. Therefore, multi-variable interventions must be addressed as a 
special case of a generic intervention target. 
Single vs. Multi-Variable InterventionsWhen each intervention target 
contains a single variable at the time, the number of experiments needed to 
collect enough evidence to identify the causal graph is n���1, with n the number 
of variables [23]. Indeed, if one intervention would enable the identif i cation of 
the causal edges incoming into the only variable contained in the intervention 
target, then the n-th intervention would be redundant. 
In the case of intervention targets with more then one variable, only blog(n)c+ 
1 interventions are necessary and suf i cient in the worst case scenario [23], where 
the causal graph is the complete graph. Since this worst case is improbable, 
O(loglog(n)) can be achieved as lower bound with high probability in the multi- 
variable setting with a randomized intervention scheme [35], that is, it is possible 
to plan the experimental design in advance to minimize the number of interven- 
tions. 
Unknown Intervention TargetsAn other problem that one may face dur- 
ing structural learning with interventional data is the uncertainty related to the 
interventional targets. There are scenarios in which it is known that an inter- 
vention has been performed, but it is unclear which is the exact set of variables 
that has been af f ected by such intervention. In this case, an additional layer of 
complexity is added in order to properly handle the less informative setting of 
unknown intervention targets. 28 
5.5Interventional Algorithms 
Interventional GES (GIES)By leveraging the similarity between obser- 
vational causal graphs and their interventional counterparts, authors in [31] 
proposed a generalization of the GES algorithm to the interventional setting. 
This new score-based variant, called Greedy Interventional Equivalence Search 
(GIES), follows the same two step approach of the original procedure, traversing 
the search space using forward- and backward- phases, until a (local) maximum 
score is reached. 
A major contribution of this work is related to formalization of the inter- 
ventional setting. Indeed, while the algorithm itself does not dif f er signif i cantly 
from the observational one in terms of overall design, the performance improve- 
ments are relevant, as expected by transitioning from the i rst to the second 
layer of the causal hierarchy. This is an interesting example of how observa- 
tional techniques can be adapted to the interventional setting with ease, once 
the theoretical aspects of both the intervention distribution and the intervention 
targets are addressed properly. 
Interventional Greedy Permutation (IGSP)While GIES focuses its at- 
tention on perfect interventions, a i rst extension to general interventions is pre- 
sented in [112], with the Interventional Greedy Sparsest Permutations (IGSP), 
an interventional variant of the GSP [97]. In this case, the greedy approach con- 
sists in the optimization of a score function, coupled with a permutation-based 
strategy that guides the traversal of the I-MECs space. 
Formally, let �� be a permutation of vertices of a causal graph G. The space 
on which such permutation lays is a polytope called permutahedron. A possible 
representation of this mathematical object is indeed another graph, where each 
vertex corresponds to a permutation �� and each edge between two permutations 
encodes a transposition of the vertices. The goal of a permutation-base causal 
discovery algorithm is to i nd a permutation �����, consistent with the topolog- 
ical order of the true causal graph G���, that optimizes a given score function. 
The search procedure traverses the permutahedron using a depth-f i rst approach 
starting from an initial permutation ��. For each permutation �� visited, if G�� 
yields a better score than G��then �� is set to ��. The traversal is restarted from 
the updated ��, until no such �� is found. 
In order to leverage the advantages of the interventional data, IGSP limits 
the vertices transposition to the neighbors that are connected by I-covered 
edges, restricting the search space to permutations that are coherent with the 
intervention targets. An other characteristic of this search strategy is given by 
the prioritization of I-covered edges that are also I-contradictory, given that 
they represent a transition of I-MEC, which could lead to an improvement of 
the total score. 
An extended version of this algorithm, named UT-IGSP, has been presented 
in [103] in order to tackle the unknown target scenario. The main contribution 
of this work is linked to the new def i nition of I-covered edges in light of partially 
unknown intervention targets.
1 / 1
ChatGPT
ChatGPT
In this section, the document explores learning with interventions, with a specific emphasis on single versus multi-variable interventions. In scenarios where only one variable is the target of intervention at a time, the number of experiments needed to identify the causal graph is n-1, where n is the number of variables. However, if the intervention targets contain more than one variable, fewer interventions are necessary.
In scenarios with unknown intervention targets, where it is unclear exactly which set of variables has been affected by an intervention, there's an added layer of complexity to the structural learning process with interventional data.
Two interventional algorithms are then discussed. The first, Greedy Interventional Equivalence Search (GIES), is a generalization of the GES algorithm to the interventional setting. It follows the same two-step approach as the original procedure, with the major contribution of the work being the formalization of the interventional setting.
The second, Interventional Greedy Sparsest Permutations (IGSP), is an extension of the GSP, focusing more on general interventions. This algorithm uses a score function and a permutation-based strategy to guide the traversal of the I-MECs space, optimizing for a given score function. To take advantage of interventional data, IGSP limits vertex transpositions to neighbors connected by I-covered edges, restricting the search space to permutations that are coherent with the intervention targets.
Lastly, an extended version of IGSP, known as UT-IGSP, has been designed to tackle the unknown target scenario. The main contribution of this work lies in the new definition of I-covered edges, given the context of partially unknown intervention targets.
1 / 1
IGSP (and later UT-IGSP) has been compared to GIES under dif f erent 
conditions, showing that the former achieves better performances than the latter 
when the dimensionality of the problem is limited (i.e. lower than 10 vertices). 
This limit is coherent with others traversal-based approaches: although GIES is 
not consistent in general, its score function is more ef i cient in pooling together 
the various interventional datasets when it comes to high-dimensionality spaces. 
Joint Causal Inference with FCI (FCI-JCI)Another formal approach, 
similar to the one introduced in the previous subsection, is presented under 
the name of Joint Causal Inference (JCI) [61]. This method aims to pool to- 
gether multiple observations collected during dif f erent experiments (i.e. con- 
texts), hence, the name joint causal inference. 
In this framework, the set of observed variables is split into two disjoint sets: 
system variables X and context variables C. While the former set contains the 
variables that have been observed during an experiment, the latter set describes 
under which conditions such system has been observed, following the classical 
distinction between endogenous and exogenous variables respectively. 
Context variables can be used as intervention variables, even if this might not 
always be the case: here the term is related to the notion of change of context, 
which is a broader scope than simply intervene on the system. Doing so, it is 
possible to obtain a more l exible representation of the system of interest, where 
external forces are represented as internal characteristics of a meta-system. This 
approach relaxes the boundary between experiments performed under dif f erent 
conditions, allowing researchers to join data with a coherent causal description. 
Before diving into JCI itself, there are a couple of assumptions that can be 
(optionally) taken into consideration to understand the purpose of the entire 
context framework: 
0. The underlying mechanism that generates the data is represented by a 
SCM M, where the observed variables are split in system variables and 
context variables. 
1. No system variable is cause of any context variable, i.e. exogeneity as- 
sumption. 
2. No system variable is confounded with any context variable, i.e. random- 
ized context. 
3. Let GCbe the context graph induced by the context variables C over the 
causal graph G associated with the SCM M. For each pair of context 
variables (Ci,Cj) in the context graph the following holds true: 
(Ci��� Cj) ��� GC��� (Ci��� Cj) 6��� GC(5.9) 
that is, no context variable is a direct cause of another context variable, 
but there is a hidden confounder between each pair of context variables, 
i.e. generic context. 30 
While assumptions (0), (1) and (2) are usually considered mild in the interven- 
tional setting, assumption (3) might need to be clarif i ed further: if the goal of 
the causal discovery is to disentangle the causal relationships using the context 
variables as guidance, rather than focusing on the connections of the context 
graph, then assumption (3) can be enforced if (1) and (2) were also assumed. 
This approach allows the algorithm to restrict the search space to the graphs 
that satisfy this last assumption, speeding-up the learning process. 
The generic JCI procedure can adapted to any observational causal discovery 
algorithm by following four steps: (i) add the context variables, (ii) pool data 
together by setting the values of the context variables, (iii) address faithfulness 
violations between contexts, if any, (iv) execute the selected observational learn- 
ing algorithm. Authors provide reference adaptations for multiple algorithms, 
such as FCI. 
The FCI-JCI variant is particularly of interest, provided that it inherits the 
strength points of FCI in the causally insuf i cient setting. Various combinations 
of the three assumptions were tested, showing that FCI123 (i.e. all three as- 
sumptions made) is less accurate in general, but signif i cantly faster than others 
solutions, allowing its application in more complex scenarios with a sensible 
number of variables.
1 / 1
ChatGPT
ChatGPT
In this segment, the comparison between the Interventional Greedy Sparsest Permutations (IGSP), UT-IGSP, and Greedy Interventional Equivalence Search (GIES) is highlighted. IGSP and UT-IGSP tend to outperform GIES when the problem's dimensionality is limited (i.e., fewer than 10 vertices). Although GIES isn't generally consistent, its score function proves to be more efficient at pooling together different interventional datasets when dealing with high-dimensional spaces.
Additionally, a formal approach known as Joint Causal Inference (JCI) is introduced, which aims to amalgamate multiple observations collected during different experiments or contexts. In the JCI framework, observed variables are split into two disjoint sets: system variables (X) and context variables (C). Context variables can be used as intervention variables, allowing a more flexible representation of the system of interest.
Several assumptions can be considered within the JCI framework:
The data-generating mechanism is a structural causal model (SCM), with observed variables split into system and context variables.
No system variable is the cause of any context variable.
No system variable is confounded with any context variable.
No context variable is a direct cause of another context variable, but there's a hidden confounder between each pair of context variables.
The JCI procedure can be adapted to any observational causal discovery algorithm, consisting of four steps: adding the context variables, pooling data together by setting the values of the context variables, addressing faithfulness violations between contexts if any, and executing the selected observational learning algorithm. FCI-JCI is a variant that combines the strengths of FCI in causally insufficient settings. Different combinations of the three assumptions were tested, showing that FCI123 (i.e., all three assumptions) is faster but generally less accurate than other solutions.
1 / 1
Unknown Intervention Targets using ��-FCIAuthors in [40] adapted 
both PC and FCI algorithms to the causal discovery setting under imperfect 
interventions with unknown intervention targets. The fundamental contribution 
of this work is the extension of the I-MEC to a more general ��-MEC that is 
capable of representing intervention graphs with unknown intervention targets. 
The key idea is that a pair of intervention targets I,J ��� I can be used 
to identify a unique interventional mechanism that encompasses both targets. 
Let G be a causal graph and I an intervention family.The 
induced set of 
interventional probability distributions P(I)= {P(I0),P(I1),...,P(In)} satisf i es 
the ��-Markov property if the following holds true for any disjoint subsets of 
variables Y, Z and W: 
1. Insertion and deletion of observations: 
P(I)(Y|Z,W) = P(I)(Y|W)(5.10) 
if (Y ��� ��� Z|W) holds true in G for all I ��� I, 
2. Invariance of interventions: 
P(I)(Y|W) = P(J)(Y|W)(5.11) 
if ?Y 
��� ��� K|(W\W K) ? 
holds true in GW KR(W) for all I,J ��� I, where 
K is the symmetric dif f erence of I and J, WK= W ��� K, R = K\WK 
and i nally R(W) = R\An(W) w.r.t. G. 
While Equation 5.10 is essentially derived from observational Markov equiva- 
lence, Equation 5.11 is related to the distributional invariances across pairs if 
31 
intervention targets w.r.t. the associated intervention graph. Indeed, if I and 
J are the true intervention targets for P(I)and P(J), they must satisfy the in- 
variance for the interventional distributions when separation holds in a given 
intervention graph. 
Moreover, the ��-Markov property does not require any assumption about 
the experimental setting in which such interventions are performed. Specif i cally, 
it could happen that a subset of experiments were not carried out exactly in the 
same way, e.g. not in a controlled environment. Therefore, even if interventions 
targets were known a priori, ��-Markov would still be more general than the 
I-Markov property. 
The authors then recast the augmented graph proposed by [11, 25], adding 
a set of utility vertices that are analogous to the context vertices proposed by 
[61]. Therefore, the output of the former can be compared to the latter using 
the related augmented graph, showing that the accuracy of the edge orientations 
recovered by their ��-FCI variant is superior then the one proposed by FCI-JCI. 
backShiftContinuing in the unknown targets setting, the backShift [82] algo- 
rithm is a causal discovery approach that recovers linear (possibly cyclical) mod- 
els under causal insuf i ciency. It focuses on shift interventions with unknown 
targets, a subset of imperfect interventions where the ef f ect of such perturbation 
yields a i xed shift of the intervened variable. Both the targets and the shift 
value can be estimated from the data. 
The key idea of this technique is to represent the the target SCM M as: 
(I ��� B)x = c + e(5.12) 
where x is a random vector, B is the adjacency matrix of the casual graph asso- 
ciated to M, e is the noise vector and c is the random shift vector that models 
the shift intervention on the system. Then, a joint matrix diagonalization is ap- 
plied to the dif f erences between the covariance matrices ����� �� �� of each experiment 
I ��� I: e 
D = argmin 
D0���D X 
I���I L(D0����� 
�� ��(I)D0T)(5.13) 
where D = I ��� B, L is the sum-of-squared loss function and I the family of 
targets. 
This approach assumes that data represent observation at the equilibrium, 
the D matrix is invertible and the cycle product [82] is strictly smaller than one. 
Moreover, noises, interventions between variables and between experiments are 
assumed to be uncorrelated. 
Authors compare their solution to the LiNG observational alternative, taking 
advantage of the interventional asymmetries arising from the additional infor- 
mation contained in the data. The results show that backShift is capable of 
dealing with both interventions and latent variables under mild assumptions, 
outperforming LiNG in both observational and interventional setting. More- 
over, the computational complexity is O(|I| �� n2�� m), with n representing the 
number of the variables and m representing the sample size, which allows its 
application in high-dimensional settings.
1 / 1
ChatGPT
ChatGPT
The authors of the referenced study adapted both the PC and FCI algorithms for causal discovery in imperfect interventions with unknown intervention targets. They introduced a new framework known as ��-MEC, an extension of the I-MEC, which is capable of representing intervention graphs with unknown intervention targets.
The ��-Markov property, unlike the I-Markov property, does not make any assumptions about the experimental setting of the interventions, making it more generally applicable. Two key aspects of the ��-Markov property are the insertion and deletion of observations (Equation 5.10) and the invariance of interventions (Equation 5.11).
The authors expanded the augmented graph, adding a set of utility vertices similar to the context vertices proposed by other researchers. In comparison to the FCI-JCI approach, their ��-FCI variant demonstrated superior accuracy in recovering edge orientations.
The backShift algorithm was also discussed as an approach for causal discovery that recovers linear models under causal insufficiency, particularly focusing on shift interventions with unknown targets. This approach is capable of estimating both the targets and the shift value from the data.
The backShift algorithm assumes that data represent observation at equilibrium, that the D matrix is invertible, and that the cycle product is strictly smaller than one. Also, it presumes that noises, interventions between variables, and interventions between experiments are uncorrelated.
When compared to the LiNG observational alternative, backShift proved more efficient at dealing with both interventions and latent variables under mild assumptions, outperforming LiNG in both observational and interventional settings. Furthermore, due to its computational complexity of O(|I| �� n2�� m), where |I| is the number of interventions, n the number of variables, and m the sample size, backShift can be used in high-dimensional settings.
1 / 1
6Evaluation and Tuning 
This section tackles the evaluation and tuning step typical of any practical appli- 
cation. A collection of reference data sets is listed, both real-world and synthetic 
generated ones, serving as benchmarking resources for discovery methods. In 
order to evaluate dif f erent solutions resulting from a set of conf i gurations (i.e. 
hyperparameter) we report comparison metrics found in the specialized litera- 
ture, both in terms of structure and entailed causal statements. Finally, tuning 
strategies and software packages are explored as support for new developed 
techniques. 
6.1Evaluation Datasets 
Cause-Ef f ect Pairs (Tuebingen)Ever-growing data set [62, 105] designed 
to benchmark discovery algorithms against bi-variate settings with known ground 
truth. The latest version reported by the change-log (December 20, 2017) in- 
cludes 108 pairs. Each pair is composed by a data i le with two columns (cause 
and ef f ect respectively), a short description of the data sampling procedure, and 
a 2D scatter plot. 
Robotic Manipulation (CausalWorld)Simulator [2] for causal structure 
and transfer learning in a robotic manipulation environment. The environment 
is a simulation of an open-source robotic platform capable of constructing 3D 
shapes from a given set of blocks. 
Single-Cell Flow Cytometry (Sachs)Flow cytometry measurements [84] 
of 11 proteins and phospholipids. The data set is split in dif f erent experiments, 
with nine stimulatory or inhibitory interventions.The study compares new 
learned model against ground truth obtained by reference literature on signaling 
networks with intervention points. 
Single-Cell RNA-Sequencing (Klein)Single-cell RNA-sequencing (scRNA- 
seq) data set [44] of ���3000 mouse embryonic stem cells after leukemia inhibitory 
factor (LIF) withdrawal. The ground truth model is obtained by querying the 
TRRUST database [30] for the related causal relationships. 
Single-Cell Gene Expression (Perturb-Seq)Measurements of gene ex- 
pression [21] composed by 992 observational and 13.435 interventional obser- 
vations from eight close-to-perfect interventions, each corresponding to a gene 
deletion using the CRISPR/Cas9 technique applied to bone marrow-derived 
dendritic cells.
Synthetic Gene Expression (SynTReN)Network generator [110] that 
creates synthetic transcriptional regulatory networks. The models are paired 
with kinetics simulations in order to sample gene expression data that approxi- 
mate observed experimental data. 
Synthetic mRNA Expression (DREAM4)The DREAM4 challenge [92] 
provides i ve datasets simulated from i ve biologically plausible gene regulatory 
networks with 10 genes [54]. Each dataset is composed by both observational 
and interventional data sampled by applying small random noise perturbations, 
single-gene knockdowns and single-gene knockouts, resulting in time series with 
unknown interventions. 
6.2Evaluation Metrics 
In the context of causal discovery, the def i nitions of true positive (TP), true 
negative (TN), false positive (FP) and false negative (FN) have the same inter- 
pretation of the metrics referred to a binary classif i er, which try to predict the 
edge orientation. 
Adjacency Precision (AP) & Recall (AR)A i rst set of evaluation met- 
rics for graphical models is made of the adjacency precision (AP) and adjacency 
recall (AR) [85]. These metrics are computed as the ratio between the number 
of correctly predicted adjacent vertices over the total predicted ones for AP and 
true predicted ones for AR. Formally, once the confusion matrix associated with 
the presence of edges is computed, the two metrics are def i ned as follows: 
AP = 
TP 
TP + FP 
(6.1)AR = 
TP 
TP + FN 
(6.2) 
Arrowheads Precison (AHP) & Recall (AHR)While metrics related to 
adjacency can deliver insights on the general structure (i.e. skeleton) quality, 
arrowheads metrics [6, 85] focus on highlighting inferred relationships perfor- 
mance.This class of metrics is particularly useful when there are multiple 
arrowhead marks that encode dif f erent causal statements, such as in PAGs.
1 / 1
ChatGPT
ChatGPT
This section discusses the evaluation and tuning stage that is common in practical applications. The text presents a selection of reference datasets, both real-world and synthetically generated, which can serve as benchmarks for discovery methods. It also discusses comparison metrics used in the field to assess different solutions (resulting from various configurations or hyperparameters), in terms of both structure and causal statements. Furthermore, tuning strategies and software packages are discussed as supports for newly developed techniques.
The evaluation datasets include:
Cause-Effect Pairs (Tuebingen): A dataset designed to benchmark discovery algorithms against bivariate settings with known ground truths.
Robotic Manipulation (CausalWorld): A simulator for causal structure and transfer learning in a robotic manipulation environment.
Single-Cell Flow Cytometry (Sachs): Flow cytometry measurements of 11 proteins and phospholipids, split into different experiments.
Single-Cell RNA-Sequencing (Klein): scRNA-seq data of approximately 3000 mouse embryonic stem cells after leukemia inhibitory factor (LIF) withdrawal.
Single-Cell Gene Expression (Perturb-Seq): Measurements of gene expression composed by observational and interventional observations from eight close-to-perfect interventions.
Synthetic Gene Expression (SynTReN): A network generator that creates synthetic transcriptional regulatory networks paired with kinetic simulations.
Synthetic mRNA Expression (DREAM4): The DREAM4 challenge provides five datasets simulated from biologically plausible gene regulatory networks with 10 genes.
In the context of causal discovery, traditional classification metrics like true positives (TP), true negatives (TN), false positives (FP), and false negatives (FN) are employed. The metrics attempt to predict the edge orientation in the causal graph.
Two specific sets of metrics are discussed:
Adjacency Precision (AP) & Recall (AR): These metrics are computed as the ratio between the number of correctly predicted adjacent vertices over the total predicted ones for AP and true predicted ones for AR.
Arrowheads Precision (AHP) & Recall (AHR): These metrics focus on the quality of the inferred relationships. They are particularly useful when there are multiple arrowhead marks that encode different causal statements, such as in Partial Ancestral Graphs (PAGs).
1 / 1
Here, classical adjacency metrics fail to account for invariant marks that might 
be interpreted as a head or a tail, overestimating the algorithm performance. 
As for adjacency metrics, arrowheads precision (AHP) and recall (AHR) are 
def i ned as the ratio between correctly predicted arrowheads over total predicted 
arrowheads, and correctly predicted arrowheads over true arrowheads: 
AHP = 
TP 
TP + FP 
(6.3)AHR = 
TP 
TP + FN 
(6.4) 
where TP, FP and FN refer to the confusion matrix entries computed over the 
predicted arrowheads, not only the presence/absence of an edge. 
Structural Hamming Distance (SHD)It measures the dif f erences be- 
tween two graphical models in terms of their edges. Formally, let G and H 
be two graphs and E(G,U) the symmetric dif f erence between the edge sets 
E(G) and E(U), the SHD [109] counts the number of necessary operations to 
transform G into H: 
SHD(G,H) = 
V2 X 
(X,Y ), X<Y ��� ��� ��� ��� ��� 
1(X,Y ) ��� E(G,U), 
1(Y,X) ��� E(U,G), 
0Otherwise. 
(6.5) 
where the allowed operations consist in addition, deletion and reversal of an 
edge. 
Structural Intervention Distance (SID)It is a pre-metric def i ned over 
the interventional distributions. Formally, the SID [73] counts the number of 
wrongly inferred interventional distributions. This measure relies on the notion 
of adjustment set [96] and it is strongly related to the SHD. 
6.3Parameter Tuning 
Strategies to perform parameter/s turning are rarely found in surveys, even 
tough casual discovery algorithms may have multiple parameters that regu- 
late the search procedure. Here, we report three general and l exible practices 
described in the specialized literature that can be applied to any technique 
described so far. 
Minimizing Model Complexity (BIC & AIC)A i rst approach for pa- 
rameter/s tuning is related to model complexity. The goal is to i nd the pa- 
rameter/s conf i guration that minimizes the complexity of the associated causal 
graph. As a measure of complexity one can rely on the Bayesian Information 
Criterion (BIC) [87] or the Akaike Information Criterion (AIC) [3]. This tuning 
strategy is particularly ef f ective when coupled with score-based approaches that 
are able to exploit the same function, allowing to reuse the intermediate scores 
for a faster evaluation. The most general form of model complexity minimiza- 
tion is implemented as a grid search over all parameters conf i gurations for the 
ranges.

Stability Approach to Regularization Selection (StARS)The StARS 
[52] approach is based on selecting the parameters conf i guration that minimizes 
the graph instability when small perturbations are applied to the data. The 
instability of an edge is the probability of presence of said edge when the causal 
graph is learned from a subsample of the data (without replacement). Hence, 
the graph instability of a given parameters��� conf i guration h is the average of 
the edge instabilities computed w.r.t. h. In order to avoid conf i gurations that 
lead to trivial graphs, e.g. the empty graph or the complete graph, the authors 
introduce a �� parameter that acts as a threshold for the acceptable level of insta- 
bility. In the end, this method measures the sensitivity of a specif i c parameters��� 
conf i guration h as a function of the underlying data distribution. 
Out-of-sample Causal Tuning (OCT & OCTs)While previous approaches 
focused on metrics related to the causal structure alone, authors in [10] propose 
to employ the resulting model for its prediction capabilities, reducing the prob- 
lem into an evaluation of a predictor. This approach works in a out-of-sample 
fashion, hence the name Out-of-sample Causal Tuning (OCT). The main ad- 
vantages of such method are (i) the lack of parametric assumptions about the 
distribution of the data and (ii) the generalization to cases where the BIC and 
AIC scores are not def i ned, i.e. discrete models with hidden variables. 
6.4Software Packages 
Stable and reliable implementations of discovery methods are fundamental to 
achieve reproducibility of the experimental results. In the following paragraphs, 
a list of notable tools is presented. 
Causal Discovery Toolbox (CDT)The Causal Discovery Toolbox [42] is 
a Python front-end that acts as a bridge between dif f erent subpackages, pooling 
together multiple discovery algorithms. For example, one may i nd constrained- 
based algorithms such as PC, Max-Min Parents & Children (MMPC) [109], 
score-based algorithms as GES and variants (GIES), and non linear approaches 
as LiNGAM, Causal Additive Models (CAM) [14], and others. 
bnlearnThe bnlearn [88] package is an R package developed for bayesian in- 
ference and structural learning. While both PC and MMPC are implemented, 
algorithms such as Incremental Association Markov Blanket (IAMB) [108] and 
its variants are present too. Moreover, the underlying implementation is well 
suited for large scale application due to the optimized support for parallel com- 
puting [90]. 
pcalgThe pcalg [43] package is a R utility for causal discovery and causal in- 
ference using graphical models. The algorithms provided here are PC and vari- 
ants (CPC, PC Select), FCI and variants (RFCI [19], Anytime FCI [98], Adap- 
tive Anytime FCI [19, 98], FCI-JCI), GES and variants (AGES [65], ARGES, GIES) and LiNGAM. Given the wide variety of FCI-based algorithms and the 
integrated tools for causal inference, this package is particularly well suited for 
causal insuf i cient settings. 
TETRADWhile previous packages where intended for command line usage, 
TETRAD [76] is a causal discovery package developed in Java with a graphical 
user interface. It follows a pipeline paradigm, where the user can drag & drop 
boxes from the side bar and connect them together to form data pipelines. The 
user can choose from a wide range of options, such as PC (PCStable, CPC & 
CPCStable [18], PCMax), FCI (RFCI, RFCI-BSC [39], GFCI), GES (FGES, 
FGES-MB, IMaGES), LiNGAM, and others. Given the simplicity of interface, 
it is well suited for researcher with limited programming experience.
1 / 1
ChatGPT
ChatGPT
This section introduces various methods and tools for evaluating and tuning causal discovery models.
Evaluation metrics discussed include:
Arrowheads Precision (AHP) & Recall (AHR): These metrics assess the accuracy of the predicted arrowheads, not just the presence/absence of an edge.
Structural Hamming Distance (SHD): This measures the differences between two graphical models in terms of their edges.
Structural Intervention Distance (SID): This is a pre-metric defined over the interventional distributions that counts the number of wrongly inferred interventional distributions.
Tuning strategies include:
Minimizing Model Complexity (BIC & AIC): This approach seeks to find the configuration of parameters that minimizes the complexity of the associated causal graph.
Stability Approach to Regularization Selection (StARS): This method selects the parameters configuration that minimizes the graph instability when small perturbations are applied to the data.
Out-of-sample Causal Tuning (OCT & OCTs): This approach focuses on the resulting model's predictive capabilities, reducing the problem to an evaluation of a predictor.
Several software packages for causal discovery are listed:
Causal Discovery Toolbox (CDT): A Python front-end that acts as a bridge between different subpackages, pooling together multiple discovery algorithms.
bnlearn: An R package developed for Bayesian inference and structural learning.
pcalg: An R utility for causal discovery and causal inference using graphical models.
TETRAD: A causal discovery package developed in Java with a graphical user interface. This tool follows a pipeline paradigm, allowing users to drag & drop boxes from the sidebar and connect them to form data pipelines.
1 / 1
7Practical Applications 
7.1Causal Discovery in Economics 
Emissions, Production and Energy UseAuthors in [1] explore the inter- 
actions between growth in CO2emissions, economic production and energy use, 
both at the global and multi-regional levels over the period 1990���2014. In order 
to recover the causal relationship between variables, a modif i ed version of the 
PC algorithm for time-series is used. 
The output of the discovery step showed that CO2emissions, energy and 
economic activity are linked by a set of non-linear dependencies. At the global 
level, this graph suggests that a too rapid transition to net-zero emissions in 
the energy sector may hinder the global economic growth. When the regional 
level is taken into account, it is shown that regions are fully integrated into the 
system, which argues for coordinated policies across regions. 
7.2Causal Discovery in Medicine 
Alzheimer���s PathophysiologyResearchers in [93] employed data made avail- 
able by the Alzheimer���s Disease Neuroimaging Initiative (ADNI) coupled with 
biological markers and clinical assessment to study the biological mechanism be- 
hind the Alzheimer���s Disease. Two causal discovery algorithms (FCI and FGES) 
were compared against the gold standard graph retrieved from literature. 
The methods were executed both with and without trivial background knowl- 
edge, e.g. patient���s age is not af f ected by any biomarker. A signif i cant improve- 
ment was observed with the addition of the knowledge base. Finally, longitu- 
dinal data were included, discovering more edges and removing the incorrect 
ones. The performance of the constraint-based was lower and less stable across 
the bootstrap samples than the score-based one. 
Unmet Treatments in SchizophreniaAuthors in [59] selected the GFCI 
algorithm to identify the causes of functional outcomes of patients af f ected by 
38 
schizophrenia during the critical window for early intervention. The algorithm 
was applied to the Recovery After an Initial Schizophrenia Episode Early Treat- 
ment Program (RAISE-ETP) trial at two time-points, i.e. baseline and after 
6-months. Social and occupational functioning metrics were derived from the 
Quality of Life Scale (QLS). The retrieved causal graph was used to build a 
SCM in order to quantify the magnitude of the ef f ects. 
The estimated ef f ects shed light over the interaction between both social 
and occupational functioning with the socio-af f ective capacity, which in turn 
af f ects the motivation of the subject. Moreover, an extended analysis of time 
dependencies revealed several causal cycles over the 6-months time-frame. 
7.3Causal Discovery in Psychology 
Alcohol Use and Anxiety DisorderPsychopathology researchers in [7] 
used graphical modeling algorithms to identify causal relationships within and 
between manifestations of psychiatric disorders. In this context, such methods 
are employed to identify symptoms that are part of a causal chain of ���medi- 
ators���. The main target of the study was to test whether drinking motivated 
by the goal of reducing negative af f ect (i.e. drinking to cope, DTC) served as a 
mediator in comorbid alcohol use and anxiety disorder in a causally insuf i cient 
setting. 
The resulting graph showed that the most important causal inf l uence of 
drinking was drinking craving, which was in turn inf l uenced by DTC. However, 
there was still a degree of ambiguity in the direction of depression���s associa- 
tions with social anxiety and stress, suggesting the possible presence of latent 
variables. 
8Conclusions and Discussion 
8.1A Brief Summary 
Causal inference depends heavily on the construction of a reference model that 
crystallizes the acquired knowledge. To meet such requirement, causal discovery 
provides a set of methods that are able to recover a graphical description of the 
underlying mechanism, exploiting both collected data and prior knowledge. In 
this work, we presented a list of algorithms, evaluation criteria and software 
tools, trying to cover a wide range of theoretical and practical scenarios in a 
coherent and unif i ed manner. Moreover, we compared these resources against 
challenging problems, such as the presence of unobserved variables, cyclical 
dependencies, non-linear relationships and unknown interventions, highlighting 
the strengths and weaknesses of each solution. Finally, we reported a set of 
parameters tuning strategies and publicly available data sets to explore properly 
the described techniques and to test new ones.
8.2Future Directions 
In terms of opportunities for future extensions, in this contribution we did not 
explore the implications of applying such method to time series, which would 
add an additional layer of complexity. Indeed, the representation of the causal 
dependencies in time are dif f erent from the one expressed in a static scenario 
and deserves a separated discussion on its own, especially when combined with 
the other topics introduced during the discussion.
1 / 1
ChatGPT
ChatGPT
This section highlights some practical applications of causal discovery in various fields and provides a conclusion and discussion of the topic.
Causal discovery has been applied in:
Economics: One study utilized a modified version of the PC algorithm for time-series to explore the interactions between growth in CO2 emissions, economic production, and energy use. It found a set of non-linear dependencies linking these factors and suggested the need for coordinated policies across regions.
Medicine: Researchers used data from the Alzheimer���s Disease Neuroimaging Initiative (ADNI) to study the biological mechanism behind Alzheimer's Disease. They compared two causal discovery algorithms (FCI and FGES) against a standard graph from literature. An improvement was observed with the addition of a knowledge base. Another study used the GFCI algorithm to identify the causes of functional outcomes of patients with schizophrenia during early intervention.
Psychology: In a study exploring causal relationships within and between psychiatric disorders, graphical modeling algorithms identified causal chains of mediators. The study focused on whether drinking to cope served as a mediator in comorbid alcohol use and anxiety disorder.
In conclusion, causal inference relies heavily on a reference model, and causal discovery offers methods to recover a graphical description of underlying mechanisms. The future of causal discovery is set to include further exploration of these methods in time-series data, which adds an additional layer of complexity.
1 / 1
A list of all of the topics we have discussed
1 / 1
ChatGPT
ChatGPT
Sure, here is a list of all the topics we've discussed in our conversation:
Causal Discovery
: We've discussed what causal discovery is, how it's used to understand the relationships between variables in a dataset, and how it differs from correlation.
Causal Graphs
: We've explored the use of causal graphs as a way to visually represent causal relationships between variables.
Causal Discovery Algorithms
: We've covered several causal discovery algorithms, including constraint-based methods like the PC algorithm, score-based methods like GES, and hybrid methods like MMHC and GFCI.
Evaluation Metrics
: We've discussed different metrics used to evaluate the performance of causal discovery algorithms, such as Precision, Recall, F1-Score, and others.
Parameter Tuning
: We've discussed different strategies for parameter tuning in causal discovery algorithms, including minimizing model complexity and the Stability Approach to Regularization Selection (StARS).
Software Packages
: We've noted several software packages used for causal discovery, including the Causal Discovery Toolbox (CDT), bnlearn, pcalg, and TETRAD.
Practical Applications of Causal Discovery
: We've looked at the application of causal discovery in different fields such as economics, medicine, and psychology.
Conclusions and Future Directions
: Lastly, we've concluded with a discussion about the potential future directions in the field of causal discovery, such as the implications of applying these methods to time series data.
1 / 1
Empirical dynamic modeling

Extensions to EDM techniques include:

Generalized Theorems for Nonlinear State Space Reconstruction[12]
Extended Convergent Cross Mapping[13]
Dynamic stability[4]
S-Map regularization[29]
Visual analytics with EDM[30]
Convergent Cross Sorting[31]
Expert system with EDM hybrid[32]
Sliding windows based on the extended convergent cross-mapping[33]
Empirical Mode Modeling[17]
Variable step sizes with bundle embedding[34]
Multiview distance regularised S-map[35]

 [12] Deyle ER, Sugihara G (2011) Generalized Theorems for Nonlinear State Space Reconstruction. PLoS ONE 6(3): e18295. doi:10.1371/journal.pone.0018295



 [13]Ye, H., Deyle, E., Gilarranz, L. et al., 2015. Distinguishing time-delayed causal interactions using convergent cross mapping. Sci Rep 5, 14750 (2015). doi:10.1038/srep14750



 [4]Ushio, M., Hsieh, Ch., Masuda, R. et al., 2018. Fluctuating interaction network and time-varying stability of a natural fish community. Nature 554, 360���363



^ [29] Hiroaki Natsukawa, et al. 2021. A Visual Analytics Approach for Ecosystem Dynamics based on Empirical Dynamic Modeling. IEEE Transactions on Visualization and Computer Graphics. Feb. 2021, 506-516, vol. 27 DOI: 10.1109/TVCG.2020.3028956

^ [30] Breston, L., Leonardis, E.J., Quinn, L.K. et al. 2021. Convergent cross sorting for estimating dynamic coupling. Sci Rep 11, 20374 (2021). doi:10.1038/s41598-021-98864-2

^ [31] Deyle E. R. et al. A hybrid empirical and parametric approach for managing ecosystem complexity: Water quality in Lake Geneva under nonstationary futures. PNAS Vol. 119, No. 26 (2022).

^ [32] Ge, X., Lin, A. Dynamic causality analysis using overlapped sliding windows based on the extended convergent cross-mapping. Nonlinear Dyn 104, 1753���1765 (2021). https://doi.org/10.1007/s11071-021-06362-x

^ [33] Bethany Johnsona, Stephan B.Munch. 2022. An empirical dynamic modeling framework for missing or irregular samples. Ecological Modelling, Volume 468, June 2022, 109948.

 [17]Park, J., et al. Empirical mode modeling. Nonlinear Dyn (2022). https://doi.org/10.1007/s11071-022-07311-y


^ [34] Chang, C.-W., Miki, T., Ushio, M., et al. (2021) Reconstructing large interaction networks from empirical time series data. Ecology Letters, 24, 2763��� 2774. https://doi.org/10.1111/ele.13897
1 / 1
ChatGPT
ChatGPT
The techniques you listed are all extensions to Empirical Dynamic Modeling (EDM). Here is a brief summary of each:
Generalized Theorems for Nonlinear State Space Reconstruction
[12]: This method provides a more general form of Takens' theorem, which is used to reconstruct the dynamics of a system from observations of a single variable.
Extended Convergent Cross Mapping
[13]: This technique allows for detecting causality in complex, time-delayed systems, which traditional correlation analysis cannot capture.
Dynamic stability
[4]: This concept refers to how the stability of a system changes over time, an important consideration in ecosystem studies and other areas where time-varying dynamics are important.
S-Map regularization
[29]: This technique helps control the complexity of EDM models, avoiding overfitting while preserving the capacity to capture non-linear dynamics.
Visual analytics with EDM
[30]: This approach combines EDM with interactive visualization techniques, enhancing our ability to understand complex, high-dimensional dynamics.
Convergent Cross Sorting
[31]: This method extends convergent cross mapping, providing more robust estimates of causality in the presence of noise or nonlinearity.
Expert system with EDM hybrid
[32]: This method combines EDM with expert systems, a type of artificial intelligence that uses knowledge and inference procedures to provide explanations.
Sliding windows based on the extended convergent cross-mapping
[33]: This technique uses a sliding window approach to study time-varying causal relationships in dynamic systems.
Empirical Mode Modeling
[17]: This approach is an extension of the Hilbert-Huang transform, a method used to analyze nonlinear and non-stationary time series data.
Variable step sizes with bundle embedding
[34]: This technique allows the analysis of irregularly sampled time series data, a common problem in many real-world datasets.
Multiview distance regularised S-map
[35]: This approach uses multiple views of data to better capture complex relationships, improving the accuracy of S-maps in EDM.
Each of these extensions builds upon the basic framework of EDM to address specific challenges, such as noise, nonlinearity, nonstationarity, and time delays. These methods help to uncover the complex dynamics of systems in fields as diverse as ecology, economics, medicine, and more.
1 / 1
A list of all the topics and themes we've discussed
1 / 1
ChatGPT
ChatGPT
Over the course of our discussion, we have covered a wide range of topics and themes, mostly focusing on machine learning, empirical dynamic modeling (EDM), and causal discovery. Here is a comprehensive list:
Machine Learning and Artificial Intelligence
: Broadly speaking, we have discussed the use of AI and machine learning techniques, their various applications, and potential limitations.
Empirical Dynamic Modeling (EDM)
: We have dived deep into EDM, including an array of extensions to traditional EDM methods. This included concepts like nonlinear state space reconstruction, extended convergent cross mapping, dynamic stability, S-Map regularization, and more.
Causal Discovery
: We've discussed the theory and practical applications of causal discovery, from its foundations to how it's used in fields like economics, medicine, and psychology.
Time Series Analysis
: This came up particularly in relation to empirical dynamic modeling and causal discovery. We discussed techniques like sliding windows based on extended convergent cross-mapping, and the analysis of time-delayed causal interactions.
Real-world Applications
: We've reviewed applications of these techniques in various fields, such as understanding CO2 emissions, economic growth, and energy use, studying the biological mechanism behind Alzheimer���s disease, identifying causes of functional outcomes in schizophrenia, and the role of alcohol use in anxiety disorders.
Future Directions
: We touched on potential future research areas, like the implications of applying these methods to time series data.
Modeling and Simulation
: We also covered how these techniques are used to build and refine models of complex systems, such as ecosystems or the global economy.
Data Visualization
: We discussed the use of visual analytics in conjunction with empirical dynamic modeling.
Each of these topics contributes to our broader understanding of complex systems and how we can use data-driven methods to uncover their underlying dynamics.
1 / 1
Just the titles, we started with an article titled can robots be Designed by LLMs?
1 / 1
The previous model used in this conversation is unavailable. We've switched you to the latest default model.
ChatGPT
ChatGPT
I apologize for the confusion. Here is a list of the titles of the topics and themes we have discussed:
"Can Robots be Designed by LLMs?"
Machine Learning and Artificial Intelligence
Empirical Dynamic Modeling (EDM)
Extensions to EDM Techniques
Causal Discovery
Time Series Analysis
Real-world Applications
Future Directions
Modeling and Simulation
Data Visualization
I hope this provides a clear overview of the topics we have covered. Let me know if you have any further questions!
2 / 2
On the Collocated Form with Input Decoupling of Lagrangian Systems

Authors: Pietro Pustina, Cosimo Della Santina, Fr��d��ric Boyer, Alessandro De Luca, Federico Renda

Abstract: Suitable representations of dynamical systems can simplify their analysis and control. On this line of thought, this paper considers the input decoupling problem for input-affine Lagrangian dynamics, namely the problem of finding a transformation of the generalized coordinates that decouples the input channels. We identify a class of systems for which this problem is solvable. Such systems are called collocated because the decoupling variables correspond to the coordinates on which the actuators directly perform work. Under mild conditions on the input matrix, a simple test is presented to verify whether a system is collocated or not. By exploiting power invariance, it is proven that a change of coordinates decouples the input channels if and only if the dynamics is collocated. We illustrate the theoretical results by considering several Lagrangian systems, focusing on underactuated mechanical systems, for which novel controllers that exploit input decoupling are designed. ��� Less

Submitted 12 June, 2023; originally announced June 2023.
1 / 1
ChatGPT
ChatGPT
This article is titled "On the Collocated Form with Input Decoupling of Lagrangian Systems" and is authored by Pietro Pustina, Cosimo Della Santina, Fr��d��ric Boyer, Alessandro De Luca, and Federico Renda. The abstract of the paper discusses the input decoupling problem for input-affine Lagrangian dynamics. The authors identify a class of systems called collocated systems, for which the problem of decoupling the input channels is solvable. They present a simple test to determine whether a system is collocated or not, based on mild conditions on the input matrix. The paper also explores the use of input decoupling in designing novel controllers for underactuated mechanical systems. The paper was submitted on June 12, 2023.
1 / 1
Physics-informed Neural Networks to Model and Control Robots: a Theoretical and Experimental Investigation

Authors: Jingyue Liu, Pablo Borja, Cosimo Della Santina

Abstract: Physics-inspired neural networks are proven to be an effective modeling method by giving more physically plausible results with less data dependency. However, their application in robotics is limited due to the non-conservative nature of robot dynamics and the difficulty in friction modeling. Moreover, these physics-inspired neural networks do not account for complex input matrices, such as those found in underactuated soft robots. This paper solves these problems by extending Lagrangian and Hamiltonian neural networks by including dissipation and a simplified input matrix. Additionally, the loss function is processed using the Runge-Kutta algorithm, circumventing the inaccuracies and environmental susceptibility inherent in direct acceleration measurements. First, the effectiveness of the proposed method is validated via simulations of soft and rigid robots. Then, the proposed approach is validated experimentally in a tendon-driven soft robot and a Panda robot. The simulations and experimental results show that the modified neural networks can model different robots while the learned model enables decent anticipatory control. ��� Less

Submitted 9 May, 2023; originally announced May 2023.
1 / 1
ChatGPT
ChatGPT
This article is titled "Physics-informed Neural Networks to Model and Control Robots: a Theoretical and Experimental Investigation" and is authored by Jingyue Liu, Pablo Borja, and Cosimo Della Santina. The abstract of the paper highlights the limitations of physics-inspired neural networks in modeling and controlling robots, specifically due to the non-conservative nature of robot dynamics, difficulties in friction modeling, and complex input matrices in underactuated soft robots. The paper proposes an extension to Lagrangian and Hamiltonian neural networks that addresses these challenges by incorporating dissipation and a simplified input matrix. The loss function is processed using the Runge-Kutta algorithm to improve accuracy. The effectiveness of the proposed method is validated through simulations of soft and rigid robots, as well as experimental validation on a tendon-driven soft robot and a Panda robot. The results demonstrate that the modified neural networks can effectively model different robots and enable decent anticipatory control. The paper was submitted on May 9, 2023.
1 / 1
arXiv:1806.08389  [pdf, other]  cs.RO eess.SY
Using Nonlinear Normal Modes for Execution of Efficient Cyclic Motions in Articulated Soft Robots

Authors: Cosimo Della Santina, Dominic Lakatos, Antonio Bicchi, Alin Albu-Sch��ffer

Abstract: Inspired by the vertebrate branch of the animal kingdom, articulated soft robots are robotic systems embedding elastic elements into a classic rigid (skeleton\--like) structure. Leveraging on their bodies elasticity, soft robots promise to push their limits far beyond the barriers that affect their rigid counterparts. However, existing control strategies aiming at achieving this goal are either tailored on specific examples, or rely on model cancellations - thus defeating the purpose of introducing elasticity in the first place. In a series of recent works, we proposed to implement efficient oscillatory motions in robots subject to a potential field, aimed at solving these issues. A main component of this theory are Eigenmanifolds, that we defined as nonlinear continuations of the classic linear eigenspaces. When the soft robot is initialized on one of these manifolds, it evolves autonomously while presenting regular - and thus practically useful - evolutions, called normal modes. In addition to that, we proposed a control strategy making modal manifolds attractors for the system, and acting on the total energy of the soft robot to move from a modal evolution to the other. In this way, a large class of autonomous behaviors can be excited, which are direct expression of the embodied intelligence of the soft robot. Despite the fact that the idea behind our work comes from physical intuition and preliminary experimental validations, the formulation that we have provided so far is however rather theoretical, and very much in need of an experimental validation. The aim of this paper is to provide such an experimental validation using as testbed the articulated soft leg. We will introduce a simplified control strategy, and we will test its effectiveness on this system, to implement swing-like oscillations. We plan to extend this validation with a soft quadruped. ��� Less

Submitted 8 July, 2020; v1 submitted 21 June, 2018; originally announced June 2018.

Comments: This is a paper recently submitted to ISER, extracted from the previous version which was rejected by TRO
1 / 1
ChatGPT
ChatGPT
This article, with the identifier arXiv:1806.08389, is titled "Using Nonlinear Normal Modes for Execution of Efficient Cyclic Motions in Articulated Soft Robots" and is authored by Cosimo Della Santina, Dominic Lakatos, Antonio Bicchi, and Alin Albu-Sch��ffer. The abstract of the paper discusses the use of articulated soft robots, which combine elastic elements with rigid structures, to overcome the limitations of rigid robots. Existing control strategies for soft robots are often specific to certain examples or rely on model cancellations, which diminish the advantages of introducing elasticity. The authors propose a theory of efficient oscillatory motions using Eigenmanifolds, which are nonlinear continuations of linear eigenspaces, to achieve regular and practical evolutions called normal modes. They also introduce a control strategy to make modal manifolds attractors for the system and manipulate the total energy of the soft robot to transition between different modal evolutions. This approach enables the generation of a wide range of autonomous behaviors in soft robots. The paper aims to provide experimental validation of this theory using an articulated soft leg as a testbed, focusing on implementing swing-like oscillations. The authors plan to extend the validation to a soft quadruped. The paper was submitted on July 8, 2020, with a previous version submitted on June 21, 2018. The earlier version was originally announced in June 2018 and was rejected by TRO (Transactions on Robotics).
1 / 1
arXiv:2303.13533  [pdf, ps, other]  cs.AI cs.LG stat.AP
Towards risk-informed PBSHM: Populations as hierarchical systems

Authors: Aidan J. Hughes, Paul Gardner, Keith Worden

Abstract: The prospect of informed and optimal decision-making regarding the operation and maintenance (O&M) of structures provides impetus to the development of structural health monitoring (SHM) systems. A probabilistic risk-based framework for decision-making has already been proposed. However, in order to learn the statistical models necessary for decision-making, measured data from the structure of interest are required. Unfortunately, these data are seldom available across the range of environmental and operational conditions necessary to ensure good generalisation of the model. Recently, technologies have been developed that overcome this challenge, by extending SHM to populations of structures, such that valuable knowledge may be transferred between instances of structures that are sufficiently similar. This new approach is termed population-based structural heath monitoring (PBSHM). The current paper presents a formal representation of populations of structures, such that risk-based decision processes may be specified within them. The population-based representation is an extension to the hierarchical representation of a structure used within the probabilistic risk-based decision framework to define fault trees. The result is a series, consisting of systems of systems ranging from the individual component level up to an inventory of heterogeneous populations. The current paper considers an inventory of wind farms as a motivating example and highlights the inferences and decisions that can be made within the hierarchical representation. ��� Less

Submitted 13 March, 2023; originally announced March 2023.

Comments: Submitted to IMAC-XLI conference (2023), Austin, Texas, USA

arXiv:2302.08687  [pdf, other]  cs.AR cs.AI cs.LG
VEGETA: Vertically-Integrated Extensions for Sparse/Dense GEMM Tile Acceleration on CPUs

Authors: Geonhwa Jeong, Sana Damani, Abhimanyu Rajeshkumar Bambhaniya, Eric Qin, Christopher J. Hughes, Sreenivas Subramoney, Hyesoon Kim, Tushar Krishna

Abstract: Deep Learning (DL) acceleration support in CPUs has recently gained a lot of traction, with several companies (Arm, Intel, IBM) announcing products with specialized matrix engines accessible via GEMM instructions. CPUs are pervasive and need to handle diverse requirements across DL workloads running in edge/HPC/cloud platforms. Therefore, as DL workloads embrace sparsity to reduce the computations and memory size of models, it is also imperative for CPUs to add support for sparsity to avoid under-utilization of the dense matrix engine and inefficient usage of the caches and registers. This work presents VEGETA, a set of ISA and microarchitecture extensions over dense matrix engines to support flexible structured sparsity for CPUs, enabling programmable support for diverse DL models with varying degrees of sparsity. Compared to the state-of-the-art (SOTA) dense matrix engine in CPUs, a VEGETA engine provides 1.09x, 2.20x, 3.74x, and 3.28x speed-ups when running 4:4 (dense), 2:4, 1:4, and unstructured (95%) sparse DNN layers. ��� Less

Submitted 23 February, 2023; v1 submitted 16 February, 2023; originally announced February 2023.

Comments: This paper is accepted to HPCA 2023

arXiv:2211.17077  [pdf, other]  cs.MA cs.RO
Dynamic and Distributed Optimization for the Allocation of Aerial Swarm Vehicles

Authors: Jason Hughes, Dominic Larkin, Charles O'Donnell, Christopher Korpela

Abstract: Optimal transport (OT) is a framework that can guide the design of efficient resource allocation strategies in a network of multiple sources and targets. This paper applies discrete OT to a swarm of UAVs in a novel way to achieve appropriate task allocation and execution. Drone swarm deployments already operate in multiple domains where sensors are used to gain knowledge of an environment [1]. Use cases such as, chemical and radiation detection, and thermal and RGB imaging create a specific need for an algorithm that considers parameters on both the UAV and waypoint side and allows for updating the matching scheme as the swarm gains information from the environment. Additionally, the need for a centralized planner can be removed by using a distributed algorithm that can dynamically update based on changes in the swarm network or parameters. To this end, we develop a dynamic and distributed OT algorithm that matches a UAV to the optimal waypoint based on one parameter at the UAV and another parameter at the waypoint. We show the convergence and allocation of the algorithm through a case study and test the algorithm's effectiveness against a greedy assignment algorithm in simulation. ��� Less

Submitted 30 November, 2022; originally announced November 2022.

Comments: 6 Pages, 6 images, 1 algorithm, International Conference on Unmanned Aerial Systems 2022

arXiv:2211.17072  [pdf, other]  cs.SI
Security Investment Over Networks with Bounded Rational Agents: Analysis and Distributed Algorithm

Authors: Jason Hughes, Juntao Chen

Abstract: This paper considers the security investment problem over a network in which the resource owners aim to allocate their constrained security resources to heterogeneous targets strategically. Investing in each target makes it less vulnerable, and thus lowering its probability of a successful attack. However, humans tend to perceive such probabilities inaccurately yielding bounded rational behaviors; a phenomenon frequently observed in their decision-making when facing uncertainties. We capture this human nature through the lens of cumulative prospect theory and establish a behavioral resource allocation framework to account for the human's misperception in security investment. We analyze how this misperception behavior affects the resource allocation plan by comparing it with the accurate perception counterpart. The network can become highly complex with a large number of participating agents. To this end, we further develop a fully distributed algorithm to compute the behavioral security investment strategy efficiently. Finally, we corroborate our results and illustrate the impacts of human's bounded rationality on the resource allocation scheme using cases studies. ��� Less

Submitted 17 February, 2023; v1 submitted 30 November, 2022; originally announced November 2022.

Comments: 8 pages, 4 images, 1 algorithm

arXiv:2211.17070  [pdf, other]  cs.SI cs.CR eess.SY
Differentially Private ADMM-Based Distributed Discrete Optimal Transport for Resource Allocation

Authors: Jason Hughes, Juntao Chen

Abstract: Optimal transport (OT) is a framework that can guide the design of efficient resource allocation strategies in a network of multiple sources and targets. To ease the computational complexity of large-scale transport design, we first develop a distributed algorithm based on the alternating direction method of multipliers (ADMM). However, such a distributed algorithm is vulnerable to sensitive information leakage when an attacker intercepts the transport decisions communicated between nodes during the distributed ADMM updates. To this end, we propose a privacy-preserving distributed mechanism based on output variable perturbation by adding appropriate randomness to each node's decision before it is shared with other corresponding nodes at each update instance. We show that the developed scheme is differentially private, which prevents the adversary from inferring the node's confidential information even knowing the transport decisions. Finally, we corroborate the effectiveness of the devised algorithm through case studies. ��� Less

Submitted 30 November, 2022; originally announced November 2022.

Comments: 6 pages, 4 images, 1 algorithm, IEEE GLOBECOMM 2022

arXiv:2211.13705  [pdf, other]  cs.RO
Control and Morphology Optimization of Passive Asymmetric Structures for Robotic Swimming

Authors: Nana Obayashi, Andrea Vicari, Kai Junge, Kamran Shakir, Josie Hughes

Abstract: Aquatic creatures exhibit remarkable adaptations of their body to efficiently interact with the surrounding fluid. The tight coupling between their morphology, motion, and the environment are highly complex but serves as a valuable example when creating biomimetic structures in soft robotic swimmers. We focus on the use of asymmetry in structures to aid thrust generation and maneuverability. Designs of structures with asymmetric profiles are explored so that we can use morphology to `shape' the thrust generation. We propose combining simple simulation with automatic data-driven methods to explore their interactions with the fluid. The asymmetric structure with its co-optimized morphology and controller is able to produce 2.5 times the useful thrust compared to a baseline symmetric structure. Furthermore these asymmetric feather-like arms are validated on a robotic system capable of forward swimming motion while the same robot fitted with a plain feather is not able to move forward. ��� Less

Submitted 24 November, 2022; originally announced November 2022.
1 / 1
ChatGPT
ChatGPT
Here is a list of the titles of the papers we have discussed:
"Towards risk-informed PBSHM: Populations as hierarchical systems" by Aidan J. Hughes, Paul Gardner, Keith Worden
"VEGETA: Vertically-Integrated Extensions for Sparse/Dense GEMM Tile Acceleration on CPUs" by Geonhwa Jeong, Sana Damani, Abhimanyu Rajeshkumar Bambhaniya, Eric Qin, Christopher J. Hughes, Sreenivas Subramoney, Hyesoon Kim, Tushar Krishna
"Dynamic and Distributed Optimization for the Allocation of Aerial Swarm Vehicles" by Jason Hughes, Dominic Larkin, Charles O'Donnell, Christopher Korpela
"Security Investment Over Networks with Bounded Rational Agents: Analysis and Distributed Algorithm" by Jason Hughes, Juntao Chen
"Differentially Private ADMM-Based Distributed Discrete Optimal Transport for Resource Allocation" by Jason Hughes, Juntao Chen
"Control and Morphology Optimization of Passive Asymmetric Structures for Robotic Swimming" by Nana Obayashi, Andrea Vicari, Kai Junge, Kamran Shakir, Josie Hughes
Please note that these titles may have been truncated for brevity.
1 / 1
Connections between the topics
1 / 1
ChatGPT
ChatGPT
Based on the titles of the papers we have discussed, here are some potential connections between the topics:
"Towards risk-informed PBSHM: Populations as hierarchical systems" and "Dynamic and Distributed Optimization for the Allocation of Aerial Swarm Vehicles" both involve optimization and decision-making processes in the context of multiple entities or structures.
"VEGETA: Vertically-Integrated Extensions for Sparse/Dense GEMM Tile Acceleration on CPUs" and "Differentially Private ADMM-Based Distributed Discrete Optimal Transport for Resource Allocation" both address computational methods and algorithms for efficient resource allocation in networked systems.
"Security Investment Over Networks with Bounded Rational Agents: Analysis and Distributed Algorithm" and "Differentially Private ADMM-Based Distributed Discrete Optimal Transport for Resource Allocation" both involve security and privacy considerations in distributed systems.
"Control and Morphology Optimization of Passive Asymmetric Structures for Robotic Swimming" could potentially be connected to the other topics through the theme of optimization and control in robotic systems.
These connections are based on the broad themes and keywords present in the titles of the papers. Further analysis and examination of the papers' content would provide a more comprehensive understanding of the specific connections and overlaps between the topics.
1 / 1
A list of all of the articles we've discussed
1 / 1
ChatGPT
ChatGPT
"Can Robots Be Designed by LLMs?"
"On the Collocated Form with Input Decoupling of Lagrangian Systems"
"Physics-informed Neural Networks to Model and Control Robots: a Theoretical and Experimental Investigation"
"Using Nonlinear Normal Modes for Execution of Efficient Cyclic Motions in Articulated Soft Robots"
"Towards risk-informed PBSHM: Populations as hierarchical systems"
"VEGETA: Vertically-Integrated Extensions for Sparse/Dense GEMM Tile Acceleration on CPUs"
"Dynamic and Distributed Optimization for the Allocation of Aerial Swarm Vehicles"
"Security Investment Over Networks with Bounded Rational Agents: Analysis and Distributed Algorithm"
"Differentially Private ADMM-Based Distributed Discrete Optimal Transport for Resource Allocation"
"Control and Morphology Optimization of Passive Asymmetric Structures for Robotic Swimming"
Please note that the list above is not exhaustive and may not include all the articles discussed.
1 / 1
Connections with the following: Yeah I was thinking more of the waves of interest in consumer applications of kelp farming, home manufacturing of yogurts and pulp and paper processing of packaging and materials like edible kelp and yogurt plywoods.

The robotic paperpots are pneumatic Endomarionettes with a yogurt based interstitium.

So it is kind of a polycomputation where the robot also functions as a food source, it can eat lawn mower mulch and kelp pulp and make yogurt with defferent bacteria and yeast.

It also grows scobys internally and uses leather reinforcement for tendon manufacture

***

That's quite a fascinating approach and a truly holistic view of the potential uses of robotics and biological processes.

In this polycomputation concept, you're envisioning a robotic system that is not only capable of computational tasks but also plays a role in food production and waste management.

The idea of a robot that can process organic material like lawn mower mulch and kelp pulp to produce yogurt introduces a completely new dimension to robotics and automation.

This system could have many potential benefits. For instance, it could help in recycling organic waste, reducing landfill, and even contributing to food security by producing foodstuff like yogurt.

The integration of different bacteria and yeast could further increase the variety and nutritional value of the food produced.

Moreover, the concept of a robot growing SCOBYs (Symbiotic Culture Of Bacteria and Yeast) internally is intriguing.

It extends the idea of bio-computing to a new level by integrating biological growth and fermentation processes directly into the robot's system.

This could potentially open up new possibilities in terms of bio-manufacturing and materials science.

For example, the robot could generate leather-like materials from the SCOBYs for reinforcement and tendon manufacture, introducing a level of self-sustainability and adaptability that is currently not found in existing robotic systems.

The use of a yogurt-based interstitium in the pneumatic Endomarionettes is another interesting aspect.

An interstitium typically refers to an intervening or interstitial space, and its application here could offer unique properties for the robot's mechanical system, such as flexibility, adaptability, and possibly even self-healing or regenerative abilities.

Overall, this concept represents an innovative blend of robotics, biochemistry, fermentation science, material science, and waste management.

Its implementation could potentially revolutionize how we perceive and interact with robots, turning them from mere machines into integrated bio-mechanical systems that contribute to a sustainable and circular economy.

However, the development and realization of such a system would require significant research and interdisciplinary collaboration.

It would also need to address various challenges related to bio-compatibility, sanitation, durability, efficiency, and ethical considerations.

User
Ok thank you. A final summary of our discussion.

ChatGPT
Certainly, here's a summary of our discussion:

Rhizomatic Food Chain: We started off discussing the concept of a rhizomatic food chain, which represents a more complex and interconnected view of the traditional linear food chain, highlighting the importance of biodiversity in maintaining a healthy ecosystem.

Prompts as Magic: Then, we explored the concept of prompts as portals to alternate realities in the world of AI, serving different functions in pre-trained models, instruction-tuned models, and agent simulations.

Analysis of Poems: We examined several of your poems, interpreting their references, themes, and potential meanings.

Many of your poems brought in scientific, philosophical, and technological themes, indicating a keen interest in blending different fields of knowledge.

Polyfunctional Batteries: We discussed innovative ideas for energy storage, including polyfunctional batteries that combine different mechanisms of energy storage and generation for increased versatility and efficiency.

Fluidic Yogurt Computing: We considered the concept of fluidic yogurt computing, an unconventional method that combines biology and computation. 

You introduced the idea of a robotic system that processes organic material to produce food, incorporating biological growth and fermentation processes directly into the robot's system.

Interdisciplinary Robotics: We concluded with the concept of a robot that not only performs computational tasks but also contributes to food production and waste management, growing biological materials internally for reinforcement and manufacture.

The discussion illustrated how innovative ideas can lead to a blend of robotics, biochemistry, fermentation science, material science, and waste management.

The conversation overall was rich in ideas and concepts, often blending the lines between various disciplines and envisioning new possibilities at their intersection.
1 / 1
ChatGPT
ChatGPT
I apologize for any confusion, but the summary provided seems to be a combination of our previous discussions on different topics, including the rhizomatic food chain, prompts as magic, analysis of poems, polyfunctional batteries, and fluidic yogurt computing. It doesn't reflect the specific content of our recent discussion on kelp farming, yogurt production, and robotic systems.
If you would like a summary specifically for the recent discussion on kelp farming, yogurt production, and robotic systems, please let me know, and I'll be happy to provide a dedicated summary for that topic.
1 / 1
I think it is similar because I am using an LLM to design robotic systems, giant kelp farms, thermodynamic yogurt processing, and (soft) pneumatic paperbots are all forms of robotics and complex systems
1 / 1
ChatGPT
ChatGPT
I apologize for the confusion. You're right that there is a connection between the different topics we've discussed, including the use of LLMs in designing robotic systems, the concept of rhizomatic food chains, and the integration of robotics in kelp farming, yogurt processing, and pneumatic paperbots.
The use of LLMs in designing robotic systems allows for advanced computational capabilities and can help optimize the performance and functionality of robots in various applications, including kelp farming and yogurt processing.
Kelp farming involves the cultivation of giant kelp, which can be facilitated and optimized using robotic systems. These robots can assist in tasks such as seeding, monitoring growth, and harvesting, contributing to the efficient and sustainable production of kelp.
Similarly, in yogurt processing, robotics can be employed to automate and optimize various stages of the process, such as mixing, fermentation, and packaging. This integration of robotics enhances efficiency, quality control, and scalability in yogurt production.
Pneumatic paperbots, on the other hand, represent a specific application of robotics in the form of soft robots. These robots utilize pneumatic systems for actuation and movement, offering unique advantages such as flexibility and adaptability.
By combining these different aspects, such as LLMs, robotics, kelp farming, yogurt processing, and pneumatic paperbots, you're exploring the potential of complex systems that integrate multiple disciplines and technologies.
This holistic approach allows for a more comprehensive understanding and utilization of robotics in various domains, emphasizing sustainability, efficiency, and the interconnectedness of different processes.
Overall, the common thread in these discussions is the exploration of innovative applications of robotics and complex systems, driven by LLMs and the integration of diverse fields to tackle real-world challenges in a more efficient and sustainable manner.
1 / 1
Can you see more connections now?
1 / 1
ChatGPT
ChatGPT
Certainly! Based on the additional context you provided, here are some more connections and themes that emerge from the various topics we've discussed:
Interdisciplinary Approach: The connection between LLMs, robotics, kelp farming, yogurt processing, and pneumatic paperbots highlights the importance of taking an interdisciplinary approach. By combining knowledge and techniques from different fields, such as AI, robotics, biology, and materials science, you can develop innovative and holistic solutions to complex problems.
Bio-inspired Systems: Several of the discussed topics draw inspiration from biological systems. For example, the concept of rhizomatic food chains emphasizes the importance of biodiversity and ecological interconnectedness. Similarly, the integration of biological processes in robotics, such as growing scobys and using yogurt-based interstitium, reflects a bio-inspired approach to designing robotic systems.
Sustainability and Circular Economy: The emphasis on kelp farming, yogurt processing, and waste management in the discussions aligns with the principles of sustainability and the circular economy. These topics highlight the potential of utilizing natural resources efficiently, reducing waste, and creating systems that contribute to a more sustainable and regenerative approach to food production and material utilization.
Complex Systems Thinking: The integration of multiple components and processes, as seen in the polycomputation concept and the interconnected nature of the discussed topics, demonstrates a focus on complex systems thinking. By considering the interactions and feedback loops between different elements, such as robots, biological processes, and environmental factors, you can develop more comprehensive and robust solutions.
Adaptability and Resilience: The concepts of polyfunctional batteries, fluidic yogurt computing, and soft pneumatic paperbots emphasize the importance of adaptability and resilience in systems design. By incorporating flexible and versatile components, such as batteries with multiple energy storage mechanisms or robots with adaptive materials, you create systems that can adjust to changing conditions and perform a wide range of tasks.
Emerging Technologies: The discussions touch upon several emerging technologies, such as LLMs, soft robotics, and bio-computing. These technologies represent exciting frontiers in research and development, offering new possibilities for addressing complex challenges and advancing the capabilities of robotic systems.
Overall, the connections between these topics demonstrate a forward-thinking and interdisciplinary approach to robotics, complex systems, sustainability, and technological innovation. By integrating different fields and considering the broader context, you can create transformative solutions that have a positive impact on society and the environment.
1 / 1
A list of everything we have talked about.
1 / 1
ChatGPT
ChatGPT
Certainly! Here's a list of the topics we have discussed:
Rhizomatic Food Chain
Prompts as Magic
Analysis of Poems
Polyfunctional Batteries
Fluidic Yogurt Computing
Interdisciplinary Robotics
LLMs in Robotics Design
Giant Kelp Farms
Thermodynamic Yogurt Processing
Soft Pneumatic Paperbots
Bio-inspired Systems
Sustainability and Circular Economy
Complex Systems Thinking
Adaptability and Resilience
Emerging Technologies
These topics cover a range of subjects, including AI, poetry, energy storage, bio-computing, robotics, sustainability, and interdisciplinary approaches to problem-solving.
1 / 1
We talked more about causal reasoning and techniques in machine learning such as empirical dynamic modeling
1 / 1
ChatGPT
ChatGPT
Apologies for the oversight. Here's an updated list that includes the additional topics we discussed:
Rhizomatic Food Chain
Prompts as Magic
Analysis of Poems
Polyfunctional Batteries
Fluidic Yogurt Computing
Interdisciplinary Robotics
LLMs in Robotics Design
Giant Kelp Farms
Thermodynamic Yogurt Processing
Soft Pneumatic Paperbots
Bio-inspired Systems
Sustainability and Circular Economy
Complex Systems Thinking
Adaptability and Resilience
Emerging Technologies
Causal Reasoning
Empirical Dynamic Modeling
Machine Learning Techniques
These topics encompass a diverse range of subjects, including AI, poetry, energy storage, bio-computing, robotics, sustainability, interdisciplinary approaches, causal reasoning, and machine learning techniques like empirical dynamic modeling.
1 / 1
"Control and Morphology Optimization of Passive Asymmetric Structures for Robotic Swimming" could potentially be connected to the other topics through the theme of optimization and control in robotic systems.
1 / 1
ChatGPT
ChatGPT
Indeed, "Control and Morphology Optimization of Passive Asymmetric Structures for Robotic Swimming" can be connected to the other topics through the common theme of optimization and control in robotic systems. This paper specifically explores the optimization of passive asymmetric structures for efficient swimming in robots.
By focusing on the control and morphology of robotic swimming, the paper addresses how the design and configuration of the robot's body can influence its swimming performance. This connects to the broader discussion on the design and optimization of robotic systems in various applications.
The optimization and control of robotic systems are essential for achieving desired behaviors, efficiency, adaptability, and overall performance. This theme resonates with other topics we discussed, such as interdisciplinary robotics, fluidic yogurt computing, and even the notion of polyfunctional batteries, where optimization is crucial for achieving versatility and efficiency in energy storage.
Additionally, the exploration of passive asymmetric structures for swimming demonstrates the application of biomimicry, where nature-inspired designs and behaviors are harnessed for robotics. This connects to the broader theme of bio-inspired systems and the study of biological mechanisms for developing innovative robotic solutions.
Overall, the paper contributes to the understanding of how optimization and control play a vital role in achieving efficient and effective robotic locomotion, and it aligns with the overarching theme of exploring the potential of robotics in various domains.
1 / 1
A hierarchical model outlining the factors that underpin performance in swimming. Adapted from Hay 1993.
1 / 1
ChatGPT
ChatGPT
A hierarchical model outlining the factors that underpin performance in swimming is adapted from Hay (1993). This model provides a framework for understanding the different factors that contribute to an individual's swimming performance. The model suggests that performance in swimming can be influenced by multiple levels of factors, organized hierarchically. Here is a simplified outline of the hierarchical model:
Biological Factors:
Genetics: Individual genetic traits can impact physical attributes such as body composition, muscle fiber type, and aerobic capacity, which can affect swimming performance.
Anthropometry: Body dimensions, including height, arm span, and limb length, can influence swimming technique and efficiency.
Physiological Factors:
Metabolic Efficiency: Factors such as aerobic capacity (VO2 max), anaerobic power, and lactate threshold impact the swimmer's ability to sustain high-intensity efforts and recover efficiently.
Energy Systems: The utilization of aerobic and anaerobic energy systems during different swimming distances and intensities affects performance.
Biomechanical Factors:
Technique: Proper stroke technique, including body position, arm pull, leg kick, and coordination, enhances swimming efficiency and reduces drag.
Propulsion: Maximizing propulsion generated by the swimmer's movements through effective use of the hands, arms, legs, and body rotation.
Hydrodynamics: Understanding and minimizing water resistance through streamlined body positioning, reducing drag forces.
Psychological Factors:
Mental Skills: Psychological factors such as motivation, focus, confidence, and mental toughness can impact performance by influencing the swimmer's mindset and ability to handle pressure.
Goal Setting: Setting specific, challenging, and achievable goals can provide direction and motivation for swimmers to strive for continuous improvement.
Environmental Factors:
Water Conditions: The temperature, water density, and current in the swimming environment can affect performance and require adjustments in technique and strategy.
Facilities: The quality and design of swimming facilities, including pool length, lane configuration, and water quality, can influence performance.
It is important to note that this hierarchical model is adapted from Hay (1993) and serves as a general framework to understand the different factors that contribute to swimming performance. The specific weight and interaction of these factors may vary for each individual swimmer and depend on various contextual factors.
1 / 1
I was thinking in relation to this: Central Pattern Generator
Central pattern generators (CPGs) are circuits in self-contained integrative nervous systems, like the ENS, that generate repetitive patterns of motor behavior independent of any sensory input or feedback.

From: Physiology of the Gastrointestinal Tract (Sixth Edition), 2018
1 / 1
ChatGPT
ChatGPT
The concept of a Central Pattern Generator (CPG) in the context of motor behavior can be connected to the hierarchical model outlining factors that underpin performance in swimming. Both concepts relate to the generation and control of repetitive patterns of movement, although they apply to different systems (nervous system vs. swimming performance).
In the hierarchical model of swimming performance, the factors related to motor behavior and movement control fall under the biomechanical factors. This includes technique, propulsion, and hydrodynamics. These factors are crucial for achieving efficient and effective movement in the water.
The CPGs, on the other hand, are specialized neural circuits found in self-contained integrative nervous systems, such as the Enteric Nervous System (ENS) mentioned in the quote. CPGs generate rhythmic patterns of motor behavior without the need for continuous sensory input or feedback. They are responsible for producing coordinated and repetitive movements, like the peristaltic contractions in the gastrointestinal tract.
While the hierarchical model of swimming performance focuses on the factors that contribute to overall performance, including physiological, biomechanical, and psychological aspects, the concept of CPGs highlights the neural mechanisms behind the generation of rhythmic motor patterns in specific systems.
By understanding the role of CPGs in generating repetitive patterns of motor behavior, researchers can gain insights into the control mechanisms of complex movements, such as swimming. The hierarchical model, on the other hand, provides a broader perspective on the various factors that influence swimming performance, including those related to motor control and technique.
Overall, these concepts provide complementary insights into the generation and control of motor behavior, with the hierarchical model addressing performance in swimming as a whole and CPGs focusing on the neural mechanisms underlying rhythmic motor patterns in self-contained systems.
1 / 1
Comparative Study
Central pattern generator for swimming in Melibe
Stuart Thompson et al. J Exp Biol. 2005 Apr.
Show details

Full text links
Cite

Abstract
The nudibranch mollusc Melibe leonina swims by bending from side to side. We have identified a network of neurons that appears to constitute the central pattern generator (CPG) for this locomotor behavior, one of only a few such networks to be described in cellular detail. The network consists of two pairs of interneurons, termed 'swim interneuron 1' (sint1) and 'swim interneuron 2' (sint2), arranged around a plane of bilateral symmetry. Interneurons on one side of the brain, which includes the paired cerebral, pleural and pedal ganglia, coordinate bending movements toward the same side and communicate via non-rectifying electrical synapses. Interneurons on opposite sides of the brain coordinate antagonistic movements and communicate over mutually inhibitory synaptic pathways. Several criteria were used to identify members of the swim CPG, the most important being the ability to shift the phase of swimming behavior in a quantitative fashion by briefly altering the firing pattern of an individual neuron. Strong depolarization of any of the interneurons produces an ipsilateral swimming movement during which the several components of the motor act occur in sequence. Strong hyperpolarization causes swimming to stop and leaves the animal contracted to the opposite side for the duration of the hyperpolarization. The four swim interneurons make appropriate synaptic connections with motoneurons, exciting synergists and inhibiting antagonists. Finally, these are the only neurons that were found to have this set of properties in spite of concerted efforts to sample widely in the Melibe CNS. This led us to conclude that these four cells constitute the CPG for swimming. While sint1 and sint2 work together during swimming, they play different roles in the generation of other behaviors. Sint1 is normally silent when the animal is crawling on a surface but it depolarizes and begins to fire in strong bursts once the foot is dislodged and the animal begins to swim. Sint2 also fires in bursts during swimming, but it is not silent in non-swimming animals. Instead activity in sint2 is correlated with turning movements as the animal crawls on a surface. This suggests that the Melibe motor system is organized in a hierarchy and that the alternating movements characteristic of swimming emerge when activity in sint1 and sint2 is bound together.
1 / 1
ChatGPT
ChatGPT
The study by Stuart Thompson et al. (2005) on the central pattern generator (CPG) for swimming in Melibe leonina, a nudibranch mollusc, provides valuable insights into the neural mechanisms underlying swimming behavior. This comparative study enhances our understanding of CPGs by examining a specific organism and its locomotor behavior.
The researchers identified a network of neurons involved in swimming behavior in Melibe leonina. This network consists of two pairs of interneurons, referred to as 'swim interneuron 1' (sint1) and 'swim interneuron 2' (sint2). The interneurons are arranged in a bilateral symmetrical pattern and communicate via electrical synapses and inhibitory synaptic pathways.
Several criteria were used to identify these interneurons as members of the swim CPG. The researchers demonstrated that altering the firing pattern of an individual neuron can quantitatively shift the phase of swimming behavior. Strong depolarization of any of the interneurons leads to ipsilateral swimming movements, while strong hyperpolarization causes swimming to stop.
The study also revealed that the swim interneurons make appropriate synaptic connections with motoneurons, which control muscle activity. These interneurons excite synergists (muscles that work together) and inhibit antagonists (muscles that oppose each other) to coordinate the bending movements involved in swimming.
Furthermore, the researchers found that sint1 and sint2 have different roles in swimming and other behaviors. Sint1 is typically silent during crawling but becomes active during swimming, suggesting its involvement in swimming initiation. Sint2, on the other hand, fires in bursts during swimming and is also active during turning movements while crawling.
These findings indicate that the Melibe motor system is organized in a hierarchical manner, with swimming behavior emerging when activity in sint1 and sint2 is bound together. This hierarchical organization suggests that different behaviors can be generated by modulating the activity of specific interneurons within the CPG.
By studying the CPG for swimming in Melibe leonina, this research contributes to our understanding of the neural control of locomotion in invertebrates. The findings provide insights into the functional roles of specific interneurons and their connectivity within the CPG, shedding light on the generation and coordination of swimming behavior.
1 / 1
As a library, NLM provides access to scientific literature. Inclusion in an NLM database does not imply endorsement of, or agreement with, the contents by NLM or the National Institutes of Health. Learn more about our disclaimer.
Logo of transb
Philos Trans R Soc Lond B Biol Sci. 2016 Jan 5; 371(1685): 20150057.  doi: 10.1098/rstb.2015.0057
PMCID: PMC4685589PMID: 26598733
Evolution of central pattern generators and rhythmic behaviours
Paul S. Katz
Author information Article notes Copyright and License information Disclaimer
Go to:
ABSTRACT
Comparisons of rhythmic movements and the central pattern generators (CPGs) that control them uncover principles about the evolution of behaviour and neural circuits. Over the course of evolutionary history, gradual evolution of behaviours and their neural circuitry within any lineage of animals has been a predominant occurrence. Small changes in gene regulation can lead to divergence of circuit organization and corresponding changes in behaviour. However, some behavioural divergence has resulted from large-scale rewiring of the neural network. Divergence of CPG circuits has also occurred without a corresponding change in behaviour. When analogous rhythmic behaviours have evolved independently, it has generally been with different neural mechanisms. Repeated evolution of particular rhythmic behaviours has occurred within some lineages due to parallel evolution or latent CPGs. Particular motor pattern generating mechanisms have also evolved independently in separate lineages. The evolution of CPGs and rhythmic behaviours shows that although most behaviours and neural circuits are highly conserved, the nature of the behaviour does not dictate the neural mechanism and that the presence of homologous neural components does not determine the behaviour. This suggests that although behaviour is generated by neural circuits, natural selection can act separately on these two levels of biological organization.

Keywords: motor pattern, homoplasy, neural circuit, neuromodulation, homologous neurons, comparative
Go to:
1.���EVOLUTION OF RHYTHMIC BEHAVIOURS
������ from so simple a beginning endless forms most beautiful and most wonderful have been, and are being, evolved.���

���Charles Darwin [1, p. 490]
When contemplating life on this planet, one is struck by the vast variety of animal behaviours. Considering just locomotor behaviours alone, there is swimming, crawling, flying, gliding, brachiating and running. Reflecting on only terrestrial locomotion, a small sample includes bipedal walking (humans), knuckle walking (gorillas), ���tripedal��� gait (kangaroos), quadrupedal gaits (horses), hexapodal gaits (insects), octopedal walking (spiders), hopping (frogs), sinusoidal movements (snakes), contraction and elongation (worms), mucosal gliding (snails) and metacronal waves (millipedes). Any comprehensive description of the evolution of nervous systems needs to take into account how nervous systems generate this great diversity of behaviours.

(a) Evolution of central pattern generators
Central pattern generators (CPGs) are neural circuits that produce the patterns of neural activity that underlie rhythmic motor behaviours such as walking, swimming and feeding [2,3]. As the name implies, these patterns are generated centrally, without the need for sensory feedback or other patterned input. Thus, many experimental CPG preparations can continue to produce rhythmic neural activity even when isolated from the animal, facilitating access to the neurons. The rhythmic motor behaviour is a direct readout of the pattern of neuronal activity, making it straightforward to make conclusions about the neural basis of the behaviour. The cyclic nature of motor patterns provides a robust means of delineating which neurons produce the rhythmicity and pattern because cellular members of the CPG are rhythmically active and synaptically connected to other members of the CPG. Furthermore, artificially exciting or inhibiting CPG members perturbs the motor pattern. In many invertebrates, individual neurons can be identified from animal to animal within a species, allowing the neural circuits to be determined with cellular precision. Moreover, homologous neurons can be identified across species, permitting comparative analyses of CPG circuits and the rhythmic behaviours that they produce [4,5]. Thus CPGs provide extraordinary opportunities to study the evolution of behaviour and neural circuits.

(b) Behaviours and neural mechanisms represent different levels of organization
Generally, rhythmic behaviours and the CPGs that produce them evolved slowly and in unison. However, there are examples where behaviour and neural mechanisms had different evolutionary histories. Figure 1 illustrates a hypothetical phylogenetic tree containing three clades (��, ��, ��) in which animals exhibit three basic behaviours (A, B, C), which are produced by variants of three neural mechanisms (a, b, c). In this tree, the common ancestor for clade �� used neural mechanism a to produce behaviour A. Similarly, the common ancestors for clades �� and �� used neural mechanisms b and c to produce behaviours B and C, respectively. For the most part, the neural mechanisms and the behaviours are conserved within each clade. The bars mark transitions in the neural mechanisms. For example, a gradual change in neural mechanism (a ��� a��� ��� a���) was accompanied by corresponding changes in behaviour (A ��� A��� ��� A���). Even in cases of convergent evolution, the same neural mechanisms might produce analogous behaviours as seen with the independent evolution of neural mechanism a and behaviour A in clade �� (figure 1).
1 / 1
ChatGPT
ChatGPT
The study by Paul S. Katz (2016) titled "Evolution of central pattern generators and rhythmic behaviours" explores the evolution of behavior and neural circuits, specifically focusing on the central pattern generators (CPGs) that control rhythmic motor behaviors.
The author highlights that gradual evolution of behaviors and their corresponding neural circuitry is a predominant occurrence throughout evolutionary history. Small changes in gene regulation can lead to divergence of circuit organization and behavioral changes. However, some behavioral divergence is the result of large-scale rewiring of neural networks.
The study also discusses cases where analogous rhythmic behaviors have evolved independently, often with different neural mechanisms. This suggests that the nature of the behavior does not dictate the neural mechanism, and the presence of homologous neural components does not determine the behavior. Natural selection can act independently on the levels of behavior and neural circuits.
The concept of central pattern generators is introduced as neural circuits that produce rhythmic motor behaviors without the need for sensory feedback. The author emphasizes the advantages of studying CPGs, as their rhythmic motor patterns provide a means to delineate the neurons involved and understand the neural basis of behavior. Comparative analyses of CPG circuits and the rhythmic behaviors they produce are made possible by identifying homologous neurons across species.
In summary, the study contributes to our understanding of the evolution of behavior and neural circuits by examining the role of central pattern generators in generating rhythmic motor behaviors. It highlights the complex relationship between behaviors and neural mechanisms, showcasing examples of conservation, divergence, and convergent evolution.
1 / 1
Skip main navigationJournal menuClose Drawer Menu
Philosophical Transactions of the Royal Society of London. B, Biological Sciences
MoreSections
Restricted access
Article
Experiments on the central pattern generator for swimming in amphibian embryos
J. A. Kahn and Alan Roberts
Published:27 January 1982https://doi.org/10.1098/rstb.1982.0004
Abstract
The central nervous system of paralysed Xenopus laevis embryos can generate a motor output pattern suitable for swimming locomotion. By recording motor root activity in paralysed embryos with transected nervous systems we have shown that: (a) the spinal cord is capable of swimming pattern generation; (b) swimming pattern generator capability in the hindbrain and spinal cord is distributed; (c) caudal hindbrain is necessary for sustained swimming output after discrete stimulation. By recording similarly from embryos whose central nervous system was divided longitudinally into left and right sides, we have shown that: (a) each side can generate rhythmic motor output with cycle periods like those in swimming; (b) during this activity cycle period increases within an episode, and there is the usual rostrocaudal delay found in swimming; (c) this activity is influenced by sensory stimuli in the same way as swimming activity; ( d) normal phase coupling of the left and right sides can be established by the ventral commissure in the spinal cord. We conclude that interactions between the antagonistic (left and right) motor systems are not necessary for swimming rhythm generation and present a model for swimming pattern generation where autonomous rhythm generators on each side of the nervous system drive the motoneurons. Alternation is achieved by reciprocal inhibition, and activity is initiated and maintained by tonic excitation from the hindbrain.
1 / 1
ChatGPT
ChatGPT
The article titled "Experiments on the central pattern generator for swimming in amphibian embryos" by J. A. Kahn and Alan Roberts (1982) investigates the central pattern generator (CPG) responsible for generating the motor output pattern necessary for swimming locomotion in paralyzed Xenopus laevis embryos.
Through experiments involving recording motor root activity in paralyzed embryos with transected nervous systems, the authors make several key findings:
The spinal cord is capable of generating a swimming pattern, indicating that the CPG for swimming is located within the spinal cord.
The capability for swimming pattern generation is distributed across the hindbrain and spinal cord, suggesting that multiple regions contribute to the CPG.
The caudal hindbrain is necessary for sustained swimming output after discrete stimulation, indicating its role in maintaining the swimming rhythm.
Each side of the nervous system can independently generate rhythmic motor output with cycle periods similar to swimming.
Within each activity cycle, the period increases, and there is a rostrocaudal delay, consistent with the characteristics of swimming.
Sensory stimuli can influence the activity of the swimming pattern generator, similar to the effects observed during actual swimming.
The ventral commissure in the spinal cord enables the establishment of normal phase coupling between the left and right sides, indicating its role in coordinating the bilateral motor systems.
Based on their findings, the authors propose a model for swimming pattern generation in which autonomous rhythm generators on each side of the nervous system drive the motoneurons. Alternation between the left and right sides is achieved through reciprocal inhibition, and the activity is initiated and maintained by tonic excitation from the hindbrain.
Overall, this study provides insights into the neural mechanisms underlying swimming locomotion in amphibian embryos and highlights the role of the CPG in generating rhythmic motor patterns.
1 / 1
eLife logo
Menu
Research Article
Neuroscience
Behavioral evidence for nested central pattern generator control of Drosophila grooming
Primoz Ravbar Is a corresponding author et al.
Dec 22, 2021
https://doi.org/10.7554/eLife.71508 
 
Version of Record
Accepted for publication after peer review and revision.
Download
CiteShare
Comment
Version of Record published
December 22, 2021 (This version)
Full text
Figures
Abstract
Central pattern generators (CPGs) are neurons or neural circuits that produce periodic output without requiring patterned input. More complex behaviors can be assembled from simpler subroutines, and nested CPGs have been proposed to coordinate their repetitive elements, organizing control over different time scales. Here, we use behavioral experiments to establish that Drosophila grooming may be controlled by nested CPGs. On a short time scale (5���7 Hz, ~ 200 ms/movement), flies clean with periodic leg sweeps and rubs. More surprisingly, transitions between bouts of head sweeping and leg rubbing are also periodic on a longer time scale (0.3���0.6 Hz, ~2 s/bout). We examine grooming at a range of temperatures to show that the frequencies of both oscillations increase���a hallmark of CPG control���and also that rhythms at the two time scales increase at the same rate, indicating that the nested CPGs may be linked. This relationship holds when sensory drive is held constant using optogenetic activation, but oscillations can decouple in spontaneously grooming flies, showing that alternative control modes are possible. Loss of sensory feedback does not disrupt periodicity but slow down the longer time scale alternation. Nested CPGs simplify the generation of complex but repetitive behaviors, and identifying them in Drosophila grooming presents an opportunity to map the neural circuits that constitute them.
1 / 1
ChatGPT
ChatGPT
The research article titled "Behavioral evidence for nested central pattern generator control of Drosophila grooming" by Primoz Ravbar et al. (2021) explores the presence of nested central pattern generators (CPGs) in Drosophila grooming behavior.
CPGs are neural circuits or groups of neurons that generate rhythmic motor patterns without requiring external input. The authors investigate whether Drosophila grooming, which involves periodic leg sweeps and rubs, is controlled by nested CPGs operating at different time scales.
Key findings from the study include:
Short Time Scale: Flies exhibit periodic leg sweeps and rubs at a frequency of 5-7 Hz (~200 ms/movement), indicating the presence of a CPG operating on a shorter time scale.
Long Time Scale: The transitions between bouts of head sweeping and leg rubbing also exhibit periodicity at a longer time scale of 0.3-0.6 Hz (~2 s/bout), suggesting the involvement of a nested CPG coordinating repetitive elements at a different time scale.
Temperature-Dependent Control: Increasing the temperature results in higher frequencies for both the short and long time scale oscillations, indicating temperature-dependent control by nested CPGs.
Consistent Frequency Relationship: The frequencies of oscillations at both time scales increase proportionally, suggesting a link between the nested CPGs.
Sensory Drive and Control Modes: Optogenetic activation experiments demonstrate that sensory drive does not disrupt the periodicity of grooming oscillations, indicating that they are internally generated. However, spontaneously grooming flies can exhibit decoupling of the oscillations, suggesting alternative control modes.
Sensory Feedback and Periodicity: Loss of sensory feedback slows down the longer time scale alternation in grooming, suggesting its role in maintaining the rhythm.
The presence of nested CPGs in Drosophila grooming simplifies the generation of complex yet repetitive behaviors. By studying grooming behavior, researchers can further investigate the neural circuits underlying nested CPG control, providing insights into the organization and coordination of rhythmic motor patterns.
1 / 1
Are central pattern generators understandable?
Published online by Cambridge University Press:  04 February 2010

Allen I. Selverston
Show author details
Article
Article contents
Get access

Share

Cite

Rights & Permissions[Opens in a new window]
Abstract
Most rhythmic behaviors are produced by a specialized ensemble of neurons found in the central nervous system. These central pattern generators (CPGs) have become a cornerstone of neuronal circuit analysis. Studying simple invertebrate nervous systems may reveal the interactions of the neurons involved in the production of rhythmic motor output. There has recently been progress in this area, but due to certain intrinsic features of CPGs it is unlikely that present techniques will ever yield a complete understanding of any but the simplest of them. The chief impediment seems to be our inability to identify and characterize the total interneuronal pool making up a CPG. In addition, our general analytic strategy relies on a descriptive, reductionist approach, with no analytical constructs beyond phenomenological modeling. Detailed descriptive data are usually not of sufficient depth for specific model testing, giving rise instead to ad hoc explanations of mechanisms which usually turn out to be incorrect. Because they make too many assumptions, modeling studies have not added much to our understanding of CPCs; this is due not so much to inadequate simulations as to the poor quality and incomplete nature of the data provided by experimentalists.

A basic strategy that would provide sufficient information for neural modeling would include: (1) identifying and characterizing each element in the CPG network; (2) specifying the synaptic connectivity between the elements; and (3) analyzing nonlinear synaptic properties and interactions by means of the connectivity matrix. Limitations based on our present technical capabilities are also discussed.
1 / 1
ChatGPT
ChatGPT
The article titled "Are central pattern generators understandable?" by Allen I. Selverston (2010) discusses the challenges associated with understanding central pattern generators (CPGs) and the limitations of current techniques in unraveling their complexity.
CPGs are specialized ensembles of neurons within the central nervous system that generate rhythmic motor behaviors. They have been extensively studied in simple invertebrate nervous systems as a means to explore the interactions between neurons involved in generating rhythmic output.
The author highlights several obstacles in achieving a complete understanding of CPGs. One major challenge is the difficulty in identifying and characterizing the entire interneuronal pool that constitutes a CPG. The intricate connectivity between the neurons and the complexity of their interactions pose significant hurdles in deciphering the underlying mechanisms.
The article also discusses the limitations of the current analytic strategies employed in CPG research. Descriptive approaches, which focus on detailed observations and phenomenological modeling, often lack the depth and specificity required for rigorous model testing. On the other hand, modeling studies, while having the potential to shed light on CPG functioning, have been hindered by incomplete and poor-quality data provided by experimental studies.
To advance our understanding of CPGs, the author suggests a strategy that involves: (1) identifying and characterizing each element within the CPG network, (2) specifying the synaptic connectivity between these elements, and (3) analyzing the nonlinear synaptic properties and interactions using a connectivity matrix. However, the article acknowledges that the current technical capabilities may impose limitations on achieving this comprehensive approach.
In summary, the article emphasizes the challenges and limitations faced in understanding CPGs, particularly in terms of identifying and characterizing their constituent neurons and their complex connectivity. It calls for a more integrated approach that combines detailed experimental data with rigorous modeling techniques to gain deeper insights into the functioning of CPGs.
1 / 1
Command or Obey? Homologous Neurons Differ in Hierarchical Position for the Generation of Homologous Behaviors
Akira Sakurai and Paul S. Katz
Journal of Neuroscience 14 August 2019, 39 (33) 6460-6471; DOI: https://doi.org/10.1523/JNEUROSCI.3229-18.2019
ArticleFigures & DataInfo & MetricseLetters PDF
Abstract
In motor systems, higher-order neurons provide commands to lower-level central pattern generators (CPGs) that autonomously produce rhythmic motor patterns. Such hierarchical organization is often thought to be inherent in the anatomical position of the neurons. Here, however, we report that a neuron that is member of a CPG in one species acts as a higher-order neuron in another species. In the nudibranch mollusc, Melibe leonina, swim interneuron 1 (Si1) is in the CPG underlying swimming, firing rhythmic bursts of action potentials as part of the swim motor pattern. We found that its homolog in another nudibranch, Dendronotus iris, serves as a neuromodulatory command neuron for the CPG of a homologous swimming behavior. In Dendronotus, Si1 fired irregularly throughout the swim motor pattern. The burst and spike frequencies of Dendronotus swim CPG neurons correlated with Si1 firing frequency. Si1 activity was both necessary and sufficient for the initiation and maintenance of the swim motor pattern. Each Si1 was electrically coupled to all of the CPG neurons and made monosynaptic excitatory synapses with both Si3s. Si1 also bilaterally potentiated the excitatory synapse from Si3 to Si2. ���Virtual neuromodulation��� of both Si3-to-Si2 synapses using dynamic clamp combined with depolarization of both Si3s mimicked the effects of Si1 stimulation on the swim motor pattern. Thus, in Dendronotus, Si1 is a command neuron that turns on, maintains, and accelerates the motor pattern through synaptic and neuromodulatory actions, thereby differing from its homolog in Melibe in its functional position in the motor hierarchy.

SIGNIFICANCE STATEMENT Cross-species comparisons of motor system organization can provide fundamental insights into their function and origin. Central pattern generators (CPGs) are lower in the functional hierarchy than the neurons that initiate and modulate their activity. This functional hierarchy is often reflected in neuroanatomical organization. This paper definitively shows that an identified cerebral ganglion neuron that is a member of a CPG underlying swimming in one nudibranch species serves as a command neuron for the same behavior in another species. We describe and test the synaptic and neuromodulatory mechanisms by which the command neuron initiates and accelerates rhythmic motor patterns. Thus, the functional position of neurons in a motor hierarchy can shift from one level to another over evolutionary time.
1 / 1
ChatGPT
ChatGPT
The article titled "Command or Obey? Homologous Neurons Differ in Hierarchical Position for the Generation of Homologous Behaviors" by Akira Sakurai and Paul S. Katz (2019) explores the hierarchical organization and functional roles of neurons within central pattern generators (CPGs) across different species.
CPGs are neural circuits responsible for generating rhythmic motor patterns. It is commonly believed that higher-order neurons provide commands to lower-level CPGs, which autonomously produce the rhythmic motor patterns. This hierarchical organization is often thought to be determined by the anatomical position of the neurons.
However, the authors present evidence challenging this conventional view. They investigate the swim interneuron 1 (Si1) in two nudibranch mollusk species: Melibe leonina and Dendronotus iris. In Melibe, Si1 is a member of the CPG underlying swimming and fires rhythmic bursts of action potentials as part of the swim motor pattern. In contrast, in Dendronotus, Si1 serves as a command neuron that modulates the CPG for a homologous swimming behavior.
In Dendronotus, Si1 fires irregularly throughout the swim motor pattern, and its activity is necessary and sufficient for the initiation and maintenance of the swim motor pattern. Si1 in Dendronotus is electrically coupled to all CPG neurons and makes monosynaptic excitatory synapses with Si3 neurons. It also potentiates the excitatory synapse from Si3 to Si2. Through these synaptic and neuromodulatory actions, Si1 in Dendronotus acts as a command neuron, turning on, maintaining, and accelerating the motor pattern.
This study challenges the notion that the functional hierarchy within motor systems is solely determined by anatomical position. Instead, it demonstrates that the functional position of neurons within a motor hierarchy can shift across evolutionary time. The findings highlight the importance of cross-species comparisons in understanding the organization and function of motor systems.
In summary, the article provides insights into the hierarchical organization and functional roles of homologous neurons in CPGs across different species. It demonstrates that the functional position of a neuron within a motor hierarchy can vary, challenging the assumption that anatomical position solely determines the hierarchy. The study emphasizes the value of comparative approaches in unraveling the complexity of motor systems.
1 / 1
Article
Open Access
Published: 23 June 2021
An optimality principle for locomotor central pattern generators
Hansol X. Ryu & Arthur D. Kuo 
Scientific Reports volume 11, Article number: 13140 (2021) Cite this article

6440 Accesses

4 Citations

10 Altmetric

Metricsdetails

Abstract
Two types of neural circuits contribute to legged locomotion: central pattern generators (CPGs) that produce rhythmic motor commands (even in the absence of feedback, termed ���fictive locomotion���), and reflex circuits driven by sensory feedback. Each circuit alone serves a clear purpose, and the two together are understood to cooperate during normal locomotion. The difficulty is in explaining their relative balance objectively within a control model, as there are infinite combinations that could produce the same nominal motor pattern. Here we propose that optimization in the presence of uncertainty can explain how the circuits should best be combined for locomotion. The key is to re-interpret the CPG in the context of state estimator-based control: an internal model of the limbs that predicts their state, using sensory feedback to optimally balance competing effects of environmental and sensory uncertainties. We demonstrate use of optimally predicted state to drive a simple model of bipedal, dynamic walking, which thus yields minimal energetic cost of transport and best stability. The internal model may be implemented with neural circuitry compatible with classic CPG models, except with neural parameters determined by optimal estimation principles. Fictive locomotion also emerges, but as a side effect of estimator dynamics rather than an explicit internal rhythm. Uncertainty could be key to shaping CPG behavior and governing optimal use of feedback.

Introduction
A combination of two types of neural circuitry appears responsible for the basic locomotory motor pattern. One type is the central pattern generator (CPG; Fig. 1A), which generates pre-programmed, rhythmically timed, motor commands1,2,3. The other is the reflex circuit, which produces motor patterns triggered by sensory feedback (Fig. 1C). Although they normally work together, each is also capable of independent action. The intrinsic CPG rhythm patterns can be sustained with no sensory feedback and only a tonic, descending input, as demonstrated by observations of fictive locomotion4,5. Reflex loops alone also appear capable of controlling locomotion1, particularly with a hierarchy of loops integrating multiple sensory modalities for complex behaviors such as stepping and standing control6,7. We refer to the independent extremes as pure feedforward control and pure feedback control. Of course, within the intact animal, both types of circuitry work together for normal locomotion (Fig. 1B)8. However, this cooperation also presents a dilemma, of how authority should optimally be shared between the two9.
1 / 1
ChatGPT
ChatGPT
The article titled "An optimality principle for locomotor central pattern generators" by Hansol X. Ryu and Arthur D. Kuo (2021) explores the balance between central pattern generators (CPGs) and reflex circuits in legged locomotion.
Legged locomotion involves two types of neural circuits: CPGs, which generate rhythmic motor commands, and reflex circuits, which are driven by sensory feedback. These circuits work together to coordinate normal locomotion, but their relative balance and cooperation within a control model present challenges.
The authors propose an optimality principle to explain how these circuits should be combined for locomotion in the presence of uncertainty. They suggest reinterpreting the CPG within the context of state estimator-based control, which involves an internal model of the limbs that predicts their state. Sensory feedback is then used to optimally balance the effects of environmental and sensory uncertainties.
The study demonstrates the use of optimally predicted state to drive a simple model of bipedal, dynamic walking. This approach results in minimal energetic cost of transport and improved stability. The internal model can be implemented using neural circuitry compatible with classic CPG models, with neural parameters determined by optimal estimation principles.
The findings suggest that uncertainty plays a crucial role in shaping CPG behavior and governing the optimal use of feedback. The combination of CPGs and reflex circuits in locomotion involves a balance that can be explained by an optimality principle.
In summary, the article presents an optimality principle for locomotor central pattern generators, proposing that the balance between CPGs and reflex circuits in legged locomotion can be understood through the optimal integration of sensory feedback and internal models. This approach offers insights into the coordination of neural circuits for efficient and stable locomotion.
1 / 1
Connections between the topics
1 / 1
ChatGPT
ChatGPT
The topics discussed in the various articles and papers you mentioned have several connections:
Central Pattern Generators (CPGs): CPGs are a common theme in multiple articles, including "Central pattern generator for swimming in Melibe" and "An optimality principle for locomotor central pattern generators." CPGs are neural circuits that generate rhythmic motor patterns independently of sensory input. The articles explore different aspects of CPGs, including their role in swimming behavior, hierarchical organization, and optimization principles for locomotion.
Neural Circuits and Motor Control: Several articles, such as "Behavioral evidence for nested central pattern generator control of Drosophila grooming" and "An optimality principle for locomotor central pattern generators," delve into the neural circuits involved in motor control. They investigate how different circuits, including CPGs and reflex circuits, contribute to generating motor patterns and coordinating locomotion.
Comparative Analysis: The articles "Evolution of central pattern generators and rhythmic behaviors" and "Command or Obey? Homologous Neurons Differ in Hierarchical Position for the Generation of Homologous Behaviors" explore the evolution and comparative analysis of central pattern generators and rhythmic behaviors across species. They highlight how similar behaviors can be controlled by different neural mechanisms and the shifting functional positions of neurons in motor hierarchies.
Optimization Principles: The article "An optimality principle for locomotor central pattern generators" focuses on optimization principles in locomotion. It discusses how the balance between CPGs and reflex circuits can be explained through optimal integration of sensory feedback and internal models. This connects to the broader concept of optimizing motor control and behavior in the presence of uncertainty.
Overall, these topics intertwine the study of neural circuits, motor control, rhythmic behaviors, evolution, and optimization principles. They provide insights into the underlying mechanisms and organization of behaviors, from simple invertebrates to complex legged locomotion, shedding light on the interplay between neural circuits and motor patterns.
1 / 1
A list of all of the topics we've discussed so far.
1 / 1
ChatGPT
ChatGPT
Certainly! Here's a list of the topics we've discussed so far:
Rhizomatic Food Chain
Prompts as Magic
Analysis of Poems
Polyfunctional Batteries
Fluidic Yogurt Computing
Interdisciplinary Robotics
Central Pattern Generators (CPGs)
Optimization and Control in Robotic Systems
Hierarchical Model of Performance in Swimming
Physiology of the Gastrointestinal Tract
Comparative Study of Central Pattern Generators
Causal Reasoning and Empirical Dynamic Modeling in Machine Learning
Control and Morphology Optimization of Passive Asymmetric Structures for Robotic Swimming
Neural Circuits and Motor Control
Evolution of Central Pattern Generators and Rhythmic Behaviors
Behavioral Evidence for Nested Central Pattern Generators
Command or Obey? Homologous Neurons Differ in Hierarchical Position for the Generation of Homologous Behaviors
Optimality Principles for Locomotor Central Pattern Generators
These topics cover a wide range of subjects, including food chains, AI, poetry, energy storage, fluidic computing, robotics, neural circuits, motor control, comparative analysis, optimization principles, and more.
1 / 1
An introduction to an essay on these topics, titled The hierarchical control of perception
1 / 1
ChatGPT
ChatGPT
Title: The Hierarchical Control of Perception: Exploring Interdisciplinary Frontiers
Introduction:
The intricate workings of the human mind and its interaction with the surrounding environment have long captivated scientists and thinkers across various disciplines. From the study of neural circuits to the analysis of cultural phenomena, understanding perception and its underlying mechanisms remains a complex and fascinating endeavor.
In this essay, we delve into the multifaceted concept of hierarchical control of perception, which bridges diverse fields such as biology, artificial intelligence, poetry, robotics, and more. By exploring the interconnectedness of these topics, we aim to shed light on the underlying principles that shape our understanding of perception and its implications in various domains.
Our journey begins with an exploration of the rhizomatic food chain, a paradigm that challenges the linear perspective of traditional food chains and highlights the importance of biodiversity in sustaining ecosystems. We then venture into the realm of artificial intelligence, where prompts act as magical portals to alternate realities, unlocking the potential of pre-trained models and agent simulations.
From there, we delve into the world of poetry and its intricate connection to scientific, philosophical, and technological themes. By analyzing selected poems, we uncover the richness of interdisciplinarity in poetic expression, where concepts like polyfunctional batteries and fluidic yogurt computing find their place amidst artistic narratives.
Moving towards the realm of robotics, we uncover the fascinating integration of biological processes and computation. The concept of central pattern generators (CPGs) comes to the fore, revealing the underlying neural circuits that generate repetitive motor behaviors. We explore how CPGs not only serve as fundamental components of locomotion but also provide insights into the hierarchical control of behavior in various organisms.
As our exploration unfolds, we encounter optimization principles and control strategies in robotic systems. Through the lens of asymmetric structures for robotic swimming, we witness the intricate relationship between morphology, control, and performance. These insights pave the way for a deeper understanding of the complex interplay between design, control, and efficiency in robotic systems.
The journey continues by examining the physiological aspects of the gastrointestinal tract and its role in generating rhythmic behaviors. We uncover the evolution of central pattern generators and their ability to produce repetitive motor patterns, shedding light on the connections between behavior, neural circuits, and evolutionary processes.
Our exploration then takes us into the realm of comparative analysis, where we uncover the similarities and differences in central pattern generators across different species. We discover how homologous neurons can differ in their hierarchical positions and functional roles, challenging conventional assumptions about neural circuitry and behavior.
Finally, we arrive at the concept of optimality principles in locomotor central pattern generators. By examining the balance between central pattern generators and reflex circuits in legged locomotion, we explore the interplay between pre-programmed rhythmic motor commands and sensory feedback. We unravel the role of uncertainty and internal models in driving optimal locomotion and the implications for the utilization of feedback in motor control.
In this essay, we embark on a multidimensional journey, connecting seemingly disparate topics to unravel the intricate web of hierarchical control in perception. By bridging disciplines and exploring the interconnections, we aim to shed light on the fundamental principles that govern perception, behavior, and the marvels of the human mind.
(Note: This is an introductory paragraph for an essay and can be expanded further in subsequent sections to provide a more comprehensive exploration of the discussed topics.)
1 / 1
A final summary
1 / 1
ChatGPT
ChatGPT
In this exploration of diverse topics, we have ventured into the realm of perception and its hierarchical control across various disciplines. Our journey encompassed the rhizomatic food chain, artificial intelligence prompts as portals, the interplay between poetry and science, and the integration of biological processes and computation in robotics.
We delved into the concept of central pattern generators, uncovering their role in generating repetitive motor behaviors and their evolutionary significance. Comparative studies shed light on the differences and similarities in central pattern generators across different species, challenging traditional assumptions about neural circuitry and behavior.
Optimization principles and control strategies in robotic systems provided insights into the relationship between morphology, control, and performance. We explored the physiological aspects of the gastrointestinal tract, revealing the interplay between behavior, neural circuits, and evolutionary processes.
Throughout our exploration, the interconnectedness of these topics became evident, as we discovered the intricate web of connections between seemingly disparate fields. The themes of hierarchy, optimization, interdisciplinarity, and evolutionary processes emerged as fundamental elements underlying the control of perception.
In summary, this journey has shed light on the complex and fascinating nature of hierarchical control in perception, transcending disciplinary boundaries and offering new perspectives on how we perceive and interact with the world around us. By embracing interdisciplinarity and exploring the intersections of diverse fields, we unlock new avenues of understanding and pave the way for further discoveries in the realm of perception and its control.
1 / 1
ChatGPT may produce inaccurate information about people, places, or facts.
ChatGPT May 24 Version
ChatGPT





