Evolution of State Space
Open sidebar
Evolution of State Space
Default (GPT-3.5)
Summarize: 1.3.2 Tracking a Ship
We consider the classical bearings-only tracking problem of tracking a ship being
observed from a constant (not moving) observation position. State space models
have been proposed for tracking various objects since the late 70s, see e.g. Aidala
(1979); for a recent discussion of this problem see Fearnhead (2002) and S��rkk��
(2013). Figure 1.5 depicts, with solid points, the position of the ship (in x ��� y
coordinates), at each time t. At each time t, and for each of the points mentioned
above, angular data of the ship���s position is obtained. The objective is, using this
data, to project future positions and thus to track the movement of the ship.
From Fig. 1.5 we have that tan(��t) = yt/xt, where xt is the x coordinate of the
ship at time t, yt is the y coordinate of the ship at time t and ��t is the respective
angle. From this we have that ��t = arctan(yt/xt) and it can be postulated that the
angular data we observe are inflated by noise, thus the observation model is
zt = arctan yt xt 	 +  t . (1.10)
In this model we only can observe ��t inflated by noise, i.e. we observe zt . Thus ��t
and xt, yt are assumed unobserved or hidden processes.
In order to provide a model for zt , we need to set a model for the dynamics or
unobserved coordinates xt and yt . First we postulate that the derivative of xt at t,
denoted by x��t , is close to the derivative at t ��� 1, i.e. x��t ��� ��xt���1. This is interpreted
as describing a model when the ship will not have abrupt moves in the x coordinate.
1 / 1
ChatGPT
The text discusses the classical problem of bearings-only tracking of a ship from a stationary observation position. The objective is to track the ship's movement using angular data obtained from its position at each time step. The angular data is affected by noise. The relationship between the ship's coordinates (x, y) and the observed angle (��) is given by �� = arctan(y/x), where (x, y) are the ship's coordinates at time t. The observed angular data is modeled as z = arctan(y/x) + noise. The ship's actual coordinates (x, y) are hidden and assumed to follow a certain dynamics model, where the change in x-coordinate (x��t) is related to the previous change (��xt���1), implying the ship's movement is smooth without abrupt changes.
1 / 1
Similarly we postulate that y��t ��� ��yt���1. Finally, we can see that the x position at time
t can be determined by xt ��� xt���1 + ��xt���1, which originates from
x��t���1 ��� ��xt ��� xt
t ,
where xt = xt ��� xt���1 and t = t ��� (t ��� 1) = 1. A similar formula applies for the
y-coordinate, that is yt ��� yt���1 + ��yt���1. It is worth noting that time t is considered
to be discrete (as data is obtained at discrete times) and in this sense the derivatives
(with respect to time) are merely described as ratios xt = xt ��� xt���1 and yt.
Putting the above together, we can define a state vector
��t = ������������ xt yt��xt��yt ������������ ,
which with the above evolutions of xt, yt, x��t, y��t implies
��t = ������������ xt yt��xt��yt ������������ = ������������
1010
0101
0010
0001
������������ ������������ xt���1 yt���1 ��xt���1 ��yt���1 ������������ + ������������ ��1t ��2t ��3t ��4t ������������
1 / 1
ChatGPT
Likewise, it's assumed that the change in the y-coordinate (y��t) is similar to the previous change (��yt���1). The x position at time t is approximated as xt ��� xt���1 + ��xt���1, where ��xt���1 is the change in x-coordinate between time steps. Similarly, the y-coordinate is approximated as yt ��� yt���1 + ��yt���1. These approximations are valid in the discrete time setting where the derivatives with respect to time are expressed as ratios.
By combining these approximations, a state vector ��t is defined as:
��t = [xt yt ��xt ��yt],
And the evolution of this state vector can be represented as a matrix equation:
��t = A ��� ��t���1 + ��t,
Where A is the transformation matrix:
A = [ 1 0 1 0
0 1 0 1
0 0 1 0
0 0 0 1 ],
And ��t is a noise vector:
��t = [��1t ��2t ��3t ��4t].
1 / 1
or
��t = F��t���1 + ��t . (1.11)
Finally, from the definition of ��t , the observation model (1.10) can be written as
zt = arctan [0, 1, 0, 0]��t [1, 0, 0, 0]��t 	 +  t . (1.12)
Model (1.12)���(1.11) is a non-linear state space model. The evolution model (1.11)
is linear, but the observation model (1.12) is non-linear. If Gaussian distributions are
placed on the innovations  t, ��t and on the initial state ��0, then the above model is a
conditionally Gaussian non-linear state space model. Following a similar approach
as in Berzuini and Gilks (2001), Fig. 1.6 shows a simulated smooth trajectory of the
ship in the x ��� y plane.
1 / 1
ChatGPT
The equation ��t = F��t���1 + ��t (1.11) is another representation of the state evolution in the tracking model. Moving forward, based on the definition of the state vector ��t, the observation model (1.10) can be expressed as:
zt = arctan([0, 1, 0, 0]��t) ��� [1, 0, 0, 0]��t + noise (1.12).
The combined equations (1.12) and (1.11) describe a non-linear state space model. While the evolution model (1.11) is linear, the observation model (1.12) is non-linear. By assuming Gaussian distributions for the innovations t, ��t, and the initial state ��0, this model can be seen as a conditionally Gaussian non-linear state space model. Similar to the approach in Berzuini and Gilks (2001), Figure 1.6 presents a simulated smooth trajectory of the ship in the x ��� y plane.
1 / 1
 is ��
1 / 1
ChatGPT
Certainly, it seems like there might be a formatting issue. If you meant to use "��" (epsilon) instead of "" in the context you provided, I'll correct that for you:
The equation ��t = F��t���1 + ��t (1.11) is another representation of the state evolution in the tracking model. Moving forward, based on the definition of the state vector ��t, the observation model (1.10) can be expressed as:
zt = arctan([0, 1, 0, 0]��t) ��� [1, 0, 0, 0]��t + ��t (1.12).
The combined equations (1.12) and (1.11) describe a non-linear state space model. While the evolution model (1.11) is linear, the observation model (1.12) is non-linear. By assuming Gaussian distributions for the innovations ��t, ��t, and the initial state ��0, this model can be seen as a conditionally Gaussian non-linear state space model. Similar to the approach in Berzuini and Gilks (2001), Figure 1.6 presents a simulated smooth trajectory of the ship in the x ��� y plane.
1 / 1
1.3.3 Stochastic Volatility
The subject of dynamic change of financial asset prices has been a core interest
of finance over a very long time. Such assets can be share prices and other stocks
trading in global stock markets, or indices such as the Standard and Poor 500 index
or international exchange rates, such as the exchange rate of the British pound
with the US dollar and so forth. Unfortunately, forecasting asset prices is nearly
impossible because prices are exposed to excessive variability, which severely
affects forecast performance.
Suppose that pt denotes the price at time t of an asset (or the value of an index
or exchange rate) and define the logarithmic returns, known also as log-returns, as
yt = log pt ���logpt���1. We note that if pt is similar to pt���1, then yt ��� 0, and yt will
be high when pt is large compared to pt���1 or yt will be low when pt will be low
compared to pt���1. The conditional variance of the returns yt , known as volatility,
is a measure of the variability of the share prices and hence a measure of the risk
associated in forecasting the returns. For example, the 2008 credit crisis resulted
in high volatility in most assets, which in turn reflected the increased uncertainty
associated with investment decisions. It is now well known that volatility estimation
plays a crucial role in constructing portfolios of assets in risk management.
Figure 1.7 shows log-returns of 1776 trading days of IBM share prices. We
observe that the returns fluctuate around zero, but at the start of the time series the
returns are particularly volatile. This simple picture illustrates that returns should
fluctuate around zero and that their variance or volatility is time-varying. Thus, a
plausible model is to assume that
yt = exp(ht /2)t, (1.13)
where  t is independent of  s (t  =
component, which follows an autoregressive process
s),  t ��� N(0, 1) and ht is some unobserved
ht ��� �� = ��(ht���1 ��� ��) + ��t, ��t ��� N(0, ��2��), (1.14)
where �� is the mean of ht , �� is an autoregressive coefficient and some variance
��2��. An initial Gaussian distribution is set for h0. The unobserved process ht is the
logarithm of the volatility Var(yt | ht) = exp(ht).
Model (1.13)���(1.14) basically postulates that given ht , yt follows a Gaussian
distribution with zero mean and variance, which is the volatility exp(ht), where ht
follows an autoregressive process with mean ��. This model is known as a stochastic
volatility model, as it provides a framework for the estimation of the volatility
exp(ht), via the stochastic process ht ; for more details on stochastic volatility the
reader is referred to Tsay (2002, ��10.7).
We can put the above model in state space form as follows. Define the bivariate
state vector ��t = [ht, 1] to be state at time t. Now Eq. (1.13) can be written as
yt | ��t ��� N[0, exp([1, 0]��t)], (1.15)
1.3 Examples of State Space Models 13
Returns of IBM share prices
Trading day
0 500 1000 1500
Fig. 1.7 Returns of IBM share prices
and Eq. (1.14) can be written as
��t = 
 ht 1  = 
 �� ��(1 ��� ��)
0 1  

ht���1 1  + 
 ��t 0  = F��t���1 + ��t (1.16)
Equations (1.15)���(1.16) define a Gaussian non-linear state space model, where the
observation model (1.15) is non-linear in ��t , while the transition model (1.16) is
linear.
1 / 1
ChatGPT
The section discusses the topic of stochastic volatility in financial asset prices. Financial assets like stocks and indices are subject to significant price fluctuations, making price forecasting challenging. The logarithmic returns, denoted as yt, are defined as the difference in the logarithm of prices at time t and t-1.
The volatility, which represents the conditional variance of these returns, indicates the variability and risk associated with forecasting asset returns. High volatility is seen during times of uncertainty, as exemplified by the 2008 credit crisis. Volatility estimation is crucial for risk management and asset portfolio construction.
A model is introduced where yt = exp(ht /2) * ��t, where ��t is independent of ��s (t ��� s), ��t follows a standard normal distribution (N(0,1)), and ht is an unobserved component following an autoregressive process:
ht ��� �� = ��(ht���1 ��� ��) + ��t, where ��t ��� N(0, ��^2��),
Here, �� is the mean of ht, �� is an autoregressive coefficient, and ��^2�� represents variance. This unobserved process ht captures the logarithm of volatility Var(yt | ht) = exp(ht).
The model is framed as a state space model, with the bivariate state vector ��t = [ht, 1]'. Equation (1.13) is represented as yt | ��t ��� N[0, exp([1, 0]��t)], and equation (1.14) as ��t = F��t���1 + ��t. These equations establish a Gaussian non-linear state space model, where the observation model (1.15) is non-linear in ��t, while the transition model (1.16) is linear. This model provides a framework for estimating volatility through the unobserved process ht, known as a stochastic volatility model. For more details, Tsay (2002, ��10.7) is referenced.
1 / 1
1.3.4 Hookean Spring Force Dynamics
Consider a simple mechanical system describing the motion of an object in one���dimension (translational mode). Suppose that the object has mass m and it moves Return -30 -20 -10 0 10 20 30
14 1 State Space Models
Fig. 1.8 Spring single-mass
system, including a spring
and damping
m u
horizontally on a plane and attached to a wall on a spring, sometimes referred to
as Hookean spring (see Fig. 1.8). If y(t) is the position of the object at time t (here
t is continuous), then according to Newton���s laws of motion y(t) is driven by the
differential equation
m d2y(t)
dt2 + k1
dy(t)
dt + k2y(t) = u(t), (1.17)
where dy(t)/dt = ��y(t) is the velocity of the object at t (the first derivative of the
position y(t)), d2y(t)/dt2 is the acceleration (the second derivative of y(t)), u(t) is
an applied force at t the constants k1 is the damping or viscous friction constant and
k2 is the spring constant. The linear restoring force is ���k2y(t) and the friction force
is ���k1 ��y(t). This model and its derivation are discussed in Anand (1984, pp. 18���19)
and illustrated in Fig. 1.8.
We can represent the above differential equation in state space form by defining
the state vector
x(t) = 
 y(t)
��y(t) 
and writing down the position y(t) as
y(t) = [1, 0] 
 y(t)
��y(t)  = H x(t), (1.18)
with transition equation driven by (1.17) ��x(t) = 
 ���
0 1
k2m ���km1  x(t) + 
 01m 
u(t). (1.19)
It is easy to observe that (1.18) and (1.19) give the differential equation (1.17).
1.4 A Short History of the Kalman Filter 15
Now suppose that an experiment is conducted whereby measurements of the
position y(t), for some points of time t are collected. In this case it is expected
that y(t) will be inflated by noise, due to
1. measurement error;
2. the differential equation (1.17) not perfectly describing the system.
As a result it is natural to keep the transition (1.19) and to adopt an observation
equation
y(t) = H x(t) + (t), (1.20)
where (t) is an error term measuring discrepancies between the observed y(t)
and the theoretical signal H x(t) driven by (1.17). The transition (1.19) is basically
a deterministic transition equation, which describes the dynamical system, while
uncertainty is passed onto y(t) via the innovation or error term (t). Model (1.19)���
(1.20) is a continuous-time state space model, which is considered in more detail in
Sect. 8.4.
Having discussed some detailed numerical examples we now give a historical
account of the development of the Kalman filter.
1.4 A Short History of the Kalman Filter
Least Squares: Gauss and Plackett
The state space model can be regarded as an extremely useful extension of
regression models. Therefore, it is natural to think that estimation of the state space
model shares some common ground with least squares regression (Bingham & Fry,
2010). Least squares regression was discovered independently by Adrien-Marie
Legendre (Legendre, 1805) and by Carl Friedrich Gauss (Gauss, 1809). Although
Gauss claimed he had discovered the method much earlier than 1809, it seems that
Legendre is credited with the discovery of least squares, while the probabilistic
treatment of the errors (today usually referred to as residuals) is due to Gauss. The
debate of the priority of least squares has been one of the most famous in the history
of mathematics and statistics and it is discussed in detail in Stigler (1986). Even
in Gauss���s first published account in 1809 we find his original idea of assigning
a probability distribution to the errors; he refined his theory substantially in a
series of papers, which appear collectively under the title ���Theoria combinationis
observationum erroribus minimis obnoxiae��� in 1821, (Gauss, 1821/23/26). In those
papers he presents a systematic and probabilistic treatment of least squares methods
with application to astronomy and geodesy. It is here that Gauss effectively assigns
the exponential exp(���x2) (normal distribution) to describe the evolution of the
errors x. Gauss goes on to assign a uniform prior to the parameters and to obtain
a normal posterior distribution of the parameters. In doing this he gives the first
16 1 State Space Models
account of Bayesian estimation in least squares. As B��uhler (1981, p. 140) observes
���Least squares were Gauss���s indispensable theoretical tool in experimental research;
increasingly, he came to see it as the most important witness to the connection
between mathematics and nature���. Based on this view, Gauss was ahead of his time
and had the capacity of a modern applied mathematician and statistician.
After Gauss several scientists contributed on the development of least squares,
such as Pierre-Simon Laplace (on least squares computation), George Udny Yule
(on relating least squares to correlation), Sir Francis Galton and Karl Pearson (on
regression and correlation); reviews of the developments of least squares can be
found in Stigler (1986) and in Aldrich (1998). Although the least squares discovery
was the first mathematical attempt to build an optimal model based on observed
data, regression remained moderately explored in the nineteenth century and in
the beginning of the twentieth century; Robin Lewis Plackett in 1950 rediscovered
least squares and established regression analysis as we know it now (Plackett, 1950,
1991). Plackett, ahead of his time, derived a recursive estimation approach, known
as recursive residuals, which enables the recursive calculation of the least squares
solution. Plackett���s path-breaking work had two important implications: on the
applied side large data sets could be handled with notable flexibility (by performing
a small number of recursive calculations), and on the theoretical side his work set a
framework for the derivation of more complex estimation procedures.
1 / 1
ChatGPT
The text discusses the historical development of the Kalman filter and its relationship with least squares regression.
Least Squares: Gauss and Plackett
The foundation of the state space model and its estimation can be traced back to the concept of least squares regression. Least squares regression was independently discovered by Adrien-Marie Legendre and Carl Friedrich Gauss. Legendre and Gauss laid the groundwork for the method, with Gauss introducing the probabilistic treatment of errors (residuals) and assigning a normal distribution to describe the evolution of errors.
Gauss's work on least squares was based on a probabilistic treatment, assigning a normal distribution to errors and using a uniform prior for parameters, which led to a normal posterior distribution of parameters. This approach demonstrated a form of Bayesian estimation within least squares. Gauss's insights were ahead of his time and demonstrated the connection between mathematics and nature in experimental research.
Following Gauss, several scientists contributed to the development of least squares, including Pierre-Simon Laplace, George Udny Yule, Sir Francis Galton, and Karl Pearson. Although the concept of least squares was established, it wasn't extensively explored in the nineteenth and early twentieth centuries.
Robin Lewis Plackett's work in 1950 was instrumental in reviving and advancing the field of regression analysis. Plackett introduced a recursive estimation approach known as "recursive residuals," enabling efficient calculations of least squares solutions for large datasets. This approach allowed handling large datasets with flexibility and set the stage for more complex estimation procedures.
In summary, the historical evolution of the Kalman filter and state space models can be traced back to the development of least squares regression, with key contributions from Gauss and Plackett, among others. Plackett's work in the mid-twentieth century played a significant role in advancing the application and theory of regression analysis.
1 / 1
The Filtering Problem: Kolmogorov and Wiener
The modern axiomatic treatment of probability theory is attributed to Russian math���ematician Andrey Nikolaevich Kolmogorov (1903���1987). Kolmogorov���s research
interests span a wide range of mathematical topics, including probability theory,
topology, intuitionistic logic, turbulence, classical mechanics and computational
complexity. Kolmogorov worked on stochastic processes and in 1941 he gave a
solution to the problem of optimal estimation of a discrete-time stationary stochastic
process (Kolmogorov, 1941).
At the same time independently of Kolmogorov, Norbert Wiener (1894���1964)
was working on the equivalent problem for continuous-time stationary stochastic
processes. In 1942 he completed his work on smoothing stationary time series,
known today as Wiener filter, but his work was only published post-war in 1949
(Wiener, 1949). In 1942 a classified report with Wiener���s method appeared with the
nickname ���the yellow peril��� because of the colour of the cover and the difficulty of
the subject. Wiener derived the solution of the least squares errors in a continuous���time stationary process as a function of the autocorrelation functions of the signal
and the noise. Together with Kolmogorov, Wiener is credited with introducing the
term ���filtering��� in time series or stochastic processes. Filtering is used for the
operation of removing noise from a process inflated by noise. Today, the term ���de���noising��� is also used, for such a filtering operation.
1.4 A Short History of the Kalman Filter 17
The Kalman Filter: Kalman
Rudolf Emil Kalman was born in 19 May 1930 in Budapest. Kalman received his
bachelor���s and master���s degrees from MIT in electrical engineering, in 1953 and
1954, respectively. In 1955 Kalman obtained a position at Columbia University as a
graduate student and lecturer. He then developed strong research interests in systems
theory and systems dynamics. He was interested in algebraic methods and he is
credited for finding the algebraic expression of observability and controllability of
a dynamical system, see e.g. Grewal and Andrews (2015).
Although the state space form of a time-varying system was known since 1956
(Halcombe Laning & Battin, 1956), it was not connected with the filtering problem
mentioned above; see also Zadeh and Desoer (1963). In 1958 Kalman had the idea to
use state variables and the linear state space model for the Wiener filtering problem.
Although the Wiener filter is applied to continuous-time processes, Kalman first
considered discrete-time processes. In order to be able to derive the equivalent to the
Wiener filter using the state space form in discrete time, Kalman equated expectation
with projection, after he read Lo��ve���s book on probability theory (Lo��ve, 1955).
The use of projections has been a central element in the derivation of the new
filter, known as the Kalman filter, which was published in a mechanical engineering
journal in 1960 (Kalman, 1960). Kalman���s new filter had two important elements:
(a) it used the state state space form (which is regarded today as a very advantageous
consideration as it can describe a large number of physical phenomena) and (b) it
did not require the assumption of stationarity (which is significant as most real-life
systems are not stationary). Just as Plackett���s recursive estimation was ahead of his
time, so was Kalman���s method, as it was able to deal with non-stationary processes.
Kalman considers the state space model (1.3) where the states ��t represent
some physical entities of the system (usually they represent an ideal theoretic
state of the system���sometimes driven by differential or difference equations���
while the observations yt represent noisy versions of the states, which are subject
to measurement error). Then Kalman provides an optimal filter, provided that the
design vector xt and the transition matrix Ft as well as the variance of  t and the
covariance matrix of ��t are known and specified by the user. Kalman suggested
that these model components may be known by the system or by past experiments
and he did not study how they may be specified or estimated from the data. Later
on when the Kalman filter was applied in economics (see the discussion below),
the states were considered as unobserved or hidden components, not necessarily
assigned to physical entities, which assist on understanding the generating process
of the observations. In this approach the problem of specifying or estimating the
components xt and Ft as well as the variance and covariance components mentioned
above, becomes more crucial as there is weak or no information provided by
the physical/socio-economic system (other than the observed data) to help the
identification of these components. This difference between Kalman���s state space
modelling framework and later applications of the Kalman filter often is not made
clear. In cases where the model components xt and Ft are unknown and subject to
estimation, the Kalman filter is not optimal in the sense that Kalman has proved,
18 1 State Space Models
because the additional estimation of such components introduce certain limitations
and biases. On one hand this has led in certain cases to misuse of the Kalman filter,
and in other cases it has motivated the development and extension of Kalman-type
algorithms, such as the expectation maximisation algorithm or sequential Monte
Carlo methods. Both of these approaches are discussed in detail in Chaps. 4 and 5
in this book.
Kalman���s method was not received without controversy. He was not able to
publish his seminal paper in an electrical or systems engineering journal, and people
who attended his talks found it hard to realise the potential of the Kalman filter.
However, Stanley F. Schmidt from the Ames Research Centre of NASA was able
to see its importance when Kalman explained it to him in a visit in 1960. Schmidt
was the first to implement the Kalman filter for the trajectory estimation and control
problems for the Apollo project. He then soon discovered what is now known as
the ���extended Kalman filter���, which is basically a linearisation of observation and
state functions to deal with non-linearities and is discussed in Chap. 5. In the 1960s
apart from the work of Schmidt and his colleagues, several people were involved in
application and developments related to the Kalman filter, most notably Kalman and
Bucy (1961), Raunch et al. (1965), Schweppe (1965) and Young (1968, 1969). Since
then the Kalman filter and its generalisations have been applied widely in science
and in real-life applications, e.g. in navigation systems, in digital filters, systems
engineering, time series forecasting and control.
Kalman is currently emeritus professor in three universities. Kalman received
the 2008 Charles Stark Draper Prize ���for the development and dissemination of the
optimal digital technique (known as the Kalman Filter) that is pervasively used to
control a vast array of consumer, health, commercial and defence products���.
1 / 1
ChatGPT
The Filtering Problem: Kolmogorov and Wiener
The development of the Kalman filter is rooted in the work of Andrey Nikolaevich Kolmogorov and Norbert Wiener, who independently worked on problems related to optimal estimation in stochastic processes.
Andrey Nikolaevich Kolmogorov, a Russian mathematician known for his contributions to various fields, including probability theory, tackled the problem of optimal estimation of discrete-time stationary stochastic processes in 1941. At the same time, Norbert Wiener, an American mathematician, was working on the equivalent problem for continuous-time stationary stochastic processes. Wiener completed his work on smoothing stationary time series, known as the Wiener filter, in 1942, but its publication was delayed due to wartime circumstances, and it was finally published in 1949.
Kolmogorov and Wiener independently introduced the concept of "filtering" in the context of time series or stochastic processes. Filtering involves removing noise from a signal that is affected by noise. They provided solutions to optimal estimation problems, with Wiener focusing on continuous-time processes and deriving solutions based on autocorrelation functions of the signal and noise.
The Kalman Filter: Kalman
Rudolf Emil Kalman, a Hungarian-American mathematician, developed what is now known as the Kalman filter. Kalman was born in 1930 and received his education in electrical engineering. He established his career in systems theory and dynamics.
Although the state space form of time-varying systems was known since 1956, it wasn't directly connected to the filtering problem that Kolmogorov and Wiener had worked on. In 1958, Kalman had the idea to use state variables and linear state space models for the Wiener filtering problem, starting with discrete-time processes.
Kalman's significant contribution was the development of the Kalman filter, which combined the state space representation with optimal estimation. His filter addressed both continuous and non-stationary processes, making it more versatile than previous methods. Kalman's work utilized projections as a central element, deriving an optimal filter that didn't require the assumption of stationarity.
Kalman's filter incorporated the state space model, where states represented physical entities of a system and observations were noisy versions of those states. He provided an optimal filter solution given certain known components of the model. Kalman acknowledged that these components might be known or specified by the system or past experiments.
The Kalman filter, despite initial challenges and misunderstandings, became a crucial tool for applications in various fields. Stanley F. Schmidt recognized its importance and implemented it for NASA's Apollo project. Kalman's work spurred further development and applications, leading to the "extended Kalman filter" and its widespread use in navigation, control, forecasting, and various other areas.
In 2008, Kalman was awarded the Charles Stark Draper Prize for his contribution to the Kalman filter, a technique widely used to control numerous consumer, health, commercial, and defense products.
1 / 1
Bayesian Forecasting: Jones, Harrison and Stevens
Although the state space model has been known to engineers since 1956 (Halcombe
Laning & Battin, 1956), statisticians and economists did not seem to exploit it.
John Fraser Muth (1930���2005), an eminent economist known as the father of the
rational expectations revolution in economics, introduced in 1960 the local level
model (see Sect. 1.2 above), and gave its relationship with the popular exponentially
weighted moving average forecasting procedure Muth (1960). In the 60s the closest
effort of statistical filtering of the state space model was the attempt of Richard
H. Jones in 1966 (Jones, 1966). Jones introduced Kalman filtering to the statistics
and econometrics scientific communities and he applied it in order to estimate the
parameters of Muth���s exponential smoothing model. He is also the first to provide a
derivation of the Kalman filter based on conditional Gaussian distribution theory as
opposed to projections adopted by Kalman. Jeff P. Harrison almost independently
was arriving at the same line of research, with some preliminary results in his
exponential smoothing 1967 paper (Harrison, 1967), which was generalised later
by Godolphin and Harrison (1975). However, these studies did not realise their
potential, perhaps, due to the technological and computational limitations of the
1960s.
1.5 Layout of the Book 19
Jeff Harrison met C. F. Stevens at Imperial Chemical Industries (ICI) where
they worked together on a series of papers. They first observed that many different
types of time series (especially non-stationary time series, exhibiting trend and
seasonal variation) could be described by state variables and indeed by state space
models. This important observation led to the adoption of the Kalman filter (as this
was developed for non-stationary processes) and had a significant advantage over
competitive time series methods, based upon the assumption of stationarity, e.g.
Box-Jenkins methods (Box et al., 2008). In 1971 Harrison and Stevens published
their first account of state space modelling for time series data (Harrison & Stevens,
1971). They adopted a derivation of the Kalman filter similar to that of Jones, but
made the important observation that the state variables could incorporate many
familiar time series models; this came under the title Bayesian forecasting, because
the Bayes theorem was used to provide the forecast distribution. This new approach
offered an array of new possibilities utilising Bayesian statistics, which at the time
was undergoing rapid development. Some of these possibilities were explored in
their seminal paper in 1976 (Harrison & Stevens, 1976) which was read before
the Royal Statistical Society. It is worth mentioning that in the early 70s several
economists used the Kalman filter for inventory forecasting and control (see the
introduction of Morrison and Pike (1977) for references). In particular, in 1977,
independently of Harrison and Stevens, Morrison and Pike (1977) rediscovered
the Kalman filter as a general estimation procedure of non-stationary time series
forecasting.
In the 80s many scientists realised the potential of state space modelling using
the Kalman filter. Among many authors, A.C. Harvey and his co-authors published
articles in 1980 and 1981 that demonstrate state space models and the Kalman filter
(Harvey et al., 1980; Harvey, 1981). In 1984 Harvey showed how the Kalman filter
can provide a unified estimation methodology for stationary and non-stationary time
series (Harvey, 1984); he was one of the first to point out the correspondence of state
space models and the Box-Jenkins autoregressive moving average models. Soon
after in 1989 Harvey published his book on structural models and the Kalman filter
(Harvey, 1989), which is known to have educated generations of statisticians, since
1989. The growth of state space models is reflected by textbooks and monographs,
such as Anderson and Moore (1979), West and Harrison (1997) [first published in
1989] and later Grewal and Andrews (2015) and Durbin and Koopman (2012).
1.5 Layout of the Book
The aim of the book is to introduce state space models and the Kalman filter and to
illustrate its wide application in science and commerce. The book aims to present
a concise yet self-contained treatment of state space models with applications in
many different disciplines. Throughout the book we adopt the Bayesian paradigm
for estimation and forecasting, as this is judged to be the modern statistical
approach which benefits from the high-performance computational power. However,
20 1 State Space Models
in places we discuss frequentist-based inference, such as least squares and maximum
likelihood estimation. The layout of the book is as follows.
Chapter 2 provides the mathematical background related to Kalman filter,
including matrix algebra, analysis, probability and statistics. In this chapter we
summarise the main results which are necessary for the reader to develop the
technical arguments that follow in Chap. 3.
Chapter 3 introduces the state space model and develops filtering and smoothing
estimation methodologies, including the Kalman filter, as well as forecasting. We
motivate the state space model by considering it as an extension of the linear
regression model. The chapter describes a package of the programming language
for statistical computing R, which is used throughout the book. Two proofs of the
Kalman filter are given, each of which provides different insight into the structure
of the filter. Throughout the chapter examples illustrate the applicability of the
estimation algorithms.
Chapter 4 discusses further topics on the implementation of linear state space
models. We start by discussing an array of useful state space forms of real-life
situations, such as time series comprising trend, seasonal, autoregressive and timevarying regression components. These are discussed with particular data in mind
and the Kalman filter is applied to provide forecasting solutions. Then maximum
likelihood and related concepts are described to provide a way of estimating or
specifying parameters of the models. Error analysis and diagnostic checks as well
as prior specification are considered in some detail. The chapter concludes by
discussing model monitoring and intervention analysis.
Chapters 5 and 6 discuss models that go beyond the Kalman filter. The aim of
these chapters is to give an account of modern statistical methodologies for state
space models and therefore to illustrate recent trends of research in this area. These
include covariance estimation of multivariate state space models, non-linear and
non-Gaussian state space models, and sequential Monte Carlo estimation methods.
Chapters 7 and 8 discuss the application of state space methods in finance and
in dynamical systems. While finance and economics paid particular attention to
the Kalman filter, as noted earlier, systems engineering is where the Kalman filter
was discovered. As a result these major areas of application of the Kalman filter
and related methods are considered in Chaps. 7 and 8. The selection of illustrative
examples presented in those two chapters aim to showcase the applicability and
contribution of the state space models to these areas.
1 / 1
ChatGPT
Bayesian Forecasting: Jones, Harrison, and Stevens
The state space modeling approach gained traction in the 1960s and beyond, particularly in the fields of statistics and economics. John Fraser Muth introduced the local level model and its connection to exponentially weighted moving average forecasting in 1960. Richard H. Jones attempted statistical filtering of the state space model in 1966, introducing the Kalman filter to the statistics and econometrics communities. Jones provided a derivation of the Kalman filter based on conditional Gaussian distribution theory, distinguishing it from Kalman's projection-based approach.
Jeff P. Harrison independently pursued similar research, with preliminary results in his 1967 paper on exponential smoothing. Harrison's work continued in collaboration with C. F. Stevens at Imperial Chemical Industries (ICI), where they discovered that various time series, even non-stationary ones with trend and seasonal variation, could be described using state space models. They realized that the Kalman filter's advantage in handling non-stationary processes made it superior to other methods like Box-Jenkins, which assumed stationarity. In 1971, Harrison and Stevens published their first account of state space modeling for time series data, adopting the Kalman filter and incorporating well-known time series models into the state variables. This approach was termed "Bayesian forecasting" due to the use of Bayes' theorem to provide forecast distributions.
Harrison and Stevens explored the potential of Bayesian statistics in their 1976 paper, opening up new avenues for state space modeling and forecasting. Independently, economists also began using the Kalman filter for inventory forecasting and control during the early 1970s. The late 1970s saw a resurgence of interest in the Kalman filter, with A.C. Harvey and others contributing articles that demonstrated the utility of state space models and the Kalman filter.
The 1980s witnessed the further growth of state space modeling and Kalman filter applications. Harvey's works demonstrated the unification of stationary and non-stationary time series using the Kalman filter. His influential book on structural models and the Kalman filter in 1989 educated generations of statisticians. The field continued to expand with the publication of various textbooks and monographs on state space models and the Kalman filter.
Layout of the Book
The book's objective is to provide a concise, self-contained introduction to state space models and the Kalman filter, highlighting their broad applications across different disciplines. The Bayesian approach to estimation and forecasting is emphasized due to its compatibility with modern computational resources. The book also touches on frequentist-based inference, such as least squares and maximum likelihood estimation.
The book's chapters are organized as follows:
Mathematical Background:
Covers matrix algebra, analysis, probability, and statistics relevant for understanding the Kalman filter.
Introduction to State Space Models:
Presents the state space model, filtering, smoothing, and forecasting methodologies using the Kalman filter.
Implementation of Linear State Space Models:
Discusses various state space forms for real-world situations, estimation methodologies, error analysis, diagnostics, and more.
Beyond the Kalman Filter (Parts 1 and 2):
Explores advanced topics, including multivariate models, non-linear and non-Gaussian models, and sequential Monte Carlo methods.
Applications in Finance:
Discusses the application of state space methods in finance and economics.
Applications in Dynamical Systems:
Examines state space modeling's applications in systems engineering and dynamical systems.
The book aims to provide a comprehensive understanding of state space models and their practical implementation, catering to various applications in diverse fields.
1 / 1
The passage describes the historical evolution and development of state space modeling in statistics and econometrics. Specifically, it focuses on how the model was applied to time series analysis. Here's a summary:

Origin: The state space model has its origins in engineering, introduced by Laning & Battin in 1956.

Muth's Contribution: John Fraser Muth, an economist, introduced the local level model in 1960. This model was related to the exponentially weighted moving average forecasting procedure.

Introduction of Kalman Filtering: Richard H. Jones, in 1966, introduced Kalman filtering to statistics and econometrics, using it to estimate the parameters of Muth���s exponential smoothing model. Jones' approach to the Kalman filter was based on the conditional Gaussian distribution theory.

Harrison and Stevens: Almost parallelly, Jeff P. Harrison started exploring similar research. Later on, Harrison met C. F. Stevens at ICI, and they collaborated on various papers. Their significant contribution was recognizing that many time series could be described by state space models. They also observed that the state variables could incorporate various time series models and introduced Bayesian forecasting using the Bayes theorem.

Growing Acceptance in the 70s and 80s: Harrison and Stevens' work was further acknowledged in the early 70s with applications like inventory forecasting. The 80s saw a greater recognition of the potential of state space modeling. A.C. Harvey and his co-authors made several noteworthy contributions during this time, demonstrating the versatility and capabilities of the Kalman filter in state space modeling.

Textbooks & Monographs: The growth and acceptance of state space models are evidenced by the publication of numerous textbooks and monographs, which have become staple resources for students and professionals in the field. Some of these notable publications include works by Anderson and Moore (1979), West and Harrison (1997), Grewal and Andrews (2015), and Durbin and Koopman (2012).

Overall, the passage underlines the progressive journey of the state space model from its introduction in engineering to its widespread use and acknowledgment in econometrics and statistics. It also highlights the contributions of various researchers and scholars who played a pivotal role in its evolution and adoption.
1 / 1
ChatGPT
Your summary is accurate and provides a clear overview of the historical evolution and development of state space modeling in statistics and econometrics. You've captured the key contributions, individuals, and milestones that shaped the adoption and application of the state space model, especially in the context of time series analysis. Well done!
1 / 1
Chapter 2
Matrix Algebra, Probability
and Statistics
This chapter offers notation and the necessary background for the development of
state space methods that follows. The background lies on three basic areas, matrix
algebra, probability and statistics. We start with matrix algebra in Sect. 2.1; we
develop in some detail vector and matrix differentiation, while other elements of
more standard matrix algebra are just mentioned. Section 2.3 discusses probability
and distribution theory relevant to the needs of the book. This section aims to
establish notation and to remind to the reader the notion of distribution theory
relevant to filtering and state space modelling. Several examples of discrete and
continuous distributions, which will be used in later chapters, are presented. Sec���tion 2.4 discusses the ideas behind statistics necessary for the chapters that follow.
The maximum likelihood principle is introduced and the expectation maximisation
(EM) algorithm is discussed in detail. The chapter concludes with an introduction
to Bayesian inference.
2.1 Vectors, Matrices and Basic Operations
We assume that the reader is familiar with calculus of one and several variables,
with matrix algebra and with basic probability and statistics. For detailed coverage
of calculous of one and several variables the reader is referred to Spivak (1995)
and to Lang (1987), respectively. Matrix algebra and matrix analysis are discussed
in Magnus and Neudecker (1988), Harville (1997) and Horn and Johnson (2013).
Below we establish some basic notation and highlight a few important concepts. We
assume that the reader is familiar with matrix addition and multiplication, inverse of
a matrix, symmetric and positive definite matrices, trace of a square matrix and the
determinant of a square matrix.
Scalars and vectors are denoted by small letters, e.g. a, b, c, while matrices are
denoted by boldface capital letters, e.g. A,B, C. Sets are denoted by capital letters,
�� The Author(s), under exclusive license to Springer Nature Switzerland AG 2021
K. Triantafyllopoulos, Bayesian Inference of State Space Models, Springer Texts
in Statistics, https://doi.org/10.1007/978-3-030-76124-0_2
21
22 2 Matrix Algebra, Probability and Statistics
e.g. A = {a, b, c} and R denotes the set of real numbers. An m �� n matrix A is
usually denoted by A = (aij )i=1,...,m;j=1,...,n, where aij is the ij -th element of A, m is the number of rows and n is the number of columns; sometimes we simply
write A = (aij ), if the range of i, j is implied. If m = n A is usually referred
to as a square matrix. Unless otherwise stated, we will assume that A is a matrix
with elements from the real field. A matrix with zero elements (aij = 0, for all
i, j ) is referred to as the zero-matrix and is denoted by 0; a square matrix with
aii = 1 and aij = 0, for i  = j and m = n is known as the identity matrix and
is denoted by I. A diagonal matrix is a matrix with diagonal elements a11,...,ann
and aij = 0, for i  = j ; usually we write A = diag(a11,...,ann) or A = diag(a),
where a is a column vector a = [a11,...,ann] and sometimes we may write
diag(A) to denote a diagonal matrix with diagonal elements the diagonal elements
of a matrix A. A symmetric matrix A = (aij )i,j=1,2,...,n is a n �� n square matrix
with aij = aj i, for all i, j = 1, 2,...,n. A non-negative definite matrix is a matrix
A with x Ax ��� 0, for any vector x. If x Ax > 0, for all non-zero vectors x, then A
is called positive definite. In statistics symmetric and positive definite matrices play
a significant role as they represent the variance and covariance structure of random
vectors (see Sect. 2.3).
The inverse of an n �� n non-singular matrix A is denoted by A���1, the trace of
an n �� n matrix A is denoted by trace(A), the determinant of A is demoted by |A|.
In the sections below we discuss in some detail vector and matrix differentiation,
optimisation and limit of matrices. Below we list some of the properties of matrix
operations.
1. Integer powers of a matrix. Consider a square matrix A. The power of order k
of A, denoted as Ak, is defined to be the product
Ak = AA ������ A  
 k���times
= Ak���1A = AAk���1,
for some integer k > 0, while for k = 0 we define A0 = I.
2. Determinant of a matrix. Associated with any n �� n matrix A there is a scaler,
known as the determinant of A and denoted by |A|. For a full definition of the
determinant the reader is referred to Harville (1997, page 177). Here we just
mention two of the properties of determinants.
a. For any constant c, it is |cA| = cn|A|;
b. For two n��n matrices A and B, it is |AB|=|A||B| (in words: the determinant
of the product of two matrices is equal to the product of the determinant of
the two matrices).
3. Trace of a matrix. Associated with any square matrix A = (aij ) is a scalar,
called the trace of A, which is denoted by trace(A) and defined as
trace(A) = a11 + a22 +������+ ann = ni=1 aii.
2.1 Vectors, Matrices and Basic Operations 23
We give two basic properties of the trace of a matrix
a. For any n �� n matrices A = (aij ) and B = (bij ), we have
trace(A + B) = trace(A) + trace(B).
b. For two n �� n matrices A and B we have
trace(AB) = trace(BA).
4. The vec, vech and the Kronecker product.
For an m��n matrix A the vec operator rearranges the elements of A into a vector
by stacking the columns of A one after the other. For example, if A = (aij ) = [a1, a2,...,an], where ai = [a1i, a2i,...,ami] represents the i-th column of
A, for i = 1, 2,,...,n, then
vec(A) = ��������������� a1 a2...an ���������������
which is an mm �� 1 vector.
The Kronecker product of an m��n matrix A and a p��q matrix B is the mp��nq
matrix defined by
A ��� B = (aijB).
For example, for n = m = p = q = 2 we have
A ��� B = 
 a11B a12B a21B a22B = ������������ a11b11 a11b12 a12b11 a12b12
a11b21 a11b22 a12b21 a12b22
a21b11 a21b12 a22b11 a22b12
a21b21 a21b22 a22b21 a22b22
������������ .
A few properties of the vec and Kronecker operations are listed below
a. If A, B and C are matrices so that the product ABC is defined, then
vec(ABC) = (C ��� A)vec(B).
When matrix A is symmetric, then since aij = aj i not all elements of the vector
vec(A) are distinct. In such a case we can obtain a vectored rearrangement of
24 2 Matrix Algebra, Probability and Statistics
A by stacking columns of A with distinct elements only and this is denoted as
vetch(A). For example, for m = n = 2 we have
vech 
 a11 a12
a12 a22 
= ������ a11
a12
a22
������ .
2.2 Vector and Matrix Differentiation
2.2.1 Background and Notation
In this section we describe the notion of vector and matrix partial derivatives,
necessary for the development of this book. We will restrict our attention to two
cases (1) partial derivatives of scalar function of a vector x and (2) partial derivatives
of scalar function of a symmetric matrix X. We denote with Rn the cartesian product
Rn = R �� R ���������� R, so that Rn contains vectors x = [x1, x2,...,xn] , with
xi ��� R, for i = 1, 2,...,n. For a more detailed treatment of matrix calculus the
reader is referred to Magnus and Neudecker (1988) and Harville (1997).
1. It is assumed that a function f (��) of the vector x is continuously differentiable
at an interior vector point c in the domain D (a subset of Rn); for a precise
definition of differentiability the reader is referred to Magnus and Neudecker
(1988) and Harville (1997). We shall write f (x) to denote the value of f (��) at
point x and usually it will be assumed that D = Rn. The partial derivative of
f (x) (defined at an interior point of a subset of D) with respect to xi, denoted by
���f (x)/���xi, is the scalar derivative of f (x) = f (x1,...,xi,...,xn) when this is
viewed as a function of xi alone and all xj  = xi are viewed as constants, where
x = [x1,...,xn] . The partial derivative of f (x) with respect to x, denoted
by ���f (x)/���x is defined to be the vector with elements the respective partial
derivatives ���f (x)/���xi, i.e.
���f (x)
���x = ���������������
���f (x)/���x1
���f (x)/ ���x2 ...
���f (x)/���xn ��������������� .
Some special derivatives used in this book are covered in Sect. 2.2.2.
1 / 1
ChatGPT
Your provided text discusses the content of Chapter 2 of the book, which is focused on providing the necessary background in matrix algebra, probability, and statistics for the development of state space methods. The chapter is divided into different sections to cover these topics. The main points covered in this chapter are as follows:
Section 2.1: Vectors, Matrices, and Basic Operations
Provides notation and terminology used in matrix algebra.
Discusses concepts like square matrices, identity matrix, diagonal matrix, symmetric matrix, non-negative definite matrix, and positive definite matrix.
Introduces concepts like matrix inverse, trace of a matrix, and determinant of a matrix.
Introduces the vec operator and the Kronecker product.
Lists properties of matrix operations like integer powers of a matrix, determinant properties, and trace properties.
Section 2.2: Vector and Matrix Differentiation
Covers background and notation for vector and matrix partial derivatives.
Focuses on partial derivatives of scalar functions of vectors and symmetric matrices.
Assumes functions to be continuously differentiable.
Defines partial derivatives with respect to individual elements of vectors or matrices.
Provides special derivatives used in the book.
The chapter aims to equip the reader with the necessary mathematical tools to understand the subsequent development of state space models and the application of the Kalman filter. It lays the foundation for handling matrices, vectors, and derivatives, which are fundamental in the state space framework.
1 / 1
In the same way we can define second and higher order partial derivatives of
f (x). The second partial derivative of f (x), denoted as ���2f (x)/���x���x is the
transpose of the first partial derivative of ���f (x)/���x, i.e.
���2f (x)
���x���x = ��������������� ���2f (x)/���x21 ���2f (x)/���x1���x2 ������ ���2f (x)/���x1���xn ���2f (x)/���x2���x1 ���2f (x)/���x22 ������ ���2f (x)/���x2���xn ... ... ... ... ���2f (x)/���xn���x1 ���2f (x)/���xn���x2 ������ ���2f (x)/���x2n ��������������� ,
assuming that all ���2f (x)/���xi���xj are defined, for i, j = 1,...,n.
2. For the purposes of partial derivatives of a function of matrices, we consider f (��)
to be a continuously differentiable function on an n �� n matrix X and we write
f (X) for the value of f (��) at X; we note that the value of f (X) is a real number.
The domain of such a function is a subset D of all matrices with real elements,
usually denoted by Rn��n. This is basically a special case of (1) as X can be
rearranged as a long vector (by stacking all column vectors of X). However, for
the purposes of matrix calculation and convenience in the presentation we retain
the matrix notation. Since X has n2 elements f (X) can be viewed as a function
of n2 variables. If, however, some of the elements of X are not distinct, e.g. when
X is a symmetric matrix, then we need to consider only the distinct elements in
the differentiation of f (X).
Writing X = (xij ) for an unrestricted matrix of variables, we define the partial
derivative matrix of f (X) as the matrix with ij -th element the partial derivative
of f (X) with respect to xij , defined at an interior point of D, i.e.
���f (X) ���X = ���������������
���f (X)/���x11 ���f (X)/���x12 ������ ���f (X)/���x1n
���f (X)/���x21 ���f (X)/���x22 ������ ���f (X)/���x2n ... ... ... ...
���f (X)/���xn1 ���f (X)/���xn2 ������ ���f (X)/���xnn
��������������� .
If X is symmetric, this logic does not work, since there are only n(n + 1)/2
distinct elements in X (out of the possible n2). In such a case X is rearranged
into a [n(n + 1)/2] �� 1 column vector x = vech(X), as discussed in the previous
section. Now the partial derivative of f (X) can be formed as the vector partial
derivative with respect to x, which then is rearranged as a symmetric matrix to
form ���f (X)/���X.
Dealing with case (2) we will also need to define a matrix-valued function F(��) =
(fkl)k=1,2,...,q;l=1,2,...,s defined on a matrix of variables X = (xij ) (unrestricted
or restricted). In such a case we write F(X) for the value of F(��) on matrix X;
the domain D of F(��) is all n �� n real-valued matrices (if X is unrestricted) or
D can be defined as above for symmetric matrices. Each constituent function
fkl(X) of the matrix F(X) is a scalar-valued function on the matrix X and it is
assumed to be continuously differentiable at an interior point of D. Then the
26 2 Matrix Algebra, Probability and Statistics
partial derivative of F(X) with respect to xij is defined to be the matrix
���F(X)
���xij
= ���������������
���f11(X)/���xij ���f12(X)/���xij ������ ���f1s(X)/���xij
���f21(X)/���xij ���f22(X)/���xij ������ ���f2s(X)/���xij
... ... ... ...
���fq1(X)/���xij ���fq2(X)/���xij ������ ���fqs(X)/���xij
��������������� .
2.2.2 Differentiation of Linear and Quadratic Forms
We consider here the partial derivatives of a linear form
f (x) = a x = ni=1 aixi
and a quadratic form
g(x) = x Ax = ni=1 nk=1 aikxixk,
where a = [a1, a2,...,an] is a vector of constants, A = (aij ) is an n �� n matrix
of constants and x = [x1, x2,...,xn] .
The partial derivative of f (x) with respect to xj is
���f (x)
���xj = ���
���xj ni=1 aixi = ni=1 ai
���xi
���xj = aj , (2.1)
since
���xi
���xj =  1 if i = j
0 if i  = j .
Recasting (2.1) in matrix form, we obtain
���(a x)
���x = a. (2.2)
2.2 Vector and Matrix Differentiation 27
Moving on now to the quadratic form g(x) we have
���g(x)
���xj = ���
���xj ni=1 nk=1 aikxixk = ���
���xj  ni=1 ai1xix1 +������+
ni=1 ai,j���1xixj���1 + ni=1 ai,j+1xixj+1
+������+
ni=1 ainxixn + i=j aij xixj + ajjxj2������ = aj1x1 + ...aj,j���1xj���1 + aj,j+1xj+1 +������+ ajnxn + i=j aij xixj + 2ajjxj = nk=1 ajkxk + ni=1 aij xi.
Recasting this in matrix form we obtain
���(x Ax)
���x = (A + A )x, (2.3)
since the j -th element of the vector Ax is  nk=1 ajkxk and the j -th element of the
vector A x is  ni=1 aij xi.
Next we compute the second partial derivative matrix of the quadratic form g(x).
As mentioned in the previous section, this will be the n��n matrix with ij -th element
���2g(x)/���xlxj , for l, j = 1,...,n.
We have
���2(x Ax)
���xl���xj = ���
���xj 

���(x Ax)
���xl  = ���
���xj  ni=1 ailxi + nk=1 alkxk = ni=1 ail
���xi
���xj + nk=1 alk
���xk
���xj = ajl + alj
and recasting this in matrix form we obtain
���2(x Ax)
���x���x = A + A . (2.4)
28 2 Matrix Algebra, Probability and Statistics
If A is a symmetric matrix, then the partial derivatives (2.3) and (2.4) simplify to
���(x Ax)
���x = 2Ax and
���2(x Ax)
���x���x = 2A. (2.5)
This equation is applied in Sect. 3.1.1 in order to derive the least squares solution in
regression.
1 / 1
ChatGPT
In this section of the text, the concept of second and higher-order partial derivatives of functions is discussed. It outlines the definitions and notations for these derivatives in the context of scalar functions of vectors and matrices. Here's a summary:
Second and Higher-Order Partial Derivatives:
Second Partial Derivative of Scalar Function of Vectors:
The second partial derivative of a scalar function f(x) with respect to xi and xj is denoted as �����f(x)/���xi���xj.
It is the transpose of the first partial derivative matrix of ���f(x)/���x.
In matrix form, it is presented as a matrix with elements �����f(x)/���xi���xj.
Partial Derivatives of Scalar Function of Symmetric Matrices:
For scalar functions f(x) that depend on symmetric matrices X, the partial derivative matrix ���f(X)/���X is introduced.
When X is symmetric, only distinct elements are considered for differentiation due to the symmetry.
The matrix-valued function F(X) is defined on matrix X, where each constituent function fkl(X) is scalar-valued and differentiable.
The partial derivative of F(X) with respect to xij is a matrix with elements ���fkl(X)/���xij.
Differentiation of Linear and Quadratic Forms:
Linear Form f(x) = a^T x:
The partial derivative of the linear form f(x) with respect to xj is aj (a constant vector element).
The matrix form of the partial derivative is ���(a^T x)/���x = a.
Quadratic Form g(x) = x^T A x:
The partial derivative of the quadratic form g(x) with respect to xj is (A + A^T) x (where A^T is the transpose of matrix A).
The matrix form of the partial derivative is ���(x^T A x)/���x = (A + A^T) x.
If A is symmetric, the partial derivatives simplify to ���(x^T A x)/���x = 2Ax and �����(x^T A x)/���x���x^T = 2A.
This section lays the groundwork for understanding how to differentiate functions involving vectors and matrices, particularly in the context of state space modeling and the Kalman filter. It provides essential mathematical tools for working with the derivatives of linear and quadratic forms, which are frequently encountered in the state space framework.
1 / 1
Partial Derivatives of Scalar Functions:

The first section defines the first and second order partial derivatives of a scalar function 
���
(
���
)
f(x), where 
���
x is a vector.
The first partial derivative of 
���
(
���
)
f(x) is a vector, while the second partial derivative, also known as the Hessian, is a matrix.
The matrix contains all possible second order partial derivatives.
Partial Derivatives of a Matrix Function:

The second section discusses how to compute the derivative of a function 
���
f that takes in a matrix 
���
X and outputs a scalar. The output, in this case, is a matrix of partial derivatives.
If the matrix 
���
X is symmetric, then the matrix of partial derivatives needs to consider only distinct elements.
Matrix-Valued Functions:

For a function 
���
F that takes in a matrix 
���
X and outputs another matrix, the partial derivative of each scalar element of the output matrix is computed. The result is a three-dimensional array of partial derivatives, which can be represented as a matrix for each unique pair of row and column indices of 
���
X.
Differentiation of Linear and Quadratic Forms:

The derivative of a linear form 
���
(
���
)
=
���
���
���
f(x)=a 
T
 x is the vector 
���
a.
The derivative of a quadratic form 
���
(
���
)
=
���
���
���
���
g(x)=x 
T
 Ax is a matrix expression, given by 
(
���
+
���
���
)
���
(A+A 
T
 )x. The second derivative, or the Hessian, of the quadratic form is 
���
+
���
���
A+A 
T
 . If 
���
A is symmetric, these simplify to 
2
���
���
2Ax and 
2
���
2A.
Applications:

The provided equations, especially the ones for quadratic forms, are fundamental in optimization. For instance, the passage mentions that these equations will be applied in the context of least squares regression, a fundamental technique in statistics.
1 / 1
ChatGPT
You've provided an excellent summary of the key concepts discussed in the section about partial derivatives of scalar functions, matrix functions, and the differentiation of linear and quadratic forms. This summary effectively captures the main points and applications of these mathematical concepts in various fields like optimization and statistics. Understanding these concepts is crucial for tackling problems involving functions of vectors and matrices, particularly in the context of state space modeling, optimization, and other related areas.
1 / 1
A list of everything we've talked about so far.
1 / 1
ChatGPT
Certainly! Here's a list of the main topics we've discussed during our conversation:
Bayesian Forecasting and State Space Models:
Evolution and development of state space modeling in statistics and econometrics.
Contributions of various researchers, including Muth, Jones, Harrison, and Stevens.
Growing acceptance and recognition of state space models in the 70s and 80s.
Textbooks and monographs that have contributed to the understanding of state space models.
Chapter 2: Matrix Algebra, Probability and Statistics:
Introduction to mathematical background for state space methods.
Notation and concepts related to matrix algebra, probability, and statistics.
Vector and matrix differentiation, including higher-order derivatives.
Derivatives of scalar functions, matrix functions, and matrix-valued functions.
Differentiation of linear and quadratic forms.
Applications of these concepts in optimization and statistics.
Feel free to ask if you have any further questions or if you'd like to discuss any specific aspect in more detail!
1 / 1
2.2.4 Optimisation, Integration and Limits
Optimisation Many statistical problems involve finding the maximum or min���imum of a function f (x) of a vector x = [x1, x2,...,xn] ; for example, in
Sect. 3.1.1 we are interested in maximising the likelihood function in the context
of ordinary regression. Assuming that f (x) is continuously differentiable in the
domain D
equation
��� Rn, then the possible maximum/minimum is identified by solving the
���f (x)
���x = 0.
32 2 Matrix Algebra, Probability and Statistics
The solution x = x��� of this equation gives what is known as stationary point. If the
second partial derivative evaluated at this point is a positive definite matrix (negative
definite matrix), then x��� is the required minimum (maximum).
For example, consider the function
f (x) = x Ax + 2a x + c,
where A is an n �� n positive definite matrix of knowns, a and c are n-dimensional
vectors of knowns and x is an n-dimensional vector of variables.
From derivatives (2.5) and (2.2) we have
���f (x)
���x = 2Ax + 2a
and equating this to zero we obtain x��� = ���A���1a. Here we note that since A is
positive definite it is also non-singular, hence A���1 exists.
From (2.5) the second partial derivative matrix is
���2f (x)
���x���x = 2A,
which is a positive definite matrix, hence x��� is a minimum. The minimum value of
f (x) is
f (x���) = (���Aa) A(���A���1a) + 2a (���A���1a) + c = ���a A���1a + c.
Integration In this section we give a brief discussion on integration of scalar
functions of argument a vector or a matrix. Suppose that f (x) is a scalar function of
argument a vector x = [x1,...,xn] . We assume that f (x) is continuous in each
interior point of D (a subset of Rn). Then the multiple integral of f (x) over D is
denoted by
"
D
f (x) dx =
" " ������ " D
f (x1, x2,...,xn)dx1 dx2 ������ dxn.
It is outside the scope of this book to discuss a technical development of multiple
integrals. For a formal definition and calculus of several variables, the reader is
referred to Lang (1987); an excellent treatment of calculus of one variable is given
in Spivak (1995). Similarly, multiple integrals of a scalar function f (X) of a matrix
X may be considered and the notation
"
D
f (X) dX
2.2 Vector and Matrix Differentiation 33
will be adopted, where D is the domain of f (X). Note that when X is a symmetric
matrix, the integral needs to be amended to include only the variables that are
distinct. Following standard notation we still keep the above notation, but it is
understood that integration is carried over the n(n + 1)/2 distinct elements of X.
Limit of a Sequence of Matrices A sequence of square matrices is a collection of
matrices A1, A2,..., indexed by t = 1, 2,.... We write At for the t-th term of the
sequence. In order to form the limit of At as t ��� ���, we need first to introduce the
notion of norm or distance for matrices. The norm of a matrix (usually referred to
as matrix norm) is denoted by 
��
 and is a function of any m �� n matrix A that
satisfies the following conditions
2.
1. 
 A 
��� 0 ;


A 
= 0, if and only if A = 0;
3. cA = |c| 
 A 
 , where c is a scalar and |c| denotes the modulus of c;
4. 
 A + B 
���
 A 
+
 B 
 , for any m �� n matrices 
 A 
 and 
 B 
 .
Some matrix norms also satisfy the property


AB 
���
 A 

 B 
 ,
for any n �� n matrices A and B.
For example, the Euclidean norm for any vector a (a matrix with n = 1) is
defined as


a 
=  a21 + a22 +������+ a2m 1/2 =  a a 1/2 ,
where a = [a1, a2,...,am] .
The Frobenius norm is a generalisation of the Euclidean norm to matrices and is
defined for any m �� n matrix A = (aij ) as


A 
=
������ mi=1 nj=1 a2
ij
������1/2 = % trace(A A)& 1/2 .
Having defined the matrix norm, we can define the distance between two matrices
A and B as d =
 A ��� B 
 . Note that in case of n = 1, this definition gives the
well known Euclidean distance between two vectors a = [a1, a2,...,am] and
b = [b1, b2,...,bm] d = ' (a1 ��� b1)2 + (a2 ��� b2)2 +������+ (am ��� bm)2.
34 2 Matrix Algebra, Probability and Statistics
The distance of two vectors or matrices are used to measure convergence of iterative
algorithms, e.g. in the context of the EM algorithm discussed in Sect. 2.4.2 below
and in Sect. 4.3.1.
The sequence of matrices A1, A2,..., sometimes written as {At} is said to
converge to a limiting matrix A, if and only if for any �� > 0, there exists some
t0 > 0 such that for any t>t0 it is


At ��� A 
 < ��.
This definition extends immediately the definition of a limit of sequence of real
numbers by replacing scalars by matrices and moduli by norms; for more details
on limits of one variable the reader is referred to Spivak (1995). Matrix norms are
covered in detail in the classic text of matrix analysis (Horn & Johnson, 2013).
1 / 1
ChatGPT
Here's a summary of the additional topics covered in the provided text:
Optimization:
Many statistical problems involve finding the maximum or minimum of a scalar function f(x) of a vector x.
A stationary point is identified by solving the equation ���f(x)/���x = 0.
If the second partial derivative is positive definite (negative definite), the stationary point is a minimum (maximum).
Example involving quadratic forms: f(x) = x^T Ax + 2a^T x + c, where A is a positive definite matrix.
Integration:
Integration of scalar functions of a vector or matrix.
Notation for multiple integrals, including symmetric matrices where only distinct elements are considered.
Limit of a Sequence of Matrices:
Introduction to matrix norms as a measure of distance for matrices.
The Frobenius norm and its use for matrices.
Definition of the distance between two matrices as d = ||A - B||.
Convergence of a sequence of matrices {At} to a limiting matrix A based on the distance between them.
These concepts are important in various mathematical and statistical contexts, including optimization problems, integration, and analyzing the behavior of sequences of matrices. If you have any questions or need further clarification on any of these topics, feel free to ask!
1 / 1
Here are the key points about optimization, integration, and limits of matrices from the passage:

- Optimization of a continuously differentiable function f(x) of a vector x involves finding values of x that satisfy the first-order optimality condition ���f(x)/���x = 0. If the Hessian matrix ���^2f(x)/���x���x^T is positive/negative definite at this point, it indicates a minimum/maximum.

- Integration of a scalar function f(x) of a vector or matrix argument x over a domain D is written as ���_D f(x) dx, where dx represents integrating over all elements of x.

- The limit of a sequence of matrices {A_t} as t������ is defined using a matrix norm ||.||. The sequence converges to a limit A if for any ��>0, there exists t_0 such that ||A_t - A|| \< �� for all t > t_0.

- Examples of matrix norms are the Frobenius norm (analog of Euclidean norm for vectors) and induced norms from vector norms. Distance between matrices is defined using a norm as d = ||A - B||.
1 / 1
ChatGPT
You've summarized the key points about optimization, integration, and limits of matrices very well! This captures the essential concepts and provides a clear understanding of these mathematical concepts. If you have any more questions or if there's anything else you'd like to learn about, feel free to ask.
1 / 1
Give some examples
1 / 1
ChatGPT
Certainly! Let's go through examples for each of the concepts: optimization, integration, and limits of matrices.
Optimization:
Suppose we have a function f(x) = x^2 - 4x + 5, and we want to find the minimum value of this function. To do this, we calculate the first derivative with respect to x: f'(x) = 2x - 4. Setting f'(x) = 0 gives us the critical point x = 2. Now, to determine if this critical point is a minimum, we calculate the second derivative: f''(x) = 2, which is positive. Since the second derivative is positive, the critical point x = 2 corresponds to a minimum of the function.
Integration:
Consider the scalar function f(x) = x^2 over the interval [0, 3]. To integrate this function over the interval, we write ���[0, 3] x^2 dx. Applying the power rule of integration, we get ���[0, 3] x^2 dx = [x^3/3] from 0 to 3 = (3^3/3) - (0^3/3) = 9.
Limits of Matrices:
Let's say we have a sequence of matrices {A_t} where each A_t is given by A_t = [t 0; 0 1/t]. As t approaches infinity, the limit of this sequence can be found. Using the Frobenius norm, we can calculate the distance between A_t and the limiting matrix A as ||A_t - A|| = ||[t 0; 0 1/t] - [0 0; 0 0]|| = ||[t 0; 0 1/t]|| = (t^2 + 1/t^2)^0.5. As t gets larger and larger, this distance approaches infinity. Thus, the sequence {A_t} diverges as t approaches infinity.
These examples showcase the application of optimization, integration, and limits of matrices in various mathematical contexts.
1 / 1
2.3 Probability and Distribution Theory
2.3.1 Random Vectors and Probability Distributions
Probability and statistics study uncertainty. Uncertainty arises when possible out���comes of an experiment, or a phenomenon in general, are uncertain. For example,
the future movements of shares in the stock market are uncertain; likewise the
possible increase or decrease in global temperature, which may be accounted for
an indicator of climate change, are uncertain. Probability provides the mathematical
notion for describing such phenomena, by introducing random outcomes responsi���ble for the underlying uncertainty. Statistics adopts probability models with the aim
to make inference based on observed data.
It is assumed that the reader is familiar with the basic notions of probability,
with random vectors and with probability distributions and with their properties.
Below we provide a brief summary of some of the basic definitions and properties
necessary for establishing notation.
1. A p-dimensional random vector, demoted by X is a vector whose elements are
random variables; we shall write
X = ��������������� X1 X2... Xp ��������������� .
Clearly when p = 1, X is reduced to a random variable. If the values of Xi
are discrete (the domain of Xi is a discrete set), then X is known as a discrete
2.3 Probability and Distribution Theory 35
random vector, while if the domain of Xi is a continuous set, then X is known as
a continuous random vector.
2. Associated with X is its joint cumulative distribution function (c.d.f.) defined as
FX(x) = FX(x1, x2,...,xp) = P (X1 ��� x1, X2 ��� x2,...,Xp ��� xp),
which is the probability of the event A = {X1 ��� x1}���{X2 ��� x2}������������{Xp ��� xp}, a subset of Rp.
If X is discrete we define the joint probability mass function (p.m.f.) of X as
pX(x) = pX(x1, x2,...,xp) = P (X1 = x1, X2 = x2,...,Xp = xp),
(2.15)
the probability of the event A = {X1 = x1}���{X2 = x2}������������{Xp = xp},
where A ��� B denotes the intersection of the events A and B.
If X is continuous, we define the joint probability density function (p.d.f.) of X
is
pX(x1, x2,...,xp) = ���pFX1X2������Xp (x1, x2,...,xp)
���x1���x2 ������ ���xp . (2.16)
It follows that
FX(x) = " x1
������
"
x2
������
������ "
xp
������
pX(u1, u2,...,up)du1 du2 ������ dup (2.17)
and a similar formula applies for discrete random vectors, if we replace the
integrals by sums. When p = 1 and X is continuous the above formulae simplify
to
pX(x) = dFX(x)
dx
and FX(x) = " x
������
pX(u) du.
Equations (2.15)���(2.17) indicate that knowledge of the p.m.f. or p.d.f. implies
knowledge of the c.d.f. and vice versa.
3. The marginal distribution of Xi is defined by integrating out the rest Xj (j  = i)
random variables, i.e.
pXi(xi) = " ���
������
pX(x1, x2,...,xp)dx1 ������ dxi���1 dxi+1 ������ dxp
and similarly we define the marginal c.d.f. of Xi. If X is discrete there is an
analogous definition of the marginal p.m.f. where the above integral is replaced
by a sum.
We define the conditional p.d.f. (p.m.f. if discrete) as the ratio of the joint
p.d.f. (p.m.f. if discrete) and the marginal p.d.f (p.m.f. if discrete), e.g. if X =
36 2 Matrix Algebra, Probability and Statistics
[X1, X2] is continuous, then the conditional p.d.f. of X1, given X2 = x2 is
defined as
pX1|X2 (x1 | x2) = pX(x1, x2) pX2 (x2) .
The random variables X1, X2,...,Xp are said to be independent, if their joint
p.d.f. (or p.m.f. if they are discrete) can be written as a product of their marginals,
i.e.
pX(x1, x2,...,xp) = pX1 (x1)pX2 (x2)������ pXp (xp).
If the random variables X1, X2,...,Xp are not independent, then they are said
to be dependent.
4. If X is continuous then the expectation of X is defined as
E(X) = " Rp
xpX(x1, x2,...,xp)dx1 dx2 ������ dxp
and if X is discrete the above integral is replaced by a sum.
For any continuous random vectors X, Y with conditional p.d.f. pX|Y (x | y) we
define the conditional expectation of X, given Y = y as the expectation of X,
with respect to the conditional density X | Y = y, i.e.
E(X | Y = y) = " Rx
xpX|Y (x | y) dx,
for any value y of Y , where Rx is the domain of pX|Y (x | y).
5. The covariance matrix of X is
Var(X) = E[X ��� E(X)][X ��� E(X)]
and the covariance of two random vectors X and Y is
Cov(X, Y ) = E[X ��� E(X)][Y ��� E(Y )] .
The following two properties are referred to as the tower properties and are useful
when we can calculate expectations of variances of X | Y , but not directly of X.
For any random vectors X and Y we have
a. E(X)
b. Var(X)
==E[EE[
(X
Var
|
(X
Y )
|];
Y )] + Var[E(X | Y )].
Property (a) is used to derive the independence of the residuals in Theorem 4.2
(Sect. 4.4).
1 / 1
ChatGPT
Here are the key points about probability and distribution theory from the passage:
Random Vectors and Probability Distributions:
Random vectors involve uncertain outcomes, like stock market movements or climate change indicators.
A p-dimensional random vector X has elements that are random variables.
The joint cumulative distribution function (c.d.f.) FX(x) gives the probability of all elements of X being less than or equal to corresponding values x.
For discrete X, the joint probability mass function (p.m.f.) pX(x) gives the probability of each element of X taking specific values.
For continuous X, the joint probability density function (p.d.f.) pX(x1, x2,...,xp) is defined.
Marginal and Conditional Distributions:
The marginal distribution of an element Xi involves integrating out other elements Xj (j ��� i).
Conditional p.d.f. (or p.m.f. for discrete) is the ratio of joint p.d.f. (p.m.f.) to marginal p.d.f. (p.m.f.).
Independence of random variables X1, X2,...,Xp means their joint p.d.f. can be expressed as a product of their marginals.
Expectation and Covariance:
Expectation E(X) is the average value of a random vector X.
Conditional expectation E(X | Y = y) is the expectation of X given Y = y.
Covariance matrix Var(X) measures the spread of elements of X around their mean.
Covariance Cov(X, Y) measures the joint variability between random vectors X and Y.
Tower Properties:
Tower properties involve conditional expectations and variances.
Property (a) states that the expectation of the expectation of X given Y is the same as the expectation of X.
Property (b) relates the variance of X to the expectation of the conditional variance of X given Y, plus the variance of the conditional expectation of X given Y.
These concepts are fundamental in understanding how uncertainty is quantified and how probability distributions are used to model and analyze uncertain phenomena.
1 / 1
2.3.2 Common Discrete Distributions
In this section we discuss four discrete distributions, the Poisson, the binomial,
the negative binomial and the multinomial distribution. Some of these distributions
are used to formulate discrete-response state space models, such as the dynamic
generalised linear models of Sect. 6.2.The binomial distribution is also discussed
in Sect. 2.4.3. For simplicity in the notation we drop the subscript X in the p.m.f.
pX(x), i.e. we write p(x) where reference to the random variable X is implied.
Poisson Distribution Let X be a discrete random variable taking values
0, 1, 2, 3,.... If its p.m.f. is given by
p(x) = P (X = x) = exp(�����)
��xx!
, x = 0, 1, 2,....
then X is said to follow the Poisson distribution, written X ��� Pois(��), for �� > 0.
The mean and the variance of X is
E(X) = Var(X) = ��.
Figure 2.1 shows the p.m.f. of the Poisson distribution, for �� = 2.
0 2 4 6 8 10
p.m.f. of the Poisson distribution
x
Fig. 2.1 Probability mass function of the Poisson distribution 0.00 0.05 0.10 0.15 0.20 0.25 p(x)
38 2 Matrix Algebra, Probability and Statistics
Binomial Distribution The random variable X with p.m.f.
p(x) = P (X = x) =  xn	 ��x (1 ��� �� )n���x , x = 0, 1, 2,...,n
is said to follow the binomial distribution of size n and probability ��; we write
X ��� Binom(n, �� ). The binomial coefficient is defined by

nx	 = n! x!(n ��� x)!.
The distribution is generated as the probability of the sum of n independent trials,
each following the Bernoulli distribution with p.m.f. ��x (1 ��� �� )1���x , where x = 0, 1
(here P (X = 0) = �� and P (X = 1) = 1 ��� ��).
The mean and variance of the binomial distribution are
E(X) = n�� and Var(X) = n��(1 ��� �� ).
Figure 2.2 shows the p.m.f. of the binomial distribution, for n = 10 and �� = 0.6.
0 2 4 6 8 10
p.m.f. of the binomial distribution
x
Fig. 2.2 Probability mass function of the binomial distribution 0.00 0.05 0.10 0.15 0.20 0.25 p(x)
2.3 Probability and Distribution Theory 39
Negative Binomial Distribution The negative binomial distribution is generated
as the distribution of the number of successes in independent Bernoulli trials before
a fixed number of failures �� occur. More specifically, X is said to follow the negative
binomial distribution, if its p.m.f. is
p(x) = P (X = x) =  x + x�� ��� 1	 (1 ��� �� )����x, x = 0, 1, 2, 3,...,
where �� is the probability of success of each Bernoulli trial. We shall write
X ��� NegBinom(��, �� ) to indicate that X follows the negative binomial distribution
with number of failures �� and probability of success ��. More generally the negative
binomial p.m.f. p(x) is defined for real-valued �� > 0, although in this case the
above genesis of the distribution does not apply.
The mean and variance of X are
E(X) = 1
����
��� ��
and Var(X) = (1 ���
����
�� )2 .
We observe that
Var(X) = 1E���
(X)
�� > E(X),
and so the negative binomial distribution is suitable for describing over-dispersed
data, i.e. data which variance is larger than its mean.
and
Figure 2.3 shows the p.m.f. of the negative binomial distribution, with �� = 0.6 �� = 8.
Multinomial Distribution The multinomial is a generalisation of the binomial
distribution to accommodate for several (more than two) categories. Suppose that
we have k ��� 2 categories and in each category i we have a fixed probability of
success ��i, with ��1+��2+������+��k = 1. Let n be the total number of trials or counts
and let Xi be the random variable that records the number of success in category i,
for i = 1, 2,...,k and X1 + X2 +������+ Xk = n. With these definitions the random
vector X = [X1, X2,...,Xk] is said to follow the multinomial distribution, if its
joint p.m.f. is given by
p(x) = P (X1 = x1, X2 = x2,...,Xk = xk) = n! x1!x2!������ xk!��x1 1 ��2x2 ������ ��kxk .
By means of notation we use X ��� Multin(n, �� ), where �� = [��1, ��2,...,��k] .
We observe that if k = 2 this distribution is reduced to the binomial distribution,
since X1+X2 = n or X2 = n���X1 and ��1+��2 = 1 or ��2 = 1�����1. We note that X
is only random in k ��� 1 variables, because we can always write Xk = n ��� ki=���11 Xi
40 2 Matrix Algebra, Probability and Statistics
0 2 4 6 8 10
p.m.f. of the negative binomial distribution
x
Fig. 2.3 Probability mass function of the negative binomial distribution
and ��k = 1 ���  ki=���11 ��i (e.g. in the binomial distribution k = 2 there is one random
variable X1.
The mean vector and the covariance matrix of X are
E(X) = n ��������������� ��1 ��2...��k ���������������
and
Var(X) = n ��������������� ��1(1 ��� ��1) �����1��2 ������ �����1��k �����1��2 ��2(1 ��� ��2) ������ �����2��k ... ... ... ... �����1��k �����2��k ������ ��k(1 ��� ��k)��������������� .
0.02 0.04 0.06 0.08 0.10 0.12 0.14
p(x)
2.3 Probability and Distribution Theory 41
2.3.3 Common Continuous Distributions
In this subsection we discuss the main continuous distributions, namely Gaussian
or normal, gamma and inverse gamma, beta, continuous uniform and the Student
t distributions. Most of Chaps. 3 and 4 make use of the multivariate Gaussian
distribution (Sect. 2.4.3 includes an example on Bayesian regression using the
Gaussian distribution); the gamma and Student t distributions are discussed in
Sect. 4.3.3.
Gaussian or Normal Distribution First we discuss the univariate version of the
Gaussian distribution. Let X be a continuous random variable. If its p.d.f. is
p(x) = ���21
����
exp 
 ���(x ��� ��)2 2��2  , x ��� R,
where ��2 > 0, then X is said to follow the Gaussian distribution with mean �� and
variance ��2; we write X ��� N(��, ��2).
and
Figure 2.4 shows the p.d.f. and the c.d.f. of the Gaussian distribution with �� = 0 ��2 = 1, i.e. X ��� N(0, 1).
Moving on to the multivariate Gaussian distribution, let X = [X1,...,Xp]
denote a random vector. X is said to follow the multivariate Gaussian or normal
distribution, with notation X ��� N(��, V), if its p.d.f. is given by
p(x) = 1 (2�� )p/2|V|1/2 exp 
 ���21
(x ��� ��) V���1(x ��� ��) , x ��� Rp,
where �� = [��1, ��2,...,��p] is the mean vector of X and V is the covariance
matrix of X. We note that if p = 1, this is reduced to the above univariate Gaussian
distribution with V = ��2.
Figure 2.5 shows the p.d.f. p(x, y) of the Gaussian distribution above with x1 =
x,x2 = y, �� = 
 00 
and V = 
 1 0.8 0.8 1  .
Gamma Distribution The positive random variable X > 0 is said to follow the
gamma distribution with parameters ��, �� > 0, if its p.d.f. is given by
p(x) = ����
(��) x�����1 exp(�����x), x > 0,
42 2 Matrix Algebra, Probability and Statistics
-3 -2 -1 0 1 2 3
p.d.f. of N(0,1)
x
-3 -2 -1 0 1 2 3
c.d.f. of N(0,1)
x
Fig. 2.4 Probability density function and cumulative distribution function of a normal distribution
N (0, 1)
where (��) denotes the gamma function of argument �� defined by
(��) = " 0��� u�����1 exp(���u) du.
If �� = ��/2 and �� = 1/2, the distribution is known as the chi-square distribution
with �� degrees of freedom, where �� is a positive real number.
The mean and the variance of X are
E(X) = ����
and Var(X) = ����2 .
Figure 2.6 shows the p.d.f.���s of three gamma distributions, namely G(1, 1),
G(2, 1) and G(5, 1). 0.0 0.1 0.2 0.3 0.4 p(x)
0.0 0.2 0.4 0.6 0.8 1.0
F(x)
2.3 Probability and Distribution Theory 43
-3
-2
-1
0 1 2 3 -3
-2
-1
0 1 2 3
0.00
0.05
0.10
0.15
0.20
0.25
p.d.f. of the bivariate normal distribution
Fig. 2.5 Probability density function of the bivariate normal distribution
Closely related to the gamma distribution is the inverse gamma distribution. Let
X ��� G(��, ��), then the random variable Y = 1/X is said to follow the inverse
gamma distribution, with p.d.f.
p(y) = ����
(��)
1 y��+1 exp �����y 	
, y> 0.
We shall write that Y ��� IG(��, ��).
The mean and the variance of Y is
E(Y ) = �� ����� 1 ,
for �� > 1 and
Var(Y ) = ��2
(�� ��� 1)2(�� ��� 2),
for �� > 2.
1 / 1
ChatGPT
Here are the key points about common discrete and continuous distributions from the passage:
Common Discrete Distributions:
Poisson Distribution:
Models the number of events occurring in a fixed interval of time or space. The p.m.f. is given by
p
(
x
)
=
e
���
��
��
x
x
!
p(x) = \frac{e^{-\lambda} \lambda^x}{x!}
p
(
x
)
=
x
!
e
���
��
��
x
���
where
x
=
0
,
1
,
2
,
���
x = 0, 1, 2, \ldots
x
=
0
,
1
,
2
,
���
and
X
���
Pois
(
��
)
X \sim \text{Pois}(\lambda)
X
���
Pois
(
��
)
.
Binomial Distribution:
Describes the number of successes in a fixed number of independent Bernoulli trials. The p.m.f. is
p
(
x
)
=
(
n
x
)
��
x
(
1
���
��
)
n
���
x
p(x) = \binom{n}{x} \pi^x (1 - \pi)^{n - x}
p
(
x
)
=
(
x
n
���
)
��
x
(
1
���
��
)
n
���
x
where
x
=
0
,
1
,
2
,
���
,
n
x = 0, 1, 2, \ldots, n
x
=
0
,
1
,
2
,
���
,
n
and
X
���
Binom
(
n
,
��
)
X \sim \text{Binom}(n, \pi)
X
���
Binom
(
n
,
��
)
.
Negative Binomial Distribution:
Represents the number of successes in Bernoulli trials before a fixed number of failures occur. The p.m.f. is
p
(
x
)
=
(
x
+
��
���
1
x
)
(
1
���
��
)
��
��
x
p(x) = \binom{x + \lambda - 1}{x} (1 - \pi)^\lambda \pi^x
p
(
x
)
=
(
x
x
+
��
���
1
���
)
(
1
���
��
)
��
��
x
where
x
=
0
,
1
,
2
,
3
,
���
x = 0, 1, 2, 3, \ldots
x
=
0
,
1
,
2
,
3
,
���
and
X
���
NegBinom
(
��
,
��
)
X \sim \text{NegBinom}(\lambda, \pi)
X
���
NegBinom
(
��
,
��
)
.
Multinomial Distribution:
Generalizes the binomial distribution to accommodate multiple categories. Describes counts in each category. The joint p.m.f. is
p
(
x
)
=
n
!
x
1
!
x
2
!
���
x
k
!
���
i
=
1
k
��
i
x
i
p(x) = \frac{n!}{x_1!x_2! \ldots x_k!} \prod_{i=1}^k \pi_i^{x_i}
p
(
x
)
=
x
1
���
!
x
2
���
!
���
x
k
���
!
n
!
���
���
i
=
1
k
���
��
i
x
i
���
���
for
x
1
+
x
2
+
���
+
x
k
=
n
x_1 + x_2 + \ldots + x_k = n
x
1
���
+
x
2
���
+
���
+
x
k
���
=
n
, and
X
���
Multin
(
n
,
��
)
X \sim \text{Multin}(n, \pi)
X
���
Multin
(
n
,
��
)
.
Common Continuous Distributions:
Gaussian or Normal Distribution:
Describes continuous random variables with a bell-shaped curve. The p.d.f. is
p
(
x
)
=
1
2
��
��
exp
���
(
���
(
x
���
��
)
2
2
��
2
)
p(x) = \frac{1}{\sqrt{2\pi}\sigma} \exp\left(-\frac{(x - \mu)^2}{2\sigma^2}\right)
p
(
x
)
=
2
��
���
��
1
���
exp
(
���
2
��
2
(
x
���
��
)
2
���
)
for
x
���
R
x \in \mathbb{R}
x
���
R
and
X
���
N
(
��
,
��
2
)
X \sim \mathcal{N}(\mu, \sigma^2)
X
���
N
(
��
,
��
2
)
.
Gamma Distribution:
Models positive random variables. The p.d.f. is
p
(
x
)
=
��
��
x
��
���
1
e
���
��
x
��
(
��
)
p(x) = \frac{\beta^\alpha x^{\alpha - 1} e^{-\beta x}}{\Gamma(\alpha)}
p
(
x
)
=
��
(
��
)
��
��
x
��
���
1
e
���
��
x
���
for
x
>
0
x > 0
x
>
0
, where
��
(
��
)
\Gamma(\alpha)
��
(
��
)
is the gamma function and
X
���
Gamma
(
��
,
��
)
X \sim \text{Gamma}(\alpha, \beta)
X
���
Gamma
(
��
,
��
)
.
Inverse Gamma Distribution:
Reciprocal of the gamma distribution. The p.d.f. is
p
(
y
)
=
��
��
��
(
��
)
y
���
(
��
+
1
)
e
���
��
/
y
p(y) = \frac{\beta^\alpha}{\Gamma(\alpha)} y^{-(\alpha + 1)} e^{-\beta/y}
p
(
y
)
=
��
(
��
)
��
��
���
y
���
(
��
+
1
)
e
���
��
/
y
for
y
>
0
y > 0
y
>
0
and
Y
���
IG
(
��
,
��
)
Y \sim \text{IG}(\alpha, \beta)
Y
���
IG
(
��
,
��
)
.
These distributions are fundamental tools in probability and statistics, used to model a wide range of phenomena and make statistical inferences.
1 / 1
Beta Distribution The random variable 0 ��� X ��� 1 is said to follow the beta
distribution, if its p.d.f. is given by
p(x) = (�� + ��)
(��)(��) x�����1(1 ��� x)�����1, 0 ��� x ��� 1,
where ��, �� > 0. By means of notation we write X ��� Beta(��, ��).
The mean and the variance of X are
E(X) = �� �� + ��
and Var(X) = ����
(�� + ��)2(�� + �� + 1).
Figure 2.7 shows the p.d.f.���s of three beta distributions, namely Beta(0.5, 0.5),
Beta(2, 2.5) and Beta(5, 1).
Uniform Distribution The random variable X follows the continuous uniform
distribution, if its p.d.f. is given by
p(x) =  1 b���a , a ��� x ��� b 0, otherwise
0.0 0.2 0.4 0.6 0.8
p(x)
2.3 Probability and Distribution Theory 45
0.0 0.2 0.4 0.6 0.8 1.0
p.d.f. of the beta distribution
x
Beta(0.5,0.5)
Beta(2,2.5)
Beta(5,1)
Fig. 2.7 Probability density function of the beta distribution
for any a<b. We will use the notation X ��� U (a, b) to indicate that X follows the
uniform distribution with parameters a and b.
The mean of X is
E(X) = a + b 2 ,
and the variance of X is
Var(X) = (b ���
12
a)2 .
Figure 2.8 shows the p.d.f. of U (���1, 1).
Student t Distribution We start with the univariate case. The random variable X
is said to follow the Student t distribution, if its p.d.f. is
p(x) = ����/2  ��+1 2  �����  ��2  �� 
 �� + (x ��� ��)2 ��2  ���(��+1)/2
, x ��� R,
01234
p(x)
46 2 Matrix Algebra, Probability and Statistics
-2 -1 0 1 2
p.d.f. of uniform U(-1,1)
x
Fig. 2.8 Probability density function of the uniform distribution
where �� > 0, �� and ��2 > 0 are the parameters of the distribution. By means of
notation we write X ��� t (��, ��, ��2). The parameter �� > 0 is known as the degrees of
freedom.
The mean of X is
E(X) = ��,
for �� > 1 and the variance of X is
Var(X) = ����2 �� ��� 2,
for �� > 2. If 0 < �� ��� 2 the variance is not defined; an example of a distribution with
undefined mean and variance is the Cauchy distribution, obtained as the Student t
distribution for �� = 1.
Figure 2.9 shows the p.d.f. of Student t distribution with 3 and 10 degrees of
freedom as compared with a Gaussian N(0, 1) distribution. We observe that the
Student t distribution has heaver tails than the Gaussian and that the smaller the
degrees of freedom the heavier the tails are. In fact we can see that as �� ��� ���, the 0.0 0.2 0.4 0.6 0.8 1.0 p(x)
2.3 Probability and Distribution Theory 47
-3 -2 -1 0 1 2 3
p.d.f. of the Student t distribution
x
N(0,1)
t(3,0,1)
t(10,0,1)
Fig. 2.9 Probability density function of the Student t distribution
p.d.f. of the Student t distribution converges to that of a Gaussian distribution (see
Exercise 2.10).
Now consider a random vector X = [X1, X2,...,Xp] . X is said to follow the
multivariate Student t distribution with �� > 0 degrees of freedom, mode or location
vector �� and scale covariance matrix V, if its density is
p(x) = ����/2  ��+p2  ��p/2  ��2  |V|1/2 % �� + (x ��� ��) V���1(x ��� ��)& ���(��+p)/2 .
In terms of notation we write X ��� t (��, ��, V).
The mean vector of X is
E(X) = ��,
for �� > 1 and the covariance matrix of X is
Var(X) = �� �� ��� 2V,
for �� > 2.
0.0 0.1 0.2 0.3 0.4
p(x)
48 2 Matrix Algebra, Probability and Statistics
We observe that for p = 1 and V = ��2, we obtain the univariate Student t
distribution, with p.d.f. given above.
As in the univariate case, the multivariate Student t distribution has heavier tails
than the multivariate Gaussian distribution N(��, V), controlled by the degrees of
freedom �� (the smaller �� the heavier the tails). As �� ��� ��� the Student t p.d.f.
converges to the p.d.f. of the normal N(��, V).
1 / 1
ChatGPT
Here are the key points about the remaining common continuous distributions from the passage:
Beta Distribution:
The random variable
0
���
X
���
1
0 \leq X \leq 1
0
���
X
���
1
follows the beta distribution if its p.d.f. is
p
(
x
)
=
��
(
��
+
��
)
��
(
��
)
��
(
��
)
x
��
���
1
(
1
���
x
)
��
���
1
p(x) = \frac{\Gamma(\alpha + \beta)}{\Gamma(\alpha)\Gamma(\beta)} x^{\alpha - 1} (1 - x)^{\beta - 1}
p
(
x
)
=
��
(
��
)
��
(
��
)
��
(
��
+
��
)
���
x
��
���
1
(
1
���
x
)
��
���
1
where
��
,
��
>
0
\alpha, \beta > 0
��
,
��
>
0
, and
X
���
Beta
(
��
,
��
)
X \sim \text{Beta}(\alpha, \beta)
X
���
Beta
(
��
,
��
)
.
The mean and variance of
X
X
X
are
E
(
X
)
=
��
��
+
��
E(X) = \frac{\alpha}{\alpha + \beta}
E
(
X
)
=
��
+
��
��
���
and
V
a
r
(
X
)
=
��
��
(
��
+
��
)
2
(
��
+
��
+
1
)
Var(X) = \frac{\alpha\beta}{(\alpha + \beta)^2 (\alpha + \beta + 1)}
Va
r
(
X
)
=
(
��
+
��
)
2
(
��
+
��
+
1
)
��
��
���
.
Uniform Distribution:
The continuous random variable
X
X
X
follows the uniform distribution if its p.d.f. is
p
(
x
)
=
1
b
���
a
p(x) = \frac{1}{b - a}
p
(
x
)
=
b
���
a
1
���
for
a
���
x
���
b
a \leq x \leq b
a
���
x
���
b
and
0
0
0
otherwise. The mean of
X
X
X
is
E
(
X
)
=
a
+
b
2
E(X) = \frac{a + b}{2}
E
(
X
)
=
2
a
+
b
���
, and the variance is
V
a
r
(
X
)
=
(
b
���
a
)
2
12
Var(X) = \frac{(b - a)^2}{12}
Va
r
(
X
)
=
12
(
b
���
a
)
2
���
.
Student t Distribution:
For the univariate case, the random variable
X
X
X
follows the Student t distribution with
��
��
��
degrees of freedom, mean
��
��
��
, and scale parameter
��
��
��
. The p.d.f. is
p
(
x
)
=
��
(
��
+
1
2
)
��
��
��
��
(
��
2
)
(
1
+
(
x
���
��
)
2
��
��
2
)
���
��
+
1
2
p(x) = \frac{��\left(\frac{�� + 1}{2}\right)}{�� \sqrt{����} ��\left(\frac{��}{2}\right)} \left(1 + \frac{(x - ��)^2}{����^2}\right)^{-\frac{�� + 1}{2}}
p
(
x
)
=
��
��
��
���
��
(
2
��
���
)
��
(
2
��
+
1
���
)
���
(
1
+
��
��
2
(
x
���
��
)
2
���
)
���
2
��
+
1
���
.
The mean of
X
X
X
is
E
(
X
)
=
��
E(X) = ��
E
(
X
)
=
��
for
��
>
1
�� > 1
��
>
1
, and the variance is
V
a
r
(
X
)
=
��
��
2
��
���
2
Var(X) = \frac{����^2}{�� - 2}
Va
r
(
X
)
=
��
���
2
��
��
2
���
for
��
>
2
�� > 2
��
>
2
.
For the multivariate case of the Student t distribution, similar properties hold for the mean and covariance matrix of the random vector
X
=
[
X
1
,
X
2
,
���
,
X
p
]
T
X = [X_1, X_2, \ldots, X_p]^T
X
=
[
X
1
���
,
X
2
���
,
���
,
X
p
���
]
T
, where
X
���
t
(
��
,
��
,
V
)
X \sim t(��, ��, V)
X
���
t
(
��
,
��
,
V
)
.
These distributions are fundamental tools in probability and statistics, used to model a wide range of phenomena and make statistical inferences.
1 / 1
Simplified.
1 / 1
ChatGPT
Sure, here's a simplified summary of the key points about the remaining common continuous distributions:
Beta Distribution:
The beta distribution describes random variables between 0 and 1.
Its shape is controlled by parameters
��
\alpha
��
and
��
\beta
��
.
The mean is
��
��
+
��
\frac{\alpha}{\alpha + \beta}
��
+
��
��
���
, and variance depends on
��
\alpha
��
and
��
\beta
��
.
Uniform Distribution:
The uniform distribution is for values within a specified range.
Mean is
a
+
b
2
\frac{a + b}{2}
2
a
+
b
���
, and variance depends on the range.
Student t Distribution:
It's used for studying small sample sizes and dealing with outliers.
The degrees of freedom (
��
��
��
) control its shape.
As
��
��
��
increases, it approaches the normal distribution.
Mean and variance depend on
��
��
��
,
��
��
��
, and
��
��
��
for both univariate and multivariate cases.
1 / 1
2.4 Statistics
2.4.1 Principle Set-Up and Objectives
Statistics is concerned with quantifying and managing uncertainty in observational
studies, may these be controlled experiments, or socio-economic or physical
phenomena. With observational study broadly we mean studies based on observed
outcomes, which are uncertain. A good introductory treatment of Statistics can be
found in Trosset (2009).
Uncertainty arises in all walks of life. In climate change, for example, the
potential increase of the global temperature is hugely uncertain, as is the extent
to this increase attributed to human-made actions. In a physical system the output
signal may be uncertain due to measurement error and in economics fluctuations of
asset prices are uncertain due to market and investors��� movements.
Considering a vector x = [x1, x2,...,xn] of n observed or recorded mea���surements we can postulate that uncertainty around the values of x is generated
by x being a particular realisation of a random vector X = [
Thus, it is natural to think that the probability distribution of
X1, X2,...,Xn] . X will summarise the
uncertainty around x and thus it may be used to describe the generating process of
x and to forecast future values of x. This distribution will typically depend upon
some parameter vector ��, which is responsible for the shape of the distribution of
X. We write p(x | �� ) to indicate the p.d.f. of X (if X is continuous) or the p.m.f.
of X (if X is discrete). Statistical inference is concerned about estimating ��, based
on the observed data x1, x2,...,xn and hence provide forecasts of future values of
x, based on this observed data. Within the above model-based framework two main
estimation approaches may be deployed
1. Maximum likelihood inference (frequentist statistics); and
2. Posterior inference (Bayesian statistics).
In (1) we consider that �� is a fixed, but known quantity and estimation procedures
largely involve maximum likelihood analysis, see e.g. Sect. 2.4.2 below. In (2)
�� is assumed to be random and estimation procedures involve the derivation of
the conditional distribution of ��, given the data x; Sect. 2.4.3 discusses Bayesian
inference and gives two examples for illustration purposes.
2.4 Statistics 49
2.4.2 Maximum Likelihood Estimation: The EM Algorithm
The likelihood function has a central role on the development of estimators in
statistical inference. The likelihood function, a function of the unknown param���eter or parameter vector ��, is just the joint distribution of the random sample
X1, X2,...,Xn, given ��, i.e.
L(��; x) = p(x1, x2,...,xn | �� ),
where x = [x1, x2,...,xn] . The likelihood function is a (deterministic) function
of ��, which gives a measure of how likely is �� for a given sample x1, x2,...,xn. If
�� is not very likely then L(��; x) should be small, while if it is likely the value of
L(��; x) should be large. Thus, say for two values ��1 and ��2 of �� the inequality
L(��1; x) > L(��2; x)
will favour ��1 as being more likely than ��2, for this sample. This naturally leads us
to choose the value �� that maximises the likelihood function, hence the maximum
likelihood principle. Usually, for computational efficiency, we choose to maximise
the logarithm of the likelihood, known as log-likelihood function, written as
(��; x) = logL(��; x).
In many situations it is hard to find the maximum of the likelihood function.
The expectation maximisation (EM) algorithm, originally developed in Dempster
et al. (1977) and further discussed in many textbooks, see e.g. Fahrmeir and Tutz
(2001), is a popular choice of an indirect maximisation algorithm of the likelihood
L(��; x). The EM algorithm is an iterative algorithm, which consists of two steps:
in the E-step the conditional expectation of the log-likelihood function is computed,
given the past sample values and the current estimate of ��, and in the M-step a new
estimate of �� is computed that maximises the expected log-likelihood function from
the E-step. Below we provide description and the foundations of the EM algorithm,
which is then used in Sect. 4.3.1 to derive the EM algorithm for state space models.
Our aim is to maximise L(��; x) with respect to ��. Consider an unobserved
discrete random vector Z and denote with Rz the domain of the p.m.f. of Z. We
can express L(��; x) in terms of Z as
L(��; x) = p(x | �� ) =  z���Rz
p(x, z | �� ).
50 2 Matrix Algebra, Probability and Statistics
Notice that this is the marginal distribution of X, after we sum out Z. Then the EM
algorithm is
1. Initialise ����(0);
2. E-step: calculate the conditional expectation
Q(�� | ����(k)) = E[logL(��; x, z) | x, ����(k)];
3. M-step: find the maximum
����(k+1) = arg max�� Q(�� | ����(k));
4. Update k ��� k + 1 and repeat steps (2)-(3) until convergence (see below).
As k ��� ���, ����(k) converges to the maximum likelihood estimate ���� of L(��; x). In
practice the algorithm terminates when ����(k+1) and ����(k) are close to each other, hence
convergence is assumed. The rule to check for convergence is if the Euclidean norm
of ����(k+1) ��� ����(k) is smaller to a pre specified tolerance, i.e.


����(k+1) ��� ����(k) 
 < Tol.
Usually we set Tol = 0.001 or smaller.
Before we prove the correctness of the algorithm we give the Gibbs inequality.
For any two discrete distributions with p.m.f.���s p(x) and q(x), respectively, we have
x���Rx
p(x)log p(x) ��� x���Rx
p(x)log q(x), (2.18)
with equality if and only if p(x) = q(x), for all x ��� Rx , where Rx is the domain of
p(x).
To prove this we use the well known inequality log x ��� x ��� 1, for any x > 0.
x���Rx
p(x)log q(x)
p(x) ��� x���Rx
p(x) 
 q(x)
p(x) ��� 1 =  x���Rx
q(x) ��� x���Rx
p(x)
=  x���Rx
q(x) ��� 1 ��� 0
and so  x���Rx p(x)log p(x) ���  x���Rx p(x)log q(x).
2.4 Statistics 51
Now we prove the correctness of the EM algorithm. From the definition of the
joint distribution p(x, z | �� ) = p(z | x, �� )p(x | �� ) we have
p(x | �� ) = p(x, z | �� )
p(z | x,��)
and so
log p(x | �� ) = log p(x, z | �� ) ��� logp(z | x,��)
=  z���Rz
p(z | x, ����(k))log p(x, z | �� )




expectation of logp(x,z|�� )
��� z���Rz
p(z | x, ����(k))log p(z | x,��)




expectation of logp(z|x,�� )
= Q(�� | ����(k)) + G(�� | ����(k)).
Then
log p(x | �� ) ��� log p(x | ����(k)) = Q(�� | ����(k)) ��� Q(����(k) | ����(k)) +G(�� | ����(k)) ��� G(����(k) | ����(k)).
Now applying the Gibbs inequality (2.18) with p(x) = p(z | x, ����(k)) and q(x) =
p(z | x,��), we obtain
G(�� | ����(k)) ��� G(����(k) | ����(k)).
Hence
log p(x | �� ) ��� log p(x | ����(k)) ��� Q(�� | ����(k)) ��� Q(����(k) | ����(k)).
From this inequality, an improvement in Q (Q(�� | ����(k)) ��� Q(����(k) | ����(k))) results in
an improvement in L(��; x) = log p(x | �� ) (logp(x | �� ) ��� p(x | ����(k))).
As we obtain in the next iteration ����(k+1 as the maximum of Q(�� | ����(k)), an
improvement in L(��; x) is achieved. As a result, as k ��� ���, ����(k) converges to the
maximum likelihood estimate ����.
1 / 1
ChatGPT
In summary, here are the key points about the "Maximum Likelihood Estimation" and the "EM Algorithm":
Maximum Likelihood Estimation:
The likelihood function is the joint distribution of observed data given the parameters.
Maximum Likelihood Estimation (MLE) involves finding parameter values that maximize the likelihood function.
It's often more convenient to work with the log-likelihood, denoted as
L
(
��
;
x
)
\mathcal{L}(\theta; x)
L
(
��
;
x
)
.
MLE aims to find parameter values that make the observed data most likely under the assumed model.
MLE can be applied in both univariate and multivariate settings.
The EM (Expectation-Maximization) algorithm is a popular method for finding MLE when there are unobserved variables involved.
EM algorithm consists of alternating between an E-step (compute conditional expectations of log-likelihood) and an M-step (update parameter estimates to maximize the expected log-likelihood).
The EM algorithm iteratively refines parameter estimates until convergence.
The EM algorithm is widely used in statistics and data analysis to estimate parameters in models where some information is missing or latent variables are involved.
1 / 1
ChatGPT may produce inaccurate information about people, places, or facts.
ChatGPT August 3 Version
?
ChatGPT





