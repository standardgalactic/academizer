Free-Energy Framework
Open sidebar
Free-Energy Framework
Model: Default (GPT-3.5)
Summarize: An informal reconstruction of the free-energy framework, examining the 
conceptual problems that arise 
Chris Thornton 
University of Sussex, Brighton, UK 
A R T I C L E I N F O 
Keywords: 
Free energy 
Information 
Prediction 
Uncertainty 
Bayesian inference 
Approximate inference 
A B S T R A C T 
Recent decades have seen increasing attention given to the free-energy framework. This proposes that many of 
the phenomena we associate with life, intelligence and adaptation can be explained in terms of a single, 
mathematical precept: the free-energy principle. This is claimed to apply to the adaptive behavior of primitive 
organisms as much as it does to the physiological structures of human brains. The proposal is potentially of 
interest to theorists from multiple disciplines. But as presentations often make intensive use of mathematical 
notation, it can be hard to understand, even for experts. The present article presents an informal reconstruction, 
using schematic illustrations. Mathematical notation is largely avoided, while detail and precision are retained as 
far as possible. The specif i cally conceptual problems that come to notice in the reconstruction are highlighted 
and discussed. 
1. Part 1 
1.1. Introduction 
In recent years, research related to free energy has become increas- 
ingly prominent (Colombo and Wright, 2018; Ramstead et al., 2020a, 
2020b; van Es, 2021; Wiese and Friston, 2021). This framework ad- 
vances a remarkable proposal: that many forms of intelligent and 
adaptive behavior ��� among other phenomena ��� can be explained in 
terms of a single, precept: the free-energy principle (FEP). The proposal is 
likely to be of interest to any theorist concerned with the mind, cogni- 
tion, and living things more generally. The diff i culty is that it can be 
hard to understand, even for experts. As Herreror and Verschure note, 
���for the broader audience of the uninitiated, the goal of FEP, its theo- 
retical framework, and leverage may still be elusive��� (Herreros and 
Verschure, 2015, p. 218). 
Particularly challenging are the mathematical aspects of the pro- 
posal. As Bogacz comments, ���the description of mathematical details of 
the theory in these papers requires a very deep mathematical back- 
ground��� (Bogacz, 2017, p. 199). In their presentation of the framework 
��� itself a work involving more than 100 equations ��� Buckley et al. 
acknowledge that 
��� [t]he mathematics involved is non-trivial and has been presented 
over different stages of evolution and using varying notations 
(Buckley et al., 2017, p. 56) 
The present article aims to address this diff i culty by setting out an 
informal reconstruction of the framework, which largely avoids use of 
mathematical notation. It is not a free-energy publication of the tradi- 
tional kind, or a tutorial in the conventional sense. It does not describe 
an experiment, or present a review of published critiques. The intention, 
rather, is to reconstruct the framework to a level of detail that allows an 
informed assessment to be developed, without signif i cant use of math- 
ematical formalism. 
Reconstructing a mathematically complex framework in this way is a 
challenging exercise that requires special measures to be taken. The 
approach taken here is strictly bottom-up and incremental. It is also to 
some degree historical, in that it recapitulates the key theoretical in- 
novations in the order they were developed. The reconstruction is built- 
up layer by layer, with the simplest and longest-established element ��� 
Bayesian inference ��� set out fi rst. Introduction of this basic constituent 
leads on to introducing approximate Bayesian inference, uncertainty 
reduction, and to exploring how the two are combined in free energy 
minimization. With this central core established, the focus moves on to 
the agent-level interpretation associated with the work of Karl Friston 
and colleagues. 
The reconstruction is assembled in this somewhat long-winded way 
E-mail address: c.thornton@sussex.ac.uk. 
Contents lists available at ScienceDirect 
Neuropsychologia 
journal homepage: www.elsevier.com/locate/neuropsychologia 
https://doi.org/10.1016/j.neuropsychologia.2022.108281 
Received 22 November 2021; Received in revised form 19 May 2022; Accepted 26 May 2022 
Neuropsychologia 173 (2022) 108281 
2 
in order to achieve the level of detail and precision normally secured 
through use of notation. The intent is that there should be no disconti- 
nuities ��� a concept used at one level of the reconstruction should have 
been introduced previously. Content that is ambiguous should not be 
encountered. This approach also inf l uences the evaluation of conceptual 
problems in Part 3. While diff i culties affecting the free-energy frame- 
work are subject to wide-ranging debate, the ones examined here are of 
a restricted type. It is specif i cally the conceptual problems that come to 
notice within the reconstruction that are considered. 
Also key strategically is the adoption of a ruling reference. The 
variation that exists between published presentations of the framework 
poses a problem for anyone wishing to understand it. To overcome this, 
it is useful to treat one source as def i nitive. For present purposes, the 
source so treated is (Buckley et al., 2017). One implication of this is that 
features of the framework introduced relatively recently are given no 
coverage. The treatment of applications is affected in similar fashion. 
Recent applications in areas such as metaphysics, intentionality and 
consciousness (Beni, 2021; van Es, 2021; Wiese and Friston, 2021) are 
given no attention.1 
The neutrality of the enterprise should also be emphasized. The aim 
is to present a reconstruction that enables an informed assessment to be 
formed, based on computational ideas rather than mathematical nota- 
tion. It is not the intention to stipulate what the assessment should be. 
On the basis of what is set out, it should be possible to identify the 
specif i cally conceptual problems that arise, and judge whether they are 
damaging to the framework as a whole. 
The body of the paper is structured in three parts. The next section 
introduces Bayesian inference, the oldest and most fundamental con- 
stituent of the framework. Following that, Section 1.3 introduces the 
concept of approximate Bayesian inference (i.e., use of approximation to 
avoid the diff i culty of identifying observation probabilities). Section 1.4 
introduces the information metric of surprisal, and examines its poten- 
tial use in minimizing the uncertainty of a posterior distribution. This 
leads on to identifying free energy minimization itself, and to examining 
its use in the Helmholtz machine formulation. Part 2 of the paper then 
sets out the agent-level reinterpretation of the process, also highlighting 
(Section 2.3) the role played by active inference. Part 3 addresses the 
specif i cally conceptual problems that come to notice in the reconstruc- 
tion, and examines the origins of the framework���s exceptional 
versatility. 
1.2. Bayesian inference 
Fundamental to the free-energy framework is the long-established 
concept of Bayesian inference. (Readers familiar with this topic can 
safely skip to Section 1.3). As well as being a key element within the free- 
energy proposal, Bayesian inference is also relatively easy to explain. 
Consider the following example. Suppose that, in a box of avocados, 10 
are rotten, and we know that 50% of rotten avocados are soft to the 
touch. An interesting question then concerns an avocado that is found to 
be soft. What should we assume are the chances of it being rotten? It will 
be seen that the answer must depend on the total number of soft 
avocados. If there are 20, for example, 20 �� 0.5 = 10 of these will be 
rotten. As these make up 100% of all rotten cases, it is seen there is a 
100% chance of a soft avocado being rotten. If there are only 10 soft 
avocados, these make up only half of all rotten cases, meaning there is 
only a 50% chance. 
This method of calculation can also use conditional and uncondi- 
tional probabilities, in which form it is called Bayes��� rule or Bayes��� 
theorem. The method of derivation is termed Bayesian inference. The 
avocado scenario would be dealt with by stating the probability of an 
avocado being soft, the probability of it being rotten, and the conditional 
probability of it being rotten given it is soft. The conditional probability 
we���re interested in ��� the probability of a soft avocado being rotten ��� is 
then found by dividing the probability of seeing a soft, rotten avocado, 
by the probability of seeing a rotten avocado.2 The result obtained is as 
described above, except that the referenced quantities are now 
probabilities. 
An advantage of Bayesian inference is that it is completely general. It 
can be applied to any scenario involving states and observations.3 By 
combining state/observation probabilities with the conditional proba- 
bility of the observation given the state, we can compute a probability 
that is usually more interesting: the probability of the state given the 
observation. This can be the basis for diagnosis in a medical scenario. 
Suppose the probability of seeing the skin condition psoriasis is 0.01, 
and the probability of seeing scaly skin ��� one of psoriasis���s main 
symptoms ��� is 0.005. If the conditional probability of psoriasis pro- 
ducing scaly skin is 0.2, we can calculate the conditional probability that 
scaly skin indicates psoriasis. This is 0.01��0.2 
0.005 = 0.4. The observation of 
scaly skin suggests there is a 40% chance of psoriasis. Diagnosis can 
proceed on this basis. (The fi gures are purely illustrative.) 
In practice, Bayesian inference is often used to fi nd which of a set of 
states is most probable in light of given observations. The formula is 
applied to each individual case, and results are normalized to obtain an 
overall judgement. The process is given the name maximum a posteriori 
(MAP) inference, in accordance with the way quantities are labelled in 
Bayesian theory. A conditional probability derived using Bayes��� rule is 
termed a posterior probability (or just a posterior). The two uncondi- 
tional probabilities are termed prior probabilities (or just priors) if they 
are provided in advance. The conditional probability of the observation 
given the state is termed the likelihood. In what follows, the product of 
the likelihood and the state probability (the numerator in Bayes��� for- 
mula) will also be given a name. This will be termed the predicted 
observation probability or just the prediction. This and other names to be 
used are set out diagrammatically in Fig. 1.
1 / 1
ChatGPT
ChatGPT
The article discusses the free-energy framework, which proposes that various phenomena related to life, intelligence, and adaptation can be explained using the free-energy principle. However, understanding the framework can be challenging due to its mathematical notation. The article presents an informal reconstruction of the framework using schematic illustrations and minimal mathematical notation. The reconstruction starts with Bayesian inference and gradually introduces concepts such as approximate Bayesian inference, uncertainty reduction, and free energy minimization. The agent-level interpretation associated with the work of Karl Friston is also discussed. The article aims to identify and examine the conceptual problems that arise within the reconstruction. It follows a bottom-up and incremental approach, presenting the key theoretical innovations in the order they were developed. The reconstruction is based on a specific source, and recent applications and critiques are not covered. The article is structured into three parts: introduction, Bayesian inference, and the evaluation of conceptual problems within the reconstruction. Bayesian inference, which is a fundamental component of the free-energy framework, is explained using examples such as avocados and medical diagnosis. The article aims to provide an informed assessment of the framework based on computational ideas rather than mathematical notation.
1 / 1
1.2. Bayesian inference 
Fundamental to the free-energy framework is the long-established 
concept of Bayesian inference. (Readers familiar with this topic can 
safely skip to Section 1.3). As well as being a key element within the free- 
energy proposal, Bayesian inference is also relatively easy to explain. 
Consider the following example. Suppose that, in a box of avocados, 10 
are rotten, and we know that 50% of rotten avocados are soft to the 
touch. An interesting question then concerns an avocado that is found to 
be soft. What should we assume are the chances of it being rotten? It will 
be seen that the answer must depend on the total number of soft 
avocados. If there are 20, for example, 20 �� 0.5 = 10 of these will be 
rotten. As these make up 100% of all rotten cases, it is seen there is a 
100% chance of a soft avocado being rotten. If there are only 10 soft 
avocados, these make up only half of all rotten cases, meaning there is 
only a 50% chance. 
This method of calculation can also use conditional and uncondi- 
tional probabilities, in which form it is called Bayes��� rule or Bayes��� 
theorem. The method of derivation is termed Bayesian inference. The 
avocado scenario would be dealt with by stating the probability of an 
avocado being soft, the probability of it being rotten, and the conditional 
probability of it being rotten given it is soft. The conditional probability 
we���re interested in ��� the probability of a soft avocado being rotten ��� is 
then found by dividing the probability of seeing a soft, rotten avocado, 
by the probability of seeing a rotten avocado.2 The result obtained is as 
described above, except that the referenced quantities are now 
probabilities. 
An advantage of Bayesian inference is that it is completely general. It 
can be applied to any scenario involving states and observations.3 By 
combining state/observation probabilities with the conditional proba- 
bility of the observation given the state, we can compute a probability 
that is usually more interesting: the probability of the state given the 
observation. This can be the basis for diagnosis in a medical scenario. 
Suppose the probability of seeing the skin condition psoriasis is 0.01, 
and the probability of seeing scaly skin ��� one of psoriasis���s main 
symptoms ��� is 0.005. If the conditional probability of psoriasis pro- 
ducing scaly skin is 0.2, we can calculate the conditional probability that 
scaly skin indicates psoriasis. This is 0.01��0.2 
0.005 = 0.4. The observation of 
scaly skin suggests there is a 40% chance of psoriasis. Diagnosis can 
proceed on this basis. (The fi gures are purely illustrative.) 
In practice, Bayesian inference is often used to fi nd which of a set of 
states is most probable in light of given observations. The formula is 
applied to each individual case, and results are normalized to obtain an 
overall judgement. The process is given the name maximum a posteriori 
(MAP) inference, in accordance with the way quantities are labelled in 
Bayesian theory. A conditional probability derived using Bayes��� rule is 
termed a posterior probability (or just a posterior). The two uncondi- 
tional probabilities are termed prior probabilities (or just priors) if they 
are provided in advance. The conditional probability of the observation 
given the state is termed the likelihood. In what follows, the product of 
the likelihood and the state probability (the numerator in Bayes��� for- 
mula) will also be given a name. This will be termed the predicted 
observation probability or just the prediction. This and other names to be 
used are set out diagrammatically in Fig. 1. 
A problem often considered of interest involves the observation 
probability. Assuming this is not simply given in advance (as a prior), it 
has to be derived in some way. The diff i culty is that in many situations 
there are inf i nitely many potential observations. Identifying the proba- 
bility of a single one is then problematic at best. Even with a fi nite 
number of observations, calculating the probability of a single case in- 
volves iterating over all situations that potentially apply, hence all 
Fig. 1. Bayes��� rule and its constituent terms, expressed in terms of state s and 
observation o. 
1 This also applies to Friston���s recent monograph A free energy principle for a 
particular physics, which attempts a theory of every ���thing��� that can be distin- 
guished from other ���things��� in a statistical sense; see arxiv.org/ftp/arxiv/pa- 
pers/1906/1906.10184.pdf. 
2 Given s = soft and r = rotten, this would be notated as p(r|s) = p(s|r)��p(r) 
p(s) . 
3 More accurately, it can be applied to any variables with the required 
probabilistic properties. For simplicity, this article will always use ���state��� and 
���observation��� respectively, in naming the conditioned and conditional variables 
used in calculating a posterior probability. 
C. Thornton Neuropsychologia 173 (2022) 108281 
3 
relevant likelihoods. This is the process of marginalization. Its compu- 
tational complexity increases exponentially with the number of cases 
taken into account. The process is generally considered unacceptably 
costly for this reason (Bishop, 2007).4 Due to this, a way of proceeding 
that avoids the need to calculate the observation probability is generally 
considered desirable. 
1.3. Approximate Bayesian inference 
How can we calculate a posterior probability if we don���t know the 
observation probability on which it depends? For application of Bayes��� 
rule, it seems both the quantity to be divided, and the amount by which 
it is divided are required. It will be seen that the two quantities are 
closely related, however. Both specify the probability of the relevant 
observation. The denominator expresses it directly, while the numerator 
(i.e., prediction) expresses it as the product of two values: the proba- 
bility of seeing a particular state, and the probability of the state pro- 
ducing the observation. 
The way the value is constructed as a product has to be consistent 
with the way it is expressed as a single quantity, however. The latter 
cannot be greater than the former, for example, as a posterior greater 
than 1 would then result. There are also cases in which the two values 
are identical by def i nition. If the likelihood and state probability have 
the same extremal value (1 or 0), this must also be the observation 
probability. That these constraints arise is not surprising, as the obser- 
vation probability depends entirely on the predicted probabilities taken 
overall (i.e., on their marginalization). The former expresses all of the 
latter in a summative way. 
This close relationship between numerator and denominator sug- 
gests a solution to the problem at hand. A way of deriving posteriors can 
be envisaged, that gets around the unavailability of observation prob- 
abilities by treating the numerator as a constraint. A method of this kind 
cannot deliver precise posterior probabilities ��� it cannot reproduce 
exact Bayesian inference. It can be the means of producing a reasonable 
approximation, however. It offers a way of performing Bayesian infer- 
ence in an approximate way. The method is illustrated schematically in 
Fig. 2. 
1.4. Informational surprise 
Whether or not approximation is used, the effectiveness of Bayesian 
inference depends ultimately on the probabilities taken into account. 
They should lead to the correct state being identif i ed in every case, and 
the identif i cation should be made with minimum ambiguity. To achieve 
the latter, the distribution of probabilities over states should be as 
peaked as possible. To achieve the former, the peak should be in the 
right position. The worst case arises when the distribution is completely 
fl at. Then there is no peak. With every state having the same posterior 
probability, the inference obtained is completely uncertain. 
The fl atness of the posterior distribution is critical, then. For pur- 
poses of measuring this, the free-energy framework uses a metric from 
information theory (Shannon, 1948; Shannon and Weaver, 1949). Key 
to this is recognition that probability and uncertainty have an inverse 
relationship. More of one always implies less of the other. (To illustrate: 
where an outcome is given greater probability, there is correspondingly 
less uncertainty. If, on the other hand, the outcome is given less prob- 
ability, there must be correspondingly more uncertainty.) In practice, 
information theory def i nes the level of uncertainty associated with a 
particular probability as its negative logarithm. The level of uncertainty 
associated with probability p is def i ned as ��� log p. This produces the 
required inverse variation. 
A way of assessing the uncertainty inherent to a posterior distribu- 
tion can be obtained on this basis. As log values can be summed, a 
weighted average of the quantities arising provides what is required. 
There are some terminological pitfalls to be wary of, however. A nega- 
tive log probability is often dubbed a surprise (Tribus, 1961), on the 
grounds that it quantif i es the degree to which the relevant outcome 
would be surprising were it to occur. Intuitively, an outcome considered 
to have lower probability should produce a greater degree of surprise, 
and vice versa.5 Usage of this term is potentially misleading, though, as 
it implies the relevant outcome has occurred, whereas the probability 
itself does not. For this reason, the present approach will use the more 
neutral term surprisal in naming negative log-probability (cf. Weissbart 
et al., 2020). The associated misconception will also be named. Taking 
the derivation of a surprisal to imply the occurrence of an outcome will 
be termed the literal surprise fallacy. 
Using this information-theoretic def i nition, the uncertainty inherent 
to a distribution can be def i ned as a weighted average of the relevant 
surprisals6: 
��� 
p p �� (���log p) 
As a probabilistically weighted average this quantity can also be 
described as the expected surprisal. Shannon originally characterized it 
simply as the uncertainty (Shannon, 1949). When it was seen that the 
formula is mathematically equivalent to that used in physics to describe 
the disorder of a closed physical system, Shannon���s colleague,As a probabilistically weighted average this quantity can also be 
described as the expected surprisal. Shannon originally characterized it 
simply as the uncertainty (Shannon, 1949). When it was seen that the 
formula is mathematically equivalent to that used in physics to describe 
the disorder of a closed physical system, Shannon���s colleague, von 
Neumann, suggested that ���entropy��� ��� the term used by physicists ��� 
should be adopted.7 The result is that the term entropy is now used quite 
generally to refer to average surprisal (Rioul, 2018). 
Using this formulation, the objective applying to the posterior dis- 
tribution can be expressed precisely. Making the distribution as peaked 
as possible can be considered to involve minimizing its entropy. Putting 
it another way, maximizing the peakedness of the distribution is 
accomplished by minimizing its fl atness, which is also its entropy. The 
fi rst of the two over-arching objectives can be dealt with in this way ��� 
by requiring minimization of the posterior distribution���s entropy. 
Before moving on to consider the second objective, it is worth brief l y 
examining the wider information-theoretic applications of these for- 
mulations. In information theory, surprisal and entropy become the 
means of measuring quantities of information. In this case it is outcomes 
that do occur that are of interest. Where an outcome is awarded a 
probability implying a certain level of uncertainty, its occurrence has the 
effect of eliminating the uncertainty in question. Given this is 
Fig. 2. The prediction (predicted observation probability) viewed as a con- 
straining inf l uence on the observation probability. 
4 Buckley et al. note that the process involves ���evaluating diff i cult integrals��� 
(Buckley et al., 2017, p. 57). In their work with the Helmholtz machine, Dayan 
et al. observe, the problem is to ���compute expectations under the posterior 
distribution P which, in general, has exponentially many terms and cannot be 
factored into a product of simpler distributions��� (Dayan et al., 1995, p. 3). 
5 The effect can be illustrated using the avocado example. Say the chances of 
encountering a rotten avocado are extremely low (e.g., probability 0.01), the 
surprise produced by this outcome is then ��� log 0.01 ��� 4.6. Conversely, if the 
chances are high (probability 0.8, say), the surprise is then lower: ��� log 0.8 ��� 
0.2. 
6 The formula is normally simplif i ed to ��� ��� 
pp log p. 
7 According to legend, von Neumann advised Shannon that ��� ��� no one knows 
what entropy really is, so in a debate you will always have the advantage��� 
(Tribus and McIrvine, 1971). 
C. Thornton Neuropsychologia 173 (2022) 108281 
4 
informative by def i nition, the quantif i cation of uncertainty can be 
viewed as measuring the information produced by (or contained within) 
the outcome itself. As Shannon (1948) demonstrated, a general frame- 
work for quantifying information can be developed in this way. 
Central to it is the metrical use of logarithmic quantities. With logs 
taken to base n, a surprisal is expressed in well-def i ned units, each being 
the uncertainty arising for n equiprobable outcomes. Rounded down, the 
surprisal is then the number of digits in a base n encoding needed to 
represent the corresponding uncertainty ��� it is the number of alterna- 
tives that would create the uncertainty in question. Uncertainties 
become the basis for information-measurement in this way. Use of base 
2 leads to the familiar terminology of ���bits���. With the uncertainty of an 
outcome measured using logs taken to base 2, the outcome���s information 
content corresponds to a number of digits in a binary encoding. The 
information content of the outcome can then be characterized as a 
certain number of ���binary digits��� or ���bits���.
1 / 1
ChatGPT
ChatGPT
Bayesian inference is a fundamental concept in the free-energy framework. It involves calculating probabilities based on prior knowledge and observations. The article provides examples to explain the concept, such as determining the probability of a soft avocado being rotten based on the total number of soft avocados and the known percentage of rotten avocados. Bayesian inference can be applied to various scenarios, including medical diagnosis, by combining state/observation probabilities with conditional probabilities. The article introduces the notion of approximate Bayesian inference, which approximates posterior probabilities when the observation probability is unknown or costly to calculate. The uncertainty inherent in a posterior distribution is measured using information theory, specifically surprisal, which quantifies the degree of surprise associated with an outcome based on its probability. The article also highlights the connection between surprisal and entropy, with entropy representing the average surprisal and the measure of uncertainty in a distribution. The objective is to minimize the entropy or maximize the peakedness of the posterior distribution. Bayesian inference and information theory have broader applications in quantifying information and measuring uncertainties in various domains.
1 / 1
Your summary is very good and captures the key concepts explained in the article regarding Bayesian inference, free energy framework, Bayesian probability calculation, and information theory. Here are some additional notes:

1. Bayesian inference allows incorporating prior knowledge into probability calculations, in contrast to frequentist inference which only considers observed data. The prior distribution represents initial beliefs before considering new observations.

2. The posterior distribution obtained after combining the prior with new observations captures the updated beliefs after seeing the data. The posterior incorporates both prior knowledge and current observations.

3. The probabilistic formulation of Bayesian inference enables a quantitative approach to uncertainty. The variance or entropy of the posterior distribution reflects the degree of uncertainty in inferences.

4. Approximate Bayesian inference is needed when the true probabilistic model is intractable or expensive to compute exactly. Approximations retain the basic probabilistic formulation while simplifying calculations.

5. Information theory concepts like surprisal and entropy provide a quantitative measure of uncertainty in probabilistic inferences. Minimizing these measures leads to more peaked and certain posteriors.

So in summary, Bayesian inference is a probabilistic approach to reasoning and learning that incorporates prior knowledge, quantifies uncertainty, and allows inferring posteriors using information from both priors and observations. Approximations and information theory concepts help scale Bayesian inference to real-world problems.
1 / 1
ChatGPT
ChatGPT
Thank you for the additional notes! They provide a more comprehensive understanding of the concepts discussed in the article. Bayesian inference indeed allows for the incorporation of prior knowledge and the quantification of uncertainty, while approximate inference methods and information theory concepts play crucial roles in practical applications and measuring uncertainty in probabilistic inferences. Together, these concepts contribute to the scalability and usefulness of Bayesian inference in various real-world scenarios.
1 / 1
This form of processing is illustrated by Dayan et al.���s Helmholtz 
machine (Dayan et al., 1995; Dayan and Hinton, 1996). An unsupervised 
learning system implemented as a neural network, this aims to develop a 
generative model10 of presented patterns. With these treated as obser- 
vations, minimizing free energy has the effect of inducing states that best 
serve prediction. A predictive model of the data is then derived.11 
In practice, the Helmholtz machine is somewhat more complex in 
that it develops two models simultaneously: 
A recognition model is used to infer a probability distribution over the 
underlying causes from the sensory input, and a separate generative 
model, which is also learned, is used to train the recognition model. 
(Dayan et al., 1995, p. 1, emphasis added). 
Minimizing free energy in both models encourages their alignment, 
both with each other and with presented patterns. The generative model 
develops to produce patterns that the recognition model recognizes, 
while the recognition model develops to recognize the patterns that the 
generative model produces. The generative model can be thought of as 
���dreaming��� patterns, and the recognition model as ���sensing��� them; the 
two phases may be described as the ���sleep��� and ���wake��� phase accordingly 
(Dayan and Hinton, 1996).12 
2. Part 2 
2.1. The agent-level interpretation 
The method of free-energy minimization developed by Hinton, 
Dayan and colleagues was put forward as a computationally tractable 
way of solving a problem in statistical inference.13 Subsequent to the 
dissemination of this work,14 Friston and colleagues began to explore an 
agent-focused interpretation (Friston, 2003, 2005, 2009; Friston et al., 
2006, 2009, 2011; Friston and Stephan, 2007; Hohwy et al., 2008; 
Feldman and Friston, 2010; Friston et al., 2012a, 2012b; Buckley et al., 
2017; Allen and Friston, 2018). The possibility of treating the method as 
a model of perceptual inference was examined. Its potential to provide 
the basis of a broader account of cognitive agency was also pursued. 
The reconceptualization in question has several foundations. One 
involves the inferential process itself. Bayesian inference produces the 
most probable state for a given observation. By equating the observation 
with a pattern of sensory input, and the state with a perceptual inter- 
pretation, this statistical calculation can be conceived as a form of 
perceptual interpretation. A mathematical model of the perceptual act 
can be obtained in this way (van Rooij et al., 2018). As a perceptual 
interpretation and a pattern of sensory inputs are generally complex 
entities, treating each as a single variable simplif i es the situation to some 
degree, however. 
A second foundation involves use of surprisals. Reducing free-energy 
in the case of a single observation involves reducing the contribution 
made by the observation surprisal. If surprisal is taken to quantify sur- 
prise in a literal sense, an interpretation involving surprise avoidance can 
be framed. An agent that minimizes free-energy can be seen as ref i ning 
its perceptual capacities so as to avoid perceptual surprises. A still 
broader generalization can also be proposed. If perceptual behavior can 
be modelled as the attempt to avoid surprises, why not cognitive 
behavior more generally? Can we not say that all behaviors of an agent 
ref l ect avoidance of surprise in some sense? If so, minimization of free- 
energy is potentially repositioned as a general model of cognitive 
behavior. 
One objection is def i nitional in nature. It will be seen that free- 
energy is not def i ned purely in terms of surprisal. Relative to a single 
observation, it is def i ned as the sum of the observation surprisal and the 
KL divergence between the true and approximate posterior distributions 
arising (see Fig. 5). Equating free-energy with surprisal is generally 
based on the way the latter bounds the former, therefore. Free-energy is 
noted to provide an upper bound on surprisal.15 
This relationship exists by def i nition. As the sum of two quantities 
cannot be less than either, free energy cannot be less than the obser- 
vation surprisal. That free-energy ���bounds��� surprisal is a consequence of 
the latter being one of the constituents in terms of which the former is 
def i ned. 
Although the two quantities still have the potential to diverge, it is 
the identif i cation of free-energy with surprisal that largely sets the stage 
for viewing free-energy minimization as a governing principle in 
cognitive behavior. Given the assumption that surprisals quantify sur- 
prise in a literal sense (see above), free-energy minimization can be 
viewed as the means of ensuring survival through the avoidance of 
surprises. 
Free-energy theorists take surprise-avoidance to be key on this basis, 
with the result that ���minimization of expected free energy is referred to 
as minimizing surprise��� (James III, 2015, p. 220). The effect may be 
characterized in different ways, however. In (van Es, 2021, p. 316) it is 
Fig. 4. Free energy as introduced uncertainty (cf. Fig. 3). Fig. 5. Decomposition of free energy into surprisal and divergence. 
10 A generative model is one that includes the distribution of the modelled 
data, such that the probability of any example can be determined. 
11 Use of generative models in unsupervised learning has a long history. The k- 
means algorithm offers a simple example (MacQueen, 1967). This is a 
computationally eff i cient way of fi nding a clustering of datapoints. Processing 
is begun by randomly creating k cluster centres (called centroids), and assigning 
each datapoint to its closest centroid. The centroids ��� which form the gener- 
ative model in this context ��� are then adjusted to better represent the data- 
points captured. Next, the assignment of datapoints to centroids is adjusted 
using the new centroid positions. The process repeats until no further change is 
made. In the expectation-maximization (EM) algorithm of Dempster et al. 
(1977), the capture of datapoints is represented probabilistically. What is ob- 
tained is then a ���soft��� non-partitional clustering of the data (Neal and Hinton, 
1998). 
12 In Kirby���s view, ���Helmholtz machines are artif i cial neural networks that, 
through many cycles of sensing and dreaming, gradually learn to make their 
dreams converge to reality, and, in the process, create a succinct model of a 
fl uctuating world��� (Kirby, 2006, p. 2). 
13 The approach gave rise to ���variational Bayes��� approaches in machine 
learning (MacKay, 1995; Winn et al., 2005). 
14 Hinton went on to play a leading role in developing deep learning, a 
mainstay of modern machine learning (Bengio, 2009; LeCun et al., 2015). 
15 Relevant to this is the assumption that reduction of free energy cannot 
eliminate surprisal completely. Buckley et al. note that, while the mathematical 
reduction of free energy ��� ��� furnishes the organism with an approximation of 
surprisal it does not minimize it. Instead the organism can minimize VFE further 
by minimizing surprisal indirectly by acting on the environment and changing 
sensory input��� (Buckley et al., 2017, p. 59). 
C. Thornton Neuropsychologia 173 (2022) 108281 
6 
noted that ���by minimizing free energy, the organism will indirectly also 
minimize surprise in the long run.��� Buckley and Tschantz state that ���to 
ensure survival [agents] must simply avoid surprising situations��� 
(Buckley and Tschantz, 2019). Buckley et al. (2017), write that agents 
must ��� ��� minimize the occurrence of events which are atypical (���sur- 
prising���) in their habitable environment��� (Buckley et al., 2017, p. 56). 
Friston states that the agent���s ���model of the world (or the form of an 
agent) is optimum when it minimizes surprise��� (Friston, 2010a, p. 2), 
while Seth records that ���organisms ��� in virtue of their survival ��� must 
avoid surprising states��� (Seth, 2014, pp. 270���271). 
Identif i cation of survival with surprise-avoidance is not universally 
accepted, however. It is noted that natural agents often do not behave in 
surprise-averse ways (Fiorillo, 2010; Sun and Firestone, 2020). It is also 
argued that some forms of surprise-avoidance must be 
counter-productive for survival (Colombo and Wright, 2018). For 
example, an agent might eliminate the risk of being surprised by 
conf i ning itself to a part of the environment where sensory stimulation is 
blocked out. In the literature, this objection is termed the ���dark room 
problem���, on the grounds that a locale of this kind is akin to a dark room 
(Friston et al., 2012a, 2012b; Klein, 2018; Sun and Firestone, 2020; See 
also section 3.2.1.) 
Free-energy theorists support their position by introducing a second, 
closely related argument. This focuses on the effect that free-energy 
minimization has, not on individual surprisals, but on average sur- 
prisal. As noted, the formula def i ning this is identical to that def i ning 
physical entropy, which is considered a measure of physical disorder. 
This leads to the idea that free-energy minimization might be the means 
by which a physical agent maintains its physical order. On this basis, it 
can be proposed that agents minimize free-energy as a consequence of 
their existence, rather than as a means of maintaining it. Coming to grips 
with this argument requires a brief examination of the second law of 
thermodynamics. 
2.2. The second law of thermodynamics 
Imagine a bathtub is fi lled using the hot tap alone. Initially, the 
hottest water tends to congregate near the tap. Over time, the temper- 
ature evens-out, eventually becoming completely uniform. Physical 
systems that are orderly in structure and well separated from their 
environment are often found to behave in this convergent way. This 
gives rise to the second law of thermodynamics, which states that the 
entropy (homogeneity) of an isolated physical system will always evolve 
towards its maximum (Uff i nk, 2001). 
Since at least (Schroedinger, 1944), it has been noted that living 
things appear to violate this physical law (Gatlin, 1972). Their physical 
order seems to be maintained, rather than progressively lost (Boltz- 
mann, 1974). On this basis, it may be envisaged that a fundamental 
property of living things is that they resist the increase of physical dis- 
order implied by the second law. Friston and Stephan (2007, p. 421), for 
example, state that ���biological systems and especially neuronal systems 
appear to contravene the second law of thermodynamics���, a claim that is 
seen by Bruineberg et al. (2018) to be an information-theoretic exten- 
sion of (Schroedinger, 1944). 
Building on this idea, free-energy theorists develop a new justif i ca- 
tion for viewing free-energy minimization as the governing principle in 
cognitive behavior. Since the formula def i ning entropy is identical to 
that def i ning average surprisal, minimization of free-energy can be seen 
as having the effect of minimizing entropy which, on the argument 
above (and in the case of a living agent), can be seen as an entailment of 
existence. This suggests that living agents minimize free-energy 
necessarily. 
Again there are diff i culties. The argument stemming from (Schroe- 
dinger, 1944) is that living things resist increase of entropy, not that they 
pursue its decrease. The process that free-energy theorists have in mind 
involves minimization. Another objection involves the distinction be- 
tween sensory processing and physical constitution. The entropy 
quantif i ed in free-energy measurement stems from observational un- 
certainty, rather than physical disorder. The same formula is applied, 
but to different data. The distinction between physical and sensory en- 
tropy seems important nevertheless. Free-energy theorists deal with this 
by treating sensory processing as a transition through sensory states. On 
this basis, reduction of sensory entropy can be related to reduction
1 / 1
ChatGPT
ChatGPT
The first part of the provided text discusses the Helmholtz machine, which is an unsupervised learning system implemented as a neural network. Its goal is to develop a generative model of presented patterns by minimizing free energy. The Helmholtz machine consists of two models: a recognition model, which infers a probability distribution over the underlying causes from sensory input, and a generative model, which trains the recognition model. By minimizing free energy in both models, they align with each other and with presented patterns, resulting in the development of a predictive model.
In the second part of the text, the agent-level interpretation of free-energy minimization is explored. It is suggested that Bayesian inference and surprise-avoidance, which are inherent in free-energy minimization, can be seen as models of perceptual interpretation and cognitive behavior. The idea is that agents minimize free energy to refine their perceptual capacities and avoid surprises. This view is supported by the argument that free-energy minimization is related to the maintenance of physical order, in contrast to the increase of physical disorder predicted by the second law of thermodynamics. Living agents are seen as resisting the increase of physical disorder and, therefore, minimize free energy.
However, there are objections to this interpretation. It is argued that surprise-avoidance is not universally observed in natural agents, and some forms of surprise-avoidance might be counterproductive for survival. Additionally, the claim that living agents contravene the second law of thermodynamics is debated, as living systems may not necessarily pursue a decrease in entropy. Furthermore, there is a distinction between physical entropy and sensory entropy, with free-energy minimization primarily related to the reduction of sensory entropy.
Overall, the agent-level interpretation of free-energy minimization suggests that agents minimize free energy to refine their perceptual capacities, avoid surprises, and maintain order. However, there are ongoing debates and challenges regarding this interpretation, which require further exploration and investigation.
1 / 1
2.2. The second law of thermodynamics 
Imagine a bathtub is fi lled using the hot tap alone. Initially, the 
hottest water tends to congregate near the tap. Over time, the temper- 
ature evens-out, eventually becoming completely uniform. Physical 
systems that are orderly in structure and well separated from their 
environment are often found to behave in this convergent way. This 
gives rise to the second law of thermodynamics, which states that the 
entropy (homogeneity) of an isolated physical system will always evolve 
towards its maximum (Uff i nk, 2001). 
Since at least (Schroedinger, 1944), it has been noted that living 
things appear to violate this physical law (Gatlin, 1972). Their physical 
order seems to be maintained, rather than progressively lost (Boltz- 
mann, 1974). On this basis, it may be envisaged that a fundamental 
property of living things is that they resist the increase of physical dis- 
order implied by the second law. Friston and Stephan (2007, p. 421), for 
example, state that ���biological systems and especially neuronal systems 
appear to contravene the second law of thermodynamics���, a claim that is 
seen by Bruineberg et al. (2018) to be an information-theoretic exten- 
sion of (Schroedinger, 1944). 
Building on this idea, free-energy theorists develop a new justif i ca- 
tion for viewing free-energy minimization as the governing principle in 
cognitive behavior. Since the formula def i ning entropy is identical to 
that def i ning average surprisal, minimization of free-energy can be seen 
as having the effect of minimizing entropy which, on the argument 
above (and in the case of a living agent), can be seen as an entailment of 
existence. This suggests that living agents minimize free-energy 
necessarily. 
Again there are diff i culties. The argument stemming from (Schroe- 
dinger, 1944) is that living things resist increase of entropy, not that they 
pursue its decrease. The process that free-energy theorists have in mind 
involves minimization. Another objection involves the distinction be- 
tween sensory processing and physical constitution. The entropy 
quantif i ed in free-energy measurement stems from observational un- 
certainty, rather than physical disorder. The same formula is applied, 
but to different data. The distinction between physical and sensory en- 
tropy seems important nevertheless. Free-energy theorists deal with this 
by treating sensory processing as a transition through sensory states. On 
this basis, reduction of sensory entropy can be related to reduction of 
physical entropy. 
Underlying these diff i culties is a more conceptual concern. If the 
uncertainty of sensory processing is identif i ed with the disorder of 
physical constitution, there are implications for agents that experience 
increasing uncertainty. According to the proposal, an agent experi- 
encing a change of this kind must suffer increasing physical disorder. 
The prediction is that an increasingly uncertain agent will slowly 
disintegrate. Dubbed the disintegrating agent problem, this is further dis- 
cussed in Section 3.2.4.16 
Setting these diff i culties aside, the potential utility of the second law 
will be apparent. A new way of conceptualizing the process of free- 
energy minimization can be envisaged. Rather than seeing free-energy 
minimization as a general survival technique, it can be seen as an 
entailment of physical existence in the case of living things. Free-energy 
minimization can be viewed as fundamental in two different ways then. 
It can be supposed that this is how agents ��� in Friston���s words17 ��� 
���avoid surprises and last longer���. Alternatively, it can be argued that 
agents are effectively compelled to operate in this way by the ���very fact 
of their existence��� (Buckley et al., 2017, p. 57). 
2.3. Active inference 
The free-energy framework has now been constructed in rough 
outline. It is possible to move on to consider some of its more detailed 
aspects. The core of the proposal, recall, is that minimization of free- 
energy governs behavior of all kinds, in a process that can be viewed 
either as the secret to survival, or an entailment of physical existence. 
The model is seen to apply not only to internal (sensory/perceptual) 
behavior: avoidance of surprise is also considered to govern action. Free- 
energy theorists argue that action can be the means of avoiding surprises 
not only directly, but also indirectly, by changing the environment to 
make it less surprising. 
Avoiding surprises in the latter way ��� by modifying the environ- 
ment to satisfy expectations ��� is termed active inference within the 
framework (Friston et al., 2017a, 2017b). Buckley et al. distinguish this 
form of behavior as follows: 
��� while perception minimises VFE [free energy] by changing brain 
states to better predict sensory data, action instead acts on the 
environment to alter sensory input to better fi t sensory predictions. 
(Buckley et al., 2017, p. 65) 
This inverts the traditional model of inference. Whereas prediction is 
normally evaluated against existing state, here, new state is produced to 
conform to what is predicted. Rather than inference being a purely in- 
ternal activity, it becomes a potentially external activity, affecting states 
of the world. 
For purposes of integrating active inference into the free-energy 
framework, some new features have to be introduced. The key 
requirement is for an implementation mechanism. An arrangement is 
needed via which free-energy minimization can give rise to actions of 
16 Invoking thermodynamic laws in biological contexts is recognized to be 
troublesome. Morowitz (1981, p. 1) notes that ���[t]he use of thermodynamics in 
biology has a long history rich in confusion��� (Morowitz, 1981, p. 1). In his 
opinion, such attempts have generally not been fruitful: ���the audacious attempt 
to reveal the formal equivalence of the ideas of biological organization and 
thermodynamic order ��� must be judged to have failed��� (Medawar, 1984, p 
226). 
17 See Friston et al. (2012a, p. 2.). 
C. Thornton Neuropsychologia 173 (2022) 108281 
7 
the agent, which can then produce the requisite changes in the envi- 
ronment. It might seem that something akin to a full-f l edged agent/ 
environment simulation must be required, with time, actions and action 
sequences all independently coded, such that achievement of environ- 
mental results can be represented and computed in a precise way.18 To a 
considerable extent, however, changes of this complexity are avoided. 
Free-energy theorists envisage an arrangement in which the current 
action of the agent is considered part of environmental state. On this 
basis, an action of the agent can be represented in a simple way, as the 
making of a prediction. Action sequences can also take a convenient 
form: they can be seen to arise when one prediction induces an action 
that creates a state which encodes a further action, in an ongoing way. 
The complication is that minimization benef i ts may then be offset 
arbitrarily far into the simulated future. The objective becomes mini- 
mization of predicted future surprise (Friston et al., 2017a, 2017b), 
necessitating a redef i nition of the minimization criterion. Minimization 
must be def i ned, not in terms of free-energy itself, but in terms of ex- 
pected free-energy. Roughly, this is a weighted average of the 
free-energies (of observations) that are obtained when the agent pursues 
a particular sequence of actions. 
It might seem that modelling action in this way will involve differ- 
entiating agent from environment. It is key to the framework, however, 
that they are representationally indivisible. The framework assumes the 
existence of just two def i nitional constructs: the R-density and the G- 
density. The R-density is characterized as ���a probabilistic representation 
of what caused a particular sensation��� (Friston, 2010b, p. 128). The 
G-density, in contrast, is said to encode ���the brain���s beliefs about envi- 
ronmental causes��� of sensory input (Buckley et al., 2017, p. 61): 
��� the organism needs some implicit assumptions about how 
different environmental states shape sensory input. These assump- 
tions are presumed to be in the form of a ��� joint probability density 
between sensory data and environmental variables, the ���generative 
density���, or G-density. (Buckley et al., 2017, p. 57) 
The G-density is the def i nitive structure. The R-density simply ex- 
presses the posterior probabilities the G-density gives rise to.19 The G- 
density may be seen as encoding the agent���s ���beliefs��� about the envi- 
ronment (as above), or alternatively as its ���model��� of the environment (e. 
g. Seth, 2014, p. 271; Buckley et al., 2017, p. 57). 
The unexpected feature of this arrangement is the lack of any inde- 
pendent representation of the environment. The state of the external 
world is def i ned, in effect, by the agent���s beliefs. The world is as the 
agent believes it to be at all times. This fusion of agent and environment 
is not entailed by the concept of active inference, which is stated at a 
level of abstraction that makes no assumptions about def i ning data 
structures. In the free-energy framework, it cannot be avoided. Free- 
energy is a property of a single data structure, constituted of the likeli- 
hoods and priors expressing a posterior distribution. (A structure of this 
kind is termed a posterior mapping below.) If the agent���s behavior in an 
environment is considered to be governed by minimization of free- 
energy, it is inevitable that all aspects of the process must be repre- 
sented by this one structure. 
Fusion of agent and environment offers advantages, however. At a 
theoretical level, it can be seen to connect the framework to approaches 
that emphasize concepts of extension, embodiment and enaction 
(Varela et al., 1991; No�� e, 2004; Clark, 2013; Allen and Friston, 2018), 
although the validity of such connections remains controversial 
(Di Paolo et al., 2022; See also Hutto, 2018). At a more practical level, it 
offers a natural way to capture goal-directed behavior. 
With agent and environment fused in the envisaged way, a state of 
the world that has (or acquires) high surprisal will naturally elicit ac- 
tions that tend to reduce that surprisal. A state or feature of the envi- 
ronment that is unexpected will tend to elicit actions that render it less 
so. The result is seen to be goal-directed behavior. Buckley et al. char- 
acterize the effect in the following way: 
expectations in the organism���s G-density (its ���beliefs��� about the 
world) cannot be met directly by perception and thus an organism 
must act to satisfy them. In effect these expectations effectively 
encode the organism���s desires on environmental dynamics. (Buckley 
et al., 2017, p. 57, p. 57) 
Fusion of agent and environment leads to a simple account of goal- 
directed behavior, then. There are some conceptual drawbacks, how- 
ever. The arrangement has an element of circularity, in suggesting the 
agent takes action in accordance with its beliefs, while also changing its 
beliefs in accordance with its actions. The objective remains to improve 
predictability through reduction of free energy; but this is now under- 
stood to be achieved in a way that induces actions that the agent believes 
will eliminate the free energy originally reduced. Friston et al. (2012b), 
for example, observe that the process 
��� requires an agent to select or sample sensations that are predicted 
and believe that this sampling will minimize uncertainty about those 
predictions. (Friston et al., 2012b)
1 / 1
ChatGPT
ChatGPT
The second law of thermodynamics states that the entropy, or disorder, of an isolated physical system tends to increase over time. However, living things seem to maintain their physical order instead of progressively losing it, which contradicts the second law. This observation has led to the proposition that living agents resist the increase of physical disorder implied by the second law.
Free-energy theorists build on this idea and argue that free-energy minimization, which is the principle governing cognitive behavior, can be seen as a consequence of living agents resisting the increase of entropy. Since the formula defining entropy is identical to that defining average surprisal, minimizing free energy can be interpreted as minimizing entropy, which is an entailment of existence for living agents. Therefore, living agents are suggested to necessarily minimize free energy.
However, there are difficulties with this interpretation. The argument from Schr��dinger's work suggests that living things resist the increase of entropy rather than pursuing its decrease. The process of free-energy minimization involves minimization, which is different from the resistance to entropy increase. Additionally, there is a distinction between sensory entropy (stemming from observational uncertainty) and physical entropy (stemming from physical disorder), even though they are quantified using the same formula. Free-energy theorists address this distinction by treating sensory processing as a transition through sensory states, relating the reduction of sensory entropy to the reduction of physical entropy.
Despite these difficulties, the second law of thermodynamics offers potential utility in conceptualizing free-energy minimization. It provides a new perspective, suggesting that minimizing free energy is fundamental to living agents either as a means of avoiding surprises and increasing survival or as a consequence of their existence.
In the context of the free-energy framework, active inference is introduced as a form of behavior where actions are taken to modify the environment and make it less surprising. Active inference allows agents to avoid surprises not only directly but also indirectly by changing the environment to fit their sensory predictions. This inversion of the traditional model of inference implies that inference can be an external activity affecting states of the world, not just an internal process.
To integrate active inference into the framework, an implementation mechanism is needed. Rather than requiring a full-fledged agent/environment simulation, free-energy theorists propose an arrangement in which the agent's current action is considered part of the environmental state. Actions and action sequences can be represented as predictions, and the objective becomes the minimization of predicted future surprise. This requires redefining the minimization criterion in terms of expected free energy.
In this arrangement, the agent and environment are representationally indivisible. The framework assumes the existence of two constructs: the R-density (probabilistic representation of what caused a sensation) and the G-density (the brain's beliefs about environmental causes of sensory input). The G-density can be seen as encoding the agent's beliefs or model of the environment. Notably, there is no independent representation of the environment; its state is defined by the agent's beliefs.
Fusing the agent and environment offers theoretical connections to approaches emphasizing extension, embodiment, and enaction, and it provides a natural way to capture goal-directed behavior. When agent and environment are fused, states of the world with high surprisal elicit actions that reduce the surprisal, leading to goal-directed behavior. However, this arrangement has conceptual drawbacks, such as circularity in suggesting that the agent takes actions according to its beliefs while also changing its beliefs based on its actions. The objective remains to improve predictability through the reduction of free energy, but now actions are understood to induce changes that the agent believes will eliminate the originally reduced free energy.
1 / 1
of free-energy may engage different mechanisms at different timescales��� 
(Friston and Stephan, 2007, p. 436), and emphasize how this may pro- 
duce prior beliefs that override the imperative to avoid surprise in 
particular situations (Friston, 2013, p. 213). Friston (2013, p. 212���213), 
for example, argues that ���the state of a room being dark is surprising, 
because we do not expect to occupy dark rooms���. This point is made in 
more detail as follows: 
[the] resolution of the dark room problem is fairly simple: prior 
beliefs render dark rooms surprising. The existence of these beliefs is 
assured by natural selection, in the sense that agents that did not fi nd 
dark rooms surprising would stay there indef i nitely, until they die of 
dehydration or loneliness. (Friston et al., 2012b, p 1) 
There is an element of circularity in this, however (Thornton, 2010). 
It is argued that the dark-room problem is not a problem because agents 
drawn to such situations would not survive. Surprisal is claimed to 
benef i t survival because it benef i ts survival, but as Klein notes, ���we don���t 
explain how an organism stays alive by starting with the premise that it 
stays alive��� (Klein, 2018, p. 2544). There is also a sense in which the 
argument leads to a contradiction. If the proposal is that surprises are 
avoided in some scenarios but not others, surprise-avoidance cannot be 
the sole, governing principle. 
In Friston���s opinion (personal communication), the dark-room issue 
���was resolved many years ago.��� For present purposes, it seems it can be 
legitimately set aside. The reconstruction reveals that the problem is 
inconsequential for the framework, regardless of its potential relevance 
to the predictive-processing proposal (or the FEP). Given any connection 
between free-energy minimization ��� as construed within the frame- 
work ��� and avoidance of surprises is doubtful, the hypothesis targeted 
by the dark-room argument would appear not to be implied. 
3.3.1. The impact-characterization problem 
If what is accomplished by free-energy minimization cannot be 
related to surprise-avoidance, how should it be characterized? Within 
the reconstruction, free-energy is recognized to be statistical noise in the 
form of introduced uncertainty (see Section 1.5). A posterior mapping23 
provides the means of predicting states from observations. If the un- 
certainty of observations is found to be lower than that of the states they 
predict, this is inherently suboptimal. The uncertainty at the output end 
of the mapping should be no greater than at the input end. Minimization 
of free-energy serves to eliminate unwarranted uncertainty of this kind. 
While free-energy theorists may portray the effect slightly differ- 
ently, the imperative nature of the process is often emphasized. That 
free-energy should be minimized is put forward as a principle, with the 
effect linked to Bayesian inference, resistance to physical disintegration, 
or some combination of the two. Different fl avours of the FEP are then 
forthcoming. In the view of Colombo and Wright, for example, the ���free- 
energy principle states that all systems that minimize their free energy 
resist a tendency to physical disintegration��� (Colombo and Wright, 2018, 
p. 3463). Aguilera et al. (2021, p. 1) propose, on the other hand, that ���[t] 
he free energy principle (FEP) states that any dynamical system can be 
interpreted as performing Bayesian inference upon its surrounding 
environment.��� Adopting a more intermediate position, Friston (personal 
communication) takes the principle to be that ���all self organizing sys- 
tems can be read ��� or interpreted ��� as performing approximate 
Bayesian inference by optimizing a variational (free energy) bound on 
marginal likelihood.��� 
How elimination of introduced uncertainty relates to Bayesian 
inference has been examined in Sections 1.3���5. How it relates to resis- 
tance of physical disintegration has been examined in Sections 2.1���2. 
Both connections face diff i culties, as noted. The proposition that the 
process is a natural imperative is unambiguously conf i rmed, however. 
The introduction of uncertainty in a posterior mapping is always un- 
desirable, regardless of context. Minimizing such uncertainty is a 
context-free aspiration, applicable in all conceivable situations. If we 
interpret the free-energy principle in these terms, its universality cannot 
be questioned. 
This generality undoubtedly contributes to the framework���s 
remarkable adaptability. Its capacity to be applied in such a wide range 
of contexts ref l ects not only the casting off of contextual commitments 
��� as detailed above ��� but also the adoption of this mathematically 
unavoidable dictum as its central tenet. Less favourably, the context-free 
nature of the proposal may impugn the Bayesian interpretation of 
impact. While the process may produce an effect of a Bayesian kind, this 
differs from what is normally desired, namely, context-sensitive infer- 
ence directed towards an independently represented domain or data-set. 
Free-energy minimization produces Bayesian inference of a sort, but it is 
not the context-sensitive kind we generally seek. The explanatory value 
of the scheme may have limitations in result. 
4. Concluding comments 
An attempt has been made to reconstruct the free-energy framework 
in a way that allows an informed assessment to be derived, without 
undue reliance on mathematical notation. What the assessment should 
be is left open. The cautious approach is due in part to the variability of 
published assessments in the literature. Those originating from free- 
energy theorists tend to be positive, as we might expect. Buckley 
et al., for example, see the framework as the basis for a ���global unif i ed 
theory of cognition��� (Buckley et al., 2017, p. 56). Theorists interested in 
general neuroscientif i c models of cognition may also take a favourable 
view (e.g. Clark, 2016; Seth, 2021). Others may be more sceptical. In the 
view of Herreros and Verschure, for example, the free-energy framework 
��� seems to strive toward the super power of explaining everything. 
This, however, will make it transcend the obligation of each theory to 
be testable. (Herreros and Verschure, 2015, p. 218) 
Similar in spirit is the assessment of Colombo and Wright (2018). 
These authors cast the ambitious claims originating from the framework 
in a quite negative light: 
Although these dramatic ascriptions have attracted attention in 
philosophy and the life sciences, FEP���s epistemic status remains 
opaque, along with its exact role in biological and neuroscientif i c 
theorizing. Conspiring against its accessibility are the varying for- 
malisms and formulations of FEP, the changing scope of application, 
reliance on undef i ned terms and stipulative def i nitions, and the lack 
of clarity in the logical structure of the reasoning leading to FEP. 
(Colombo and Wright, 2018, p. 2) 
The framework is not uncontroversial, then. Ideally, the scientif i c 
worth of a proposal of this kind is established by evaluating its empirical 
predictions. Setting aside the issues discussed above, the key prediction 
emerging from the framework is that surprise-avoidance governs 
behavior. To what degree is this conf i rmed? According to some, what is 
observed in nature does not conf i rm this prediction in any degree. Sun 
and Fireston (2020, p. 346, emphasis added) note that ���[h]umans and 
other animals tend not to seek such experiences���, while Fiorillo (2010) 
records that 
��� animals tend to explore the least predictable sensory inputs while 
avoiding predictable inputs. (Fiorillo, 2010, p. 1, emphasis added) 
Such evidence might be seen as falsifying the framework���s central 
prediction. The potential for free-energy to be minimized at multiple 
23 The term refers to the priors and likelihoods expressing a posterior 
distribution. 
C. Thornton Neuropsychologia 173 (2022) 108281 
11 
timescales however (see Section 3.2.4), means that the prediction of 
surprise-avoidance has to be seen as qualif i ed. What seems to be pre- 
dicted, in actuality, is that agents will avoid surprises in some situations 
but not others. 
The prediction deriving from the thermodynamic argument also 
raises questions. This way of motivating free-energy minimization relies 
on the hypothesis that all systems that minimize their free-energy resist 
a tendency to physical disintegration (Colombo and Wright, 2018, p. 
3463). The diff i culty is that, given the way free-energy is linked to 
physical entropy, the posited relationship exists necessarily. Rather than 
expressing a valid prediction, the claim seems to reduce to a tautology. 
Friston does not object to this judgment. He writes (personal 
communication) ���I think this is absolutely right and should be empha- 
sized, this is the essence of the free energy principle.��� In the past, 
however, he has also emphasized the potential value of empirical 
evaluation. Simulations and working models illustrating free-energy 
minimization can be constructed (e.g. Bastos et al., 2012), and this 
can be a way of exploring explanatory potential. It can also serve to 
reveal practical problems, e.g., the potential for the process to be trap- 
ped in local minima.24 In the conclusion to the 2012 dark-room debate 
(Friston et al., 2012a, 2012b), Friston particular stressed the importance 
of systems-building work of this kind: 
This is the real challenge ahead ��� If, in a few years time, we do not 
have neuronally plausible accounts of all these [cognitive and 
adaptive] faculties that appeal to, and only to, free-energy minimi- 
zation, then I will be surprised and will search for a better model! 
(Friston et al., 2012a, p. 6) 
It is hoped that more demonstrations of this kind will be developed in 
the future, and that their assessment can inform further developments of 
the reconstruction set out above.
1 / 1
ChatGPT
ChatGPT
In the evaluation of the free-energy framework, assessments vary in the literature. Free-energy theorists tend to view it positively, seeing it as a basis for a "global unified theory of cognition." Some general neuroscientists also hold a favorable view. However, there are also skeptical voices that raise concerns about the framework's testability, varying formalisms and formulations, reliance on undefined terms, and lack of clarity in its logical structure.
The scientific worth of the framework should ideally be established by evaluating its empirical predictions. One key prediction is that behavior is governed by surprise-avoidance. However, evidence from studies suggests that humans and animals tend to avoid predictable inputs while exploring the least predictable sensory inputs, which challenges this prediction. The framework's prediction of surprise-avoidance needs to be qualified, as it seems to suggest that agents will avoid surprises in some situations but not others.
Another prediction derived from the thermodynamic argument is that systems minimizing free-energy resist physical disintegration. However, the connection between free-energy and physical entropy is such that the relationship exists necessarily, making the prediction tautological rather than a valid prediction.
Although Friston acknowledges the tautological nature of the prediction, he emphasizes the potential value of empirical evaluation and the construction of simulations and working models to explore the explanatory potential of free-energy minimization. Such evaluations can reveal practical problems and help refine the framework. It is hoped that further demonstrations and assessments of the framework will inform its development and shed light on its validity.
1 / 1
The key takeaways from this detailed analysis and reconstruction of the free-energy framework are:

1. Free-energy minimization can be understood as eliminating unwarranted uncertainty in a mapping between posterior predictions and observations. This makes it a context-free imperative, applicable in all situations.

2. The connections between free-energy minimization and Bayesian inference, or resistance to physical disintegration, face difficulties. The essential process is eliminating introduced uncertainty, regardless of context. 

3. The framework's proponents make ambitious claims, but its epistemic status remains opaque due to varying formulations, undefined terms, and lack of clear reasoning.

4. Empirical evidence does not clearly confirm the prediction of surprise-avoidance. In fact, animals tend to explore surprising inputs, not avoid them. 

5. The thermodynamic argument linking free-energy minimization to physical stability is tautological, not a valid prediction.

6. Building explanatory models and simulations to demonstrate free-energy minimization could help evaluate and refine the framework. But more work of this kind is needed.

In summary, while the free-energy framework exhibits flexibility and generalizability due to its context-free basis, doubts remain around its empirical support, testability, and explanatory power. The key process of eliminating statistical uncertainty provides a mathematically unavoidable imperative, but its impact and implications require further clarification. More thorough models and simulations could help address current limitations and inform a nuanced assessment of the framework's value.
1 / 1
ChatGPT
ChatGPT
That is an accurate summary of the key takeaways from the analysis and reconstruction of the free-energy framework. It highlights the central concepts, challenges, and unresolved issues surrounding the framework. The context-free imperative of free-energy minimization, its connections to Bayesian inference and physical stability, and the lack of empirical confirmation of surprise-avoidance are important points to consider. The need for further empirical evaluation, clear formulations, and explanatory models is emphasized to address the framework's limitations and assess its validity more comprehensively.
1 / 1
A list of the topics we've discussed.
1 / 1
ChatGPT
ChatGPT
Certainly! Here is a list of the topics we have discussed:
Introduction to the free-energy framework
The concept of free energy and its relation to Bayesian inference
The application of the free-energy framework to cognitive behavior
The role of surprise-avoidance in the framework
The thermodynamic argument and its implications for physical stability
Difficulties and objections regarding the framework's connections and predictions
The need for empirical evaluation and working models
The opaque epistemic status and challenges of the framework
The context-free nature of free-energy minimization as a universal imperative
Empirical evidence and the lack of confirmation for surprise-avoidance
The tautological nature of the thermodynamic argument
The potential of simulations and models to refine and evaluate the framework
I hope this list helps summarize our discussions effectively!
1 / 1
More about the role of suprise avoidance
1 / 1
ChatGPT
ChatGPT
Certainly! Let's delve further into the role of surprise avoidance in the free-energy framework.
The free-energy framework proposes that organisms have a fundamental drive to minimize surprise or uncertainty. Surprise, in this context, refers to the discrepancy between an organism's predictions (based on its internal models or beliefs) and the sensory information it receives from the environment. Minimizing surprise is seen as a way for organisms to maintain internal coherence and adaptive behavior.
The framework suggests that organisms continuously generate predictions about their sensory inputs based on their internal models. When the sensory information matches these predictions, there is minimal surprise. However, if the sensory information deviates significantly from the predictions, it leads to increased surprise.
To reduce surprise, organisms engage in active inference. Active inference involves two complementary processes: perception and action. Perceptual processes aim to minimize surprise by adjusting internal models or beliefs to better match the sensory inputs. Action processes, on the other hand, involve modifying the environment to bring it more in line with the organism's predictions, thereby reducing surprise indirectly.
By minimizing surprise, organisms aim to maintain internal coherence and increase their understanding of the environment. This process allows them to make accurate predictions and take appropriate actions to ensure their survival and well-being. The avoidance of surprise can be seen as a fundamental principle governing cognitive behavior, shaping perception, decision-making, and action selection.
However, it's important to note that the role of surprise avoidance in the free-energy framework has faced some criticisms and challenges. Empirical evidence suggests that organisms do not always avoid surprises but, in certain cases, actively seek out surprising or novel experiences. This contradicts the strict prediction of surprise avoidance as a governing principle. Additionally, the circularity of the argument, where surprise avoidance is assumed to be beneficial because it is beneficial, raises questions about its explanatory power.
Overall, surprise avoidance plays a central role in the free-energy framework, driving organisms to minimize the discrepancy between their predictions and sensory inputs. However, the extent to which surprise avoidance accurately characterizes all aspects of cognitive behavior and its relationship to other cognitive processes requires further investigation and empirical evaluation.
1 / 1
It is possible that in certain situations we expect surprise, and are surprised if there is none
1 / 1
ChatGPT
ChatGPT
Yes, you're absolutely right. In certain situations, we may indeed have expectations of surprise, and when those expectations are not met, we may experience surprise. This highlights the complex nature of surprise and its relationship to our cognitive processes.
Surprise is not solely about the absence or presence of unexpected events. It also depends on our internal expectations, beliefs, and context. We form predictions and generate expectations based on our past experiences, knowledge, and understanding of the world. These predictions can encompass a range of possibilities, including expectations of surprise or novelty.
If we enter a situation or engage in an activity where we anticipate surprises or unexpected events, the absence of those surprises may, in itself, be surprising. Our expectations play a crucial role in shaping our experience of surprise. If the actual sensory information matches our predictions, including the prediction of surprise, we may still perceive it as surprising if our expectations were set accordingly.
This highlights the importance of considering the dynamic interplay between predictions, expectations, and surprise in cognitive processes. Surprise is not solely determined by the objective properties of the environment but also influenced by our internal states and subjective interpretations.
In the free-energy framework, surprise avoidance is often discussed as minimizing the discrepancy between predictions and sensory inputs. However, it is essential to recognize that surprise can arise in various ways, including both the presence and absence of expected surprises. The interplay between expectations, surprise, and cognitive processes is an active area of research, aiming to unravel the complex mechanisms underlying our cognitive experiences.
1 / 1
How was this described in perceptual control theory?
1 / 1
ChatGPT
ChatGPT
Perceptual Control Theory (PCT) offers a different perspective on the role of surprise and its relationship to cognitive processes compared to the free-energy framework. PCT focuses on how individuals actively control their perceptions to achieve desired goals or states.
According to PCT, individuals engage in a continuous process of comparing their current perceptions with a reference or desired state. They strive to minimize the discrepancy between their actual perceptions and the desired state by making adjustments to their actions or the environment. This control process is known as negative feedback control.
In the context of surprise, PCT suggests that surprise occurs when there is a significant discrepancy between our expected perceptions and the actual perceptions. Surprise indicates that our control system did not anticipate or adequately adjust for the observed outcome.
However, in PCT, surprise is not seen as something inherently negative or to be avoided. Instead, surprise serves as a valuable signal that alerts the individual to the need for adjustments in their control process. It prompts the individual to re-evaluate their expectations, actions, or the environment to bring their perceptions closer to the desired state.
PCT emphasizes the active role of individuals in regulating their perceptions and maintaining control. The occurrence of surprise prompts individuals to engage in adaptive behavior to address the unexpected outcome and reduce the discrepancy. In this way, surprise serves as an important feedback signal that guides the individual's ongoing control processes.
Overall, in PCT, surprise is viewed as a dynamic aspect of the control process, highlighting the need for continuous monitoring and adjustment of perceptions to achieve desired states. It plays a crucial role in prompting adaptive behavior and promoting effective control in response to unexpected events or outcomes.
1 / 1
Surprisal = - log(probability)

This means that:

- Less probable outcomes have higher surprisal (more surprising)
- More probable outcomes have lower surprisal (less surprising)

Surprisal provides a quantitative measure of how surprising an outcome is, based on its probability. Some key points:

1. Highly likely outcomes have low surprisal, close to 0. This makes sense intuitively - very probable events are not surprising.

2. Outcomes with a probability close to 0 have very high surprisal, tending towards infinity. Extremely improbable events are very surprising.

3. Surprisal is measured in units of bits. This relates to information theory, where surprisal quantifies the amount of information gained from an outcome. More surprising outcomes provide more information.

4. The expected surprisal, or average surprisal across all possible outcomes, is equivalent to the entropy of the probability distribution. Entropy quantifies the uncertainty or "spread" of a distribution.

5. Bayesian inference aims to maximize the probability of observations, minimizing their surprisal. This leads to the most compact and certain inferences.

So in summary, surprisal provides a fundamental link between probability, surprise, information and uncertainty. Outcomes with low probability - and thus high surprisal - tend to be the most surprising, informative and uncertain.
1 / 1
ChatGPT
ChatGPT
Yes, you've provided an accurate description of surprisal and its relationship to probability, surprise, information, and uncertainty. Surprisal, as measured by the negative logarithm of probability, captures the notion of how unexpected or surprising an outcome is based on its likelihood.
Your key points effectively summarize the important characteristics of surprisal:
Highly likely outcomes have low surprisal, reflecting their lack of surprise. When an event is highly probable, its occurrence is expected, and thus it carries little surprise.
Outcomes with a probability close to zero have extremely high surprisal. As the probability approaches zero, the surprisal increases towards infinity, indicating that extremely improbable events are highly surprising.
Surprisal is measured in bits, which is a unit from information theory. It represents the amount of information gained or conveyed by an outcome. More surprising events provide more information because they deviate from what is expected.
The expected surprisal, or average surprisal across all possible outcomes, is equivalent to the entropy of the probability distribution. Entropy quantifies the uncertainty or variability of the distribution. Higher entropy corresponds to a higher degree of uncertainty and surprise.
Bayesian inference seeks to minimize surprisal by maximizing the probability of observations. By updating beliefs based on new evidence, Bayesian inference aims to reduce uncertainty and minimize surprise, leading to more accurate and certain inferences.
Overall, surprisal serves as a quantitative measure that connects probability, surprise, information, and uncertainty. It provides a foundation for understanding the relationship between these concepts and plays a crucial role in various fields, including information theory, cognitive science, and probabilistic modeling.
1 / 1
Connections to the previous topics.
1 / 1
ChatGPT
ChatGPT
The concept of surprisal and its relationship to probability and surprise have connections to the previous topics we discussed, particularly the free-energy framework and the second law of thermodynamics. Here's how they relate:
Free-Energy Minimization: In the free-energy framework, one of the proposed principles is that living agents minimize free-energy, which can be viewed as minimizing surprisal or the divergence between their predictions and observations. By minimizing surprisal, agents aim to reduce the discrepancy between their internal models of the world and the sensory data they receive. This connection highlights the role of surprisal in shaping behavior and cognitive processes.
Bayesian Inference: The connection between surprisal and Bayesian inference is significant. Bayesian inference involves updating beliefs based on new evidence by maximizing the probability of observations. This process aligns with the idea of minimizing surprisal. When observations are consistent with prior expectations, surprisal is low, and beliefs are confirmed. Conversely, when observations deviate from expectations, surprisal is high, and beliefs are updated to account for the surprising information. Thus, surprisal serves as a measure of the discrepancy between prior expectations and observed evidence, guiding the updating of beliefs through Bayesian inference.
Second Law of Thermodynamics: The second law of thermodynamics describes the tendency of isolated physical systems to evolve towards maximum entropy or homogeneity. Living systems, however, appear to resist the increase of entropy and maintain physical order. The proposition that living agents minimize surprisal, or equivalently minimize free-energy, can be seen as a violation or contravention of the second law. By actively minimizing surprisal, living agents resist the natural tendency towards increasing disorder and maintain their internal organization.
These connections highlight how the concept of surprisal plays a role in understanding cognitive processes, adaptive behavior, and the interplay between living systems and their environment. It provides a framework for explaining how organisms minimize surprisal, update their beliefs, and maintain order in the face of uncertainty and changing environmental conditions.
1 / 1
Give an informal Reconstruction of Free-Energy Framework
1 / 1
ChatGPT
ChatGPT
The free-energy framework is a theoretical framework that aims to explain how biological systems, including the brain and cognition, function and adapt to their environment. It is based on the concept of free-energy, which can be understood as a measure of uncertainty or surprise.
Informally, the free-energy framework can be reconstructed as follows:
Bayesian Inference: At its core, the framework relies on the principles of Bayesian inference. Bayesian inference is a process by which beliefs are updated based on new evidence. In the free-energy framework, this process is applied to perception and cognition, where the brain constantly makes predictions about the world and updates those predictions based on incoming sensory information.
Prediction Errors: In the free-energy framework, the brain's predictions are compared with the actual sensory inputs it receives. The mismatch between predictions and actual sensory inputs is referred to as prediction errors. Prediction errors signal a discrepancy between what was expected and what was actually observed.
Minimization of Free-Energy: The key idea in the free-energy framework is that biological systems, including the brain, strive to minimize free-energy. Free-energy is a measure of the discrepancy between the brain's predictions and the sensory inputs it receives. By minimizing free-energy, the system aims to reduce the surprise or uncertainty associated with prediction errors.
Active Inference: In order to minimize free-energy, the system engages in what is called active inference. Active inference involves both perception and action. The system not only updates its internal predictions based on sensory inputs but also takes actions to actively sample and gather new sensory information that can help reduce prediction errors.
Hierarchical Organization: The free-energy framework suggests that the brain is hierarchically organized. At higher levels of the hierarchy, more abstract predictions about the world are made, while at lower levels, more detailed sensory information is processed. The hierarchical structure allows for efficient processing and prediction generation.
Learning and Adaptation: The free-energy framework also accounts for learning and adaptation. The system learns from previous prediction errors and updates its internal models or beliefs to improve future predictions. This learning process helps the system become more attuned to the statistical regularities and patterns in the environment.
Overall, the free-energy framework provides a theoretical account of how biological systems, such as the brain, perceive, learn, and adapt to their environment. By minimizing free-energy through active inference, the system aims to reduce surprise and uncertainty, making predictions that are in line with the observed sensory inputs. This framework has implications for understanding cognition, behavior, and the interactions between organisms and their environment.
1 / 1
Your reconstruction of the free-energy framework is very good and captures the key ideas accurately. Here are some additional notes:

1. Prediction errors are crucial in the free-energy framework. They signal a discrepancy between the brain's predictions and actual sensory inputs, which needs to be resolved. 

2. Minimizing free-energy (i.e. uncertainty or surprise) is the driving force in the framework. It motivates the system to resolve prediction errors and make better predictions.

3. Active inference, in which the system both updates its predictions and takes actions to gather information, is how the system minimizes free-energy. It involves both perception and action.  

4. The hierarchical organization of the brain allows for efficient generation of predictions - from abstract high-level predictions to more specific low-level predictions.

5. Learning and adaptation through updating of internal models helps the brain become better attuned to the environment's regularities, ultimately improving its predictive ability.

6. The framework views the brain fundamentally as a "prediction machine" - one that tries to minimize the discrepancy between its predictions and sensory inputs.

7. The appeal of the framework lies in its ability to provide a unifying account of diverse cognitive phenomena in terms of probabilistic inference and minimization of free-energy/surprise.

So in essence, perception, cognition, learning and behavior in biological systems are seen in the free-energy framework as resulting from the drive to minimize free-energy or resolve uncertainty by making better and better predictions via Bayesian inference and active inference. The hierarchical, adaptive and predictive nature of the brain is emphasized.
1 / 1
ChatGPT
ChatGPT
Thank you for the additional notes! They provide valuable insights into the free-energy framework. The points you've mentioned further highlight the significance of prediction errors, the active role of the system in minimizing free-energy, and the predictive nature of the brain. The emphasis on the hierarchical organization and the learning process adds depth to our understanding of how the framework accounts for cognitive phenomena. Overall, these additional notes contribute to a more comprehensive and nuanced description of the free-energy framework.
1 / 1
Give some more examples
1 / 1
ChatGPT
ChatGPT
Certainly! Here are a few more examples to illustrate how the free-energy framework can be applied to different domains:
Perception: When perceiving the visual world, the brain constructs predictions about what it expects to see based on prior knowledge and sensory input. If the actual sensory information matches the predictions, the prediction errors are minimized, resulting in a more accurate perception. For example, when we see a familiar object, our brain quickly generates predictions about its shape, color, and texture, and any deviations from these predictions would generate prediction errors.
Action Selection: The free-energy framework also applies to action selection. When faced with multiple possible actions, the brain evaluates the expected free-energy associated with each action based on its predictions. Actions that are predicted to lead to outcomes with lower surprise (i.e., lower free-energy) are more likely to be selected. For instance, when crossing a busy street, we predict the trajectories of approaching vehicles and choose actions that minimize the surprise (e.g., avoiding collisions) based on these predictions.
Learning and Skill Acquisition: Learning can be seen as a process of updating internal models to better align with the regularities in the environment. By minimizing free-energy, the brain seeks to reduce prediction errors and improve the accuracy of its predictions. This is evident in skill acquisition, where initially novel and challenging tasks become more automatic and effortless with practice. Through repeated experience, the brain refines its predictions and minimizes surprise, leading to improved performance.
Emotion and Affective States: The free-energy framework can also be applied to emotions and affective states. Emotions can be seen as the brain's response to prediction errors that are particularly salient or significant. For example, a surprising event that deviates greatly from our predictions may elicit a strong emotional response. Emotion regulation involves minimizing free-energy by updating our predictions and reducing the impact of surprising events on our affective states.
These examples demonstrate how the free-energy framework can be applied across various domains, from perception and action selection to learning and emotion. By framing these processes in terms of minimizing free-energy and resolving prediction errors, the framework offers a unified perspective on how biological systems interact with their environment and maintain adaptive behavior.
1 / 1
In thermodynamics, the thermodynamic free energy is one of the state functions of a thermodynamic system (the others being internal energy, enthalpy, entropy, etc.). The change in the free energy is the maximum amount of work that the system can perform in a process at constant temperature, and its sign indicates whether the process is thermodynamically favorable or forbidden. Since free energy usually contains potential energy, it is not absolute but depends on the choice of a zero point. Therefore, only relative free energy values, or changes in free energy, are physically meaningful.
1 / 1
ChatGPT
ChatGPT
That's correct! In the field of thermodynamics, the thermodynamic free energy, often referred to as just "free energy," is a fundamental concept that helps describe the behavior of thermodynamic systems. It is a state function, meaning it depends only on the current state of the system and not on the path taken to reach that state.
The free energy of a system is associated with its capacity to do useful work. Specifically, the change in free energy during a process at constant temperature and pressure represents the maximum work that the system can perform or the maximum amount of energy that can be extracted from the system. If the change in free energy is negative, the process is thermodynamically favorable and can occur spontaneously. Conversely, if the change in free energy is positive, the process is thermodynamically forbidden or non-spontaneous without external energy input.
It's important to note that free energy is a relative quantity and depends on the choice of a reference point or zero energy state. This is because the free energy includes both the internal energy and the potential energy of the system. Therefore, only changes in free energy, rather than absolute values, have physical significance. By comparing the free energy of a system in different states or under different conditions, we can determine the thermodynamic favorability of processes and make predictions about their behavior.
The concept of free energy has broad applications in various branches of science and engineering, including chemistry, physics, biology, and materials science. It provides valuable insights into the energetics and stability of systems, helping us understand and manipulate their behavior.
1 / 1
HYPOTHESIS AND THEORY article
Front. Psychol., 08 May 2012
Sec. Perception Science
Volume 3 - 2012 | https://doi.org/10.3389/fpsyg.2012.00130
This article is part of the Research Topic
Predictive coding

View all 11 Articles 
Free-energy minimization and the dark-room problem
Karl Friston1, Christopher Thornton2 and Andy Clark3*
1 The Wellcome Trust Centre for Neuroimaging, University College London, London, UK
2 Informatics, University of Sussex, Brighton, UK
3 School of Philosophy, Psychology, and Language Sciences, University of Edinburgh, Edinburgh, UK
Recent years have seen the emergence of an important new fundamental theory of brain function. This theory brings information-theoretic, Bayesian, neuroscientific, and machine learning approaches into a single framework whose overarching principle is the minimization of surprise (or, equivalently, the maximization of expectation). The most comprehensive such treatment is the ���free-energy minimization��� formulation due to Karl Friston (see e.g., Friston and Stephan, 2007; Friston, 2010a,b ��� see also Fiorillo, 2010; Thornton, 2010). A recurrent puzzle raised by critics of these models is that biological systems do not seem to avoid surprises. We do not simply seek a dark, unchanging chamber, and stay there. This is the ���Dark-Room Problem.��� Here, we describe the problem and further unpack the issues to which it speaks. Using the same format as the prolog of Eddington���s Space, Time, and Gravitation (Eddington, 1920) we present our discussion as a conversation between: an information theorist (Thornton), a physicist (Friston), and a philosopher (Clark).

Philosopher: The ���free-energy principle��� (see e.g., Friston and Stephan, 2007; Friston, 2010a) suggests that all biological systems are driven to minimize an information-theoretic (not thermodynamic, though the two are mathematically close) quantity known as ���free energy.��� Free energy, as here defined, bounds surprise, conceived as the difference between an organism���s predictions about its sensory inputs (embodied in its models of the world) and the sensations it actually encounters. In this discussion, surprise is used explicitly as a measure of improbability from information theory. This is also known as surprisal or self information. Although the psychological notion of surprise is distinct, events with high surprisal are generally surprising. Organisms that succeed, the free-energy principle mandates, do so by minimizing their tendency to enter into this special kind of surprising (that is, non-anticipated) state. But at first sight this principle seems bizarre. Animals do not simply find a dark corner and stay there. Play and exploration are core features of many life-forms. Can the free-energy principle really be justified in information-theoretic terms?

Theorist: It seems right to begin with a few general remarks about Information Theory. Remarkably, it is now more than six decades since Claude Shannon set out this framework, with its beautifully simple, core idea of equating generation of information with reduction of uncertainty (i.e., ���surprise���). Showing that surprise could be quantified in terms of the range of choice applying, Shannon gave the framework mathematical teeth, enabling the extraordinary diversity of applications that continue to this day.

Part of the excitement about Information Theory has long related to its potential for explaining natural processes and phenomena. One proposition has been that we can explain adaptive behavior by saying it is a mechanism by which agents ���reduce their surprise��� about the environment they inhabit. This idea appeals in a number of ways. Not only does it promise to provide adaptive behavior with a mathematical foundation, it also generalizes straightforwardly to the case of learning and cognition. The acquisition of any form of knowledge can be viewed in the same way, as an attempt to reduce surprise. Last but not least, there is the intriguing link between informational uncertainty and physical disorder. Mathematically, they are identical. Cutting a long story short, it then becomes possible to envisage a deep, underlying unity connecting generative processes of adaptation, mind, and life.

A proposal of this general form has recently been made in the ���free energy��� framework, particularly associated with the work of the Physicist in the debate (Friston). To the underlying ���surprise-reduction��� hypothesis, this adds a rich assembly of mathematics, with the aim of explaining many architectural and developmental aspects of brain function. Controversy continues, however, regarding the value of the underlying hypothesis. The proposal that adaptive behavior can be interpreted in broadly informational terms seems beyond dispute. In brief, we can certainly view the process by which agents adapt to their environments as a process by which they reduce their surprise. The problem is we can also view it the other way around, seeing the situation in terms of agents reducing their surprise by adapting to the environment.

Furthermore, the hypothesis seems to have an element missing. Surprise is calculated with reference to the agent���s interpretation or ���model��� of the world. It is the way probabilities are assigned to features of the world that fixes the level of surprise in any given case. Let us say I turn up to watch a football match and discover that the players are all running around the edge of the pitch for some reason. I might respond adaptively, by re-interpreting the situation as an athletics meeting rather than a football match. But this only functions to reduce surprise if it is calculated from probabilities arising from my original interpretation. Surprise must be defined relative to the right interpretation (i.e., model of probability) in order to get the adaptive effect. Invoking the surprise-reduction hypothesis as an account of how this interpretation comes into existence then seems to produce a circular explanation. Some crucial element must be missing.

Initially, it seems we can escape the circularity by taking the position that agents do not form any substantive interpretation. We envisage them attending to uninterpreted sensory data, instead. On this basis, the probabilities that mediate measurement of surprise can be taken to be the frequencies with which different data are acquired. This strategy does not really work, however. If this really is the basis on which agents operate, reduction of surprise dictates blocking out sensory data altogether. Alternatively but equally absurdly, agents should proceed directly to the least stimulating environment and stay there. That is to say, they should take up position in the nearest ���dark room��� and never move again. This will always be the best way to reduce surprise for an agent that operates in the absence of an adaptively appropriate interpretation. This, then, is the so-called ���dark-room problem��� that provides the title and focus for the debate.

Appealing as it seems at the outset, then, the surprise-reduction hypothesis leads into a serious tangle. If we allow unlimited rein over the interpretations agents are assumed to apply, the dark-room problem can be eliminated. But the hypothesis then seems to be stating something that is true by definition. If we go the other way, ruling out substantive interpretation, the hypothesis becomes contentful but dictates that agents will tend to behave very stupidly indeed. There seems to be something wrong. All three of us are convinced, however, that there is also something importantly right in the general idea.

I would like to start, then, by putting to the Physicist the central question about explanatory content. Specifically, should we view the surprise-reduction hypothesis (of adaptive behavior) as having content independently? Or should we take it to be part of some larger explanatory package, in which other elements resolve the problems noted. If it is part of some larger package, what other principles might be involved other than reduction of surprise?

Physicist: From the point of view of the free-energy formulation there is no need to recourse to any other principles. Of course, one might find that one���s favorite principle emerges from a particular application of the free-energy principle; however, the whole point of the free-energy principle is to unify all adaptive autopoietic and self-organizing behavior under one simple imperative; avoid surprises and you will last longer.

It might be useful to contextualize the free-energy principle in relation to other principles here. From an information theory or statistical perspective, free-energy minimization lies at the heart of variational Bayesian procedures (Hinton and van Camp, 1993) and has been proposed as a modus operandi for the brain (Dayan et al., 1995) ��� a modus operandi that appeals to Helmholtz���s unconscious inference (Helmholtz, 1866/1962). This leads naturally to the notion of perception as hypothesis testing (Gregory, 1968) and the Bayesian brain (Yuille and Kersten, 2006). Indeed, some specific neurobiological proposals for the computational anatomy of the brain are based on this formulation of perception (Mumford, 1992). Perhaps the most popular incarnation of these schemes is predictive coding (Rao and Ballard, 1999). The free-energy principle simply gathers these ideas together and summarizes their imperative in terms of minimizing free energy (or surprise). However, the free-energy principle brings something else to the table ��� it says that action should also minimize free energy. With this simple addition, we are now in a position to consider behavior and self organization; however, the same basic principle remains ��� namely, minimizing free energy or surprise.

Having said this, I think you are right to invoke the notion of a ���larger package,��� in that ���surprise��� is minimized over multiple scales. For example, the fact that you can reinterpret football as athletics rests upon having an internal model of both football and athletics. The acquisition of these concepts depends upon free-energy minimization during learning. The fact you survive long enough to learn rests on free-energy minimization at an evolutionary scale; and so on.

Avoiding surprises means that one has to model and anticipate a changing and itinerant world. This implies that the models used to quantify surprise must themselves embody itinerant wandering through sensory states (because they have been selected by exposure to an inconstant world): Under the free-energy principle, the agent will become an optimal (if approximate) model of its environment. This is because, mathematically, surprise is also the negative log-evidence for the model entailed by the agent. This means minimizing surprise maximizes the evidence for the agent (model). Put simply, the agent becomes a model of the environment in which it is immersed. This is exactly consistent with the Good Regulator theorem of Conant and Ashby (1970). This theorem, which is central to cybernetics, states that ���every Good Regulator of a system must be a model of that system.��� This means a Dark-Room agent can only exist if there are embodied agents that can survive indefinitely in dark rooms (e.g., caves). In short, Dark-Room agents can only exist if they can exist. The tautology here is deliberate, it appeals to exactly the same tautology in natural selection (Why am I here? ��� because I have adaptive fitness: Why do I have adaptive fitness? ��� because I am here). Like adaptive fitness, the free-energy formulation is not a mechanism or magic recipe for life; it is just a characterization of biological systems that exist. In fact, adaptive fitness and (negative) free energy are considered by some to be the same thing.

The particular minimum free-energy solutions associated with that existence will be unique to each conspecific and its econiche. Interestingly, Dark-Room agents do exist: Troglophiles have evolved to model and navigate environments like caves (Barr and Holsinger, 1985). So why do they exist? Surprise is a function of sensations and the agent (model) itself. This means that the surprise can be reduced by changing sensory input (action), predictions of that input (perception), or the model per se; through evolution to minimize free energy or maximize free-fitness (Sella and Hirsh, 2005). Evolutionary or neurodevelopmental optimization of a model is distinct from perception and entails changing the form and architecture of an agent. In this sense, every agent represents a viable solution to the free-energy minimization problem that is supported by the real world
1 / 1
ChatGPT
ChatGPT
The article discusses the application of the free-energy principle in the context of the dark-room problem. The free-energy principle posits that all biological systems strive to minimize surprise or free energy, which is an information-theoretic quantity that bounds the difference between an organism's predictions and actual sensory inputs. The surprise-reduction hypothesis suggests that adaptive behavior can be understood as a process of reducing surprise or improbability.
However, the dark-room problem challenges the surprise-reduction hypothesis. It questions why organisms do not simply seek out unchanging and predictable environments if their goal is to minimize surprise. The article explores the tension between the need to reduce surprise and the presence of exploratory behavior in organisms.
The physicist in the conversation argues that the free-energy principle, which encompasses the minimization of free energy and surprise, is sufficient as a guiding principle for adaptive behavior. By minimizing surprise, organisms can adapt to a changing and uncertain world. The principle applies at different scales, from perception and learning to evolutionary adaptation.
The philosopher raises concerns about the circularity of the surprise-reduction hypothesis and the necessity of an appropriate interpretation or model of the world to calculate surprise. The absence of an adaptive interpretation leads to absurd behaviors, such as remaining in a dark room indefinitely. The conversation highlights the need for a larger explanatory package to address these issues.
The physicist argues that surprise reduction operates at multiple scales and involves modeling and anticipating a changing world. Organisms become optimal models of their environment through minimizing surprise, which aligns with the Good Regulator theorem. The existence of "dark-room agents" that can navigate specific environments like caves supports the idea that surprise reduction is tied to adaptive fitness.
In conclusion, the article presents a discussion on the free-energy principle, surprise reduction, and the dark-room problem. It explores the challenges and implications of the surprise-reduction hypothesis in understanding adaptive behavior and the role of modeling and perception in minimizing surprise.
1 / 1
Connections to this: Here: Biological Systems as Evolved, Overloaded, Multi-Scale Machines
by Joshua Bongard 1,���,���ORCID andMichael Levin 2,*,���,���ORCID
1
Department of Computer Science, University of Vermont, Burlington, VT 05405, USA
2
Allen Discovery Center at Tufts University, 200 Boston Ave., Suite 4600, Medford, MA 02155, USA
*
Author to whom correspondence should be addressed.
���
These authors contributed equally to this work.
���
Institute for Computer Designed Organisms.
Biomimetics 2023, 8(1), 110; https://doi.org/10.3390/biomimetics8010110
Received: 20 December 2022 / Revised: 24 February 2023 / Accepted: 1 March 2023 / Published: 8 March 2023
(This article belongs to the Special Issue Editorial Board Members��� Collection Series: Bioinspired Sensorics, Information Processing and Control)
Download Browse Figures Versions Notes
Abstract
The applicability of computational models to the biological world is an active topic of debate. We argue that a useful path forward results from abandoning hard boundaries between categories and adopting an observer-dependent, pragmatic view. Such a view dissolves the contingent dichotomies driven by human cognitive biases (e.g., a tendency to oversimplify) and prior technological limitations in favor of a more continuous view, necessitated by the study of evolution, developmental biology, and intelligent machines. Form and function are tightly entwined in nature, and in some cases, in robotics as well. Thus, efforts to re-shape living systems for biomedical or bioengineering purposes require prediction and control of their function at multiple scales. This is challenging for many reasons, one of which is that living systems perform multiple functions in the same place at the same time. We refer to this as ���polycomputing������the ability of the same substrate to simultaneously compute different things, and make those computational results available to different observers. This ability is an important way in which living things are a kind of computer, but not the familiar, linear, deterministic kind; rather, living things are computers in the broad sense of their computational materials, as reported in the rapidly growing physical computing literature. We argue that an observer-centered framework for the computations performed by evolved and designed systems will improve the understanding of mesoscale events, as it has already done at quantum and relativistic scales. To develop our understanding of how life performs polycomputing, and how it can be convinced to alter one or more of those functions, we can first create technologies that polycompute and learn how to alter their functions. Here, we review examples of biological and technological polycomputing, and develop the idea that the overloading of different functions on the same hardware is an important design principle that helps to understand and build both evolved and designed systems. Learning to hack existing polycomputing substrates, as well as to evolve and design new ones, will have massive impacts on regenerative medicine, robotics, and computer engineering.
1 / 1
ChatGPT
ChatGPT
The article "Biological Systems as Evolved, Overloaded, Multi-Scale Machines" by Bongard and Levin explores the idea that biological systems, including living organisms and intelligent machines, can be understood as polycomputing systems. Polycomputing refers to the ability of the same substrate to simultaneously perform multiple functions and provide computational results to different observers. The authors argue for an observer-dependent, pragmatic view that dissolves rigid boundaries between categories and embraces a more continuous understanding of form and function.
The connection to the discussion on the free-energy framework and surprise reduction lies in the exploration of how living systems function at multiple scales and perform multiple functions simultaneously. The idea of polycomputing aligns with the notion of minimizing surprise or free energy by modeling and anticipating a changing world. Biological systems are seen as computational materials, capable of adapting and evolving their functions in response to environmental demands.
The concept of overloading different functions on the same hardware is highlighted as an important design principle in both evolved and designed systems. This aligns with the adaptive nature of biological systems and the need to maximize their computational efficiency and versatility. By understanding and learning from existing polycomputing substrates, as well as designing new ones, the field of regenerative medicine, robotics, and computer engineering can benefit greatly.
In summary, the article contributes to the understanding of biological systems as polycomputing machines, capable of performing multiple functions simultaneously. It emphasizes the importance of overloading different functions on the same hardware and explores the potential impact on various fields of science and technology. This perspective aligns with the idea of adaptive behavior, surprise reduction, and the free-energy framework in understanding the computational nature of biological systems.
1 / 1
A list of all of the topics we've discussed
1 / 1
ChatGPT
ChatGPT
Certainly! Here is a list of the topics we have discussed:
Free-energy minimization and the dark-room problem
Surprisal and its connection to probability and information theory
Perceptual Control Theory and surprise avoidance
The role of surprise in the free-energy framework
The reconstruction of the free-energy framework
The relationship between free energy and thermodynamics
The thermodynamic free energy and its significance in thermodynamics
The informal reconstruction of the free-energy framework
The concept of prediction errors in the free-energy framework
Active inference and its role in minimizing free energy
The hierarchical organization of the brain in making predictions
Learning and adaptation in the free-energy framework
The brain as a prediction machine in the free-energy framework
Theoretical criticisms and controversies surrounding the free-energy framework
Empirical evidence and the prediction of surprise-avoidance
The connection between free-energy minimization and Bayesian inference
The relationship between free-energy minimization and resistance to physical disintegration
The potential limitations and challenges of the free-energy framework
The article on free-energy minimization and the dark-room problem by Friston, Thornton, and Clark
The concept of surprise-reduction and its relation to adaptive behavior
The role of surprise in adaptive behavior and information theory
The circularity and tautological nature of surprise-reduction hypothesis
The philosophical, physicist, and information theorist perspectives on surprise-reduction
The broader principles and applications of the free-energy principle
The connection between surprise reduction and modeling of the environment
The concept of polycomputing and its relevance to biological and technological systems
The overloading of different functions on the same hardware as an important design principle in evolved and designed systems
The potential impacts of understanding polycomputing on regenerative medicine, robotics, and computer engineering.
These are the main topics we have covered throughout our discussion. Let me know if there's anything specific you would like to delve into further!
1 / 1
Connections between The overloading of different functions on the same hardware as an important design principle in evolved and designed systems and Deleuze's idea of multiple overlapping times.
1 / 1
ChatGPT
ChatGPT
The concept of overloading different functions on the same hardware as an important design principle in evolved and designed systems can be connected to Deleuze's idea of multiple overlapping times in the following ways:
Multiplicity: Deleuze emphasizes the idea of multiplicity, where multiple processes or events coexist and intersect in complex ways. Overloading different functions on the same hardware can be seen as a manifestation of multiplicity, where a single entity or system can perform multiple functions simultaneously.
Non-linear time: Deleuze's notion of multiple overlapping times challenges the linear, chronological understanding of time. Similarly, overloading different functions on the same hardware implies a non-linear and non-sequential organization of functions. Instead of strict temporal order, these functions can interact and intertwine in a dynamic and concurrent manner.
Rhizomatic structure: Deleuze uses the concept of rhizome to describe a non-hierarchical, interconnected network of relations. Overloading functions on the same hardware reflects a rhizomatic structure, where functions are interwoven and interconnected without a fixed hierarchical order. This allows for flexibility, adaptability, and the emergence of novel behaviors or patterns.
Complexity and emergence: Both Deleuze's idea of multiple overlapping times and the principle of overloading functions on the same hardware acknowledge the complexity of systems. In complex systems, interactions and relationships between functions or processes give rise to emergent properties and behaviors that cannot be reduced to the individual components. The overlapping of functions and times can lead to new and unpredictable emergent phenomena.
In summary, the overloading of different functions on the same hardware aligns with Deleuze's idea of multiple overlapping times by emphasizing multiplicity, non-linear time, rhizomatic structure, and the emergence of complexity. Both concepts challenge traditional linear and hierarchical notions, allowing for a more dynamic and interconnected understanding of systems and their behaviors.
1 / 1
A final summary
1 / 1
ChatGPT
ChatGPT
Throughout our discussion, we have explored several topics related to the free-energy framework, surprisal, and the overloading of functions on the same hardware. Here is a final summary of the key points:
Free-energy minimization: The free-energy framework proposes that biological systems are driven to minimize surprise or uncertainty by making predictions and updating their internal models based on sensory inputs. This principle applies to perception, cognition, learning, and behavior.
Surprisal: Surprisal is a measure of how surprising an outcome is based on its probability. Less probable outcomes have higher surprisal, while more probable outcomes have lower surprisal. Surprisal quantifies the amount of information gained from an outcome and is connected to information theory.
The dark-room problem: Critics of the free-energy framework raise the dark-room problem, which questions why biological systems engage in exploratory behaviors and do not simply avoid surprises by staying in unchanging environments. This highlights the tension between surprise-avoidance and the drive for exploration and adaptation.
Overloading of functions: The idea of overloading different functions on the same hardware refers to the ability of a system to perform multiple functions simultaneously. This design principle is observed in evolved and designed systems, where different functions coexist and interact within the same system or substrate.
Connections to Deleuze's multiple overlapping times: Deleuze's concept of multiple overlapping times shares similarities with the overloading of functions. Both emphasize multiplicity, non-linear time, rhizomatic structure, and the emergence of complexity. They challenge linear, hierarchical notions and provide a more dynamic and interconnected understanding of systems.
In conclusion, the free-energy framework and surprisal offer insights into how biological systems minimize surprise and adapt to their environments. The overloading of functions on the same hardware and Deleuze's concept of multiple overlapping times provide alternative perspectives on complexity, non-linear time, and the interconnectedness of functions within systems. These ideas contribute to our understanding of cognition, behavior, and the design principles of evolved and designed systems.
1 / 1
ChatGPT may produce inaccurate information about people, places, or facts.
ChatGPT May 24 Version
ChatGPT



