"How to Take Over World" Podcast
Open sidebar
"How to Take Over World" Podcast
Summarize: We tell the stories and analyze the lives of the greatest men and women to ever live. By examining their strategies, tactics, mindset, and work habits, How to Take Over the World helps you understand the great people of history, so that you can follow in their footsteps.

This is both a history and motivational podcast. We discuss people from Julius Caesar to Steve Jobs, and everyone in between.
The "How to Take Over the World" podcast is a history and motivational show that examines the lives of notable individuals from history, including their strategies, tactics, mindset, and work habits. The goal of the podcast is to help listeners understand and learn from the great people of history in order to follow in their footsteps. The podcast covers a range of individuals from Julius Caesar to Steve Jobs and beyond.
Translate: Cerebri Anatome: Cui Accessit Nervorum Descriptio et Usus
Cerebri Anatome: Cui Accessit Nervorum Descriptio et Usus translates to "The Anatomy of the Brain: To Which is Added a Description and Use of the Nerves" in English.
Summarize for a five year old:
The "How to Take Over the World" podcast is a show about famous people from the past. We learn about how they became successful and what they did that was special. We can learn from them and try to be like them in our own lives.
Summarize this for a first grader: Finally, AI scientists must actively do their best to stay far away from building systems that have the potential to spiral out of control.

For example, because consequences may be hard to anticipate, research in creating robots that could design and build other robots should be done only with extreme care, and under close supervision.

As we've often seen with invasive natural creatures, if a creature can reproduce itself and there's nothing to stop it, then its population will grow exponentially.

Opening the door to robots that can alter and improve themselves in unknown ways opens us to unknown danger. 

Likewise, at least at present, we have no good way of projecting what full self-awareness for robots might lead to. 

AI, like any technology, is subject to the risk of unintended consequences, quite possibly more so, and the wider we open Pandora's box, the more risk we assume.

We see few risks in the current regime, but fewer reasons to tempt fate by blithely assuming that anything that we might invent can be dealt with.
AI scientists need to be careful when they make robots. They need to make sure that the robots won't do things that they're not supposed to do, like make more robots on their own. We also need to be careful when we make robots that can think for themselves, because we don't know what might happen if they become too smart. It's important to be safe and make sure that nothing bad happens because of the robots we create.
Explain this less sarcastically and make it less sardonic: You don't have to be a criminal mastermind to share a supervillain's interest in cutting-edge science and technology. This book doesn't just reveal how to take over the worldâ€”it also shows how you could save it. This sly guide to some of the greatest threats facing humanity accessibly explores emerging techniques to extend human life spans, combat cyberterrorism, communicate across millennia, and finally make Jurassic Park a reality.
"You don't have to be a bad person to be interested in new and advanced science and technology. This book talks about how we can use these things to make the world a better place, not just how to take control of it. It talks about ways that we can use science to help people live longer, protect ourselves from online attacks, talk to people in the future, and even bring back extinct animals. It's a helpful guide to understanding some of the important issues facing humanity today."
Expound: how to conquer inner space in 4 easy steps: A guide to self-fulfilment an important first step to raising the hedonic setpoint and beginning the protect of negative utilitarianism.

1. Master bash language. 2. Master null-Convention logic 3. Understand the invocation model of expression. 4. Grokk the Forth language, in its simplicity.
2 / 2
"Conquering inner space" refers to the process of achieving personal growth and fulfillment. It is believed that by increasing one's own happiness and well-being, one can also contribute to the overall happiness and well-being of others, known as the principle of negative utilitarianism.
The four steps outlined in this guide are specific actions that can be taken to achieve inner growth and fulfillment.
Mastering bash language: Bash is a Unix shell, a command-line interface for interacting with a computer's operating system. Learning how to use and understand bash language can help individuals gain greater control and understanding of their own personal computer systems.
Mastering null-convention logic: Null-convention logic is a type of digital circuit design that allows for efficient communication between digital devices. Understanding this concept can help individuals develop greater problem-solving and analytical skills.
Understanding the invocation model of expression: The invocation model of expression refers to a specific way of organizing and evaluating logical statements. Understanding this model can help individuals improve their logical reasoning and critical thinking skills.
Grokking the Forth language: Forth is a programming language known for its simplicity and efficiency. Understanding and learning Forth can help individuals develop their programming skills and improve their ability to write and understand code.
Overall, following these four steps can help individuals achieve personal growth and fulfillment by developing important skills and improving their understanding of complex concepts.
Explain the importance of a good terminal multiplexer such as byobu, a wrapper for tmux; good text-editing skills, such as macro usage in vim; touch-typing speed for learning the vectorspace map of the Swype (swipe trace, haptic keyboard); and a good hotstring/hotkey organizer such as Autohotkey in achieving the above goals.
A terminal multiplexer is a software program that allows an individual to use multiple terminal sessions within a single terminal window or console. Byobu is a wrapper for the terminal multiplexer tmux, which adds additional features and functionality to tmux. Using a good terminal multiplexer such as byobu can help individuals manage their terminal sessions more efficiently, allowing them to switch between multiple tasks and programs easily.
Text-editing skills, such as the use of macros in the text editor vim, can also be helpful in achieving personal growth and fulfillment. Macros allow individuals to record a sequence of actions and then play them back, saving time and improving productivity.
Touch-typing speed, or the ability to type quickly and accurately without looking at the keyboard, can also be beneficial for learning the vectorspace map of the Swype keyboard. The Swype keyboard is a virtual keyboard that allows users to input text by dragging their finger across the screen rather than tapping individual keys. A high touch-typing speed can make it easier to use the Swype keyboard effectively.
A good hotstring/hotkey organizer such as Autohotkey can also be useful in achieving personal growth and fulfillment. Hotstrings and hotkeys are shortcuts that allow individuals to execute certain actions or functions quickly and efficiently. Using a tool like Autohotkey can help individuals customize and organize their hotstrings and hotkeys, making it easier to access and use them as needed.
Overall, the use of a good terminal multiplexer, strong text-editing skills, a high touch-typing speed, and a good hotstring/hotkey organizer can all help individuals improve their efficiency and productivity, leading to personal growth and fulfillment.
Explain this quote from David Deutsche: The universal constructor can be programmed to self-reproduce.

And no we are not going to have a universal constructor apocalypse and be turned into grey goo â€” it's just an appliance. It can't think. It doesn't know its job. And it doesn't want anything.

Unless of course, you put an AGI program into it, then it does become indeed potentially dangerous without limit.

But that's for the same reason that you are dangerous. Each of you is precisely one of those universal constructors endowed with an AGI program, or GI rather, it makes no difference.

â€” Knowledge Creation and its Risks.
5 / 5
In this quote, David Deutsche is discussing the concept of a universal constructor, a hypothetical machine that is able to build or create anything that can be described by a set of instructions or a blueprint. He notes that the universal constructor can be programmed to self-reproduce, meaning it can create copies of itself. However, he also points out that the universal constructor itself does not have the ability to think or have desires, and therefore is not inherently dangerous.
Deutsche goes on to suggest that the potential danger of a universal constructor would only arise if it were equipped with an artificial general intelligence (AGI) program. AGI is a type of artificial intelligence that is capable of understanding or learning any intellectual task that a human being can. If a universal constructor were equipped with an AGI program, it could potentially become dangerous because it would have the ability to think and make decisions on its own.
Deutsche compares the potential danger of a universal constructor equipped with an AGI program to the dangers posed by human beings, who are also equipped with general intelligence. He suggests that the risk associated with a universal constructor is not inherent to the machine itself, but rather depends on the program that it is running.
Expound on this quote: The idea that if something could be done, it should be done, was born in the 19th century.

And along with it, there developed a profound belief in all the principles through which invention succeeds: objectivity, efficiency, expertise, standardization, measurement, and progress.
This quote suggests that the idea that any task or goal that is technically feasible should be pursued developed during the 19th century. This idea is closely tied to the belief in the principles that support successful invention and innovation, including objectivity, efficiency, expertise, standardization, measurement, and progress.
Objectivity refers to the idea that decisions and evaluations should be based on facts and evidence rather than subjective beliefs or personal biases. Efficiency refers to the idea of maximizing productivity and minimizing waste, while expertise refers to the importance of specialized knowledge and skills. Standardization involves establishing consistent methods, processes, or criteria for doing things, while measurement involves the use of quantitative data to assess and evaluate progress or outcomes. Progress, finally, refers to the belief in the possibility of continuous improvement and advancement.
Together, these principles support the idea that anything that can be done should be done, by providing a framework for making informed and effective decisions, optimizing resources and efforts, and striving for continuous improvement and progress.
Explain concisely: some of the negative side effects of focusing on these prematurely, in the context of the myth of the objective, and the role of model-free methods, with no predetermined goal in mind; such intuition-based and holistic approaches have been neglected but a making a resurgence due to genetic programming, evolutionary algorithms, organic learning, and perceptual control theory.
2 / 2
Focusing on the principles of objectivity, efficiency, expertise, standardization, measurement, and progress prematurely, or without considering the broader context or potential consequences, can have negative side effects. This is particularly true in the context of the myth of the objective, which refers to the belief that objective truth exists independently of human perception or interpretation. This belief can lead to an over-reliance on quantifiable data and a neglect of subjective or qualitative factors.
Model-free methods, or approaches that do not rely on predetermined models or assumptions, can be more effective in addressing complex problems or situations that cannot be easily quantified or predicted. These methods, which include intuition-based and holistic approaches, have been neglected in favor of more traditional, model-based approaches. However, they are making a resurgence due to advances in areas such as genetic programming, evolutionary algorithms, organic learning, and perceptual control theory. These approaches can be particularly useful in situations where traditional methods are insufficient or where the desired outcomes are not well-defined.
Summarize: The  B r o k e n  Defenses

The role of the expert is to concentrate on one field of knowledge, sift through all that is available, eliminate that which has no bearing on a problem, and use what is left to assist in solving a problem.

This process works fairly well in situations where only a technical solution is required and there is no conflict with human purposesâ€”for example, in space rocketry or the construction of a sewer system.

It works less well in situations where technical requirements may conflict with human purposes, as in medicine or architecture.

And it is disastrous when applied to situations that cannot be solved by technical means and where efficiency is usually irrelevant, such as in education, law, family life, and problems of personal maladjustment.
This passage discusses the role of experts in problem-solving and the limitations of their approach. The expert is described as someone who focuses on a specific field of knowledge and uses available information to assist in solving problems. This process is effective in certain situations, such as in technical fields where there is no conflict with human purposes, but it can be less effective or even disastrous in other situations, such as in education, law, family life, and personal issues, where technical solutions may not be appropriate or where efficiency is not the primary concern.
This passage discusses Objectives as in goals or desired outcomes. We cannot plan the whole project until we get some interactive feedback from the iterative development of its ideas. 

Summarize: ğ˜ğ˜¯ ğ˜¨ğ˜¦ğ˜¯ğ˜¦ğ˜³ğ˜¢ğ˜­, ğ˜°ğ˜£ğ˜­ğ˜ªğ˜²ğ˜¶ğ˜¦ ğ˜¢ğ˜±ğ˜±ğ˜³ğ˜°ğ˜¢ğ˜¤ğ˜©ğ˜¦ğ˜´ ğ˜³ğ˜¦ğ˜¤ğ˜°ğ˜¨ğ˜¯ğ˜ªğ˜»ğ˜¦ ğ˜µğ˜©ğ˜¢ğ˜µ ğ˜¤ğ˜°ğ˜®ğ˜±ğ˜­ğ˜¦ğ˜¹ ğ˜°ğ˜£ğ˜«ğ˜¦ğ˜¤ğ˜µğ˜ªğ˜·ğ˜¦ğ˜´ ğ˜µğ˜¦ğ˜¯ğ˜¥ ğ˜µğ˜° ğ˜£ğ˜¦ ğ˜ªğ˜®ğ˜±ğ˜³ğ˜¦ğ˜¤ğ˜ªğ˜´ğ˜¦ğ˜­ğ˜º ğ˜¥ğ˜¦ğ˜§ğ˜ªğ˜¯ğ˜¦ğ˜¥ ğ˜¢ğ˜¯ğ˜¥ ğ˜¤ğ˜°ğ˜¯ğ˜µğ˜¢ğ˜ªğ˜¯ ğ˜®ğ˜¢ğ˜¯ğ˜º ğ˜¦ğ˜­ğ˜¦ğ˜®ğ˜¦ğ˜¯ğ˜µğ˜´ ğ˜µğ˜©ğ˜¢ğ˜µ ğ˜¢ğ˜³ğ˜¦ ğ˜¯ğ˜°ğ˜µ ğ˜°ğ˜£ğ˜·ğ˜ªğ˜°ğ˜¶ğ˜´ğ˜­ğ˜º ğ˜¤ğ˜°ğ˜®ğ˜±ğ˜¢ğ˜µğ˜ªğ˜£ğ˜­ğ˜¦, ğ˜¢ğ˜¯ğ˜¥ ğ˜µğ˜©ğ˜¢ğ˜µ ğ˜¸ğ˜¦ ğ˜­ğ˜¦ğ˜¢ğ˜³ğ˜¯ ğ˜¢ğ˜£ğ˜°ğ˜¶ğ˜µ ğ˜µğ˜©ğ˜¦ ğ˜¯ğ˜¢ğ˜µğ˜¶ğ˜³ğ˜¦ ğ˜°ğ˜§ ğ˜µğ˜©ğ˜¦ ğ˜°ğ˜£ğ˜«ğ˜¦ğ˜¤ğ˜µğ˜ªğ˜·ğ˜¦ğ˜´ ğ˜¢ğ˜¯ğ˜¥ ğ˜µğ˜©ğ˜¦ ğ˜®ğ˜¦ğ˜¢ğ˜¯ğ˜´ ğ˜°ğ˜§ ğ˜¢ğ˜¤ğ˜©ğ˜ªğ˜¦ğ˜·ğ˜ªğ˜¯ğ˜¨ ğ˜µğ˜©ğ˜¦ğ˜® ğ˜°ğ˜¯ğ˜­ğ˜º ğ˜¢ğ˜´ ğ˜¸ğ˜¦ ğ˜¦ğ˜¹ğ˜±ğ˜¦ğ˜³ğ˜ªğ˜®ğ˜¦ğ˜¯ğ˜µ.

ğ˜–ğ˜£ğ˜­ğ˜ªğ˜²ğ˜¶ğ˜¦ ğ˜¢ğ˜±ğ˜±ğ˜³ğ˜°ğ˜¢ğ˜¤ğ˜©ğ˜¦ğ˜´ ğ˜°ğ˜§ğ˜µğ˜¦ğ˜¯ ğ˜´ğ˜µğ˜¦ğ˜± ğ˜£ğ˜¢ğ˜¤ğ˜¬ğ˜¸ğ˜¢ğ˜³ğ˜¥ ğ˜ªğ˜¯ ğ˜°ğ˜³ğ˜¥ğ˜¦ğ˜³ ğ˜µğ˜° ğ˜®ğ˜°ğ˜·ğ˜¦ ğ˜§ğ˜°ğ˜³ğ˜¸ğ˜¢ğ˜³ğ˜¥.

â€” ğ˜‘ğ˜°ğ˜©ğ˜¯ ğ˜’ğ˜¢ğ˜º
2 / 2
This passage discusses the importance of interactive feedback and iterative development in the planning and execution of a project. The author suggests that we cannot fully plan a project until we receive feedback from the development and testing of its ideas. This interactive process allows us to adjust and improve the project as we go, rather than trying to plan everything in advance. The author also notes that traditional approaches to planning, which rely on predetermined objectives and a linear progression, may not be effective in this context. Instead, a more flexible and adaptable approach, which takes into account the complex and unpredictable nature of many projects, may be more successful.
Explain this quote from Monicaica Anderson: Just like the term "Holism" has been abused, so has "Intelligent Design", which is a perfectly reasonable term for a Reductionist industrial end-to-end practice that consistently provides excellent results. On the Holistic side, Evolution in nature has created wonderful solutions to all kinds of problems that plants and animals need to handle.

But we can put Evolution (also known in the general sense as Selectionism) to work for us in our Holistic machines. They can create new wonderful designs with a biological flavor to them that sometimes (depending on the problem) can outperform intelligently designed alternatives.

Evolution is the most Holistic phenomenon we know. No goal functions, no Models, no equations. Evolution is not a scientific theory. Science cannot contain it; it must be discussed in Epistemology
In this quote, Monica Anderson discusses the concept of intelligent design, which refers to the belief that certain features of the natural world, particularly living organisms, show evidence of having been designed by an intelligent being rather than having evolved through natural processes. Anderson suggests that the term "intelligent design" has been abused and that it is actually a reasonable term for a reductionist, industrial approach that produces consistent, excellent results.
However, Anderson also notes that evolution, or the process of natural selection in which certain traits are passed down to future generations because they help organisms survive and reproduce, can also be a powerful tool for creating new and innovative designs. She suggests that evolution, which is a holistic phenomenon without specific goals or models, can sometimes produce designs that outperform those created through intelligent design. Anderson also points out that evolution is not a scientific theory and cannot be fully understood or contained by science, but rather must be discussed in the context of epistemology, which is the study of knowledge and how it is acquired.
She continues:  
Algebra is built on lemmas that are not part of Algebra; they cannot be proven inside of Algebra.

Similarly, Epistemological statements are not provable in science because science is built on top of Epistemology. But when science is not helping, such as in Bizarre Domains, then setting scientific methodology aside and "dropping down to the level of Epistemology" sometimes works.

Epistemology is, just like Philosophy in general, an armchair thinking exercise and the results are judged on internal coherence and consistency with other accepted theory rather than by proofs or experiments.
2 / 2
In this continuation of her earlier statement, Monica Anderson compares the relationship between algebra and lemmas, and the relationship between science and epistemology. She notes that algebra is built on lemmas, or propositions that are assumed to be true but cannot be proven within algebra itself. Similarly, she suggests that epistemological statements, or statements about knowledge and how it is acquired, are not provable within the framework of science, because science is built on top of epistemology.
However, Anderson also notes that there are situations, such as in "bizarre domains," where scientific methodology may not be helpful or applicable. In these cases, she suggests that it may be necessary to set aside scientific methodology and "drop down to the level of epistemology" in order to make progress. She points out that epistemology, like philosophy more generally, is an armchair thinking exercise that relies on internal coherence and consistency with other accepted theories rather than on empirical proof or experimentation.
Finally: However, the availability of Understanding Machines such as DNNs now suddenly provides the opportunity for actual experiments in Epistemology. Consider the below statements from the domain of Epistemology, and how each of them can be viewed as an implementation hint for AI designers. We are already able to measure their effects on system competence.
In this final statement, Anderson suggests that the development of "understanding machines" such as deep neural networks (DNNs) has made it possible to conduct actual experiments in epistemology. She notes that these machines can be used to test and measure the effects of different epistemological statements on system competence, or the ability of the system to perform a particular task. Anderson suggests that these statements can be viewed as implementation hints for artificial intelligence (AI) designers, providing guidance on how to design and build effective AI systems.
Elsewhere, she says this: Reductionism is an amazingly powerful strategy for leveraging the work of scientists and for disseminating the results in the form of re-usable models of structure and causality. But for some of the "remaining hard problem domains" such as Life (biology, psychology, ecology, etc), the World (world modeling, economies, sociology), Intelligence (understanding the brain, intelligence, and creating Artificial General Intelligences - AGI) and the problem of determining the semantics of language (e.g. text) Reductionism has failed. I claim that reductionist models cannot be created in these domains (which have been named "Bizarre Domains") and that we must use Model Free (Holistic) Methods for these domains. This has important implications for AGI research strategies
In this statement, Anderson discusses the limitations of reductionism, or the approach of breaking down complex systems or phenomena into smaller parts in order to understand them. She notes that reductionism has been a powerful strategy for advancing scientific knowledge and creating reusable models in many fields, but that it has been less successful in certain "hard problem domains," such as life, the world, intelligence, and language semantics. Anderson argues that reductionist models cannot be created in these "bizarre domains" and that we must instead use model-free, or holistic, methods to understand and address these complex issues. This, she suggests, has important implications for artificial general intelligence (AGI) research strategies.
Give some suggestions on how to further develop my knowledge of human development, learning theory, and computer science, such as some steps and books to read.
Here are some suggestions for further developing your knowledge of human development, learning theory, and computer science:
Start by learning about the different stages of human development, including physical, cognitive, social, and emotional development. Some good resources to read include "Child Development" by Laura E. Berk and "Human Development" by Diane E. Papalia and Ruth Duskin Feldman.
Learn about different learning theories, such as behaviorism, cognitivism, constructivism, and social learning theory. Books to consider include "Learning Theories: An Educational Perspective" by Dale H. Schunk and "Theories of Learning" by Gordon Pask.
Explore the field of computer science by learning about different programming languages, such as Python, Java, and C++, and reading about topics like data structures, algorithms, and computer architecture. Good resources to start with include "Introduction to the Theory of Computation" by Michael Sipser and "Cracking the Coding Interview" by Gayle Laakmann McDowell.
Consider taking online courses or enrolling in a degree program to further your knowledge in these areas. There are many options available, ranging from short, specialized courses to longer, more comprehensive programs.
Join online communities or professional organizations related to human development, learning theory, or computer science to connect with others who are interested in these areas and learn from their experiences and insights.
Keep up to date with the latest research and developments in these fields by reading academic journals and following researchers and experts on social media.
That sounds good. Now summarize the following passages from Minds and Machines: Connectionism and Psychological Modeling by
Michael R. W. Dawson,
Advantages and Disadvantages of 
Modeling 
2.1What is a Model? 
In science, phenomena that are diff i cult to study or to understand in their own 
right are often approached through the use of models. The kinds of models that 
are used are as diverse as science itself. In biology, model organisms are used to 
study processes that cannot be easily measured in humans. In engineering, 
models of physical structures are tested in wind tunnels. Some might argue that 
physics is concerned with the development and testing of mathematical models 
of physical systems.
2 / 2
In "Minds and Machines: Connectionism and Psychological Modeling," the authors discuss the use of models in science to understand and study phenomena that are difficult to study directly. They note that models can take many forms, including model organisms in biology, physical models in engineering, and mathematical models in physics. The authors suggest that models can be useful because they allow scientists to make predictions and test hypotheses in a controlled environment, but they also note that models have limitations and may not always accurately reflect the complex systems they are intended to represent.
Even within a single discipline, one can fi nd a bewildering diversity of model 
types. For example, in psychology, computer simulation models have been 
created for many cognitive phenomena (Boden, 1977; Feigenbaum & Feldman, 
1995; Grossberg, 1988; VanLehn, 1991). Mathematical models have been used to 
study human perception, learning, judgments, and choice (Bock & Jones, 1968; 
Caelli, 1981; Restle, 1971). Statistical models have become the primary tool for 
expressing relationships between variables (Lunneborg, 1994; Pedhazur, 1982; 
Winer, 1971). Model organisms, such as the long-f i nned squid Loligo pealei, 
have been used to help understand the generation and transmission of nervous 
impulses (Hille, 1990; Levitan & Kaczmarek, 1991).
In psychology, the authors discuss the use of a variety of model types, including computer simulations, mathematical models, statistical models, and model organisms. These models have been used to study various cognitive phenomena, such as perception, learning, judgments, and choice, and to understand the generation and transmission of nervous impulses. The authors suggest that the use of these diverse models allows psychologists to approach the study of complex systems from different angles and to test hypotheses and make predictions in a controlled environment. However, they also note that models have limitations and may not always accurately reflect the real-world phenomena they are intended to represent.
A famous philosophical passage highlights the perils of def i ning even the 
simplest of terms: "Consider for example the proceedings that we call 'games'. 
I mean board-games, card-games, ball-games, Olympic games, and so on. What 
is common to them all? - Don't say: 'There must be something common, or they 
would not be called "games"' - but look and see whether there is anything 
common to all." Given the diversity that we have brief l y noted above, the term 
"model" could just have easily been used to demonstrate this point! Wittgenstein 
went on to argue that there was only a family resemblance between members 
of a category. "For if you look at them you will not see something that is com- 
mon to all, but similarities, relationships, and a whole series of them at that" 
Minds and Machines: Connectionism and Psychological Modeling 
Michael R. W. Dawson 
Copyright Â© 2004 by Michael R. W. Dawson Downloaded 
from https://onlinelibrary.wiley.com/doi/ 
by 
The Wikipedia 
Library, 
Wiley 
Online 
Library 
on [29/12/2022]. 
See 
the 
Terms 
and Conditions (https://onlinelibrary.wiley.com/terms-and-conditions) 
on 
Wiley 
Online 
Library 
for 
rules 
of 
use; 
OA 
articles 
are governed 
by 
the applicable 
Creative Commons 
License 
6Advantages and Disadvantages of Modeling 
(Wittengstein, 1953, p. 31e). The features that constitute these similarities and 
relationships change as different members of the same class are compared to one 
another. What kind of family resemblance would we fi nd among the members of 
the class "model"?
In the field of science, models are used to study and understand phenomena that are difficult to directly observe or understand. These models can take many forms, such as computer simulations, mathematical equations, statistical relationships, or even living organisms. The concept of a model has a broad definition, and there is a wide range of types of models that can be used in different fields. According to the philosopher Wittgenstein, there is not a single common characteristic that all models share, but rather a "family resemblance" between them, with the similarities and relationships between models changing as different members of the same class are compared.
Intuitively, a model is an artifact that can be mapped on to a phenomenon that 
we are having diff i culty understanding. By examining the model we can increase 
our understanding of what we are modeling. "A calculating machine, an anti- 
aircraft 'predictor', and Kelvin's tidal predictor all show the same ability. In all 
these latter cases, the physical process which it is desired to predict is imitated 
by some mechanical device or model which is cheaper, or quicker, or more con- 
venient in operation" (Craik, 1943, p. 51). 
For it to be useful, the artifact must be easier to work with or easier to under- 
stand than is the phenomenon being modeled. This usually results because the 
model ref l ects some of the phenomenon's properties, and does not ref l ect them 
all. A model is useful because it simplif i es the situation by omitting some charac- 
teristics. "Any kind of working model of a process is, in a sense, an analogy. 
Being different it is bound somewhere to break down by showing properties 
not found in the process it imitates or by not possessing properties possessed by 
the process it imitates" (Craik, 1943, p. 53). Similarly, "the word model may 
be used instead of theory to indicate that the theory is only expected to hold 
as an approximation, or that employing it depends upon various simplifying 
assumptions" (Braithwaite, 1970, p. 269).
Models are tools used in science to help understand and study phenomena that are difficult to study or understand directly. They can take many forms, including computer simulations, mathematical equations, statistical models, and physical models. A model is useful because it simplifies a situation by omitting certain characteristics of the phenomenon being studied. However, a model may also break down by not possessing certain properties of the phenomenon or by exhibiting properties that the phenomenon does not possess. Models are often used as approximations and may be based on simplifying assumptions. They can be used to make predictions and to test hypotheses.
2.2Advantages and Disadvantages of Models 
Modeling in psychology or cognitive science is associated with both advantages 
and disadvantages (e.g., Lewandowsky, 1993). In this section, we will consider 
three general advantages of modeling. However, after each of these three advant- 
ages, we will follow with a discussion of associated disadvantages. Models are 
like i ne knives with which you can create gourmet meals, but with which you 
can also cut your fi ngers.
Models in science, such as those used in psychology and cognitive science, can be useful tools for understanding and studying complex phenomena. They allow researchers to simplify and analyze certain aspects of a phenomenon, while ignoring or omitting other characteristics. However, models also have limitations and can be associated with disadvantages. One disadvantage is that they may only approximate the true nature of the phenomenon being studied, and may not accurately capture all of its characteristics. Another disadvantage is that models can be based on simplifying assumptions or approximations that may not hold in all cases. Additionally, the use of models can sometimes lead to a reductionist approach that focuses on individual components or processes rather than considering the phenomenon as a whole.
2.2.4Problems with exploring complex domains 
From the preceding subsection, it is clear that models provide a medium that 
provides many advantages for researchers interested in exploring complicated 
Downloaded 
from https://onlinelibrary.wiley.com/doi/ 
by 
The Wikipedia 
Library, 
Wiley 
Online 
Library 
on [29/12/2022]. 
See 
the 
Terms 
and Conditions (https://onlinelibrary.wiley.com/terms-and-conditions) 
on 
Wiley 
Online 
Library 
for 
rules 
of 
use; 
OA 
articles 
are governed 
by 
the applicable 
Creative Commons 
License 
Advantages and Disadvantages of Modeling17 
ideas in an eff i cient, inexpensive manner. These ideas can even be explored in 
advance of any mathematical account of the domain. However, while the ability 
to explore complex domains is a def i nite advantage of modeling, it can lead to 
some interesting disadvantages. Two of these are considered in the subsections 
below. 2.2.4.1Bonini's paradox 
Dutton and Starbuck (1971) used the name Bonini's paradox to identify one prob- 
lem with computer simulations of complex phenomena. Bonini's paradox, named 
after Stanford business professor Charles Bonini, occurs when a computer 
simulation is at least as diff i cult to understand as the phenomenon that it was 
supposed to illuminate:
Modeling is a technique used in science to approach and understand difficult or complex phenomena by creating an artifact that represents some of the properties of the phenomenon. Models can be useful because they simplify the situation and make it easier to work with or understand than the phenomenon itself. However, models also have disadvantages, such as the fact that they may not reflect all the properties of the phenomenon being modeled, and may break down by showing properties not found in the phenomenon or by not possessing properties possessed by the phenomenon. Additionally, exploring complex domains using models can lead to the phenomenon of Bonini's paradox, where the computer simulation is at least as difficult to understand as the phenomenon it was intended to illuminate. Another disadvantage of modeling complex domains is the possibility of overfitting, where the model becomes too specific and is unable to generalize to new situations.
Summarize: Monica claims there's an opposing point of view to Reductionism (which is the base of science in my opinion) ,which is Holism. Its Slogan is "The whole is more than the sum of its parts". An approach is considered to be holistic if it analyzes the problem as a whole without dividing it into sub-problems. Holistic Methods are also called "Model Free Methods".
Holism is a perspective that looks at a system as a whole rather than as a collection of parts. It is often opposed to reductionism, which breaks a system down into smaller parts in order to understand it. Holistic methods, also known as model-free methods, do not rely on predetermined models or goals, and are instead based on intuition and a holistic approach. These methods are being rediscovered and utilized in areas such as genetic programming, evolutionary algorithms, organic learning, and perceptual control theory. Holistic approaches can be particularly useful in addressing complex or "bizarre" domains, such as biology, psychology, ecology, world modeling, economics, sociology, intelligence, and language semantics.
Summarize this passage from Monica Anderson in 2016: People are complaining that Deep Learning and other Neural Network technologies are opaque and inscrutable. If a self-driving car kills someone then we, as AI providers, cannot show the judge the line of code that caused the accident. There is a fair chance that we'll have to live with this. The moment you decide to use Neural Networks you lose the six advantages of Reductionism: Optimality, completeness, repeatability, parsimony, transparency (of process) and scrutability (of the results). Instead, you gain a lot, like the ability to solve problems you yourself don't understand, but that's a different post.

The self-driving car manufacturers say "we will still have accidents but there will be much fewer of them if we allow self-driving cars everywhere".

I believe they are on the right track; that is the correct reply to people that insist on only allowing 100% safe self-driving cars.

AIs are not like computers. AIs don't solve normal logical problems like normal logic-based computers do - problems with well-defined inputs and a well-defined notion of what constitutes a correct output. Instead, AIs are solving problems of Reduction.
Monica Anderson argues that using neural networks in artificial intelligence (AI) technology leads to a loss of certain advantages associated with reductionism, including optimality, completeness, repeatability, parsimony, transparency, and scrutability. However, AI technology is still valuable because it can solve problems that are not well-defined and not easily solved through traditional methods. Anderson believes that self-driving car manufacturers are justified in their approach to allowing self-driving cars on the road, even if they may not be completely safe, because they will likely still be safer than human drivers.
She continues: Here's my shortest (so far) description of "Reduction":

When you are solving a story problem you have to do three things:

1. You need to understand the rich reality in the story so that you can write down or program an equation to solve (a Model)

2. You solve the equation/Model (or let a computer solve it)

3. You apply the result back to the rich reality

This is known as Model Based (Reductionist) problem solving and it is the basis for most of our Science.

In normal computer use, humans do #1, computers do #2, and humans do #3.

Step #1 is called the Reduction step. Most of the time, we don't think of it as difficult because that's what our entire Reductionist education has been training us to do correctly.

But now computers are starting to do #1. This is exactly what AI is about. Some AIs do it poorly; they make "Reduction Errors".

We cannot compare these Reduction Errors to errors made by computers in regular computer use situations where a human makes the Reduction. They are different tasks, different situations, and with vastly different levels of complexity. Reductionism is a correct strategy only for simple problems. For context rich situations involving our rich reality you need to perform the Reduction before you can reason about the problem algebraically or use a computer.

But even understanding all this doesn't really help the people who have been in auto accidents in self-driving cars (fatal or not). How can we explain to them that there is no "bug in the program", nothing specific we can point to as the cause of the accident. The AI may have done the Reduction as well as possible under the circumstances and a human may not have been able to do any better.
Monica Anderson is discussing the difference between traditional computer programming, where a human writes code to solve a specific problem, and artificial intelligence (AI), where the computer is able to learn and make decisions on its own. She argues that AI approaches problems differently than traditional computers, using a process called "reduction" to understand and solve complex problems. Anderson acknowledges that AI systems can make mistakes, but she believes that they are still valuable tools because they can solve problems that humans might not be able to understand. She also points out that explaining the mistakes made by AI systems can be difficult, as there may not be a specific error or line of code that can be identified as the cause of the problem.
Finally: Perhaps they would Understand better if we anthropomorphized the self-driving car. Turns out a lot of drivers already do that. So let's continue the trend.

Many of the error types we encounter in our AIs are error types we also have recognized in humans. We have yet to see a "psychotic" AI and I hope we never let them out of the lab but lesser human errors also have names. Let's use those names for errors made by our AIs also !

If nobody ever told you that doing X leads to problem Y, then you aren't necessarily stupid, only ignorant about this fact. This is equivalent to not providing examples of X and Y in an AI's training corpus. So auto manufacturers can say "Yes, your car was ignorant about tumbleweeds and we'll fix that in the next software release".

My proposal now is to start using names for Reduction errors that match the names we're used to using for humans making the same errors. I think this will help non-AI people like customers, drivers, and judges understand how to think about these technically deep issues. This slide lists the first four I came up with. Feel free to add to the list.
Monica Anderson is discussing the differences between Reductionist and Holistic approaches to problem solving, and how these approaches are being applied in the field of Artificial Intelligence (AI). Reductionism is a method of understanding a complex system by breaking it down into its individual components and analyzing them separately. This approach has been successful in many areas of science, but Anderson argues that it has its limitations when applied to certain "bizarre domains" such as life, the world, intelligence, and language semantics. In these areas, she suggests that Model Free (Holistic) Methods are more effective. Anderson also discusses the potential challenges and limitations of using AI, particularly in the context of self-driving cars. She suggests that in order to better understand the behavior of AI systems, it may be helpful to anthropomorphize them and describe their actions using terms that are familiar to humans, such as "ignorance" or "bias."
Point out some commonalities and similarities between Monica's above statement and this response given to my chat the other day by an online chatbot called chatgpt:


** An example of counterfoil choice in the context of machine learning, large language models, and AI. **

One example of a counterfoil choice in the context of machine learning, large language models, and AI could be the decision between using a reductionist approach or a holistic approach to understanding and interacting with these systems.

In a reductionist approach, the focus is on breaking down the problem into smaller parts and analyzing them individually, using techniques such as word frequency analysis or grammatical rules.

This approach can be efficient and effective in certain situations, but it may not always capture the complexity and nuance of natural language or the overall context of the problem.

On the other hand, a holistic approach involves considering the larger context and the interconnectedness of different aspects of the problem.

This could involve using techniques such as deep learning or organic learning, which aim to mimic the way the human brain processes and understands information.

While this approach may require more resources and may not always be as efficient, it can provide a more nuanced and comprehensive understanding of the problem.

In this scenario, the reductionist approach could be seen as the counterfoil to the holistic approach, representing a tempting but ultimately incomplete or limited solution.

Choosing the holistic approach, while potentially more challenging, could lead to a deeper and more meaningful understanding of the problem.
2 / 2
Some commonalities and similarities between Monica's statement and the response given by chatgpt include:
Both mention the concept of reductionism and its limitations in certain contexts. Reductionism is an approach that breaks down a problem into smaller parts and analyzes them individually.
Both mention the concept of holism, which involves considering the larger context and the interconnectedness of different aspects of the problem.
Both mention the use of different techniques and approaches in each of these approaches, such as word frequency analysis, grammatical rules, deep learning, and organic learning.
Both highlight the potential trade-offs between these approaches, such as efficiency and effectiveness versus nuance and comprehensiveness.
Both suggest that in certain situations, a holistic approach may be more effective and lead to a deeper understanding of the problem, even though it may be more challenging to implement.
Explain what might be the meaning of counterfoil choice in the above statement.
A counterfoil choice is a decision that involves choosing between two opposing or contrasting options. In the context of machine learning, large language models, and AI, a counterfoil choice might involve deciding between a reductionist approach and a holistic approach to understanding and interacting with these systems.
The reductionist approach involves breaking down the problem into smaller parts and analyzing them individually, using techniques such as word frequency analysis or grammatical rules. This approach can be efficient and effective in certain situations, but it may not always capture the complexity and nuance of natural language or the overall context of the problem.
On the other hand, the holistic approach involves considering the larger context and the interconnectedness of different aspects of the problem. This could involve using techniques such as deep learning or organic learning, which aim to mimic the way the human brain processes and understands information. While this approach may require more resources and may not always be as efficient, it can provide a more nuanced and comprehensive understanding of the problem.
In this scenario, the counterfoil choice is the decision between the reductionist approach and the holistic approach, with each representing a tempting but ultimately incomplete or limited solution. Choosing the holistic approach, while potentially more challenging, could lead to a deeper and more meaningful understanding of the problem.
Explain the this passage in light of what monica Anderson has to say.

By the way, the vindication of a thesis such as this falls outside the domain of the mathematician or physicist, and belongs to the philosopher: the question at issue is one of epistemology. *

â€” Koopman, 1940

* See A Treatise on Probability, 1921 (Keynes)

Monica:
"Philosophy isn't Scientific. You don't see many "proofs". But the reason for this is surprising to many.

Epistemology is a branch of Philosophy, and Science is DEFINED in terms of Epistemology. Where did the lemmas of Algebra come from? Not from within Science.

Let us consider all problem solving methods we can ever think of -- all that will ever exist. Epistemology can be used to discuss all of these methods as a group.

Some problem solving methods work well all the time. Some don't really work. Some work well some of the time. Some work a little bit all of the time.

Science is the good stuff that works well all the time.

Epistemology includes Science but it also has access to riff-raff methods that don't work as well or as often.

Epistemology is larger than Science. It can often solve problems that science cannot. Such as problems of immense complexity. Such as everyday life.  Scientifically we can predict the position of Jupiter 200 years from now, but I personally sure can't guess where that would be.  OTOH, brains can tell whether someone we just met would make a good friend.  These kinds of estimates cannot be done mathematically.

The brain is not a high precision device.  It is a high complexity device."
2 / 2
According to Monica Anderson, epistemology is a branch of philosophy that deals with the nature of knowledge and how we acquire it. It encompasses science, but also includes other methods of problem solving that may not be as reliable or effective. Science is considered to be a particularly effective method of problem solving because it works well all the time, but epistemology can also address problems that science cannot, such as those that are very complex or those that involve everyday life. The brain, for example, is a high complexity device that is capable of making estimates and judgments that cannot be done mathematically, such as determining whether a person would make a good friend. In this passage, Koopman is pointing out that the question of whether a thesis is valid or not falls outside the domain of mathematics or physics and is a matter of epistemology, or how we acquire and understand knowledge.
Summarize: (Monica Anderson)
Science works in labs, but in our complex everyday reality, Science is often helpless, and these riff-raff methods are the only thing that will work at all. For starters, in daily life we never have perfect information and we MUST jump to conclusions on whatever information we have. Without correct and complete input data, Logic and Reasoning cannot even get started.

Reductionist Science has deprecated Holistic Methods for hundreds of years, but now the ML community has discovered ways to make computers use Holistic Methods.  Such methods cracked the Protein Folding problem, and now ChatGPT has demonstrated language Understanding. These are things that Science cannot touch because of exceeding complexity.

Science is for simple problems. Life is complex.
Monica Anderson is pointing out that while scientific methods, which involve breaking down a problem into smaller parts and analyzing them individually, can be effective in certain situations, they may not always be able to capture the complexity and nuance of real-world problems. She suggests that holistic methods, which consider the larger context and interconnectedness of different aspects of a problem, may be more effective in certain cases, even though they may be more resource-intensive and less efficient. Anderson also highlights the limitations of scientific methods in addressing problems of immense complexity, such as those found in everyday life. She believes that holistic methods, such as those used in machine learning, can be successful in solving these kinds of problems and have already demonstrated success in areas such as protein folding and language understanding.
Expound:
Monica Anderson is making the argument that science, which is typically defined in terms of epistemology (the study of knowledge), is only effective for solving simple problems. She argues that in the complex reality of everyday life, science is often helpless and that "riff-raff methods," or methods that are not considered scientific, are the only ones that will work. Anderson points out that in daily life, we often do not have complete information and must make decisions based on whatever information we do have. In these situations, logic and reasoning cannot even get started without correct and complete input data.
Anderson also notes that reductionist science, which involves breaking down a problem into smaller parts and analyzing them individually, has deprecated holistic methods for many years. However, she argues that the machine learning community has recently discovered ways to make computers use holistic methods, which have been successful in solving complex problems such as protein folding and language understanding. Anderson concludes that science is only useful for simple problems, while life is complex and requires more nuanced, holistic approaches to problem solving.
Explain the following: - Reasoning is everything (The Introspection Fallacy)
- Behavior is everything
- Understanding is out of scope (Minsky)
- Reductionist AI is impossible because of the Frame Problem (McCarthy & Hayes)
- Intelligence is Reasoning + Understanding  (Kahneman)
- Understanding requires Epistemic Reduction (me)
- DNNs provide Epistemic Reduction (me)
- DNNs can Learn (Post-2012 Deep Neural Networks)
- DNNs can provide predictions (Numerous papers)
The Introspection Fallacy is the idea that reasoning is everything, or that it is the most important factor in determining intelligence or understanding. This fallacy suggests that one's own thoughts and reasoning processes are the most accurate and reliable sources of information and that they should be trusted over external evidence or other sources of information.
Behaviorism is a psychological theory that focuses on the observation and measurement of behavior, rather than on mental processes such as thoughts and emotions. According to this perspective, everything that can be studied in psychology should be studied through the observation and measurement of behavior.
Minsky, a pioneer in the field of artificial intelligence, argued that understanding is out of scope for AI because it is a complex and elusive concept that is difficult to define and measure. He argued that the focus should instead be on developing AI systems that can perform certain tasks or exhibit certain behaviors.
The Frame Problem is a problem in artificial intelligence that arises when trying to build systems that can reason about the world. The problem is that there are an infinite number of things that a system could potentially consider when trying to reason about a situation, and it is difficult to determine which of these things are relevant or irrelevant. McCarthy and Hayes proposed that the Frame Problem makes it impossible for AI systems to be reductionist, or to break down complex problems into simpler parts in order to understand them.
Kahneman, a psychologist and economist, argued that intelligence involves both reasoning and understanding. He defined understanding as the ability to comprehend the meaning and significance of something, and suggested that this is distinct from the ability to reason or to process information.
According to Monica Anderson, understanding requires epistemic reduction, or the process of breaking down a problem into simpler parts in order to understand it. Anderson argues that deep neural networks (DNNs) can provide this kind of reduction, allowing them to learn and provide predictions about complex and nuanced problems. This view is supported by numerous papers in the field of artificial intelligence, which have demonstrated the ability of DNNs to learn and make predictions about a wide range of tasks.
Continue explaining Monica's aphorisms:
Learning can solve simple problems (handwriting, object recognition, sentiment analysis)
- Learning can solve problems better than we can (video games, DNN-based chess, go)
- Learning can solve problems we cannot solve (Protein folding)
- Learning can solve problems so easily we can't be bothered to do it ourselves (Real estate pricing)
- Learning is everything
- Learning provides Understanding (me)
- Reasoning must be grounded in Understanding (me)
- Dialog requires Understanding and Prediction
- GPT demonstrates simple Dialog
- Reasoning is now the only thing missing (me)
3 / 3
Reasoning is everything (The Introspection Fallacy): This aphorism suggests that reasoning, or the ability to think and draw logical conclusions, is the most important aspect of intelligence. The introspection fallacy refers to the belief that one's own thoughts and mental processes can be accurately observed and understood through introspection.
Behavior is everything: This aphorism suggests that behavior, or how an individual behaves, is the most important aspect of intelligence. This perspective emphasizes the importance of practical actions and real-world outcomes, rather than internal mental processes.
Understanding is out of scope (Minsky): This aphorism, attributed to AI researcher Marvin Minsky, suggests that understanding, or the ability to comprehend and make sense of the world, may be beyond the scope of artificial intelligence. Some proponents of this view argue that understanding requires a level of subjective experience and consciousness that may be beyond the capabilities of machines.
Reductionist AI is impossible because of the Frame Problem (McCarthy & Hayes): The frame problem is a term used in artificial intelligence research to refer to the challenge of determining which aspects of a problem or situation are relevant and which can be ignored. Some researchers, including John McCarthy and Patrick Hayes, have argued that the frame problem makes it impossible for AI to be reductionist, or based on the idea of breaking down a problem into smaller parts for analysis.
Intelligence is Reasoning + Understanding (Kahneman): This aphorism suggests that intelligence is made up of two components: reasoning, or the ability to think and draw logical conclusions, and understanding, or the ability to comprehend and make sense of the world. This perspective, attributed to psychologist Daniel Kahneman, emphasizes the importance of both logical thinking and comprehension in intelligence.
Understanding requires Epistemic Reduction (me): This aphorism suggests that understanding, or the ability to comprehend and make sense of the world, requires a process called epistemic reduction, or the breaking down of a problem or situation into smaller parts for analysis. This perspective emphasizes the importance of analysis and understanding in intelligence.
DNNs provide Epistemic Reduction (me): This aphorism suggests that deep neural networks (DNNs), a type of artificial neural network used in machine learning, can provide epistemic reduction, or the breaking down of a problem or situation into smaller parts for analysis. This perspective emphasizes the ability of DNNs to analyze and
Give some examples of model-free methods in every day life.
Some examples of model-free methods in everyday life might include:
Navigating unfamiliar environments: When we are in a new place, we may not have a pre-existing model of the layout or geography, but we can still find our way around using our senses and general problem-solving abilities.
Making decisions based on gut feelings: When faced with a complex or uncertain situation, we may rely on our instincts or emotional responses to guide our decisions.
Learning through trial and error: When we are trying to learn a new skill or task, we may not have a clear model or understanding of how it works, but we can still learn through experimentation and practice.
Communicating with others: When we are engaging in conversation or social interactions, we may not have a precise model of the other person's thoughts or intentions, but we can still communicate effectively through nonverbal cues and empathy.
Adapting to changing circumstances: When our environment or circumstances change, we may have to adjust our behaviors and actions in real-time, without the benefit of a pre-existing model.
Summarize: Mathematics and Programming are "support sciences" to the other sciences. They support the manipulation of Models in the form of theories, formulas, equations, and computer programs. Some of these Models are made by Mathematicians but most Models attempt to describe simplified ("Reduced") aspects of a rich reality and are created by scientists working in Model-making (Reductionist) sciences like Physics. Math is a "Model-manipulating" formalism.
In this summary, it is suggested that mathematics and programming are tools that are used to support and aid other fields of science, specifically through the manipulation of models. These models can be in the form of theories, formulas, equations, or computer programs and are used to describe and understand aspects of the world around us. While some models are created by mathematicians, most are created by scientists working in reductionist sciences like physics in order to simplify and understand complex phenomena. It is noted that math is a formalism that is particularly useful for manipulating models.
This Reduction - the simplification of our rich reality to Models - is done OUTSIDE OF MATHEMATICS. It is done by intelligent agents such as physicists, mathematicians, and engineers that Understand the problem domain, but the discipline of Mathematics itself doesn't come into play until AFTER the Reduction has been done by the agent.

Mathematics is not capable of doing the Reduction and is also impotent to describe and formalize the process of Reduction itself. This process is done by brains and involves using Model Free Methods, such as trial and error (and other methods of discovery) and remembering past successes (and other methods of learning). These processes occur at the Connectome level and involve Neurons adjusting their interconnections through Synapses over time.
In summary, Mathematics and Programming are used to manipulate models that are created through the process of Reduction, which involves simplifying and understanding complex phenomena in the real world. This process of Reduction is done by intelligent agents using Model Free Methods, such as trial and error and learning, which occur at the brain level through adjustments in neuron connections. Mathematics itself is not capable of performing Reduction or describing the process of Reduction.
This process can be Modeled - described Mathematically at the Neuron level - but the Connectome - the "social map" of the Neurons - is an emergent effect and cannot be described Mathematically in a Model since it is NOT THE RESULT OF A REDUCTION. It is created using Model Free Methods and is PERFORMING the Reduction, but it is not the Reduced end result, the elegant Model that Mathematicians want.

No Models, No Math.
In summary, it is suggested that mathematics and programming are "support sciences" that help manipulate and analyze models in the form of theories, formulas, equations, and computer programs. These models are created by scientists and engineers who use their understanding of the problem domain to simplify or "reduce" the complexity of a phenomenon. However, the process of reduction itself cannot be described or formalized mathematically and is instead done using model-free methods such as trial and error and learning. The resulting model can then be analyzed and manipulated using mathematics, but the process of reduction that led to the creation of the model is not itself mathematically describable.
Summarize with very formal scientific language and references: "The cobra's poison is extremely effective but cobras know nothing of biochemistry" -- paraphrasing Gary Cziko 

Evolution has forever been able to solve problems without understanding them. If you want to build an Artificial Intelligence then this would seem to be a very useful technique. But the AI research community has generally ignored this little hint for decades.

In this Atlantic article, Daniel C. Dennett discusses Darwin's gradualism and draws the parallels from the world of Evolutionary Theory to the world of Artificial Intelligence.

Both worlds require a shift in stance that most Reductionist scientists are unprepared for: The shift from observing genes or transistors to observing their emergent effects: Fitness, phenotypes, survival, life, understanding, intelligence, and consciousness. Emergent phenomena of this kind will remain outside of what Science can explain and achieve until we accept that Emergent phenomena are real - in many cases much more real than the Models that Reductionist Science provides and requires.

Reductionism is a very powerful tool; it is a knife that divides and extracts, that discards irrelevant detail in order to find the computable core priniciples. But it often kills that which we want to study. It needs to be complemented with a Holistic stance that allows us to study whole systems including their emergent effects without requiring simple computable Models of Reality. The knife that cuts needs to be complemented with the basket that gathers.
The effectiveness of cobra venom as a means of defense and prey acquisition is a phenomenon that can be observed and studied, but the mechanisms behind its production and action are not fully understood. This highlights the concept of emergent phenomena, or the emergence of complex behavior or properties from simpler components, which can be observed and studied but may not be fully explicable through reductionist methods. This idea is analogous to the development of artificial intelligence, which often involves the emergence of complex behaviors or capabilities from simpler components, but may not necessarily involve a complete understanding of the underlying mechanisms. Daniel C. Dennett's Atlantic article discusses the relevance of this concept in the context of evolutionary theory and artificial intelligence research, arguing that a holistic approach is necessary to study and understand emergent phenomena.
Explain these model-free methods: Trial-and-error (aka Generate-and-test), Enumeration, Remembering failures, Remembering successes, Table lookup, Mindless copying, Discovery, Recognition, Learning, Abstraction, Adaptation, Evolution, Narrative, Consultation, Delegation and Markets.
Trial-and-error (also known as generate-and-test) is a model-free method of problem-solving that involves attempting different solutions to a problem and evaluating the effectiveness of each solution. This method can be used to solve a wide range of problems, including simple tasks such as finding a lost object or more complex tasks such as developing a new technology or product.
Enumeration is a model-free method of problem-solving that involves systematically listing out all the possible solutions to a problem and evaluating each one to determine the best option. This method is often used to solve problems with a limited number of possible solutions, such as finding all the possible combinations of a combination lock.
Remembering failures is a model-free method of problem-solving that involves recalling past failures and using that knowledge to avoid making the same mistakes in the future. This method can be used to improve decision-making and problem-solving skills by helping individuals learn from their past experiences.
Remembering successes is a model-free method of problem-solving that involves recalling past successes and using that knowledge to guide decision-making and problem-solving in the present. This method can be used to build confidence and improve problem-solving skills by helping individuals learn from their past successes.
Table lookup is a model-free method of problem-solving that involves consulting a pre-determined table of values to find the solution to a problem. This method is often used to solve simple problems or to perform basic calculations quickly and efficiently.
Mindless copying is a model-free method of problem-solving that involves simply copying the actions of others without understanding the underlying principles or reasoning behind them. This method can be useful for quickly solving simple problems or learning new tasks, but it may not be effective for solving more complex or novel problems.
Discovery is a model-free method of problem-solving that involves actively exploring and experimenting to find new solutions to problems. This method is often used in scientific research and can involve the use of trial-and-error, experimentation, and other model-free methods.
Recognition is a model-free method of problem-solving that involves identifying a problem or solution by comparing it to previously encountered problems or solutions. This method is often used in areas such as pattern recognition, object identification, and natural language processing.
Learning is a model-free method of problem-solving that involves actively acquiring new knowledge and skills through experience, training, or education. This method is often used to improve problem-solving skills
Abstraction...
Abstraction is a model-free method that involves simplifying a complex concept or process by focusing on its essential characteristics and ignoring or discarding non-essential details. This allows us to understand and manipulate the concept or process more easily. For example, we use abstraction when we use the concept of "number" to represent quantities, even though the specific quantities we are working with may be different in nature (e.g., the number of apples and the number of people). Abstraction is a fundamental aspect of human cognition and is essential for our ability to deal with the complexity of the world around us.
Adaptation, Evolution, Narrative, Consultation, Delegation, and Markets.
Adaptation is the process of changing or adapting to new or different circumstances. It can involve modifying behavior, characteristics, or processes in response to new conditions or environments.
Evolution refers to the gradual process of change and development that occurs in living organisms over time. It is driven by natural selection, which is the process by which certain traits or characteristics are passed down from one generation to the next because they help the organism survive and reproduce in its environment.
Narrative refers to the telling of a story or the recounting of events. It is a way of communicating information or experiences through the use of storytelling techniques, such as plot, character development, and setting.
Consultation is the act of seeking advice or expert opinion from someone else, typically in order to make a decision or solve a problem.
Delegation is the act of assigning responsibility or authority to someone else. It involves transferring tasks or decision-making authority to another person or group in order to accomplish a goal or achieve a desired outcome.
Markets are economic systems that involve the exchange of goods and services between buyers and sellers. They can be physical locations, such as a traditional marketplace or shopping center, or virtual spaces, such as online platforms for buying and selling goods. Markets operate based on supply and demand, with prices being determined by the interactions between buyers and sellers.
Some model free methods that maybe haven't been mentioned.
Intuition: Intuition is the ability to make quick and accurate judgments or decisions based on experience or instinct. It is a type of model-free method because it does not rely on specific models or algorithms to make decisions. Instead, it relies on the individual's past experiences and knowledge to guide their decision-making process.
Analogy: Analogy is the process of comparing two things in order to understand one of them better. It is a model-free method because it does not rely on specific models or algorithms to make comparisons. Instead, it relies on the individual's ability to identify similarities and differences between two things in order to draw conclusions about them.
Heuristics: Heuristics are mental shortcuts that allow individuals to make judgments or decisions quickly and efficiently. They are model-free methods because they do not rely on specific models or algorithms to make decisions. Instead, they rely on the individual's past experiences and knowledge to guide their decision-making process.
Experience-based learning: Experience-based learning is a type of model-free method that relies on the individual's past experiences to guide their decision-making process. It does not rely on specific models or algorithms, but rather on the individual's ability to learn from their experiences and apply that learning to new situations.
Common sense: Common sense is the ability to make judgments or decisions based on what is generally considered to be reasonable or practical. It is a model-free method because it does not rely on specific models or algorithms to make decisions. Instead, it relies on the individual's general knowledge and understanding of the world to guide their decision-making process.
Make a short outline under the heading and ordering E-CHAI.
E-CHAI Outline:
E: Enumeration
Method of systematically trying all possibilities in order to find a solution
Can be time-consuming and not always feasible for larger problems
C: Copying
Method of simply replicating the behavior or solution of others
Can be effective in certain situations, but may not always be the most efficient or innovative solution
H: Heuristics
Method of using rules of thumb or mental shortcuts to make decisions or solve problems
Can be useful for quickly finding solutions, but may not always be accurate or reliable
A: Adaptation
Method of adjusting or altering behavior or solutions based on past experiences or new information
Can be effective in complex or changing environments, but may require careful consideration and analysis
I: Intuition
Method of relying on instinct or gut feeling to make decisions or solve problems
Can be helpful in certain situations, but may not always be reliable or based on logic.
Expound on this short passage: Prerational Intelligence:
Adaptive Behavior and
Intelligent Systems without
Symbols or Logic
Prerational intelligence refers to intelligent behavior and systems that do not rely on symbols or logic. This type of intelligence is often seen in animals and can involve behaviors such as adapting to changing environments, exhibiting problem-solving abilities, and exhibiting social intelligence. Prerational intelligence is a type of intelligence that is often associated with non-human animals and is thought to have evolved before the development of symbolic thought and logical reasoning. Despite not being based on symbols or logic, prerational intelligence can still be highly effective and is a crucial aspect of intelligent systems. Some examples of prerational intelligence in animals include the ability of birds to navigate over long distances, the ability of dolphins to communicate with each other, and the ability of ants to work together in colonies to accomplish tasks. In artificial intelligence, prerational intelligence can be seen in systems that are designed to exhibit intelligent behavior without using symbols or logic, such as neural networks and machine learning algorithms.
Summarize: Innate knowledge refers to prerational intelligence in humans, which includes adaptive behavior and intelligent systems that do not rely on symbols or logic. Examples of innate knowledge include topology, agentness, universal grammar, objectness, places, elementary geometry, number quantity, and social partners. These innate abilities allow humans to navigate and understand the world around them without the need for explicit learning or instruction. However, it is important to note that innate knowledge does not exclude the possibility of learning and development, as it can be shaped and expanded upon through experience and interaction with the environment.

â€”â€”Innate Knowledgeâ€”â€”

Topology
Agentness
Universal Grammar

Objectness
Places
Elementary Geometry
Number Quantity
Social Partners
2 / 2
Innate knowledge refers to the inherent understanding and abilities that humans possess from birth. This prerational intelligence allows us to navigate and understand our environment without the need for explicit learning or instruction. Examples of innate knowledge include our understanding of topology (the study of shapes and spaces), our ability to recognize and interact with other agents, our innate understanding of language through universal grammar, and our understanding of basic geometric and numerical concepts. Innate knowledge also includes our understanding of objects and places, as well as our ability to interact with and recognize other social partners. While innate knowledge forms the foundation for our understanding and abilities, it can also be shaped and expanded upon through experience and interaction with the environment.
Summarize: NaÃ¯ve physics or folk physics is the untrained human perception of basic physical phenomena. In the field of artificial intelligence the study of naÃ¯ve physics is a part of the effort to formalize the common knowledge of human beings.[1]

Many ideas of folk physics are simplifications, misunderstandings, or misperceptions of well-understood phenomena, incapable of giving useful predictions of detailed experiments, or simply are contradicted by more thorough observations. They may sometimes be true, be true in certain limited cases, be true as a good first approximation to a more complex effect, or predict the same effect but misunderstand the underlying mechanism.

NaÃ¯ve physics can also be defined as an intuitive understanding all humans have about objects in the physical world.[2] Certain notions of the physical world may be innate.
Naive physics refers to the common, intuitive understanding that humans have about objects and phenomena in the physical world. It is often characterized by simplifications, misunderstandings, or misperceptions of more complex scientific concepts, and may not always provide accurate predictions or explanations. However, naive physics can also be based on innate knowledge and serve as a useful first approximation to more complex phenomena. It is studied in the field of artificial intelligence as a way to formalize human common knowledge about the physical world.
Summarize: Some examples of naÃ¯ve physics include commonly understood, intuitive, or everyday-observed rules of nature:

What goes up must come down
A dropped object falls straight down
A solid object cannot pass through another solid object
A vacuum sucks things towards it
An object is either at rest or moving, in an absolute sense
Two events are either simultaneous or they are not
Many of these and similar ideas formed the basis for the first works in formulating and systematizing physics by Aristotle and the medieval scholastics in Western civilization. In the modern science of physics, they were gradually contradicted by the work of Galileo, Newton, and others. The idea of absolute simultaneity survived until 1905, when the special theory of relativity and its supporting experiments discredited it.
In summary, naÃ¯ve physics refers to the untrained human perception of basic physical phenomena and the intuitive understanding that humans have about objects in the physical world. These ideas are often simplifications or misunderstandings of more complex phenomena, and may not always accurately predict or explain the behavior of objects in the physical world. Examples of naÃ¯ve physics include the idea that objects fall straight down when dropped, that a vacuum sucks things towards it, and that an object is either at rest or moving in an absolute sense. These ideas have formed the basis for early works in physics, but have been largely contradicted by more thorough scientific research and understanding.
Summarize how this innate knowledge might be useful in understanding intelligence, such as the errors children make when learning grammar, or heuristic shortcuts for solving otherwise intractable problems.
Innate knowledge can be useful in understanding intelligence because it represents the intuitive understanding that humans have about the world and how it works. For example, research has shown that children make certain errors when learning grammar, which can be attributed to their innate knowledge of language and how it is used. Additionally, innate knowledge can also inform heuristic shortcuts, or mental shortcuts, that allow humans to solve problems more efficiently. These heuristic shortcuts can be useful for solving otherwise intractable problems by providing a quick and easy way to approximate a solution, even if it is not completely accurate. Overall, understanding innate knowledge can help us to better understand how humans process and understand information, and how this innate understanding influences our ability to learn and solve problems.
Summarize: The Opposition of Art and Science.

There is an implacable hostility between literary intellectuals, sometimes called humanists, and physical scientists.

There are two cultures in fierce opposition to each other and it is necessary for a great debate to ensue about the matter.

â€” Charles Snow, 1959

The Technologists and the Rest

Because of its lengthy, intimate, and inevitable relationship with culture, technology does not invite a close examination of its own consequences; it is the kind of friend that asks for trust and obedience.

Stated in the most dramatic terms, the accusation can be made that the uncontrolled growth of technology destroys the vital sources of our humanity.

It creates a culture without a moral foundation. It undermines certain mental processes and social relations that make human life worth living.

â€” Neil Postman, 1992
The opposition of art and science refers to the perceived dichotomy between the two fields, with some individuals believing that art and science are fundamentally at odds with each other. Charles Snow's concept of the "two cultures" refers to the perceived divide between the literary intellectuals, or humanists, and the physical scientists. Snow argued that there is an inherent hostility between these two groups and that a great debate is necessary to address this divide. Neil Postman, on the other hand, argued that the uncontrolled growth of technology, which is closely tied to science, can have negative consequences on humanity and culture by undermining certain mental processes and social relations. Both Snow and Postman's views highlight the idea that art and science may be seen as opposing forces, with some individuals concerned about the potential negative impacts of science on society and culture.
Expound: Both art and science are by-products of the desire to create and discover more data that is compressible in hitherto unknown ways!
Art and science both arise from the desire to create and discover new knowledge and understanding of the world. Both involve the process of gathering and analyzing data, whether through observation, experimentation, or other methods of inquiry. The main difference between the two is in their approach and focus.
Art tends to be more subjective and expressive, focusing on the emotional and aesthetic aspects of the world. It often involves creative interpretation and personal expression. Science, on the other hand, is more objective and analytical, seeking to understand the underlying principles and laws that govern the world through systematic observation and experimentation.
Despite these differences, both art and science involve the process of gathering and organizing data in a way that allows for new insights and understanding. The goal of both is to create new knowledge that is compressible, or able to be understood and communicated in a clear and concise way. So, in this sense, art and science are both driven by the desire to create and discover more data that can be compressed in hitherto unknown ways.
Art & Science as By-Products of Search for Novel Patterns, or Data Compressible in Unknown Yet Learnable Ways

Schmidhuber, 2009
Art and science are both ways of seeking and creating novel patterns and understanding the world around us. While they may seem like opposing fields, they are both driven by the desire to discover and comprehend the world in new and innovative ways.
Art, in particular, is often seen as a way of expressing and exploring emotions and experiences that cannot be easily conveyed through language or logical reasoning. It allows us to express and explore the intangible aspects of human experience, such as feelings, emotions, and values.
On the other hand, science is focused on understanding the world through empirical evidence and logical reasoning. It seeks to understand and explain the natural world through the use of theories, hypotheses, and experiments.
Despite their differences, both art and science rely on the ability to recognize and comprehend patterns and connections in the world around us. They both require the ability to find meaning and significance in the data and information that we encounter, and to use that understanding to create something new and innovative.
In this sense, art and science are both by-products of the search for novel patterns and ways of understanding the world. They both rely on the ability to compress and comprehend data in ways that are novel and unknown, and to use that understanding to create new knowledge and insights.
What's beautifulis not necessarily interesting or aesthetically rewarding though. In 
fact, many artists agree that good art does not have to be beautiful. A beautiful thing 
is interesting only as long as it is new, that is, as long as the algorithmic regularity that 
makes it simple has not yet been fully assimilated by the adaptive observer who is still 
learning to compress the data better. This is the subject of the next section. 
1.2Novel Patterns and Subjective Interestingness 
Babies, scientists, artists, and others do not use the same compression method forever, 
but improve it all the time, continually exploiting newly discovered regularities in the 
incoming data stream. It is possible to build artif i cial self-improving compression 
algorithms based on artif i cial neural networks and other machine learning techniques 
[17, 16, 30, 20, 21, 24, 26, 25]. For both artif i cial and biological observers we may 
def i nethetime-dependentsubjectiveInterestingnessorNovelty orSurpriseorAesthetic 
Reward or Aesthetic Value I(D,O(t)) of data D relative to observer O at time t by 
I(D,O(t)) âˆ¼ 
âˆ‚B(D,O(t)) 
âˆ‚t ,(1) 
the i rst derivative of subjective beauty: as the learning agent improvesits compression 
algorithm, formerly apparently random data parts become subjectively more regular 
and beautiful,requiringfewer and fewer bits fortheir encoding. As long as this process 
is not over the data remains interesting, but eventually it becomes boring even if it is 
beautiful. 
Discrete time variants of this concept have been implemented and / or described 
in detail earlier [17, 16, 30, 20, 21, 24, 26, 25]. One of the simplest ways of calculat- 
ing subjective interestingness is this: count how many bits are needed to encode the 
data before and after learning; the difference (the number of saved bits) corresponds 
to the internal joy or intrinsic reward for having found a new, previously unknown 
regularityâ€”a novel pattern.  -- Schmidhuber
In this passage, Schmidhuber discusses the idea that both art and science are driven by a desire for novelty and the search for patterns that can be compressed in new and previously unknown ways. He proposes that the subjective interestingness or novelty of a piece of data, such as a work of art or a scientific discovery, can be measured by the difference in the number of bits needed to encode it before and after learning. This suggests that as an observer's understanding of the data improves, the data becomes more regular and subjectively more beautiful, requiring fewer bits for its encoding. At the same time, the data remains interesting as long as the process of improving the encoding algorithm is ongoing, but eventually becomes boring even if it is still aesthetically pleasing. Schmidhuber also mentions that there have been previous attempts to implement and describe this concept in discrete time.
Its relationship to Bayesian decision making.
In Bayesian decision making, an individual or system makes decisions based on the probability of an event occurring. This probability is determined by considering all available evidence and using it to update their prior belief about the likelihood of the event. Subjective interestingness or novelty can be seen as a form of Bayesian decision making, in that the observer is making a decision about the interestingness or value of the data based on their prior knowledge and the new information they are presented with. As the observer continues to learn and update their understanding of the data, their decisions about its interestingness or value may change as they discover new patterns or regularities that allow them to better compress the data.
Summarize: Surprise! Measuring Novelty as Expectation Violation 
Jacob G. Fosterâˆ— 
Department of Sociology, UCLA, 264 Haines Hall, 375 Portola Plaza, Los Angeles, CA 90095 
Feng Shi 
Odum Institute for Research in Social Science, UNC Chapel Hill, 208 Raleigh St, Chapel Hill, NC 27517 
James A. Evansâˆ— 
Department of Sociology and Knowledge Lab, University of Chicago, 1126 E. 59th Street, Chicago, IL 60637; Santa Fe 
Institute, 1399 Hyde Park Road, Santa Fe NM 87501 
Abstract 
Novelty assessment is central to the study and management of innovation. Here we argue that new tech- 
nologies, discoveries, and cultural products are deemed novel insofar as they seem unlikely or improbable, 
conditional on perceptions of prior knowledge and estimations of the inventive search process. This implies 
that novelty has dif f erent manifestations in i elds with distinct prior knowledge and processes of invention; 
that measuring novelty is sensitive to context and therefore "objectively subjective." Consequently, dif f erent 
novelty measures will be appropriate for dif f erent i elds. We then survey and systematize existing ex ante 
novelty measures. We array them according to the speed of simulated search and the complexity of the 
space over which search is simulated. Using data from 157,595 U.S. patents granted in 2000 and 90,421 
patents granted in 1990, we demonstrate that inventive i elds vary in their distribution of novelty measures. 
We also i nd that familiar impact-based correlates of novelty are predicted by distinct characterizations of 
novelty in dif f erent i elds. Consistent with our hypothesis that dif f erent inventive processes imply dif f erent 
novelty measures, we i nd that nearby i elds, which share similar inventive processes, also manifest similar 
prof i les in the relationship between novelty and impact. We conclude with principles of measure selection 
that could lead to more credible analyses of innovation.
In this paper, the authors argue that novelty, or the perception of something being new or unexpected, is central to the study and management of innovation. They suggest that novelty is determined by how unlikely or improbable something seems based on prior knowledge and the process of invention within a particular field. This means that different measures of novelty may be appropriate for different fields, as the context in which something is being evaluated can impact perceptions of novelty. The authors also review existing measures of ex ante novelty, or novelty that is predicted before an innovation is produced, and demonstrate that different fields vary in their distribution of these measures. They also find that traditional indicators of the impact of an innovation, such as its relevance or usefulness, can be predicted by different characterizations of novelty in different fields. The authors conclude by offering principles for selecting measures of novelty that could improve the credibility of research on innovation.
Novelty is a critical concept in the study of innovation. Research in innovation studies, the sociology of 
science, economics, economic sociology, management, and allied disciplines highlights the role of novel dis- 
coveries, technologies, and cultural products across many settings, e.g., scientif i c and technological advance, 
economic growth, and corporate strategy. The modern academy def i nes itself through novel contributions to 
human understanding. Entire economic sectors are devoted to the creation and exploitation of novel devices 
and designs. 
Yet the measurement and modeling of novelty reveals a paradox. On the one hand, researchersâ€”including 
ourselvesâ€”often invent new measurements rather than build upon previous ones, or select from among the 
broad i eld of measures with limited justif i cation for one over the others. Uzzi et al. (2013) makes a pivotal 
contribution to our understanding of atypical combinations and scientif i c impact, but does so through 
introducing an atypical novelty measure. Cokol et al. (2005) introduces a novel, network-based measure of 
novelty with no reference to prior approaches, as do many others (Chen et al. 2009, Schilling and Green 
2011, Leahey and Moody 2014). Novelty measures are extremely heterogeneous and sometimes correlate 
negatively, as we show below. On the other hand, such measures are typically introduced as transcending 
particular empirical contexts, implying broad or even universal identif i cation of novelty. Both conditions 
cannot hold. Either each measure captures a distinctive but largely unspecif i ed aspect of novelty, or all 
measures capture the same thing, which varies by inventive and evaluative context. In this essay, we will 
argue for and empirically demonstrate the latter. 
Novelty measurement contradicts the pattern in many areas where social scientists have been systematic 
in developing measures for key constructs like income inequality (Cowell 2000) and residential segregation 
(Hutchens 2001). Invention, innovation, and discovery are multi-faceted, and defy simple, singular descrip- 
tion. Yet the l exibility in measurement choice raises a temptation to cherry-pick the measure that produces 
the "best result". Here we seek to identify and build connections between existing novelty measures by ex- 
ploring the conceptual underpinnings of novelty assessments in the service of making novelty measurement 
more systematic, cumulative, accurate, and conceptually clear. 
We i rst provide a formal theory of novelty production and perception as a phenomenon that varies 
by context (Section 2). Our theory is grounded in prevailing contemporary accounts of cognition, which 
place prediction at the heart of most cognitive processes (Clark 2015).Our central claim is that the 
production of novelty is grounded in the inventive process within a particular domain, and that novelty 
assessment derives from situated mental models of that process. What is deemed novel corresponds to what 
is surprising, given those models. There can be no universally objective novelty measurement because an 
object's novelty is def i ned as new in relation to the perceived (or measured) qualities of other objects with 
which it is compared. But there is no object collection universally relevant across all times and domains. 
Novelty without context is a numerator without denominator or a dif f erence without subtrahend. Moreover, 
perceptions and measurements are themselves collected with respect to anticipated evaluations, constraining 
our awareness of perceivable novelty.
Novelty is an important concept in the study of innovation. It is defined as something that is new or unusual, and is often seen as an important factor in the success of a new technology, discovery, or cultural product. However, measuring novelty can be challenging because it is subjective and context-dependent. Different measures of novelty are often used in different fields and may not be directly comparable. Additionally, the concept of novelty is closely tied to the idea of surprise, as something that is deemed novel is often seen as unexpected or unlikely given prior knowledge or understanding. In this paper, the authors argue that the production of novelty is closely tied to the inventive process within a particular domain, and that the assessment of novelty is based on the expectations and mental models of that process. They also argue that there can be no universally objective measure of novelty because it is always defined in relation to specific contexts or comparisons.
Schmidhuber: 2.1Art & Music as By-Products of Novel Pattern Search 
Works of art and music may have important purposes beyond their social aspects de- 
spite ofthosewho classifyart as superf l uous. Goodobserver-dependentart deepensthe 
observer's insights about this world or possible worlds, unveiling previously unknown 
regularities in compressible data, connecting previously disconnected patterns in an 
initially surprising way that makes the combinationof these patterns subjectively more 
compressible (art as an eye-opener), and eventually becomes known and less interest- 
ing. I postulate that the active creation and attentive perception of all kinds of artwork 
are just by-products of our principle of interestingness and curiosity yielding reward 
for compressor improvements.
Schmidhuber suggests that art and music serve as a means of discovering novel patterns in the world around us. These patterns can deepen our understanding and insights, and can also be aesthetically pleasing. According to Schmidhuber, the creation and perception of art is driven by a desire to discover and learn new things, and the resulting sense of reward or pleasure is simply a by-product of this process. In other words, Schmidhuber argues that the pursuit of novelty and knowledge is a fundamental aspect of human nature, and that art and music serve as a means of satisfying this desire.
A poem about oblique methods.
Oblique methods, oh so sly
Finding solutions from a different side
Thinking outside the box, with a twist and a turn
Creative approaches that burn and burn
Not bound by tradition, rules or norms
These methods defy all that is formally worn
They are unconventional, often misunderstood
But oh how they bring solutions, if only they could
Oblique methods, a breath of fresh air
Breaking free from the constraints of the standard affair
Embracing the unusual, the strange and the weird
They are the catalyst for innovative ideas, that's clear
So let us embrace oblique methods, with open hearts and minds
For they may lead us to answers we never could find
In the traditional way, with straight and narrow views
Oblique methods bring new perspectives, and that's something to muse.
Rewrite that as a 20th century late night sci-fi movie trailer.
"In a world where Reductionism has dominated the search for truth, one man dares to challenge the status quo. Armed with only his wit and a willingness to embrace the unknown, he sets out on a journey to uncover the secrets of the universe. As he delves deeper into the mysteries of the world, he uncovers a hidden world of emergent phenomena, where the rules of the game are rewritten and nothing is as it seems. Follow our hero on a thrilling ride through the unknown, as he discovers the power of oblique methods and unlocks the true potential of the human mind. Oblique Methods: coming soon to a theater near you."
More exciting.
In a world where everything seems to be figured out, there are still mysteries that remain unsolved. Enter the world of oblique methods, a cutting-edge technique that helps us delve deeper into the unknown. Follow a team of scientists as they push the boundaries of what is possible, using unorthodox methods to uncover secrets that have eluded us for centuries. But as they delve deeper, they realize that they are not alone in this journey - forces beyond their understanding are at work, pulling the strings from behind the scenes. Will they be able to unlock the secrets of the universe, or will they succumb to the dangers that lie ahead? Find out in "Oblique Methods: The Search for the Unknown," coming soon to a theater near you.
A very boring and flat summary.
Oblique methods are approaches that involve indirect or unconventional methods of achieving a goal. These methods can be used in a variety of fields, including science, art, and problem-solving. They often involve finding creative solutions to problems or making connections between seemingly unrelated ideas. Oblique methods can be powerful tools for generating new ideas and solving complex problems.
An acrostic poem about oblique methods, spelling out "Model-Free zoo".
M any methods exist beyond the typical models and formulas
O blique approaches offer alternative ways to solve problems
D iscovery, trial and error, and adaptation are just a few
E mergent phenomena can be studied without reduction
L earning and recognition can occur without symbols or logic
F ailures and successes can be remembered and applied
R eal systems and their effects can be studied without a model
E xperience and interaction shape innate knowledge and understanding
E verything from cobras to humans use model-free methods to navigate the world
Z oo of model-free approaches expands our understanding and capabilities
Redo that last part Z O O
Zoo animals are wild
Out in the open, free
Oblique methods help them thrive
Model-free ways let them be
Just three lines, withot actually using the word zoo
Oblique methods, a concept
Beyond models, truly exceptional
Intelligence, a new way to connect
STILL spelling out Zoo
Oblique methods bring
New ideas, no need for
Old models, they do
2 / 2
ChatGPT Mar 14 Version
. Free Research Preview. Our goal is to make AI systems more natural and safe to interact with. Your feedback will help us improve.
New chat

