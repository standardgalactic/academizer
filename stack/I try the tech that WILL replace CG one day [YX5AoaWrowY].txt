Thanks to Bones and all for sponsoring
a portion of this video.
This is not video footage, it's a render.
But it looks pretty photo real, right?
Like achieving this level of quality in CGI
would normally require a huge amount of time and effort.
But what if I told you that I easily made this
in a matter of minutes,
thanks to a brand new technology called Nerf.
No, not that kind of Nerf.
No, no, not that one either.
No, I'm talking about neural radiance fields.
And in this video, I'm gonna tell you
how they're gonna change filmmaking forever.
I've managed to make a career out of experimenting
with new technologies to create short films.
And along the way, we've learned tons of tools
such as deep fakes, virtual production,
and one of my favorites, 3D scanning.
Little known fact about photo scanning is that
when Ren does it, he makes a very specific face.
Ren, would you like to show them the face
that you make when you photo scan?
There it is, it's the most serious face
I've ever seen on Ren.
It's every time he photo scans, as always.
We've been using photo scans in our videos for years.
Being able to just take some photos
and then after a few minutes get a pretty good looking
3D model that I could then just use for whatever.
I mean, come on, that's a superpower.
But here's the thing, photo scanning has always been
just a shortcut on the road to rendering 3D scenes.
A lot of effort is still needed to set up the lighting
and make sure the scanned materials look correct
because out of the box, they just never do.
However, some things are simply impossible to scan.
I should probably not do that.
How do you feel about photo scanning this?
It'd be impossible, wouldn't it?
Yeah. Yeah.
There's too many reflections.
This is not gonna work.
Why wouldn't it work?
Photo scans rely on the pixels to be consistent.
Chrome ball, it changes every single angle you look at it.
Therefore, it's impossible to reconstruct.
Beautifully said.
Did you try it?
Oh, I'm about to.
Yeah?
I don't know how it's gonna go.
It's gonna be weird.
It's gonna be weird.
It's gonna be super weird.
I'm looking forward to it.
Me too.
I processed the scans.
The table got perfect.
Look at these frickin' chips here.
Look at these cookies.
However, the chrome ball, it's like a crunched up can.
The photo scan didn't even try scanning the bottle.
It just kind of got to it in the parts
that it could see that were transparent.
It's like, there's nothing there.
Photo scanning isn't magic, it's math.
And when the math isn't perfect,
you end up with unusable garbage.
I do not know how nerves are supposed to solve this though.
Oh, well, I got a video for you, Sam.
It's the end of this one.
When this one's done and uploaded,
you can watch it and then you'll be like, ah.
Sweet.
Yeah, you guys fast forward to that.
Now I'm gonna take these same photos
and instead of processing them with photogrammetry,
I'm gonna process them into a neural radiance field.
I have no idea how long it's gonna take me
to figure out how to do a nerf.
There's a lot of things I gotta download and install
and uninstall and reinstall.
And yeah, I'll make a nerf.
Okay.
I've made it through all of this all afternoon.
If this works, when I hit enter,
this will be my first neural radiance field.
Okay, okay, I know these results are a little janky.
It's kind of blurry.
There's all these weird floating artifacts
and holes everywhere,
but the ball has accurate reflections.
It's even got the transparency of the bottle.
So here's the photo scan, but here is the nerf.
After I tweeted out these results,
freakin' Paul Franklin of all people responded
that he could see a VFX revolution on the horizon.
And he knows a couple things about VFX,
having won the Oscar for best visual effects, twice.
To understand why visual effects artists
are excited about nerfs,
we first need to understand how light works.
The holy grail for any CG artist is photorealism.
To make an image so utterly realistic,
it convinces your brain that it is real.
So what is the secret to photorealism?
Reflections.
If I was to ask you to point out
all of the reflections in this scene,
you'd probably point out the balloon,
maybe the bell, or even the water.
But look, the white wall is bouncing back
some of the red of the painting.
In fact, every object is reflecting light
from every other object.
I mean, think about it.
Everything we see is just light
that has reached our eyeballs.
Now, unless you're looking directly at a source of light,
ah, the sun, oh my eyes.
It first has to reflect off of something along the way.
So this was a trick question because technically,
everything we see is a reflection
because reflections are how we see stuff.
They play a subtle but crucial role
in convincing our brains that stuff is real.
Check out this photo scan of some pans.
Now check out this nerf of the same pans.
See the difference?
As the camera moves around,
the reflections change realistically
without having to add any CG lights.
And that's because nerfs rely on neural rendering,
just like deepfakes do.
It's able to learn what the color of every point
in a 3D space is depending on where you're viewing it from.
And when you have an entire field of these points,
you end up with something that looks shockingly similar
to the photos used to make the scan.
Now let's compare the nerf to a real video
and they look, well, they kinda look the same
because it was literally trained on this exact video.
And that is the entire point.
It's the ability to quickly, easily, and cheaply
replicate reality in a way that looks like video.
This portion of the video is sponsored by Bones and All,
a new movie coming out only to theaters this Thanksgiving.
It's a new movie about a couple of young people
who fall in love, who are also cannibals.
No!
This movie stars Taylor Russell
and of course the doon man himself,
The Story revolves around Marin and Lee,
two wayward kids trying to find their place in society
as they munch down on people.
They embark on a liberating road trip
as they try to figure out what their place in society
really is, because I mean,
where is a place for people who eat people?
It is a love story, but it's also like a horror thriller movie.
It's like bending these genres like frickin' Picasso.
The performances are incredible,
the cinematography is good, the directing is excellent.
It's directed by Luca Guadagnino.
If you're a true cinephile, chances are
you're gonna love this film, so check it out
in your local theater, this Thanksgiving,
or go to bonesandallfilm.com to get tickets today.
Once again, thanks to Bones and All
for sponsoring this section of the video.
Now it's time to get back
to this crazy new scanning technology.
My journey with the Chrome Ball started over the summer
with NVIDIA's instant nerf program,
but I had to learn a bunch of Python coding
and use a command terminal just to get the thing to run.
Now however, it's as easy as using a phone app
thanks to Luma AI.
It's like Polycam, one of my other favorite apps,
but it's for nerfs.
You take a bunch of different photos
and then you upload it to their server.
A couple minutes later, you get back a nerf.
It's great, and it's easy and it's fast,
but I need to be able to import
my own camera animation separately.
And that's something I can actually now do
with the website version of this app on my computer.
And that opens so many doors.
And I'm gonna spend the rest of this video
walking through some of them.
Apparently not that one.
So I know what you might be thinking,
what can you do with this new nerf technology?
Well, one of the more obvious and easy things to do
is just a simple background replacement.
So Niko here is standing in front of our X green screen wall.
Don't worry about it, I'm gonna road to him.
Typically most people when they're shooting
on a green screen, the camera's gonna be locked off
because the background needs to match the foreground
and the moment you add motion to everything,
you're gonna have a hard time.
The heck?
Like me, right now.
What we're doing here with Niko
is we're going to take the camera out of the tripod
and move it around.
I'm gonna be using bi-play camera app
to 3D track the scene as I record the footage here.
And then I'll replace the background
and make him look like he's anywhere.
I'm in a nerf.
So another idea you can do with this type of scanning tech
is like portals.
What if this door turned into a portal?
This is really a door to another world.
So you first gotta start by 3D tracking your shot.
This is all pretty standard.
Now I can take that exact same camera move
and apply it to any radiance field that I want.
Whoa.
Yo.
What?
Oh, man.
Now it's a new one.
How is this working?
And if, can I even peer through it?
Whoa.
No kidding.
Sick.
Oh, wait.
Where'd it go?
I wanna go back.
I had a shot for 40 seconds.
This is a 40 second one take the effect shot.
And it's not even a thing.
I mean, it was a bit of a thing,
but not too much of a thing.
So going back to the stuff I shot with Nico,
here are a few things I learned.
I was just having fun rendering out shots
because it was so easy just to take the exact same camera move
and drop it in another scene.
Whoa.
Look at that.
Wow.
What a spot.
Look at that.
That's crazy.
Like I put him on top of a mountain
where I like to go one-wheeling.
I put him in my kitchen.
I put him in a garden at night.
And I put him in a garden at night.
I put him in a garden at night.
I put him in a garden at night.
The nerfs work really well at night.
Have you ever tried photo scanning anything at night?
It doesn't work.
It doesn't work, but this worked.
Another thing is just how subtle reflections can really be.
I know we talked about this earlier,
but it's what makes all of this stuff just sell so well
is because the wall slightly getting brighter
when a reflection gets a little bit more direct
to your eyes.
That's how light works.
It just, it looks like footage.
This better look photo real,
otherwise I'm going to be disappointed.
Whoa.
The camera just kind of went through the geometry
there a little bit.
It's not geometry.
Those aren't polygons at all.
Those are radiance fields.
And lastly, check out the shot of Nico
on my kitchen table next to a plant.
You know, a 3D camera move is still something
you can scale down or up.
And that gave me an idea.
What if I scaled the world down to make someone big?
Ryan, could you scale me back down please?
Or like scale the room up or something?
I don't know how you did this, but I don't like it.
I'm gonna toss your bowl.
You must go bigger.
Bigger!
You don't have to use your phone.
It's just based off of images.
They can come from anywhere.
So I'm gonna use my drone.
What I'm looking to get here
is I'm basically trying to get a scan of that big building
so that I can then comp in a shot of a person all big.
All right, I capture the nerf.
Now I gotta capture myself
so that I can 3D track this shot and put it in the nerf.
I'm gonna have a spin around me.
Gnaw!
I'm king on!
Ah!
All right, we got something there.
Ah!
Ah!
I am the king of the world.
Yeah!
Ha, ha, ha!
What sells this shot for me is of course the lighting.
I captured both the plate and the nerf outdoors
at golden hour and the nerf has captured
all of that golden sunset goodness.
Pro tip, if you want your nerfs and plates to match,
capture both of them with the same lighting conditions
and ideally the same camera as well.
So nerfs get you cheap photorealism,
but it's not just any photorealism,
it's a specific flavor that's tuned to the camera
that actually captures it.
So I'm gonna do an experiment
comparing the scans from a red camera versus a webcam.
This is the stupidest thing I've ever done.
My theory is that one scan will have a cinematic look
and the other scan will have a bit of a cheap webcam look.
Ha, ha, yep, that's exactly what I expected.
You know, you got compression, dynamic range,
the quality of your lens.
All of these will have an impact on your finished scan
and speaking of cinematic,
the dream of a gimbal operator is get like
the smoothest most cinematic shots,
but sometimes for whatever reason,
maybe you're not able to get that sort of smoothest new one.
So I'm gonna get the most ridiculous camera move possible
here, like this is not gonna be a good shot,
but the magic of the nerfs is that it can do this.
For comparison, here's what warp stabilizer did
with the shot.
I'm also not limited to simple camera moves either.
I can get as complex as I want.
Do you remember frozen crossing alpha?
Everyone's frozen in time
and the camera's moving through that.
When they needed to speed up the shot,
they would take the footage and fast forward it,
but then you get the,
but if they were able to just do a radiance field scan
of the whole thing,
they could pick and choose their camera moves after the fact.
This is the nerf.
Dude, whoa!
Your face gets a little weird here up close.
Oh dude, it's a little bit smushy.
Can I take this and extract geometry from it still?
Yes, you can extract the geometry,
but you lose all of the reflectiveness
because it all gets baked down into a single diffuse texture.
And so that texture then doesn't change
as you orbit around and view it from different spots.
It's a little like, you know,
being handed the keys to a Rolls Royce
and then being like, sweet,
I'm only gonna need the hubcaps.
Hmm, it's almost like you're leaving
the most valuable part behind.
Exactly, the most valuable part of a nerf
is the actual neural render.
That's what you get,
that photogrammetry doesn't give you.
Nerfs are at their most powerful
when we treat them like video footage
rather than a scene to pull textures and models out of.
But it does allow you to do something cool,
like a collision geometry for simulation.
Yeah, dude, drop a cloth on that.
Now you're getting the best of both worlds.
You're still utilizing the sort of photogrammetry mesh
of the whole scan,
but what we're looking at is still the neural render.
Mm-hmm.
Oh, it'd be so sick
if you could apply motion blur inside of the nerf program
and depth of field inside of the nerf.
Oh, dude, you're totally right.
Like, it's so early days.
The tools are so rudimentary.
It feels like I'm trying to carve a statue
with other rocks.
It's like, technically I can get it to work,
but it's just taking a lot of pounding away at it.
It is literally 12.44 a.m. right now
because this video needs to be done tomorrow.
It's been difficult to explain this to people.
Why are there like jaggy edges and stuff?
It's kind of weird smearing-ness in the background.
It's a little bit smushy.
It's better look photoreal,
otherwise I'm gonna be disappointed.
Why is your green screen white?
See, it's not about how perfect the results are right now.
It's about the potential of these results, you know?
I have a very strong belief
that it's just a matter of time
before no one will be able to tell the difference
between a nerf and actual video.
I think I need one more shot,
something to like really demonstrate
how crazy useful this new tech can be.
Ah, ah, there it goes.
Yep, I guess I'm not dreaming.
I think I know what our final shot's gonna be.
We're gonna try to do an inception shot.
You know the shot in inception when like Paris folds over?
I'm doing one of those, kind of.
I'm doing a poor man's interpretation
of the very advanced effect,
but it's gonna require like a thousandth of the effort.
Ren, are we doing an inception shot
because you're trying to impress Paul Franklin?
I'm not, not doing that.
Okay, we don't really have access to this spot,
so I'm just gonna record a quick shot,
now to quickly get a rebel scan.
The rebel scans have gotten away from us this time.
Because of all the chaotic stuff going on here,
if I wanted to actually shoot a video,
it would have been too hard.
Fortunately, I was able to get a scan,
so in theory, I can go back
and actually do all the production filmmaking
after the fact.
Car.
["The Star-Spangled Banner"]
And there you have it.
I hope at least some of these ideas
have inspired you to pay attention to nerfs.
I mean, there's a lot of potential here,
and I've just barely scratched the surface.
It's our job as artists to push this tech
as far as it will go.
None of these shots in this video were like production ready,
but I hope that maybe they inspire
some sort of nugget of an idea in you,
so that you can get into this stuff as well
and push the bounds too.
Of course, I recommend Luma.
It's what I've been using
for the majority of this video,
but my buddy James helped me with
Nvidia's instant nerf thing.
In fact, I think he has a plugin out
that allows you to control all of that stuff
inside of Blender.
And also, as of today,
Polycam announced a partnership with Nerf Studio.
There's tons of options,
and if there's anyone out there
not yet convinced of the potential
of this new technology,
I implore you to just look two papers down the line
to see where this tech is going.
It's exciting.
What a time to be alive.
We keep calling it nerfs,
but I would like to make a point right now
to change the name, because I think nerfs
are a stupid name.
Let's be honest, it's confusing.
I think a better, more apt name would be lightfields.
Lightfields is a much cooler name.
Technically, it's kind of the opposite
of a Reagan's field,
but technically, lightfields are a little different.
But in the end, you're still getting
the same sort of result,
and so I think lightfields are a much cooler name than nerfs.
I implore everyone to use that name instead,
because who cares?
