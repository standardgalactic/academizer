Alright everybody, my name is Runar. Thank you for coming. I'm going to talk to you about
a new programming language that I'm working on called the Unison. So who am I? I am one
of the co-founders of a company called Unison Computing and we are working on the Unison
programming language in Boston. I'm also one of the authors of functional programming
in Scala. And one of my other co-founders of the company is also my co-author on the book.
Paul. Alright, so what is Unison? Unison is a programming language. It is superficially
similar to Haskell. It has a permissive MIT license that is all being developed in the
open. It's an open source thing. And it's sort of a modern statically typed functional language.
And whereas it looks very similar to Haskell, it has some unique features and constraints
that I want to tell you about. And so big disclaimer at the front. This is very work
in progress. Like there is basically no tooling for this language. Libraries, there are basically
no libraries for this language. We are actively working on it but we're very excited about
it and so I want to tell you what we're doing. So like this is not for production. At least
for like another year or something. Okay, so here's the plan. I'm going to tell you
about Unison and why we're making it. I'm going to show you a small Unison program as
an example. Then I'm going to tell you about what is Unison's sort of defining feature
maybe or sort of its most unique thing which is its novel appers to names.
Oh, really? Yeah. Oh, that sucks. Okay, I'll just stand very straight. Okay, I'll just show.
Okay, so then I'm going to rant about error messages. I'm going to hate on monads. And then
I'm going to tell you what we're doing sort of the near term for Unison. Okay, so why are we
making a new language? So to tell you more about that, let's go back to the beginning. We're
at the Retro Computer Museum. So when I was a kid, I loved programming. And I had one of
these. It's a Sinclair Spectrum. It had 48 kilobytes of memory. It was awesome. Programming
these machines was hugely fun. And I loved like seeing my software creations coming to
life on this little machine. And it captivated my young imagination. It seemed like anything was
possible. And it also seemed like anyone could do this. And I knew at the age of seven when I had
this computer that I wanted to be a programmer when I grew up. And like even now just like looking
at this object just fills me with joy. So then, you know, I got my first programming job and I
learned that programming professionally was nothing like this. So it was long hours of staring
at poorly organized code thinking, you know, who the heck wrote this? And usually I found out that
it was in fact I who wrote it. And, you know, whereas there were moments of, you know, of sort
of this sort of sheer joy of programming, the overall experience was often one of frustration
and stress and a sense of sort of overwhelming complexity, a sort of drowning in object oriented
stuff. So then I came to functional programming. And, you know, building software from compositional
pieces that neatly snapped together was great. And programming was fun again. And again, you know,
I get this sense of joy when I'm writing software. At least when I'm writing something small and
isolated, something I can readily understand, something that has a cohesive logic to it and
runs on just a single machine. But these days, if you're writing any kind of large scale application,
you need to make lots of programs all talk to each other. And software systems these days are
typically not big monoliths. There are lots of different programs that run on different machines.
And they all need to communicate and coordinate. So obviously, you know, you could up some JSON
serialization logic or some protobufs or something. And like this doesn't scale at all, you find.
And like soon you have lots of little servers that all interface with like different versions of
each other. And you have to hire like an army of programmers to write like all that JSON and
protobufs or whatever. So you get them to talk to each other. And it's just like a mess. And then
like it's not just protobufs and JSON. It's like lots of different technologies like this. And it's
a lot of like duct tape and bailing wire to make modern software systems work. Right. So there's
this sort of gap that exists between the things that we can specify using programming languages,
using, you know, functional programming. And then the things we need to actually talk about in order
to build a real large scale system. And we're sort of just filling that gap with various
special purpose technologies that are sort of ad hoc and don't really have a cohesive logic to them.
It's not, you know, beautiful and compositional like functional programming is. And like I don't
know about you, but like every time I learned one of these, like how to use one of these, I
immediately forget. Because it's just like totally arbitrary. And it's not my programming language.
You know, like I don't get to talk about these things in my programming language.
Because our programming languages haven't really caught up with the internet age.
Programming languages still only really generate generally talk about what a single OS process
is doing. But with Unison, we want to revisit that assumption. We want to open up the question of
what a program can even be. And so a Unison program actually describes an entire distributed system,
including deployment and orchestration, scaling up and down and failover. And it could be like an
elastic distributed computation, deploying itself to, you know, dynamically to any number of machines.
So whereas today you might write lots of little microservices and maybe come up with, you know,
protobufs or JSON or whatever to get them all to talk to each other. And then you write a
bunch of YAML files to describe the deployment and orchestration. And you write managed versions
of your builds and your various configurations. And you have to appoint a build czar and stuff.
So instead of that, you write a single program for your whole system. And then you deploy the system
by running that program. So the sort of grand vision of Unison is that it's like the whole sort of
cloud as a single global supercomputer that you program simply and directly as if you were just
programming your little microcomputer. So let me show you an example of some Unison code.
So here is a distributed batch mode MapReduce in Unison. So this code on the slide does something
similar to Hadoop. So if you're familiar with Hadoop, this does something similar to that.
So this function will take some data set, which here is a list of values called list.
It takes a function f to perform on each of the elements. So we map that over the list.
And then we have a monoid with which to reduce in a sort of distributed fashion. So this would
potentially spawn computations on a bunch of nodes and then accumulate in the monoid in a
distributed way and then return a single result to the present node. All right, so here's a type for
it in Unison. And here the argument f has access to this remoting ability. So it could be spawning
a new node or pulling a node from an autoscaling group or a node pool or something. The important
thing that is able to execute some strategy for distributing and coordinating this computation
in a sort of a first-class way. And you can, of course, write your own such strategies.
So I'll talk a little bit about what the stuff in curly brackets mean, but they are abilities or
effects that we track in the types. So this is part of Unison's effect system.
Anyway, don't worry if this code isn't immediately comprehensible. The point is that
a Unison program that does something interesting like distributed batch mode MapReduce
that includes deployment and failover and so on, doesn't have to be any more complicated than this.
And it looks very similar to the program that you might write to just do this on a single machine.
So the sort of magic that makes this work is that Unison can transfer an arbitrary computation
including its dependencies to a remote node. So the remoting API has a primitive called at.
And we just give that some location here. I'm giving it the US East production location.
And then we give it a quoted program. So note the apostrophe there, like I'm quoting this program.
So it's not computing a value. It's capturing this as sort of a lazy computation.
And Unison will transfer that whole program to the remote location and execute it there.
So click count here. So the idea here is that I have some log files.
You know, I'll have some way of calculating a click count from my log files and then I can
accumulate with some monoids on the click counts.
So click count here could be spawning nodes or pulling them from autoscaling groups or whatever.
And it'll have access to the full remoting API.
So when Unison runs this, it sends a syntax tree over the network.
But here's the thing. Will the remote node know what we mean by MapReduce and click count?
Et cetera. Will it attribute the same meanings to those names as we have on the local machine?
The answer is actually that it doesn't matter because Unison doesn't really deal in names.
The human readable names that you see on the screen are just used to communicate with the human
program. But when Unison communicates with other Unison nodes, it sends Unison names,
which are deterministic cryptographic hashes of the reference.
So Unison actually identifies a function or type by a hash of its definition.
So any piece of Unison code can be uniquely identified by taking a hash of the syntax tree
after removing all the names. So names are just metadata that we just sort of attach to the
hashes. But the hashes are sort of the true immutable eternal names of the syntax trees.
It's sort of like the Unison expressions are like stars in the sky and we can discover them
independently and like give them different names. But the stars are just always kind of there
for us to find. So the fact that every expression is deterministically identified
by its hash lets us unambiguously communicate code as data across both time and space.
So across time, it doesn't change over time, the hash of a given expression.
So it's durable and also across space. That is, we can send that across the network
and the meaning doesn't change based on the location or what's sort of available at that location.
And so when we send a hash to a remote node, the remote node may know what that hash means
because it has seen it before. So it'll have it in some kind of cache or it may respond with
I don't know what that is, please send me at the syntax tree.
Okay, so yeah, it's been said that there are two hard problems in computer science.
There's cache and validation, naming things and off by one errors.
And in Unison, naming is a lot more flexible and it matters a lot less than in other programming
languages. So let's look at this bike shed for a minute.
I really like the color of this bike shed. So yeah, I've been in a lot of heated bike shedding
sessions about what to name something. And you know, in Unison, the goal is that we'll never
have to do that again. So if you don't like the name of something, you can just change it locally
and renaming things is sort of a trivial operation, you just change the metadata on your hashes.
And it's a non breaking change for everyone. And in fact, you'll be able to publish naming
schemes as libraries for others to consume if they if they wish. Just to drive that home here is
an example of two functions that have the exact same implementation. But they have two different
names. One of them is called Curry and the other is called Schoenfinkel. In Unison, these are actually
not two different functions with the same implementation. They are the exact same function.
If you have both of these definitions in your code base, Unison will just create metadata that
contains both of those names. And it will sort of save that in a database of the code on that note.
So a Unison code base is a database. And that's an implication of what I've just said.
So Unison code base is not a mutable bag of text files that we, you know, mutate using a text
editor. We still use a text editor to editor code. But the code base is a structured immutable object.
And that gives us, so the whole model of the code base is kind of a separate talk.
But the structured object approach gives us incremental compilation, kind of for free,
that is perfectly precise. And you'll almost never spend time waiting for Unison to compile
your code no matter how large your code base is. So it's constantly compiling the new things that
you add. And you'll never have to recompile anything unless you really want to wait.
Refactoring, the idea is that refactoring should always type check. And it's sort of a controlled
experience. So you can precisely kind of measure your progress through your refactoring. Arbitrary
changes to a code base can be completed without dealing with like a depressingly long list of
type errors, you know, this sort of type error directed programming that we often do in languages
like Haskell, where you have like 72 errors. And then you have to just power through them until
your code compiles. In Unison, we're hoping you'll never have to do that. Yeah, so code
bases can be worked on concurrently by multiple developers. Situations that traditionally
result in merge conflicts or build issues can no longer occur.
Oh, so the question is like, how does that happen? How is that merge conflicts not happen?
Yeah, so the code base is like not a mutable back of text files on your disk. It's a database.
And then sort of each function will have not the text of the function in the database,
it will have the syntax tree of the function in the database. And so if you, for instance,
I don't know, you change the definition of something, you actually just add a new hash
with a different definition to your database. And so you're not merging text files
ever. And so lots of merge conflicts that usually happen, they don't come up.
I'll get to the versioning thing here in a minute. Okay, so, yeah, renaming is totally trivial,
even like bulk renaming, lots of things, it's 100% accurate and it's fast.
And also this sort of dependency hell thing is vastly reduced. Many situations that contribute
to dependency hell simply don't arise with this model. To illustrate that, let me show you the
sort of situation of a traditional kind of dependency hell situation. So it's kind of this
diamond dependency problem where your program depends on two libraries, A and B, and they both
depend on some library C. But then they diverge. And now they depend on different versions of
library C that are incompatible. So you end up with a conflict. So much of the time, the problem
is just that the libraries aren't really granular enough. That is, in a situation where library A
and B depend on totally different parts of library C, that should just work. And in unison it does
because unison actually tracks dependencies at the level of individual hashes. The idea of a library
is different in unison. And it's more of a first class thing. So actually what happens in unison
is that your program depends on some hashes. And then those hashes depend on some other hashes.
And as long as they don't clash, everything's fine.
And even if they depend on the same data type in two versions of a data type in a library,
we'll be able to resolve that because you should be able to just use them as two different types
because they have two different hashes. However, if you try to pass off the version three of C.fu
as an instance of version four of C.fu, unison will just give you an ordinary type error.
It'll just tell you these two types are not the same. There's a type mismatch. And here it is.
Okay, so that brings me to unison's type system. So let's talk a little bit about that.
So this is our type system. We're implementing this paper,
Complete and Easy Bidirectional Type Checking for Higher Rank Polymorphism by Josh Dunfield
and Neil Krishnaswamy. It is a really cool type system. I encourage you to check out this paper.
It's bi-directional, so it's not like a unification-based type system, which makes it pretty cool.
And we're also implementing type error provenance that we harvested from a talk by Leonard Auguston.
And that's also really cool. And those things together allow us to give really good error
messages to programmers. Now I promise that you'll usually not have to look at pages and pages of
type errors and feel the anxiety of having 72 errors that you have to fix before your quote
compiles. But when we do give the programmer type errors, we want those errors to be helpful.
Because it's kind of funny that we're almost a quarter way into the 21st century, and our
tools are still giving us errors like it's the 1970s. It hasn't really advanced from the old
computers that are out in the hall. For instance, when Haskell gives you a type error, you almost
just hear this robotic voice, and it's like doing this error or cannot metric backdate type.
Error messages are needlessly technical and terse, and they don't necessarily help you figure out
what the problem is. So I think that an error, a type error, should read like another developer
explaining the problem to you, ideally. So here's an example of a language not doing that.
I don't hate on Haskell. Haskell is my favorite programming language in the world. I love it.
And it generally gives really good error messages. Like this is good compared to a lot of other
languages. But yeah, so I have this case expression here at the bottom. Case x of 3 goes to 4,
and 4 goes to surprise. And Haskell will say, well, I don't have an instance for a number of list of
car. I'm like, what does that mean? I don't know. So Unison will give an error like this. It'll say,
well, all cases of a case of expression must have the same type. Here, one of them is Nat,
and the other one is text. So Nat is the machine level natural numbers in Unison. So there are
64-bit natural numbers. So it's just to jab Haskell a little bit. Here's Haskell of us.
Everybody know what it does? Any. Of course. So Haskell is like way ahead of the game. It's like,
this is not even an error. Like, I'm not even gonna have it. Okay, so also I think an error
should make it obvious how to fix the problem. Because we spend a lot of time looking at type
errors, and we spend a lot of time mapping what the error says to the task of fixing it.
So here's an example of Unison doing that. So I forgot to put an append operator between hello
and name here. And Unison is like, well, this kind of looks like a function call,
but there's a text where the function should be. Are you missing an operator? And I'm like,
yeah, I am. Thanks. So here's the Haskell for comparison. It's just like error war.
And it gives you a bunch of, it just bombards you with information.
And then it has like at the bottom relevant bindings include, and like they're all totally
irrelevant. Haskell is amazing. I love it. No, really. Haskell is beautiful.
Okay, so I think that a type error should generally tell you three things. It should tell you
where was the mismatch? What were the types involved? And then where did those types come
from? All right, well, was there a question? Do we pay a cost in what performance?
Probably. Yeah, probably. I mean, yeah, our errors are very structured objects.
And so at compile time, we're accumulating a lot of information. And so yeah, it's probably slower.
But it's not as slow as trying to figure out what an error message means, I think.
But yeah, I think that that's not something we're trying to optimize for right now.
Anyway, so I should give you where is the mismatch? What types are involved? And where did
those types come from? Why does Unison think that those types are involved at all?
So here's Unison doing that. So here I have a function foo that takes a number of arguments
and they all have to be of the same type A. And I've given it, you know, one, two, three.
And then the text, hello. And Unison is like, well, the fourth argument to that function
is text, but I was expecting it to be NAT because the function has the following type, where
A is NAT because I found it there. And so we try to always do this kind of thing.
To inform the programmer about why does the compiler even think that this type
is what it thinks it is. Okay, also if the solution to an error is trivial or obvious,
we should just fix it. And just like not even tell the programmer. Because like,
the way that the Unison codebase model works is like once you submit your source to the codebase,
we destroy it. It's obliterated. So if the solution is totally deterministic, trivial,
and obvious, we just fix it and we throw away the source code and you never know that you made
a mistake. And that's totally fine. For instance, if a name is ambiguous, and only one of the
solutions to that name, type checks, Unison will just pick the one that type checks.
So we're doing type directed name resolution. So that's kind of cool.
But if a name is still ambiguous, so it's able to figure out a bunch of solutions to that.
So more than one solution type checks, it'll tell you helpfully which things it knows about
the fit the type and the name. So for example here, I have said like a polymorphic function
that applies plus, but it actually doesn't know what the types of A and B are, the things I'm
trying to add. And so it's like, are you trying to add natural numbers? Are you trying to add
integers? Are you trying to add floats? Like, what's the deal here? And so you need to either
disambiguate by giving it a namespace, or you have to give a type annotation.
All right. So I'm just saying, well, whatever this thing is, it conforms to a type like that.
Great. So this actually gives us typed holes as well, kind of for free.
So here I have misspelled append, and Unison helpfully says, I'm not sure what append means,
but whatever it is, it has this type. So it's something that takes two streams of natural
numbers and smashes them together somehow. Okay, great. So another thing we really wanted to
be thoughtful about was Unison's effect system. Because, I mean, let's be honest, monads are
awkward. I came out and said it. Monads are awkward. They come with a syntactic overhead,
as well as a cognitive overhead. A lot of the time, you spend your time trying to figure out
how to lift this thing into the monad you want, or in which order is my monad transformer stack
supposed to be, and things like that. So Unison uses what's sometimes called algebraic effects.
We modeled our effect system in a language called Frank, which is detailed in this paper,
by Sam Lindley and Connor McBride and Craig McLaughlin.
And Frank calls these abilities rather than effects. And so we do that too. We call them
abilities. So here's a simple state ability. This is the ability to put and get some global
state of type S. So abilities are introduced with the ability keyword. And this defines
two functions, put and get. So put takes some state of type S, and it returns a unit with the
state ability attached to it. And then get will give you that S, given that you have the state
ability. So when we see a thing like this in curly braces, it means this requires that ability.
So put requires the state ability, get also requires the state ability.
Okay, great. So this is very similar to just like an algebraic data type,
where you just, you know, you're defining this type state S, which is sort of this
ability type. And these are the sort of the constructors of the type put and get.
So we can, for example, write effectful functions that push and pop on a sort of global stack.
So given that the state is a stack, that we have the ability to
manipulate some state that is a list of A's, we can pop and push.
So note that there is no monadic plumbing here.
They're just sort of code blocks. And so to pop, we get the stack, we drop one from the stack,
and we put that, and then we get the head of the stack. So that's pop. And then push, we just say,
well, cons A onto the front of whatever we get, and put that.
All right, so there's no applicative syntax or, or, or anything like that. Because we're
actually overloading the function application syntax. So in unison, applicative programming
is the default. We, we sort of chose that as the, as a design constraint.
So the types will ensure that you can't go wrong here. You're not talking about the wrong thing.
So for example, whereas in Scala, you might say, well, A comes from X, B comes from Y,
and then C comes from Z, and then you want to do F of A, B and C. In unison, you just say F of X, Y,
and it will just figure that out. It will do the pulling out, and, you know, it'll do all the,
the effects for you. Sorry. Oh, that's, there was a question about why is put quoted here?
Yeah, pop is quoted. The reason is that only computations can have effects, not values. So once
you have computed a value, you can no longer have effects. So the quoting is just a, a nullary
function that returns whatever this evaluates to. All right. So that's a great question. Thank you
for pointing that out. Okay. So in Haskell, you might have to do things like, you know,
this applicative syntax. And in unison, that's just the default. You just use the,
the function application syntax. And in fact, it works for sort of arbitrarily lifted or mixing
of pure things in. It's still the same, even if you're like doing lift, lift and stuff.
And then bind is this function application. So whereas in Haskell, you may have to say x bind,
you know, land of a f of a and then by G. In unison, you just say to you have effects.
So that's kind of nice. No, it's, there's a low syntactic overhead to this. And there's a low
cognitive overhead to this for the programmer. So the programmer can just, you know, use our pop
and push and write a little program that pushes and pops a stack using our state ability. So given
that we have the ability to manipulate some state of type list of NAT, we can write this sort of
stack program and we can, you know, pop an a and then check if that is five and then push five
if it is, otherwise do some other stuff.
Kind of cool. So then to handle the state ability is to make it actually do something,
we write a handler using the handle keyword. So this here is a pure handler for the state ability.
And we can use that handler then at the bottom, the run stack thing uses that handler to run the
stack program given some initial state, which is five, four, three, two, one.
So normally this kind of stuff will be hidden away in library code. Like,
you know, most, most programmers will not be writing their own handlers. But if you, you know,
if you have your own sort of abilities and things, you'll be able to write your own handlers.
So this proceeds by pattern matching on the sort of constructors of the effect of the ability.
So here, you know, the definition of state is like handle H of S in
Bank C, where the exclamation point means force this computation. So it's a C is some quoted
computation. You can see that it's quoted in the type. It's something of type state SA.
And then I'm saying like, force that, like actually evaluate it, but handle using the
handler H or H of S, where S is the initial state coming in, you know, it's that five, four, three,
two, one thing. And then I, we just proceed by pattern matching on the, on the ability. And we
say, well, if, if the call was to a get, then we end up in that, that case. And what we, what we
get out of that pattern is a continuation for the program. And what we do is we either call
that continuation or not, and we call it with the result of whatever we do when we get.
So this is a recursive definition here. So it's actually calling the handler again,
because K of S might also need access to the state ability.
And then to put, we get a state that somebody wanted to put, and we get the continuation
of the program. And we say, well, handle using that new state, and then continue by passing the,
the unit to the, to the continuation. Okay. And then in the pure case, when there's no
effect, we just return the value that, that we ended up with. Cool. So here's another example
of handling. So here I'm handling an ability into another ability. So here I have some ability to
receive text. And receive has a single constructor, just receive dot receive. And I just get a
continuation. And my job is to pass whatever I want the caller to receive to K. So I just say
K of read line, where read line is in IO. So read line requires the IO ability,
which I'm not handling here. And so this whole feed function will require the IO ability.
And so I'm sort of turning this receive text effect into an IO effect.
So if you try to, for example, use IO in pure code, and you don't have the IO ability,
you get an ability check failure, which warms my Dungeons and Dragons heart.
I don't know if there are any D&D nerds out there, but okay. So yeah. So this is saying,
this location requires the IO ability, but doesn't have it.
All right. So we can't read line in a place that where we, so here I'm declaring that I have the
IO ability or that I require the IO ability. But then if I don't do that, I get a type error.
And yes, you can still use monads if you want. You don't have to use this ability stuff.
You can still just use monads and it'll work just fine.
Okay. So where are we with this? As of right now, we have, you know,
lots of unison written in both Haskell and Scala. We have a Lexar, a Parzer.
We're, you know, we have the hashing thing down. We got like serialization of code.
And we got the bi-directional type checking and the type-directing name resolution,
type error, provenance and all that stuff. The languages, algebraic data types and abilities.
We implemented ability polymorphism. So you have polymorphic abilities.
We're giving you structure type errors and type error provenance.
We've implemented a JVM-based runtime that uses partial evaluation.
And it has tail calls, which is awesome. And we also have a native runtime reference
implementation that is really slow and runs out of memory really fast, written in Haskell.
But it's sort of a demo of what a unison runtime should look like.
Okay. So, you know, it's coming together. It's looking like a real language now.
And we feel super excited about it. And then the near-term roadmap for us is to implement the
sort of unison command line interfaces, like a code-based editing tool and a REPL.
You know, work on the distributed runtime and write some actual libraries,
you know, documentation and, of course, a sweet-looking website with, like, you know,
things, JavaScript-y stuff flying across with tutorials and examples. All right. So, we're
hoping to have a unison release sometime in the spring of 2019 if we hold to our schedule.
So, in the meantime, if you want to know more and follow our progress, go to unisonweb.org.
And if you want to contribute to the development of unison, please talk to me
after the talk during the conference. Thank you.
