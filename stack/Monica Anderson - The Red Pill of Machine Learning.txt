The Red Pill of Machine Learning. Welcome, Monica. It's great to have you here.
Thank you.
Monica Anderson is an independent AI and ML researcher and the founder of Cinti and Inc.
Correct.
Her work is focused on epistemology of AI, but all her theory is based on her experiences
of design and implementation of understanding machines based on deep, discreet neural neural
networks since January 1, 2001 and used to work for Google as well. I might slide in.
So welcome and thanks for joining us and I'll hand the floor over to you, Monica.
So my goal with this talk is to sort out the reasons for the cognitive dissonance experienced
by machine learning practitioners, especially newcomers.
Let's start with a slide to introduce myself.
I'm from Sweden. I'm a US citizen. I got the master's in computer science with a double E minor.
English is my third language and I have used 30 plus programming languages professionally.
I'm in a very small group of people that have been researching deep neural networks full time since 2001.
Most people only got to 12 years later, 11 years later in 2012.
I'm an ex-googler. I founded three California C corporations and Cintians is my research company founded in 2004.
Let's start with a motivating example.
What is machine learning really like? Beginning machine learning students are given exercises like this one.
A large spreadsheet lists data about houses sold a certain year in the US, which includes among other information.
The zip code of the house, the living area in square feet, lot size, number of bedrooms, number of bathrooms, the year the house was built, and the final sale price of the house.
We would like to be able to predict this final sale price given the corresponding data for the current house that we are about to list for sale.
The spreadsheet is the data the student will use to train a deep neural network. It is the entire learning corpus that we need.
The students can download deep neural network libraries like Keras and TensorFlow and runnable examples from many kinds of problems from places like GitHub and glue slash superglue.
The code may require some customization to depth of file formats, etc.
This the student trains the network using the given data. This may take a while, but when it finishes, they can give the system data for a house that didn't exist in the training set and the network will quite reliably predict what the house might sell for.
This was the goal of the exercise. The student has created a system that understands how to estimate real estate prices from listings.
But when done, the student still doesn't understand anything about real estate, the predictive capability that many people in real estate should be willing to pay money for is 100% based on understanding in the deep neural network in the computer.
This is a rather important point. And because all libraries and many pre-sold examples of this nature are freely available, the student will not have to do much programming at all.
This is desirable. This is what AI means to me personally at least. The computer understands the problem so that we don't have to, and we don't have to do almost any programming either.
So deep neural networks, they may seem very strange to you, especially if you are a scientist.
The new cognitive capabilities in our machines are the result of a shift in the way we think about problem solving.
This shift is the most significant change in artificial intelligence ever if you're not in the science as a whole.
Machine learning-based systems are now successfully attacking both simple and complex problems using these novel methods. You're experiencing a revolution at the level of epistemology.
This will affect much more than just the field of machine learning. We want to add more of these new methods to our standard problem solving toolkit, but we need to understand the trade-offs.
So this talk intends to demonstrate that understanding deep neural networks requires a holistic stance, which is at important levels blatantly incompatible with the reductionist stance of modern science.
As practitioners of deep neural network technologies, we sometimes adopt strategies that seem to contradict many of our core scientific convictions.
We may get the feeling something is wrong. These trade-offs are real and meaningful, and the counterintuitive choices make perfect sense when viewed at the level of epistemology.
The truth clarity in these matters should alleviate some of these cognitive dissonance and may accelerate progress in the field.
The title of the talk is a matrix movie reference to the eye-opening clarity some machine learning practitioners might achieve after learning about and adopting a holistic stance.
These ideas are important in several problem domains at multiple different levels. We have two very different processes in brains.
We have two ways to understand the world, two ways for humans to solve problems, two ways to approach artificial intelligence, and two ways to use computers to deal with human languages.
So we're going to mostly work on the third one from top two ways for humans to solve problems, but I will introduce my specialty which is a natural language understanding the fifth point first in one slide.
So NLP is classical natural language processing. It relies on human main models of some language such as English and perhaps models of parts of the world.
NLP uses things like grammars, word lists, dictionaries, lists of city names and people names, etc.
In contrast, research in NLU, which is natural language understanding, tries to get computers to learn to understand human languages simply by reading books in the target language, any language.
The goal is some kind of human style and useful but not necessarily human level understanding of language. Any world knowledge would be a bonus.
We know that classical NLP and modern NLU have chosen opposite answers to many difficult binary choices.
In the tables that follow, NLP is left column and NLU is right column. Let's consider human solving everyday problem now at various levels.
There's basically you can have any problem you have can be solved in either of two different ways.
You can plan something in some detail before doing it or you can just do it.
The difference is much larger than one might think.
Our brains can handle either strategy but when it does is running different kinds of processes. That's another talk.
So programming is an example of planning something in detail, but now starting with the AI slash ML revolution or 2012.
We also know nowadays how to teach computers how to just do it, which means no further programming.
This has totally changed how we are building systems with cognitive capabilities. Everyone working in machine learning or AI needs to understand the trade also you must make at the most fundamental which is the epistemological levels.
The 2012 revolution requires unlearning and seriously rethinking many things we were taught to vigilantly strive for in our STEM educations.
Things like correlation bad, causality good and do not jump to conclusions on scant evidence.
But when building understanding systems correlation discovery and handling of sparse and inconsistent input that are exactly the kinds of tasks we will have to perform and perform well at these pre scientific levels.
Think about walking across the floor, you're sequencing your leg muscles.
Well, it's you're doing it with your brain but you can't consciously plan the sequencing because you don't even know what leg muscles you have.
So what we're walking is a learned skill as another example tell me how you understand this sentence, do you use grammars, do you even know how your language understanding works.
No, I really don't. These are subconscious skills that we cannot examine using introspection any more than we can say what a specific retina pixel is seeing.
You actually do most of the things you do every day without any reasoning whatsoever. These are learned skills. Let's look at some terminology here.
At the epistemology level.
The math and science approach to solving complicated problems by reasoning is called a reductionist stance.
Let's do the obvious thing approach to solving complex mundane problems or complicated complex scientific problems like like protein folding by only using understanding of the problem is called the holistic stance.
Reductionism and Holism are not dirty words. They are established terms of the art in epistemology reductionism as a methodology is often blamed for bad and bias science. Yes, sometimes reductionist science is bad we just need to get even better at the science.
Reduction is the greatest invention our species has ever made.
Let me say that again reductionist reductionism is the greatest invention our species has ever made.
It's not really about crystals and aromatherapy.
Many people prefer one or the other if they can choose but most of the time there is no choice as practitioners.
We need to know which tools to use in any project and we need to know this at the planning stage so let's define and discuss these terms.
Very different but compatible definitions for reductionism in this talk I will use my own definition for reasons of simplicity and clarity.
Reductionist reductionism is the use of models, which are scientific models theories hypothesis equations formulas naive models which we don't communicate to others superstitions which are theories which are wrong, and most computer programs.
Models are simplifications of reality a model airplane has no instrument panel.
We can design new models using observations experiments math and reasoning models ostensibly require a scientist to create the model we can you a PhD degree as a license to create models.
Models ostensibly require an engineer to select the model and use it to solve a real world problem.
Both creation and use of models requires that you first understand the problem and the problem domain. This is very important.
You in the reductionist solution a human has to understand the problem and the problem domain and come up with a model using a process.
We will talk about later called epistemic reduction.
In contrast, holism is the avoidance of models.
Holistic agents solve problems directly in the problem domain using understanding not models.
Even holistic systems can discover new solutions by trial and error or other holistic methods also known as model free methods, and then remember what worked and what didn't.
When faced with new problems a holistic system performs pattern match operations.
To prior problems hoping to be able to reuse previously discovered working solutions, well working solutions are remembered.
Holistic agents all learn from their mistakes. This is one of the most certain signs that you're dealing with a holistic system because in reductionist systems they are designed and implemented by humans.
And improvements require a new version to be created and released for instance after a program has learned from their mistakes.
So that's a very important point. We'll get back to that.
Let's go over science and epistemology briefly. We know what scientific models look like equations for instance.
But what do epistemological equations so to speak look like they can't be expressed the scientific equations because they are pre scientific or non scientific.
Consider this statement, you can only learn that which you already almost know. This is a quote from Patrick Winston at MIT.
MIT is Cal teach calculus to one year olds.
To detect novel novelty you need to remember everything old. I figured this one out not too hard.
You are known by the company you keep. This is the naive holistic version of the unit a lemma which justifies the use of embeddings in language understanding.
Science has no equations for concepts like understanding reasoning learning abstraction and modeling.
And to me as an implementer is very important epistemological truth can be used as a implementation hints but science provides very few such hints.
I specifically am avoiding neuroscience.
So, let's look at difference at the level of epistemology here is the first real table we're looking at.
We have discussed most of these already so the first three are explained in more detail in my artificial dash understanding blog.
So, we have the use of models and avoidance of them we are reasoning versus understanding we have reduction systems require understanding and holistic systems can provide it.
So I've been a model space versus directly in the problem domain and the reductionist solution is optimal in many domains and I have to emphasize this again.
If you have a reductionist solution for a problem that works well by all means continue using it forever there's no point in bringing in an AI at that point.
You use, you use holistic methods only when reductions methods cannot be used such as when you're dealing with complex systems for everything affects everything else consider cell cellular biology, the stock market the global economy, etc.
These are all the brain, the drug interactions in the body these are all systems for everything affect everything else and reductionist systems are really poor dealing with that.
And this is where machine learning and holistic methods really shine.
So, let's look at some main trade offs here from between these two systems in the reductionist case.
We have optimality which is the best answer, complete and which is all answers repeatability the same answer every time extrapolation in low dimension low dimensionality problem spaces.
Transparency is one which means don't waste resources and favor succinct explanations. Transparency which means we can understand the process to get the answer.
Explainability means that we understand the answer. These are great things. This is stuff reductionist love.
It would be really hard to give up on all of them but that's what we have to do if you want to switch to holistic state stance.
In the holistic end, they are rough comparisons rough equals to each other here but don't take that too seriously.
In the holistic case, instead of optimality we get economy and we reuse non useful answers.
We get promptness which means we accept the first found useful answer rather than trying to find all of them.
We get improvement. I mean instead of repeatability, if you learn from our mistakes, the system gets better over time.
And if you want repeatability, you can have it. If you want improvement, you can have it. But you can have the repeatability even in the holistic end with trickery.
Interpolation. This was where extrapolation could only work in low dimension domains because they had to work in the model space.
When you're working directly in a gigantic space with millions of dimensions, for instance, you can do interpolation in those spaces, even in super high dimensions problem domains.
Because the interpolation is at this point is more reliable. You can use it over very large sets of data.
We can use parallelism, which we do when we don't know exactly how to solve a problem, we can pursue multiple paths in parallel.
And this is much more expensive than the parsimony of the reductionist side. But sometimes that's the only way we do basically exhaustive searches if we have to.
Intuition means that we accept useful answers even if they're achieved by unknown or subconscious means.
And ignorance means that there's no need for humans to model or even understand the problem or problem domain.
And this is the most controversial statement I think in the entire talk so let's look at it a little bit closer.
Ignorance is a feature.
The reality in the reductionist and means basically understand the solution.
But ignorance in the holistic side says no need to even understand the problem.
No need to even understand the problem. Like we said in the original example we had this student who didn't understand anything with real estate.
Our machines understand our problem for us.
And I will manifest as machines that can solve problems we ourselves cannot understand or cannot be bothered to understand it turns out to be very common that instead of trying to analyze some problem with the domain we just say learn it be done with it.
Otherwise we're already at the state we already have what I think of as AI as so as programs that can solve problems that we can solve ourselves because we have for instance protein folding and go and so on.
So, we are delegating our understanding to our machines, because we want to be increasingly ignorant about increasing amounts of stuff that we don't care about.
We're pursuing pursuing a GI so that we may play as Philip is fond of saying.
So let's look at some advantages of these holistic methods, such as neural networks.
NP harness it is because we desire certainty optimality completeness etc at NP harness becomes a problem.
There are many problems where it is relatively easy to find a provably valid solution but we're finding all solutions all the optimal solution can be very expensive real world traveling salesman merrily travel along reasonable routes.
The holistic systems garbage, I do which means stands for garbage in garbage out is not a problem.
It is a requirement of real world intelligence systems that they be able to detect what is salient and likely to be correct in their input streams in order to avoid attempting to learn from noise.
They need to be able to detect omissions duplications errors noise slice etc and the only way is to relate the input to similar input they have understood in the past.
In other words, an example of the utility of this is the current crop of image and video apps featuring image understanding based on deep learning.
These apps can remove backgrounds restore restore damaged photographs etc.
We need to keep in mind that the ability of holistic systems to fill in data data and detect noise depends on them having learned from similar data in the past.
Here's an example.
Garbage in garbage out is not a problem.
This should not be a surprise to you if you have correct expectations about artificial intelligence.
Brittleness most of cognition is the recognition being able to recognize what that something has occurred before and knowing what might happen next has enormous survival value for any animal species.
A mature human has used their eyes and other senses for decades this represents an enormous learning corpus and they can understand anything they have a prior experience of.
As they get closer to the edge of their competence the quality and precision of their guesses will start to decline.
The mistakes made by humans animals and by holistic machine learning systems are very often of the near miss variety which provides an opportunity to learn to do better next time.
Contrast this to the reductionist software systems created for similar goals rule based systems have long been infamous for their brittleness.
As long as the rules in the rules that match the current reality perfectly results will be repeatable and reliable.
But in corner cases when the matches become more tenuous, the quality rapidly drops and simple internal mistakes will lead to such systems to return spectacularly incorrect results.
Let's consider obsolescence.
As I said, sometimes repeatability is important to sometimes tracking a changing world by learning more about it is important.
Human main models tend to have more abrupt edges to their competence and few ways to make partial mistakes in machine learning continuous incremental learning makes it possible to stay up to date.
Without repeatability, we can emit the condensed and clean and frozen competence, so to speak, from a learner that can be loaded into a non learning read only cloud based understanding and understanding machines that serve the world and provide
repeatability between schedule schedule software and competence releases.
A few higher level advantages to holistic systems.
One understanding, decent estimates of saliency, epistemic reduction and abstraction which is abstract. They are discussed in my blog.
Disambiguation of language for instance self organization and self repair scaling and very nicely and creativity cannot be cannot be had in in very strict reductionist systems.
Let's go back and look at the details of the original list of the five dichotomies that I had.
We had epistemology with which has reductionism and wholism.
The brain you has reasoning versus understanding human problem solving uses stem and other reasoning based stem education other reasoning based methods versus just do the obvious as the whole list again.
Strategies for AI, the reductionist AI was basically 20th century AI before 2012. And after that, we think of machine learning and deep neural networks as the way to go.
And finally human languages is the split between NLP and NLU that I described earlier.
So this is the basically the red pill of the talk, cognitive dissonance.
Throughout our stem education we have been caught taught how to build and use models.
Do not use correlation to employ apply causation do not jump to conclusions on scant evidence.
We have separated wholism for 400 years.
Suddenly after 2012 deep learning starts solving complex problems holistically science itself itself now has a cognitive dissonance.
Okay.
We can double our toolkit. All humans use holistic methods from birth. We, and then we learn to walk and talk using holistic methods.
A stem education comes after that it reaches. It teaches reductionist problem solving methods as a discipline, and I'm serious when I say discipline this is something you have to vigilantly maintain in a disciplined manner because the temptation to jump to conclusions is always too large because
it's a natural way of solving problems.
So the deep learning revolution in 2012, it made these holistic methods available to computers.
Okay.
And it's a wonderful match because many of the remaining hard problems that we have in society such as global economy and other things they are complex problems everything affects everything else and we need something like AI or in the in the beginning we need something
like holistic methods and later we will segue into AI is using those holistic methods.
Natural language understanding is mostly holistic.
It's another simple mundane everyday problem to our brains. If you can learn one thing we can learn another.
Full intelligence in my opinion is something that combines a holistic subconscious understanding subconscious in the case of human brain.
Reductionist conscious to reason and you can remember that you cannot reason about that which you do not understand, which means that holistic methods provide understanding.
Reason requires understanding which means that understanding must precede reasoning and it does this in multiple places it does it in our brains because it takes about half a second for our brain to understand the visual field and send signals to the reasoning and conscious parts of
the brain.
What's going on, and this half second is masked by the brain itself it lies to us to make sure we don't know this you can read all about this in then it's consciousness explained.
And so there the understanding precedes the reasoning part, we also see that in the evolutionary record where lower animals such as dogs for instance they can understand a lot but they don't reason much and we are basically the only species that reasons in the serious
manner some birds do too.
So general intelligence.
Humans are not general intelligences.
At birth.
Humans are general learners.
capable of learning anything.
a GI employee implies a human designed hard wire programmed ready to run from birth artificial intelligence.
This is silly we need a GL.
Learning is the only path to AI in my to true AI in my opinion.
If at first you don't succeed lower your expectations.
If we can't make an intelligent machine can we make just the understanding part.
And others developed a deep learning 1984 2012 I developed organic learning from 2001 to 2017.
So how do you design an understanding machine brains jump to conclusions on scant evidence, because scant evidence is all we will ever have.
You don't know what goes on behind your back.
You don't know what goes on in the next block if you want to estimate Apple stock price of week from now you have no idea what goes on in Apple's customers heads in the white house in Apple's boardroom or in China.
Scant evidence is all we ever have and everything we do is a guess.
Rain is guessing get it right most of the time because we rely on our experience for making those gases.
We interpolate over experience.
Let's make our machines do the same.
The DNN's like deep learning are designed to learn to guess and to jump to conclusions on scant evidence deep learning is not very scientific.
Neither is my own stuff.
Just putting it there.
I have invented a machine learning algorithm that is about a million times faster than deep learning in for natural language understanding tasks. There's an asterisk there.
There's a useful amount of any human language in five minutes on the laptop without a GPU by just reading on tag text using 100% unsupervised learning.
It learns enough to pass my very simple non adversarial classification tests at 99% correct.
There's a deep discrete neural network DDN and the neurons are discrete they are not a race of floating point numbers like they are in deep learning I have Java objects that simulate neurons and synapses.
It's designed to learn any language by just reading text.
There are continuous text streams and deep learning in general I think only works with 300 words in a batch and then it has because it has to do all this jazz with a GPUs.
And then it takes another 300 words this can learn in real time at about two to two kilo characters per second 2000 Unicode characters per second.
Auditor indefinitely.
It emits a competence to be loaded into you and one which we'll discuss in the next slide.
We'll illustrate a learner after the last slide here. So if you hang around you'll see a demo.
You and one is understanding machine one. It's a cloud based arrest service. I have had it running since December of 2020.
It provides understanding as a service.
It's in the same class as GPT three and Google bird but it is nowhere near as powerful is just very fast and cheap.
The text didn't get back you once understanding.
This understanding may differ from yours.
Just like yours might different from that of a coworker, but will still be useful in most apps that need understanding technology.
Look at the algorithm that the understanding machine uses. I will not show you the learner algorithm because that is proprietary to my company, but they understand that we will license to customers and others and so we can see how that works.
This is basically pretty, pretty revealing stuff about my algorithm.
So we have an input text stream. This happens to be from Jane Austin.
And we have a letter that's being read.
Unicode character and we looked that up in a hash map in our Java program and we basically find a reference to a node. There is one input node per character in the Unicode alphabet.
And when we read that character we tickle that node and the node will signal its target links like external tree in brains.
And they will in turn tickle their target links under certain conditions and only some nodes propagated, depending on many things.
Every node has an ID number. We start out empty system startups absolutely empty and we create nodes as we need them we find a new character we never seen before we create a node to hook up to that.
And when we want to signal somewhere, we might want to make normal more nodes.
So every node is sequentially numbered from zero for one. And what we do is, we know that some nodes are known to be more salient than others the more important nodes, because we think that our algorithm thinks that they embody important higher level concepts.
And then the ID numbers of those nodes are returned to the API caller. This is this array of numbers is basically the UN one understand.
To recapitulate we use a number neuron API, we send in some text get back a list of numbers representing salient concepts found in the supplied text.
The number is the idea of an activated discrete pseudo neuron, their meaning is opaque to the caller, but that's turns out not to be a problem this is described in the API manual is another talk.
Some closing words. Science was created to stop people from overrating correlations and jumping to erroneous conclusions on scant evidence and then sharing those conclusions with others leading to compounded mistakes and much wasted effort.
Consequently promoting the holistic stance as long in a career ending move in academia and especially in computer science.
We always suddenly have machine learning that performs cognitive tasks at useful levels using exactly holistic stance.
Ignorance of these stances and these people to develop significant significant cognitive dissonance is which is why discussions about these issues are very unpopular among reductionist, but the dichotomies really need to deal with it.
The reductionist stance also make it difficult to imagine except useful things like systems capable autonomous epistemic reduction, which means they can discover abstractions on their own systems that do not have a goal function systems that
improve with practice systems that exploit emergent effect systems that by themselves make decisions about what matters most and systems that occasionally give a wrong answer but are nevertheless very useful.
We are also finding that after full college education in machine learning we don't actually need to do almost any programming at all. We don't need to understand anybody else's problem, problem domains, because we don't have to perform any epistemic reduction ourselves.
And so data science work is reduced to massaging spreadsheets of gathered data.
AI was supposed to solve our problems for us so we would not have to learn or understand any new problem domains to not have to think.
And that's what we have today in machine learning and with holistic methods in general, why are some people surprised or unhappy about this.
This is what I think of as AI this is what we have been trying to accomplish for decades.
We accepted reasonable improvements in understanding capabilities in our machines and acceptance that acceptance of the holistic stance will become a requirement for machine learning and AI based work, but it will likely likely take a decade for our educational system to adjust.
Now let's do before the Q&A I would like to just do the demo. See if I can do this in some easy way here.
Get up.
So can everybody see a blue screen here in the screen share?
No, no, no, I think you need to change the shared screen within.
Yeah, yeah. Okay, let me find the right windows for this.
I can see the console for directory listing.
Oh, you can see a console?
I can see a console for directory listing, but the text is too small for me to see.
I can see it on my 4k monitor. Are you sure you don't just need to do all shift meta option escape to run the whole demo.
What are you talking about?
Emacs, bad joke.
Oh, I know Emacs. I use all the time.
Anyways, let me do this.
Can you see anybody see white text on a blue background in my shared screen? Okay, fine.
I have lost my main screen for.
Yes, so there you go.
Now you're back.
Okay, so maybe you need to share a different screen from within the.
All right.
From within the zoom from within the zoom interface.
So when you click on share screen, you can choose which screen to choose to highlight.
Okay, okay, okay.
I'm going to share. I couldn't find a toolbar for this because it was covered as usual.
All right, here's the zoom and where is my screen share. There it is. And now we try to share.
Maybe I have to share this one.
You know, there was traditional conferences for people to fiddle with cables.
Okay, can you see a screen now? Anybody see a screen share now?
Yes.
So it is blue with white text on it.
Correct. Yeah.
Cat sample TSP.
Correct.
What we're looking at here is the test cases I will be running.
This is two test cases.
They are basically there is.
The one we start with the number four and one that starts with number zero.
We have a target sentence, which is in the first case is, is there a reason why we should travel alone and then there are five sentences to choose from and the task for the computer is to pick the one that has the closest meaning to the target
sentence.
So this is a multiple chains class, multiple choice classification test in a non adversarial test batch.
So it is easy to get to 100% here and we expect to get there.
But this is basically the kinds of class or questions that the test case is trying to solve.
And this is roughly the level you need in the chat bot and this is roughly the level you will need in a email classifier, for instance.
So now let's go here and do a control L and type demo.
And we will see here it's, it's, I hope I don't oversize it because this would be embarrassing if it falls. Yeah, it falls. Hang on a second. Let me minimize just a bit.
So what we're seeing here is basically a system running.
And it is, hang on a second.
I can't see the screen anymore.
Now you should see it.
Okay, so what goes on here is you're running this test and you can see the columns. I hope you can see the columns they say test runtime learn characters learn seconds learn.
So what we are doing here the important column is the error percent which is in yellow the right most error call that basically says how many errors we make.
And with 200 test cases, 5% error means that you miss 10 questions.
And it has come to learn 200,000 characters, and it has, it is 95% correct.
So let's run it for three seconds.
Let's have it run a little bit.
So it's now at 400,000 characters.
I'm reading at five kilos like a kilo character per second.
It's a little bit of a spoiler alert it with it, roughly third of the time it will get to 100% correct.
We have two megabytes because at two megabytes we pretty much know what it does. Anyways, so this is machine learning of English from scratch from absolute scratch on tagged corpus plain text, we are actually reading Robert Heinlein here.
For the six first six megabytes.
Any questions about this while we're running it.
What's the event saying in the right calm.
If I do something special like what we call a loser collector, the loser collector output, the notification of the loser collector event will happen at the right.
Now we're at 98% correct after 800,000 characters that's basically one high line book.
Would you be able to do it on a math test textbook.
Excuse me, would you be able to do it on a textbook.
Don't we earn.
This is language understanding.
Yeah, and we don't know I mean, to be to be to be honest, the things that you learn at this level are very, very low level stuff.
And if you give the math textbook, yes, it would learn a lot, a lot of character sequence useful for math, etc.
I don't think it would get a higher level understanding of it, at least not in my implementation here, but we're looking at this basically this code is running on my laptop on my Mac.
I have done all my development of the system over the past 21 years on a Macintosh 2013.
I cancel it on Macintosh.
And I need something like a three terabyte RAM machine with 200 plus cores and those exist and they cost 80,000 so if I get me one of those I will put in the math book.
We're at 98 and a half percent correct that they're one megabyte of learning.
So basically the learning the test that I use every few so often.
We basically turn off the learning in the learner and use it just as an understander.
So it cannot learn from the test so we can run the test as often if we want and it takes less than a second to do 1200 strings for 200 test cases with six each.
Lull here it went to 1.2 megabytes and it didn't improve it's just staying the same sometimes you get the regressions.
But basically this is what we are looking at.
It's a
We can let it learn a little bit longer. Any other questions.
What is the created and deleted columns.
Those are, I'm going to discuss those. They are experimental things that are currently turned off.
The current display incidental is over twice as long. This is over a year old demo. I had this demo made in December 2020.
And it matches the cloud based understander that can do exactly the same thing except in the cloud and it does it in a couple of seconds for all 1200 strings.
Okay we are 99 and a half percent correct after 1.4 megabyte.
So compare this to deep learning how long would it take to create something like Google birth my Google birth is they ran 100 TPU equipped machines in their cloud for a month.
Okay so they get into the millions of nodes and stuff.
I don't show you how many nodes I have in this display the latest display has a node count and we typically get into the tense to hundreds of millions.
I can fit about 26 million nodes in the Raspberry Pi for some decent level of understanding because the runtime is very small.
The learner requires a lot of memory but the runtime is very small.
And it's read only.
So one machine in the cloud with the language model and thousands of processes that are using the same language model in the read only version it's going to be very efficient.
Can everybody see a annotated version of the of the run.
In the share screen.
Yes, we see that.
Forget it. Anyway, you can see here the annotations to this thing and this version of the round it reached 100% correct in three and a half minutes after reading two megabytes of corpus.
It's way below what is required. I mean this is a million times or so small faster and cheaper from an energy point of view, then what deep learning is for for the big language models that they are doing.
Okay.
So that concludes my presentation.
Are there any questions.
Let's go into more detail on how you what exactly is the question and how do you know what what metrics you have for correctness and.
Oh, they are right. Sorry, that is actually very common effect of watching those questions because what we are basically just looking at sentences.
There are no questions per se there are sentences, and they are core a question pair, they come from the core question pair data set which is one of the super one of the glue data sets.
And what we are doing is I'm basically using just a pair pairs of similar sounding questions I'm not answering them I say is this question the same as that one so it's just a doc sim.
Comparison is not a question answering machine.
So it is much simpler than you might think.
Doesn't answer questions you just classifies text.
So how does the performance on that task compared with things like Google birds, like, it is.
Well, it's, it's a different application I mean, this is only for classification at the moment. I have language generation in the works.
You can use Google bird for creating just embedding some sentences and just like semantic similarity search right so you can compare them on the same time.
Yeah, I have not compared them I mean my stuff is is very simple very fast and very cheap to use and Google bird is large expensive and I haven't even don't even know what the API looks like the API for this like I discussed is basically you get these numbers.
I think of my system as a half transformer it takes text, and in the transformer you have the decoder, the decoder and the encoder encoder and the decoder.
And what I do is I only use the encoder part and then I take the internal representation and directly use that for the classification.
I have two questions for you if you don't mind.
First of all, I think I really like the presentation the things you went over right like the theoretical foundations between science and epistemology that's like right up my alley as well.
And also, actually, what you're working on sounds really interesting related to what I'm working on as well so I think we should be in touch if we have some room for collaboration because that that difference between understanding and reasoning is something I thought about as well.
What I want to hear you elaborate on a little bit is you said ignorance as a feature.
What are your thoughts on trust and AI safety, and how does that tie in for you with ignorant ignorance.
As we're going to, we're going to talk about it in a few minutes I understand in the panel, but I can say as much as at this level.
Everything is very obvious. Everything is very we basically the machine has no goal function.
It's basically the only goal function you can think of is to learn more.
And it's so it's so basically in the system that it's not changeable.
So trust isn't really an issue it just does exactly what we what what we it learns whatever we give it and this is kind of interesting from a futures point of view because AI, like I explained in the talk is going away.
It's not going to be programming anymore.
It's going to be corpus curation.
And that means that us techies aren't the right people to do this what we have to do is we have to delegate the corpus creation to the humanists to teachers to explainers of various kinds to authors etc.
We have somebody has to write the book gravity for dummies because it doesn't exist.
And if you're going to learn machines based on incoming text, which is what my belief is what that's how we get first to AI is basically we give we create machines that can learn from text and we give them enough to read so that they understand a large chunk of the
world and it will take a large machine but it doesn't take any GPUs.
Makes sense makes sense. Yeah, I have a similar view on that as well to where you know the understanding is facilitated through what what's out there and people's brains and all of that.
And then I have a little bit of a challenging question for you. Thanks for elaborating on a previous one, but you your comparison between wholism and models.
Let me ask you this isn't wholism also a particular kind of model.
In my, in my, in my talks, it is the absence of models.
The important thing is that the main task of creating the model is shifted from the human programmer, if you will, or scientists to a machine that basically examines the world in in many, many instances by reading a corpus and it devices.
And then it invents the something that we might want to call a model which I actually really hard refused to call a model. Let me explain.
There's multiple levels in these stacks I mean I think of the text is coming from the bottom and then the notes at the bottom they basically signal to higher abstraction levels if you will or something else like that I won't go into the details, but
where was I going with this are the.
That was a question again.
If, if, you know, wholism is not a particular model as well and you make it you make a choice here to say well it's specifically not by definition which I mean I guess that's fair enough.
So what what happens is that the input layers that we look at the most closest layer to the input so sense is so to speak.
They are basically recognition layers they only determine whether we've seen something before. And if we haven't then it's alarming and it gets pushed up the chain to the top.
If it is known before we, we basically push up a little thing that we will had one of these again and it's harmless.
I have an example of a holistic system. This is a thought experiment of a holistic system where, and I'm going to go into division domain which I'm completely very poor at but anyways, the imagine a camera that wants to do face recognition.
And it basically takes the picture. I mean there's a picture coming into review finder and machine starts thinking about the picture and somewhere there is a deep learning machine and it finds basically here is a light green thing next to black dark green thing.
And they're her and the vertical.
And then the others and here's another one and here's another one and now level up there is some neuron that collects these things and we are seeing a lot of grass like things here so maybe there's a lawn here it basically the picture we're looking at this a family seeing on the lawn with a picnic.
So it decides that there is a lawn and basically the bottom layer says here's green and dark green and light green next layer says here's grass the next layer says here's a lawn.
And it goes all the way up to the top and at the top there is somebody who sits and says lawn I don't care about lawns I'm a face recognition expert forget that.
And the trick of basically of epistemic reduction is that at every level in the system.
And those levels those levels and deep learning this level to my systems at every level every level in the system you have to discard that which is recognized as irrelevant and discard it, possibly with a small signal going upward to the next level says hey we have a lawn here I know you're
really interested but it's there.
The main reason this is why deep learning is deep.
The layers discard irrelevant context and they can only be recognized they can only recognize that their own level.
The lawn can only be recognized at the lawn level in the grass can only be recognized to grass level.
So you have to have multiple layers of increasing abstractions, and, and then you have to be each of those layers have the power to throw away that this is irrelevant because he's already known and harmless.
Thanks for elaborating appreciate it.
Thank you.
Hello.
Your tool.
The party of Le Pen in France.
Benefits from using it.
You think.
I couldn't quite understand what you said could you rephrase that.
Yes, the party of Le Pen in France could benefit.
Okay, using your tool.
I should cancel it immediately.
My point is that many anthropological constructions tribalism's
will use your tool or tools like it and training your tool on Le Pen would be interesting.
Yeah, you can put anything in it as a corpus we like Heinlein because it's Heinlein.
And we have to read the stuff occasionally we might as well read stuff we like or we could put in the collected thoughts of Chairman Mao or the Bible or the Quran if we've wanted to.
I think it doesn't matter because it's all the same it's all just characters and the only thing is fines in the beginning like what we just saw is very simple low level patterns.
And so there is really no nothing that even reminds you of ethics at that point.
You mentioned the word Le Pen, not suddenly flashed.
Is there was a result yet the elections in France known yet. I know it's completely irrelevant but well.
I thought I heard Le Pen lost. I thought I heard of Le Pen lost.
Yes, I don't know.
Yeah.
So of course, any technology will be used for war and sex.
Yes.
Let's change the topic to sex.
Actually that's a very important thing. I mean, your show already mentioned this but neural Darwinism is what I use and neural Darwinism is basically reinforcement learning plus sex.
So there.
I have to dig just a little deeper, the algorithm so they're more, more like mushrooms than die points.
Finally, you should ask that they look like mushrooms once upon a time I have a picture somewhere, but that was a bug in the system.
We had over this project has been going for 21 years now since 2001 and we have 23 versions of the system and only version 21 and 23 work it took me 16 and a half years to get the system that could get to 0% for errors.
Now we're working with the levels about that. Yes.
I remember your demo from four years ago.
Thank you.
Yep. Yeah, this is the third event that I showed this learner to I showed it and then I meet up and I showed it that the thing at the place.
All right, I'm going to ask one last question. What are you going to do next.
I'm working. I can't talk about it too much in public but if you come if you talk to me on the phone I'll explain what's going on because.
But in general, there is a layer that I have to figure out and I am four years into figuring it out.
And we have very interesting results but basically, we are going to the point where this very fast learning that we're doing is not going to scale to the next step.
And we need a much larger machine so $80,000 machine from super micro would be what we need next.
And that would have, like I said three terabytes of RAM and 220 cores or something like that.
And I think my system is completely thread safe, which means that I can have 200 pairs of eyes reading a book each, so to speak, I mean 200 input senses of text, loading in learning into the same brain at the same time, not a problem.
So I can read 200 times faster if I just open more books at the same time.
Maria coach you just get a grant and get access to a supercomputer and get all the computing capacity that you want.
I mean, I don't I can't I don't even know how to get a grant nobody understands my tech.
I've been turned by down by so many it's not funny.
So yeah, really, and by the way I'm not good at grant writing I'm not good at grant writing either.
You've gone through the SBIR process.
I tried.
I talked to the Air Force I talked to Google I talked to Apple I talked to.
Yeah, I had some of these were very brief encounters because they didn't want to even talk to me so yeah, but I basically tried to push it for a decade or so and nobody understands it enough to bite.
I mean, part of the problem was that first part of the decade the system didn't even work I mean everything I did before 2017.
Last time I asked the VC for money was two months before it started working that's not really what you want to do.
And I was too tired at that point to go back and ask for more.
Sorry.
That sounds like you should take a break so you're no longer tired, and then the benefits of your algorithm.
When you are relaxed again and optimistic again and know how hard it can be to deal with rejection and setbacks and so on and eventually you'll settle back into the healthy equilibrium.
I hope that this all works out in the best possible way.
Actually, it turns out that I am actually economically stable at this point I just don't have the ability to pull in $80,000.
I mean, I'm working for a company that use is going to use my technology I'm writing their end of the API.
And I get to keep all the intellectual property myself for the server and those stuff and for the learner.
So that's a very sweet deal so I have a little bit of an income and I can continue doing this for years and I've done it for like I said since 2017 I've done a lot of work on the next thing next secret layer and that has all been paid for by this company.
Well, can you have you paid videos that you put on YouTube and so on and.
Yes, I have made I have made lots of videos I mean you should.
The easiest way for anybody to find my references to go to Facebook and look at my homepage I also have a whole look at my left column in my on my profile but I also have
so I have videos on video if you go to video and look for my name you can find a playlist of
videos that I made and there's six of mine and one by Peter Norvig or something like that.
And then I have a website, the main site that discusses these things is artificial dash understanding.com.
It's a medium site, but I have a DNS name on it and that basically tells the story of epistemology, epistemology, it's a primary in epistemology for AI people, basically, discusses what I just talked about.
This goes a bit further because that site was written four years ago, five years ago but but it's all the same story the reductionism holism has been the theme of my talks is 2005 2006.
After Google.
Have you tried China.
I mean they got tax money.
Yeah they do and one of the things that the system is good at should be good at I actually tested it only very many years ago but it should be good at Chinese word segmentation which is an important problem it's worth millions of dollars per year.
And basically because Chinese don't put spaces in and you have to put the spaces in, if you want to do anything with the language, such as index it in a search engine and the majority of errors in Chinese search engines are still segmentation errors.
So this could do that.
If there was money for it, maybe there is I don't know.
My Chinese, I tried it on six different languages, English, Swedish, Finnish, French, German and Russian, and I did Chinese like five years ago.
I don't mean the language.
I mean, going, going to China or approaching Chinese people and asking them for money. There's lots of money in China, especially for AI. It's one of the government's target areas that one of the 25 areas.
I am looking for collaborators of all kinds and that implies people who understand these kinds of things it also means that I'm looking to hire people that are basically what is it called deep stack cloud people and others otherwise like that.
And because I've been doing these stuff forever and I'm tired of it.
Full stack Java programming. So now, but I'm actually I'm, I'm currently
in at least two different processes to look for funding. I won't talk any more like that but there's a couple of ways that I could fund this shortly and it will be important for me to find people to work with
at all levels, including CEO is open.
I mean, as soon as the windows are open and and cloud programmers positions will be open. So, yeah.
Yeah, I was going to ask if you have thought of publishing some of these ideas of the, especially the models.
Like the, sorry, like methods in like some research journals or venues of any kinds. I mean, it can be hard if the ideas are like quite different from some like, I mean, you can try to find some, I mean, not necessarily the hardest to get venues but some trade
of which at least can give some visibility and also like some feedback from the community, which can help in future interactions.
So I have, I have a paper called exactly the red pillow machine learning that I'm trying to publish and I'd like to find a magazine to publish it but it's I'm not an academic I'm not in academia I haven't been in academia since or whatever.
And so what I write is typically slightly easier prose and it's more like a blog post if you will if you read my artificial understanding website.
And that's basically that's the style I'm writing in and the paper that I have is not scientific enough it's not the formatted right etc to even make it on to archival org I mean it's it's, it's, it's, I see.
I mean, it may be worse.
By the way, I need, I need, I would need, I would need the endorsements from two published authors or something like that before archive even will look at it.
Well, so maybe that will be an interesting area of future collaboration if you find people who are in academia that are willing to collaborate and maybe that could be.
Yeah, I actually have a couple of signatures already so I might go this route but I'd rather publish it on some other site I mean.
And I have, I had, I have about five websites, and they all died in the stupid IP enumeration reenumeration event and I haven't had the time to set them up again.
And so, now I'm planning to start over with a new corporate website that contains all of this information and all of my blogs and lots of stuff from elsewhere and I'm content relating writing a book to.
Okay.
It seems to me like especially if you're saying that you know you provide understanding as a service that you know your, your API or whatever could be used for monthly subscription if you are something.
Yes, and in that regard I'm also wondering, like right now understand okay your model is like much much faster learning and requires much fewer resources so you can just upload the text and then the, the output is just as reliable as any of
these, you know, bigger, more expensive systems.
Yeah, how does it change now the more data you feed into your system is there any, any difference that you can describe because I mean, you know, honestly speaking at this point we've all right so we've we've used the bad models, but the output is the same so
where's the difference now of course I understand again you have a system that learns much faster and everything. But if I compare to GPT three or deep mind or whatever else I'm not even.
I don't know half of those, but where's the difference now that they already have their understanding and their network populated and everything's developed.
Right. Well, let's start with the fact that the tens of thousands of deep learning researchers have been working on this stuff since 2006 when Hinton came up Hinton and friends came up with it.
And since 2012 when it became public and I'm the only one me and my co researcher Roger, we are the only ones who are working on my stuff.
I mean, the injection of money would mean that we could hire more staff and more researchers and we could take this to the next level very quickly. And also we need this bigger machine.
So that's that's been the current plan for a while now.
One thing that's interesting is what is that you can try to sell the service as I do as as understanding as a service, but there's also the temptation to start something else, such as a social media.
So this is the core technology.
And one of my funding funding efforts is actually going in this direction.
So I might end up working in the social media company, doing this research and everybody else is doing the system.
That's my hopefully that will be a good way to get to the next stage.
It wouldn't be adult friend finder what it.
So it would be something you would very likely like.
We can talk about it offline.
If anybody's interested in working in the social media using this as a using my technology as a base. Tell me and we'll make it happen.
See if you fit.
We've done language generation experiments. We haven't. It turns out that the reason I got into this model in the first place because I stumbled upon the secret of how to generate language in the book.
And so I thought about it for a year and then I started writing code.
So the generation is fallen by the wayside at the moment, but every now and then we dust it off and run it again and it still runs pretty poorly but give us an 80,000 dollar machine that we can make better.
In general, all of the machine learning systems, I mean they get better with bigger corpora because they fill in more of the corner cases.
I mean, if I'm writing. This is one reason we have multiple dictionaries I mean the system I'm building is going to basically have an it's going to be a set of understanding machines but for each of the service calls to the understanding machine you have to specify what's going to be your
competence what competence are going to use and the competence we looked at now is basically called in basic it's English slash basic it's like a directory path if you will.
The first two letters are always the country code so we have Ian slash basic and we will in the future have things like Ian slash medical Ian slash legal Ian slash social media.
JP is Japanese slash social media finish slash medical etc. So we can create hundreds of languages, cross it by dozens of domains in each language and we can sell razor blades until the cows come home.
One of the tricks I'm going to use is I'm going to have basically people I'm going to have an online service where people can submit the corpus, and it automatically learns the corpus and puts it out there for others to use and the person who submitted
the corpus gets a fractional kickback.
So we can use basically volunteers all over the world to do things like give us a decent corpus for Urdu and we don't even have to test it they just create it and if people like it they will get a kickback and we'll get the money.
So yeah one understand one the core the code size for the runtime in the in the cloud.
The one that I have in the cloud at the moment that the inner loop is 91 lines of Java 91.
The whole code is mostly loading the core loading the competence that comes from the learner and some logging and some accounting features, but 91 lines and you can understand English, if you are given the right neural network structure.
And I can warn you that you will not understand the code on the second or third read through.
It's very compact.
Monica I have a challenging question.
What.
So, language understanding.
It was often to satire irony.
Yep.
Yep.
Yep.
Yeah, one of my early projects which I never finished was basically write the version that will read gain Austin and underline every joke in green.
She's pretty dense with jokes I have to say.
And over 40 years since I read pasta.
The rules were too large for her to move in with ease.
Anyway, anyway.
So I'm a bit curious about this machine you want to buy you're going to get from super micro.
You're going to get from super micro. Okay.
What are you building.
I'm building a track unit with three terabytes of RAM in 128 gig chips and two AMD CPUs with 110 or so course each.
And also last time I looked they make new machines so quickly that I may have to ask for more money because now something better is available.
At the top end of this range I can get a tell 12 terabyte machine at about $2 million.
But then I have to do.
Then I have to do what is it called hyper channel programming I have to, I have to.
There's this protocol that they use which is much faster than Ethernet in the hundreds of gigabytes range whatever.
I forget what it's called.
It has a stupid name like hyper channel but it's not that.
The melanox the melanox web. That's what we think we'd have to put in.
So yeah it's been a fun 20 years 21 years.
I'm doing reductionist AI most of my large fraction of my career, and then in 1998 I had a religious conversion to holism, if you will.
That's what it took. And some people, when they're confronted with this. It's like a red pill. They may be unhappy about having learned this.
It really upsets one's worldview a lot when you if you accept basically that holistic methods can do things that reductionist scientific methods cannot.
And we are in the middle of that right now with deep learning.
I'm walking to the side because my screen of everybody else is on the side of my desk here so sorry if I'm disappearing from the picture.
Almost all the way with you. There's one thing that is left out in my understanding of the general problem, and that's having a body.
My favorite topic.
So, embodiment, let me kill off embodiment right away.
Basically people who will go for the embodiment architect or embodiment.
What's called it argument that they say basically we need to have an embody body so that we can interact with the world that we have to have a robot.
That's a very human centric point of view.
And we know for instance that we can learn about, we can learn about the world from just reading text we don't need to taste and feel an orange know what an orange is we see orange enough times in the text we can make all kinds of conclusions about what an orange does.
We can learn about nuclear power plants.
We can nobody's ever seen a proton but we can build nuclear power plants based on nothing except stuff that we read in books.
Okay, so the embodiment is not necessary reason we got into the embodiment argument in the first place is because people notice that we they couldn't do epistemic reduction, but they picked the wrong answer.
We need to actually face the epistemic reduction problem directly. So you can look at the complex world and deduce what salient and what is important in it and discard the rest like I explained in the photograph example.
You have to do that and whether you have a body or not, if you can do that, the body doesn't matter.
So that's my opinion about embodiment and enactment.
You and I have been having this conversation for years and we respectfully disagree.
Yes, that's fine.
There's many people who like embodiment I just figured out that I didn't need that.
It's what produces the base upon which the other stuff grows emerges humans and Greek humans and Greek.
I'm just text on the language understanding machines. I don't have to move around. I mean, let's consider this. There's people who basically say that you heard of the Moravik slash was test about an AI should be able to walk into any house in any city in the US and make a cup of coffee.
Right, that's the Moravik boss test for AI.
And that is basically a test, right?
Yeah, it was and it was Moravik. Moravik said it first, but it was picked up.
Okay.
So that's at least that's how I remember it. Don't quote me on that.
But the point is that that that means that AI is basically a lost cause for everybody except the roboticist and I don't like that answer.
The point is that anybody who can do autonomous epistemic reduction is an understanding machine.
If you can look at the complex world and decide what's important, pick out the highest level important concepts that are important to you at this moment.
If you can do that you are basically on AI.
Thank you. Generally, I would point out that this test is actually the AI equivalent of Heinlein's compliment man.
I have to think about that but yeah.
It was a while since I read that.
I would also say that most people walking into my kitchen would not be able to make coffee.
Yeah, yeah. I use an Aeropress stay great.
But not everybody knows how to use them.
Well, people get confused by dials that have readings and bars.
Right.
I mean, that's where the what Monica mentioned about, you know, continuous learning and self improvement is important.
Of course, you know, at birth, everybody, you know, doesn't know something.
So that's where the systems need to improve over time.
Exactly. Yeah, I mean, I'm starting out in an empty machine.
And it will learn whatever language I feed into it. Finish works just fine.
Monica, has it ever occurred to you that your work amounts to a disproof of Chomsky?
Absolutely.
My point on that my sound bite on Chomsky is that any brain that's powerful enough to analyze our visual field can learn language as a minor side task. Okay.
The visual field is so much he might be using word understanding in a different way than you are Monica.
Well, Chomsky talks about this.
Chomsky talk about this inherent language model, which is what I'm objecting to and I'm saying you don't need those that's overkill.
Any pray any brain that's good enough to for vision will handle and music, for instance, will handle language just fine.
You don't need to postulate a separate language sense in the brain for any reason.
My opinion.
Fly brain.
Excuse me.
50,000 neurons fly brain.
Yeah, yeah.
It takes that you can do amazing things with very few neurons like I said I can run language understanding and get 100% right on the Raspberry Pi.
60 with some I forget 60 million neuron or 30 million neurons something like that it's in the tens of millions.
And I've done it a few times different times so it's it might have all those numbers might be correct.
Have you thought about training your models on other sequential data sets other than with there are all kinds of things.
Name one.
Yeah, DNA is definitely in that in the in the possibility.
MIDI music is another one that will be reasonably easy and I have a piano right there so yeah it would it would work.
Yeah.
