Neural Development Programs
Open sidebar
Neural Development Programs
ChatGPT
4
You
See if you can incorporate neural Darwinism and correlation tracking 

# Organic Learning Algorithm Prototype

class OrganicLearningSystem:
    def __init__(self):
        # Initialize an empty connectome, a graph of nodes and links
        self.connectome = {}

    def _create_node(self, content):
        """ Create a new node with the given content """
        node_id = len(self.connectome) + 1
        self.connectome[node_id] = {'content': content, 'links': {}}
        return node_id

    def _add_link(self, node_id, linked_node_id, weight):
        """ Add or update a link between two nodes """
        if linked_node_id in self.connectome[node_id]['links']:
            self.connectome[node_id]['links'][linked_node_id] += weight
        else:
            self.connectome[node_id]['links'][linked_node_id] = weight

    def learn(self, data_stream):
        """ Learn from a given data stream (e.g., a text) """
        previous_node_id = None

        for char in data_stream:
            # Create or find the node corresponding to the current character
            current_node_id = self._create_node(char) if char not in self.connectome else self.connectome[char]

            # If there was a previous character, create or strengthen the link
            if previous_node_id:
                self._add_link(previous_node_id, current_node_id, 1)

            previous_node_id = current_node_id

    def get_connectome(self):
        """ Return the connectome representing the learned knowledge """
        return self.connectome

# Example usage
learning_system = OrganicLearningSystem()
learning_system.learn("sample text for learning")
connectome = learning_system.get_connectome()
print(connectome)
ChatGPT
ChatGPT
Your
OrganicLearningSystem
class is an interesting approach to modeling learning in a machine learning context, inspired by biological principles. To incorporate concepts of Neural Darwinism and Correlation Tracking, let's consider each concept:
Neural Darwinism:
This theory, also known as Neuronal Group Selection, involves three main principles - developmental selection, experiential selection, and reentrant mapping. To incorporate this into your algorithm:
Developmental Selection:
You could simulate this by initializing the connectome with a larger set of nodes and links than necessary, and then pruning the less used ones over time.
Experiential Selection:
This can be modeled by strengthening the connections (links) that are frequently used, mimicking the survival of the fittest principle. Your
_add_link
function already supports this to some extent.
Reentrant Mapping:
Implementing reentrant mapping involves creating loops in the connectome, where a node or a group of nodes can influence each other in a cyclic manner. This could be achieved by allowing feedback loops in your connectome structure.
Correlation Tracking:
This involves identifying and strengthening the connections between nodes that are frequently activated together. You could modify your learning algorithm to track and increase the weight of links between nodes that often co-occur in the data stream.
Here���s an updated version of your class with these concepts:
python
Copy code
class
OrganicLearningSystem
:
def
__init__
(
self, initial_size=
100
, pruning_threshold=
5
):
# Initialize with more nodes than needed, to simulate developmental selection
self.connectome = {i: {
'content'
:
None
,
'links'
: {}}
for
i
in
range
(
1
, initial_size +
1
)}
        self.pruning_threshold = pruning_threshold
def
_create_node
(
self, content
):
# Find an unused node to assign new content
for
node_id, node
in
self.connectome.items():
if
node[
'content'
]
is
None
:
                node[
'content'
] = content
return
node_id
raise
Exception(
"No available nodes to assign content. Consider increasing initial size."
)
def
_add_link
(
self, node_id, linked_node_id, weight
):
# Add or update a link between two nodes, for correlation tracking
self.connectome[node_id][
'links'
].setdefault(linked_node_id,
0
)
        self.connectome[node_id][
'links'
][linked_node_id] += weight
def
_prune_connectome
(
self
):
# Prune rarely used nodes and links
for
node_id, node
in
list
(self.connectome.items()):
if
sum
(node[
'links'
].values()) < self.pruning_threshold:
del
self.connectome[node_id]
else
:
for
linked_node_id, weight
in
list
(node[
'links'
].items()):
if
weight < self.pruning_threshold:
del
node[
'links'
][linked_node_id]
def
learn
(
self, data_stream
):
        previous_node_id =
None
for
char
in
data_stream:
            current_node_id =
None
for
node_id, node
in
self.connectome.items():
if
node[
'content'
] == char:
                    current_node_id = node_id
break
if
current_node_id
is
None
:
                current_node_id = self._create_node(char)
if
previous_node_id:
                self._add_link(previous_node_id, current_node_id,
1
)
                self._add_link(current_node_id, previous_node_id,
1
)
# For reentrant mapping
previous_node_id = current_node_id

        self._prune_connectome()
def
get_connectome
(
self
):
return
self.connectome
# Example usage
learning_system = OrganicLearningSystem()
learning_system.learn(
"sample text for learning"
)
connectome = learning_system.get_connectome()
print
(connectome)
This modified version introduces a basic form of Neural Darwinism through initial over-provisioning of nodes, and correlation tracking via strengthening of frequently co-occurring nodes' connections. It also attempts to implement reentrant mapping by creating two-way links between nodes. Additionally, a pruning mechanism is added to remove less used nodes and links, reflecting the developmental selection aspect of Neural Darwinism.
You
Evolutionary-based NDP 
The NDP consists of a series of developmental cycles ap- 
plied to an initial seeding graph; our experiments always use 
a seeding graph consisting of a single node or a minimal 
network connecting the neural network���s inputs directly to 
its outputs. Each of the nodes of the graph has an internal 
state represented as n���dimensional latent vector whose val- 
ues are updated during the developmental process through 
local communication. The node state-vectors ���or embed- 
dings��� encode the cells��� states and are used by the NDP 
to determine which nodes will duplicate to make new nodes. 
Similarly to how most cells in biological organisms con- 
tain the same program in the form of DNA, each node���s 
growth and the synaptic weights are controlled by a copy 
of the same NDP, resulting in a distributed self-organized 
process that incentives the reuse of information.An 
overview of the approach is shown in Fig. 1. 
The NDP architecture consists of a Multilayer Percep- 
tron (MLP) ���acting as a Graph Cellular Automata (GNCA) 
(Grattarola et al., 2021)��� which updates the node embed- 
dings after each message-passing step during the develop- 
mental phase.Subsequently, a replication model in the 
form of a second MLP queries each node state and predicts 
whether a new node should be added; if so, a new node is 
connected to the parent node and its immediate neighbors. 
Finally, if the target network is weighted, a third MLP deter- 
mines the edge weights based on the concatenation of each 
pair of node embeddings. The grown network can now be 
evaluated on the task at hand by assigning a subset of nodes 
as the input nodes and another subset as the output nodes. In 
our case, we select the i rst ���and last��� rows of the adja- 
cency matrix representing the network to act as input ��� and 
output��� nodes, respectively. During evaluation the activa- 
tions of the nodes are scalars (that is, R1instead of the Rn 
vectors used during development), and all node activations 
are initialized to zero. 
We refer to the NDP as the set of these MLPs which are 
identical for each cell in the policy network; in order to keep 
the number of parameters of the NDP low, the reported 
experiments make use of small MLPs with a single hidden 
layer. However, it���s worth noticing that because the NDP is 
a distributed model (i.e. the same models are being applied 
to every node), the number of parameters is constant with re- 
spect to the size of the graph in which it operates. Therefore, 
any neural network of arbitrary size or architecture could be 
used, provided that it was deemed necessary to grow a more 
complex graph. The NDP���s neural networks can be trained 
with any black-box optimization algorithm to satisfy any ob- 
jective function. In this paper, we demonstrate how the approach allows to grow neural networks capable of solving 
reinforcement learning and classif i cation tasks, or exhibiting 
some topological properties such as small-worldness. The 
pseudocode of the approach is detailed in Algorithm 1.
ChatGPT
ChatGPT
The concept you're describing is an advanced approach to neural network development, inspired by evolutionary and developmental biology. This "Evolutionary-based Neural Development Program (NDP)" combines ideas from neural Darwinism, graph theory, and machine learning to create a dynamic and adaptive neural network. Let's break down the key components and processes of this approach:
Initial Seeding Graph
: The process begins with a simple graph, which could be a single node or a minimal network connecting inputs to outputs.
Node State and Development
: Each node in this graph has an internal state represented by an n-dimensional latent vector. These vectors are updated through local communication, simulating the way cells in biological organisms interact and change.
Duplication and Growth Mechanism
: The system uses a form of a Multilayer Perceptron (MLP) acting as a Graph Cellular Automata (GNCA) to update node embeddings. Another MLP then determines whether a node should replicate, creating new nodes linked to their parent and neighboring nodes. This mimics cellular division and differentiation.
Weight Assignment for Edges
: If the network is weighted, a third MLP assigns weights to the edges based on the combined embeddings of the connected nodes.
Input and Output Node Selection
: For evaluation, subsets of nodes are designated as input and output nodes, typically selected from the first and last rows of the network's adjacency matrix.
Evaluation Phase
: During evaluation, the nodes' activations are scalar (as opposed to vectorial during development), and all node activations start at zero.
Scalability and Parameter Management
: Despite the complexity, the number of parameters remains constant relative to the graph's size, because the same models are applied to every node. This allows for scalability and adaptability to different sizes and architectures of neural networks.
Training and Application
: The NDP's neural networks can be trained using black-box optimization algorithms to achieve various objectives. The approach is versatile enough to be applied to a range of tasks, including reinforcement learning, classification tasks, and generating networks with specific topological properties.
This evolutionary-based NDP represents a sophisticated blend of bio-inspired algorithms and modern neural network techniques, offering a flexible and potentially powerful framework for creating and training neural networks in a way that closely mimics biological processes.
You
It's from this paper:

TechTalks
Home  Blog
Blog
Self-assembling neural networks can open new directions for AI research
By Ben Dickson -November 13, 2023

 Facebook Twitter
Brain activity
Image credit: 123RF
This article is part of our coverage of the latest in AI research.

Biological and artificial neural networks, despite their shared nomenclature, are fundamentally different. High-performing deep neural networks often demand significant engineering effort. In contrast, biological nervous systems evolve through a dynamic, self-organizing process that starts with a single cell. 

In a recently published paper, scientists at the IT University of Copenhagen, Denmark, have proposed the concept of self-assembling neural networks that ���grow through a developmental process that mirrors key properties of embryonic development in biological organisms.��� 

Their early findings hint at a promising new direction in AI research that emulates the organic formation of biological neural networks in digital intelligence.

Computer Science > Neural and Evolutionary Computing
arXiv:2307.08197 (cs)
[Submitted on 17 Jul 2023]
Towards Self-Assembling Artificial Neural Networks through Neural Developmental Programs
Elias Najarro, Shyam Sudhakaran, Sebastian Risi
Download PDF
Biological nervous systems are created in a fundamentally different way than current artificial neural networks. Despite its impressive results in a variety of different domains, deep learning often requires considerable engineering effort to design high-performing neural architectures. By contrast, biological nervous systems are grown through a dynamic self-organizing process. In this paper, we take initial steps toward neural networks that grow through a developmental process that mirrors key properties of embryonic development in biological organisms. The growth process is guided by another neural network, which we call a Neural Developmental Program (NDP) and which operates through local communication alone. We investigate the role of neural growth on different machine learning benchmarks and different optimization methods (evolutionary training, online RL, offline RL, and supervised learning). Additionally, we highlight future research directions and opportunities enabled by having self-organization driving the growth of neural networks.
ChatGPT
ChatGPT
The paper "Towards Self-Assembling Artificial Neural Networks through Neural Developmental Programs" by Elias Najarro, Shyam Sudhakaran, and Sebastian Risi at the IT University of Copenhagen introduces an innovative approach to neural network development. This approach is fundamentally different from current deep learning methods, which often require extensive engineering to design high-performing neural architectures���
���. In contrast, the proposed method mirrors the dynamic, self-organizing process observed in biological nervous systems, starting from a single cell and growing through local interactions���
���.
The researchers explore developmental and self-organizing algorithms in growing neural networks rather than manually designing them, a relatively underexplored area in AI research���
���. They present a graph neural network type of encoding, where the growth of a policy network is controlled by a Neural Developmental Program (NDP) running in each neuron. This NDP allows for the growth of neural networks during the lifetime of an agent, reflecting a more biological approach to neural network development���
���.
One key aspect of this approach is allowing each neuron in the artificial neural network to act as an autonomous agent, similar to its biological counterpart. This method could potentially enhance the robustness of AI systems against perturbations and unexpected changes, addressing some current limitations in artificial intelligence���
���.
You
Algorithm1:NeuralDevelopmentalProgram 
NDP: non-differentiable version 
Input: Replication model R, Graph Cellular 
Automata GNCA, Weight update model W, 
number of developmental cycles C, pruning 
threshold P, number of training generations 
T , training hyper-parameters ��� 
Output: Developmental program producing graph G 
that minimise/maximise F 
1Co-evolve or sample random embedding EN=0for 
the initial node N0of G; 
2for generation in T do 
3for developmental cycle in C do 
4Compute network diameter D; 
5Propagate nodes states ENvia graph 
convolution D steps; 
6Replication model R determines nodes in 
growing state; 
7New nodes are added to each of the growing 
nodes and their immediate neighbors; 
8New nodes��� embeddings are def i ned as the 
mean embedding of their parent nodes. 
9if weighted network then 
10Weight update model W updates 
connectivity for each pair of nodes based 
on their concatenated embeddings; 
11if pruning then 
12Edges with weights below pruning 
threshold P are removed; 
13Evaluate objective F of grown graph G;
ChatGPT
ChatGPT
The Neural Developmental Program (NDP) algorithm you've described outlines a process for developing a neural network that evolves and self-organizes in a manner akin to biological growth. Here's a breakdown of its key steps:
Initialization
: Co-evolve or randomly sample an initial embedding for the starting node of the graph.
Training Cycles
:
Iterate over a set number of training generations.
Within each generation, go through a specified number of developmental cycles.
Compute the network's diameter.
Propagate node states via graph convolution for a number of steps equal to the diameter.
Use the replication model to identify nodes in a growing state.
Add new nodes to each growing node and their immediate neighbors.
Define new nodes' embeddings as the mean embedding of their parent nodes.
Weight Update (if applicable)
:
If the network is weighted, update the connectivity for each pair of nodes based on their concatenated embeddings.
Pruning (optional)
:
Remove edges with weights below a specified pruning threshold.
Evaluation
:
Evaluate the objective of the grown graph.
This algorithm represents a novel approach to growing neural networks, emphasizing a process that mimics biological development, allowing for dynamic and adaptive network architecture.
You
Gradient-based NDP 
The gradient-based growth process is similar to the evolu- 
tionary approach, albeit with some additional constraints 
due to its requirement of complete differentiability in the 
process. In contrast to the evolutionary approach, the grown 
networks are exclusively feedforward networks, where in- 
formation is iteratively transmitted through message pass- 
ing via the topologically sorted nodes. Each node has a bias 
value, and an activation that is applied to when the incoming 
nodes��� information is aggregated, similar to the node behav- 
ior in e.g. NEAT (Stanley and Miikkulainen, 2002). 
Like in the evolutionary approach, cells��� states are rep- 
resented as vectors. However, instead of all values of each 
cell���s state vector being treated as intractable black-boxes 
for the network to store information, here each i rst and sec- 
Algorithm2:NeuralDevelopmentalProgram 
NDP: differentiable version 
Input: Developmental model D, number of 
developmental cycles ND, 
number of training 
iterations T , replication model R, edge 
prediction model E, reinforcement learning 
algorithm RL 
Output: Developmental program producing graph G 
that maximises reward F 
1Initialize trainable node embeddings Ninitof G; 
2for training iteration in T do 
3for i in NDdo 
41. Get new node embeddings Nivia Neural 
Message Passing D(Ninitial) 
52. Sample parent node Piusing 
replication 
channel probabilities in node embeddings 
Ni 
63. Create replicated child node with 
replication model Ci= R(Pi) and connect 
an incoming edge from Pito Ci. Connect an 
outgoing edge from Cito 
a random node at 
the further on in the network (layers deeper 
in the network). 
74. Add child node to the network 
85. Predict edge weights using Edge 
prediction model E for each edge in the 
network, using pairs of source and 
destination node embeddings 
9Collect trajectories with a rollout using grown 
graph G; 
10Update parameters via RL algorithm RL 
ond elements of the vectors have pre-def i ned roles: the i rst 
element represents the bias value and the second encodes 
the activation. The remaining encode hidden states for each 
cell, capturing temporal information used to guide the de- 
velopmental process. These cells are passed into a message- 
passing network, implemented as a graph Convolution (Kipf 
and Welling, 2016), where a neighborhood of cell embed- 
dings, characterized as being 1 edge away are linearly pro- 
jected and added to create new embeddings. In addition to 
using a message passing network to update nodes, we use 
a trainable small 2-layer MLP, with tanh activation, to pre- 
dict edge weights using pairs of (source, destination) nodes. 
The message-passing network is composed of a GraphConv 
layer that outputs vectors of size 32, followed by a Tanh ac- 
tivation, followed by a linear layer that maps the size vectors 
to the original cell embeddings. 
In order to add more nodes, we treat a channel in each cell 
as a replication probability, which is used to sample cells 
and connect to a random node further into the network. This 
process happens every other growth step.
ChatGPT
ChatGPT
The Gradient-Based Neural Developmental Program (NDP) is an alternative to the evolutionary approach, with a focus on maintaining complete differentiability throughout the process. This method creates exclusively feedforward networks, with node behavior akin to those in NEAT (NeuroEvolution of Augmenting Topologies). Key features include:
Node Representation
: Each node has a bias value and activation, with cell states represented as vectors. The first two elements of the vectors have predefined roles for bias and activation, while the rest encode hidden states for guiding development.
Neural Message Passing
: The developmental model utilizes a graph convolution approach for message passing, updating node embeddings and capturing temporal information.
Node Replication and Connection
: A channel in each cell's vector represents replication probability, guiding the sampling of cells and their connection to random nodes deeper in the network.
Edge Weight Prediction
: A 2-layer MLP with tanh activation is employed to predict edge weights using pairs of node embeddings (source and destination).
Network Growth and Learning
: The network grows through iterative cycles, with parameters updated via a reinforcement learning algorithm, focusing on maximizing a specified reward function.
This gradient-based approach provides a structured yet adaptable method for developing neural networks, emphasizing differentiability and feedforward architecture.
You
Detailed replication process: (1) A replication network, 
implemented as a separate graph convolution, is applied on 
cells to output replication probabilities. (2) A cell is sam- 
pled from these probabilities and is passed into a perturba- 
tion network to get a new cell, and an incoming edge is con- 
nected from the parent to the new cell. An outgoing edge is 
connected to a random child (found further in the network). 
After the new node is added to the network, we update the 
edges using an MLP that takes in incoming node and outgo- 
ing node pairs and outputs a new edge weight. 
We initialize the starting network by fully connecting 
trainable input cell embeddings and output cell embeddings, 
which we found to work better for our gradient-based NDP 
than starting with a single node. For example, if input = 4 
and output = 2, then each input node will be connected to an 
output node, resulting in 4��2 initial edges. The pseudocode 
of this approach is detailed in Algorithm 2.
ChatGPT
ChatGPT
The replication process in the gradient-based Neural Developmental Program (NDP) involves several key steps:
Replication Probability Calculation
: A separate graph convolution network calculates replication probabilities for each cell.
Cell Sampling and Perturbation
: A cell is chosen based on these probabilities and is modified by a perturbation network to create a new cell.
Edge Connection
: An incoming edge is formed from the parent cell to the new cell, and an outgoing edge is connected to a randomly chosen child cell deeper in the network.
Edge Weight Update
: After adding the new node, the network updates edge weights using an MLP that processes pairs of incoming and outgoing nodes.
Network Initialization
: The starting network is initialized by fully connecting trainable input and output cell embeddings. This initial setup includes creating multiple edges based on the number of input and output nodes.
This detailed replication process enables the gradient-based NDP to grow and evolve the network architecture effectively, with the initial fully connected setup providing a robust foundation for further development.
You
Discussion and Future Work 
We introduced the idea of a neural developmental program 
and two instantiations of it, one evolutionary-based and one 
gradient descent. We showed the feasibility of the approach 
in continuous control problems and in growing topologies 
with particular properties such as small-worldness. While 
the approach is able to solve these simple domains, many 
future work directions remain to be explored. 
For example, the current NDP version does not in- 
clude any activity-dependent growth, i.e. it will grow the 
samenetworkindependentlyoftheincomingactivationsthat 
the agent receives during its lifetime.However, biologi- 
cal nervous systems often rely on both activity and activity- 
independent growth; activity-dependent neural development 
enables biological systems to shape their nervous system de- 
pending on the environment. Similar mechanisms also form 
the basis for the formation of new memories and learning. 
In the future, we will extend the NDP with the ability to 
also incorporate activity-dependent and reward-modulated 
growth and adaptation. 
While a developmental encoding offers the possibility to 
encode large networks with a much smaller genotype, the NDP in this paper are in fact often larger than the result- 
ing policy networks. However, by running the developmen- 
tal process longer, it is certainly possible to ultimately grow 
policy networks with a larger number of parameters than the 
underlying NDP. However, as the results in this paper sug- 
gest, growing larger policy networks than necessary for the 
tasks can have detrimental effects (Table 1b), so it will be 
important to also learn when to stop growing. The exact in- 
terplay between genome size, developmental steps, and task 
performance constitutes important future work. 
We will additionally extend the approach to more com- 
plex domains and study in more detail the effects of growth 
and self-organization on the type of neural networks that 
evolution is able to discover. NDPs offer a unifying prin- 
ciple that has the potential to capture many of the proper- 
ties that are important for biological intelligence to strive 
(Versace et al., 2018; Kudithipudi et al., 2022). While in- 
natestructuresinbiologicalnervoussystemshavegreatlyin- 
spired AI approaches (e.g. convolutional architectures being 
the most prominent), how evolution discovered such wiring 
diagrams and how they are grown through a genomic bottle- 
neck are questions rarely addressed. In the future, NDPs 
couldconsolidateadifferentpathwayfortrainingneuralnet- 
works and lead to new methodologies for developing AI sys- 
tems, beyond training and i ne-tuning.
ChatGPT
ChatGPT
The discussion and future work section of the paper on Neural Developmental Programs (NDPs) outlines several important areas for further research:
Activity-Dependent Growth
: The current NDPs lack activity-dependent growth, a feature common in biological nervous systems. Future versions of NDPs aim to incorporate this, allowing the network to adapt based on environmental interactions.
Genotype vs. Phenotype Size
: While developmental encoding allows for large networks with smaller genotypes, the current NDPs are often larger than the resulting networks. Future research will focus on when to stop growing the network and the relationship between genome size, developmental steps, and task performance.
Complex Domains and Evolution
: Future work will extend NDPs to more complex domains and examine how growth and self-organization affect the types of networks that can be evolved.
New Methodologies in AI
: Ultimately, NDPs could lead to new approaches for training neural networks, beyond traditional training and fine-tuning methods, and offer insights into biological nervous system development and its application in AI.
You
Do you think you can make a simple prototype program in Python that illustrate the technique?
ChatGPT
ChatGPT
Creating a complete and functional prototype of a Neural Developmental Program (NDP) as described in the paper would be quite complex, involving advanced concepts like graph neural networks, evolutionary algorithms, and neural message passing. However, I can provide a simplified Python sketch to illustrate the basic concept. Keep in mind that this will be a highly abstracted version and won't capture the full depth or functionality of the actual NDPs described in the research.
python
Copy code
import
numpy
as
np
import
random
class
SimpleNDP
:
def
__init__
(
self, input_size, output_size
):
        self.input_size = input_size
        self.output_size = output_size
        self.nodes = np.random.rand(input_size + output_size,
2
)
# Initialize node embeddings
self.edges = np.zeros((input_size + output_size, input_size + output_size))
def
grow_network
(
self, growth_cycles
):
for
_
in
range
(growth_cycles):
# Choose a node to replicate based on some criteria (simplified here)
parent_node = random.randint(
0
,
len
(self.nodes) -
1
)
            child_node = np.mean(self.nodes[parent_node])
# Simplified new node creation
# Add the new node to the network
self.nodes = np.vstack([self.nodes, child_node])
            self.edges = np.vstack([self.edges, np.zeros(
len
(self.nodes))])
            self.edges = np.hstack([self.edges, np.zeros((
len
(self.nodes),
1
))])
# Connect parent and child
self.edges[parent_node,
len
(self.nodes) -
1
] =
1
self.edges[
len
(self.nodes) -
1
, parent_node] =
1
def
get_network_structure
(
self
):
return
self.nodes, self.edges
# Example usage
ndp = SimpleNDP(input_size=
4
, output_size=
2
)
ndp.grow_network(growth_cycles=
5
)
nodes, edges = ndp.get_network_structure()
print
(
"Nodes:"
, nodes)
print
(
"Edges:"
, edges)
This prototype initializes a network with a given number of input and output nodes, each with a simple 2-element vector representing their state. The network then grows through a specified number of cycles, randomly choosing nodes to replicate. The edges and nodes are updated accordingly. This is a basic illustration and lacks the complexity of real NDPs, like message passing, replication probability calculation, or advanced learning algorithms.
You
I guess they mentioned there will be some code released to github soon
ChatGPT
ChatGPT
It's quite common for researchers to release their code on platforms like GitHub, especially for groundbreaking work in fields like artificial intelligence. If the authors of the paper on Neural Developmental Programs mentioned plans to release their code, it would be a valuable resource for those interested in exploring and experimenting with these advanced concepts. Keep an eye on the GitHub repositories of the authors or their affiliated institutions, as these are likely places where the code could be published.
You
A summary of the whole conversation.
ChatGPT
ChatGPT
In this conversation, we discussed a research paper on Neural Developmental Programs (NDPs) that propose a new approach to developing neural networks, inspired by biological neural development. The paper details two versions of NDPs: one evolutionary-based and one gradient-based. We discussed how these approaches mimic biological processes, such as cellular growth and neural development, and how they differ from traditional neural network design. We also touched on the potential for incorporating activity-dependent growth into future NDP versions. Finally, I provided a simplified Python sketch to illustrate the concept of NDPs and mentioned the possibility of the researchers releasing their code on GitHub for public access.
ChatGPT can make mistakes. Consider checking important information.
?
ChatGPT

