I find this memory. It's usually when I discover something that I think nobody has seen before,
but that happens very rarely because most of the things you think of somebody else has done before.
This episode is sponsored by Numeri. Are you a data scientist looking to make a real world
impact with your skills? Do you love competing against the best minds in the world? Well,
introducing Numeri, the revolutionary cutting-edge AI-driven hedge fund that's changing the game
for good. Numeri combines a competitive data science tournament with powerful clean stock market
data, enabling you to predict the market like never before. Sign up now, become part of the
elite community, taking the stock market by storm, and I'll see you on the leaderboard.
Wonderful. So today is a momentous occasion. What an episode of MLST we're going to have.
We're joined not by a godfather of AI, but the father of AI, you again, Schmidhuber, the researcher
responsible for leading the research groups which invented much of the technology which is powered
the deep learning revolution. It's long been a dream to get you on the podcast, you again.
It feels like the day has finally arrived, so welcome to MLST. Thank you, Tim, for these very
kind words and this very generous introduction.
So on that, let's discuss the credit assignment problem in machine learning. Now,
you've dedicated a significant amount of time researching and publishing the actual history
of the field, and there's a significant divergence between the public narrative and what actually
happened. And amazingly, no one has pointed out any factual inaccuracies in your accounts,
but the incorrect perceptions still persevere. Now, I particularly enjoyed reading your history
of the breakthroughs in machine learning, going back to ancient times and of course even remarking
on the very first computer scientist, Leibniz. And for example, you pointed out the history of
who invented backprop and the CNN, and you explained that there wasn't really a neural
network winter at all in the 1970s. So could you just sketch out a little bit of that history?
So that's a challenge. Actually, computer science history and computing history started
maybe 2000 years ago when Heron of Alexandria built the first program controlled machine.
That was 2000 years ago in the first century, basically. And he basically built an automaton
that was programmed through a cable, which was wrapped around a rotating cylinder, which had
certain knobs. And then there was a weight which pulled it down. And the whole apparatus was able
to direct the movements of little robots, of little puppets in an automatic theater. That,
as far as I know, was the first program controlled machine in the history of mankind.
Even before that, there were other machines. The ancient Greeks had even earlier the
antiquity mechanism, which was kind of a clock, an astronomical clock. But then more recently,
we have seen many additional advances. And you mentioned Leibniz, of course, who is of
special interest to our field, because he not only is called the first computer scientist,
because he had the first machine with a memory that was in the 1680s, I think.
He not only had the first machine that could do all the basic arithmetic operations,
which are addition, multiplication, division, and subtraction. Then he not only had these
first ideas for a universal problem solver that would solve all kinds of questions,
even philosophical questions, just through computation. And he not only was the first
who had this algebra of thought, which is deductively equivalent to the much later Boolean
algebra. So in many ways, he was a pioneer. But especially in our field and deep learning,
he contributed something essential, which is really central for this field, which is the chain rule.
I think 1676, that's when he published that. And that's what is now being used to train very
deep artificial neural networks, and also shallow neural networks and recurrent neural
networks. And everything that we are using in modern AI is really, in many ways, depending on
that early work. But then of course, there was so much additional work. The first neural networks,
as we know them, they came out, they came up about around 1800. That's when Gauss and Legendre
had the linear neural networks, the linear perceptrons, in the sense that they were linear
without being without having any non differential aspect to it. So these first neural networks
back then were called method of least squares. And the the training method was regression.
And the error function was exactly the same that we use today. And it was basically just a network
with a set of inputs, and a set of outputs, and a linear mapping from the inputs to the outputs.
And you could learn to adjust the weights of these connections. So that was the first
linear neural network. And many additional later developments led to what we have today.
You had this beautiful statement. You said that machine learning is the science of credit
assignment. And we should apply that same that same science to the field itself. And I guess
what I'm really curious about is, first, if you could educate our listeners just a bit on
what credit assignment is in the context of say machine learning, and why you think it's important
that that should apply to the field in general, you know, why should we care about credit assignment,
why should we study the history of of, you know, the developments in the field, why is it important?
I'm interested in credit assignment, not only in machine learning, but also in the history of
machine learning, because machine learning itself is the science of credit assignment.
What does that mean? Suppose you have a complicated machine, which is
influencing the world in a way that leads to the solution of a problem. And maybe the machine
solves the problem. But then the big question is, which are the components of these many components
were responsible? Some of them were active a long time ago, and others later, and early
actions set the stage for later actions. Now, if you want to improve the performance of the machine,
you should figure out how did the the components contribute to the overall success. And this is
what credit assignment is about. And in machine learning, in general, we have a system consisting
of lots of many machine learning engineers, and mathematicians, and hardware builders,
and all kinds of people. And there you also would like to figure out which parts of the system
are responsible for later successes. Yeah, and it's a brilliant point. And I completely agree
with you, by the way. And I think the way I think about it is, you've got this giant
architecture of humanity. And in it are these certain nodes that may be an individual, maybe a
research group. And if they come up with things that are very helpful, right, you want to try and
direct more attention, more resources, you know, at that, that nodule at that node, right, because
it's likely to come up with additional, you know, very, very important things. And if we don't get
that right, we're just not optimizing the the algorithm of science as a whole.
That's right. Yes. Machine learning and science in general is based on this principle of credit
assignment, where credit usually doesn't come in form of money, sometimes also in form of money,
but in form of reputation. And then the whole system is set up such that you
create an incentive for people who have worked on improving some method to credit those who
maybe came up with the original method, and to just have these chain, these chains of credit
assignment that may clear who did what when, because the whole system is based on this incentive.
And yes, those who are then credited with certain valuable contributions, they also can get reasonable
jobs and, and the within the economy and so on. But that's more like the secondary
consequence of the basic principle. And that's why all PhD advisors
teach their PhD students to be meticulous when it comes to credit assignment to past work.
So one last question, if I may, I've really enjoyed studying the history of, of advancement,
because I found that when I go back and read original source materials, you know, let's say
Einstein's first paper on diffusion, okay, or anything like that, you know, because they're
breaking new ground, they're kind of considering like a wider array of possibilities. And then over
time, you know, the field becomes more and more focused kind of on a narrower avenue of that.
And you can go back and look at the original work and actually gain a lot of inspiration for
alternative approaches or alternative, you know, considerations. So in a sense, it's,
it's kind of in the, in the sense of forgetting is as important as learning, you know, sometimes we
need to go back to go down a different branch of the tree, if you will, and expand the breadth of
the search a little bit. I'm curious if you've noticed that same phenomenon.
Yes, science in general is about failure. And 99% of all scientific activity is about
creating failures. But then you learn from these
failures, and you do backtracking. And you will go back to a previous decision point where you
maybe made the wrong decision and pursued the wrong avenue. But now you have a branching
point and you pursue an alternative. And, and in a field that is rapidly moving forward,
you don't go back very far. Usually, you just go back to a recent paper, which came out five
months ago. And maybe you have a little improvement there. And then maybe there's yet another little
improvement there. And some parts of our field are at the moment, a little bit like that,
where PhD students are moving in, who just look at most recent papers, and then find a way of
improving it a little bit and, you know, 2% better results on this particular benchmark.
And then the same guys are also reviewing at major conferences, papers by similar students,
and so on. And so then sometimes what happens is that no very deep backtracking is happening,
just because the actors aren't really aware of the entire search tree that has already been
explored in the past. On the other hand, science has this way of healing itself. And
since you can gain reputation by identifying maybe more relevant points, branching points,
you, you have this incentive within the whole system to, to improve things as much as you can,
sometimes by going back much further.
So there's been a lot of discussion in the discourse around this concept of AI existential
risk. And you again, you've published quite a few pieces about this recently,
prominently in the Guardian and in Forbes, actually. And one of the things I wanted to focus on is
this concept of recursive self improvement, because that seems to be one of the plausible
explanations that these folks give. And of course, when it comes to recursive self improvement,
you are an expert in this field. I mean, Godel machines come to mind immediately.
So I want to kind of explore asymptotes and limitations.
This whole idea of recursive self improvement is very sexy, isn't it?
In fact, it is the one idea that motivated me to do all of this. So my first paper ever in 1987,
that was my diploma thesis. And it was about this recursive self improvement thing.
So it was about a machine that learns something in a domain. But not only that,
it also learns on top of that to learn a better learning algorithm based on experience in the
lower level domains, and then also recursively learns to improve the way it learns. And then
also recursively learns to improve the way it improves the way it improves the way it learns.
And yeah, I called that meta learning. And back then, I had this hierarchy with
in principle, infinite self improvement in the in the recursive way.
Although it is always limited by the limited time that you run the system like that.
And then, of course, the motivation behind that is that you don't want to have an artificial system
that is stuck always with the same old human designed learning algorithm. No, you want something
that improves that learning algorithm without any limitations, except for the limitations of
physics and computability. And so much of what I have been doing since then
is really about that self improvement and different settings where you have, on the one hand,
reinforcement learning systems that learn in an environment to better interact and better create
ways of learning from these interactions to learn faster and to learn to improve the
way of learning faster, and so on. And then also gradient based systems, artificial neural
networks that learn through gradient descent, which is a pre wired human designed learning
algorithm to come up with a better learning algorithm that works better in a given set of
environments than the original human designed one. And yeah, that started around 1992, neural
networks that learn to run their own learning algorithms on the recurrent network themselves.
So you have a network which has standard connections and input units and output units,
but then you have the special output units which are used to address
connections within the system, within this recurrent network, and they can read and write them.
And suddenly, because it's a recurrent network, and therefore it is a general purpose computer,
suddenly you can run arbitrary algorithms on this recurrent network, including arbitrary
learning algorithms that translate incoming signals, not only the input signals, but also
the evaluation signals like reinforcement signals or error signals into weight changes,
fast weight changes, where the weight changes are not dictated any longer
through this gradient descent method. But no, now the network itself is learning to do that.
But the initial weight matrix is still learned through gradient descent, which is propagating
through all these self-referential dynamics in a way that improves the learning algorithm
running on the network itself. That was 1992. And back then, compute was really,
really slow, was a million times more expensive than today, and you couldn't do much with it.
But now in recent works, all of that is working out really nicely and has become popular. And we
have, just if you look at the past few years, a whole series of papers just on that.
So that's the fast weight programming that you're referring to?
Yes. So it's fast weight programmers where you have a part of the network that learns to
quickly reprogram another part of the network. Or the original version of that was actually
two networks. So one is a slow network, and then there's another one, a fast network,
and this slow network learns to generate weight changes for the second network.
And the program of the second network are its weights. So the weight matrix of the second
network, that is the program of the second network. And the first one, what does it do?
It generates outputs, it learns to generate outputs that cause weight changes in the second
network. And these weight changes are being applied to patterns, to input patterns, to queries, for
example. And then the first network essentially learns to program the second network. And
essentially the first network has a learning algorithm for the second network. And the first
system of that kind, 1991, that was really based on keys and values. So the first network
learns to program the second network by giving it keys and values. And it says now take second
network, take this key and this value and associate both of them through an outer product,
which just means that those units that are strongly active, they get connected through
stronger connections. And the mathematical way of describing that is the outer product between
key and value. So that's how the first network would program the second network. And the important
thing was that the first network had to invent good keys and good values, depending on the context
of the input stream coming in. So it used the context to generate what is today called an
attention mapping, which is then being applied to queries. And this was a first step right before
the most general next step, which is then really about learning a learning algorithm running on
the network itself for the weights of the network itself. Could I press you a tiny bit on this concept
of meta learning and convergence and asymptotes. Now, one of the reasons I think why the X risk
people believe that it will just go on forever is they believe in this idea of a pure intelligence,
one that doesn't have physical limitations in the real world. And I'm quite amenable to this
ecological idea of intelligence that it does, you know, the world is a computer basically,
as well as the actual brain that we're building. So surely it must hit some kind of asymptote.
Do you have any intuition on what those limitations would be? So you are talking about the ongoing
acceleration of computing power and limitations thereof. Is that what you have in mind here?
Well, that's one part of it. So even if you just scale transformers, I think there would be some
kind of asymptote, but we're talking here about meta learning, learning to learn how to learn,
and recursive self improvement. And it's similar to this idea of reflection, self reflection and
language models. It actually improves the performance with successive steps of reflection.
And then it levels off, it reaches an asymptote. I just believe that there are asymptotes everywhere.
And that's the reason why I don't think recursive self improvement will go on forever.
But I just wondered if you had any intuitions on what those impressions are.
You are totally right. There are certain algorithms that we have discovered and past decades,
which are already optimal in a way, such that you cannot really improve them any further.
And no self improvement and no fancy machine will ever be able to further improve them.
There are certainly certain sorting algorithms that under given limitations are optimal and you
can further improve them. That's one of the limits. Then of course, there are the fundamental
limitations of what's computable. First identified by Kurt Gödel in 1931. He just showed that there
are certain things that no computational process can ever achieve. No computational
theorem prover can prove or disprove certain theorems in a language, in a symbolic language
that is powerful enough to encode certain simple principles of arithmetic and stuff like that.
And so what he showed was that there are fundamental limitations to all of computation.
And therefore, there are fundamental limitations to any AI based on computation.
I'm glad you brought that topic up because it's one of our favorite things to discuss,
which is, do you think the human mind ultimately reduces to just an effective computation? And
so subject to those same limits? Or do you think there's any known or unknown physics that give us
some out in which the brain can do a computation that amounts to hyper computation? Yeah.
Since we have no evidence that the brain can compute something that is not computable
in the traditional sense, in Gödel's sense and Turing's and Church's sense and everybody
who has worked on this field, since we have no evidence, we shouldn't assume that's the case.
As soon as someone shows that people can compute certain things or prove certain theorems that
machines cannot prove given the same initial conditions, we should look more closely. But
there are many things that might be possible in fairy tales and we are not really exploring them
because the probability of coming up with interesting results is so low. Fair enough.
So you mentioned so far two asymptotes, one being of the mathematical kind where there's
just mathematical proofs that certain things are optimal, the other one being the limits of
computation itself. What other asymptotes do you see applying to or putting bounds on recursive
self-improvement? Oh, the most obvious thing is probably light speed and the limits of
quality. Physically computation, yes. And we know those. For several decades, we have happily
enjoyed the fact that every five years, computers getting ten times cheaper. And this process
started long before Moore's law was defined in the 60s, I believe, because even in 1941 already,
when Tzuzer built the first program-controlled computer, this law apparently was active. So
back then he could compute maybe one instruction per second. And since then, you know, every 10
years a factor of 100, every 30 years a factor of a million, more or less until today. And there's no
reason to believe it won't hold for a couple of additional decades, because the physical limits
are much further out. The physical limits that we know are the Bremermann limit,
discovered, I think, in 1983 by Bremermann. And they basically say that one kilogram of matter
cannot compute more than 10 to the 51 instructions per second. So that's a lot of compute,
but it's limited. To give you an idea of how much compute that is, I also have a kilogram of
computer in here, and probably it cannot compute 10 to the 20 instructions per second.
Otherwise, my head would explode because of the heat problem. But maybe it can compute something
that is not so far from 10 to the 20 instructions, maybe 10 to the 17, something like that. Although
most of my neurons are not active as we speak, because again, otherwise my head would just
evaporate. Now, if you have an upper limit of 10 to the 20 instructions per brain, then the
upper limit of all of humankind would be 10 billion times that individual limit. And that would be 10
to the 30 instructions per second. And you see, it's still far away from the 10 to the 51 instructions
per second that in principle, one kilogram of matter could compute. And now we have more than
10 to the 30 kilograms of matter in the solar system. And if the current trend continues,
at some point much of that is going to be used for computation, but then it will have to slow down,
even if the exponential acceleration will still be with us for a couple of decades,
because at some point it is going to be polynomial law, because
due to the limits of light speed, at some point it will be harder and harder to acquire
additional mass. Once you have reached the limits of physical computation per kilogram,
the only way to expand is to go outwards and find additional stars and additional matter
further away from the solar system. And then you will get a polynomial
acceleration or a polynomial growth at best. So it will be much worse than the current
exponential growth that we are still enjoying.
Sure. But I would say the existential threat, that's more than sufficient to supply an existential
threat. And let me just put this a little bit differently, which is, and I agree with you on
this, which is your quoted as saying that traditional humans won't play a significant role
in spreading intelligence across the universe. And I think you're right. I think we kind of
share a vision of something like the von Neumann probes that go out into space and form this
star spanning civilization of machines and artificial intelligence that have transcended
biological limitations. So I guess my question to you is, once that space faring star spanning
civilization exists, if it becomes misaligned with us and decides that we're in the way,
right? Isn't that an existential threat? I mean, might they just repurpose the earth,
regardless of whether we're here or not for their own aims?
Yeah, I'm often getting these questions. And there is no proof that we will be safe forever
or something like that. On the other hand, it's also very clear as far as I can judge
that all of this cannot be stopped. And it can be channeled in a very natural and,
I think, good way in a way that is good for humankind. Now, first of all, at the moment,
we have a tremendous bias towards good AI, meaning AI that is good for humans. Why? Because
there is this intense commercial pressure to create stuff that humans want to buy.
And they like to buy only stuff they think is good for them, which means that all the companies
that are trying to devise AI products, they are maximally incentivized to generate AI products
that are good for those guys who are buying them, or at least where the customers think it's good
for them. So maybe 5% of all AI research is really about AI weapons and one has to be worried about
that. One always has to be worried about weapons research. But there's a tremendous bias towards
good AI. So that is one of the reasons why you can be a little bit optimistic for the future.
I'm always trying to point out the two types of AI. There are those who are
just tools of users, human users, and the others that invent their own goals,
and they pursue their own goals. And both of them we have had for a long time. Now, for the AI tools,
it's kind of clear. There's a human, and a human wants to achieve something, and so he uses or she
uses that tool to achieve certain ends. And most of those are of the type, let's improve health care,
and let's facilitate translation from one person to another one in another nation,
and just make life easier and make human lives longer and healthier. Okay, so that's the AI tools.
But then there are the other AIs, which also have existed in my lab for at least 32 years,
which invent their own goals. And they are a little bit like little scientists, where you have
an incentive to explore the environment through actions, through experiments,
self-invented experiments, that tell you more about how the world works, so that you can become
a better and better and more and more general problem solver in that world. And so these AIs,
they have for a long time created their own goals. And now, of course, the interesting question is
these more interesting AIs, what are they going to do once they are,
once they have been scaled up and can compete or maybe outperform humans in everything they want
to achieve? So on the one hand, the AI tools, and there the greatest worry is, what are the other
humans going to do to me with their AI tools? So in the extreme case, you have people who are
using AI weapons against you, and maybe your neighbor has bought a little drone for $300,
and it has face recognition, and it has a little gripper, and it flies across the hedge and puts
some poison into your coffee or something like that. So then the problem is not the AI,
which is trying to enslave humans or something silly like that. No, it's your neighbor or the other
human. And generally speaking, you have to be much more afraid of other humans than you have to be
of AIs, even those who define or set themselves their own goals, because you must mostly worry
about those with whom you share goals. So if you share goals, then suddenly there is a potential
of conflict, because maybe there is only one schnitzel over there, and two persons want to
eat the schnitzel, and suddenly they have a reason to fight against each other. Generally speaking,
if you share goals, then you can do two things. You can either collaborate or compete. An extreme
form of collaboration would be to maybe marry another person and settle about family and
master life together, and an extreme form of competition would be war. And those who share
goals, they have many more incentives to interact than those who don't share goals. And so humans
are mostly interested in other humans because they share similar goals and because they give them
reason to collaborate or to compete. More CEOs at certain companies are interested in other CEOs of
competing companies, and five-year-old girls are mostly interested in other five-year-old girls.
And the super smart AIs of the future who set themselves their own goals, they will be mostly
interested in other super smart AIs of the future who set themselves their own goals.
Generally speaking, there is not so much competition, and there are not so many shared goals
between biological beings, such as humans, and a new type of life that, as you mentioned,
can expand into the universe and can multiply in a way that is completely infeasible for
biological beings. So there's a certain long-term protection, at least, through lack of interest
on the other side. Okay, brilliant. There's a few things I wanted to touch on there. We will get on
to what it means for goals to emerge from systems later. And you started off by saying that humans
will buy products that make them feel good. And Facebook is quite an interesting example to play
with, actually, because Facebook is a little bit like an AI system, which is a collective
intelligence. And humans use Facebook, but they have some idea that it might cause them harm.
And the thing with population ethics is we know that our moral reasoning kind of decays over space
and even more so over time. And part of the reason why time is so difficult is because
it's predictive. We don't actually know what's going to happen in the future. So our kind of
reasoning about establishing what the value of something is is very, very faulty. And I think
that's one of the reasons why these people would say that we don't really know what's good for us.
I do completely agree with you, though, that the problem, I think, is humans
rather than AIs on their own. Yes, these are good points.
Feel free to offer some thought. Yes. I mean, it would, that's a whole separate discussion,
isn't it, when you discuss the limitations of what's predictable and how people often fail
to see what's good for them. Well, I think maybe, so you've already said that there's no proof
that we'll be safe forever, right? Like, I mean, there could come an existential risk,
you know, from AI. So I think my question to you is, do you have sympathy for the folks who say
we need to be putting more resources into researching alignment? Like, we need to develop the tools
in order to allow it to be easier for people to construct AI that is aligned for the goals
and to make sure that, you know, that it doesn't, that it doesn't have unintended consequences.
Like, in other words, there may not be a proof that we can go forever and be safe for AI,
but we at least want to develop the basic mechanics that we need to safely develop and deploy AI,
don't we? Yes. I sympathize with those who are devoting their lives to alignment issues and
trying to build AIs aligned with humans. I view them as part of the evolution of all kinds of
other ideas that come up as not only nations compete with other nations, but companies compete
with other companies and shareholders of different companies compete with shareholders of different
companies and so on. And so there is such a huge set of different human goals which are not aligned
with each other that makes me doubt that you will come up with a general system that all humans can
accept simply because if you put 10 humans in a room and ask them what is good, they will give
you 10 different opinions. However, I sympathize with this goal and it's good that people are worried
and they spend resources on solving some of these issues in the long run. However, I think there is
no way of stopping all kinds of AIs from having all kinds of goals that have very little to do
with humans. The universe itself is built in a certain way that apparently
derives it from very simple initial conditions to more and more complexity. And now we have
reached a certain stage after 13.8 billion years of evolution and it seems clear that this cannot
be the end of it because the universe is still young, it's going to be much older than it is now.
Now there is this drive, built in drive, of the cosmos to become more complex and it seems clear
that civilization, a civilization like ours is a stepping stone on towards something that is
more complex. Could I touch on a couple of things here? The bootloader example is kind of where I
want to go with this. So a lot of the ideas of this movement can be traced back to Derek Parfit
who is a philosopher. He was a moral realist so he thought there was such a thing as a moral fact.
And I'm a bit of a relativist myself and actually if you trace this tree of complexity
and how humans evolve over time, we might just be a stepping stone to a kind of rich diverse
transhumanist future where we become the thing over time that we're so scared of. And I think
the lens that we're using here about what's right and what's wrong is kind of like I was saying
before, it's a snapshot of humanity now and we kind of think of it as just this monolithic single
thing. So does it really work when you project out to how we're going to evolve in the future?
But first of all, humankind is not a monolithic thing. So many of these arguments go like we
should not do that because of that. We should not do that because of that. But there is no us.
There is no we. There are only almost 10 billion different people and they all have different
ideas about what's good for them. And so for thousands of years we had these evolutions of
ideas and of devices and philosophies competing, partially competing and partially compatible
with each other, which in the end led to the current values that some people agree with
and other people over there, they agree with different values. Nevertheless, there are certain
values that have become more popular than others, more successful, more evolutionary
with more success during the evolution of ideas. And so given this entire context of evolution of
concepts and accepted ideas of what should be done or what is worth being supported and what's
not worth being supported, all of this has changed a lot. If we look back 200 years,
the average people in the West had different ideas of what's good than today.
And this evolution of ideas is not going to stop any time soon.
Just a final question on this. There is a very real existential risk right now of nuclear
Armageddon, a real risk right now. And if I were a rational person, I would be devoting
all of my effort into that and other risks associated. So do you think it's a little bit
weird that so much focus is on this AIX risk? To me, it's indeed weird. Now there are all these
letters coming out warning of the dangers of AI. And I think some of the guys who are writing these
letters, they are just seeking attention because they know that AI dystopia are attracting more
attention than documentaries about the benefits of AI in healthcare and stuff like that. But
generally speaking, I am much more worried about nuclear bombs than about AI weapons.
A nuclear bomb, a big one, can wipe out 10 million people, a big city within a few milliseconds,
without any face recognition, just like that, without any AI. And so in that sense, it's much more
harmful than the comparatively harmless AI weapons that we have today and that we can currently
conceive of. So yes, I'm much more worried about 60 year old technology that can wipe out civilization
within two hours without any AI. Well, I guess since we're not really going to worry about
AI for the moment, we can turn our attention back to discussing with you how we develop AI.
So I'm really curious with just the vast breadth and depth of your knowledge over the history of
AI and the state of the art, I'm curious which current approaches you're most excited about
and or what's on the horizon that for any of our listeners out there thinking about
going into AI research, machine learning research, what maybe alternatives that aren't
getting enough attention should they look into studying and perhaps choosing to research?
At the moment, the limelight is on language models, large language models, which pass the
touring tests and do all kinds of things that seemed inconceivable just a couple of years ago,
at least to some of those who are now surprised. But of course, that is just a tiny part of
what's going to be important to develop true AI, AI, artificial general intelligence.
On the other hand, the roots of what we need to develop true AI also come from the previous
millennium, they are not new. And of course, what you need is an environment to interact with.
And you need an agent that can manipulate the environment. And you need a way of
learning to improve the rewards that you get from this environment as you are interacting
it with it within a single lifetime. So one of the important aspects of reinforcement
learning, what we are now talking about is that you have only one single life. You don't have
repeatable episodes, like in most of traditional reinforcement learning, no, you have only one
single life. And in the beginning, you know nothing. And then after 30% of your life is over,
you know something about life. And all you know is the data that you collected during these first
30% of your life. And now there is an infinite, almost infinite set of possibilities of futures.
And from this little short experience, you have to generalize somehow and try to select
action sequences that lead to the most promising futures that you can shape yourself through your
actions. Now to achieve all of that, you need to build a model of the world, a predictive model of
the world, which means that you will have to be able to learn over time and to predict the
consequences of your actions so that you can use this model of the world that you are acquiring
there to plan, to plan ahead. And you want to do that in a way that isn't the naive way,
which we had in 1990, which is millisecond by millisecond planning, where you say, okay, now
I'm moving from A to B. And the way to do it is first move that little pinky muscle a little
bit and move it a little bit more and move it a little bit more and then get up. And so,
you want to do that in a high-level way, in a hierarchical way, in a way that allows you to
focus on the important abstract concepts. For example, as you are trying to go from
your home to Beijing, you decompose this whole future into a couple of sub-goals. You say,
first important step is to go to the cap station and get a taxi to the airport. And then in the
airport, you will find your plane. And then for nine hours, nothing is going to happen. And you
exit in Beijing and have to find another cap and so on. So you don't do millisecond by millisecond
detailed planning. No, you have high-level planning to just reduce the computational effort
and focus on the essentials of what you want to do. So that is something that most current systems
don't do. But for a long time, we have had systems like that, and they are getting more
sophisticated over time. Important, you have a predictive model of the world that is not just
focusing on the pixels and predicting the how does the video change as I'm moving my hand back
and forth, the video that I get through my camera and my eyes and so on. No, higher-level concepts
that reflect islands of predictability. Many things are not predictable, but certain abstract
representations of these things are predictable. And so how can you discover these higher-level
concepts that you need to efficiently think about your own future options and select those that are
most promising in the single life? Yeah. Yeah. This is really interesting. So we've been speaking
with Carl Friston, for example, and he talks about this collective intelligence where you have this
multi-agent cybernetic framework, which is causally closed. And one of the things we're
talking about here really is not the model itself. People talk about chat GPT and it's just a model
and people have configured it in arrangements that have varying degrees of autonomy. And in the
future we will develop these collective intelligences and they're not just predicting the actions and
behaviors of other agents, but even the world that we're in is a computer to some extent. So
when you imbue agents with this kind of creativity and autonomy, that's the thing that I don't think
people really understand what might emerge from that. It's related to this discussion about
what kind of goals might emerge from that. Do you have any intuition on what that would look like?
Yeah. Let me give you just the simplest example that we had in 1990 or 32 years ago
of a system that sets itself its own goals. And it consists of two artificial neural networks.
And I know that Carl Friston is very interested in that and only recently for the first time in
my life I was on a paper where he was co-author just a year ago. And so back then it was really
about a reinforcement learning agent and it interacts with the world and it generates actions
that change the world and then there is another network which just is trying to predict the
consequences of the actions in the environment. So the reactions are the environment to these
actions. And so that becomes a world model. And then what kind of goal was there which was different
from traditional goals? Well, in the beginning this model of the world, this prediction machine,
which is a model of the world, a world model, knows nothing. So it has high error as it is
trying to predict the next thing as it is trying to predict the reactions of the environment to
the actions of the agent. So as the second network is trying to reduce its prediction
error through gradient descent, through back propagation essentially, the other one is trying to
generate actions, outputs, that maximize the same error. So basically the goal, the self-invented
goal, if you will, of the first network is to generate an action whose consequences cannot yet
be predicted by the other network, by the model of the world. So the first network is generating
outputs that surprise the second network. So suddenly you have an incentive where the first
network is trying to invent actions, experiments that fool or that surprise the second network.
And that was called artificial curiosity. So now suddenly you have a little
agent which, a little bit like a baby, doesn't learn by imitating the parents. No, it learns by
inventing its own little sub-goals and it's trying to surprise itself and have fun by playing with
the toys and observing new unpredictable things, which however become predictable over time and
therefore become boring. And then it has another incentive to invent the additional experiments
such that it still can surprise its model of the world, which in turn is improving and so on.
So artificial curiosity. Does that also have the effect of making the network which is trying to
predict? Does it have the effect of making it more robust and more generalizable, like almost a
form of regularization kind of built in in this pairing? Yeah, you can build into that network
all kinds of regularizers, an orthogonal concept which is also very important.
So that was just the first version. That was really in 1990. And then we had a long string
of papers just on improvements of this original concept of artificial curiosity. So this old system
is basically what you know as GANs, Generative Adversarial Networks, because the first network
is generating a probability distribution over outputs. And the second network is then predicting
the consequences of these outputs in the environment. And if the output is an image,
then the consequence can be either this image is of a certain type, yes or not, no.
And then that's all that the prediction machine, the world model predicts in that simple case.
And you minimize, the first network minimizes the same error function that the second one
maximizes. So then you have basically a GAN. But then you don't have what you just mentioned,
yet the regularizer. As a scientist, what you really want to learn is a model of the world that
extracts the regularities in the environment. That finds predictable things which are regular
in the sense that there's a short explanation thereof. For example, if you have falling objects
in a video, then they all fall in the same way, they accelerate in the same way, which means
it's predictable what these objects do. If you see two of the frames, you can predict the third frame
pretty well. And the law behind that is very simple. This means that you can greatly compress
the video that is coming in, because you can, instead of storing all the pixels,
you can compute many of these pixels by just looking at two successive frames and predicting
the third frame, or maybe three successive frames, and predicting the fourth frame, something like
that. And you only have to encode the deviations from the prediction. So everything else, you don't
have to store separately, which means once you understand gravity, you can greatly, greatly
compress the video. So that's what you really want to do. And so the more advanced version of
artificial curiosity is about that, where you have a motivation to find a disruption of the
data which is coming in, of the video, of the falling apples, for example, that is simpler
than the one that you had before. So before you had the simple explanation of the data,
you needed so many bits, so many bits to describe the data. And afterwards, only so many. And the
difference between before and after, that is the reward that you get. So that's the true reward that
controller, the first neural network should get in response to the improvements of the second
network, which are now measured in terms of compression progress. So first, I needed so many
resources to encode the data. But then I discovered this regularity, gravity, and I can greatly compress
all kinds of videos that are reflecting the concept of gravity. And suddenly, I have a huge
insight into the nature of the world. And that is my true joy as a scientist, my true joy as a
scientist, that I want to encode in a little number, which is given as a reward to the guy who
is inventing these experiments that lead to the data, to the data with the falling apples, for
example. Right, well, and of course, this has been a challenge in machine learning, you know,
since the beginning, which is, okay, as we add more and more parameters, how do we prevent it from
learning spurious information with those parameters, and instead have it focus on
parsimonious explanations, on regular explanations, on things that in this universe are more likely
to generalize, you know, to unseen examples. And so I think my question to you is, does this setup
that you describe, is it a form of that? And or what is the state of the art, you know, these days
for helping to push or nudge neural networks towards learning parsimonious models for the world,
rather than highly detailed, spurious, susceptible to, you know, high frequency anomalies and
adversarial examples and all this sort of thing? Yes. What is the current state of the art in
regularizing descriptors of the data, such as neural networks, such that you get simple explanations
of the data, such that you get short programs that compute the data. In other words, such that
description of the data is a short program that computes the much larger raw data. And how close
can we get to the limits which are given through this concept concept of algorithmic information
or Kolmogorov complexity? Kolmogorov complexity of any data is the length of the shortest program
on some general computer that computes it. Since in our field, the general computers are
recurrent neural networks, we want to find a simple recurrent network that computes all this data.
And given one computation of the data, we want to find an even simpler one. So we want to have
this idea of compression progress. And here, I have to say, although we have lots of regularizers
invented throughout the past few decades, there's nothing that is really convincing.
I think one of the very important missing things is to make that work in a way that is
truly convincing. That is as convincing as chat GPT is today in the much more limited domain of
generating text from previously observed texts and stuff. A very old idea of I think the 1980s
was to have weight decay in a neural network, which basically is the idea that all the weights
should have an incentive to become close to zero, such that you can prune them.
And so people built in regularizer that just punished weights for being large or being very
negative. But that didn't work really well. And something better was flat minimum search. That
was 1998. And first Arthur, my brilliant student, Sepp Hochreiter back then, roughly the same time
when the LSDM paper came out. And there, the idea is if you plot the weights of a neural network
on the x-axis and you plot the error on the y-axis, then given the weights, you have high or low error.
And then there is, for example, a sharp error function, which has a sharp minimum, which
goes like that. Can you see my finger? So here is the x-axis. Here's the y-axis. Here's the error.
And the error for a certain weight is really, really low. But then for a different weight
in the environment, in the vicinity, it's high again, which would be very different from a
flat minimum, which would be like this. So here's the error. And it's going down. And for many,
many ways, it is low with the error. And then it goes up again. So if you're a very sharp, very
sharp well versus a very broad well. Yes, a sharp well versus a broad well. Now,
if you're in a sharp well, you have to specify the weights with high precision. So you have to
spend many bits of information on encoding the weights of this network, as opposed to a large,
to a flat minimum, where it doesn't matter if you perturb the weights, because the error remains
low in this flat minimum. So what you really want to find is a network that has low complexity
in the sense that you can describe the good networks, those with low error, with very few
bits of information. And suddenly, if you maximize or if you minimize that flat minimum
second order error function, then suddenly you have a preference for networks
that, for example, do this. You have a hidden unit and the outgoing weights,
they have certain values. But if you give a very negative way to the hidden unit,
then it doesn't matter what all these outgoing weights do. And flat minimum search likes to
find weight matrices like that, where one single weight can eliminate many others,
which you certainly don't need any longer, such that the description complexity of the whole thing
is much lower than in the beginning, when you when you just had a random initialization of all
these weights. So that is much more general than weight decay, because weight decay doesn't like
these strong weights, it wants to remove them. But sometimes it's really good to have a very
negative weight coming to a hidden unit, which is switched off through that weight, such that all
the outgoing connections are meaningless. But it's not what you, it's very nice, it's a very nice
principle, but it's not as general as finding the shortest program on a university computer
that computes the weight matrix that is solving your problem.
To the extent, how do you think we're, how do you think we're going to get to that point?
How do you think what approaches are going to lead us to finding
things that approach Kalmogorov complexity? Yeah, I think that part has again a lot to do with
meta learning. And as a system is able to run its own learning algorithm on the network itself,
it can suddenly speak about the
algorithms in form of weight matrices, and it can discuss concepts such as the complexity
of a weight matrix. And then you can conduct a search in this space of networks that generate
weight matrices. And then you suddenly are in the game. So you are playing the right game.
And then it's more a question of how to
choose an initial learning algorithm such as gradient descent to come up with something
that computes the simple solutions, which you really want to see in the end.
Very recent papers on that, on aspects of that came out just a while ago with my
students, Vincent Hermann and Louis Kirsch and Francesco Faccio and my poster Kazuki Irie
and Robert Chaudash also, and also Imann Neuschelach. And there the idea is really to
have one network that computes an experiment. And the experiment itself is the weight matrix
of a RECA network. So there is a generator of an experiment which can be anything that
describes a computational interaction with an environment, so a program. So that experiment
is then executed in the real world. There's a prediction machine that predicts the outcome
of the experiment before the algorithm is executed. And so then there's just a yes or no question,
either the following outcome will occur or not. Either it will occur or not.
But now the entire setup is such that you don't have predictions all the time about every single
pixel. No, you just have something which is very abstract and which is just about whether
a certain unit of the RECA network is going to be on or off at the end of the experiment.
And this internal on and off unit can represent any computational question,
any question that you can ask at all. And now the task of the experiment generator,
which is another network which generates a recurrent network weight matrix which
represents the experiment. The task of this experiment generator is to again come up with
something that surprises the prediction machine, which looks at the experiment and says,
yeah, it's going to work or not. And suddenly you are again in this old game,
except that now you have this world of abstractions where the abstractions can be anything that is
computable. Interesting. Really cool, really cool. Could we spend the last 10 minutes or so just
talking about some of the current AI landscape. So in particular, the capabilities of GPT-4,
the moat building thing and the power that companies like Google and OpenAI have,
and also the potential for open source. So maybe we'll just start with the very current
capabilities of GPT-4. Are you impressed with it? What do you think? I'm impressed in the sense that
I like the outcomes that you get there. And it wasn't obvious a couple of years ago that it would
become so good. On the other hand, of course, it's not yet this full AGI thing. And it is not really
close to justifying those fears that some researchers sometimes now
document in letters and public letters and so on. So to me, it's a little bit
like a deja vu because for many decades I have had discussions like that and people said,
you are crazy when I said that within my lifetime I want to build something that is smarter than
myself. And now suddenly in recent years, some of the guys who said, it's never going to happen.
Suddenly they just look at chat GPT and they think, oh, now we are really close to AGI and
whatever. So I don't share these extreme, I'm less impressed than some of those guys. Let me
say that. The open source movement that you mentioned, you wanted to ask a specific question
about that, right? Well, yeah, there was that famous Google memo that got leaked. And when the
waits for Llama from Facebook went out, within about two or three weeks, it was a valing pretty
similar to chat GPT with this Laura fine tuning. And the open source community has just exploded
you can now run it on your laptop. And there is some question whether there is a significant gap
between the capability. Is it just a parlor trick? Is it really as good potentially or could it be
as good as some of the next best models from open AI? But I guess the question is, do you think that
we need open AI to have the best models? No, of course not. No, I'm very convinced of the open
source movement and have supported that. Some people say the open source movement is maybe
six or eight months behind the large companies that are now coming out with these models. And
I think the best way of making sure that there won't be dominance through some large company
is to support the open source movement. Because how can a large company compete against all these
brilliant PhD students around the world who are so motivated to, you know, within a few days,
create something that is a little bit better than what the last guy has put out there on Github
and whatever. So I'm very convinced that this open source movement is going to make sure that
there won't be a huge mode for a long time. I'm reading between the lines here, but I would guess
you would be opposed to legislation like the EU is considering where, you know, very tight
restrictions on generative models, you know, onerous kind of approval processes and things like that,
because that's going to have this chilling effect on open source innovation and the little guys,
wouldn't it? Yes, I have signed letters which support the open source movement. And whenever
I get a chance to maybe influence some EU politicians, then I'm trying to
contribute to making sure that they don't shoot themselves in the foot by killing innovation
through the open source movement. So you certainly want to avoid that.
There are lots of different open source movements around the world. So if one big entity fails to
support open source or even makes it harder for open source, there will still be lots of other
entities which won't follow. And so no matter what's going to happen on the political level,
I think open source is not going away.
I guess just in closing, you've been in this game for decades now. And what is, I know it's
a bit of a strange question to ask, but what's your fondest memory in your career?
My fondest memory. Oh, it's usually when I discover something that I think
nobody has seen before. But that is, that happens very rarely because most of the things you think
of, well, somebody else has done before. But yeah, so yeah, what usually happens is you,
and this has happened many times, not many times, but quite a few times in my career since the 80s
as a scientist who publishes stuff. Suddenly you think, oh, that is the solution to all these
problems. And now I really figured out a way of building this universal system which
learns how to improve itself and learns the way to improve the way it improves itself and so on.
And now we are done. And now all that's necessary is to scale it up and it's going to solve everything.
And then you think a little bit long about it, and maybe you have a couple of publications,
but then it turns out something is missing. Something important is missing. And actually,
it's not that great. And actually, you have to think hard to add something important to it,
which then for a brief moment looks like the greatest thing since sliced bread.
And then you get excited again. But then suddenly you realize, oh,
it's still not finished. Something important is missing. And so it goes back and forth like that.
I think that's the life of a scientist. The greatest joys are those moments where you have
an insight where suddenly things fall into place such that along the lines of what we
discussed before, the description length of some solution to a problem suddenly shrinks because
two puzzle pieces, they suddenly match and become one or become one in the sense that
they fit each other such that suddenly you have the shared line between the two puzzle pieces.
One is negative and the other one is positive. And certainly the whole thing is much more
compressible than the sum of the things separately. So these things, that's what's driving
scientists like myself, I guess. Wonderful. Professor, you again, Schmidhubert. It's been
an absolute honor. Thank you so much for coming on the show today. Thank you. It was such a pleasure
talking to you.
