Okay, let me just tell you what the two main points are that I'm going to try to touch
on in this talk.
The first is that to understand human level intelligence, we are going to need to understand
creativity.
It's a big part of what being intelligent means from a human level is our creative aspect.
But the second part, which may be a little more subtle and a little less discussed, is
to create human level AI, which is sort of the topic of the conference, is what we want
to do is create this stuff, or maybe call it AGI or whatever you want to call it, to
talk about really high level artificial intelligence.
We may need to actually implement and run some kind of creative process or open-ended
process that produces this general intelligence.
So that's a thought that's interesting that I want to touch on over the course of this
talk is the idea that we may not be able to get to this really high level of intelligence
without some kind of process that's creative generating the intelligence itself, and that
is something that doesn't get discussed that often.
So optimization is often the topic that we hear about in machine learning today, and
optimization solves problems.
But creativity is something different because creativity changes our perception, and creativity
has more in that sense to do with sort of being human, when we talk about the new designs
for all kinds of things, buildings, architectures, clothing, et cetera, new forms of art and
music, new experiences in entertainment and video games, vast collections of solutions
to problems that we have not yet even encountered, like these are the kinds of things that creativity
provides.
And the point is, this is what we mean by human level.
If somebody can multiply two really big numbers together, that's pretty impressive, but it's
not like, wow, the humanity of it.
The humanity of it is in this kind of a stuff, and that's pretty interesting.
But there's another observation I want to make about creativity, which is that we are
not the only creative force in this universe, not just humans or animals for that matter,
it's this other thing here, it's a very creative force, which is evolution, evolution, which
is the source of all of nature, and think about creativity.
That is the most prolific demonstration of creativity that exists today, is the thing
that produced everything, all of living nature that we see, and that means things that currently
engineers can only dream of reproducing in all their glory, so things like flight and
photosynthesis, and guess what, human level intelligence, that is a product of evolution.
It produced the human level, it's the only thing we know that produced the human level.
In fact, it may be the only viable path, but what I mean is some kind of creative process,
and I'm claiming here that evolution is a creative process, but some kind of a creative
process may be necessary to get to these extremely high levels.
It's important that we grapple with what we actually mean by creativity.
I think that the creativity of evolution that I'm sort of representing with this Tree of
Life can't be understated in the sense that we're talking about prolific creativity in
a single run of something, you could think of it algorithmically, is like a run that
lasted for over a billion years, it kept producing more and more interesting stuff.
This is not optimization that we're talking about here, it's not an optimization process,
it's not optimizing towards some objective or converging towards some objective.
In fact, it's very much the opposite, this is what we would call open-endedness, that's
another theme I want to talk about, or touch on in the talk, is open-endedness, which means
processes that basically run forever and keep producing interesting things, but are not
converging towards some point that's predefined.
What can we learn then from evolution about open-endedness because we may need open-endedness
in order to get to such a distant point in the search space of all possible algorithms
that it's actually equivalent to human level intelligence.
Let's think for a second about open-endedness, what kind of thing is open-ended?
Well, I'm claiming that evolution, evolution in nature is open-ended in some sense, a billion
years and so forth running and still producing interesting stuff.
Well, we have something in computation based on this idea, at least inspired by this idea,
it's called evolutionary computation.
What about this?
Is this a candidate then?
Does this capture these ideas?
This is something that extends back to the 1950s, and many of the pioneering ideas come
from the 1960s.
This is a really simple idea at heart, and a lot of you have heard of it, basically just
to really simplify it.
Let's generate a bunch of random things, call that a population, and then let's take the
better of them, and we can call that the parents, like the better ones.
We have some metric saying what's better, and then we'll made and mutate those, which
just makes me perturb their representation in some way, to create a new generation of
stuff, and that's the next generation population, and let me just repeat this over and over
again, and it can be done for any task, because it's very generic, and so great, so here's
the algorithm, and by the way, you can also evolve neural networks, you know, we've had
a lot of talk about neural networks, you can evolve them too, that's called neuroevolution,
so it's kind of like evolving brains, so there we go, so it's creativity solved, and I'm
regretting the color that I chose here, because it doesn't show up well here, but basically
what this says is that, no, there's something wrong here, because evolutionary algorithms,
what we're just talking about here, they also converge, they converge, and then they basically
give you your solution, or they get stuck, like any other machine learning algorithm,
and then that's the end, and it basically, by the way, is the same story with deep learning,
or like anything basically, they find what you're looking for, maybe they find a few
things you're looking for, then it's the end, or if it doesn't work out well, they don't,
but either way, it's the end, and the whole point here is with open-endedness, there should
be no end, you know, like here's some things I've worked on, these are various things I've
worked on, and they're all really kind of cool, but they end, they all end, and that's
not what I'm talking about, well what are we missing here, it comes to algorithms that just
keep producing interesting stuff and never end, well so there are some more imaginative ideas,
you know, people have thought about this, like well what would keep this kind of going and
being interesting forever, and so people talk about things like co-evolutionary algorithms,
and this means where the individuals inside of the population are interacting with each other
while the evolution is happening, and that interaction then can presumably lead to things
like arms races, which might just keep on ratcheting up forever, and that would be cool,
but unfortunately hasn't proven itself yet to work, it generally will eventually get stuck,
you know, you can look at things in deep learning like a GAN, which is something like a creative
process, but really it's just interpolating among points that it's already seen, it's not like
creating completely new concepts, nothing truly new, like in the way that like humans are completely
new with respect to back in the day when there were only flatworms sort of at the cutting edge
of life, you know, because actually one of our ancestors is a flatworm, that's something really
new, what kind of process does that kind of novelty, and so we don't really, this is a very
difficult question, and what kind of process explains open-endedness and complexity explosions,
like things where things just keep on getting more and more complex up to really an astronomical
level like the human brain with its 100 trillion connections, and I want to tell you that I think
we're beginning to understand, but beginning, it's important, it's like beginning, so I still
haven't seen a program that just produces amazing stuff forever, that doesn't exist yet, like a
never-ending algorithm, but we're beginning to understand some of the important facets of such
a system, and one of the most important ingredients is divergence, it's really important because a
lot of the time we're worrying about convergence all the time, converging towards the optimum,
converging towards the objective, you know, minimizing this loss, but if it is, but for
open-endedness, if it's important to get divergence, then a lot of our intuitions that we've been
building up are not relevant to that kind of a process, because divergence is like the opposite
of convergence, and I want to show you through an example what I mean by divergence, I'm going to
show you on this system called Pick Breeder, which is actually an experiment that we put on the web
years ago to allow people to collaboratively breed basically pictures, so it's sort of
intuitive, you know, like breeding dogs or breeding horses, but here they're going to breed pictures,
so we put it on the web because we're interested, like, if you crowd-source people
and let them sort of do the breeding on their own, what kind of stuff might they find? Like,
this could be pretty interesting, but it turns out that it revealed some really pretty profound
stuff, which I'm going to get into, so, well, it may seem like a digression for a moment, like,
well, what are we talking about now with this toy? It's actually really interesting what it leads us
to seeing about divergent processes, so let me just show you how this works, like, if you came
into Pick Breeder, and it's on the web, so you could go in there, and if you said, I want to
start from scratch, that's one thing you can do, it would give you, basically here you see 15 random
blobs basically, and then what you could do is you could choose one of the random blobs that you
like, so I'd say, so I put a star there, so I said, that's the one I'm going to choose, and so now
this thing can have some children there, so you see how children look like the parent, there are
slight perturbations of the parent, there's been a fairly significant mutation over here, that's
interesting, perhaps, so I could say, okay, well, let's breed from there, then that's the new parent,
and then so forth, and so on, and then I breed more images, and if you look at these things,
it looks pretty modest, and it's like, okay, this might be fun for a few minutes, maybe,
but it turns out that what people found for me was really amazing, I think this is an amazing
result, I'm going to show you a little sampling of what people found, I mean, just think for a
second, if you haven't seen Pick Breeder, I know some of you have, but if you haven't seen it before,
just sort of try to project ahead, what are we eventually going to find if we play this game
for a while, and the result is this kind of stuff, and this is really, you wouldn't expect this,
you know, from such modest beginnings as this, and I think there's a lot to learn here, you know,
in terms of understanding how did this happen, and like, look at these things, they look reminiscent
of things that we see in the real world, like you see like a butterfly, and a skull, and even a car,
and look, even Jupiter even has the red spot there, so that's accurate, now how did this,
how did all of these discoveries get made, and it turns out that like one important ingredient
here that I want to emphasize is that if I found something in Pick Breeder, I can press a button
and have it sent back to the website, and then anybody in the world can come back and branch,
basically say I want to keep evolving or breeding from that point, so we can all branch
from each other's discoveries, and what do you get from something like that is something that
looks like this, and this is a small sub phylogeny, you might call it from biological perspective,
but it's called a family tree, this is a small sub family tree of the entire Pick Breeder phylogeny,
which is thousands and thousands of discoveries, but so what you're seeing here is basically every
node there is something that somebody published to the site, and then every connection there is a
point where somebody else said okay I want to branch from that and do my own evolutionary
sequence, and what you're seeing is that people are branching off of each other, and branching
off of each other, and branching off of each other, and this is which is leading to the leaves
of the tree, which are like the discoveries that we see in this kind of a thing, and this is what
we call divergence, in other words the system rather than converging to like the uber image,
whatever that might mean, is actually diverging across the space of all that's possible,
and collecting stepping stones, in other words everything that's found that somebody publishes
back to the site is something that has potential to lead to something else, and one of the more
profound things that we discovered about how the discoveries were made on this site is that actually
in almost every case when people discover stuff like these they were not looking for them,
and so they adjust that for a second, the way to find things in Pick Breeder is by not looking
for them, and this is basically like a rule, and the reason is because the things that lead to
these don't look like them, so like the reason that these were possible to discover is because
someone else discovered some predecessor, you might call it a stepping stone, that led to them,
and that person who discovered that predecessor was not thinking about these ultimate discoveries,
if they had been they would not have discovered the predecessor, because the predecessor does
not resemble the final discovery, so it's necessary for people to not be trying to find the things
that are ultimately found, in order for all these things to be found, and this is the power of
divergence, divergence systems are stepping stone collectors, they collect basically oases of potential
from which you can jump off and get to other places, and the more we collect the more places we can
get, and this is what's leading to a kind of an open-ended process in Pick Breeder, and I want
to point out that this is one of the only examples that exists, I think maybe the only of an artificial
system that actually does show an open-ended dynamic, except I do want to acknowledge the caveat
that they're humans in the loop, so it's cheating a bit, you know, so to create an automated system
that has this property is much much harder, but that's okay because this system is for us to learn
from, and the data here is extremely valuable as sort of an example of how an open-ended process
can develop, because we know every single choice that was made, and that's how we learned that you
can only get to these things by not trying to get to them. Okay, so that was interesting, that
insight, that you can only get to things by not trying, and that's what led Joe Lehmann, who was
a PhD student of mine at the time, and myself to develop what's called the novelty search algorithm,
we were inspired basically by Pick Breeder by the observation that there are many things in
life we will never be able to get to if we're actually trying to get to them, and so to create
a process that's more like what happens through divergence in Pick Breeder, or you could say
divergence in nature, which is basically just expanding out the number of stepping stones
that it finds, that it may be able to get even more far, and we called this novelty search,
and basically the idea was that we could tell the system to just keep on trying to find new things
starting off from things that have been deemed new in the past or novel in the past, and we mean
novel in non-trivial ways, so not just in terms of representation, but in terms of behavior,
and so what it does is it keeps on branching off and finding more and more new stuff based on new
stuff, and so basically it's an explicit implementation of a divergent search process
without an explicit objective, and that's important, if no objective at all just try to diverge,
and so it's kind of like thinking go everywhere that's interesting, now interesting is a very
slippery word, but ideally we want an algorithm that does that, but I don't know how to formalize
what's interesting, but so novelty here was kind of a proxy for interestingness, well novelty is
part of what makes things interesting, it's not the whole story, and the story for you is
different than the story for me, but it's enough to get some leverage and start this process going
actually see what an algorithm like this would do, so it's pretty interesting, and so sort of the
lesson that this is based on and that actually shows, like if you look at the results because it
has some results I'm going to show you in a second that are really interesting, is that basically
to achieve your highest goals you must be willing to abandon them, there are some things that we're
only going to find by not looking for them, now think of what that means for the pursuit of human
level artificial intelligence, and by the way this particular principle turns out to be more true the
more ambitious your objectives are, so like if your objectives are modest you probably can achieve
them by simply pursuing them or converging towards them, but if your objectives are extremely ambitious
like human level, I have an extremely ambitious objective, then you probably can't converge
towards them simply by measuring progress, so what does that mean for the progress of the field
where, like we actually are sort of guiding our field in an objective way, like we actually act
like this is an optimization problem, like we have benchmarks and then we measure our performance
on those benchmarks, and then we say okay the systems that meet the old systems on the benchmarks
that are established are the better systems and thereby we have progress, it's like the higher
you get in this tree the more likely you are to get to the moon, this is not a very good way of
thinking, you know especially based on this kind of non objective paradox I've been talking about
where I call it the objective paradox that sometimes you can't find things, you sometimes
actually looking for something makes you not able to find it, and so if some things can only be
discovered by not looking for them, and that's especially true for ambitious outcomes, you know
then what does it mean for human level AI, AGI and so forth, this is a field driven by metrics,
benchmarks, increasing performance, we act like it's an objective driven search process,
at some level we could predict that, we need to be more open to following the interesting,
you know because the interesting is sort of this proxy for a divergent concept, you know it's not
that we're going towards something or even some cluster of things, is that we're trying to build
off stepping stones which have potential that's unknown, and to collect those stepping stones
we have to be open to interestingness, and that means not necessarily being so like myopic about
objective increases and benchmark performance, and so I think there's important lessons at
sort of meta level about the field of AI here, including like sort of like the more kind of
ground level insights about what actually drives creativity in terms of divergence,
so it was interesting that like you can see a novelty search walker on the right and one from
basically trying to optimize walking distance on the left, and it was interesting that there's
many results like this with novelty search where we actually saw that it would produce a better
behavior like if we went for novelty, but which by the way means like like falling down is a good
thing from a novelty search perspective as long as you fall down in a new way, and we saw that
that kind of an algorithm is actually producing better walking gates than one that was actually
trying to walk as far as possible, so it's very counterintuitive again, and it's this objective
paradox which is there, but it's basically because the stepping stones to walking don't necessarily
look like walking, so we need to be open to things that are interesting because they could be the
thing that leads to the ultimate best behavior, and many demonstrations like this one, this one
just happens to be very sort of easy to watch, I have shown this principle, so this novelty search
principle led to a new field called quality diversity algorithms, or we call them QD now,
and so this field basically said okay, like now there's a very radical proposition and I obviously
understood that from the start, which is that like well if you don't have any way of inserting your
notion of quality into the process, then it's a fairly frightening feeling to launch the algorithm,
you don't know what the heck it's going to do, and while sometimes it does actually solve a problem,
it's no guarantee that it's going to solve a problem, it doesn't even know what the problem is
because there is no objective function, and so like of course we'd like to insert back a notion of
quality into these kinds of divergent processes, and that's what quality diversity was about,
and so these new algorithms started appearing like something called novelty search
with local competition, there's the map elites algorithm, which basically what they were trying
to do is basically collect a repertoire, so this word repertoire starts coming up, so not solution,
but a repertoire, and the repertoire would be basically a collection of things that are the
best possible representatives of a set of very diverse niches, and that means so it's a little
different than saying show me a bunch of top-level things, that's not what it means, what it means
is basically for any extreme niche, like say you're a walker, you're trying to get ambulating
creatures, like even for niches that are clearly not going to be competitive with the best, like
a telephone pole is not going to be a good walker, but nevertheless I'd still like to see
the best telephone pole walking gate anyway, I'd like to see all the best of everything that's
possible, we call that that's what we mean by quality diversity, and these algorithms do that,
and you can see that actually they have some practical impact, like this cover of nature
that I'm showing here is actually from the map elites algorithm, and what it did was it was able
to evolve a repertoire of walking gates, so a whole wide diversity of different ways for this
particular quadrupedal robot to walk, and then once you have this repertoire what could happen is
if the robot suffered damage or something changed about the robot you could just go back to your
repertoire and pick the gate that's most appropriate for the new situation, so because it had this huge
collection of possible gates, and this was just I think it's a nice demonstration that this notion
of quality diversity which is related to the notion of divergence does have practical ramifications,
it's not just about like creating images or something like that, but even quality diversity
won't invent forever, and I want to really hammer on this because I like to be like you know super
critical about like where things are going, it's not like okay now we can celebrate thank you quality
diversity is the end now we're going to get human-level intelligence, the problem is that like
open-endedness is a very profound problem, it's as profound as artificial intelligence, because
think about it what happens when the space of the possible is filled even by an algorithm like that,
like there's only so many gates that are possible at least only so many gates that are of any interest
to us that are possible at least for any given robot, so where do the new possibilities come from,
and how could they come around forever, like what will lead to something like that, nature looks a
little bit like that, you know the word forever is an extreme word and of course I don't know that
nature would somehow top out at some point or obviously the sun will explode or something like
that, but let's give it credit for going for at least a billion years that's as close to forever
as we're ever going to get, and so these algorithms are not going to go for a year or even like a few
weeks we'd be lucky, and so what's missing then, how do I would keep it to keep finding new things
to do, and the answer is that the system needs to generate not just an endless variety of solutions
but new opportunities for solving things, they're basically new problems, the system has to generate
the problems and the solutions and they have to be self-generating, since no one can anticipate a
curriculum all the way up through a billion years, it's the system's job itself to generate that
curriculum, and this is the key to earth's open-ended creativity, it's generating both the problems and
the solutions, and that's an interesting thought because where are the problems coming from
that we're solving, well they're coming from each other, you know, the thing is we are both the
problems and the solutions, like we as the organisms that are being evolved, you know, because the
existence of anything creates the opportunity for something else, so like the existence of trees
now makes it possible for us to have drafts, and drafts are solving a problem, but the problem
was actually created by the trees, which is how do you get to the leaves of the trees when they're
high up, and so all of us are creating, or maybe I shouldn't even call it problem opportunities
for each other, you know, the fact that I'm standing up here gives you an opportunity to learn
something, but also the fact that you're in the audience can be an opportunity to talk to you and
so forth, and so we're all creating opportunities for each other all the time, and that is the
source of why this process can go forever, and so this is the inspiration for new kind of cutting
edge algorithms that are trying to get closer to open-endedness, and we really would love to produce
a truly open-ended algorithm, and the space of problems is a lot different from the space of
solutions, isn't it? Like the space of solutions is, we've been grappling with that for a long time,
and like deep learning is all about a space of solutions, they happen to be a neural network
space, it's a kind of a neural network space, but the space of problems is much harder to
understand, like because what is, it seems to encompass like everything we could possibly imagine,
and so like if in the, we need to deal with representations, then how do you represent
problems as opposed to solutions, and what keeps them interesting and meaningful forever,
and so like one recent example just to give you a taste was like we introduced this thing called
minimal criterion co-evolution about a year ago, and so the idea there was let's have just the
problem space be that be mazes, and then the solution space are neural networks that can
solve these mazes, and we created this kind of co-evolutionary scenario where the mazes keep
getting more complex, and so do the solutions to the mazes, and so what happens is that the
mazes are expanding over time, and you're constantly getting solutions to those mazes,
and it's kind of interesting in the sense that actually it might be the case that if I could run
this for like a billion years and had sufficient hardware, which I don't, so this is just a
hypothetical, then maybe that this maze would be the size of the galaxy, and maybe there'd be a
neural network solving it, and so while that's not quite as great as like creating new forms of
life, it's still kind of interesting that this could be an algorithm where something interesting
or something truly different than something that we have access to right now could arise
given like hundreds, thousands, millions of years of computation, and so this is just sort of
scratching the surface of like where we might be able to go with open-endedness of course.
I wanted to end by just showing some examples of creativity or open-end or attempts to move
towards open-endedness in some applications today just to give you a taste so that you can kind of
basically spark your imagination, you know, because it's one thing to talk theoretically
about these things, but it's probably going to give you a different kind of a way of thinking if
you see, well what could I do with something like this? Like here's a video game called Galactic
Arms Race that we introduced where it was using a pick-breeder-like process to actually open-endedly
generate new kinds of weapons, and so basically the game just generates weapons like endlessly
inside of the game based on the players playing, and these are all new inventions of types of
weapons I've never seen in other types of games, and it's it actually sold a couple thousand copies
and I'm not saying that to show off, but basically I'm saying that because it's worth noting that
that, you know, there's actually economic value in open-endedness, you know, people do want things
that keep on inventing forever, you know, you can imagine designs of objects and manufacturing,
music is something that is now being generated a lot. This is interesting, this is where people,
we let people on Facebook evolve flowers in a pick-breeder-like way, but look the interesting
part which I just to just maybe spark your imagination is that you could press a button and
order one back to your house, and so you could like give it to your girlfriend or your boyfriend,
and this is a custom flower unlike any other flower, and there it is in 3D, what other things could
we do like this? It's pretty interesting. So if you're interested in more thoughts on
Divergent Search, we wrote a whole book about this, myself and Joel Aiman, it's called Why Greatness
Cannot Be Planned, I can't obviously cover all these ideas in just 30 or 25 minutes, and actually
I'm already over 25 minutes trying to end, but there's a place you could hear more of our thoughts.
So if we're interested in open-ended creativity and we want to match what we see in nature,
and even perhaps to achieve human level AI, we must travel the road with no destination,
we must be willing to do that, and that's going to require us to change some of our thinking.
There are many destinations that are reachable through no other process than not trying to reach
them, and therefore we have to confront the grand challenge of open-endedness, and I want to highlight
that grand challenge of open-endedness. I believe this is a challenge just as worthy as AI of huge
amounts of resources, if we could conquer open-endedness and create algorithms that create forever,
we will have opened up a massive sphere of possibilities that's currently not available
to us. The power of creation is the power of open-endedness. I wrote this article with some
co-authors to help introduce the field, and so if you wanted to get into the field and you never
heard of it before, this is a really nice layman's intro, and finally please feel free to email me,
I'll really like to talk about these things or tweet me or whatever, here's some links to some
of the things that I've mentioned. Okay, thank you.
Okay, thank you for your talk, and here I am again, the voice of the people.
We have one from Martin Rabovek from Revolver. Martin? Okay, got him.
Isn't evolution just a reactive process towards more effective reproduction,
open-ended because of the always-changing environment?
So, that's actually, anytime, so it's a, it's a, what evolution is, like is a very complicated
topic, you know, and anytime that somebody might say, well evolution is just this, like so,
to like this question, just this, this is what it really is. Okay, let's, let's then,
then write that as an algorithm, you know, sort of my bar is that, like, if you really think you
understand something from an AI perspective, then you should be able to formalize it and write it
down, and then it should give me what I'm asking for. So, if that's all evolution is, then let's
just write it down, and then we'll have open-endedness, and you've just revolutionized the field, and
which is why I think it probably is not just that, it's probably not just anything that anybody here
thinks it just is, you know, you may have read your biological, biology textbooks, and you may
think that you know what evolution is, and it may make sense, but actually, I can almost assure you
that there's going to be a few counter-intuitive kind of curveballs that are thrown into that,
that story, that we learned in high school, which aren't in the textbook, because if there weren't,
we would just take the textbook and formalize it, and it's not that simple, and people have been
trying to do this for 50, 60 years. So, evolution is a very complicated and subtle process, and what
we mean by it is different than just optimization if we're talking about the open-endedness aspect
of it, which is probably the more interesting aspect of it. If you're just saying from an
optimization perspective, like, how can I explain why hyenas are as fast as they are? Okay,
there maybe you can give me an explanation, but that's not the same as sort of formalizing the
whole thing as a process that's going to produce this kind of grandiosity of nature on earth.
Then Lukas Linhardt strikes again. How would you evaluate whether something is creative?
That's a really good question, and it's really hard, and actually, it's one of the obstacles
that we face towards creating systems like the type that I'm advocating, because they're so hard to
evaluate, and that actually causes us to not want to create those systems because we've created a
culture in the field of artificial intelligence and machine learning where everything has to be
evaluated in order for us to make progress. So, like, I can't submit it to a conference if I don't
have a very, very concrete evaluation metric that's very objective, and so what we may have to do is
to acknowledge that some things are actually subjective. I mean, we're talking about creativity.
Ultimately, it is kind of in the eye of the beholder. I mean, I could argue with you,
actually, that everything in nature is not interesting. It's just a matter of opinion.
I find it interesting. There's no objective proof that everything is interesting, including us,
but we have accepted that it's interesting. We accepted intelligence is interesting.
Why are we all pursuing it? So, ultimately, creativity is partially a subjective matter,
and so, yeah, there's some interaction between our subjective view of the kinds of things that we
find interesting and some, like, modicum of objectivity, which is sufficient, at least,
to present to you an experiment, and that's something we're very uncomfortable with,
like, we want it to be entirely objective, so you may have to grapple with that in order to
actually start to really flourish with creative systems. Okay, and for the sake of time,
final question, and for once, let's take the most controversial one, Jeff Thompson, over there.
I don't want my taxi creatively evolving while driving me to the airport. Why is Uber creating
human-level AI? Okay, so I 100% agree with that comment. Like, I do not. I don't advocate creative
driving at all. I'm just speaking for myself here, but yeah, that's not a good idea, and certainly
there are tasks here that machine learning is addressing that we just do not want to be creative.
Let's acknowledge that, and I don't mean to convey through what I'm saying that everything
should be creative, and creative driving is one of those. Now, human-level AI is a different
matter, I think. So you're saying why is Uber pursuing human-level AI? So Uber is just pursuing
artificial intelligence and machine learning because these are very important and have a huge
potential impact on their businesses, as is obvious across many aspects of the business,
not just cars. So I don't think Uber necessarily would have an answer on human-level AI. I mean,
I'm at the human-level AI conference. This isn't necessarily an endorsement by Uber for human-level
AI, but it does so happen to be the case with many companies, I think, that in the pursuit of
increasingly sophisticated and effective artificial intelligence technology, it's moving in a
direction that has some relation to the human level. So of course, moving in that direction
is important for being able to compete in the world of machine learning, which is really important
today. Okay, let's thank Kenneth.
