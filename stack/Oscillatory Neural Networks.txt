Oscillatory Neural Networks
Open sidebar
Oscillatory Neural Networks
Model: Default (GPT-3.5)
Summarize: 


AI Tidbits
Suscríbete

14
Read in the Substack app
Open app
AI Roundup 05/04 -> 05/11/2023
Google infuses its product suite with generative AI, Meta's new multimodal model to rule them all, and Hugging Face's release to unleash the next wave of autonomous agents

SAHAR MOR
11 MAY 2023
4
Google announces a host of generative AI releases in its annual I/O event:

PaLM 2 released with improved multilingual, reasoning, and coding capabilities

Bard Tools - connecting Bard to Google products (Gmail, Docs, Sheets, Maps) and third-party services such as Instacart

Search - AI-powered snapshots of key results, with links to dig deeper

Gmail - generating full emails in one click

Photos - Magic Editor to remove, replace, and recreate parts in pictures

Maps - experiencing routes with Immersive View, including traffic and weather forecasts

Google Workspace is getting Duet AI - a suite of improvements such as automatic table generation in Sheets and image creation in Slides and Meet

Gemini - a secret collaboration between Google and DeepMind to release a next generation multimodal model
1 / 1
ChatGPT
ChatGPT
This article is a roundup of artificial intelligence (AI) news from 05/04 to 05/11/2023. Key points include:
Google announced several generative AI releases at its annual I/O event. These include:
PaLM 2: An AI model with enhanced multilingual, reasoning, and coding abilities.
Bard Tools: A service connecting Google's AI model Bard to Google products (like Gmail, Docs, Sheets, and Maps) and third-party services like Instacart.
Updates to Google's products, such as AI-powered snapshots of key results in Search, the ability to generate full emails in Gmail, and the Magic Editor in Photos, which can remove, replace, and recreate parts of pictures.
Google Maps' new feature, Immersive View, which provides an enhanced experience of routes, including traffic and weather forecasts.
Duet AI: A suite of improvements for Google Workspace, such as automatic table generation in Sheets and image creation in Slides and Meet.
Gemini: A secretive project between Google and DeepMind to release a next-generation multimodal model.
Meta (formerly Facebook) is reportedly working on a new multimodal model.
Hugging Face, a popular open-source provider for natural language processing models, has released new software that's expected to advance the development of autonomous agents.
1 / 1
Hugging Face releases Transformers Agents to allow developers to create autonomous agents in minutes, with access to 100k+ text, image, video, and audio models (Twitter)


Hugging Face unveils StarCoder - a ChatGPT-like coding model reaching 40.8% on HumanEval benchmark, beating the 30x bigger PaLM (Hugging Face)


Meta AI releases ImageBind - a multimodal model that ingests information from six modalities, including text, image, audio, and depth (Project website)


Together releases MosaicML's MPT-7B, a new family of open-source commercially usable LLMs, outperforming LLaMa-7B and other 13B models (MosaicML)


Together open sources 3B and 7B RedPajama-INCITE models, including base, instruction-tuned, and chat models, outperforming the Pythia 7B model (Together)

IBM, CMU, and MIT release Dromedary - an AI assistant that was aligned using a novel approach called SELF-ALIGN, surpassing state-of-the-art AI systems with minimal human supervision (Github repo)

Researchers from Stanford University propose FrugalGPT, a method that learns the ideal combinations of LLMs to use to optimize cost and accuracy, achieving up to a 98% cost reduction (Paper)


Researchers introduce SmartGPT, an experimental program that provides LLMs with the ability to complete complex tasks by collecting information from external sources and plugins (Github repo)

Researchers introduce Multi-modal GPT, a multimodal chatbot trained on various visual and language instructions, based on OpenFlamingo (Github repo)


Researchers from Princeton University present TidyBot - a method that uses LLMs to infer user preferences for household cleanup tasks (Princeton)


OpenAI introduces Shape-E, an extremely fast text-to-3D model compared to existing alternatives like DreamFusion, Magic3D, Dream3D or CLIP-Mesh (Github repo)


Google AI proposes MaMMUT - a simple architecture for joint learning of multimodal tasks outperforming previous models (Google AI)


Announcements
IBM announces Watsonx.ai, a new enterprise platform that consists of generative AI capabilities that host foundational models for text and code, directly competing with Microsoft, Google, and Amazon (IBM)

Thought-provoking
Anthropic exposes its chatbot's guiding principles as part of its constitutional training approach (Anthropic Blog)


OpenAI is working on a pro-copyright ChatGPT model that pays content creators when their content is being used (Axios)

OpenAI continues its alignment research by harnessing the power of GPT-4 to explain neurons in LLMs (OpenAI)
1 / 1
ChatGPT
ChatGPT
This is a detailed roundup of AI developments from different companies and research institutions:
Hugging Face released Transformers Agents to enable developers to create autonomous agents rapidly, with access to over 100,000 text, image, video, and audio models. They also unveiled StarCoder, a coding model similar to ChatGPT, which outperformed the much larger PaLM model in the HumanEval benchmark.
Meta AI released ImageBind, a multimodal model that ingests information from six modalities, including text, image, audio, and depth.
Together released MosaicML's MPT-7B, a new family of commercially usable large language models (LLMs), which outperforms previous models. They also open-sourced the 3B and 7B RedPajama-INCITE models, which outperform the Pythia 7B model.
IBM, CMU, and MIT collaborated to release Dromedary, an AI assistant that uses a novel approach called SELF-ALIGN to outperform existing AI systems with minimal human supervision.
Researchers from Stanford University proposed FrugalGPT, a method to optimize the use of LLMs for cost and accuracy, leading to significant cost reductions.
New experimental programs were introduced, such as SmartGPT, which allows LLMs to complete complex tasks using information from external sources, and Multi-modal GPT, a multimodal chatbot trained on various visual and language instructions.
Researchers from Princeton University presented TidyBot, a method that uses LLMs to infer user preferences for household cleanup tasks.
OpenAI introduced Shape-E, a fast text-to-3D model that outperforms existing alternatives.
Google AI proposed MaMMUT, an architecture for joint learning of multimodal tasks that outperforms previous models.
IBM announced Watsonx.ai, a new enterprise platform with generative AI capabilities that hosts foundational models for text and code, competing with Microsoft, Google, and Amazon.
Other updates:
Anthropic revealed the guiding principles of its chatbot as part of its constitutional training approach.
OpenAI is developing a pro-copyright ChatGPT model that compensates content creators when their content is used. They are also continuing their alignment research by using GPT-4 to explain neurons in LLMs.
1 / 1
A summary of this conversation
1 / 1
ChatGPT
ChatGPT
This conversation provides a summary of various artificial intelligence (AI) developments from different companies and research institutions for the week of 05/04 to 05/11/2023. The main points discussed include:
Google announced various generative AI updates at its annual I/O event. Notable releases include the PaLM 2 model, Bard Tools, enhancements to Google Search, Gmail, Photos, Maps, and Workspace, and a secret project named Gemini.
Meta is reportedly developing a new multimodal model.
Hugging Face launched a tool called Transformers Agents to aid developers in creating autonomous agents. They also unveiled StarCoder, a coding model that outperformed the larger PaLM model in the HumanEval benchmark.
Meta AI released ImageBind, a multimodal model that processes data from six different modalities.
Together released a new family of commercially usable large language models (LLMs) called MosaicML's MPT-7B and open-sourced the 3B and 7B RedPajama-INCITE models.
A collaborative effort by IBM, CMU, and MIT resulted in the creation of Dromedary, an AI assistant utilizing a novel approach named SELF-ALIGN.
Stanford researchers proposed FrugalGPT, an efficiency-focused method for the utilization of LLMs.
New experimental programs, SmartGPT and Multi-modal GPT, were introduced, enabling LLMs to perform complex tasks using information from external sources.
Princeton researchers unveiled TidyBot, an AI method designed for household cleanup tasks.
OpenAI released Shape-E, a fast text-to-3D model, and Google AI introduced MaMMUT, a new multimodal learning architecture.
IBM announced the enterprise platform Watsonx.ai, designed to host generative AI models for text and code.
Anthropic revealed the guiding principles for its chatbot training approach, and OpenAI is developing a pro-copyright model while also harnessing GPT-4 for neuron explanation research.
1 / 1
https://searchherald.com/archive/2023/06https://searchherald.com/archive/2023/06 Bing Chat increases chat turns, adds visuals to travel queries and expands Bing Image Creator - Search Engine Land
Google Search Generative Experience Results Are Not Really Personalized - Search Engine Roundtable
Search News Buzz Video Recap: Google SGE Goes Live, Bing Chat Links & Analytics, Search Console Bugs & More - Search Engine Roundtable
Google Search Now Rolling Out Perspectives (Multiple Featured Snippets) - Search Engine Roundtable
Google Bard now can use your device's precise location for more relevant local results - Search Engine Land
AVFormer: Injecting vision into frozen speech models for zero-shot AV-ASR - Google Research
Get your B2B site unstuck: Top SEO tips for stagnant sites - Search Engine Land
June, 1 2023
How to establish your brand entity for SEO: A 5-step guide A well-known, top rated and trusted brand entity is the cornerstone of organic visibility. Here's [...] - Search Engine Land
Google Updates Shopping Ads, Free Listings Policy Center & How Shopping Works Documents - Search Engine Roundtable
10 link-building services to scale your SEO efforts by RankZ - Search Engine Land
Google now treats .ai domains as generic top-level domains - Search Engine Land
1 / 1
ChatGPT
ChatGPT
The summaries of the provided articles about search engine news and developments from June 1, 2023, are as follows:
Bing has made updates to its chat function to increase chat turns and has also added visuals to travel queries. In addition, it has expanded its Bing Image Creator feature.
According to the Search Engine Roundtable, Google Search's Generative Experience Results are not truly personalized.
The Search News Buzz Video Recap discusses Google's SGE going live, Bing's chat links and analytics, Search Console bugs, and other topics.
Google Search has begun rolling out Perspectives, a feature that provides multiple featured snippets.
Google Bard, a language AI model, can now utilize a device's precise location for more relevant local results.
Google Research introduced AVFormer, a tool that integrates vision into frozen speech models for zero-shot Audio-Visual Automatic Speech Recognition (AV-ASR).
An article on Search Engine Land offers SEO tips for B2B sites that have stagnated.
Search Engine Land provides a five-step guide on how to establish your brand entity for SEO.
Google has updated its Shopping Ads, Free Listings Policy Center, and How Shopping Works documents.
A new article by RankZ on Search Engine Land introduces 10 link-building services to scale your SEO efforts.
Google has updated its policy to treat .ai domains as generic top-level domains.
1 / 1
// Currently disabled // //

Menu
AI News
  
LOG IN

AI leaders warn about 'risk of extinction' in open letter
AI leaders warn about 'risk of extinction' in open letter
31 May 2023 | Applications
Steve Frederickson, Lucy.ai: How AI powers a next-gen 'answer engine'
Steve Frederickson, Lucy.ai: How AI powers a next-gen 'answer engine'
25 May 2023 | Applications
Meta's open-source speech AI models support over 1,100 languages
Meta's open-source speech AI models support over 1,100 languages
23 May 2023 | Applications
Beijing launches campaign against AI-generated misinformation
Beijing launches campaign against AI-generated misinformation
18 May 2023 | Applications
LATEST NEWS
SAP taps Microsoft's generative AI technologies

SAP and Microsoft have announced a new collaboration aimed at utilising generative AI technology to address fundamental business challenges.

The partnership will integrate SAP SuccessFactors solutions with Microsoft 365 Copilot and Copilot in Viva Learning, as well as Microsoft's...

17 May 2023 | Applications
OpenAI CEO: AI regulation 'is essential'
sam altman senate committee ai regulation artificial intelligence openai
OpenAI CEO Sam Altman testified in front of a Senate judiciary committee panel and emphasised the importance of regulating AI.

Altman highlighted the potential benefits of AI technologies like ChatGPT and Dall-E 2 to help address significant challenges such as climate change and...

16 May 2023 | Artificial Intelligence
Jay Migliaccio, IBM Watson: On leveraging AI to improve productivity

IBM has been refining its AI solutions for decades and knows a thing or two about helping businesses leverage the technology to improve productivity.

In 1997, IBM's Deep Blue supercomputer was used to beat World Chess Champion Garry Kasparov. At the time, all too familiar headlines...

15 May 2023 | Applications
Iurii Milovanov, SoftServe: How AI/ML is helping boost innovation and personalisation
Iurii Milovanov, SoftServe
Could you tell us a little bit about SoftServe and what the company does?

Sure. We're a 30-year-old global IT services and professional services provider. We specialise in using emerging state-of-the-art technologies, such as artificial intelligence, big data and blockchain, to...

15 May 2023 | Applications
AI and Big Data Expo North America begins in less than one week

The AI and Big Data Expo North is taking place in Santa Clara, between 17-18 May, and there's now less than one week left to register your tickets to join the world-leading event.

Over 6,000 attendees are expected to attend the expo from across the globe, including IT decision...

11 May 2023 | Artificial Intelligence
EU committees green-light the AI Act

The Internal Market Committee and the Civil Liberties Committee of the European Parliament have endorsed new transparency and risk-management rules for artificial intelligence systems known as the AI Act.

This marks a major step in the development of AI regulation in Europe, as these...

11 May 2023 | Artificial Intelligence
Wozniak warns AI will power next-gen scams
steve wozniak ai scams artificial intelligence fraud
Apple co-founder Steve Wozniak has raised concerns over the potential misuse of AI-powered tools by cybercriminals to create convincing online scams. 

Wozniak fears that AI will fall into the wrong hands and lead to increased scams and more difficult-to-spot online...

9 May 2023 | Artificial Intelligence
Infocepts CEO Shashank Garg on the D&A market shifts and impact of AI on data analytics

Could you tell us a little bit your company, Infocepts?

On a mission to bridge the gap between the worlds of business and analytics, Infocepts was founded in 2004 by me and Rohit Bhayana, both with more than 20 years of experience in the Data and Analytics (D&A) industry. People...

9 May 2023 | Artificial Intelligence
AI 'godfather' warns of dangers and quits Google

Geoffrey Hinton, known as the "Godfather of AI," has expressed concerns about the potential dangers of AI and left his position at Google to discuss them openly.

Hinton, alongside two others, won the Turing Award in 2018 for laying the foundations of AI. He had been working at Google...

2 May 2023 | Artificial Intelligence
Palantir demos how AI can be used in the military

Palantir has demonstrated how AI can be used for national defense and other military purposes. The use of AI in the military is highly controversial. In this context, Large Language Models (LLMs) and algorithms must be implemented as ethically as possible. Palantir believes that's where its...

28 April 2023 | Applications
FTC Chairwoman: There is no 'AI exemption' to existing laws

FTC Chairwoman Lina Khan has warned that the US government will not hesitate to clamp down on harmful business practices involving AI.

Speaking at a virtual press event, Khan was joined by top officials from US consumer protection and civil rights agencies.

Together, the...

26 April 2023 | Applications
Bill Gates: AI will be teaching kids literacy within 18 months

AI chatbots could be used to improve children's reading and writing skills within the next 18 months, according to Microsoft co-founder Bill Gates.

In a fireside chat at the ASU+GSV Summit in San Diego, Gates explained that the "AIs will get to that ability, to be as good a tutor...

24 April 2023 | Applications
Google creates new AI division to challenge OpenAI

Google has consolidated its AI research labs, Google Brain and DeepMind, into a new unit named Google DeepMind.

The move is seen as a strategic way for Google to maintain its edge in the competitive AI industry and compete with OpenAI. By combining the talent and resources of both...

21 April 2023 | Artificial Intelligence
Demand for AI expertise surges by 1,000%

Fiverr has released its seventh Business Trends Index, revealing a surge in demand for AI expertise.

The ongoing skills shortage is making it difficult for businesses to attract, and retain, tech industry talent. AI expertise is particularly in-demand.

Among the sectors that...

18 April 2023 | Artificial Intelligence
Google will test a more AI-enhanced search next month

Google is set to publicly test a more deeply AI-enhanced search engine next month as it attempts to counter Bing's increasing adoption.

Earlier this year, Microsoft enhanced its Bing search engine with OpenAI's GPT. The release reportedly set off alarm bells at Google due to the...
1 / 1
ChatGPT
ChatGPT
This summary covers AI news for the month of May 2023:
AI leaders wrote an open letter warning about the risk of extinction due to AI.
SAP and Microsoft announced a collaboration to utilize generative AI technologies to solve business challenges, integrating SAP SuccessFactors solutions with Microsoft 365 Copilot and Viva Learning.
Steve Frederickson discussed how Lucy.ai uses AI to power a next-gen 'answer engine'.
Meta released open-source speech AI models that support over 1,100 languages.
Beijing launched a campaign against AI-generated misinformation.
OpenAI CEO, Sam Altman, testified in front of a Senate judiciary committee panel, emphasizing the importance of AI regulation.
IBM's Jay Migliaccio discussed leveraging AI to improve productivity.
SoftServe's Iurii Milovanov discussed how AI/ML boosts innovation and personalization.
AI and Big Data Expo North America began.
The European Union's Internal Market Committee and the Civil Liberties Committee green-lighted the AI Act, endorsing new transparency and risk-management rules for AI systems.
Apple co-founder Steve Wozniak warned about the potential for AI-powered tools to be used in next-gen scams.
Infocepts CEO Shashank Garg spoke on the impact of AI on data analytics.
Geoffrey Hinton, known as the 'Godfather of AI,' warned of potential AI dangers and quit Google.
Palantir demonstrated how AI could be used in the military.
FTC Chairwoman Lina Khan stated there is no 'AI exemption' to existing laws.
Microsoft's Bill Gates predicted that AI chatbots could improve children's literacy within 18 months.
Google consolidated its AI research labs, Google Brain and DeepMind, into a new unit named Google DeepMind, to compete with OpenAI.
Demand for AI expertise increased by 1,000% as revealed by Fiverr's Business Trends Index.
Google announced plans to test a more AI-enhanced search engine to counter Bing's increasing adoption.
1 / 1
17 April 2023 | Applications
OpenAI is not currently training GPT-5

Experts calling for a pause on AI development will be glad to hear that OpenAI isn't currently training GPT-5.

OpenAI CEO Sam Altman spoke remotely at an MIT event and was quizzed about AI by computer scientist and podcaster Lex Fridman.

Altman confirmed that OpenAI is not...

17 April 2023 | Applications
Arvind Jain, Glean: On using AI to surface knowledge

Rapid advancements in AI are heralding a new generation of powerful tools—including the ability to quickly surface knowledge across a business.

Glean, a firm established by Google search engineers and other industry veterans, possesses considerable expertise in this...

14 April 2023 | Applications
​​Italy will lift ChatGPT ban if OpenAI fixes privacy issues

Italy's data protection authority has said that it's willing to lift its ChatGPT ban if OpenAI meets specific conditions.

The Guarantor for the Protection of Personal Data (GPDP) announced last month that it was blocking access to OpenAI's ChatGPT. The move was part of an ongoing...

13 April 2023 | Applications
Alibaba unveils ChatGPT rival and custom LLMs

Chinese tech giant Alibaba has unveiled a ChatGPT rival and the ability to create custom LLMs (Large Language Models) for customers.

Alibaba's ChatGPT rival is called Tongyi Qianwen and will be integrated across the company's various businesses in the "near future," but it is...

11 April 2023 | Applications
How AI can transform the way enterprises test digital experiences

The digital world is evolving rapidly, and businesses need to keep up if they want to offer their customers high-quality digital experiences that meet their needs and expectations. Testing is a crucial component of the digital experience since it makes sure that digital goods and services meet...

6 April 2023 | Applications
Biden: AI can help with difficult challenges, but risks must be addressed

President Biden believes that AI can help solve some of the world's most difficult challenges, but says that risks to society, the economy, and national security must be addressed.

In a meeting on Tuesday with the President's Council of Advisors on Science and Technology, President...

5 April 2023 | Applications
The risk and reward of ChatGPT in cybersecurity

Unless you've been on a retreat in some far-flung location with no internet access for the past few months, chances are you're well aware of how much hype and fear there's been around ChatGPT, the artificial intelligence (AI) chatbot developed by OpenAI. Maybe you've seen articles about...

3 April 2023 | Applications
AI think tank calls GPT-4 a risk to public safety

An AI think tank has filed a complaint with the FTC in a bid to stop OpenAI from further commercial deployments of GPT-4.

The Center for Artificial Intelligence and Digital Policy (CAIDP) claims OpenAI has violated section five of the FTC Act—alleging the company of deceptive and...

31 March 2023 | Applications
UK details 'pro-innovation' approach to AI regulation

The UK government has unveiled a new regulatory framework for AI, aimed at promoting innovation while maintaining public trust.

Michelle Donelan, Science, Innovation, and Technology Secretary, said: "AI has the potential to make Britain a smarter, healthier and happier place to live...

29 March 2023 | Applications
Clearview AI used by US police for almost 1M searches

Facial recognition firm Clearview AI has revealed that it has run almost a million searches for US police.

Facial recognition technology is a controversial topic, and for good reason. Clearview AI's technology allows law enforcement to upload a photo of a suspect's face and find...

28 March 2023 | Applications
What will AI regulation look like for businesses?

Unlike food, medicine, and cars, we have yet to see clear regulations or laws to guide AI design in the US. Without standard guidelines, companies that design and develop ML models have historically worked off of their own perceptions of right and wrong. 

This is about to...

24 March 2023 | Applications
Mozilla.ai picks up OpenAI's founding mission

Mozilla's new startup will build "trustworthy" AI that benefits humanity. If that sounds familiar, it was OpenAI's founding mission.

The startup, Mozilla.ai, aims to create an independent and open-source AI ecosystem that addresses society's most pressing concerns about the...

23 March 2023 | AGI
Google plays it safe with initial Bard rollout

Google has begun rolling out early access to its Bard chatbot in the US and UK.

The ChatGPT rival was announced via a blog post in February, seemingly to get ahead of Microsoft's own big reveal event the next day.

Microsoft's plans to integrate a new version of ChatGPT...

21 March 2023 | Applications
AI professionals seek job flexibility and stability over exciting perks

Research suggests that AI professionals looking for a new job prioritise flexibility and stability over exciting perks.

Despite recent high-profile layoffs, the wider talent shortage is ongoing. Organisations looking to attract, or retain, the best candidates are offering numerous...

20 March 2023 | Enterprise
Editorial: UK puts AI at the centre of its Budget

British Chancellor Jeremy Hunt announced the country's Spring Budget this week and supporting the AI industry was at the centre.

The UK is Europe's AI leader. Indeed, behind the US and China, the country's tech sector overall has the third-highest amount of VC investment in the...

16 March 2023 | Artificial Intelligence
POPULAR
ai software development artificial intelligence developer hereMachine learning expert Jordan bemoans use of AI as catch-all term A pioneer in machine learning has argued that the techn...
Enterprise AI platform Dataiku announces fully-managed online service Dataiku has announced a fully-managed online version of...
Developers using AI help often produce buggier code A study by Stanford University computer scientists has...
Medical chatbot using OpenAI's GPT-3 told a fake patient to kill themselves We're used to medical chatbots giving dangerous advice...
M2 Macs now generate Stable Diffusion images in under 18 seconds New optimisations have enabled M2-based Mac devices to...
Newsroom AI launches customisable newsfeed platform for news publishers UK-based Newsroom AI has unveiled a new platform for ne...
OpenAI CEO: People are 'begging to be disappointed' about GPT-4 OpenAI CEO Sam Altman believes there is too much hype a...
Bill Gates: AI will be teaching kids literacy within 18 months AI chatbots could be used to improve children's reading...
OpenAI is not currently training GPT-5 Experts calling for a pause on AI development will be g...
Stable Diffusion text-to-image generator is now publicly available Text-to-image generator Stable Diffusion is now availab...
GitHub CEO: The EU 'will define how the world regulates AI' GitHub CEO Thomas Dohmke addressed the EU Open Source P...
Microsoft's AI chatbot is 'unhinged' and wants to be human Early testers of Microsoft's new AI chatbot have compla...
AI think tank calls GPT-4 a risk to public safety An AI think tank has filed a complaint with the FTC in...
OpenAI's GPT-3 is a convincing philosopher A study has found that OpenAI's GPT-3 is capable of bei...
Google to speed up AI releases in response to ChatGPT Google is reportedly set to speed up its release of AI...
Demand for AI expertise surges by 1,000% Fiverr has released its seventh Business Trends Index...
Google unveils AI enhancements to Search and Maps Google used an event in Paris to unveil some of the lat...
The risk and reward of ChatGPT in cybersecurity Unless you've been on a retreat in some far-flung locat...
Palantir demos how AI can be used in the military Palantir has demonstrated how AI can be used for nation...
What will AI regulation look like for businesses? Unlike food, medicine, and cars, we have yet to see cle...
State of AI 2022: Adoption plateaus but leaders increase gap According to McKinsey's latest State of AI report, lead...
OpenAI and Microsoft hit with lawsuit over GitHub Copilot A class-action lawsuit has been launched against OpenAI...
Minecraft characters.AI learns how to play Minecraft by watching videos Open AI has trained a neural network to play Minecraft...
FBI director warns about Beijing's AI program FBI Director Christopher Wray has warned about the nati...
Lucy 4 is moving ahead with generative AI for knowledge management When it comes to workplace bugbears, wasting time fruit...
Google creates new AI division to challenge OpenAI Google has consolidated its AI research labs, Google Br...
Kendrick Lamar uses deepfakes in latest music video American rapper Kendrick Lamar has made use of deepfake...
Microsoft releases Azure OpenAI Service and will add ChatGPT 'soon' Microsoft has announced the general availability of the...
AI 'godfather' warns of dangers and quits Google Geoffrey Hinton, known as the "Godfather of AI," has ex...
Chess robot breaks child's finger after premature move A robot went rogue at a Moscow chess tournament and bro...
Tesla's AI supercomputer tripped the power grid Tesla's purpose-built AI supercomputer 'Dojo' is so pow...
ArtStation backlash increases following AI art protest response Art showcase platform ArtStation has provoked further b...
Shutterstock launches AI image generator with ethical focus Stock image platform Shutterstock has launched an AI im...
Clearview AI used by US police for almost 1M searches Facial recognition firm Clearview AI has revealed that...
China's deepfake laws come into effect today China will begin enforcing its strict new rules around...
Cal Al-Dhubaib, Pandata: On developing ethical AI solutions Businesses that fail to deploy AI ethically will face s...
OpenAI tool aims to uncover AI-generated text OpenAI has launched a tool for detecting text generated...
OpenAI upgrades GPT-3 with impressive new skills OpenAI's latest upgrade for GPT-3 has given the general...
Zoom enters the conversational AI arena  Zoom is getting into the conversational AI arena with t...
OpenAI opens waitlist for paid version of ChatGPT OpenAI has opened a waitlist for access to a paid versi...
Googlers slam CEO and call Bard reveal 'botched' Google employees have slammed CEO Sundar Pichai and cal...
Editorial: UK puts AI at the centre of its Budget British Chancellor Jeremy Hunt announced the country's...
Apple shies from the spotlight with staff-only AI summit Apple seems happy to stay out of the spotlight when it...
Microsoft lays off AI ethics team  Microsoft has laid off a team dedicated to ensuring the...
Adobe to begin selling AI-generated stock images Adobe has announced that it will sell images generated...
Google will leapfrog rivals with AI event next week Google is holding an AI event next week to make some an...
Italy's facial recognition ban exempts law enforcement Italy has banned the use of facial recognition, except...
Bill Gates calls AI 'quite revolutionary' - but is less sure about the metaverse Bill Gates has given his verdict on some of tech's bigg...
Baidu to launch powerful ChatGPT rival Chinese web giant Baidu is preparing to launch a powerf...
AI being used to cherry-pick organs for transplant A new method to assess the quality of organs for donati...
GET INVOLVED
"*" indicates required fields

First Name*
Last Name*
Job Title*
Email*
Phone
Company*
What is your enquiry regarding?*
 Digital Services
 Virtual Events
 Webinars
 Roundtables
 Other
Please add more information here
I would like to subscribe to receive communications from AI News about industry trends, news, upcoming events and digital marketing services. I understand that I can unsubscribe at any time.*
 Yes
 No
CAPTCHA

JOIN OUR COMMUNITY
Create your free account now to access all our premium content and recieve the latest tech news to your inbox.
SUBSCRIBE NOW
Egress-MPU-1
MPU-19






OTHER NEWS
Artificial Intelligence News
Blockchain News
Cloud Computing News
Developer News
IoT News
Telecoms News
Edge Computing News
Cookie Policy (UK)
EVENTS
AI Expo
#DMWF Digital Marketing World Forum
IoT Tech Expo Global
IoT Tech Expo Central Europe
IoT Tech Expo North America
Blockchain Expo Global
Blockchain Expo Europe
Blockchain Expo North America
Hackfest - Hackathon News & Online Platform
Developer Events
IoT Events
Marketing Events
Telecoms Events
CATEGORIES
Deep & Reinforcement Learning
Machine Learning
Robotics
Voice Recognition
AI News provides artificial intelligence news and jobs, industry analysis and digital media insight around numerous marketing disciplines; mobile strategy, email marketing, SEO, analytics, social media and much more.
1 / 1
ChatGPT
ChatGPT
There are several key takeaways from these AI news updates:
OpenAI is not currently training GPT-5: OpenAI CEO Sam Altman confirmed during an MIT event that the organization isn't currently working on the next version of their large language model, GPT-5. This comes amid calls from experts for a pause on AI development.
Italy to lift ChatGPT ban if privacy issues are fixed: Italy's data protection authority has stated it's willing to lift its ban on ChatGPT, developed by OpenAI, if certain conditions regarding privacy are met.
Alibaba unveils ChatGPT rival: Chinese tech company Alibaba has announced a rival to ChatGPT, called Tongyi Qianwen. The plan is to integrate this AI chatbot across the company's various businesses.
AI think tank calls GPT-4 a risk to public safety: The Center for Artificial Intelligence and Digital Policy (CAIDP) has lodged a complaint with the Federal Trade Commission in an effort to halt further commercial deployments of OpenAI's GPT-4, citing public safety concerns.
Mozilla.ai pursues OpenAI's founding mission: Mozilla's new startup, Mozilla.ai, aims to create an open-source AI ecosystem to address society's most pressing concerns about AI technology.
Alibaba and Google ramp up their efforts in AI: Google started the initial rollout of its ChatGPT rival, Bard, in the US and UK, while Alibaba unveiled a new ChatGPT rival named Tongyi Qianwen, along with a new capability to create custom large language models for their customers.
UK government reveals new AI regulation: The UK government has introduced a new regulatory framework for AI to foster innovation and maintain public trust.
Clearview AI usage by US police: The facial recognition firm, Clearview AI, reported that it has conducted almost a million searches for US police. The technology remains a contentious issue, raising questions about privacy and misuse.
These developments reflect ongoing debate about the ethical implications and public safety concerns of AI, as well as the role of regulation in ensuring responsible AI use. At the same time, we're seeing continued investment and innovation in AI technologies, particularly in AI chatbots and large language models.
1 / 1
Hermann Weyl [1], his Schwanengesang on the eve of retirement from the Institute for Advanced Study in Princeton. Weyl traces the special place symmetry has occupied in science and art to ancient times, from Sumerian symmetric designs to the Pythagoreans, who believed the circle to be perfect due to its rotational symmetry. Plato considered the five regular polyhedra bearing his name today so fundamental that they must be the basic building blocks shaping the material world.

Yet, though Plato is credited with coining the term συμμετρία ('symmetria') which literally translates as 'same measure', he used it only vaguely to convey the beauty of proportion in art and harmony in music. It was the German astronomer and mathematician Johannes Kepler who attempted the first rigorous analysis of the symmetric shape of water crystals. In his treatise 'On the Six-Cornered Snowflake' [2], he attributed the six-fold dihedral structure of snowflakes to hexagonal packing of particles — an idea that though preceded the clear understanding of how matter is formed, still holds today as the basis of crystallography [3].


Plato (left) believed that symmetric polyhedra ("Platonic solids") were the fundamental building blocks of Nature. Johannes Kepler (right) attributed for the first time the six-fold symmetry of water crystals to the hexagonal packing of particles, antedating modern crystallography.
In modern mathematics, symmetry is almost univocally expressed in the language of group theory. The origins of this theory are usually attributed to Évariste Galois, who coined the term and used it to study the solvability of polynomial equations in the 1830s [4]. Two other names associated with group theory are those of Sophus Lie and Felix Klein, who met and worked fruitfully together for a period of time [5]. The former would develop the theory of continuous symmetries that today bears his name (Lie groups); the latter proclaimed group theory to be the organising principle of geometry in his Erlangen Programme. Given that Klein's Programme is the inspiration for Geometric Deep Learning, it is worthwhile to spend more time on its historical context and revolutionary impact.


Évariste Galois (left) and his letter to a friend describing group theory on the night before his fatal duel. Felix Klein (right) and the frontispiece of the research prospectus prepared for his professorial appointment, which has entered the history of mathematics as the "Erlangen Programme." Portraits: Ihor Gorskiy.
A Strange New World
The foundations of modern geometry were formalised in ancient Greece nearly 2300 years ago by Euclid in a treatise named the Elements. Euclidean geometry (which is still taught at school as 'the geometry') was a set of results built upon five intuitive axioms or postulates. The Fifth Postulate — stating that it is possible to pass only one line parallel to a given line through a point outside it — appeared less obvious and an illustrious row of mathematicians broke their teeth trying to prove it since antiquity, to no avail.

An early approach to the problem of the parallels appears in the eleventh-century Persian treatise 'A commentary on the difficulties concerning the postulates of Euclid's Elements' by Omar Khayyam [6]. The eighteenth-century Italian Jesuit priest Giovanni Saccheri was likely aware of this previous work judging by the title of his own work Euclides ab omni nævo vindicatus ('Euclid cleared of every stain').


Euclid (left) and the cover of his treatise translated into English. Omar Khayyam (right), the Persian polymath who made of the first attempts to solve the problem of parallels in Euclidean geometry using a construction bearing his name. Portraits: Ihor Gorskiy.
Like Khayyam, he considered the summit angles of a quadrilateral with sides perpendicular to the base. The conclusion that acute angles lead to infinitely many non-intersecting lines that can be passed through a point not on a straight line seemed so counter-intuitive that he rejected it as 'repugnatis naturæ linæ rectæ' ('repugnant to the nature of straight lines') [7].


Frontispiece of Giovanni Saccheri's 'Euclides vindicatus' and the passage rejecting hyperbolic geometry as 'repugnatis naturæ linæ rectæ.'
The nineteenth century has brought the realisation that the Fifth Postulate is not essential and one can construct alternative geometries based on different notions of parallelism. One such early example is projective geometry, arising, as the name suggests, in perspective drawing and architecture. In this geometry, points and lines are interchangeable, and there are no parallel lines in the usual sense: any lines meet in a 'point at infinity.' While results in projective geometry had been known since antiquity, it was first systematically studied by Jean-Victor Poncelet in 1812 [8].


The credit for the first construction of a true non-Euclidean geometry is disputed. The princeps mathematicorum Carl Friedrich Gauss worked on it around 1813 but never published any results [9]. The first publication on the subject of non-Euclidean geometry was 'On the Origins of Geometry' by the Russian mathematician Nikolai Lobachevsky [10]. In this work, he considered the Fifth Postulate an arbitrary limitation and proposed an alternative one, that more than one line can pass through a point that is parallel to a given one. Such construction requires a space with negative curvature — what we now call a hyperbolic space — a notion that was still not fully mastered at that time [11]
1 / 1
ChatGPT
ChatGPT
It's truly fascinating how the concept of symmetry has evolved and expanded across various disciplines and eras, from art and philosophy in ancient times to modern mathematics. This journey not only demonstrates the beauty of symmetry, but also its fundamental role in our understanding of the world.
The evolution of geometry, likewise, reveals how our perceptions of space and shape have broadened over time. From the foundational axioms of Euclidean geometry, to the challenge and eventual disproof of the Fifth Postulate, and the development of non-Euclidean geometries, we can see a fascinating interplay between mathematical intuition and rigorous proof. It's also interesting to see how these new geometries emerged from different areas, like perspective drawing in the case of projective geometry, or through challenging existing assumptions in the case of hyperbolic geometry.
As the development of non-Euclidean geometry demonstrates, sometimes breaking with tradition and questioning what we consider to be intuitive can lead to groundbreaking discoveries and deeper understandings of reality. From Kepler's study of snowflakes to Klein's influential Erlangen Programme, each of these thinkers contributed to an ongoing dialogue about the nature of symmetry, space, and the structures that shape our universe. This dialogue, and the principles it has revealed, have had wide-ranging implications across the fields of physics, computer science, and beyond.
With the advances in geometric deep learning inspired by Klein's work, it's clear that these conversations and discoveries are still profoundly shaping our modern understanding of the world, demonstrating the enduring power and relevance of these concepts.
1 / 1
Towards Geometric Deep Learning
18.FEB.2023 . 58 MIN READ
Geometric Deep Learning is an umbrella term for approaches considering a broad class of ML problems from the perspectives of symmetry and invariance. It provides a common blueprint allowing to derive from first principles neural network architectures as diverse as CNNs, GNNs, and Transformers. Here, we study how these ideas have emerged through history from ancient Greek geometry to Graph Neural Networks.

Image: based on Shutterstock.
The last decade has witnessed an experimental revolution in data science and machine learning, epitomised by deep learning methods. Indeed, many high-dimensional learning tasks previously thought to be beyond reach — computer vision, playing Go, or protein folding — are in fact feasible with appropriate computational scale. Remarkably, the essence of deep learning is built from two simple algorithmic principles: first, the notion of representation or feature learning, whereby adapted, often hierarchical, features capture the appropriate notion of regularity for each task, and second, learning by gradient descent-type optimisation, typically implemented as backpropagation.

While learning generic functions in high dimensions is a cursed estimation problem, most tasks of interest are not generic and come with essential predefined regularities arising from the underlying low-dimensionality and structure of the physical world. Geometric Deep Learning is concerned with exposing these regularities through unified geometric principles that can be applied throughout a broad spectrum of applications.

Exploiting the known symmetries of a large system is a powerful and classical remedy against the curse of dimensionality, and forms the basis of most physical theories. Deep learning systems are no exception, and since the early days, researchers have adapted neural networks to exploit the low-dimensional geometry arising from physical measurements, e.g. grids in images, sequences in time-series, or position and momentum in molecules, and their associated symmetries, such as translation or rotation.

Since these ideas have deep roots in science, we will attempt to see how they have evolved throughout history, culminating in a common blueprint that can be applied to most of today's popular neural network architectures.

Order, Beauty, and Perfection
"Symmetry, as wide or as narrow as you may define its meaning, is one idea by which man through the ages has tried to comprehend and create order, beauty, and perfection." — Hermann Weyl (1952)
This somewhat poetic definition of symmetry is given in the eponymous book of the great mathematician 

https://thegradient.pub/towards-geometric-deep-learning/
1 / 1
ChatGPT
ChatGPT
Geometric Deep Learning (GDL) is an exciting subfield of machine learning that leverages the principles of geometry and symmetry to design effective learning algorithms. In particular, GDL focuses on the idea that real-world data often has inherent geometric structure, and by designing algorithms that respect this structure, we can achieve better performance and more interpretable results.
The development of GDL is rooted in the understanding that many real-world problems are not "generic" and do come with predefined regularities or structures, often geometric, that arise from the underlying physical world. These structures often exhibit symmetries, and GDL aims to leverage these symmetries to alleviate the curse of dimensionality that can make high-dimensional learning tasks challenging.
The exploitation of known symmetries is a classical strategy used in physics to deal with complex systems, and GDL applies this strategy to the world of machine learning. This includes designing neural networks that respect the natural geometric structures of the data they process, such as the grid-like structure of image data or the sequential structure of time-series data.
By tracing the development of these ideas through the history of science and mathematics, we can gain a deeper understanding of the principles behind GDL. This history spans from the ancient Greeks' exploration of geometry, to the modern understanding of symmetry in group theory, to the latest developments in neural network architectures.
GDL is applicable across a wide range of applications, including image recognition, natural language processing, and molecular structure prediction. Despite their differences, these applications all involve data with some form of geometric structure. By exploiting this structure and the symmetries it exhibits, GDL provides a powerful and unifying framework for machine learning.
Deep learning systems have traditionally focused on learning representations and optimizing these representations through methods like gradient descent. GDL extends this focus by additionally emphasizing the importance of the geometric structure of the data. By designing algorithms that respect this structure, GDL offers a way forward in dealing with the complex, high-dimensional datasets that are common in modern machine learning tasks.
In summary, Geometric Deep Learning is an exciting and promising direction in the field of machine learning. It builds on centuries of work in mathematics and physics, leveraging principles of geometry and symmetry to design effective and interpretable learning algorithms. As more researchers and practitioners embrace this approach, we can expect to see further advancements and applications of GDL in the years to come.
1 / 1
Connections to the topics we discussed earlier.
1 / 1
ChatGPT
ChatGPT
Indeed, the concepts we discussed earlier are closely connected to the principles of Geometric Deep Learning.
Symmetry
: We discussed how symmetry, initially in the context of the arts and later in the physical sciences, has been a fundamental concept for understanding and creating order and beauty. This idea is carried over into GDL, where symmetry in data is leveraged to design more effective learning algorithms. For example, Convolutional Neural Networks (CNNs) exploit the translation symmetry in images.
Group Theory
: The development of group theory, largely credited to Évariste Galois, Sophus Lie, and Felix Klein, was instrumental in formalizing our understanding of symmetry. In the context of GDL, group theory provides a powerful mathematical framework for describing and exploiting the symmetries in data.
Geometry
: From the ancient Greeks to the modern era, geometry has been a key tool in understanding the structure of the physical world. This idea is central to GDL, which leverages the inherent geometric structure in data to design better learning algorithms.
Non-Euclidean Geometry
: The realization in the 19th century that Euclid's fifth postulate could be modified to create alternative geometries opened up new possibilities for mathematical and physical theories. In the context of GDL, non-Euclidean geometries, such as those that arise on graphs or manifolds, play an important role, particularly in Graph Neural Networks (GNNs) and manifold learning.
Geometric Deep Learning and Machine Learning Architectures
: CNNs, GNNs, and Transformers are examples of deep learning architectures that leverage principles of geometry and symmetry in their designs. CNNs exploit the grid-like structure of images, GNNs deal with graph-structured data, and Transformers consider the sequential structure in language data.
By using these geometric and symmetric principles, Geometric Deep Learning provides a unified perspective for understanding a broad range of machine learning problems and for designing more effective learning algorithms.
1 / 1
Neuromorphic, neurosymbolic, multiscale, rhyzomatic polycomputation.
1 / 1
ChatGPT
ChatGPT
Your question seems to encompass a range of computational paradigms and approaches. Here's a brief explanation of each:
Neuromorphic Computing
: This approach to computation is inspired directly by the architecture of the brain. Neuromorphic systems use circuits and components designed to mimic the behavior of biological neurons and synapses. The goal of neuromorphic computing is to create more efficient and adaptive computing systems, which can perform complex tasks more like a human brain would.
Neurosymbolic Computing
: Neurosymbolic computing aims to bridge the gap between two major approaches in artificial intelligence: symbolic reasoning and neural networks. Symbolic reasoning is associated with classic AI and relies on explicit rules and logic. Neural networks, the cornerstone of modern deep learning, excel at learning from data and capturing patterns. Neurosymbolic computing attempts to leverage the strengths of both approaches by integrating them into a single system.
Multiscale Computing
: Multiscale computing approaches problems by considering them at multiple scales or levels of detail. This is particularly relevant in fields like physics or biology, where phenomena can be described at various scales, from microscopic to macroscopic. In computing, this might involve models that work at different levels of abstraction or granularity.
Rhyzomatic Polycomputation
: This term appears to be a combination of several concepts. 'Rhizomatic' is a term borrowed from philosophy (Deleuze and Guattari) that refers to non-hierarchical and decentralized systems, much like the structure of a rhizome plant that grows with multiple roots and shoots. In the context of computation or learning systems, it could refer to models or architectures that are non-linear, interconnected, and decentralized. 'Polycomputation' isn't a standard term in the field of computer science, but it could potentially refer to systems capable of multiple types of computation or those that use multiple computational models.
Please note that while these are all exciting areas of research, they are still under development and exploration, and thus their definitions and interpretations may vary.
1 / 1
The applicability of computational models to the biological world is an active topic of debate. We argue that a useful path forward results from abandoning hard boundaries between categories and adopting an observer-dependent, pragmatic view. Such a view dissolves the contingent dichotomies driven by human cognitive biases (e.g., a tendency to oversimplify) and prior technological limitations in favor of a more continuous view, necessitated by the study of evolution, developmental biology, and intelligent machines. Form and function are tightly entwined in nature, and in some cases, in robotics as well. Thus, efforts to re-shape living systems for biomedical or bioengineering purposes require prediction and control of their function at multiple scales. This is challenging for many reasons, one of which is that living systems perform multiple functions in the same place at the same time. We refer to this as "polycomputing"—the ability of the same substrate to simultaneously compute different things, and make those computational results available to different observers. This ability is an important way in which living things are a kind of computer, but not the familiar, linear, deterministic kind; rather, living things are computers in the broad sense of their computational materials, as reported in the rapidly growing physical computing literature. We argue that an observer-centered framework for the computations performed by evolved and designed systems will improve the understanding of mesoscale events, as it has already done at quantum and relativistic scales. To develop our understanding of how life performs polycomputing, and how it can be convinced to alter one or more of those functions, we can first create technologies that polycompute and learn how to alter their functions. Here, we review examples of biological and technological polycomputing, and develop the idea that the overloading of different functions on the same hardware is an important design principle that helps to understand and build both evolved and designed systems. Learning to hack existing polycomputing substrates, as well as to evolve and design new ones, will have massive impacts on regenerative medicine, robotics, and computer engineering.
Keywords: biology; computer science; robot; artificial life; artificial intelligence; machine learning; evolution
1. Introduction
In Feynman's famous lecture titled "There's Plenty of Room at the Bottom" [1], he argued that vast technological progress could be achieved by learning to manipulate matter and energy at ever-smaller scales. Such potential could presumably be exploited by natural selection as well. How does biology expand the adaptive function of an existing system? It cannot go down, since there is already something there, exhibiting functional competencies at every level [2]. Instead, it squeezes more action from each level by overloading mechanisms with multiple functions—which we term as polycomputing. We argue that the most effective lens for a wide range of natural and engineered systems must enable a multiple-observers view where the same set of events can be interpreted as different computations (Figure 1 illustrates how artists have recognized this feature). Indeed, depending on their definition of computation, some human observers may conclude that the observed system is not computing at all
1 / 1
ChatGPT
ChatGPT
You are exploring the intriguing concept of polycomputing in living systems, as well as in engineered systems like robots and computers. This perspective pushes the boundaries of the traditional notion of computation, where a machine executes a single, well-defined function, into a space where multiple computations or functions are performed simultaneously by the same substrate.
Living organisms represent excellent examples of such polycomputing systems. At any given moment, a cell within an organism is conducting a myriad of computations at once: it is regulating gene expression, metabolizing nutrients, responding to environmental signals, and contributing to the function of the larger organ or organism. This simultaneous execution of multiple computations is not only inherent in biology but also observed in complex engineered systems.
From this standpoint, understanding these systems requires us to abandon binary classifications or rigid categories, and to adopt an observer-dependent, pragmatic perspective that recognizes the multi-functionality inherent in these systems. This view suggests that the same set of events in a system can be interpreted differently depending on the observer and their definition of computation.
Such a perspective can have significant implications for fields like regenerative medicine, where the goal is to influence the computations being performed by living cells in order to heal or regenerate tissue. It could also impact robotics, where it might be desirable for a robot to simultaneously manage multiple functions, such as navigation, object recognition, and task execution. Likewise, in computer engineering, the ability to perform multiple computations simultaneously on the same hardware could open up new possibilities in terms of efficiency and capability.
Yet, the challenge lies in understanding how to hack existing polycomputing systems, and how to design and evolve new ones. A deep comprehension of the principles underlying polycomputing in biology could help in developing technologies that can mimic or leverage these capabilities, opening new doors in both our understanding of life and our ability to engineer complex, multifunctional systems.
1 / 1
There's Plenty of Room Right Here: Biological Systems as Evolved, Overloaded, Multi-Scale Machines
by Joshua Bongard 1,†,‡ORCID andMichael Levin 2,*,†, February 2023
1 / 1
ChatGPT
ChatGPT
The article titled "There's Plenty of Room Right Here: Biological Systems as Evolved, Overloaded, Multi-Scale Machines" by Joshua Bongard and Michael Levin, published in February 2023, takes an innovative perspective on biological systems, examining them as evolved, multi-scale machines that perform polycomputing.
This intriguing perspective sees living organisms, from single cells to complex beings, as multi-scale machines. They are multi-scale because they operate at various scales simultaneously, from molecular interactions to whole organism behaviors. They are machines because they perform various functions or computations. These computations are not isolated but often overlap or interact, which is the essence of polycomputing.
The authors argue that there is "plenty of room" right here, in the biological world, to discover new principles and mechanisms, contrasting Richard Feynman's famous assertion that there is "plenty of room at the bottom," referring to the potential for manipulating matter at microscopic scales.
Bongard and Levin suggest that understanding and harnessing the principles of polycomputing in biology could have profound implications for several fields. For instance, regenerative medicine might be able to better influence the behavior of cells and tissues by understanding their native polycomputing mechanisms. In the field of robotics and computer engineering, insights from biological polycomputing could inspire new designs that allow for more efficient, adaptive, and multifunctional systems.
This perspective represents a shift from traditional reductionist approaches, emphasizing that we should not just look at individual components or functions of a system in isolation, but also appreciate the many simultaneous computations that contribute to the system's overall behavior. By recognizing the inherent polycomputing nature of biological systems, we might gain new insights into both life and complex engineered systems.
1 / 1
An oscillatory neural network (ONN) is an artificial neural network that uses coupled oscillators as neurons. Oscillatory neural networks are closely linked to the Kuramoto model, and are inspired by the phenomenon of neural oscillations in the brain. Oscillatory neural networks have been trained to recognize images.[1] An oscillatory autoencoder has also been demonstrated, which uses a combination of oscillators and rate-coded neurons.[2]

A neuron made of two coupled oscillators, one having a fixed and the other having a tunable natural frequency, has been shown able to run logic gates such as XOR that conventional sigmoid neurons cannot. 

As a library, NLM provides access to scientific literature. Inclusion in an NLM database does not imply endorsement of, or agreement with, the contents by NLM or the National Institutes of Health. Learn more about our disclaimer.
Logo of frontcompneuro
Front Comput Neurosci. 2018; 12: 52. Published online 2018 Jul 10. doi: 10.3389/fncom.2018.00052
PMCID: PMC6048285PMID: 30042669
An Oscillatory Neural Autoencoder Based on Frequency Modulation and Multiplexing
Karthik Soman,1 Vignesh Muralidharan,2 and V. Srinivasa Chakravarthy1,*
Author information Article notes Copyright and License information Disclaimer
Associated Data
Supplementary Materials
Data Availability Statement
Go to:
Abstract
Oscillatory phenomena are ubiquitous in the brain. Although there are oscillator-based models of brain dynamics, their universal computational properties have not been explored much unlike in the case of rate-coded and spiking neuron network models. Use of oscillator-based models is often limited to special phenomena like locomotor rhythms and oscillatory attractor-based memories. If neuronal ensembles are taken to be the basic functional units of brain dynamics, it is desirable to develop oscillator-based models that can explain a wide variety of neural phenomena. Autoencoders are a special type of feed forward networks that have been used for construction of large-scale deep networks. Although autoencoders based on rate-coded and spiking neuron networks have been proposed, there are no autoencoders based on oscillators. We propose here an oscillatory neural network model that performs the function of an autoencoder. The model is a hybrid of rate-coded neurons and neural oscillators. Input signals modulate the frequency of the neural encoder oscillators. These signals are then multiplexed using a network of rate-code neurons that has afferent Hebbian and lateral anti-Hebbian connectivity, termed as Lateral Anti Hebbian Network (LAHN). Finally the LAHN output is de-multiplexed using an output neural layer which is a combination of adaptive Hopf and Kuramoto oscillators for the signal reconstruction. The Kuramoto-Hopf combination performing demodulation is a novel way of describing a neural phase-locked loop. The proposed model is tested using both synthetic signals and real world EEG signals. The proposed model arises out of the general motivation to construct biologically inspired, oscillatory versions of some of the standard neural network models, and presents itself as an autoencoder network based on oscillatory neurons applicable to time series signals. As a demonstration, the model is applied to compression of EEG signals.

Keywords: oscillatory autoencoder, Kuramoto oscillator, adaptive Hopf oscillator, frequency modulation, multiplexing, phase synchronization, EEG
Go to:
Introduction
Despite decades of research, the question of neural code is still controversial. Currently there are two well-accepted approaches to the problem: the spike rate code and the spike timing code. The former assumes that the neural code lies in the spike rate and has given rise to large class of rate-coded neural networks (Lippmann, 1989; Ruck et al., 1990; Lawrence et al., 1997). The latter holds that the code lies in the spike timing and has led to creation of a large class of spiking neuron networks (Maass, 1997b; Izhikevich, 2003, 2004; Ghosh-Dastidar and Adeli, 2009). Both rate-coded and spiking neuron networks are endowed with universal computational properties (Maass, 1997a; Auer et al., 2008). However the basic functional unit of the brain seems to be, not a single neuron, but a "cell assembly" (Buzsáki et al., 2012), a cortical column being an example of such a unit (Buzsáki and Draguhn, 2004). The collective activity of a cell assembly is not a spike train but a smoother signal called the local field potential (LFP) (Buzsáki et al., 2012). Most of the functional neuro-imaging data including the electroencephalogram (EEG) and functional Magnetic Resonance Imaging (fMRI) encompass the description of the neural activity at this level (Logothetis et al., 2001; David and Friston, 2003; Whittingstall and Logothetis, 2009). Thus when it comes to the description of neural activity at the level of cell assemblies the standard tools and concepts of signal processing could be deployed. The activity of a single cell assembly can then be described in terms of amplitude, frequency, and phase. Communication between two cell assemblies can be described in terms of phase difference at a given frequency. Hence observed neuro physiological phenomena may be explained in terms of oscillator entrainment and phase synchronization.

It is then natural to envisage neural models of three broad classes—rate code based, spike-based, and oscillator based. There are indeed neural models of oscillators (Wang and Terman, 1995; Campbell et al., 1999; Ijspeert, 2008) but they seem to be often applied to specialized purposes and do not seem to enjoy the universality of both rate coded and spiking neuron network models. Oscillatory neuron models are used to model extensively oscillatory phenomena of the brain like building generative models of cortical oscillations to understand brain rhythms and neuronal synchronization (Cumin and Unsworth, 2007; Breakspear et al., 2010). Furthermore when it comes to modeling behavior, they are also restricted to those behaviors that are intrinsically rhythmic like the locomotor movements, rhythmic hand movements, or swimming movements (Ijspeert et al., 2005; Ijspeert, 2008). Such restricted use of oscillator models is untenable since the very same brain oscillations which drive the hand when making rhythmic tapping movements also enable it to perform non-rhythmic point-to-point reaching movements. Although there are exceptions to this case (see Hoppensteadt and Izhikevich, 2000; Heitmann et al., 2015) there exist only a minimal literature on using oscillatory dynamics to explain non-oscillatory behavior
1 / 1
ChatGPT
ChatGPT
The article "An Oscillatory Neural Autoencoder Based on Frequency Modulation and Multiplexing" by Karthik Soman, Vignesh Muralidharan, and V. Srinivasa Chakravarthy, published in Frontiers in Computational Neuroscience in 2018, presents an oscillatory neural network model that serves the function of an autoencoder.
Oscillatory Neural Networks (ONNs) use coupled oscillators as neurons and are inspired by the phenomenon of neural oscillations in the brain. These types of networks are closely linked to the Kuramoto model, which describes a large set of weakly interacting oscillators. Oscillatory neural networks have been used to recognize images and even to construct an oscillatory autoencoder.
An oscillatory autoencoder in this context is a type of feed-forward network that uses a hybrid of rate-coded neurons and neural oscillators. The model uses input signals to modulate the frequency of neural encoder oscillators, and these signals are then multiplexed using a network of rate-code neurons. The output is then de-multiplexed using an output neural layer for signal reconstruction.
The authors demonstrated that neurons composed of two coupled oscillators - one with a fixed frequency and one with a tunable frequency - can perform complex logical operations, like XOR, which cannot be achieved by conventional sigmoid neurons.
The proposed model was tested using both synthetic signals and real-world EEG signals, showing that the model could compress EEG signals effectively. The authors argue that this approach could lead to biologically inspired, oscillatory versions of standard neural network models.
In addition to their technical achievements, the authors emphasize that there are still open questions about the nature of neural coding in the brain. They suggest that different perspectives - including rate coding, timing coding, and now oscillatory coding - could all have a role to play in understanding and modeling brain function.
1 / 1
February 6, 2023
0
Neuroscience
Radiology & Imaging
 Editors' notes
This article has been reviewed according to Science X's editorial process and policies. Editors have highlighted the following attributes while ensuring the content's credibility:

 fact-checked
 peer-reviewed publication
 proofread
Ok!
Study suggests the brain works like a resonance chamber
by Champalimaud Centre for the Unknown

Would you like to receive trending story notifications on your smartphone?
Study suggests the brain works like a resonance chamber
Spectral power of fMRI signals differs significantly between anesthesia conditions. a Spatial maps of spectral power in 8 non-overlapping frequency bands, averaged across the s = 12 fMRI scans recorded under each anesthesia conditions and normalized by the mean power in sPM = 2 postmortem scans. fMRI recordings were recorded from n = 6 genetically similar female rats, each scanned twice in 3 anesthesia conditions (with isoflurane at 0%, 1%, and 3%. b Power in 8 frequency bands averaged across all brain voxels relative to the mean power in 2 postmortem scans. Dots correspond to the mean power in each scan and error bars represent the mean ± standard error across the s = 12 scans. c Two-sided p-values obtained from 10,000 t-tests on randomly permuted data comparing the power in s = 12 scans over the 3 anesthesia conditions and in each of 8 non-overlapping frequency bands, reported with respect to the standard threshold of 0.05 and the Bonferroni-corrected threshold of 0.0021. Credit: Nature Communications (2023). DOI: 10.1038/s41467-023-36025-x
Researchers at the Champalimaud Foundation and the University of Minho, in Portugal, have found evidence of resonant waves in rat brain activity using ultrafast and ultrahigh field magnetic resonance imaging. Their work demonstrates from simple fundamental principles how such waves—much in the manner of sound vibrations in a guitar chamber—can form connections between distant brain areas, which are key for healthy brain function.


It's been over 20 years since neuroimaging studies—using functional magnetic resonance imaging (fMRI), a widely-used technology to capture live videos of brain activity—have been detecting brain-wide complex patterns of correlated brain activity that appear disrupted in a wide range of neurological and psychiatric disorders. These patterns form spontaneously, even at rest when no particular task is being performed, and have been detected not only in humans but also across mammals, including monkeys and rodents.

Although such spatial patterns of correlated activation have been consistently detected across neuroimaging centers around the world, the nature of these correlations was not clear.

"We do not yet fully understand how the brain communicates over long distances. We know that distant areas exhibit signal correlations, and that they are implicated in brain function, but we do not completely understand their nature," says Noam Shemesh, principal investigator of the Preclinical MRI Lab at the Champalimaud Foundation, in Lisbon, and senior author of a study published in Nature Communications.

"In this study, we wanted to understand what lies underneath those correlations and investigate the mechanisms involved," stresses Shemesh.


The video shows that brain activity captured with fMRI can be reconstructed as the superposition of a small number of macroscopic stationary waves, or resonant modes, oscillating in time. Credit: Joana Cabral
A number of theoretical works had proposed that these patterns could be explained by standing waves (whose peaks and troughs do not move in space) resonating in the brain structure—that is, by waves analogous to the modes of vibration in musical instruments. But there was little experimental evidence to support this hypothesis due to the poor temporal resolution of fMRI, reaching only an image or two per second.

"If we could find that the spatial patterns oscillate, this would provide evidence supporting the resonance hypothesis," says Joana Cabral, first author of the study, from the Life and Health Sciences Research Institute of the University of Minho and a visiting scientist in Shemesh's lab since 2019.

So what the team did was to speed up image acquisition, and they discovered that the signals in distant brain regions actually oscillate together in time. "These oscillatory patterns look like a higher-dimensional analog of resonance modes in musical instruments; they are akin to reverberations, to echoes inside the brain," says Cabral.

"Our data show that the complex spatial patterns are a result of transiently and independently oscillating underlying modes, just like individual instruments participate in creating a more complex piece in an orchestra," says Shemesh.

"The distinct modes, each contributing something to the overall picture at different time scales and different wavelengths, can be added up together, generating complex macroscopic patterns similar to the ones observed experimentally [see below]. To our knowledge, this is the first time that brain activity captured with fMRI is reconstructed as the superposition of standing waves," he points out.

The new study thus strongly points to a key role for these resonant waves, or modes, in brain function. These resonant phenomena, the authors believe, are at the root of the coherent, coordinated brain activity that is needed for normal brain function as a whole.

Ultrafast MRI
The researchers detected the resonant modes in rats in the resting state, which means the animals were not subjected to any specific external stimulus. Indeed, no tasks were needed, for as already mentioned, even when we (and mammals in general) are doing nothing in particular, our brains continue to generate spontaneous activity patterns that can be captured by fMRI.

To visualize the oscillations, the researchers created "videos" of activity using the potent ultrahigh-field experimental MRI scanner in Shemesh's lab and performed ultrafast experiments developed some time ago by that lab for other purposes.

"Noam and I met in 2019, and we decided to obtain recordings of brain activity at the maximum temporal resolution we could achieve in the 9.4 Tesla scanner at his lab," recalls Cabral. "Noam designed the experiments, Francisca Fernandes [the third author of the study] performed them, and I did the data analysis and the visualization. Noam managed to achieve a temporal resolution of 26 images per second, and thus obtained 16,000 images per 10 minute scan (instead of 600 images at the typical resolution of one image per second)."

Like waves in the ocean
"When we first saw the videos of the recorded brain activity, we saw clear waves of activity, like waves in the ocean, propagating in complex patterns within the cortex and the striatum [a subcortical region of the forebrain]", says Cabral.

"And we found that the signals could be described by the superposition of a small number of macroscopic stationary waves, or resonant modes, oscillating in time. Notably, each standing wave was found to cover extended areas of the brain, with peaks distributed in distinct cortical and subcortical structures, forming functional networks."

The researchers experimented with rats in three different conditions: sedated, lightly anesthetised and deeply anesthetised. (In fact, the animals were lightly sedated in the resting state, to avoid any discomfort to them.) "The spatial configuration of these stationary waves was very consistent across rats scanned in the same condition," Cabral points out.

Shemesh adds, "We showed that brain functional networks are driven by resonance phenomena. This explains the correlations that are otherwise observed when you do slow imaging. Long-range brain interactions are governed by a 'flow' of information that is oscillatory and repetitive."

Pathological states
They also found that increasing the amount of anesthetic reduces the number, frequency and duration of the resonant stationary waves. As already mentioned, previous studies have shown that certain patterns of brain activation are consistently altered in disorders of consciousness. So this experimental design, says Cabral, was actually also meant to mimic different pathological states.

"Functional networks appear disrupted in several neurological and psychiatric disorders," she points out. If confirmed in humans, she speculates, their results could also lead to the use of resonant modes as biomarkers for disease.

"Our study also provides a new 'lead' in looking at disease," corroborates Shemesh. "We know that long-range brain activity is strongly impacted in disease, but we do not understand why or how. Understanding the mechanism of long-range interactions could lead to a completely new way of characterizing disease and hinting on the type of treatment that may be necessary: for example, if a specific resonant mode was missing from a patient, we might want to find ways to stimulate that particular mode."

More work will obviously be needed to confirm all these results, the researchers agree, and whether they are replicable in humans. But "once we understand better the nature of functional networks, we can design informed strategies to modulate these network patterns," says Cabral.


More information: Joana Cabral et al, Intrinsic macroscale oscillatory modes driving long range functional connectivity in female rat brains detected by ultrafast fMRI, Nature Communications (2023). DOI: 10.1038/s41467-023-36025-x

Journal information: Nature Communications 

Provided by Champalimaud Centre for the Unknown

Feedback to editors
Related

New ultrafast fMRI technique may help inform brain stimulation
Jan 27, 2023

High frequency brain wave patterns in the motor cortex can predict an upcoming movement
Jan 19, 2023

Researchers provide new framework for studying brain organization
Jul 28, 2022

How the brain encodes social network structure
Apr 26, 2021

Researchers uncover brain waves related to social behavior
Jun 24, 2022

Spatial signatures of anesthesia-induced burst-suppression differ between primates and rodents
May 24, 2022
Recommended
Load comments (0)


More than 3 in 5 children do not receive timely mental health services after firearm injury, study finds
3 hours ago
Real-world data suggests stopping immunotherapy after two years is reasonable in patients with advanced lung cancer
18 hours ago
Trial demonstrates one-year progression-free survival in 94% of patients with stage 3 or 4 classic Hodgkin lymphoma
19 hours ago
Experts warn bird flu virus changing rapidly in largest ever outbreak
Jun 3, 2023
Genetic variants may affect treatment response to commonly prescribed type 2 diabetes medication
Jun 3, 2023
For advanced, HER2-amplified bile duct cancers, antibody treatment trial shows promising results
Jun 2, 2023
Sensory adapted dental rooms significantly reduce autistic children's physiological stress during teeth cleanings
Jun 2, 2023
'Tipping the balance' of immune cells from bad to good reverses multiple sclerosis symptoms in mice
Jun 2, 2023
Scientists reveal new details of cellular process that prevents spread of cancer
Jun 2, 2023
Study helps explain what drives psoriasis severity and offers clues as to how disease may spread to other body parts
Jun 2, 2023
Wharton's jelly found to halt diabetes progression in Phase II trial
Jun 2, 2023
REPORT

GET IN TOUCH
Contact us
OUR PRODUCTS
Phys.org
Tech Xplore
Science X
OTHER PUBLICATIONS
Android app
iOS app
RSS feeds
EXTRAS
Help
FAQ
LEGAL
About
Terms of use
Privacy policy
Science X Account
Premium Account
Newsletter
Archive
© Medical Xpress 2011 - 2023 powered by Science X Network
1 / 1
ChatGPT
ChatGPT
This article from Science X discusses a study conducted by researchers at the Champalimaud Foundation and the University of Minho, Portugal, which suggests that the brain operates similarly to a resonance chamber. They used ultrafast and ultrahigh field magnetic resonance imaging (fMRI) to find evidence of resonant waves in rat brain activity.
These waves can form connections between distant brain regions, crucial for healthy brain function. This idea has been proposed theoretically before, but this is one of the first times it has been observed in experimental settings, largely due to the high temporal resolution of the fMRI technology used.
The study showed that spatial patterns of brain activity can be explained by the superposition of a small number of macroscopic stationary waves, or resonant modes, oscillating over time. This finding hints at a key role for these resonant waves in brain function. The resonance phenomena may underpin the coordinated brain activity necessary for normal brain function.
Interestingly, the researchers found that increasing anesthesia reduces the number, frequency, and duration of these resonant stationary waves. This could be useful for investigating disorders of consciousness and other neurological and psychiatric conditions. It is also speculated that resonant modes could be used as biomarkers for disease.
However, the researchers acknowledge that more work is needed to confirm these results, and to determine if the same phenomena occur in human brains. If confirmed, these findings could provide important insights into how the brain functions and how neurological and psychiatric disorders can be diagnosed and treated.
The study was published in the journal Nature Communications.
1 / 1
As a library, NLM provides access to scientific literature. Inclusion in an NLM database does not imply endorsement of, or agreement with, the contents by NLM or the National Institutes of Health. Learn more about our disclaimer.
Logo of frontcompneuro
Front Comput Neurosci. 2018; 12: 52. Published online 2018 Jul 10. doi: 10.3389/fncom.2018.00052
PMCID: PMC6048285PMID: 30042669
An Oscillatory Neural Autoencoder Based on Frequency Modulation and Multiplexing
Karthik Soman,1 Vignesh Muralidharan,2 and V. Srinivasa Chakravarthy1,*
Author information Article notes Copyright and License information Disclaimer
Associated Data
Supplementary Materials
Data Availability Statement
Go to:
Abstract
Oscillatory phenomena are ubiquitous in the brain. Although there are oscillator-based models of brain dynamics, their universal computational properties have not been explored much unlike in the case of rate-coded and spiking neuron network models. Use of oscillator-based models is often limited to special phenomena like locomotor rhythms and oscillatory attractor-based memories. If neuronal ensembles are taken to be the basic functional units of brain dynamics, it is desirable to develop oscillator-based models that can explain a wide variety of neural phenomena. Autoencoders are a special type of feed forward networks that have been used for construction of large-scale deep networks. Although autoencoders based on rate-coded and spiking neuron networks have been proposed, there are no autoencoders based on oscillators. We propose here an oscillatory neural network model that performs the function of an autoencoder. The model is a hybrid of rate-coded neurons and neural oscillators. Input signals modulate the frequency of the neural encoder oscillators. These signals are then multiplexed using a network of rate-code neurons that has afferent Hebbian and lateral anti-Hebbian connectivity, termed as Lateral Anti Hebbian Network (LAHN). Finally the LAHN output is de-multiplexed using an output neural layer which is a combination of adaptive Hopf and Kuramoto oscillators for the signal reconstruction. The Kuramoto-Hopf combination performing demodulation is a novel way of describing a neural phase-locked loop. The proposed model is tested using both synthetic signals and real world EEG signals. The proposed model arises out of the general motivation to construct biologically inspired, oscillatory versions of some of the standard neural network models, and presents itself as an autoencoder network based on oscillatory neurons applicable to time series signals. As a demonstration, the model is applied to compression of EEG signals.

Keywords: oscillatory autoencoder, Kuramoto oscillator, adaptive Hopf oscillator, frequency modulation, multiplexing, phase synchronization, EEG
Go to:
Introduction
Despite decades of research, the question of neural code is still controversial. Currently there are two well-accepted approaches to the problem: the spike rate code and the spike timing code. The former assumes that the neural code lies in the spike rate and has given rise to large class of rate-coded neural networks (Lippmann, 1989; Ruck et al., 1990; Lawrence et al., 1997). The latter holds that the code lies in the spike timing and has led to creation of a large class of spiking neuron networks (Maass, 1997b; Izhikevich, 2003, 2004; Ghosh-Dastidar and Adeli, 2009). Both rate-coded and spiking neuron networks are endowed with universal computational properties (Maass, 1997a; Auer et al., 2008). However the basic functional unit of the brain seems to be, not a single neuron, but a "cell assembly" (Buzsáki et al., 2012), a cortical column being an example of such a unit (Buzsáki and Draguhn, 2004). The collective activity of a cell assembly is not a spike train but a smoother signal called the local field potential (LFP) (Buzsáki et al., 2012). Most of the functional neuro-imaging data including the electroencephalogram (EEG) and functional Magnetic Resonance Imaging (fMRI) encompass the description of the neural activity at this level (Logothetis et al., 2001; David and Friston, 2003; Whittingstall and Logothetis, 2009). Thus when it comes to the description of neural activity at the level of cell assemblies the standard tools and concepts of signal processing could be deployed. The activity of a single cell assembly can then be described in terms of amplitude, frequency, and phase. Communication between two cell assemblies can be described in terms of phase difference at a given frequency. Hence observed neuro physiological phenomena may be explained in terms of oscillator entrainment and phase synchronization.

It is then natural to envisage neural models of three broad classes—rate code based, spike-based, and oscillator based. There are indeed neural models of oscillators (Wang and Terman, 1995; Campbell et al., 1999; Ijspeert, 2008) but they seem to be often applied to specialized purposes and do not seem to enjoy the universality of both rate coded and spiking neuron network models. Oscillatory neuron models are used to model extensively oscillatory phenomena of the brain like building generative models of cortical oscillations to understand brain rhythms and neuronal synchronization (Cumin and Unsworth, 2007; Breakspear et al., 2010). Furthermore when it comes to modeling behavior, they are also restricted to those behaviors that are intrinsically rhythmic like the locomotor movements, rhythmic hand movements, or swimming movements (Ijspeert et al., 2005; Ijspeert, 2008). Such restricted use of oscillator models is untenable since the very same brain oscillations which drive the hand when making rhythmic tapping movements also enable it to perform non-rhythmic point-to-point reaching movements. Although there are exceptions to this case (see Hoppensteadt and Izhikevich, 2000; Heitmann et al., 2015) there exist only a minimal literature on using oscillatory dynamics to explain non-oscillatory behavior. Therefore it is important to investigate if oscillatory neural network models possess the property of universal computation that forms the core strength of its rival models: rate-coded and spiking neural network models.

The strength of the rate coded and
1 / 1
ChatGPT
ChatGPT
This scientific paper presents an oscillatory neural network model that operates as an autoencoder. An autoencoder is a type of neural network used in machine learning, typically for tasks such as feature learning and dimensionality reduction. Traditional autoencoders are based on rate-coded or spiking neuron networks.
The researchers propose an oscillatory neural autoencoder which is a mix of rate-coded neurons and neural oscillators. The function of this network is to modulate the frequency of input signals via neural encoder oscillators. These modulated signals are then multiplexed through a network of rate-coded neurons, specifically a Lateral Anti Hebbian Network (LAHN) that uses afferent Hebbian and lateral anti-Hebbian connectivity.
Subsequently, the output from the LAHN is demultiplexed using another neural layer. This layer is a blend of adaptive Hopf and Kuramoto oscillators which perform signal reconstruction. This method of describing a neural phase-locked loop is novel and has potential applications for the study of neural signals and neural networks.
The model was tested using both synthetic signals and real-world EEG signals. Its performance was evaluated with regards to signal compression and reconstruction. The authors suggest that their oscillatory autoencoder model could have applications for time series signals and be helpful in developing more biologically inspired neural network models.
The model was motivated by a broader goal to construct oscillatory versions of standard neural network models, with an emphasis on developing oscillator-based models that can explain a wide range of neural phenomena. This is particularly important as the researchers highlight the significance of oscillatory phenomena in the brain, and the potential insights that could be gained by focusing on models that harness these oscillations. They argue that if neuronal ensembles are taken to be the basic functional units of brain dynamics, it's crucial to create models that can interpret and utilize these phenomena.
The authors also discuss the long-standing controversy in the field of neuroscience regarding neural coding - whether the code lies in the spike rate or spike timing. Their work introduces a third possibility, oscillator-based coding, which could lead to new understanding and models in the field. They argue that it's necessary to extend the use of oscillator models, not just for rhythmic behaviors, but also for non-rhythmic ones.
In conclusion, this paper presents an oscillatory neural network model that functions as an autoencoder, contributing a fresh perspective to the understanding of neural networks and neural coding.
1 / 1
spiking neuron networks lies in the fact that they have been designed to solve a wide of range of useful information processing problems: to construct transformations from one space to another (Lippmann, 1989; Schmidhuber, 2015), to map high dimensional information onto bounded two-dimensional spaces (Kohonen, 1998), to process sequences (Frasconi et al., 1995), to store patterns as attractors (Hopfield, 1982; Trappenberg, 2003), to construct dimensionality reduced representations by autoencoding (Oja, 1989; Sanger, 1989; Hinton and Salakhutdinov, 2006) and so on. In this realm of applications, in most cases, equivalent oscillatory neural network models have not been designed which, when realized, could form another dimension for understanding standard neural network theory.


Methods
Here we propose the architecture of an autoencoder using oscillatory neurons. The motivation for an oscillatory autoencoder is explained above in the introduction section. The model architecture described here consists of Encoder and Decoder modules as shown in Figure ​Figure1.1. The encoder process the input signals and makes a lower dimensional representation of the same. The decoder module reconstructs back the original input signal from this abstract representation.

An external file that holds a picture, illustration, etc.
Object name is fncom-12-00052-g0001.jpg
Figure 1
The network architecture of the oscillatory autoencoder network. In the encoder module, the incoming message signals (s1,..., sN(t)) are encoded, via FM, onto carrier signals with intrinsic frequencies ωE1,..., ωEN. A lateral anti-Hebbian layer is used for frequency multiplexing (MUX) of the modulated signals. In the decoder module, frequency tracking (FT) is performed by a series of adaptive Hopf oscillators which can tune their intrinsic frequency dynamically, followed by a layer of Kuramoto oscillators which synchronize to the frequency of the Hopf oscillators to form a basic unit which extracts the embedded input signals (DM). This is further passed to a leaky integrator (LPF) to get smoothened output of the network [ŝ1,..., ŝN(t)].

The encoder receives inputs as an array of N band limited signals, s1(t),., sN(t). These signals are frequency modulated and multiplexed by the encoder. The multiplexed signals are demultiplexed and demodulated by the decoder. Both the encoder and the decoder are networks of oscillators. The networks are hybrids of Hopf and Kuramoto oscillators (Kuramoto, 1984; Righetti et al., 2006). The motivation for choosing two different phase oscillators is described in the decoder section. The encoder and decoder modules are modeled as follows.

A. encoder
The encoder has two stages viz. Frequency Modulation (FM) stage and MUX stage.

FM stage
FM stage has N phase oscillators each with different intrinsic frequencies. N is equal to the dimension of the input. Each of the input signals is connected to one of these oscillators. Input is encoded by the phase dynamics as given in (1). This phase dynamics is equivalent to FM (Haykin et al., 1989) and hence the name FM stage.

θi∙=ωEi+si(t)
(1)
θi is the phase of the ith oscillator in the encoder layer. ωEi is the intrinsic angular frequency of the ith oscillator in the encoder layer. (Note: The superscript E stands for Encoder layer).

MUX stage
A classical MUX in electronics literature ensures harmonious transfer of information between the sender and receiver by acting like a multiple switch (Omotayo, 1985). Hence, a MUX usually has n number of input lines and 1 output line. However, in the proposed model we do not use this strict definition of MUX instead we take the idea of compressing the n input signals to m dimensions where m < n. This is what is exactly achieved through the hidden layers of a traditional autoencoder. The reason why we named it MUX is to bring about a direct comparison of neural information transfer to the radio FM communication principles.

The MUX stage is implemented by a neural network architecture known as Lateral Anti-Hebbian Network (LAHN) that has Hebbian (excitatory) afferent and anti-Hebbian (inhibitory) lateral connections (Földiak, 1990). The dynamics of a neuron in LAHN is given by Equation (2). Hebbian learning applied to the afferent weight connections (Equation 5) brings the afferent weight vector close to the input data ensuring feature selection by that particular neuron. The anti-Hebbian rule applied to the lateral connections induces competition among the LAHN neurons. Hence each LAHN neuron learns different features from the input data. This network was shown to extract optimal features from the input data by converging transformation weight vectors to the subspace of the principal components of the input data (Földiak, 1990). Since this network maximizes the variance of the output (Földiak, 1990), it extracts optimal features from the input data. The information required for the unsupervised learning of LAHN neuron is available locally at its synaptic connections (Equations 4, 5) and this makes the network biologically plausible.

This LAHN layer acts as the hidden layer for the oscillatory autoencoder. The low-dimensional representations constructed by the hidden layer of a traditional autoencoder are constructed by this MUX stage in the proposed model. Hence, the number of inputs going to the MUX layer is same as the number of encoder oscillators in the FM stage and the number of outputs from the MUX should be essentially lesser in number than its input to achieve a dimensionality reduction.

The dynamics of a neuron in the MUX stage is given in (2) and (3).

Yi(t)=∑j=1NqijOj(t)+∑k=1nwikYk(t−1)
(2)
Oj = sin(θj)	(3)
Yi is the output of ith neuron, q and w are the afferent and lateral weight connections of MUX respectively, N is the dimension of the input, n is the total number of neurons in the LAHN. Oj is the state of the jth input oscillator. In MUX, lateral weights are updated using anti-Hebbian learning and afferent weights are updated using Hebbian learning (Földiak, 1990) as given in (4) and (5).

Δwik =  - ηLYi(t)Yk(t - 1)	(4)
Δqij = ηF[Oj(t)Yi(t) - qijYi2(t)]	(5)
ηL and ηF are the learning rates for lateral and feed forward weights respectively. MUX with n nodes trained using (4) and (5) mixes the input FM signals with a minimal overlap in their frequency spectrums which further decreases the reconstruction error.

B. decoder
The decoder has three stages such as Frequency Tracking (FT) performed by adaptive Hopf oscillators, Demodulation (DM) using Kuramoto oscillators, and final smoothening of signal by low-pass filtering (LPF) using leaky integrator neurons stages respectively. Each section is explained in detail below.

FT stage
Initially the responses of MUX are passed onwards to the FT stage. The purpose of this stage is to tease out the individual frequencies which are mixed by the MUX stage. This frequency tracking is achieved by using Hopf oscillators with adaptive frequency dynamics. Hopf oscillators were successfully implemented as an adaptive frequency system that updates its intrinsic frequency in an iterative way until it converges to one of the frequencies of the input data (Righetti et al., 2006). This system of Hopf oscillators was previously shown by Righetti et al. (2006) to learn the frequency components of its input signals. This was achieved by adding a frequency adaptation variable to the classical two variable Hopf oscillator dynamics (Righetti et al., 2006). This was shown in phase oscillators having unit circle phase plane limit cycles i.e., using Hopf oscillators. They have further explained similar frequency adaptation dynamics for relaxation oscillators too. However, in this model we are using harmonic phase oscillators for the frequency tracking stage as explained below.

Here, we wanted to achieve the aforementioned phenomena of tracking the frequency of input data. The adaptive frequency Hopf oscillators act like band-pass filters and filter out different frequency bands from the mixed input signal. The adaptive frequency dynamics is accomplished using the following equations:

ri∙=ri(μ−ri2)
(6)
ϕ∙i=ωDi−εriYsin(ϕi)
(7)
ω∙Di=−εYsin(ϕi)
(8)
r, φ and ωD are the radius, phase and angular frequency variables of a Hopf oscillator respectively (Note: the superscript D stands for Decoder module). μ is the parameter that controls the radius of the limit cycle. For μ = 1, it produces a unit circle limit cycle. ε is the coupling factor between the MUX and the Hopf oscillators (Righetti et al., 2006). Because of linearity of the MUX, ε can be computed directly using (9).

ε = P+	(9)
P = (I-W)-1Q	(10)
P is the transformation matrix of the MUX and P+ is the pseudo inverse of matrix P. I is the identity matrix. W and Q are the lateral and afferent weight matrices of LAHN respectively. P can be derived by virtue of the linearity of LAHN as given in (2).

DM stage
The purpose of the DM stage is to extract the low-frequency, band limited message signals from the outputs of the FT stage. The DM consists of a layer of Kuramoto oscillators. This shift from Hopf oscillator (in FT stage) to Kuramoto oscillator (in DM stage) is to implement the process of phase synchronization. Kuramoto oscillatory dynamics have been previously implemented to achieve phase synchronization (Kuramoto, 1984). This synchronization in the phase of two oscillators is essential for extracting the message from the FM signal (Haykin et al., 1989) (see Supplementary Material). Each Hopf oscillator in the FT stage is coupled in a one-to-one fashion to a Kuramoto oscillator in the DM stage. The pairs of oscillators (the Kuramoto oscillators of DM and the Hopf oscillators of FT stage) are coupled through their respective phase variables as shown in (11) and (12).

γ∙i=ωEi+KDi
(11)
Di = sin(ϕi - γi)	(12)
γi is the phase variable of ith Kuramoto oscillator. It has the same intrinsic frequency, ωEi, as that of the encoder oscillators (Equation 1) and K is a positive coupling factor (Kuramoto, 1984). This stage is crucial since phase synchronization occurs at this stage and the synchronization dynamics further decodes the low frequency message signal embedded in the output of the Hopf Oscillator (see Supplementary Material).

LPF stage
Di shown in (12) is the output of the decoder which is further passed through a leaky integrator to smoothen the outputs, i.e., low pass filtering (LPF stage). Leaky integrator acts as a low pass filter which further smoothens out the decoded signal, and eliminates any high frequency components present. Dynamics of leaky integrator is given in (13).

ds∧idt=−Asi∧+Di(t)
(13)
sI is the state of ith leaky integrator which is the reconstructed version of the input signal si(t); A is the leakage factor which is a positive constant.

Hence the proposed model is a hybrid one consisting of oscillatory layers sandwiching a rate coded layer. Hopf oscillators are used in the model for FM. A layer of linear neurons with lateral connections is used for frequency multiplexing which essentially mixes the FM signals. Hopf oscillators with adaptive frequency are used to track the carrier frequencies of the FM signals. Finally, Kuramoto oscillators are used to demodulate the FM signal and extract the message signal. 

Results
We now test the model described in the previous section on an array of synthetic signals and also on real world EEG signals.

A. simulation of the model on synthetic signals
The synthetic signals used for the simulation are of the general form s(t) = A1sin(ω1t) + A2sin(ω2t).


The synthetic input signals used for simulating the oscillatory autoencoder network. The waveforms (A-D) follow the equations (14) - (17) respectively.


The initial intrinsic angular frequencies of the FM oscillators are taken as ωE = [200 Hz, 350 Hz, 850 Hz, 1000 Hz]. The input signals, as given by Equations (14)-(17), are used to modulate the encoder oscillators as per Equation (1). Let the resultant frequency modulated signals be O1, O2, O3, O4 respectively as given by Equation (3). Figures 2A-D shows the waveforms of the input signals (for a short duration) as given by Equations. (14)-(17). Figures 3A-D shows the corresponding frequency spectra. All the frequency spectra are obtained using the Fourier Transformation on the input signals. Figures 3A-D clearly show that the input signals are modulated to the higher frequency regime corresponding to the respective carrier waves.
1 / 1
ChatGPT
ChatGPT
This is an extensive and detailed description of an oscillatory autoencoder network, which is designed to simulate the way information is processed in neural networks, specifically spiking neuron networks.
The model consists of two modules: an encoder and a decoder. The encoder takes in the input signals and reduces their dimensions. The decoder then reconstructs the original signals from the reduced versions.
The encoder uses Frequency Modulation (FM) and a Multiplexing (MUX) stage, involving phase oscillators and a Lateral Anti-Hebbian Network (LAHN) to process and encode the input signals.
The decoder, on the other hand, uses Frequency Tracking (FT), Demodulation (DM), and Low-Pass Filtering (LPF) stages. These involve adaptive Hopf oscillators, Kuramoto oscillators, and leaky integrator neurons respectively, to recover the original signal from its encoded form.
The model was then tested on synthetic signals, with results indicating that the input signals were successfully modulated to a higher frequency regime corresponding to their respective carrier waves.
It's worth noting that this description involves complex concepts from computational neuroscience and machine learning, and requires some background knowledge in these areas to fully understand. It also makes references to mathematical equations, presumably provided in the original context of this description, which are not included here. As such, some details might be hard to follow without these equations and accompanying diagrams.
Overall, the development of such oscillatory neural networks is a significant step towards creating more biologically accurate models of neural information processing. By better simulating the dynamic, oscillatory nature of neuronal activity, these models could potentially offer new insights into how the brain processes information and pave the way for the development of more effective and efficient machine learning algorithms.
1 / 1
Apart from the aforementioned research on neural codes, in the realm of neural signal processing, it becomes natural to link the brain signals arising from EEG and MEG to an underlying oscillatory process which connects to the mechanistic underpinnings of brain circuitry. Utilizing these ideas, a large body of literature exists in the domain of EEG related applications like Brain Computed Interfaces (BCIs). Often in these studies motor imagery EEG signals are recorded, classified and the results of classification are used to drive a machine like the wheelchair (Leeb et al., 2007a,b). The dependence on the stationarity of signals is very important for current methods, including optimal spatial filtering (Ramoser et al., 2000) to solve these class of problems posing difficulty in reliable processing of EEG. The stochastic and non-linear nature of EEG signal thus poses critical challenges in its processing such as feature extraction and further classification (Pfurtscheller and Neuper, 2001). As of now, there exists no benchmark method to decipher this problem of EEG processing. We believe that a better understanding of the oscillatory neural network models, mimicking the underlying neural process, could pave way to a novel class of algorithms for processing EEG signals.

Although the objective of the proposed model is to shed light on the oscillatory neural code, we would also like to briefly cite literature on time series data mining and time series representations. Time series data mining is apparently a challenging one because of the unique characteristic features of the time series data such as presence of noise, and non-linear relation of the data elements (Wilson, 2017). A problem that often arises in time series data processing is to form an optimal representation of the data either by reducing or approximating it, but making sure that the approximated version of the data still carries the local/global features of the original version. For the ease and efficient use of the data, the main challenge is to choose an optimal representation of the same. Time series data representation is a well-studied area where methods such as Discrete Fourier Transform (DFT) (Faloutsos et al., 1994), Discrete Wavelet Transform (DWT) (Percival and Walden, 2006), time series Piecewise approximation (Keogh et al., 2001a,b) have been proposed. Due to the current trends in the use of "big data" processing, other novel methods such as transformation of the time series data to discrete variables or symbols has become popular (Lin et al., 2007). The main idea behind this type of methodology is to transform time series data to a sequential data of symbols by initially discretizing the time series using methods like Piecewise Aggregate Approximation (PAA) (Keogh et al., 2001b). This can be treated as a way to reduce the number of points in the time series data and this is followed by converting the approximated numerical data to corresponding symbols using popular algorithms like SAX (Lin et al., 2007). The advantage of converting the time series to symbolic sequences is that, once the transformation is made, standard pattern matching algorithms can be applied to the sequences for further processing. The aforementioned methods are successful in data mining area, but carry little information on the neural processing of time series data. This is not a flaw of the aforementioned methods because they are not intended to provide any neural perspective on time series data processing. However, the real brain is adept at time series processing since most of the sensory inputs coming from different sensory modalities such as vision, proprioception, auditory, vestibular, tactile, and olfactory stimulus are dynamic in nature. Hence, the objective of this study is to propose a computational model that implements the autoencoding of time series data using biologically plausible neural principles. The very next sub section named as "background" gives the impetus behind the proposed modeling architecture.

Background
In response to the aforementioned general motivation, we now present a network of neural oscillators that serves as an oscillatory autoencoder. The reason why we choose the autoencoder architecture is due to the function it serves i.e., encoding the high dimensional input to a low dimensional abstract representation and further decoding it back to the original input signal. From a neural perspective this can be broadly viewed as different stages of neural information transfer. The first stage starts with the encoding of high dimensional sensory stimulus coming from multiple sensory modalities to a more compatible abstract representation in the subcortical structures. For example, visual information fetched by ~125 million retinal photoreceptors converge to ~1 million neurons of the lateral geniculate nucleus in the thalamus (Hubel, 1995). This is one of the instances (among many) of huge dimensionality reduction that takes place in the real brain. The decoder can be viewed as the stage in which the information is transferred from the subcortical structures to other cortical structures with more number neurons i.e., transfer of information from lower dimension to higher dimension (Guillery and Sherman, 2002). Standard autoencoder networks use static neurons that have limitations in capturing the temporal features of the input in a naturalistic fashion. The proposed model uses the dynamics of oscillatory system such as phase synchronization, frequency tuning, and also uses the signal processing concepts such as frequency modulation (FM) and multiplexing (MUX) to shed light on the possible information transfer mechanisms in the brain.

We brief out here the methods that are adopted to accomplish the aforementioned objective (this is explained in detail in the following methods section). In this model, a set of band-limited signals are frequency modulated by a layer of neural oscillators, multiplexed by a layer of rate-coded neurons, and subsequently demultiplexed and demodulated by oscillatory neurons. The network is a hybrid model consisting of two kinds of oscillator models (Kuramoto and Hopf oscillators) and rate-coded dynamic neurons. The signals obtained at the output of the MUX stage may be considered as a reduced-dimensional representation of the input signals. Finally we test the model on actual EEG signals (real world data). The paper is outlined as follows. Section II presents the methods and the model equations, followed by the results in Section III and finally the discussion in Section IV.
1 / 1
ChatGPT
ChatGPT
The preceding section provides a comprehensive overview of the challenges and opportunities in processing EEG signals, the principles of time series data mining, and a brief introduction to the proposed model of an oscillatory autoencoder. The proposed model aims to emulate biological neural principles to process time series data, which could potentially lead to improved methods of EEG signal processing.
This model suggests that EEG signal processing could benefit from better understanding of the oscillatory neural network models, which could lead to novel classes of algorithms for processing these signals. The transformation of time series data into discrete variables or symbols, often used in data mining, may provide a useful framework for understanding how neural circuits process and represent information over time.
Autoencoders are an integral part of the proposed model. They are used in machine learning for unsupervised learning of efficient codings. In this model, they are used to encode high dimensional input into a low dimensional abstract representation and then decode it back to the original input signal. This methodology is proposed to mimic the neural information transfer in the brain, and the mechanism of dimensionality reduction observed in biological systems, like the visual system.
The model consists of two kinds of oscillator models (Kuramoto and Hopf oscillators) and rate-coded dynamic neurons. It is then tested on actual EEG signals, providing a practical application for real-world data. This approach seems to promise novel insights into the brain's neural coding and the processing of time series data, with the potential for meaningful contributions to the field of EEG signal processing.
However, the proposal is quite complex and would need rigorous testing and validation. The final proof of its usefulness will depend on the quality of the results it produces and the extent to which they lead to improved methods for processing EEG signals and understanding neural coding in the brain.
1 / 1
Connections with the previous topics.
1 / 1
ChatGPT
ChatGPT
The proposed oscillatory autoencoder model ties together several previous topics in a unique and innovative way:
Neural Codes:
The research on the topic of neural codes is integral to the proposed model. The encoding and decoding mechanisms of autoencoders in the model serve as a computational interpretation of neural codes in the brain. This interpretation can provide insights into how neurons encode and decode signals, which could potentially lead to improved methods of EEG signal processing and new discoveries about the brain's neural coding.
Neural Signal Processing:
The model uses the principles of oscillatory neural networks to interpret EEG signals, tying in directly with the topic of neural signal processing. By simulating these networks, the model could provide a new perspective on how the brain processes EEG signals and contribute to a better understanding of brain circuitry.
Time Series Data Mining and Representation:
The principles of time series data mining and representation are directly applied in the proposed model, particularly the transformation of time series data into discrete variables or symbols. This transformation could provide insights into how neural circuits process and represent information over time, which aligns with the goal of understanding neural coding.
Brain-Computer Interfaces (BCIs):
While the proposed model doesn't directly deal with BCIs, the potential improvements it offers for EEG signal processing could significantly benefit BCI applications. More accurate and efficient processing of EEG signals could improve the reliability and functionality of BCIs, which rely on these signals to facilitate communication between the brain and external devices.
In sum, the proposed oscillatory autoencoder model connects several disparate topics to propose a new method for understanding and interpreting EEG signals and neural codes. By combining concepts from these various fields, it offers a novel approach to solving longstanding challenges in neuroscience and could potentially contribute to advancements in related areas, such as BCIs.
1 / 1
Adhikari A., Topiwala M. A., Gordon J. A. (2010). Synchronized activity between the ventral hippocampus and the medial prefrontal cortex during anxiety. Neuron 65, 257-269. 10.1016/j.neuron.2009.12.002 [PMC free article] [PubMed] [CrossRef] [Google Scholar]
Alloy L. B., Nusslock R., Boland E. M. (2015). The development and course of bipolar spectrum disorders: an integrated reward and circadian rhythm dysregulation model. Annu. Rev. Clin. Psychol. 11, 213-250. 10.1146/annurev-clinpsy-032814-112902 [PMC free article] [PubMed] [CrossRef] [Google Scholar]
Alonso A., Garcia-Austt E. (1987). Neuronal sources of theta rhythm in the entorhinal cortex of the rat. Exp. Brain Res. 67, 493-501. 10.1007/BF00247282 [PubMed] [CrossRef] [Google Scholar]
Auer P., Burgsteiner H., Maass W. (2008). A learning rule for very simple universal approximators consisting of a single layer of perceptrons. Neural Netw. 21, 786-795. 10.1016/j.neunet.2007.12.036 [PubMed] [CrossRef] [Google Scholar]
Bliss T. V., Lømo T. (1973). Long-lasting potentiation of synaptic transmission in the dentate area of the anaesthetized rabbit following stimulation of the perforant path. J. Physiol. 232, 331-356. 10.1113/jphysiol.1973.sp010273 [PMC free article] [PubMed] [CrossRef] [Google Scholar]
Breakspear M., Heitmann S., Daffertshofer A. (2010). Generative models of cortical oscillations: neurobiological implications of the kuramoto model. Front. Hum. Neurosci. 4:190. 10.3389/fnhum.2010.00190 [PMC free article] [PubMed] [CrossRef] [Google Scholar]
Burbank K. S. (2015). Mirrored STDP Implements Autoencoder Learning in a Network of Spiking Neurons. PLoS Comput. Biol. 11:e1004566. 10.1371/journal.pcbi.1004566 [PMC free article] [PubMed] [CrossRef] [Google Scholar]
Buzsáki G. (2002). Theta oscillations in the hippocampus. Neuron 33, 325-340. 10.1016/S0896-6273(02)00586-X
1 / 1
ChatGPT
ChatGPT
These references provide supporting evidence and background knowledge for the development and validation of the proposed oscillatory autoencoder model. Below, I'll provide a brief summary of their relevance to the research topic.
Adhikari et al., 2010:
This study suggests that synchronized activity between different regions of the brain may play a role in emotional states such as anxiety. It helps inform the theory that brain activity is characterized by synchronized oscillations, which underpins the proposed model.
Alloy et al., 2015:
This paper proposes a model for the development and course of bipolar spectrum disorders, integrating reward and circadian rhythm dysregulation. It relates to the general theme of complex brain functions being encoded in oscillatory neural activity.
Alonso & Garcia-Austt, 1987:
The authors explored the neuronal sources of theta rhythm in the entorhinal cortex of the rat, which contributes to the understanding of the nature and origins of brain oscillations.
Auer et al., 2008:
This study presents a learning rule for single-layer perceptron networks, which are a type of artificial neural network. The concepts could inform the design and training algorithms of the proposed oscillatory autoencoder.
Bliss & Lømo, 1973:
The landmark study on long-lasting potentiation of synaptic transmission, it has foundational importance in neuroscience and understanding the mechanisms of learning and memory.
Breakspear et al., 2010:
The authors provide generative models of cortical oscillations and discuss the neurobiological implications of the Kuramoto model, which could directly inform the development of the proposed model.
Burbank, 2015:
This study presents a model where Mirrored Spike-Time Dependent Plasticity (STDP) is used to implement autoencoder learning in a network of spiking neurons. This could have direct implications for the proposed model, which also employs autoencoder learning.
Buzsáki, 2002:
This paper explores theta oscillations in the hippocampus, providing valuable information about the characteristics and possible functions of these oscillations.
All these references provide valuable background and supporting information, contributing to the development and understanding of the proposed oscillatory autoencoder model.
1 / 1
Hinton G. E., Salakhutdinov R. R. (2006). Reducing the dimensionality of data with neural networks. Science 313, 504-507. 10.1126/science.1127647 [PubMed] [CrossRef] [Google Scholar]
Hopfield J. J. (1982). Neural networks and physical systems with emergent collective computational abilities. Proc. Natl. Acad. Sci. U.S.A. 79, 2554-2558. [PMC free article] [PubMed] [Google Scholar]
Hoppensteadt F. C., Izhikevich E. M. (1998). Thalamo-cortical interactions modeled by weakly connected oscillators: could the brain use FM radio principles? BioSystems. 48, 85-94. [PubMed] [Google Scholar]
Hoppensteadt F. C., Izhikevich E. M. (2000). Pattern recognition via synchronization in phase-locked loop neural networks. IEEE Trans. Neural Netw. 11, 734-738. 10.1109/72.846744 [PubMed] [CrossRef] [Google Scholar]
Hu T., Pehlevan C., Chklovskii D. B. (2015). A hebbian/anti-hebbian network for online sparse dictionary learning derived from symmetric matrix factorization, in 2014 48th Asilomar Conference on Signals, Systems and Computers (Pacific Grove, CA: IEEE; ), 613-619. [Google Scholar]
Hubel D. H. (1995). Eye, Brain, and Vision. New York, NY: Scientific American Library; Scientific American Books. [Google Scholar]
Ijspeert A. J. (2008). Central pattern generators for locomotion control in animals and robots: a review. Neural Netw. 21, 642-653.
1 / 1
ChatGPT
ChatGPT
These references provide insights and support to the development and validation of the proposed oscillatory autoencoder model. Here is a brief summary of their relevance to the research topic:
Hinton & Salakhutdinov, 2006:
This study is a seminal work in the area of dimensionality reduction using neural networks. It provides a crucial theoretical foundation for the proposed model, which seeks to use oscillatory dynamics to encode high-dimensional data.
Hopfield, 1982:
This foundational paper presents a model of neural networks as physical systems with emergent collective computational abilities. The insights from this paper could be crucial to understanding how neural oscillations can give rise to complex computational abilities.
Hoppensteadt & Izhikevich, 1998:
This study presents a model of thalamo-cortical interactions using weakly connected oscillators, which could provide insights into the mechanisms through which the proposed model could work.
Hoppensteadt & Izhikevich, 2000:
This paper introduces a model for pattern recognition via synchronization in phase-locked loop neural networks. This provides a direct link to the research proposal as it discusses the possible information transfer mechanisms in the brain.
Hu et al., 2015:
This study presents a Hebbian/Anti-Hebbian network for online sparse dictionary learning. It provides insights into the use of Hebbian learning principles in the proposed model.
Hubel, 1995:
This book provides fundamental insights into the structure and function of the visual system, which could inform the development of the proposed model.
Ijspeert, 2008:
This review article discusses central pattern generators for locomotion control in animals and robots. It provides an example of a biological system where rhythmic, oscillatory patterns of activity play a crucial role, which is relevant to the overarching theme of the research proposal.
Collectively, these references provide a rich and diverse array of perspectives and insights into the functioning of neural systems, the principles of neural computation, and the potential roles of oscillatory dynamics in these processes.
1 / 1
A better title for this conversation
1 / 1
The previous model used in this conversation is unavailable. We've switched you to the latest default model.
ChatGPT
ChatGPT
"Exploring Oscillatory Neural Networks and Autoencoders: Insights and References"
1 / 1
Connections between the Themes
1 / 1
ChatGPT
ChatGPT
"Interconnections: Neural Codes, Oscillatory Networks, Autoencoders, and Brain Signal Processing"
1 / 1
I mean a summary of the topics we've discussed and their interconnections
1 / 1
ChatGPT
ChatGPT
"Exploring Interconnections: Neural Codes, Oscillatory Networks, Autoencoders, and Brain Signal Processing"
Summary:
In our conversation, we have delved into various topics and explored their interconnections. We started by discussing the concept of neural codes, which refer to the representation of information in the brain through different coding schemes such as spike rate and spike timing. These neural codes play a crucial role in information processing within the brain.
Next, we explored oscillatory networks, which are models that capture the oscillatory phenomena observed in the brain. While oscillatory models have been used to study specific rhythmic behaviors, their universal computational properties, like those of rate-coded and spiking neuron networks, are still being explored. We highlighted the importance of investigating whether oscillatory neural network models possess the same universal computation capabilities.
From there, we delved into autoencoders, a type of neural network used for constructing large-scale deep networks. Autoencoders are commonly based on rate-coded and spiking neuron networks, but we discussed the absence of autoencoders based on oscillators. We then introduced an oscillatory neural network model that functions as an autoencoder, combining rate-coded neurons and neural oscillators. This model allows for the encoding and decoding of input signals using frequency modulation, multiplexing, and demodulation techniques.
We also touched upon the application of these concepts in brain signal processing, particularly with respect to electroencephalogram (EEG) signals. Processing EEG signals poses challenges due to their stochastic and nonlinear nature, requiring feature extraction and classification methods. We highlighted the potential of oscillatory neural network models, such as the proposed oscillatory autoencoder, in developing novel algorithms for EEG signal processing.
Throughout our discussion, we established the interconnectedness of these topics. Neural codes provide the foundation for encoding and decoding information, which can be explored using oscillatory neural network models, including autoencoders. These models, in turn, offer insights into brain signal processing, particularly in the context of EEG analysis.
By exploring these interconnections, we gain a deeper understanding of how neural information is represented, processed, and decoded in the brain, paving the way for advancements in neuroscience, cognitive science, and artificial intelligence.
2 / 2
A list of all the topics we've discussed.
1 / 1
ChatGPT
ChatGPT
Certainly! Here's a list of all the topics we've discussed:
Neural codes: Spike rate coding and spike timing coding as mechanisms for representing information in the brain.
Oscillatory networks: Models that capture oscillatory phenomena in the brain and their potential for universal computation.
Autoencoders: Neural network models used for constructing large-scale deep networks and their application in encoding and decoding information.
Oscillatory autoencoder: An oscillatory neural network model that combines rate-coded neurons and neural oscillators to function as an autoencoder.
Brain signal processing: The application of these concepts in processing brain signals, particularly electroencephalogram (EEG) signals.
Challenges in EEG processing: The stochastic and nonlinear nature of EEG signals, requiring feature extraction and classification methods.
Interconnections between topics: How neural codes serve as the foundation for encoding and decoding information, the utilization of oscillatory neural network models like autoencoders, and their potential for improving EEG signal processing.
This list encompasses the main topics we've discussed throughout our conversation.
1 / 1
A final summary.
2 / 2
ChatGPT
ChatGPT
Throughout our conversation, we delved into several interconnected topics related to neural codes, oscillatory networks, autoencoders, and brain signal processing. We explored the significance of spike rate coding and spike timing coding as mechanisms for representing information in the brain. Additionally, we discussed the potential of oscillatory neural network models, such as autoencoders, which combine rate-coded neurons and neural oscillators to encode and decode information.
We also touched upon the application of these concepts in processing brain signals, specifically electroencephalogram (EEG) signals. We highlighted the challenges posed by the stochastic and nonlinear nature of EEG signals, which necessitate robust feature extraction and classification methods.
The interconnections between these topics are apparent as neural codes serve as the foundation for encoding and decoding information in various neural network models. Oscillatory networks offer a different perspective by incorporating the dynamics of neural oscillators and their potential for universal computation. Autoencoders, in particular, provide a framework for constructing large-scale deep networks that can effectively encode and decode information.
Overall, our discussion has shed light on the relationship between neural codes, oscillatory networks, autoencoders, and their applications in brain signal processing. These topics collectively contribute to advancing our understanding of information processing in the brain and provide insights into potential avenues for further research and development.
1 / 1
Neuromorphic Computing based on Oscillatory Neural Networks
Aida Todri Sanial (1)
1 SmartIES - Smart Integrated Electronic Systems
Summary 
Neuro-inspired computing employs technologies that enable brain-inspired computing hardware for more efficient and adaptive intelligent systems. Mimicking the human brain and nervous system, these computing architectures are excellent candidates for solving complex and large-scale associative learning problems. In this talk, we will showcase a novel and alternative neuromorphic computing paradigm where information is encoded in the phase of coupled oscillating neurons or oscillatory neural networks. The VO2 metal-insulator transition devices emulate biological neurons and 2D MoS2 memristors emulate synapses. Current research efforts on neuromorphic computing are mainly focused on the various hardware implementation of synapses using memristors such as resistive random access memory (RRAM) technology, spin-based and ferroelectrics. But, several challenges hinder current memristive neuromorphic hardware from going mainstream due to latency (delay in accessing the device and charging/discharging wires), and density of synapse matrix allowed by the manufacturing process. In the framework of EU H2020 NeurONN project, we implement and explore energy-efficient neuromorphic computing based on oscillatory neural networks (ONN) [1] using metal-insulator-transition (MIT) for emulating "neurons" and 2D material memristors for emulating " synapses" to achieve a truly neuro-inspired computing paradigm for enabling AI at the edge [2-4]. In ONN, the principle of operations is "oscillation phase" rather than the charge-time dependent switching to emulate biological neurons. By describing neurons as oscillators, one can limit their oscillation amplitudes to small values ​​to limit power consumption, as interconnects would only vary their voltage swings to a limited range. On the other hand, more conventional Spiking Neural Networks (SNN) or non-spiking Artificial Neural Networks (ANN), must use full voltage swings to communicate and compute. Therefore, ONNs have an unprecedented efficient way of drastically reducing power consumption for the same system complexity. A comparison of the computing in "time" vs "phase" and the architecture with energy efficient components of NeurONN is illustrated in Figure 1. In this talk, several ONN aspects will be covered from devices, architecture design to training algorithms in order to highlight the potential and novelty of ONN for energy efficient brain-inspired computing
1 / 1
ChatGPT
ChatGPT
In this summary, the focus is on neuromorphic computing based on oscillatory neural networks (ONN) as a novel and alternative paradigm for brain-inspired computing. The talk emphasizes the encoding of information in the phase of coupled oscillating neurons, using devices such as VO2 metal-insulator transition and 2D MoS2 memristors to emulate biological neurons and synapses, respectively.
The current challenges in memristive neuromorphic hardware, such as latency and density limitations, are highlighted. The NeurONN project, within the EU H2020 framework, aims to address these challenges by exploring energy-efficient neuromorphic computing using ONN. The principle of operation in ONN is based on oscillation phase rather than charge-time dependent switching, which allows for reduced power consumption. Comparisons are made between ONN and conventional Spiking Neural Networks (SNN) or non-spiking Artificial Neural Networks (ANN), highlighting the efficiency of ONN in reducing power consumption.
The talk covers various aspects of ONN, including devices, architecture design, and training algorithms, with a focus on their potential and novelty for energy-efficient brain-inspired computing.
Overall, this talk presents an alternative approach to neuromorphic computing using oscillatory neural networks, offering potential advantages in energy efficiency and enabling AI at the edge.
1 / 1
Workshop on Artificial Intelligence 2020 
Talk Abstract Submission 
Title: Neuromorphic Computing based on Oscillatory Neural Networks 
Speaker: Aida Todri-Sanial, LIRMM, University of Montpellier, CNRS, France 
Abstract: 
Neuro-inspired computing employs technologies that enable brain-inspired computing 
hardware for more efficient and adaptive intelligent systems. Mimicking the human brain and 
nervous system, these computing architectures are excellent candidates for solving complex 
and large-scale associative learning problems. In this talk, we will showcase a novel and 
alternative neuromorphic computing paradigm where information is encoded in the phase of 
coupled oscillating neurons or oscillatory neural networks. The VO2 metal-insulator transition 
devices emulate biological neurons and 2D MoS2 memristors emulate synapses. 
Current research efforts on neuromorphic computing are mainly focused on the various 
hardware implementation of synapses using memristors such as resistive random access 
memory (RRAM) technology, spin-based and ferroelectrics. But, several challenges hinder 
current memristive neuromorphic hardware from going mainstream due to latency (delay in 
accessing the device and charging/discharging wires), and density of synapse matrix allowed 
by the manufacturing process. In the framework of EU H2020 NeurONN project, we 
implement and explore energy-efficient neuromorphic computing based on oscillatory neural 
networks (ONN) [1] using metal-insulator-transition (MIT) for emulating "neurons" and 2D 
material memristors for emulating "synapses" to achieve a truly neuro-inspired computing 
paradigm for enabling AI at the edge [2-4]. 
In ONN, the principle of operations is "oscillation phase" rather than the charge-time 
dependent switching to emulate biological neurons. By describing neurons as oscillators, one 
can limit their oscillation amplitudes to small values to limit power consumption, as 
interconnects would only vary their voltage swings to a limited range. On the other hand, 
more conventional Spiking Neural Networks (SNN) or non-spiking Artificial Neural Networks 
(ANN), must use full voltage swings to communicate and compute. Therefore, ONNs have an 
unprecedented efficient way of drastically reducing power consumption for the same system 
complexity. A comparison of the computing in "time" vs "phase" and the architecture with 
energy efficient components of NeurONN is illustrated in Figure 1. 
In this talk, several ONN aspects will be covered from devices, architecture design to training 
algorithms in order to highlight the potential and novelty of ONN for energy efficient brain- 
inspired computing. The talk is in line with the innovative themes of the Workshop on AI 2020 
on "Development of the foundation of AI".
1 / 1
ChatGPT
ChatGPT
Title: Neuromorphic Computing based on Oscillatory Neural Networks
Speaker: Aida Todri-Sanial, LIRMM, University of Montpellier, CNRS, France
Abstract:
Neuro-inspired computing offers promising technologies for efficient and adaptive intelligent systems by mimicking the human brain and nervous system. In this talk, we present a novel approach to neuromorphic computing using oscillatory neural networks (ONN), where information is encoded in the phase of coupled oscillating neurons. The VO2 metal-insulator transition devices are used to emulate biological neurons, while 2D MoS2 memristors emulate synapses.
While current research focuses on hardware implementations of synapses using memristors, such as RRAM, spin-based, and ferroelectrics, there are challenges in terms of latency and density limitations. To address these challenges, the EU H2020 NeurONN project explores energy-efficient neuromorphic computing based on ONN. By leveraging metal-insulator-transition (MIT) for emulating neurons and 2D material memristors for emulating synapses, the project aims to achieve a truly neuro-inspired computing paradigm for AI at the edge.
In ONN, the principle of operation is based on the oscillation phase of neurons, rather than charge-time dependent switching. This allows for reduced power consumption as the voltage swings of interconnects can be limited. In comparison, conventional Spiking Neural Networks (SNN) or non-spiking Artificial Neural Networks (ANN) require full voltage swings for communication and computation. Thus, ONNs offer a highly efficient approach to reducing power consumption while maintaining system complexity.
The talk will cover various aspects of ONN, including device characteristics, architecture design, and training algorithms. By highlighting the potential and novelty of ONN for energy-efficient brain-inspired computing, the presentation aligns with the innovative themes of the Workshop on AI 2020 on the "Development of the foundation of AI."
1 / 1
Figure 1. a) spiking neuron voltage amplitude used for encoding information such as in spiking 
neural networks. b) ONNs use a different paradigm by using phase to compute, rather than 
current or voltage. c) Illustration of ONN implementation with 5 oscillating neurons and 
resistive synaptic coupling, d) oscillating neuron is based on MIT VO2 devices, and e) synapse 
weights with 2D memristor (MoS2) devices. The fully-connected array uses only n(n-1)/2 
memristors compared to n2 as in other topologies such as SNN and ANN. 
References: 
[1] E. M. Izhikevich, Computing with Oscillators, Neural Networks, 2000. 
[2] E. Corti, A. Khanna, K. Niang, J. Robertson, K. Moselund, B. Gotsmann, S. Datta, S. Karg, 
Time-Delay Encoded Image Recognition in a Network of Resistively Coupled VO2 in Si 
Oscillators, Electron Device Letters, 2020. 
[3] E. Corti, B. Gotsmann, K. Moselund, A. Ionescu, J. Robertson, S. Karg, Scaled Resistively 
Coupled VO2 Oscillators for Neuromorphic Computing, Solid State Electronics, 2020. 
[4] Th. Jackson, S. Pagliarini, L. Pileggi, An Oscillatory Neural Network with Programmable 
Resistive Synapses in 28nm CMOS, IEEE ICRC, pp.118-124, 2018.
1 / 1
ChatGPT
ChatGPT
Figure 1:
a) Illustration of spiking neuron voltage amplitude used for encoding information in spiking neural networks.
b) ONNs (Oscillatory Neural Networks) use phase rather than current or voltage for computation.
c) Implementation of ONN with 5 oscillating neurons and resistive synaptic coupling.
d) Oscillating neuron based on MIT (Metal-Insulator Transition) VO2 devices.
e) Synapse weights represented by 2D memristor (MoS2) devices. The fully-connected array requires fewer memristors compared to other topologies such as SNN (Spiking Neural Networks) and ANN (Artificial Neural Networks).
References:
[1] E. M. Izhikevich, "Computing with Oscillators," Neural Networks, 2000.
[2] E. Corti et al., "Time-Delay Encoded Image Recognition in a Network of Resistively Coupled VO2 in Si Oscillators," Electron Device Letters, 2020.
[3] E. Corti et al., "Scaled Resistively Coupled VO2 Oscillators for Neuromorphic Computing," Solid State Electronics, 2020.
[4] Th. Jackson et al., "An Oscillatory Neural Network with Programmable Resistive Synapses in 28nm CMOS," IEEE ICRC, 2018.
1 / 1
BREADBOARDING WITH A 144-CORE PROCESSOR
 36 Commentsby: Mike Szczys
October 3, 2012    


At the center of that green PCB is a tiny little processor with way too many cores. It's the GA144 which was taken for a test-drive on a breadboard by [Andrew Back]. We saw a multi-core Kickstarter project last month. This will cost a lot less and get you more than twice the number of cores. But as was mentioned in the comments on that post, the drawback is the programming language. This chip's IDE uses Forth.

There is a dev board available, but [Andrew] went instead with a QFN-to-Through-Hole adapter board which he hand soldered. Once he has access to the pins the chip can be programmed with an FTDI adapter which is compatible with the 1.8V logic levels. The provided Forth IDE (arrayForth) is a Windows only program but it does run under Wine. We followed the project through to see him twiddling I/O pins. But we still have trouble thinking of applications for it. In a world of complex and inexpensive FPGA chips, what would you use this type of processor for?
1 / 1
ChatGPT
ChatGPT
The article discusses the GA144 processor, which is a small chip with a large number of cores. The author mentions that while there is a dev board available, they decided to use a QFN-to-Through-Hole adapter board to test the chip on a breadboard. The programming language for this chip is Forth, which some people may find limiting.
The author showcases their progress in accessing the pins and programming the chip using an FTDI adapter. They mention that the Forth IDE, called arrayForth, is a Windows-only program but can be run under Wine. However, the author admits to having trouble thinking of practical applications for this processor, especially when there are more complex and affordable FPGA chips available.
In summary, the article highlights the use of the GA144 processor, the challenges of programming it in Forth, and the author's curiosity about its potential applications in a world where FPGA chips offer more versatility.
1 / 1
Any progress having been made on 144 core computers?
1 / 1
ChatGPT
ChatGPT
Since the publication of the article in 2012, there have been advancements in multi-core processors, but specific developments related to 144-core computers are not widely reported. It's important to note that the GA144 processor mentioned in the article is a specialized chip with a unique architecture, and it may not have gained significant traction in mainstream computing applications.
In general, the focus in the computing industry has been on increasing the number of cores in processors to improve parallel processing capabilities and overall performance. Modern processors, particularly those used in high-performance computing and server applications, can have dozens or even hundreds of cores.
However, the practicality and scalability of extremely high-core count processors, such as those with 144 cores, can be challenging. Issues such as power consumption, thermal management, software optimization, and workload parallelization become more complex as the number of cores increases. Therefore, most commercial processors today tend to have a more moderate number of cores that strike a balance between performance and practicality.
It's worth noting that the field of specialized processors, such as GPUs (Graphics Processing Units) and accelerators for specific tasks like AI and machine learning, often employ a large number of cores or processing units to handle highly parallel workloads efficiently. These processors may not have 144 general-purpose cores but can still achieve high levels of parallelism for specific tasks.
Overall, while there have been advancements in multi-core processors, the specific concept of 144-core computers as discussed in the article has not become a mainstream trend. The computing industry continues to explore new architectures and technologies to improve performance and efficiency, but the specific core count of processors can vary depending on the target application and specific requirements.
1 / 1
Parallel Processing for Embedded Systems
parallel processors for embedded systems
Multi-computer chips from GreenArrays offer an unrivaled combination of great computing power, small size, low energy consumption, and high value. They are simple, practical, versatile, and affordable. These chips change the game, enabling new embedded applications in a massively parallel world.

We are here to help you develop products that leverage this ground-breaking technology into a commanding position in your market.

multi-computer vs. multi-core architecture

Latest developments:

As of Fall 2022, a new release of arrayForth-3 has been posted along with several documentation updates and a resurrection of the arrayForth Institute as part of this website. Ongoing activities continue such as shipment of EVB002 evaluation kits and G144A12 chips, improvements in chip design, improvement of hardware and software development tools, and production of application notes and other documentation.
One small example:


144 fully fledged computers.
Just one chip.
This gem is our GA144 multi-computer chip. It is designed to give you options that have never before existed and to place them under your control by writing software.

With 144 independent computers, it enables parallel or pipelined programming on an unprecedented scale. Map a data flow diagram or an analog block diagram onto its array of computers for continuous processes without interrupts or context switching.
With instruction times as low as 1400 picoseconds and consuming as little as 7 picojoules of energy, each of the 144 computers can do its work with unprecedented speed for a microcontroller and yet at unprecedentedly low energy cost, transitioning between running and suspended states in gate delay times. When suspended, each of the computers uses less than 100 nanowatts.
With completely programmable I/O pins, transitions between driving high, driving low, weak pull-down and high impedance can be made under software control in nanoseconds. This permits bit-banging on an unprecedented scale, yet the input impedance of each pin is higher, and circuit loading less, than many expensive FET probes. Add to that the ability to devote one or more very high-speed computers to each pin, and you have true software-defined I/O with unprecedented attentiveness to each pin when it is desired.
With multilevel programming you may choose between very fast, very simple programs in microcode, or working in a high level language using external memory, or various methods in between, trading off speed against complexity; and you may mix these techniques in an application, building your own clusters of computers or nodes to produce architectures with "hosts" and "coprocessors" as your application demands.
When your product requirements include responsiveness and computational power but with energy limitations, this superb chip is ideal. Use it right out of the box for prototyping and development; as your end product matures, GreenArrays will rapidly configure custom chips to further minimize both the cost and energy consumption of your product.

Learn more

It's part of the package:

arrayForth® Developer Tools
Our software development tools are available free of charge to our customers. Collectively called arrayForth, they are the foundation of our proprietary CAD system used for chip design. It includes assembler/compiler, example source code including all ROM on each chip, a full software-level simulator for each chip, and an Interactive Development Environment for use with real chips. arrayForth will always be the gold-standard software for working with our chips, so we feel we owe it to our customers to give it to them. Free.

Engineering Support
It can be lonely at the front of the pack. But we will stand beside you throughout the development process — to support our product and to see your project through to a successful deployment.

Complete Documentation
We share what we know — and not just about the hardware and the development system. We provide enough information that you can even implement your own development tools, if you wish. That's fine with us. (But we think you'll find our tool set very convincing, it's built around the same tools we use to create these chips!)

Online Courseware
To supplement our written documentation, the arrayForth Institute provides free online learning experiences.

Online Customer Support
Customer Support Central provides notifications of up-to-date documentation and software, as well as links to direct, personal online support for our customers.
1 / 1
ChatGPT
ChatGPT
Parallel processing in embedded systems refers to the utilization of multiple processing units or cores to perform computations simultaneously. It offers several advantages such as increased performance, improved throughput, and enhanced efficiency in handling computationally intensive tasks. In the context of embedded systems, which are designed to perform specific functions with constrained resources, parallel processing can be highly beneficial.
One notable approach to parallel processing in embedded systems is the use of multi-computer chips like the GA144 from GreenArrays. These chips integrate multiple independent computers onto a single chip, providing a massively parallel processing capability. The GA144 chip offers features such as low energy consumption, small size, and high value, making it suitable for various embedded applications.
The GA144 chip stands out with its 144 fully fledged computers, each capable of executing instructions with extremely low times and consuming minimal energy. These independent computers can operate in parallel or pipelined fashion, allowing for continuous processes without interruptions. With programmable I/O pins, the chip enables precise control over input/output operations, enabling unprecedented scalability in bit-banging and software-defined I/O.
The multi-computer architecture of the GA144 chip empowers developers to leverage parallel programming techniques and trade off speed for complexity based on their application requirements. The chip supports both microcode and high-level language programming, allowing for flexible development approaches. It is particularly suitable for products that demand both responsiveness and computational power while adhering to energy limitations.
GreenArrays provides a comprehensive package for developing applications with their multi-computer chips. This includes the arrayForth® Developer Tools, which comprise an assembler/compiler, example source code, software-level simulators, and an Interactive Development Environment. The company also offers engineering support throughout the development process, complete documentation, online courseware, and customer support services.
Overall, parallel processing in embedded systems, especially with multi-computer chips like the GA144, opens up new possibilities for achieving high-performance and energy-efficient computing in a compact form factor. It enables developers to tackle complex computational tasks and create innovative embedded applications with enhanced capabilities.
1 / 1
Flow Computing
Escape the Box - Rethink Computer Science
MenuSkip to content
Search for:
Talking Oscillations
A flow path can consist of multiple rails only one of which asserts "data" per wavefront. The simplest such relation is a "one of" relation. For instance, given three rails only one rail will become red("data"), then it will transition to blue("not data"), and all rails will be blue then one rail will transition to red, then transition to blue and so on. There is one completeness condition with exactly one rail red and another completeness condition with all rails blue. The two completeness relations can be appreciated with a "one of" combination behavior. The detection of each completeness relation  generates the single rail closure path.

Oscillations can now transmit information. With a three rail flow path one of three different meanings can propagate along the flow path each oscillation. This is illustrated in the movie. The leftmost oscillation has a producer that sets a different rail to red each oscillation: a "one of" three condition that is propagated through the flow path from oscillation to oscillation.

Video Player

00:00
00:04

The oscillations are flowing successive wavefronts of different data values through their flow paths each data value separated by "not data" wavefront. The "data" completeness relation for a flow path can be arbitrarily complex with a following transition to all "not data". The flow path of each oscillation below has two groups of rails with a 1 of 4 relation and two groups of rails with a 1 of 2 relation. The flow path can encode and flow 64 different meanings. The "data" completeness is the transition of one rail from each group to "data". The "not data" completeness is all rails at "not data". The "one of" behaviors recognize both completeness relations and collect both into transitioning the single rail closure path that closes each oscillation.

Oscillations can talk to each other in an arbitrarily complex language. A flow path of 32 groups of two rails each can represent and transmit  32 bit 2s complement numbers
1 / 1
ChatGPT
ChatGPT
Flow computing is a paradigm that reimagines computer science by utilizing oscillations to transmit information through flow paths. In this approach, a flow path consists of multiple rails, with only one rail asserting "data" per wavefront. The concept of completeness is crucial, with conditions where exactly one rail is red and conditions where all rails are blue.
By leveraging oscillations, different meanings can propagate along the flow path with each oscillation. This is achieved by setting a different rail to red in each oscillation, creating a "one of" three condition that propagates through the flow path. The oscillations flow successive wavefronts of different data values separated by "not data" wavefronts.
The flow path's completeness relations can be arbitrarily complex, transitioning from a "data" state to an "not data" state. Each oscillation has groups of rails with different "one of" relations, allowing for encoding and flow of various meanings. The completeness of the "data" state involves transitioning one rail from each group to "data," while the completeness of the "not data" state requires all rails to be at "not data." By recognizing both completeness relations, the oscillations can close their single rail closure paths.
Flow computing enables oscillations to communicate with each other using an intricately complex language. For example, a flow path with 32 groups of two rails each can represent and transmit 32-bit 2s complement numbers. This approach offers a novel perspective on computing and opens up possibilities for designing systems with rich communication capabilities based on oscillatory flow paths.
1 / 1
Flow Logic
Flow logic consists of two assertable conditions "data" and "not data" and two combination behaviors "all of" and "one of". In each table D is "data", N is "not data", "-" is no transition.


For "one of" combination behaviors:

"one of" the inputs transitioned to "data" is "data" completeness/fulfillment and the output transitions to "data"
"all of" the inputs transitioned to "not data" is "not data" completeness and the output transitions to "not data"
For "all of" combination behaviors:

"all of" inputs transited to "data" is "data" completeness/fulfillment and the output transitions to "data"
"all of" the inputs transitioned to "not data" is "not data" completeness and the output transitions to "not data"
For all combination behaviors:

The asserted output is maintained (remembered) between meeting completeness criteria
1 / 1
ChatGPT
ChatGPT
Flow logic is a logic system that encompasses two assertable conditions: "data" and "not data." It also includes two combination behaviors: "all of" and "one of." In flow logic, these conditions and behaviors are used to determine the state transitions of the outputs based on the inputs.
In the case of "one of" combination behaviors, if any of the inputs transition to the "data" state, it is considered "data" completeness, and the output transitions to the "data" state. Conversely, if all inputs transition to the "not data" state, it is considered "not data" completeness, and the output transitions to the "not data" state.
Similarly, in "all of" combination behaviors, if all inputs transition to the "data" state, it is considered "data" completeness, and the output transitions to the "data" state. On the other hand, if all inputs transition to the "not data" state, it is considered "not data" completeness, and the output transitions to the "not data" state.
In both combination behaviors, the output state is maintained (remembered) once the completeness criteria are met. This means that once the output transitions to a certain state, it remains in that state until the completeness conditions change.
Flow logic provides a framework for determining the state transitions of outputs based on the input conditions and combination behaviors, allowing for flexible and dynamic behavior in logic systems.
1 / 1
version, is the closure path that closes each oscillation around its link behaviors. Below that the pipeline of oscillations is redrawn to emphasize the flow path which is the straight path through the middle and the closure paths which close each oscillation.



The movie below illustrates the behavior of the redrawn structure of linked oscillations

Video Player

00:00
00:01

The demonstration below is a series of linked oscillations forming a pipeline through which transitioning colors flow.  The pipeline has 3 variable speed oscillations: one at the input, one at the output and one in the middle of the pipeline. Each oscillator oscillates between red and blue through a single point of color inversion. Oscillators are linked through a completeness behavior with the completeness logic red & red and blue & blue. When both inputs to the linking behavior are the same color the color passes through the behavior and each oscillation progresses. Play with the speeds and get familiar with the adaptive behavior of the logically determined pipeline. Notice how each oscillation waits indefinitely on linking completeness to complete its oscillation. Green is start and red is stop.



This a pipeline of linked oscillations in the form of the redrawn structure above making it easier to observe the flow path. The big red dots are the oscillation links and the little black dots are the flowing  transitions. The sliders left, mid and right control delays in the left most, right most and middle oscillations respectively. The slider hide is a toggle that will hide the closure flow and show only the data path flow through the pipeline. Play with the delays and observe how the delays affect the flow path and how they interact with each other.
1 / 1
ChatGPT
ChatGPT
The description provided refers to a demonstration of a series of linked oscillations forming a pipeline where transitioning colors flow. The pipeline consists of three variable speed oscillations, with one at the input, one at the output, and one in the middle. Each oscillator alternates between red and blue through a single point of color inversion.
The oscillators are linked together using a completeness behavior that checks for the same color in the input signals (red & red and blue & blue). When both inputs to the linking behavior have the same color, the color passes through the behavior, and each oscillation progresses.
The demonstration allows for adjusting the speeds of the oscillations using sliders for left, mid, and right delays. The delays affect the flow path and how the oscillations interact with each other. There is also a toggle slider to hide the closure flow and only show the data path flow through the pipeline.
By experimenting with the delays and observing the behavior, one can explore how the delays affect the flow path and how they interact with each other in the pipeline of linked oscillations.
Overall, this demonstration showcases the behavior of the linked oscillations and the flow path in the pipeline, providing insights into how the logically determined pipeline adapts to different conditions and delays.
1 / 1
Do you understand it enough to make some matplotlib illustrations in Python jupyter?
1 / 1
ChatGPT
ChatGPT
Yes, I can certainly help you create matplotlib illustrations in a Python Jupyter notebook. Please provide me with the specific details of the illustrations you would like to create based on the description you provided, and I will do my best to assist you.
1 / 1
This a pipeline of linked oscillations in the form of the redrawn structure above making it easier to observe the flow path. The big red dots are the oscillation links and the little black dots are the flowing  transitions. The sliders left, mid and right control delays in the left most, right most and middle oscillations respectively. The slider hide is a toggle that will hide the closure flow and show only the data path flow through the pipeline. Play with the delays and observe how the delays affect the flow path and how they interact with each other.
1 / 1
ChatGPT
ChatGPT
To create a matplotlib illustration in Python Jupyter notebook based on the description you provided, we can use interactive widgets from the ipywidgets library to create sliders and a toggle button. Here's an example code that demonstrates the concept:
python
Copy code
import
numpy
as
np
import
matplotlib.pyplot
as
plt
from
ipywidgets
import
interact, widgets
# Define the pipeline of linked oscillations
pipeline_length =
10
pipeline_data = np.zeros((pipeline_length,
2
))
pipeline_links = np.zeros((pipeline_length -
1
,
2
))
# Function to update the pipeline based on slider values
def
update_pipeline
(
left_delay, mid_delay, right_delay, hide_closure
):
# Update the pipeline data based on slider values
pipeline_data[
0
,
0
] = left_delay
    pipeline_data[-
1
,
1
] = right_delay
    pipeline_data[
int
(pipeline_length/
2
),
0
] = mid_delay
# Update the pipeline links based on completeness conditions
for
i
in
range
(pipeline_length -
1
):
if
pipeline_data[i,
0
] == pipeline_data[i,
1
]:
            pipeline_links[i] =
1
else
:
            pipeline_links[i] =
0
# Plot the pipeline
plt.figure(figsize=(
8
,
4
))
    plt.scatter(
range
(pipeline_length), np.zeros(pipeline_length), c=
'r'
, s=
200
)
    plt.scatter(
range
(pipeline_length-
1
), np.zeros(pipeline_length-
1
), c=
'k'
, s=
50
)
if
not
hide_closure:
for
i
in
range
(pipeline_length -
1
):
if
pipeline_links[i] ==
1
:
                plt.plot([i, i+
1
], [
0
,
0
],
'k-'
)
    plt.xlim(-
1
, pipeline_length)
    plt.ylim(-
0.1
,
0.1
)
    plt.xlabel(
'Pipeline Oscillation'
)
    plt.title(
'Linked Oscillations Pipeline'
)
    plt.show()
# Create sliders and toggle button
left_delay_slider = widgets.FloatSlider(
min
=
0
,
max
=
1
, step=
0.01
, description=
'Left Delay:'
)
mid_delay_slider = widgets.FloatSlider(
min
=
0
,
max
=
1
, step=
0.01
, description=
'Mid Delay:'
)
right_delay_slider = widgets.FloatSlider(
min
=
0
,
max
=
1
, step=
0.01
, description=
'Right Delay:'
)
hide_closure_toggle = widgets.ToggleButton(value=
False
, description=
'Hide Closure'
)
# Define the interactive function
@interact(
left_delay=left_delay_slider, mid_delay=mid_delay_slider, right_delay=right_delay_slider, hide_closure=hide_closure_toggle
)
def
update_plot
(
left_delay, mid_delay, right_delay, hide_closure
):
    update_pipeline(left_delay, mid_delay, right_delay, hide_closure)
# Initialize the plot
update_pipeline(
0
,
0
,
0
,
False
)
When you run this code in a Jupyter notebook, it will display the interactive plot showing the pipeline of linked oscillations. You can use the sliders to adjust the delays for the left, middle, and right oscillations, and use the toggle button to hide or show the closure flow. Observing the changes in the plot, you can see how the delays affect the flow path and how they interact with each other.
Feel free to modify the code as needed to match your specific requirements and aesthetics.
1 / 1
In Julia Pluto
1 / 1
ChatGPT
ChatGPT
Certainly! Here's an example code using Julia and the Pluto notebook to create an interactive visualization of the pipeline of linked oscillations:
julia
Copy code
using Plots
using Interact

# Define the pipeline of linked oscillations
pipeline_length = 10
pipeline_data = zeros(pipeline_length, 2)
pipeline_links = zeros(pipeline_length - 1, 2)

# Function to update the pipeline based on slider values
function update_pipeline(left_delay, mid_delay, right_delay, hide_closure)
    # Update the pipeline data based on slider values
    pipeline_data[1, 1] = left_delay
    pipeline_data[end, 2] = right_delay
    pipeline_data[div(pipeline_length, 2), 1] = mid_delay

    # Update the pipeline links based on completeness conditions
    for i in 1:pipeline_length - 1
        if pipeline_data[i, 1] == pipeline_data[i, 2]
            pipeline_links[i] = 1
        else
            pipeline_links[i] = 0
        end
    end

    # Plot the pipeline
    plot(legend = false, xticks = 0:pipeline_length-1, xlims = (-1, pipeline_length), ylims = (-0.1, 0.1),
         xlabel = "Pipeline Oscillation", title = "Linked Oscillations Pipeline") do
        scatter!(1:pipeline_length, zeros(pipeline_length), markercolor = :red, markersize = 8)
        scatter!(1:pipeline_length-1, zeros(pipeline_length-1), markercolor = :black, markersize = 4)
        if !hide_closure
            for i in 1:pipeline_length-1
                if pipeline_links[i] == 1
                    plot!([i, i+1], [0, 0], color = :black)
                end
            end
        end
    end
end

# Define the sliders and toggle switch
left_delay_slider = slider(0:0.01:1, value = 0, label = "Left Delay:")
mid_delay_slider = slider(0:0.01:1, value = 0, label = "Mid Delay:")
right_delay_slider = slider(0:0.01:1, value = 0, label = "Right Delay:")
hide_closure_toggle = toggle(false, label = "Hide Closure")

# Create the interactive visualization
@manipulate for left_delay = left_delay_slider, mid_delay = mid_delay_slider, right_delay = right_delay_slider,
                hide_closure = hide_closure_toggle
    update_pipeline(left_delay, mid_delay, right_delay, hide_closure)
end
To use this code, you need to have the
Plots
and
Interact
packages installed in Julia. You can install them by running
] add Plots Interact
in the Julia REPL. Then, in a Pluto notebook, create a new code cell and paste the code into it. When you run the cell, it will display an interactive plot showing the pipeline of linked oscillations. You can adjust the sliders to change the delays for the left, middle, and right oscillations, and use the toggle switch to hide or show the closure flow.
Feel free to modify the code according to your specific requirements and design preferences.
1 / 1
In Jupyter Python illustrations of rings with sub rings illustrating information flow in ncl
1 / 1
ChatGPT
ChatGPT
Certainly! Here's an example code using Python in Jupyter Notebook to create illustrations of rings with subrings to illustrate information flow:
python
Copy code
import
matplotlib.pyplot
as
plt
# Function to plot a ring with subrings
def
plot_ring_with_subrings
(
n, subrings, title
):
# Calculate angles for the main ring and subrings
angles = [i *
360
/ n
for
i
in
range
(n)]
    subring_angles = [
360
/ subrings] * subrings
# Create a figure and axes
fig, ax = plt.subplots(figsize=(
6
,
6
))
    ax.set_aspect(
'equal'
)
    ax.set_xlim(-
1.1
,
1.1
)
    ax.set_ylim(-
1.1
,
1.1
)
    ax.axis(
'off'
)
    ax.set_title(title)
# Plot the main ring
for
angle
in
angles:
        x = [
0
,
0.9
* cosd(angle)]
        y = [
0
,
0.9
* sind(angle)]
        ax.plot(x, y,
'k'
, linewidth=
2
)
# Plot the subrings
for
i
in
range
(n):
for
angle
in
angles:
for
subring_angle
in
subring_angles:
                x = [
0.9
* cosd(angle),
0.9
* cosd(angle + subring_angle)]
                y = [
0.9
* sind(angle),
0.9
* sind(angle + subring_angle)]
                ax.plot(x, y,
'k'
, linewidth=
1
)
# Plot the connections between main ring and subrings
for
angle
in
angles:
for
i
in
range
(n):
            x = [
0.9
* cosd(angle),
0.9
* cosd(angle + subring_angles[i])]
            y = [
0.9
* sind(angle),
0.9
* sind(angle + subring_angles[i])]
            ax.plot(x, y,
'r'
, linewidth=
2
)
# Plot the nodes in the main ring
for
angle
in
angles:
        x =
0.9
* cosd(angle)
        y =
0.9
* sind(angle)
        ax.plot(x, y,
'ko'
, markersize=
10
)
# Plot the nodes in the subrings
for
angle
in
angles:
for
subring_angle
in
subring_angles:
            x =
0.9
* cosd(angle + subring_angle)
            y =
0.9
* sind(angle + subring_angle)
            ax.plot(x, y,
'ko'
, markersize=
6
)

    plt.show()
# Function to calculate cosine of an angle in degrees
def
cosd
(
angle
):
return
math.cos(math.radians(angle))
# Function to calculate sine of an angle in degrees
def
sind
(
angle
):
return
math.sin(math.radians(angle))
# Example usage
plot_ring_with_subrings(n=
6
, subrings=
3
, title=
"Ring with Subrings (n=6, subrings=3)"
)
In this example, the
plot_ring_with_subrings
function takes two parameters:
n
(number of nodes in the main ring) and
subrings
(number of subrings per node). The function creates a Matplotlib figure and axes, plots the main ring, subrings, and connections between them, and finally plots the nodes in the main ring and subrings.
You can modify the
n
and
subrings
parameters to create different configurations of rings with subrings. You can also customize the appearance of the plot by modifying the plotting functions and parameters according to your needs.
Feel free to adjust
1 / 1
Calculating Oscillations
Flow path data can combine  through completeness behaviors and calculate. The movie shows a two rail flow path representing a binary value and a three rail flow path representing a trinary value combining through a completeness behavior that adds the two values resulting in a four rail quaternary sum.

Video Player

00:00
00:09

The completeness behavior is derived from a transition table (truth table, function table, ...) defining the output value for all possible combinations of input values.

Trinary	
0	1	2	
binary	0	0	1	2	quaternary
1	1	2	3
Any table can be implemented as an completeness behavior performing calculation as data flows through the flow paths.

Oscillations, linked in arbitrarily complex structures, spontaneously oscillating can perform arbitrarily complex computations purely in terms of logical relationships: no time referent (clock), no explicit control (state machine), no special memory (registers), just animate logical structure
1 / 1
ChatGPT
ChatGPT
To illustrate the concept of calculating oscillations and flow path data combining through completeness behaviors, you can use the following code in Jupyter Notebook:
python
Copy code
import
matplotlib.pyplot
as
plt
import
numpy
as
np
def
plot_oscillation_calculation
():
# Define the binary and trinary values
binary_values = [
0
,
1
]
    trinary_values = [
0
,
1
,
2
]
# Define the completeness behavior transition table
transition_table = np.array([[
0
,
0
,
0
,
0
], [
0
,
0
,
1
,
1
], [
0
,
1
,
1
,
2
]])
# Create a figure and axes
fig, ax = plt.subplots(figsize=(
8
,
4
))
    ax.set_aspect(
'equal'
)
    ax.set_xlim(
0
,
6
)
    ax.set_ylim(
0
,
4
)
    ax.axis(
'off'
)
# Plot the binary flow path
for
i, binary_value
in
enumerate
(binary_values):
        ax.text(i +
0.5
,
3.5
,
str
(binary_value), ha=
'center'
, va=
'center'
, fontsize=
12
)
        ax.arrow(i,
3
,
1
,
0
, width=
0.1
, color=
'black'
, head_width=
0.3
, head_length=
0.2
)
# Plot the trinary flow path
for
i, trinary_value
in
enumerate
(trinary_values):
        ax.text(i +
0.5
,
1.5
,
str
(trinary_value), ha=
'center'
, va=
'center'
, fontsize=
12
)
        ax.arrow(i,
1
,
1
,
0
, width=
0.1
, color=
'black'
, head_width=
0.3
, head_length=
0.2
)
# Plot the completeness behavior calculation
for
i, binary_value
in
enumerate
(binary_values):
for
j, trinary_value
in
enumerate
(trinary_values):
            quaternary_value = transition_table[trinary_value][binary_value]
            ax.text(i +
0.5
,
0.5
,
str
(quaternary_value), ha=
'center'
, va=
'center'
, fontsize=
12
)
            ax.arrow(i,
0
,
1
,
0
, width=
0.1
, color=
'black'
, head_width=
0.3
, head_length=
0.2
)
    
    plt.show()
# Example usage
plot_oscillation_calculation()
This code creates a visualization of oscillations and flow paths representing binary and trinary values. The completeness behavior is represented by a transition table that defines the output value based on the combinations of input values. The code plots the flow paths for binary values, trinary values, and the resulting quaternary sum calculated through the completeness behavior.
You can customize the values, transition table, and appearance of the plot according to your needs. This example provides a basic visualization of oscillation calculations, and you can expand upon it to create more complex computations and behaviors.
1 / 1
For most of its history, computer architecture has been able to benef i t from a rapid 
scaling in semiconductor technology, resulting in continuous improvements to CPU design. 
During that period, synchronous logic has dominated because of its inherent ease of design and 
abundant tools. However, with the scaling of semiconductor processes into deep sub-micron 
and then to nano-scale dimensions, computer architecture is hitting a number of roadblocks 
such as high power and increased process variability. 
Asynchronous techniques can potentially offer many advantages compared to con- 
ventional synchronous design, including average case vs. worse case performance, robust- 
ness in the face of process and operating point variability and the ready availability of high 
performance, i ne grained pipeline architectures. Of the many alternative approaches to asyn- 
chronousdesign, NullConventionLogic(NCL)hastheadvantagethatitsquasidelay-insensitive 
behavior makes it relatively easy to set up complex circuits without the need for exhaustive 
timing analysis. 
This thesis examines the characteristics of an NCL based asynchronous RISC-V CPU 
and analyses the problems with applying NCL to CPU design. While a number of university 
and industry groups have previously developed small 8-bit microprocessor architectures using 
NCL techniques, it is still unclear whether these offer any real advantages over conventional 
synchronous design. A key objective of this work has been to analyse the impact of larger 
word widths and more complex architectures on NCL CPU implementations. The research 
commenced by re-evaluating existing techniques for implementing NCL on programmable de- 
vices such as FPGAs. The little work that has been undertaken previously on FPGA implemen- 
tations of asynchronous logic has been inconclusive and seems to indicate that asynchronous 
systems cannot be easily implemented in these devices. However, most of this work related to 
an alternative technique called bundled data, which is not well suited to FPGA implementation 
because of the diff i culty in controlling and matching delays in a "bundle" of signals. On the 
other hand, this thesis clearly shows that such applications are not only possible with NCL, but 
vi 
there are some distinct advantages in being able to prototype complex asynchronous systems 
in a i eld-programmable technology such as the FPGA. 
A large part of the value of NCL derives from its architectural level behavior, inher- 
ent pipelining, and optimization opportunities such as the merging of register and combina- 
tional logic functions. In this work, a number of NCL multiplier architectures have been an- 
alyzed to reveal the performance trade-offs between various non-pipelined, 1D and 2D orga- 
nizations. Two-dimensional pipelining can easily be applied to regular architectures such as 
array multipliers in a way that is both high performance and area-eff i cient. It was found that
1 / 1
ChatGPT
ChatGPT
the use of Null Convention Logic (NCL) in asynchronous CPU design has the potential to overcome some of the limitations of synchronous design, such as high power and process variability. NCL's quasi-delay-insensitive behavior makes it easier to implement complex circuits without extensive timing analysis.
This thesis focuses on examining an NCL-based asynchronous RISC-V CPU and analyzing the challenges associated with applying NCL to CPU design. While smaller 8-bit microprocessor architectures using NCL have been developed before, it remains unclear whether they offer significant advantages over synchronous design. The objective of this work is to assess the impact of larger word widths and more complex architectures on NCL CPU implementations.
The research begins by re-evaluating existing techniques for implementing NCL on programmable devices like FPGAs. Previous studies on FPGA implementations of asynchronous logic have been inconclusive, but this thesis demonstrates that NCL applications can be successfully prototyped on FPGAs. The architectural-level behavior of NCL, its inherent pipelining, and optimization opportunities, such as merging register and combinational logic functions, contribute to its value.
The thesis also explores the performance trade-offs between different non-pipelined and pipelined NCL multiplier architectures. Two-dimensional pipelining, specifically applied to regular architectures like array multipliers, proves to be high performance and area-efficient.
Overall, this research sheds light on the feasibility and advantages of using NCL in larger and more complex asynchronous CPU designs, demonstrating the potential for improved performance, reduced power consumption, and better robustness in the face of process and operating point variability.
1 / 1
pipelined 32bit x 32bit signed Baugh-Wooley multiplier with Wallace-Tree Carry Save Adders 
(CSA), which is representative of a real design used for CPUs and DSPs, was used to further 
explore this concept as it is faster and has fewer pipeline stages compared to the normal array 
multiplier using Ripple-Carry adders (RCA). It was found that 1D pipelining with ripple-carry 
chains is an eff i cient implementation option but becomes less so for larger multipliers, due to 
the completion logic for which the delay time depends largely on the number of bits involved 
in the completion network. The average-case performance of ripple-carry adders was explored 
using random input vectors and it was observed that it offers little advantage on the smaller 
multiplier blocks, but this particular timing characteristic of asynchronous design styles be- 
comes increasingly more important as word size grows. 
Finally, this research has resulted in the development of the i rst 32-Bit asynchronous 
RISC-V CPU core. Called the Redback RISC, the architecture is a structure of pipeline rings 
composed of computational oscillations linked with l ow completeness relationships. It has 
been written using NELL, a commercial description/synthesis tool that outputs standard Ver- 
ilog. The Redback has been analysed and compared to two approximately equivalent industry 
standard 32-Bit synchronous RISC-V cores (PicoRV32 and Rocket) that are already fabricated 
and used in industry. While the NCL implementation is larger than both commercial cores 
it has similar performance and lower power compared to the PicoRV32. The implementation 
results were also compared against an existing NCL design tool l ow (UNCLE), which showed 
howmuchtheresultsoftheseimplementationstrategiesdiffer. TheRedbackRISChasachieved
1 / 1
ChatGPT
ChatGPT
This research involved the development of a pipelined 32-bit x 32-bit signed Baugh-Wooley multiplier using Wallace-Tree Carry Save Adders (CSA), which is commonly used in CPUs and DSPs. The purpose was to explore the concept of pipelining in asynchronous designs and compare it to the traditional array multiplier with Ripple-Carry Adders (RCA).
The study found that 1D pipelining with ripple-carry chains is an efficient implementation option for smaller multipliers but becomes less efficient as the size of the multiplier increases. This is primarily due to the completion logic, which introduces delays depending on the number of bits involved in the completion network. The average-case performance of ripple-carry adders was examined using random input vectors, revealing that they offer little advantage for smaller multiplier blocks. However, as the word size grows, the specific timing characteristics of asynchronous design styles become increasingly important.
As a result of this research, the first 32-bit asynchronous RISC-V CPU core called Redback RISC was developed. It is structured using pipeline rings composed of computational oscillations linked by low completeness relationships. The Redback RISC was implemented using NELL, a commercial description/synthesis tool that outputs standard Verilog code. A comparison was made between the Redback RISC and two industry-standard synchronous RISC-V cores (PicoRV32 and Rocket). Although the NCL implementation was larger than the commercial cores, it exhibited similar performance and lower power consumption compared to the PicoRV32. Additionally, the implementation results were compared against an existing NCL design tool called low (UNCLE), highlighting the differences in the outcomes of these implementation strategies.
Overall, this research contributes to the understanding and evaluation of asynchronous design techniques in CPU architectures, particularly in the context of large-scale multipliers. It demonstrates the potential benefits of pipelining and asynchronous design in terms of performance, power consumption, and robustness.
1 / 1
2.1Asynchronous Technology 
Conventional synchronous design uses a centralized clock signal to control the data 
l ow through combinational logic between edge-triggered register stages. In contrast, asyn- 
chronous design relies on localized handshaking signals (e.g., Request and Acknowledge) to 
control its data l ow. Of the three types of asynchronous models in current use (Bounded De- 
lay model, QDI and NCL), NCL has the least sensitive delay constraint. In the Bounded Delay 
model (such as micro-pipelines), delays in both gates and wires are bounded and delays are 
evaluated based on worst-case scenarios to avoid signal hazard conditions. QDI is constrained 
by the isochronic fork, which assumes uniform wire delays and uniform switching thresholds 
at the inputs to the gates associated with the forking branches. The latter is often diff i cult to 
achieve in the face of PVT variability. NCL is constrained by orphan paths, nets that do not 
contribute to the output and are therefore not part of the completion network, and which are 
therefore considerably easier to manage and/or remove than the isochronic fork. 
122. Background and Literature Review 
Null Convention Logic is based on threshold logic gates exhibiting internal state hold- 
ing in each gate except, of course, for the TH1x set of gates. It is a symbolically complete logic 
system that exhibits delay insensitive behaviour within some minor constraints. Unlike other 
DI techniques, NCL gates rely on one simple timing assumption: that the feedback path for its 
state holding elements must be faster than the forward delay through the gate. This limitation 
is usually straightforward to overcome. 
Input completeness requires that the output signal of the gate may not transition to 
DATA until all inputs have transitioned to DATA. Similarly, the output will not transition to 
NULL until all inputs have reached NULL. Input completeness is achieved by adding the value 
NULL to the basic logic values (TRUE and FALSE), to represent a status of "no data". Output 
Data will only be valid when all input signals have transitioned from NULL to DATA. An 
NCL circuit consists of an interconnection of primitive modules known as M-of-N threshold 
gates with hysteresis. All functional blocks, including both combinational logic and storage 
elements, are constructed out of these same primitives. To represent these three values in this 
work we use a dual rail coding style which expresses the value '00' (NULL), '01' FALSE and 
'10' TRUE. A NULL wave front separates two DATA wave fronts. 
Ack- 
input 
Ack- 
output 
DI 
RegisterDual-Rail 
Logic Ack- 
input 
Ack- 
output 
DI 
Register 
Ack- 
input 
Ack- 
output 
DI 
RegisterDual-Rail 
Logic Ack- 
input 
Ack- 
output 
DI 
Register 
Dual Rail 
Input 
Dual Rail 
Output 
AcknowledgeAcknowledge 
Acknowledge Acknowledge 
Input 
Acknowledge 
Output Figure 1: Delay Insensitive Model 
NullDataNullData 
Ack 
Dual Rail Data 
+ Req Figure 2: Delay Insensitive Protocol 
Figure 1 shows the data l ow of the delay insensitive model. The Request signals are 
embedded within the dual-rail data input and the circuit has separate Acknowledge signals.
1 / 1
ChatGPT
ChatGPT
In conventional synchronous design, a centralized clock signal is used to control the flow of data through combinational logic between edge-triggered register stages. Asynchronous design, on the other hand, relies on localized handshaking signals (such as Request and Acknowledge) to control the data flow. Among the three types of asynchronous models (Bounded Delay model, QDI, and NCL), NCL has the least sensitive delay constraint.
NCL, or Null Convention Logic, is a delay-insensitive logic system based on threshold logic gates with internal state holding. It exhibits delay-insensitive behavior within certain constraints. NCL gates have a simple timing assumption: the feedback path for state holding must be faster than the forward delay through the gate. This assumption is typically easy to meet.
Input completeness in NCL requires that the output signal of a gate cannot transition to "DATA" until all inputs have transitioned to "DATA". Similarly, the output will not transition to "NULL" until all inputs have reached "NULL". Input completeness is achieved by adding the value "NULL" to the basic logic values of "TRUE" and "FALSE". Output data is valid only when all input signals have transitioned from "NULL" to "DATA".
NCL circuits consist of interconnected primitive modules called M-of-N threshold gates with hysteresis. All functional blocks, including combinational logic and storage elements, are constructed using these primitives. In this work, a dual-rail coding style is used to represent the three values: '00' for NULL, '01' for FALSE, and '10' for TRUE. A NULL wavefront separates two DATA wavefronts.
Figure 1 depicts the data flow of the delay-insensitive model, where Request signals are embedded within the dual-rail data input, and separate Acknowledge signals are used. Figure 2 shows the delay-insensitive protocol, illustrating the flow of Null and Data signals, along with the use of Acknowledge signals.
1 / 1
See if you can make illustrations or examples in Jupyter
1 / 1
ChatGPT
ChatGPT
Certainly! I can provide you with some code examples in Python using Jupyter Notebook. Please let me know specifically what type of illustrations or examples you would like to see related to the topic of asynchronous design or NCL. For example, if you have a specific circuit or logic gate in mind, or if you would like to see a step-by-step simulation of an asynchronous system. Providing more specific details will help me create relevant illustrations for you.
1 / 1
Like as if there was water flowing through pipes
1 / 1
ChatGPT
ChatGPT
Certainly! Here's an example of a simple illustration in Jupyter Notebook using matplotlib library to visualize water flowing through pipes:
python
Copy code
import
matplotlib.pyplot
as
plt
# Define pipe coordinates
pipe_coords = [(
0
,
0
), (
2
,
0
), (
2
,
3
), (
4
,
3
), (
4
,
0
), (
6
,
0
)]
# Define pipe connections
pipe_connections = [(
0
,
1
), (
1
,
2
), (
2
,
3
), (
3
,
4
), (
4
,
5
)]
# Define pipe flow rates
flow_rates = [
0.5
,
0.7
,
0.3
,
0.8
,
0.6
]
# Plotting the pipes
fig, ax = plt.subplots()
for
i, (start, end)
in
enumerate
(pipe_connections):
    ax.plot([pipe_coords[start][
0
], pipe_coords[end][
0
]], [pipe_coords[start][
1
], pipe_coords[end][
1
]],
'k-'
, lw=
2
)
    ax.text((pipe_coords[start][
0
] + pipe_coords[end][
0
]) /
2
, (pipe_coords[start][
1
] + pipe_coords[end][
1
]) /
2
,
f'Flow rate:
{flow_rates[i]}
'
, ha=
'center'
, va=
'center'
)
# Plotting the pipe labels
for
i, (x, y)
in
enumerate
(pipe_coords):
    ax.text(x, y,
f'Pipe
{i}
'
, ha=
'center'
, va=
'center'
)
# Setting the axis limits and labels
ax.set_xlim(-
1
,
7
)
ax.set_ylim(-
1
,
4
)
ax.set_xlabel(
'X'
)
ax.set_ylabel(
'Y'
)
# Display the plot
plt.show()
This code will create a simple diagram illustrating pipes with different flow rates. Each pipe is represented by a line segment connecting two points, and the flow rate is displayed as a text label in the middle of each pipe. You can customize the coordinates, connections, and flow rates to suit your specific scenario.
Please note that this is a basic example, and you can further enhance it by adding colors, gradients, or animations to depict the flowing water more realistically.
1 / 1
A final summary.
2 / 2
ChatGPT
ChatGPT
In this conversation, we discussed various topics related to neural networks, neuromorphic computing, oscillatory neural networks, and parallel processing. Here's a summary of the key points:
Neural Networks: We explored the concept of neural networks, which are computing systems inspired by the human brain and nervous system. They are used to solve complex problems and enable adaptive intelligent systems.
Neuromorphic Computing: Neuromorphic computing refers to the development of brain-inspired computing hardware for efficient and adaptive intelligent systems. It aims to mimic the structure and functionality of the human brain, enabling new possibilities for solving large-scale associative learning problems.
Oscillatory Neural Networks: Oscillatory neural networks (ONNs) are a novel approach to neuromorphic computing where information is encoded in the phase of coupled oscillating neurons. ONNs use oscillations instead of charge-based switching, leading to energy-efficient computation and reduced power consumption.
Parallel Processing: We discussed the concept of parallel processing, which involves the simultaneous execution of multiple tasks or operations. Parallel processing can significantly improve computational speed and efficiency, making it suitable for complex problems that require massive computational power.
Flow Computing: Flow computing refers to the propagation of information through flow paths in oscillatory systems. By utilizing oscillations and completeness behaviors, information can flow through interconnected oscillations, enabling complex computations without the need for explicit timing or control mechanisms.
Asynchronous Computing: Asynchronous computing is an alternative to conventional synchronous design and relies on localized handshaking signals rather than a centralized clock signal. Asynchronous techniques offer advantages such as average-case performance, robustness in the face of process variability, and high-performance pipeline architectures.
Null Convention Logic (NCL): NCL is an asynchronous logic system that exhibits delay-insensitive behavior, making it relatively easy to design complex circuits without exhaustive timing analysis. NCL gates rely on a simple timing assumption and can be implemented using a dual-rail coding style.
Illustrations: We provided examples of illustrations using matplotlib in Jupyter Notebook to visualize concepts such as water flowing through pipes and interconnected components in a circuit.
Overall, these topics shed light on the advancements in neural networks, neuromorphic computing, and asynchronous computing, showcasing the potential for more efficient and adaptive intelligent systems in the future.
1 / 1
Any thing else you would like to add?
1 / 1
ChatGPT
ChatGPT
In addition to the topics discussed, it's worth noting that the fields of neural networks, neuromorphic computing, and parallel processing are rapidly evolving. Researchers and engineers are continuously exploring new techniques, algorithms, and hardware architectures to improve the efficiency, scalability, and performance of intelligent systems.
One notable area of interest is the integration of neuromorphic computing with other emerging technologies such as quantum computing and edge computing. These integrations have the potential to further enhance the capabilities of intelligent systems and enable new applications in areas like quantum machine learning and edge AI.
Furthermore, the development of more user-friendly tools, frameworks, and programming languages for designing and implementing neural networks and neuromorphic systems is also a focus of ongoing research. This will democratize access to these technologies and empower a wider range of developers and researchers to explore their potential.
Overall, the advancements in these fields hold great promise for revolutionizing various industries, including healthcare, robotics, autonomous systems, and more. The future will likely witness even more innovative applications and breakthroughs as we continue to explore the potential of neural networks and neuromorphic computing in creating intelligent and adaptive systems.
1 / 1
ChatGPT may produce inaccurate information about people, places, or facts.
ChatGPT May 24 Version
ChatGPT



