Preguntas sobre prejuicios AI
Open sidebar
Preguntas sobre prejuicios AI
ChatGPT
4
You
Translate to Spanish: RESEARCH HIGHLIGHT
23 November 2023
Il gioco del telefono senza fili rivela i pregiudizi di ChatGPT
I grandi modelli linguistici si comportano in modo simile agli esseri umani negli esperimenti in cui si deve scegliere quali informazioni conservare o scartare.
Ginevra Chelli
Twitter Facebook Email
Read in English


Una versione da laboratorio del gioco del telefono senza fili ha permesso di studiare i bias dei sistemi di intelligenza artificiale. Credit: Image Professionals GmbH / Alamy Stock Photo.

I critici dei sistemi di intelligenza artificiale come ChatGPT fanno spesso notare che essi rischiano di ereditare dagli esseri umani pregiudizi culturali che possono influenzare il modo in cui producono e trasmettono le informazioni. Un esperimento ha ora dimostrato che questo avviene davvero.

ChatGPT appartiene alla classe dei grandi modelli linguistici (LLM), che sono sistemi di intelligenza artificiale progettati per generare testi simili a quelli umani, addestrati su enormi collezioni di testi presi da Internet. Per indagare sui pregiudizi degli LLM, il team ha applicato il metodo della "catena di trasmissione", che ha una lunga storia in psicologia ed �� essenzialmente una versione da laboratorio del gioco del telefono senza fili per bambini. In questo metodo, i partecipanti umani devono trasmettere iterativamente l���uno all���altro una storia, e i ricercatori tengono traccia di come la storia viene modificata a ogni passaggio della trasmissione. Questa metodologia �� particolarmente adatta per indagare i pregiudizi nei LLM, perch�� pu�� essere confrontata direttamente con i risultati degli esseri umani e pu�� evidenziare la presenza di sottili pregiudizi che altrimenti non verrebbero identificati.

In questo studio1, Alberto Acerbi dell'Universit�� di Trento e Joseph Stubbersfield dell'Universit�� di Winchester, nel Regno Unito, hanno chiesto a un LLM di riassumere e riformulare una storia. Poi hanno restituito il riassunto cos�� ottenuto all'intelligenza artificiale e le hanno chiesto di riassumerlo di nuovo, ripetendo l'operazione per tre volte.

Acerbi e Stubbersfield hanno utilizzato le stesse storie usate in cinque esperimenti di psicologia con partecipanti umani precedentemente pubblicati, ognuna di esse con lo scopo per evidenziare uno specifico pregiudizio. Ad esempio, una storia includeva elementi coerenti con gli stereotipi di genere (come una moglie che cucina per una cena a cui il marito ha invitato degli ospiti) insieme a elementi che li contraddicevano (la stessa moglie che esce a bere con gli amici prima della cena). Un'altra storia presentava sia elementi negativi che positivi, nonch�� elementi ambigui che potevano essere interpretati in entrambi i modi (ad esempio, un uomo che "prende la borsa di un'anziana signora", che pu�� essere letto come un gesto di aiuto o come un furto). Una terza storia includeva elementi che suggerivano una minaccia accanto ad altri rassicuranti, e cos�� via.

In tutti e cinque gli esperimenti, ChatGPT ha riprodotto gli stessi pregiudizi osservati nei partecipanti umani. Nel selezionare cosa includere nei riassunti, ha scelto di mantenere le informazioni conformi agli stereotipi di genere e di scartare quelle non conformi. Ha scelto informazioni negative piuttosto che positive e ha mostrato una tendenza a interpretare le situazioni ambigue come negative. Inoltre, ha favorito la trasmissione di contenuti legati a possibili minacce, e di contenuti sociali rispetto alle informazioni prive di aspetto sociale.

Gli autori sottolineano come, senza l'intervento dell'uomo, questi pregiudizi possano produrre effetti dannosi, amplificando tendenze umane preesistenti. I pregiudizi umani possono essere legati a caratteristiche cognitive che sono state selezionate durante l'evoluzione (ad esempio, prestare particolare attenzione alle potenziali minacce �� sicuramente utile), ma non forniscono necessariamente contenuti informativi o di valore. Di fronte a informazioni ambigue, ad esempio, i LLM possono finire per produrre sempre un risultato negativo anzich�� neutro.

"Come per molte tecnologie, dobbiamo imparare a usarle al meglio", afferma Acerbi. "Man mano che si adattano a noi, noi, come individui e come societ��, ci adattiamo a loro. La cosa pi�� importante �� sapere che questi pregiudizi esistono", conclude.

doi: https://doi.org/10.1038/d43978-023-00177-7

References
Acerbi A, Stubbersfield JM. Proc Natl Acad Sci USA. 120, 44 (2023).

From: https://www.nature.com/articles/d43978-023-00177-7
ChatGPT
ChatGPT
DESTACADO DE INVESTIGACI��N
23 de noviembre de 2023
El juego del tel��fono sin hilos revela los prejuicios de ChatGPT
Los grandes modelos ling����sticos se comportan de manera similar a los seres humanos en experimentos en los que deben elegir qu�� informaci��n conservar o descartar.
Ginevra Chelli
Twitter Facebook Email
Leer en ingl��s
Una versi��n de laboratorio del juego del tel��fono sin hilos ha permitido estudiar los sesgos de los sistemas de inteligencia artificial. Cr��dito: Image Professionals GmbH / Alamy Stock Photo.
Los cr��ticos de los sistemas de inteligencia artificial como ChatGPT a menudo se��alan que estos pueden heredar prejuicios culturales humanos que influyen en c��mo producen y transmiten informaci��n. Un experimento ha demostrado que esto es realmente as��.
ChatGPT pertenece a la clase de grandes modelos ling����sticos (LLM), que son sistemas de inteligencia artificial dise��ados para generar textos similares a los humanos, entrenados en enormes colecciones de textos de Internet. Para investigar los prejuicios de los LLM, el equipo utiliz�� el m��todo de "cadena de transmisi��n", que tiene una larga historia en psicolog��a y es esencialmente una versi��n de laboratorio del juego del tel��fono sin hilos para ni��os. En este m��todo, los participantes humanos deben transmitir una historia de uno a otro iterativamente, y los investigadores rastrean c��mo la historia cambia en cada paso de la transmisi��n. Esta metodolog��a es especialmente adecuada para investigar los prejuicios en los LLM, ya que puede compararse directamente con los resultados humanos y puede resaltar la presencia de prejuicios sutiles que de otra manera no ser��an identificados.
En este estudio1, Alberto Acerbi de la Universidad de Trento y Joseph Stubbersfield de la Universidad de Winchester, en el Reino Unido, pidieron a un LLM que resumiera y reformulara una historia. Luego devolvieron el resumen a la inteligencia artificial y le pidieron que lo resumiera nuevamente, repitiendo la operaci��n tres veces.
Acerbi y Stubbersfield utilizaron las mismas historias empleadas en cinco experimentos de psicolog��a con participantes humanos previamente publicados, cada una dise��ada para resaltar un prejuicio espec��fico. Por ejemplo, una historia inclu��a elementos consistentes con estereotipos de g��nero (como una esposa que cocina para una cena a la que su marido invit�� a hu��spedes) junto con elementos que los contradec��an (la misma esposa saliendo a beber con amigos antes de la cena). Otra historia presentaba elementos negativos y positivos, as�� como elementos ambiguos que podr��an interpretarse de ambas maneras (por ejemplo, un hombre que "toma la bolsa de una anciana", que podr��a interpretarse como un gesto de ayuda o como un robo). Una tercera historia inclu��a elementos que suger��an una amenaza junto con otros reconfortantes, y as�� sucesivamente.
En los cinco experimentos, ChatGPT reprodujo los mismos prejuicios observados en los participantes humanos. Al seleccionar qu�� incluir en los res��menes, eligi�� mantener la informaci��n que se ajustaba a los estereotipos de g��nero y descartar la que no se ajustaba. Prefiri�� la informaci��n negativa sobre la positiva y mostr�� una tendencia a interpretar las situaciones ambiguas como negativas. Adem��s, favoreci�� la transmisi��n de contenido relacionado con posibles amenazas y de contenido social sobre informaci��n sin aspecto social.
Los autores destacan que, sin la intervenci��n humana, estos prejuicios pueden producir efectos da��inos, amplificando tendencias humanas preexistentes. Los prejuicios humanos pueden estar vinculados a caracter��sticas cognitivas seleccionadas durante la evoluci��n (por ejemplo, prestar especial atenci��n a posibles amenazas es ciertamente ��til), pero no necesariamente proporcionan contenido informativo o valioso. Ante informaci��n ambigua, por ejemplo, los LLM pueden terminar produciendo siempre un resultado negativo en lugar de uno neutro.
"Como con muchas tecnolog��as, debemos aprender a usarlas de la mejor manera", afirma Acerbi. "A medida que se adaptan a nosotros, nosotros, como individuos y como sociedad, nos adaptamos a ellos. Lo m��s importante es saber que existen estos prejuicios", concluye.
doi:
https://doi.org/10.1038/d43978-023-00177-7
Referencias
Acerbi A, Stubbersfield JM. Proc Natl Acad Sci USA. 120, 44 (2023).
De:
https://www.nature.com/articles/d43978-023-00177-7
You
A summary.
ChatGPT
ChatGPT
El art��culo del 23 de noviembre de 2023, destaca un estudio sobre los prejuicios de sistemas de inteligencia artificial como ChatGPT. Los investigadores utilizaron una versi��n de laboratorio del juego del tel��fono sin hilos para examinar c��mo los grandes modelos ling����sticos (LLM) procesan y transmiten informaci��n, en comparaci��n con los humanos. El estudio, realizado por Alberto Acerbi de la Universidad de Trento y Joseph Stubbersfield de la Universidad de Winchester, involucr�� a ChatGPT en la reiteraci��n de historias dise��adas para resaltar prejuicios espec��ficos. ChatGPT demostr�� prejuicios similares a los humanos, como la preferencia por informaci��n que confirma estereotipos de g��nero y la tendencia a interpretar de manera negativa situaciones ambiguas. Los autores resaltan la necesidad de ser conscientes de estos prejuicios para minimizar sus efectos potencialmente da��inos.
ChatGPT can make mistakes. Consider checking important information.
?
ChatGPT





