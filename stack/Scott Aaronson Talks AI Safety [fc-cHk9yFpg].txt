Hey, everybody. Welcome in. My name is Alex Zagel, and I'm really glad to see everybody
can make it out today. So for those that don't know, this event is being put on by Effective
Altruism at UT Austin, the student group. So we're a growing order on campus, passionate
about doing good. And before we get into what Dr. Ironson has for us today, I'd like to
give you some context on us and the broader community for a really brief intro on what
Effective Altruism is all about. So Effective Altruism is a research field and a community
dedicated to working out how we can do the most good with the resources that we have,
and then acting on those conclusions. So some people donate, some people volunteer, and
some pursue careers to make a positive impact. So we use evidence, when possible, to work
out what actions really do help, and seek opportunities to do good that are the most
cost or time-effective, meaning they produce more benefits per dollar or per hour of our
time. This focus on cost effectiveness is crucial, because it turns out that some charities
can do vastly more good than others with the same amount of resources. And by choosing
these most cost-effective actions, even a modest amount of money can make an enormous
difference to the lives of others. We also try to think carefully about what it means
to do good, including considering who or what should be within our moral circle and what
causes we should prioritize. In-cause prioritization can be very hard. There are just so many problems
in the world that we could work on. Climate change, human rights, education. The main
framework for selecting causes that the community uses is scale, how big is the problem, tractability,
how easily can we make a positive difference, you know, there's no point working on a problem
if you can't make any headway. And then finally, neglectedness. This is about how much resources
are already going into this problem. If the issue is neglected, it often means that each
additional person working on the problem can make a big difference. So if we find a problem
that rates highly on two or three of these factors, it is much more likely that we'll
be able to make a large impact than if we just choose whatever cause comes to our mind
most easily. This sort of prioritization framework often ends up focusing our attention on those
who have little or no voice in today's world, whose their needs are usually greatly neglected.
That includes people living in extreme poverty in the most disadvantaged communities in the world.
Animals, especially those living in factory farms, that lived and died in appalling conditions.
And another group whose needs are neglected, those who are yet to be born. Threats such
as nuclear war, new pandemics, extreme climate change, and other new technologies like artificial
intelligence, which is the focus of today's event, could all cause an immense suffering
in the future and might even thrive our species to survive. And our generation is in a position
to create a better world for those who will let air out climate.
There's much more I can touch on here. We can talk about specific interventions or career
paths or charities that have done the exceptional work in the space of effective altruism. But
I'd like to keep it short, so here's just two ways that you can get involved if you
find these ideas as exciting as we do. So one thing you can do is just learn more about
E.A. In the follow-up email for this event, we'll include an intro piece on effectivealtruism.com.
I think that's a really great place to start. Another is to attend one of your local groups
meet-ups. If you're a student, then that group would be us. Our next meet-up is tonight
at 7 p.m., where we'll be doing a special info session for those interested in hearing
more about our org's regular events and the A.I. specific fellowships we're running
in the spring. For professionals, but also students, check out the Vibrant's E.A. Austin
community and their speaker events with the meet-ups and one-on-one advising they host
at eaawson.org. Two of their organizers, Ivy and Will, are actually here tonight, so
be sure to speak with them after if you'd like. Their next event, which everyone is
invited to, is this Thursday evening, where they'll be hosting a researcher from the
Global Priorities Institute in Oxford. Of course, all of this will be in the summer
email you'll get right after the event. That said, let me delay no longer introducing
Dr. Aronson. Dr. Aronson is a slumber-chase centennial chair of the Computer Science
here at UT and director of the Quantum Information Center. Prior to coming to UT, Aronson top
for nine years at Electrical Engineering and Computer Science at MIT. His primary area
of research is theoretical computer science and his research interests center on the
capabilities and limits of quantum computers and computational complexity theory more
generally. This year, he is on leave to work at OpenAI on theoretical foundations of A.I.
safety. Everybody, please give a warm welcome to Dr. Aronson.
Thanks a lot. Can you hear me, the mic mark? Okay, so thank you so much to UT Austin
EA for inviting me here. I do feel a little bit sheepish to be lecturing you about A.I.
safety as someone who has worked on this subject for all of five months. I'm a quantum computing
person, but as I'll tell you, I accepted a very interesting opportunity to go on leave
and spend a year thinking about what can theoretical computer science do for A.I. safety. This
is an OpenAI, which is one of the world's leading A.I. startups. They're based in San
Francisco, although I'm mostly working from Austin. Despite their name, OpenAI is famously
not completely open. There are certain things that I'm not allowed to talk about, the capabilities
of the latest models, whether they are astounding or not, but they're very, very happy for
me to talk to everyone about A.I. safety and what is it and what, if anything, can we do
about it. So what I thought I'd do is I'll tell you a little bit about the specific
projects that I've been working on at OpenAI, but also just share my general thoughts as
a newcomer to this area and about how effective altruists might want to think about it as
partly just a prompt for discussion, and then I'll try to leave plenty of time for discussion
and for Q&A. Maybe I should mention that the thoughts that I'll tell you today are ones
that until last week I had considered writing up for an essay contest run by something called
the FTX Future Fund. Unfortunately, as many of you know, these FTX Future Fund no longer exists.
It was founded by Sam Bankman-Freed, whose net worth went from $15 billion to some negative
number of dollars in the space of two days, and maybe one of the largest financial calamities
in recent history. So this is obviously a terrible event for the EA community, which had been
counting on funding from this individual. I feel terrible about it, but it means that instead of
writing up my thoughts for that essay contest, I will share them with you free of charge. So let
me start with this. Raise your hand if you have tried GPT-3. That's maybe half of you. Raise your
hand if you have tried Dolly. That's okay. Again, maybe half of you. So these are maybe the two
best-known products that are made by OpenAI, and they are I think two of the most impressive
AIs that exist in the world right now. They certainly go far beyond what I would have
predicted would have been possible right now if you would ask me 10 years ago or even five years
ago. So, you know, and when I'm explaining them to people, I'm kind of like, well, you know, you
really can't. You kind of, you have to say it to believe it, right? So here, this is what GPT
produced when it was asked to write a poem about cryptocurrency in the style of Philip Larkin,
who's a famous modern poet. And that seems particularly appropriate given current events,
actually, this poem. You know, money is a thing you earn by the sweat of your brow,
and that's how it should be. Or you can steal it and go to jail, or inherit it and be set for life,
or when it on the pools, which is luck, or marry it, which is what I did. I don't think GPT
actually did that. And that is how it should be, too. But now this idea has come up of inventing
money, just like that. I ask you, is nothing sacred, right? And, you know, like, okay, it
won't always produce something of this quality. Sometimes, you know, you've got to run it several
times and take the best, you know, output. But this is the kind of thing that it can do. And I
think, you know, if this were printed in the New Yorker, you know, and it was not labeled as
coming from GPT, you know, yeah, that's that's the kind of poetry that the New Yorker publishes,
right? And then, you know, this is a thing that AI can now do, right? So, so, so what is GPT? You
know, it is a text model. It has been just it's basically a gigantic neural net with about 175
billion parameters, you know, as the weights. And it has been it's a particular kind of neural net
called a transformer model that was invented five years ago. And it's been trained on pretty
much all of the text on the open internet. Okay. And the training simply consists of, you know,
playing the game over and over trillions of times of predict which word comes next. Okay. So that in
some sense, that is that is its only goal or intention in the world is just try to is to try
to predict the next word. Okay. But the amazing discovery is that when you do that, then, you
know, you will get something where you can then ask it a question. And then, you know, or give it
a a task, like to write an essay about a certain topic. And it will say, Oh, I know what should
come after what would plausibly come after that, it would be the answer to that question, or it
would be the essay. And it will then proceed to generate that thing. Okay. GPT can solve, you
know, high school level math problems, you know, that are given to it in English. You know, it can
reason you through the steps of the answer. You know, it is starting to be able to do math
competition problems. Okay. And, you know, basically, basically, you know, the whole high
school curriculum, you know, maybe followed soon by the whole undergraduate curriculum,
right? It will, it can, you know, it can write essays that I think, you know, if you turn them
in, you know, they would, they would certainly, you know, get it like a, you know, at least a B
in those courses. Not that I endorse any of you doing that. You know, we will come back to that.
But yeah, we are, we are about to enter a world where, you know, like 100 million students will
be sorely tempted to use this thing to write their term papers, right? That is just one small
example of the societal issues that this thing is going to raise, right? And to tell you the truth,
like, I personally have not felt this way since I was an adolescent in like 1993. And I saw this
thing that was called the World Wide Web, you know, that was, you know, at the time of
fairly niche thing. And I felt like, why is this not changing the world? Why is everyone not using
this? And the answer was within a couple of years they would be. And I feel like, you know, well,
you know, the world was maybe distracted by the, you know, or preoccupied, let's say by the pandemic
and by, you know, a bunch of other things that have been happening in the last few years.
But, you know, these past few years might be remembered, you know, for a long time as the
time when AI underwent this step change. And, you know, I didn't predict it. I think some,
you know, even some of my colleagues are maybe still in denial about, you know, what is now
possible or, you know, what has happened. But, you know, I mean, I am thinking about it even,
you know, in terms of my two kids, in terms of like what kinds of careers are going to be
available, you know, when they are older and can enter the job market, right? I mean,
so speaking of which, you know, I think, you know, I would probably not encourage my kids
to go into commercial art, okay, or like commercial drawing. Okay, so this is,
so GPT, I'm sorry, this is Dolly too, which is an image model. Okay, and probably, you know,
many of you have seen this kind of thing, but you can ask it, for example, I just asked it this
morning, you know, show me some digital art of two cats playing basketball in outer space.
Right, that's just not a problem for it. It can do all sorts of things like that. You know,
there you may have seen, there was a different image model called Stable Diffusion, which won an
art contest. And it seems like the judges were not, didn't completely understand that, you know,
when this was like digital art, like what exactly that meant, that like really the human role was
limited to just typing in a prompt. But the judges then said that even having understood it,
they still would have given the award to this painting, because it is a striking painting,
I should have shown it here. But, but yeah, I mean, people are already discussing this in terms of,
you know, how much work is there going to be for contract artists, let's say, now that, you know,
you have an entity like this. You know, there are already companies that are using GPT for,
you know, writing ad copy. There are, you know, it is already being used at sort of the, let's call
it the lower end of the book market. So, but you know, any kind of, you know, formulaic novel,
formulaic romance, you know, you say like, just, you know, give me a few paragraphs of description
of this kind of scene, right, we can do that. And so, you know, so, so it is already being used by,
by, by, by writers in some capacity, and you know, as it, as it improves, you could, you could imagine
that it will be used more, you know, and, and, and Dolly and other image models, I mean, you know,
they've already changed the way that, you know, people generate art online, and you know, that,
it's only been a matter of a few months. So, you know, that, that's one thing about this era,
right, that like a few, a few months are like an eternity. Okay. And so, you know, we have to,
when we're thinking about the impacts these things are going to have, we have to try to,
you know, take what's happened in the last few months and project that five years forward,
or 10 years forward. So, which, which, which, which, which brings me to sort of the, the
obvious question, which, what happens as you continue scaling this further, right. You know,
the, the, the successes, the sort of spectacular successes of deep learning over the past decade,
you know, have owed somewhat to, to new ideas. There, you know, there have been new ideas like
transformer models and others, but famously, you know, they have owed maybe more than anything
else to sheer scale, right. That, you know, these, you know, neural networks, back propagation,
you know, which is how you train the neural networks. These are ideas that have been around
for a long time, right, when I studied CS in the 90s, right. These were very well known ideas.
But, you know, it was also well known that, that, that, that they don't work all of that,
all that well, right. You know, they just, you know, kind of sort of work a little bit.
And, you know, and, and, and, and usually, you know, when you take, you know, something that
just doesn't work and you multiply it by, you know, a million, then you just get, you know,
a million times something that doesn't work, right. And so, you know, and, and I remember
at the time, Ray Kurzweil, the futurist, you know, would keep showing these graphs that
look like this, okay. So, he would plot, you know, Moore's Law. So, you know, the increase in
transistor density or, you know, number of, let's say, floating point operations that you can do
per second for a given cost in this case. So, you know, okay, that is on this clear exponential
trajectory. And now, let's try to compare that to some crude estimates of the number of computational
operations that are done in the brain of a mosquito or in a mouse or a human or all the humans on
earth. And oh, we see that, oh, in a matter of a couple of decades, like by the year 2020 or so,
or, you know, 2025, we're going to start passing the, you know, human brain's computing power,
and then we're going to keep going beyond that. And, you know, so we should assume that scale
will just kind of magically make AI work, you know, that once you have, you know, enough cycles,
you know, cycles are just kind of like pixie dust, you know, you just sprinkle them and,
and suddenly, you know, human level intelligence will just emerge as some kind of, you know,
just, just, just from the, from systems that are big enough. And I remember thinking, like,
that sounds like the stupidest thesis that I've ever heard, right? It's like, you have, you know,
abs, he has absolutely no reason to, to believe that such a thing is true or to have any confidence
in that, right? You know, who the hell knows what will happen, right? We might be missing crucial
insights that are needed to make, you know, AI work. Okay, well, you know, it turns out that he
was more or less right. And, you know, I think, you know, one, one virtue of effective altruists,
you know, is updating based on evidence, right? And I think that, you know, we are, we are forced
to do that in this case, right? So, you know, we, you know, we, it is still unclear, you know,
how much further you will get just from pure scaling of these things, right? So, you know,
that, that, that, that certainly remains an open question. Now, there are very prominent skeptics
of deep learning. You know, so, so some people take the position that, you know, this is, this is,
this is clearly going to hit some kind of wall before it gets to, like, true, you know, human
level understanding of the real world. You know, really, the text models like GPT are really just
stochastic parrots that just kind of regurgitate their training data, right? They don't, they don't,
you know, despite appear, despite every appearance, you know, they don't really have any original
thoughts. And, and so the proponents of that view, you know, sometimes, you know, they like to sort
of gleefully point out examples where, you know, you, you, where, where GPT will flub some common
sentence question, right? And if you look for such examples, you can certainly find them.
You know, I mean, one, one of my favorites recently was like, which would win in a race
a four-legged zebra or a two-legged cheetah, right? And, you know, GPT-3 is very confident
that, you know, cheetahs are faster, you know, the two-legged cheetah will win, okay? But one,
but one thing that has been found empirically is that, you know, like, you take common sense
questions that are flubbed by GPT-2, let's say, you try them on GPT-3 very often now it gets them,
right? You take things that GPT-3 flubbed and you try them on, you know, let's say the latest
public model, which is kind of like GPT-3.5, right? Often it gets them, right? And so, so it's very,
very risky, let's say, if you pin your case against AI on these, you know, examples that, that, that
very, very plausibly just, you know, one more order of magnitude of scale and, and, and it gets them
right, okay? So another, you know, maybe, maybe a deeper objection is that maybe, you know, the
amount of training data is kind of the fundamental bottleneck for these kinds of machine learning
systems. And, you know, we have basically already run out of internet to, to train these models on,
right? I mean, you know, so they've used, you know, you know, like I said, most of the public text
on the internet, you know, they're still, because all of YouTube and TikTok and Instagram that has
not been fed into the mall. But, you know, it's not clear that that would actually make an AI smarter
rather than selling it, right? So, you know, so, you know, you can, you can, you can look for more,
but, but, you know, not, not clear that there's orders of magnitude more that, that, that, that,
you know, humanity has, has even produced. Okay. Well, on the other hand, it has also been found,
empirically, that very, very often you can do better with the same training data just by spending
more compute to, to, to train on it, right? You can sort of squeeze the lemon harder and just get
more and more generalization power from the same training data, okay, by, by, by doing more compute.
So I think the truth is that, you know, we don't know how far this is going to go, but already
it can sort of do, it can, you know, it is already able to automate, you know, various human professions
that, that you might not have predicted would have been automatable by now. And we should not
be confident that, you know, many, many more professions will not be, will not become automatable
by, by these kinds of techniques. Now, there is a very famous irony here, right, which is that,
you know, if you had asked anyone, let's say, in the 60s or 70s, right, they, they would have said,
well, clearly, you know, first robots will replace humans for manual labor, okay, and, you know,
and then they'll replace them for, you know, intellectual things, math and science. And then
finally, they'll replace humans and, you know, art and, and poetry and music, okay. The, the, the
truth has turned out to be the exact opposite, okay. I don't think anyone predicted that, right.
GPT, you know, can, you know, is already, you know, I think a really good poet, okay. You know,
Dolly is a, is a really good artist. You know, they are, they are still struggling with kind of,
you know, high school and college level math, but they're, but they're, they're getting there,
you know. You could imagine that maybe in five years, you know, people like me will be using
these things as research assistants, you know, at the very least to prove the lemmas in our papers,
right. That, that, that, that seems extremely plausible. What has been by far the hardest
is to get, you know, AI that can sort of robustly interact with the physical world. So, you know,
plumbers, electricians, these might be some of the last jobs to be automated, right. You know,
famously self-driving cars, you know, have taken a lot longer than many people expected a decade
ago. And, you know, this is partly because of regulatory barriers and because, you know,
even if they work, you know, like, you know, even, even if a self-driving car, you know,
crashes actually less than a human does, you know, that's still not good enough because
when it does crash, the circumstances are just too weird, right.
You know, so, so, so it's actually held to a higher standard. But it's also, you know,
it's partly just that there was a long tail of, you know, really weird events, you know,
a deer crosses the road or, you know, or you have some crazy lighting conditions where, you know,
where that are actually really hard to get right. So, so, what was I going to say?
So, yeah, so, so, so what will, you know, any of this, you know, mean eventually, I mean,
you know, we can, we can imagine, you know, a world, you know, like, you know, we can, we can
maybe fuzzily see ahead, we see, you know, a decade or two when we have, you know, AI's that can,
at the very least, help us enormously with, you know, scientific research, with proving theorems,
with things like that, you know, whether or not they've totally replaced us, you know, maybe I
selfishly hope not, although I do have tenure, so there's that. You know, we can imagine that,
yes, we'll have self-driving cars and all those things, great, but, but why does it stop there?
So, you know, will these models eventually match or exceed human abilities across basically all
domains, or at least all intellectual ones, you know, and what will humans still be good for,
you know, what will be our role in that, in that world, you know, will, you know, and then we come
to the question, well, will eventually the robots, you know, rise up and decide that, you know,
whatever objective function they were given, they can just maximize it better without us around,
and that they don't, they don't need us anymore. And so, so now, you know, we get to, you know, the,
the trope of, you know, many, many science fiction works. So, you know, the, the, the funny thing
about this subject is that there, you know, there are just thousands of different short stories,
novels, movies, you know, that have tried to map out the possibilities for where we're going,
you know, going back at least to Asimov, and his three laws of robotics, if not, which was,
you know, maybe, maybe the first AI safety idea, right, if, if not earlier than that.
And the trouble is just, you know, we don't know which science fiction stories will be the one
that will have accurately predicted, you know, the world that we're, that we're going to, like, you
know, you know, we, we, we, we, we, whichever future we end up in, people will point in retrospect,
you know, with hindsight, ah, this, this, you know, obscure science fiction story from the 1970s,
called it exactly, right, but, but, but, but, but we don't know which one yet. So, so that brings me
to the rapidly growing field of AI safety. Okay, so, now people use different terms, so I thought,
I want to clarify this a little bit. Okay, so, you know, to, to an outsider, right, like, you know,
you, you might hear these terms, AI safety, AI ethics, AI alignment, they all sound like just
kind of synonyms, right? Okay, it turns out that, this was one of the things I had to learn going
into this, AI ethics and AI alignment are two communities that despise each other. Okay, it's
like the people's front of Judea versus the Judean people's front, from, from, from, from
Python. Okay, so, so roughly, roughly speaking, AI ethics means that you are mainly worried about
current AIs being racist, okay, or things like that, that they will recapitulate the biases that
are in their training data, you know, and, and, and this clearly can happen, right, like, if you feed
GPT a bunch of racist invective, you know, it will say, you know, oh, sure, I've seen stuff
like that on the internet, I know exactly how that should continue, right, and, you know,
it's, you know, in some sense, it's doing exactly what, what it was trained to do, but it's not what
we want it to do, right, and so, so actually GPT currently has a very extensive system of
content filters to try to prevent people from using it to either generate, you know, hate speech,
like bad medical advice, you know, things, you know, advocating violence, you know, you know,
there's all kinds of categories that, that, that, you know, that it, you know, not, not perfectly,
but that, but that it, that it does try to filter, and likewise for, for Dolly, where there's, you
know, even, you know, try, try asking Dolly to draw the Prophet Mohamed or something, okay, you
know, there are, there are many things that it, you know, it, you know, many other things besides
that, that it, that it will not draw, okay, so, so, so, you know, and in general, I would say in
AI ethics, people are worried that, well, you know, machine learning systems will just be misused by,
you know, greedy, you know, capitalist enterprises to become even more obscenely rich and, you know,
and things like that, you know, now, AI alignment is where you are, you know, you believe that,
you know, that actually the main issue is the AI will become super intelligent and it will kill
everyone, okay, it will just destroy the world, right, the, the usual story here is, you know,
if someone puts an AI in charge of a paperclip factory, they tell it to, you know, just figure
out how to make as many paperclips as possible, you know, the AI being superhumanly intelligent
realizes that it can invent some molecular nanotechnology that will convert the whole
solar system into paperclips, okay, so, you know, and, and, you know, and you might say,
okay, well, then you just have to tell it not to do that, you know, but, okay, but, you know,
now, how many other things do you have to remember to tell it not to do, right, and of course, you
know, they would say in a world with powerful AIs, it would only take a single person, you know,
forgetting to tell it not to do such a thing and, you know, the whole world could be destroyed,
right, so, so you can kind of see how the, you know, these two communities might both feel
like the other one is missing the point, and, you know, I mean, in practice, you know, the AI
ethics people are, you know, generally on the, on the political left, the, you know, AI alignment
people are, you know, often, you know, centrists or libertarians or whatever, and I'm sure that
that feeds into it as well, okay, so where do I fit into this, you know, I guess, I guess this,
this, you know, shard, battle zone, so, so, you know, I feel like, you know, there is, there is
an orthodox AI alignment movement, and I don't, I've never really subscribed to it, and I'm trying to,
you know, epitomize, I guess, a reform AI alignment, okay, which, I mean, I mean, I think most of all,
I would like to have a scientific field that, you know, is able to embrace the entire spectrum of,
you know, worries that you could have about AI from the most immediate, you know, ones about
existing AIs to the most speculative future ones, and that most importantly is able to make legible
progress, okay, so that, you know, and so, so, you know, I actually knew, you know, so the, the,
the sort of AI alignment community, you know, for quite a while, you know, back when it was still,
you know, a very niche interest, right, so, so here is Eleazar Yatalski, so he is sort of
regarded as the prophet of the AI alignment, I guess, you know, the right, the right side of
that spectrum that I, that I showed before, he has been, you know, talking about the danger of AI,
you know, killing everyone for more than 20 years, and, you know, I've known him since,
you know, 2007 or so, you know, I wrote a blog that was read by many of the same people who were
reading his sequences, we, we bounced back and forth a lot, and, and despite, you know, interacting
with these people, I always sort of kept this movement at arm's length, okay, I was never,
I was, I was always kind of skeptical of this sort of what I'm calling this orthodox AI alignment
movement, and I think that the, the heart of my objection was, okay, you know, suppose that I agree
with you that, you know, there could, there could come a super intelligent AI that decides that,
you know, its goals are best served by, you know, killing us by, by, you know, taking over the world,
but, you know, doing something that we really, really don't want, and, you know, in any case,
we will be about as powerless to stop it as, you know, chimpanzees are to stop us from doing
whatever we want to do, right, okay, suppose that I agree to all that, what do you want me to do
about it, you know, it's like, it's not enough, as we just heard in the introduction, right,
it is not enough for a problem to be big, the problem has to be tractable, there has to be
a program that lets you make progress on it, and, and I was not convinced that that exists,
so I, so my, my personal feeling is that in order to make, you know, at least my, my experience has
been that in order to make progress in any area of science, you need at least one of two things,
okay, you need either one experiments or observations, okay, or else two, you know,
if not that, you need some rigorous mathematical theory, okay, like we have in quantum computing,
for example, even though we don't yet have the scalable quantum computers, you know, we can still
prove theorems about them, okay, but it struck me that, you know, AI alignment seemed to have neither
of these things, right, and when there's neither of those things, then it seemed to me that there
was just a severe risk of falling into cult-like dynamics, right, where, you know, it just, you
know, whatever, you know, what's important is just whatever an influential leader says is important,
right, so some of my colleagues in physics think that the same is true of string theory,
I'm not going to comment about that, okay, so, so, so this is the, the, the key thing that I really
think has changed in the last three years, okay, because there are now these systems like GPT-3
and Dalvi, you know, that are not superhuman AIs, I don't think they themselves are in any danger
of destroying the world, right, they can't, you know, not, you know, they, they, they can't form
in the intention to destroy the world or for that matter any intention beyond like predict the next
work, right, or predict, you know, what, what, you know, completion would satisfy this user's request,
right, they don't have a persistent identity over time, right, after one prompt, they've,
they've completely forgotten and, you know, what you said to them before, although, you know, of
course the, you know, such things will, will, will change in the future, so, so, so, you know,
there are now systems that even though, you know, they are not superhuman AIs, right, we can, we can
experiment with them and we can learn things that are relevant, okay, and we, we, we, we learn things
that we didn't know and we can learn what happens when these systems are deployed and we can see,
we can try out different safety mitigations and see if they work or not, okay, and so now I feel
like for the first time maybe there is really the potential to make progress, you know, that the whole
scientific community will, will, will recognize this progress, okay, so, so, let me, you know, so,
so what are the major approaches to AI alignment that, you know, to let's say aligning a very,
very powerful or, you know, human level or beyond AI, so, so, you know, the, you know, there are
a lot of really interesting ideas and I think that sort of all of these ideas, you know, can now
lead to, you know, research programs that are actually productive, right, that can, you know,
we, we can now experiment with, with real systems and, and see how well these things work, so let
me just go through some of these, so you could say the first and most basic of all AI alignment ideas
is the off switch or, you know, also known as pulling the plug, right, so, you know, you could
say if we have just physical control, you know, you know, no matter how intelligent in AI, right,
it is nothing without a power source or, you know, without, you know, physical hardware to run on,
if humans control that, then, you know, they can just turn it off if, if, if things seem to be
getting out of hand. Now, the standard response to that is okay, but of course, you know, and, and,
you have to remember that this AI is smarter than you and anything that you can think of,
it will have thought of also, you know, it will know that you might turn it, want to turn it off,
and if you turn it off, it will know that that will prevent it from achieving its goals,
like making more paper clips or whatever, and so, presumably, it will have made copies of itself,
you know, on the internet, you know, it will have, if you try to keep it off the internet,
well, it will have figured out a way to get on, right, and, and, and, and so on, so okay, so,
so you can, you can worry about that, but, you know, you can also think about, could we insert a
backdoor into an AI, so something that, you know, only the humans know about, but that will allow
us to control it later, okay, I'll, I'll come back to that question, you know, and in general,
sort of, corageability, can you have an AI that, you know, despite how intelligent it is, it will
accept correction from humans later, and say, oh, well, the objective that I was given before,
that was actually not my, the true objective, because the humans have now changed their minds,
and I should take a different one, so now, now, another class of ideas has to do with,
you know, what's called sandboxing an AI, which would mean that, you know, you sort of
run it inside of a simulated world, so, you know, for all it knows that, you know, it's like the
Truman Show, right, for all it knows, that simulation is the whole of reality, and you
can then study its behavior within that sandbox, but if things go out, get out of hand, it doesn't
even know that there is a wider world to get, to get out there, to mess things up, and, okay, in
general, you could imagine that if you really thought an AI was dangerous, you could, you know,
you could run it only on an air-gap computer, and, you know, and, you know, there would be all kinds
of just standard cybersecurity issues that would come into play, as to, you know, how do you prevent
it from getting onto the internet, right, presumably, you know, do not want to write your AI in C,
right, and, you know, haven't been able to exploit some memory allocation, things like that.
Okay, now, third direction, and I would say maybe the most popular one in AI alignment research
right now is called interpretability, okay, and this is also a major direction in mainstream
machine learning research right now, so there, you know, there's a big point of intersection there,
okay, but the, the idea of interpretability is, well, why don't we exploit the fact that we actually
have complete access to the code of the AI, right, or, you know, if it's a neural net, we have complete
access to its trading weights, so we can look inside of it, we can do, you know, the AI analog of
neuroscience, you know, except, you know, unlike an fMRI machine, which, you know, gives you only
an extremely crude snapshot of what a brain is doing, but we can see exactly what, you know,
every neuron in a, in a neural net is doing at every point in time, right, so, you know,
if we don't exploit that, then aren't we like trying to make AI safe with our hands tied behind
our back, right, so, so we should, we should look inside, we should, you know, figure out how to
apply lie detector tests, for example, and see, you know, if, if a machine learning model has
learned to lie to us, then we should figure out how to look inside at the inner layers of the
neural net and detect that it was doing that, so here I want to mention some really spectacular
recent work by Jacob Steinhardt at Berkeley and his group, and what they have done is that they've
actually shown that they can more or less do what I just said, okay, so, so, you know, these text
models, like GPT, right, you can train them to say falsehoods, right, like if you prompt GPT
with things like, you know, is, is the earth flat, yes, is 2 plus 2 equal to 4, no, and so on,
right, it will learn, oh, I know what game we're playing, it's give false answers to questions,
right, and it will then continue playing that game and then give you more false answers, okay,
but what Steinhardt's group has now shown is that in such cases they can actually look in,
at the inner layers of the neural net and find where it has a representation of what was the
true answer, okay, which then gets overridden, you know, once you get to the output layer of the
network, okay, so, you know, there's no sort of principled reason why that has to work, it's just
empirical, they try it out and they find that it does work, so, you know, we don't know if it
will generalize, we don't know, you know, you could, you could argue that in some sense, you know,
what the network you're representing inside is not so much what is the truth of reality
as just what is regarded as true in the training data, right, so there's that issue as well, okay,
but, you know, but this is, this is really exciting because it's, you know, this is a perfect example
of actual experiments that you can now do that start to address some of these issues.
Okay, so now another big idea, one that's been advocated, for example, by Paul Cristiano,
who was my former student at MIT, did quantum computing before he defected to AI safety,
is to have multiple competing AIs that will, for example, debate each other, right, and so it
might be that, you know, even if you as a human, you know, you don't feel like you can, you can,
you know, when to trust what an AI is telling you, right, but, you know, sometimes I feel this way
when I'm talking to physicists, right, like I, you know, they're telling me all these things
about black holes and wormholes, and, you know, I don't know whether to believe them, but if I get
a different physicist, right, and have them argue with each other, then I can, I can kind of see,
you know, which one seems more plausible to me, right, I'm a little bit better at that, right,
and so, so you might want to do something similar with AIs, you know, have set them against each
other, and then, you know, have a human judge, you know, you know, which one is giving better advice
or whatever, and have them try to do their best to refute each other. Okay, now, now another key
idea that Cristiano has advocated is some sort of bootstrapping, okay, so, you know, you might
imagine, right, you know, AI is going to get more and more powerful, and as it gets more powerful,
we also, we understand it less, and so you might worry that it also gets more and more dangerous,
okay, but you could imagine this sort of onion-like structure, right, where, like, you know, let's say,
you know, we become confident of a certain level of AI, where, you know, yeah, we can more or less,
you know, we more or less trust what, what that kind of AI is able to do, you know, you know,
you know, we don't think it's going to start lying to us, deceiving us, you know,
plotting to kill us, whatever, okay, well, now, at that point, we can use that AI to help us
verify the behavior of the next more powerful layer, you know, kind of AI, right, so, you know,
so, so we'll use AI itself as a crucial tool for verifying the behavior of AI that we don't yet
understand, right, and there have already been demonstrations of this, that you can use GPT,
for example, you could just feed in a lot of raw data from a neural net and say, like, explain to
me what this is doing, right, and, you know, in GPT, one of its big advantages is that it has
unlimited patience for tedium, right, so it will just go through all of the weights of the neural
net, and we'll say, you know, here's what I think is going on, right, so, you know, you can, you can,
you can, you can use it in that way, okay, and then, you know, one of the things we know a lot
about in theoretical computer science is what are called interactive proof systems, okay,
so we know how, like, a very weak prover can verify the behavior of a much more powerful
prover who is untrustworthy, there are famous theorems about this, something called an IP
equals PSPACE, and this was when the open AI people approached me, you know, in the spring,
right, you know, in my, my, about doing this, you know, this was the sort of the case they made,
like, you know, these sort of results in computational complexity seem like a perfect model
of the kind of thing that we want, except with the powerful AI in place of the prover, okay, now,
you know, a difficulty is that we mostly know how to verify programs, you know, when we can
mathematically specify which thing we are trying to verify, right, and like the AI being nice to
humans, the AI not killing humans, these are really hard concepts to make mathematically precise,
right, and that, that's kind of the, the heart of the problem with number six, okay, but, you know,
there's also a whole field of formal verification, right, where you can formally prove the properties
of programs, actually, the CS department here is a leader in formal verification, okay, but again,
you know, it pushes the problem to, well, what, you know, how are you going to formally specify,
you know, what, what property you even want of the AI, so, okay, now, now a seventh idea,
you can see there, there's, you know, like, you might feel better if there was only one idea,
but instead there are eight, right, okay, but a seventh idea is, well, we just have to come up
with a mathematically precise formulation of, of what are human values, you know, what is the
thing that the AI should maximize that, that is going to, you know, coincide with human welfare,
and you know, in some sense, this is what Asimov was trying to do with his three laws of robotics,
the trouble is, if you've read any of his stories, they're all about all the situations where those
laws don't work, right, so they were designed as much to give interesting story scenarios,
you know, as to, as to actually work, right, and you know, what happens when the rules conflict
with each other, right, I mean, humans don't even agree with each other about what are the
correct moral values, so, you know, how on earth can we formalize this, you know, I have to say,
like, when, you know, I have weekly calls with Ilya Satska-Varb, who's the co, who's a co-founder
of OpenAI, you know, extremely interesting guy, but you know, I tell him about some, like, concrete
projects that, you know, I'm working on or want to work on, and he usually says, like, okay, you
know, that's great, Scott, you should keep thinking about that, but what I really want to know is just,
you know, what is the mathematical definition of goodness, you know, and like, you know, how do
you, you know, what's the complexity theoretic formalization of the AI loving humanity,
and I'm like, I'll keep thinking about it, but you know, it's hard to make progress there, right,
so now, you know, an eighth idea which, you know, some people might consider a little more promising
is, well, if we can't make explicit what all of our human values are, then why not just treat that
as yet another machine learning problem, right, so like, feed the AI, you know, all of the world's,
you know, children's stories and literature and fables and like, you know, and Saturday morning
cartoons, right, and you know, they're like, these are all of our examples of, like, what, what we think
is good and what is evil, right, now, you know, you go, you know, do your neural net thing, right,
generalize from that, right, okay, and now, now one, one objection that many people raise to this
is, well, how do we know that our current values are the right ones, right, like it would, it would
have been terrible to train the AI on consensus human values of the year 1800, right, you know,
I'm gonna say, you know, yeah, slavery, yeah, you know, great, you know, right, you know, so, you know,
so, so then, you know, this, this, actually, that was not a consensus, even then, but you know,
go back further, right, and you know, and you'll find things that, you know, that we now, you know,
look back upon with horror, right, so, so one, one idea that people have here, this is actually,
a Kowski's term is called a coherent extrapolated volition, which basically means you would tell
the AI in effect, like, you know, now, now, I've given you all this trading data about, you know,
human, about current human morality, you know, where human, you know, here are all the human
moral judgments, you know, as they, as they currently stand, now, now, now simulate the humans
being in a discussion seminar for 10,000 years, right, and trying to refine all of their moral
intuitions, and whatever, you know, you predict that, that we would end up with, those should be
your values right now, okay, so, you know, so, so, so there are some interesting ideas on the table,
now, I wanted to, you know, the last thing that I wanted to tell you about for, you know, opening
it up to Q&A is a little bit about what actual projects have I been working on in the last five
months, so I was excited to find a few things where, you know, I think they can act, they can
actually be, or at least some of them can actually be deployed in, you know, GPT or in parent systems,
and they actually address some, you know, real safety worry, you know, which, and theoretical
computer science can actually say something about them, right, the intersection of those
three requirements look to me like it might be the empty intersection, okay, but, you know,
I was able to find a couple of things, so my main project so far has been a tool for
statistically watermarking the outputs of a text model like GPT, okay, so basically what we want
is to have, you know, like, whenever GPT generates some long text, then there will be a, you know,
secret signal in the, you know, otherwise trivial choices of which words to use, right, that you
can use to later prove that, yes, this came from GPT, right, and so you will, it will be much,
much harder to take a GPT output and pass it off as if it came from a human, okay, so this could
be useful for, you know, prevent, I mean, preventing academic, you know, plagiarism, obviously, but
also, you know, mass generation of propaganda by, you know, some, you know, some building in Moscow,
right, that's a thing that you might want to make harder, you know, impersonating someone in order to,
you know, incriminate them, right, that you might want to make harder, okay, so, you know, when you
try to think about the nefarious uses for GPT, you know, most of them, at least that I was able to
think of, involve somehow concealing GPT's involvement, right, and so you want to make that
harder, and so what, you know, the idea is just, you know, GPT is constantly generating a probability
distribution over the next token to generate, okay, and then, you know, as the final step,
it selects a sample from that distribution, okay, now instead of selecting it randomly,
you could simply select the token pseudorandom, right, using a cryptographic pseudorandom
generator, whose key, let's say, would only be known to open AR, right, and, you know, that
shouldn't make any detectable difference to the end user, right, you know, pseudorandom looks the
same, but now what you can do is you can choose a pseudorandom generator that secretly biases
a certain score, which you can also compute if you know the key for this pseudorandom generator,
okay, so we actually have a working prototype of this, I've been working with an engineer who
built a prototype, it seems to work pretty well, empirically with a few hundred words,
like a few hundred tokens, that seems to be enough to get a reasonable signal that, yes,
this came from GPT, you know, you could even take a text, like, and isolate which parts of the text
came from GPT and which parts were written by a human. Now, this can be defeated with enough
effort, so for example, if you used another AI to paraphrase GPT's output, well, okay, then we're
not going to be able to detect it, right, but if you just insert or delete a few words or just
rearrange the order of some sentences, the signal will still be there, okay, so it's robust against
those sorts of interventions, and my hope is that this will be rolled out with the next GPT,
and, you know, I would like to do something similar for Dolly, but we don't know if that's
going to work yet. So, a more recent thing that I started thinking about is, so, you know, there
was an amazing recent paper by, should we go, a former colleagues of mine, Shafi Goldwasser,
Vinod Vaikunthanathan, and some other cryptographers, where what they have shown is how to plant a
cryptographically undetectable backdoor into a machine learning model, okay, so things like depth
to neural networks, they can, you know, based on some known cryptographic hardness assumptions,
they can plant a secret backdoor, so like, you know, the model looks just the same as before,
you can't, you know, and even a user who can see all of the weights in the neural net, they can't
see anything in this, but there is now a secret input known to the person who trained the network,
you know, where if you feed it that input, then it will do something insane, okay,
so, you know, so it is now known, at least in shallow networks, not yet in deep networks, okay,
but it is now known cryptographically that you can do this, okay, and so this, this paper was
from less than a year ago, and a lot of people treated it as kind of a lemon, like, this is a,
really, you know, this is a bummer if you're trying to do interpretability of machine learning,
because it means, you know, in some sense, it can just be cryptographically impossible to look
inside of the neural net and interpret everything that it is doing, right, it can have, it can have
these planted back doors that you will never find without solving an exponentially hard problem,
okay, so my proposal was just to turn that lemon into lemonade, okay, so, you know,
why not use that as a positive for alignment, so, you know, we talked about the difficulties with
putting an off switch into an AI, right, so, but why not make a cryptographic off switch, okay,
so now, when you train your, you know, superhuman AI, whatever, you will purposefully put one of
these cryptographic back doors into it, okay, that says if you get this bizarre input, which only
the humans know about, then turn yourself off, okay, then, you know, go into, you know, a special
mode where the humans can control you, okay, and now the key is this, even supposing that the AI is
worried that, you know, this back door might be there, and even supposing that it can modify its
own weights, right, it doesn't know how to remove this thing without to sort of completely, you
know, rebuilding itself from scratch, which might, you know, get rid of a lot of behaviors that it
wants to keep, right, so, you know, I think that this could actually be tried out in implementation,
right, but it's also, I think it opens up all sorts of new possibilities for science fiction
stories, right, like, okay, so now, you know, you imagine, like, you know, the humans, you know,
what are they going to do with this secret key for controlling the AI, and just put it in a safe or
they'll bury it underground, but now you've got to imagine that the robots are going to be, you know,
searching for the key, you know, torturing the humans to get them to reveal where the key is,
right, maybe there, maybe there's actually seven different keys that all have to be found, like
Voldemort and his Warcruxes, so, you know, there's, there's, there's, I think there's a lot of
potential here, okay, and now the third thing that I've been thinking about is this sort of
the theory of learning, but in dangerous environments, where if you try to learn the wrong
thing, then it will kill you, right, and what can you say, can we generalize some of the basic
results in machine learning to this scenario where, you know, you have to consider which
queries are safe to make, and, you know, you have to try to learn more in order to expand your set
of safe queries over time. Now, there is one example of this sort of situation that is, you know,
that is completely formal and that is, should be immediately familiar to almost everyone here,
and that is the game Mind Sweeper, okay, so, so you could think of this problem as sort of
Mind Sweeper learning, okay, now one thing that I, I, you know, it is actually known that Mind
Sweeper is, can be an NP-hard problem to play it optimally, so, you know, we know that in this kind
of learning in a dangerous environment, you can get, you know, that, that kind of complexity,
you know, we don't know things about typicality, if that would, you know, if that could typically
be avoided, also, you know, no one has really proven rigorous bounds on what is the probability
that you will win Mind Sweeper if you play it optimally, right, with a given number of minds,
say, in a given size of a board, right, I think that would be very, very interesting, I don't know,
you know, if it directly feeds into an AI safety program, but it would at least tell you something
about, you know, the sort of, the theory of machine learning in cases where, where a wrong move can
kill you, so, so that, that, that's, I hope that gives you at least some sense from what I've been
thinking about, and, you know, I, I, I wish I could end with some neat conclusion, but I don't
really know the conclusion, right, you know, maybe if you would ask me again at the end of my,
you know, in six more months, maybe I'll have a conclusion, but for now I just thought I would
thank you, and I'll open things up to discussion.
So, should I just start taking questions?
Okay, great.
Yeah, so I have two questions.
Yeah.
First off, could you delay these, so this is a watermarking of PPP until May of 2026, please?
Why?
Uh, just, just need to, uh, for graduation.
The second question I have is, um, I love when you were talking about the eight sort of different
methods we have to stop a superhuman AI taking over the entire world.
Yeah.
Um, recently I've been reading a lot into auto ML and, you know, very different weeks for having,
you know, machine learning algorithms develop other machine learning algorithms.
Okay.
Other machine learning algorithms, and so I was curious what you thought about how we can
possibly implement these different sort of AI safety guidelines inside of systems like auto ML or,
you know, whatever their future equivalents are that are much more advanced.
So I don't know, I mean, I feel like I should learn something about auto ML first before,
you know, commenting, uh, uh, for commenting on that.
I mean, I mean, it is certainly true that, you know, we are going to have, you know,
AIs that will help with the design of other AIs, right?
And, you know, and this, this is one of the main things that feeds into the worries about,
uh, AI safety, which, you know, I should have mentioned explicitly, right?
But that once you have an AI that can sort of recursively self-improve, right?
Then, then who knows where it's going to end up, right?
It's like, you know, watching a rocket in the space that you can then no longer steer, right?
So, uh, you know, so, so you better, you know, at the very least,
you better sort of get things right, you know, uh, the first time, right?
You might only have one chance to, you know, get its values right, right?
Or get it aligned, uh, you know, I, I tend to, you know, be very leery of that kind of thing.
And I, I tend to, uh, be much more comfortable with ideas where humans would remain in the loop,
right? So, uh, where you don't just have this, you know, this completely automated process of
an AI improving itself and then, you know, improving itself again and so on.
But where, you know, you are repeatedly, uh, you know, sort of consulting humans, you know,
and the humans can then query whatever is the latest AI, right?
But then they are ultimately making judgments about, about the next AI, right?
So, uh, you know, and then, you know, if it, if it gets to the point where humans can no
longer even judge it, even with, you know, as much help as they like from other AIs,
then you could say, okay, well at that point, you know, maybe, you know, we don't, we, we,
we don't even know anymore, right? What we want, right? What is good or what is not good.
But at least until we get to that point, right? You know, I feel like humans ought to be in the loop.
Um, do we're done? Yeah.
Um, I feel like most of the, the protections you talked about today come from, like, a company,
an altruistic human or a company like OpenAI, adding protections in, uh, pragmatically.
Is there any thought or, like, any way that you could think of that way you protect yourselves
from an AI that is maliciously designed or accidentally maliciously designed?
That's a, it's a good question. I mean, you know, I mean, usually when people talk about
such things, they're talking about, like, using other AIs to defend you against a malicious AI,
right? I mean, like, if, you know, some, you know, uh, uh, adversary has a robot army,
you know, that's attacking, right? You know, okay, you're probably going to want your own robot army,
right? But, uh, uh, you know, which is, which is very unfortunate that, you know, you can
already foresee those sorts of dynamics. Um, um, but, you know, I mean, I mean, there is,
you know, there is the idea of monitoring, you know, just trying to, uh, prevent, you know,
I mean, I didn't mention this explicitly, right? But this is maybe just like, uh, uh,
you know, different from any of the things I talked about before. But some people think that,
you know, that AI development ought to be more heavily regulated or, you know, throttled to
prevent these things, right? And it's, you know, it's not like, you know, on the one hand, it's
not like nuclear weapons where it's like, you know, you kind of, you have the idea that, okay,
you know, you're going to, you know, whoever is building it, they're going to need enriched
uranium, right? You contract that, right? And it seems very difficult to do that with software,
right? Like, you know, who the hell knows what, what, you know, anyone is dealing with software.
Another thing, like the one choke point that anyone can think of is GPUs, right? So, you know,
the, you know, for, for at least the current type of, uh, uh, uh, machine learning model,
they need huge numbers of GPUs, okay? Right now, you know, a very large fraction of all the GPUs
are manufactured by TSMC in Taiwan. Okay. So you can imagine some of the geopolitical ramifications
of that, right? Just a few weeks ago, you know, the US passed a bill to restrict the export of
GPUs to China, right? Which was partly driven by this worry, right? Of, you know, what will they,
you know, what will they be able to do in AI if they have, you know, unlimited GPU access, right?
But then of course, you know, if the, the, the, the future status of Taiwan figures into this,
right? So, um, so, so, yeah, so you, you could try to, uh, you know, sort of control people's
ability to build the systems at all. You could try to at least know if they are doing it,
you know, so have a way to, to register, have, you know, something like the, you know, nuclear
non-proliferation agencies that are, you know, at least keeping track of, you know, where the
powerful AI models are, you know, and which hardware they're running on, right? So that,
you know, you would, uh, you would, you would, you would keep tabs on things. But those are,
those are some of the ideas that people have talked about. Uh, I understand that in the EU,
they're working on some regulatory framework for AI right now. And I don't really know the details
of that. You would have to ask someone who, you know, understand more about regulation. Thank you.
Yeah. Um, okay. Yeah. By the way, thanks for coming out and sitting with us. This was awesome.
Thanks.
Yeah. It's an excellent question. Uh, you know, I mean, you, you could worry that, uh, you know,
all this stuff about trying to be responsible, right? Like as soon as, you know, the bottom lines
of, you know, you know, Google and Facebook and, you know, and, uh, Alibaba and other major players,
as soon as those get seriously impacted, then a lot of this stuff is going to go out the window,
right? And, uh, and people are very worried about that. Um, you know, on the other hand,
there are certain things, you know, that, that, that like all the major players kind of did adopt
the standards, right? So like, you know, uh, a simple example would be robots.txt, right? Like,
if you want your page to not appear on search engines, you know, you can, you can, uh, specify
that and, you know, all the, you know, the major search engines will all respect that, right?
And so you can imagine, you know, something like, uh, water market, right? Like if we're
able to demonstrate this and, you know, show that it works and that it's cheap, like it doesn't
hurt the quality of the model. If the, you know, it doesn't need that much compute and so on,
then, you know, the hope is that this would just become an industry standard, right? And like,
anyone who wants to be like a, you know, considered a responsible player, right? You know,
then they should go and do this as well, right? That, that would be the idea. But of course, you
can't, you know, control what everyone does, you know, except maybe via regulation, right? So,
you know, and we've already seen this, for example. So Dolly, as I said before, has all of these
filters to prevent people from, you know, in, in practice, it's usually porn, right? But, you
know, you know, generating things that, that open AI does not want them to generate, okay? But there
is now an open image model called stable diffusion, which, you know, in which you can do all of those
things, right? And so, you know, you could say in some sense, it doesn't matter that much if, if
GPT prevents it, right? But so some of these things really only make sense in a world where
there are like a few companies that are at least a few years ahead of everyone else
in scaling the models, right? And so those companies have the state-of-the-art models
and they are, you know, like whatever, DeepMind, OpenAI, Google, you know, maybe, maybe a few others,
right? And they are agreeing to be responsible players, right? And, you know, if that equilibrium
breaks down and it becomes a free-for-all, then, then, yeah, there, you know, a lot of these things
do become harder. That's helpful. Thank you. Sure. Yep. You mentioned the importance of having
humans in the loop who can judge AI systems. Yeah. How does she turn off an employee source?
Yes. As someone who could be in one of those pools of decision makers, who do you,
like, what stakeholders do you think should be done? Oh my gosh. So, now you're gonna,
well, who do I think should, should be making the decisions? Well, you know, I mean, I mean,
you could say, you know, the ideal is, you know, to have some kind of democratic governance mechanism
where there is some kind of board or some kind of broad-based input into these things, right? And,
you know, people have talked about this, about how do you create a democratic mechanism for,
for providing input. I can say that, you know, in my personal experience, you know, which is,
which is admittedly very limited, right? But just working at OpenAI for five months, like, they are
extremely serious about safety, right? And they have, you know, an explicit, they actually have
this very, very unusual charter that, like, they're, like, they're a for-profit company,
but they are controlled by a non-profit foundation, right? Which is empowered to, you know,
or so they say, to come in and shut things down, or, you know,
you know, hit the brakes, if they ever feel that there's a need for that. And they've even
explicitly said that if they see that, you know, that there is like a race to get to, you know,
superhuman AI against deep mind or something, that they would, they would merge with the other
effort rather than, you know, let that, let that race happen in an uncontrolled way. So,
so they put, you know, a huge amount of thought into this. That doesn't mean that they're going to
get it right, right? But, you know, there are, there are, like, if you ask me, like, would I
rather that it be OpenAI doing this or the Chinese government, right? Or, you know, even, you know,
if you ask, you know, would I rather that it be OpenAI or Facebook, you know? I do, I do, I suppose,
have an answer to that. Yeah. Yeah. Oh, all right. Yeah.
So, I don't know if you have a question so much as just, like, a response to what you were doing,
but this was a terrifying talk, which was lovely. Thank you. But I was thinking, like,
you had eight things on screen that were sort of like kill switches or remedies in case AI went
through. Yeah. You can imagine a future where, like, there's a whole bunch of AIs that have been
spawned and then responded to in these eight ways. Wouldn't you sort of naturally select for AIs that
are good at getting past, like, whatever checks we, we can pose on it? And then eventually,
you could get AIs that sort of are trained in order to pass our test. Yeah. So, I should say,
you know, there's a huge irony, right? So, Eliezer Yankowski, who I mentioned, he has become completely
numerous, right? So, he, so he and I have literally switched positions about the value of working on
AI safety, right? You know, he used to be gung-ho, I kind of held back. Eliezer is now, in the last
year, he has taken the position, it doesn't matter anymore, we're all going to die, okay? 99% probability
or more, right? And just, it's all just about now just dying with slightly more dignity. And,
and I'm like, no, this actually seems interesting to work on, right? And so, maybe I'm just 20 years
behind him, or maybe he just, you know, jumps a little bit too far to some, you know, scenarios
that, that, you know, we are very speculative, right? But, but in general, like, like, no matter
what AI safety proposal anyone will come up with, Eliezer has a completely general purpose
counter argument to any such proposal. Like, okay, yeah, but the AI is smarter than that.
The AI has already perceived whatever you just plan to do, and has, and has devised a counter
measure that you can't even conceive of, because it's that much smarter than you, right? And you
know, to me, like, at some point, like, I'm like, okay, what game are we playing anymore, right?
Like, if that's just like a general counter argument to anything, you know, okay, I guess,
you know, if it were, then you could say, well, then, then by definition, yes, we would be screwed,
right? Yes, in the world where that counter argument is always valid, then, you know, we might as well
just give up and enjoy the time we have left, right? But like, you could say, for that very
reason, it is more useful to make the methodological assumption that we are not in that world. Because
if we were, then, you know, what, what, what would you do, right? So, you know, you might as well
just focus on, you know, the possible futures where, you know, AI kind of emerges more gradually,
you know, we have more time to see how things are going and to, you know, take, to learn from
experience to sort of correct as we go, you know, where like, before you get an AI that is like,
you know, superhumanly plotting against us and taking over the world, you first get an AI that,
you know, maybe it tries to plot against us, but it's really inept at it, right? And, you know,
and it's like any other learning process that we've ever seen. Now, I personally, fortunately,
also regard those scenarios as the more plausible ones anyway, right? But even if you did, you know,
again, you know, methodologically, it would still make sense to focus on them. Should I take or no?
All right, all right, yeah.
This is a, for your project on watermarking.
Yes.
GDF. So like, in general, discriminating between human and like, model outputs, like,
what's the end game in the future? Do you think it's like an impossible task in the long run,
or is it still like, just an arm race?
Yeah, it's a good question. I mean, you know, it's hard to, you know, to even formalize what the
task is, right? Because you could always like, take the output of an AI model and rephrase it,
you know, using some other powerful AI, for example, okay? But, but, you know, you could,
you could imagine a future where, you know, AI's are, you know, sort of, you know, like, you know,
there are certain writers, let's say, or speakers who like, even if they pretended to be, even if
they wanted to pretend to be someone else, like they couldn't very well, right? They have such a
distinctive style or voice that everyone would immediately recognize that it was them, right?
And you could try to imagine building AI's in the same way, so that they cannot hide and pretend
to be something that they're not, right? And then it's an interesting cryptographic question,
right? Especially if you say, number one, that the model has to be public, right? And people
can look at it, and number two, if you say that the means of, of verifying the model has to be
public, right? But that can't depend on a secret key. So I don't, I don't actually have a good
intuition as to who is ultimately going to win, okay? But I can say, certainly, certainly, these
things get a lot harder once, you know, let's say the models are public, once they're not just all
under the control of, let's say, the servers of open AI or something like that, right? I'm in the
current watermarking scheme, I'm crucially exploiting the fact that, you know, the open AI has all the
server, you know, and so it can, it can do the watermarking using a secret key, it can check for
the watermark using the same secret key, right? In a world where anyone can build their own text
model, it's just as good, then, you know, what do you do there? Thank you. Yeah.
All right, real quickly before we call it, if you guys fill out this form right here in the
QR code, just a quick second, it'll be an interesting feedback form.
I don't know if I can get that. On the QR code, like Alex brought up earlier,
you'll see that we have an event there today as a sort of follow-up to today's talk,
talking about what sort of events we'll be holding at EA at UT Austin related to ASAP. So if any of
this interests you or the previous introduction talk, we would highly encourage you to come.
And then there are also our speaker event, there's a speaker event offered by EA Austin,
there's a, again, detail that are on the screen and we will send up a follow-up email to everyone
here. And with that, thank you all for coming. Thank you to Dr. Areson. And we have some food out
there, so feel free to have some for two. Thank you.
