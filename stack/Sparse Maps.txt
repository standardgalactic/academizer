Sparse Maps
Open sidebar
Sparse Maps
Model: GPT-4
Summarize: Monge, Bregman and Occam: Interpretable Optimal Transport in 
High-Dimensions with Feature-Sparse Maps 
Marco Cuturi1Michal Klein1Pierre Ablin1 
Abstract 
Optimal transport (OT) theory focuses, among 
all mapsT : Rd��� Rdthat 
can morph a prob- 
ability measure onto another, on those that are 
the ���thriftiest���, i.e. such that the averaged cost 
c(x,T(x))betweenxand its imageT(x)be 
as small as possible. Many computational ap- 
proaches have been proposed to estimate such 
Monge maps whencis the���2 2distance, e.g., us- 
ing entropic maps (Pooladian and Niles-Weed, 
2021), or neural networks (Makkuva et al., 2020; 
Korotin et al., 2020). We propose a new model 
for transport maps, built on a family of transla- 
tion invariant costsc(x,y) := h(x ��� y), 
where 
h := 
1 2k �� k 
2 
2+ �� and��is a regularizer. We pro- 
pose a generalization of the entropic map suitable 
forh, and highlight a surprising link tying it with 
the Bregman centroids of the divergenceDhgen- 
erated byh, and the proximal operator of��. We 
show that choosing a sparsity-inducing norm for�� 
results in maps that apply Occam���s razor to trans- 
port, in the sense that the displacement vectors 
���(x) := T(x) ��� xthey induce are sparse, with 
a sparsity pattern that varies depending onx. We 
showcase the ability of our method to estimate 
meaningful OT maps for high-dimensional single- 
cell transcription data, in the34000-dspace of 
gene counts for cells, without using dimensional- 
ity reduction, thus retaining the ability to interpret 
all displacements at the gene level. 
1. Introduction 
A fundamental task in machine learning is learning how to 
transfer observations from a source to a target probability 
measure. For such problems, optimal transport (OT) (San- 
tambrogio, 2015) has emerged as a powerful toolbox that 
can improve performance and guide theory in various set- 
tings. For instance, the computational approaches advo- 
cated in OT have been used to transfer knowledge across 
1Apple. {cuturi,michalk,p ablin}@apple.com 
datasets in domain adaptation tasks (Courty et al., 2016; 
2017), train generative models (Montavon et al., 2016; Ar- 
jovsky et al., 2017; Genevay et al., 2018; Salimans et al., 
2018), and realign datasets in natural sciences (Janati et al., 
2019; Schiebinger et al., 2019). 
High-dimensional Transport.OT i nds its most straight- 
forward and intuitive use-cases in low-dimensional geomet- 
ric domains (grids and meshes, graphs, etc...). This work 
focuses on the more challenging problem of using it on 
distributions inRd, withd ? 1. InRd, the ground cost 
c(x,y)between observationsx,yis often the���2metric or 
its square���2 2. However, when used on large-ddata samples, 
that choice is rarely meaningful. This is due to the curse-of- 
dimensionality associated with OT estimation (Dudley et al., 
1966; Weed and Bach, 2019) and the fact that the Euclidean 
distance loses its discriminative power as dimension grows. 
To mitigate this, practitioners rely on dimensionality reduc- 
tion, either in two steps, before running OT solvers, using, 
e.g., PCA, a VAE, or a sliced-Wasserstein approach (Rabin 
et al., 2012; Bonneel et al., 2015); or jointly, by estimating 
both a projection and transport, e.g., on hyperplanes (Niles- 
Weed and Rigollet, 2022; Paty and Cuturi, 2019; Lin et al., 
2020; Huang et al., 2021; Lin et al., 2021), lines (Deshpande 
et al., 2019; Kolouri et al., 2019), trees (Le et al., 2019) or 
more advanced featurizers (Salimans et al., 2018). However, 
an obvious drawback of these approaches is that transport 
maps estimated in reduced dimensions are hard to interpret 
in the original space (Muzellec and Cuturi, 2019). 
Contributions.To target highdregimes, we introduce 
a radically different approach. We use the sparsity tool- 
box (Hastie et al., 2015; Bach et al., 2012) to build OT maps 
that are, adaptively to input x, drastically simpler: 
���We introduce a generalized entropic map (Pooladian 
and Niles-Weed, 2021) for translation invariant costs 
c(x,y) := h(x ��� y), 
wherehis strongly convex. That 
entropic mapTh,��is def i ned almost everywhere (a.e.), and 
we show that it induces displacements���(x) := T(x)���x 
that can be cast as Bregman centroids, relative to the 
Bregman divergence generated by h. 
���Whenhis an elastic-type regularizer, the sum of a 
strongly-convex term���2 
2 and a sparsifying norm��, we 
show that such centroids are obtained using the proximal
1 / 1
ChatGPT
ChatGPT
The paper presents a new model for transport maps in Optimal Transport (OT) theory, a concept used in machine learning and statistics to transfer observations from a source to a target probability measure. The proposed approach is different from previous ones that typically used Euclidean distances, which often lose their discriminative power in high-dimensional data due to the curse of dimensionality.
The authors use a sparsity-inducing norm to reduce the complexity of OT maps in high-dimensional data spaces. This is achieved by constructing a cost function that incorporates a regularizer, resulting in maps that are thriftier, i.e., they minimize the average cost between an observation and its image.
The paper introduces a generalized entropic map for translation invariant costs, which results in displacements that are cast as Bregman centroids. If the cost function includes an elastic-type regularizer (a sum of a strongly-convex term and a sparsifying norm), these centroids are obtained using the proximal operator of the regularizer, leading to a sparse displacement map.
The authors argue that these sparsity-inducing maps apply Occam���s razor to transport, simplifying the transport map while preserving its essential features. This approach makes it possible to deal with high-dimensional data without resorting to dimensionality reduction techniques, which often render the maps hard to interpret.
The paper demonstrates the applicability of this method to high-dimensional single-cell transcription data, retaining interpretability at the gene level.
1 / 1
Figure 1.Plots of entropic map estimatorsTh,��, as def i ned in Prop. 4.2, to map a 2D measure supported on(xi)onto that supported on 
(yj), for various costsh. The displacements���(x) = Th,��(x) ��� xof 
unseen points are displayed as arrows. From left to right: standard 
���2 2norm, Elastic���1, STVS, andk-support costs (k = 1). For each proposed cost, the regularization��is small on the top row and high on 
the bottom. Displacements are not sparse for the���2 2cost but become increasingly so as��grows, with a support that varies with inputx. 
Note that Elastic���1and STVS tend to censor displacements as��grows, to the extent that they become null. In contrast, thek-support 
cost encourages sparsity but enforces displacements with at least k non-zero values. See also Figure 2 for aggregate results. 
operator of��. This induces sparse displacements���(x), 
with a sparsity pattern that depends onx, controlled by the 
regularization strength set for��. To our knowledge, our 
formulation is the i rst in the computational OT literature 
that can produce features-wise sparse OT maps. 
���We apply our method to single-cell transcription data us- 
ing two different sparsity-inducing proximal operators. 
We show that this approach succeeds in recovering mean- 
ingful maps in extremely high-dimension. 
Not the Usual Sparsity found in Computational OT.Let 
us emphasize that the sparsity studied in this work is un- 
related, and, in fact, orthogonal, to the many references to 
sparsity found in the computational OT literature. Such 
references arise when computing an OT plan fromntom 
points, resulting in largen �� moptimal coupling matrices. 
Such matrices are sparse when any point in the source mea- 
sure is only associated to one or a few points in the target 
measure. Such sparsity acts at the level of samples, and is 
usually a direct consequence of linear programming dual- 
ity (Peyr��e and Cuturi, 2019, Proposition 3.4). It can be also 
encouraged with regularization (Courty et al., 2016; Dessein 
et al., 2018; Blondel et al., 2018) or constraints (Liu et al., 
2022). By contrast, sparsity in this work only occurs relative 
to the features of the displacement vector���(x) ��� Rd, 
when 
moving a givenx, i.e.,k���(x)k0? d. 
Note, i nally, that we 
do not use coupling matrices in this paper. 
Links to OT Theory with Degenerate Costs.Starting 
with the seminal work by Sudakov (1979), who proved the 
existence of Monge maps for the original Monge problem, 
studying non-strongly convex costs with gradient disconti- 
nuities (Santambrogio, 2015,��3) has been behind many key 
theoretical developments (Ambrosio and Pratelli, 2003; Am- 
brosio et al., 2004; Evans and Gangbo, 1999; Trudinger and 
Wang, 2001; Carlier et al., 2010; Bianchini and Bardelloni, 
2014). While these works have few practical implications, 
because they focus on the existence of Monge maps, con- 
structed by stitching together OT maps def i ned pointwise, 
they did, however, guide our work in the sense that they 
shed light on the diff i culties that arise from ���f l at��� norms 
such as���1. This has guided our focus in this work on elastic- 
type norms, which allow controlling the amount of sparsity 
through regularization strength, by analogy with the Lasso 
tradeoff where an ���2 2loss is paired with an ���1regularizer.
1 / 1
ChatGPT
ChatGPT
The paper further details the entropic map estimator, which helps map a 2D measure supported on points (xi) onto that supported on points (yj), for different costs. The displacements are illustrated using arrows and vary based on the cost and regularization. The standard L2 norm, Elastic L1, STVS, and k-support costs are discussed, demonstrating that displacement sparsity increases with the regularization, and the sparsity pattern varies with the input.
The study distinguishes its approach to sparsity from common practices in the computational OT literature. The conventional sparsity arises when computing an OT plan, resulting in large optimal coupling matrices. In contrast, the paper's approach relates to the sparsity in the features of the displacement vector when moving a given point. Moreover, it's clarified that this work does not involve the use of coupling matrices.
The paper also draws links to OT Theory with Degenerate Costs, acknowledging key theoretical developments in studying non-strongly convex costs with gradient discontinuities. Although these works have limited practical implications, they've guided the authors in focusing on elastic-type norms that control the sparsity amount through regularization strength.
Application of this approach to single-cell transcription data has been successful in recovering meaningful maps in extremely high dimensions, showing its practical potential.
1 / 1
Background 
2.1. The Monge Problem. 
Consider a translation-invariant cost functionc(x,y) := 
h(x ��� y), whereh : Rd��� R. 
The Monge problem (1781) 
consists of i nding, among all mapsT : Rd��� Rdthat 
push-forward a measure�� ��� P(Rd)onto�� ��� P(Rd), 
the 
map which minimizes the average length (as measured by 
h) of its displacements: 
T?:= arginf 
T]��=�� Z Rd h(x ��� T(x))d��.(1) 
From Dual Potentials to Optimal Maps.Problem(1) 
is notoriously diff i cult to solve directly since the set of 
Monge, Bregman and Occam 
h(z) = 
1 2kzk 
2 
2 
h(z) = 
1 2kzk 
2 2+ 1kzk1 
h(z) = 
1 2kzk 
2 2+ 0.5��stvs(z) h(z) = 
1 2kzk 
2 2+ 10kzk 
2 
ovk 
h(z) = 
1 2kzk 
2 2+ 2kzk1 
h(z) = 
1 2kzk 
2 2+ 2��stvs(z) h(z) = 
1 2kzk 
2 2+ 100kzk 
2 
ovk 
Base measure xiTarget measure yjEntropic map distribution (Th,��(xi)) 
Figure 2.Follow-up to Figure 1, where a fresh sample of points from the base measure is transported using the various entropic map 
estimatorsTh,��that were considered. Paired with Figure 1, this plot shows the tradeoff, controlled by��, between the sparsity of displacements and the ability to recover the target measure (as �� increases, ultimately, the map no longer moves points.). An interesting 
feature of the k �� kovknorm 
resides in its ability, no matter what ��, to enforce at least one displacement (here k = 1). 
admissible mapsTis not even convex. The def i ning feature 
of OT theory to obtain an optimal push-forward solution 
T?is to cast Problem(1)as a linear optimization problem: 
relax the requirement thatxis mapped onto a single point 
T(x), to optimize instead over the space of couplings of 
��,��, namely on the set��(��,��)of probability distributions 
in P(Rd�� Rd) with marginals ��,��: 
P?:= arginf 
P�����(��,��) ZZ Rd��Rd 
cdP .(2) 
IfT?is optimal for Equation 1, then(Id,T?)]��is trivially 
an optimal coupling. To recover a mapT?from a coupling 
P?requires considering the dual to (2): 
f?,g?��� argsup 
f,g:Rd���R 
f���g���c Z Rd 
f d�� + Z Rd 
g d�� ,(3) 
where ���x,y we write (f ��� g)(x,y) := f(x) + g(y). 
Leaving aside how such couplings and dual potentials can 
be approximated from data (this will be discussed in the next 
section), suppose that we have access to an optimal dual pair 
(f?,g?). By a standard duality argument (Santambrogio, 
2015,��1.3), if a pair(x0,y0)lies in the support ofP?, 
supp(P?), the constraint for dual variables is saturated, i.e., 
f?(x0) + g?(y0) = h(x0��� y0), 
Additionally, by a so-called c-concavity argument one has: 
g?(y0) = inf 
x h(x ��� y0) ��� f?(x). 
Assumingf?is differentiable atx0, combining these two 
results yields perhaps the most pivotal result in OT theory: 
(x0,y0) ��� supp(P?) ��� ���f?(x0) ��� ���h(x0��� y0), 
(4) 
where���hdenotes the subdifferential ofh, see, e.g. (Carlier 
et al., 2010). Let h���be the convex conjugate of h, 
h���(y) := sup 
x���Rd hx,yi ��� h(x). 
Depending on h, two cases arise in the literature: 
���Ifhis differentiable everywhere, strictly convex, one has: 
���f?(x0) = ���h(x0��� y0). 
Thanks to the identity���h���= (���h)���1, 
one can uniquely 
characterize the only pointy0to whichx0is associated in 
the optimal couplingP?asx0��� ���h���(���f?(x0)). 
More 
generally one recovers therefore for any x in supp(��): 
T?(x) = x ��� ���h������ ���f?(x).(5) 
The Brenier theorem (1991) is a particular case of that 
result, which states that whenh = 1 2k �� k 
2 2, we have 
T(x) = x ��� ���f?(x0), 
since in that case���h = ���h���= 
(���h)���1= 
Id, see (Santambrogio, 2015, Theo. 1.22). 
���Ifhis ���only��� convex, then one recovers the sub- 
differential inclusiony0��� x0+ ���h���(���f?(x0))(Am- 
brosio et al., 2004)(Santambrogio, 2015, ��3). 
In summary, given an optimal dual solutionf?to Prob- 
lem(3), one can use differential (or sub-differential) calcu- 
lus to def i ne an optimal transport map, in the sense that it defines (uniquely or as a multi-valued map) where the mass of a point x should land.
1 / 1
ChatGPT
ChatGPT
This section provides the background on the Monge Problem, a key foundational concept in optimal transport theory.
The Monge Problem is about finding the map that minimizes the average length of its displacements. The problem is expressed mathematically as T* := arg inf T]��=�� ��� Rd h(x ��� T(x))d��. However, this problem is difficult to solve directly due to the non-convex set of admissible maps.
The central tenet of optimal transport theory to find an optimal push-forward solution T* is to formulate the Monge problem as a linear optimization problem. This is achieved by relaxing the requirement of mapping a single point x onto T(x) and optimizing over the space of couplings of ��, �� instead. The latter can be expressed as P* := arg inf P�����(��,��) ������ Rd��Rd c dP. If T* is optimal, then (Id, T*)]�� is an optimal coupling.
To recover a map T* from a coupling P* involves considering the dual to the Monge problem, expressed as f*, g* ��� arg sup f, g: Rd ��� R, where f ��� g ��� c ��� Rd f d�� + ��� Rd g d��.
From this, the most important result in OT theory emerges: a pair (x0, y0) belongs to the support of P* if and only if ���f*(x0) ��� ���h(x0��� y0), where ���h represents the subdifferential of h. This result is crucial for defining an optimal transport map because it specifies where the mass of a point x should land.
The Brenier theorem, a special case of this result when h = 1/2 |.|^2, is also mentioned. This theorem states that T(x) = x - ���f*(x0).
This understanding forms the base for the subsequent discussion on entropic map estimators and their applications in the paper.
1 / 1
���505 
z 
0 
2 
4 kzk1 
��stvs(z) ���505 
z 
���5 
0 
5 proxkzk1(z) 
prox��stvs(z) 
Figure 3.The STVS regularizer is not convex, but its proximal 
operator is well-def i ned and tends to shrink values less than the 
usual soft-thresholding operator. For instance, its values near 
{���5,5} are close to the identity line. 
2.2. Bregman Centroids 
We suppose in this section thathis strongly convex, in 
which case its convex conjugate is differentiable everywhere 
and gradient smooth. The generalized Bregman divergence 
(or B-function) generated byh(Telgarsky and Dasgupta, 
2012; Kiwiel, 1997) is, 
Dh(x|y) = h(x) ��� h(y) ���sup 
w������h(y)hw,x ��� yi. 
Considerafamilyofkpointsz1,...,zm��� Rdwithweights 
p1,...,pm> 0 summing to 1. A point in the set 
argmin 
z���Rd X 
j 
pjDh(z,zj), 
is called a Bregman centroid (Nielsen and Nock, 2009, Theo. 
3.2). Assuminghis differentiable at eachzj, one has that 
this point is uniquely def i ned as: 
Ch ?(zj) 
j,(p j)i? 
:= ���h? ��� ��� 
m X 
j=1 pj���h(zi) ��� ���. 
(6) 
2.3. Sparsity-Inducing Penalties 
To form relevant functions h, we will exploit the following 
sparsity-inducing functions: the���1andk��kovknorms, 
and a 
handcrafted penalty that mimics the thresholding properties 
of ���1but with less shrinkage. 
���For a vectorz ��� Rd,kzkp:= (Pd 
i=1|zi| p)1/p. We write 
���2and ���2 2for k �� k2and k �� k22respectively. 
���We write���1fork��k1. Its proximal operatorprox�����1(z) = 
ST��(z) = (1�����/|z|)+?zis 
called the soft-thresholding 
operator. 
���Schreck et al. (2015) propose the soft-thresholding opera- 
tor with vanishing shrinkage (STVS), 
��stvs(z) = ��21T 
d ? 
��(z) + 
1 
2 ��� 
1 2e ���2��(z)? ��� 0,(7) 
with��(z) := asinh ? 
z 
2�� ? 
, and where all operations are 
element-wise.��stvsis a non-convex regularizer, non- 
negative thanks to (our) addition of+1 2, to recover a non- 
negativequantitythatcancelsifandonlyifz = 0.Schreck 
et al. show that the proximity operatorprox��stvs, written 
STVSfor short, decreases the shrinkage (see Figure 3) 
observed with soft-thresholding: 
STVS��(z) = ?1 
��� ��2/|z|2? 
+ ? z.(8) 
The Hessian of��stvsis a diagonal matrix with values 
1 2|z|/ pz2 
+ ��2��� 
1 
2 and is therefore lower-bounded (with 
positive-def i nite order) by ���1 2Id. 
���LetGkbe 
the set of all subsets of sizekwithin{1,...,d}. 
Argyriou et al. (2012) introduces the k-overlap norm: 
kzkovk= min{ X 
I���Gk kvIk2|supp(vI) ��� I, X 
I���Gk vI= z}. 
For any vectorzinRd, we writez���for the vector com- 
posed with all entries ofzsorted in a decreasing order. 
Thisformula canbeevaluatedas follows toexhibit a���1/���2 
norm split between the d variables in a vector: 
kzk2 ovk= 
k���r���1 
X 
i=1 (|z|��� 
i) 
2 + 
d X 
i=k���r |z|��� 
i !2 
/(r + 1) 
where r ��� k ��� 1 is the unique integer such that 
|z|��� k���r ��� 
d X 
i=k���r |z|��� 
i < |z|��� k���r���1. 
Its proximal operator is too complex to be recalled here 
but given in (Argyriou et al., 2012, Algo. 1), running in 
O(d(logd + k)) operations. 
Note that both���1and��stvsare separable���their proximal 
operators act element-wise���but it is not the case ofk��kovk. 
3. Generalized Entropic-Bregman Maps 
Generalized Entropic Potential.Whenhis the���2 2cost, 
and when��and��can be accessed through samples, i.e., 
�� ��n= 
1 
n P 
i��xi, �� ��m = 1 
m P 
j��yj, a convenient estimator 
forf?and subsequentlyT?is the entropic map (Pooladian 
and Niles-Weed, 2021; Rigollet and Stromme, 2022). We 
generalize these estimators for arbitrary costs h. Similar to 
the original approach, our construction starts by solving a 
dual entropy-regularized OT problem. Let�� > 0and write 
Kij= [exp(���h(xi��� yj)/��)]ijthe 
kernel matrix induced 
by cost h. Def i ne (up to a constant): 
f?,g?= argmax 
f���Rn,g���Rmhf, 
1n 
n i + hg, 
1m 
m i ��� ��he 
f ��,Ke 
g ��i. (9) 
Monge, Bregman and Occam 
Problem(9)is the regularized OT problem in dual 
form (Peyr��e and Cuturi, 2019, Prop. 4.4), an unconstrained 
concave optimization problem that can be solved with the 
Sinkhorn algorithm (Cuturi, 2013). Once such optimal vec- 
tors are computed, estimatorsf��,g��of the optimal dual 
functionsf?,g?of Equation 3 can be recovered by extend- 
ing these discrete solutions to unseen points x,y, 
f��(x) = min��([h(x ��� yj) ��� g? j]j), (10) 
g��(y) = min��([h(xi��� y) ��� f? 
i]i), (11) 
where for a vectoruor arbitrary sizeswe def i ne the log- 
sum-exp operator as min��(u) := �����log(1 s1 
T 
se���u/��).
1 / 1
ChatGPT
ChatGPT
The provided text dives deep into the mathematical aspects of a generalized version of the optimal transport (OT) problem, famously known as the Monge problem.
In general, the Monge problem is concerned with finding an optimal way to transport mass from one configuration to another while minimizing the total cost associated with the transportation. In mathematical terms, given a cost function c(x, y) := h(x - y) and measures �� ��� P(Rd) and �� ��� P(Rd), the objective is to find a mapping T that transforms �� into �� while minimizing the average cost of the displacement as per the function h. However, this problem is notoriously difficult to solve directly due to its non-convex nature.
The authors propose dealing with this problem using a linear optimization approach, by moving from a single point mapping x -> T(x) to the space of couplings of ��, ��. They provide details on how to extract a map T from the optimal coupling P by considering the dual to the optimization problem.
In a nutshell, the authors lay down a strategy for solving the Monge problem by employing OT theory and principles of Bregman centroids. They also introduce and discuss several sparsity-inducing penalties and norms such as '1 and k-overlap norms, as well as a handcrafted penalty to induce less shrinkage than the usual soft-thresholding operator.
Furthermore, the paper discusses the concept of "Generalized Entropic Potential", which extends the entropic map approach for arbitrary cost functions h. It explains how to construct optimal vectors for this entropic regularized OT problem using the Sinkhorn algorithm and provides estimators for the optimal dual functions.
It's crucial to note that while this brief summary provides an overview of the topic, the full understanding of these methods and the mathematical formulations presented would require familiarity with concepts from mathematical optimization, linear algebra, and probability theory, among other mathematical fields.
1 / 1
Generalized Entropic Maps.Using the blueprint given 
in Equation 4, we use the gradient of these dual potential 
estimates to formulate maps. Such maps are only properly 
def i ned on a subset of Rddef i ned as follows: 
����� ��m(h) := {x|���j ��� m,���h(x ��� yj) exists.} ��� Rd. 
(12) 
However, because a convex function is a.e. differentiable, 
����� ��m(h)has measure 1 inRd. With this,���f��is properly 
def i ned for x in ����� ��m(h), as: 
���f��(x) 
= 
m X 
j=1 pj(x)���h(x ��� yj),(13) 
using the x-varying Gibbs distribution in the m-simplex: 
pj(x) := 
exp ?���?h(x 
��� yj) ��� g? 
j ? /��? 
Pm 
k=1exp(���(h(x ��� yk) ��� g 
? k)/��) 
.(14) 
One can check that ifh = 
1 2��� 
2 2, Equation 13 simplif i es to 
the usual estimator (Pooladian and Niles-Weed, 2021): 
T2,��(x) := x ��� ���f��(x) 
= 
m X 
j=1 
pj(x)yj.(15) 
We can now introduce the main object of interest of this 
paper, starting back from Equation 5, to provide a suitable 
generalization for entropic maps of elastic-type: 
Def i nition 3.1.The entropic map estimator forhevaluated 
at x ��� ����� ��m(h) is x ��� ���h������ ���f��(x). 
This simplif i es to: 
Th,��(x) := x ��� Ch((x ��� yj)j,(pj(x))j)(16) 
Bregman Centroids vs. WcGradient l owTo displace 
points, a simple approach consists of followingWcgradient 
l ows, as proposed, for instance, in (Cuturi and Doucet, 
2014) using a primal formulation Equation 2. In practice, 
this can also be implemented by relying on variations in 
dual potentials���f��, 
as advocated in Feydy et al. (2019,��4). 
xi 
yj 
xi 
yj 
Dh-descent rf" descent rf" 
Figure 4.Difference betweenWcgradient descents, minimizing 
loss directly in direction��������f��, or Bregman descent as described 
in Equation 17 whenh = 
1 2��� 
2 
2+ ���1, see Theorem 4.1 for details 
on computing ���h���. Six steps are plotted with stepsize �� = 
1 4. 
This approach arises from the approximation ofWc(�� ��n, �� ��m) 
using the dual objective Equation 3, 
Sh,�� ? 
1 
n P 
i��xi, 
1 
m P 
j��yj ? 
= 
1 
n P 
if��(xi)+ 
1 
m P 
jg��(yj), 
differentiated using the Danskin theorem. As a result, any 
pointxin��is then pushed away from���f��to 
decrease that 
distance. This translates to a gradient descent scheme: 
x ��� x ��� �����f��(x) 
Our analysis suggests that the descent must happen relative 
toDh, to use, instead, a Bregman update (here���� = 1�����): 
x ��� ���h��� ?�� 
�����h(x) + �����h(x ��� ���h������ ���f��(x))?(17) 
Naturally, these two approaches are exactly equivalent as 
h = 1 2��� 
2 2but result in very different trajectories for other 
functions h as shown in Figure 4. 
4. Structured Monge Displacements 
We introduce in this section cost functionshthat we call of 
elastic-type, namely functions with a���2 2term in addition to 
another function��. When��is sparsity-inducing (minimized 
on sparse vectors, with kinks) and has a proximal operator 
in closed form, we show that the displacements induced by 
this function h are feature-sparse. 
4.1. Elastic-type Costs 
By reference to (Zou and Hastie, 2005), we callhof elastic- 
type if it is strongly convex and can be written as 
h(z) := 
1 2kzk 
2 + ��(z).(18) 
where�� : Rd��� Ris a function whose proximal operator is 
well-def i ned. Since OT algorithms are invariant to a posi- 
tive rescaling of the costc, our elastic-type costs subsume, 
without loss of generality, all strongly-convex translation 
invariant costs with convex��. They do also include useful 
cases arising when �� is not (e.g., ��stvs). 
Monge, Bregman and Occam 
Proposition 4.1.Forhas in(18)andx ��� ����� ��m(��)one has: 
Th,��(x) := x���prox�� ��� ���x 
��� 
m X 
j=1 
pj(x) ?yj 
+ �����(x ��� yj)? ��� ��� 
(19) 
Proof. The result follows from ���h���= prox��. Indeed: 
h���(w) = sup 
z wTz ��� 
1 2kzk 
2 ��� ��(z) 
= ���inf 
z ���wTz 
+ 
1 2kzk 
2 + ��(z) 
= 
1 2kwk 
2 ��� inf 
z 
1 2kz ��� wk 
2 + ��(z). 
Differentiating on both sides and using Danskin���s lemma, 
we get the desired result by developing���hand 
taking ad- 
vantage of the fact that the weights pj(x) sum to 1. 

4.2. Sparsity-Inducing Functions �� 
We discuss in this section the three choices we introduced 
in��2 for proximal operators and their practical implications 
in the context of our generalized entropic maps. 
1-Norm���1.Asaf i rstexample, weconsider��(z) = ��kzk1 
in Equation 18. The associated proximal operator is the soft- 
thresholding operatorprox��(��) = ST(��,��)mentioned 
in 
the introduction. We also have���h(z) = z + �� sign(z)for 
zwith no0coordinate. Plugging this in Equation 5, we i nd 
that the Monge map T�����1,��(x) is equal to 
x ��� ST�� ��� ���x 
��� 
m X 
j=1 
pj(x) ?yj 
+ �� sign(x ��� yj)? ��� ���, 
where thepj(x)are evaluated atxusing Equation 14. 
Applying the transport consists in an element-wise oper- 
ation onx: for each of its featurest ��� d, 
one substracts 
ST�� ?Pm 
j=1p j(x)���h(xt ��� yj t) ? 
. The only interaction be- 
tween coordinates comes from the weights pj(x). 
The soft-thresholding operator sparsif i es the displacement. 
Indeed, when for a given x and a feature t ��� d one has 
|xt��� 
m X 
j=1 
pj(x)yj t+ �� sign(xt��� y 
j t)| ��� ��, 
then there is no change on that feature :[T�����1,��(x)]t= xt. 
That mechanism works to produce, locally, sparse displace- 
ments on certain coordinates.Another interesting phe- 
nomenon happens whenxis too far from theyj���s on some 
coordinates, in which case the transport defaults back to 
a���2 2average of the target pointsyj(with weights that are, 
however, inf l uenced by the �����1regularization): 
Proposition 4.2.Ifxis such thatxt��� maxjyj 
t orxt��� 
minjyj 
t then T�����1,��(x)t= P 
jp j(x)yj t. 
Proof.For instance, assumext��� maxjyj t. Then, for 
allj, we havesign(xt��� yj 
t) = 1, and as a consequence 
Pm 
j=1p j(x)���h(x ��� yj)t 
= xt��� P 
jp j(x)yj 
t + ��. This 
quantity is greater than��, so applying the soft-thresholding 
givesST��(Pm 
j=1p j(x)���h(x���yj)t) 
= xt���P 
jp j(x)yj t, 
which gives the advertised result. Similar reasoning gives 
the same result when xt��� minjyj t. 
Interestingly, this property depends on��only through the 
pj(x)���s, and the condition thatxt��� maxjyj 
t orxt��� 
minjyj 
t does not depend on �� at all. 
Vanishing Shrinkage STVS:The���1term added to form 
the elastic net has a well-documented drawback, notably 
for regression: on top of having a sparsifying effect on the 
displacement, it also shrinks values. This is clear from the 
soft-thresholding formula, where a coordinate greater than�� 
is reduced by��. This effect can lead to some ���shortening��� of 
displacement lengths in the entropic maps. We use the Soft- 
Thresholding with Vanishing Shrinkage (STVS) proposed 
by Schreck et al. (2015) to overcome this problem. The 
cost function is given by Equation 7, and its prox in Equa- 
tion 8. When|z|is 
large, we haveprox��stvs(z) = z + o(1), 
which means that the shrinkage indeed vanishes. Interest- 
ingly, even though the cost��stvsis non-convex, it still has a 
proximal operator, and 
1 2k �� k 
2 + ��stvsis 
1 2-strongly convex. 
k-Overlap: �� = k �� kovk.Thek-overlap 
norm offers the 
distinctive feature that its proximal operator selects any- 
where betweend(small��) andk(large��) non-zero vari- 
ables, see Figure 1. Applying this proximal operator is, 
however, signif i cantly more complex, because it is not sepa- 
rableacrosscoordinatesandrequiresd(k+logd)operations, 
instantiating ak �� d ��� kmatrix 
to select two integersr,l 
(r ��� k ��� l) 
at each evaluation. We were able to use it for 
moderate problem sizes but these costs became prohibitive 
on larger scale datasets, wheredis a few tens of thousands.
1 / 1
ChatGPT
ChatGPT
This passage seems to be discussing a methodology for creating Generalized Entropic Maps. The methodology employs convex functions, Bregman centroids, and several types of costs, such as elastic-type costs and sparsity-inducing costs.
The paper defines a subset of Rd, represented as ����� ��m(h), using the gradient of dual potential estimates. Then, using this definition and the concept of convex functions, the paper discusses the formulation of entropic maps.
A part of the methodology involves the use of an x-varying Gibbs distribution in an m-simplex, represented as pj(x). This distribution, along with the gradient of these dual potential estimates, is used to define a new function ���f��(x), and an associated entropic map estimator.
The paper then discusses the use of Wc gradient flows to displace points, a concept that has been proposed in other studies. The authors suggest, however, that the descent must be relative to Dh, leading to a Bregman update.
Next, the paper introduces the main object of interest, the cost function h, which is elastic-type if it's strongly convex and can be written in a particular way. The authors also introduce sparsity-inducing functions �� that, when minimized on sparse vectors, make the displacements feature-sparse.
Finally, the paper discusses three sparsity-inducing functions in detail: the 1-Norm ���1, Vanishing Shrinkage STVS, and k-Overlap. Each has specific properties and uses that are beneficial in different contexts. However, the use of the k-Overlap norm can be computationally expensive for large datasets.
1 / 1
5. Experiments 
We start this experimental study with two synthetic tasks. 
For classic costs such as���2 2, several examples of ground- 
truth optimal maps are known. Unfortunately, we do not 
know yet how to propose ground-truthh-optimal maps, nor 
dual potentials, whenhhas the general structure considered 
in this work. As a result, we study two synthetic problems, 
where the sparsity pattern of a ground truth transport is ei- 
ther constant acrossxor split across two areas. We follow 
with an application to single-cell genomics, where the mod- 
eling assumption that a treatment has a sparse effect on gene 
Monge, Bregman and Occam 
���1��stvsk �� k2 
ovk, k = 5 k �� k2 
ovk, k = 7 ���2 
2cost 
10���1101 
0.05 
0.06 
NMSE 
d = 8,s = 5 10���1101 
0.04 
0.05 
d = 20,s = 5 10���1101 
0.025 
0.050 
d = 100,s = 5 
10���1101 
Regularization �� 
10���5 
10���3 
Support 
error 10���1101 
Regularization �� 
10���3 
10���2 10���1101 
Regularization �� 
10���2 
10���1 
Figure 5. Synthetic experiment:ability of the estimatedTh,��to 
recover a ground-truth transport which displacesscoordinates in 
dimensiond, withn = 1000samples. We compare the different 
costshproposed in the paper with the classical���2 2cost. We identify 
3regimes.Left:whens ��� d, the���2 2cost is already good, and the 
proposed costs barely improve over it in terms of MSE.Middle: 
whendis moderately larger thans, all the proposed costs improve 
over the���2 
2cost, and the optimal regularization for���1and��stvs 
are i nite.Right:Whend ? s, the proposed methods vastly 
improve over the���2 
2cost. The optimal regularization for���1and 
��stvsis inf i nite even for the MSE. In terms of support error, larger 
regularization always leads to better results. 
activation (across34kgenes) is plausible. In terms of im- 
plementaiton, the entire pipeline described in��3 and 4 rests 
on running the Sinkhorn algorithm i rst, with an appropriate 
cost, and than differentiating the resulting potentials. This 
can be carried out in a few lines of code using a parame- 
terizedTICost, fed into theSinkhornsolver, to output 
aDualPotentialsobject in OTT-JAX1(Cuturi et al., 
2022). We a class of regularized translation invariant cost 
functions, specifying both regularizers��and their proximal 
operators. We call such costs RegTICost. 
5.1. Synthetic experiments. 
Constant sparsity-pattern.We measure the ability of 
our method to recover a sparse transport map using a set- 
ting inspired by (Pooladian and Niles-Weed, 2021). Here 
�� = U[0,1]d. 
For an integers < d, we set�� = T? 
s]��, where 
the mapT? 
s acts on coordinates independently with the for- 
mulaT? 
s(x) = [exp(x1),...,exp(xs),xs+1,...,xd]: it 
only changes the i rstscoordinates of the vector, and cor- 
responds to a sparse displacement whens ? d. Note that 
this sparse transport plan is much simpler than the maps our 
model can handle since, for this synthetic example, the spar- 
sity pattern is i xed across samples. Note also that while it 
might be possible to detect that only the i rstscomponents 
have high variability using a 2-step pre-processing approach, 
1https://github.com/ott-jax/ott 
101102103 
Dimension d 
10���2 
10���1 
NMSE ���2 2cost 
���1 
��stvs 
k �� k2 
ovk, k = 5 
k �� k2 
ovk, k = 7 
Figure 6. Scalingwith 
dimension:The number 
ofsamplesisf i xed 
ton = 100, and the 
sparsity tos = 2. For 
each dimension, we do 
a grid search over��and 
retain the one with the 
lowest MSE. 
or an adaptive, robust transport approach (Paty and Cuturi, 
2019), our goal is to detect that support in a one-shot, thanks 
to our choice ofh. We generaten = 1,000i.i.d. samplesxi 
from��, andyjfrom��independently; the samplesyjare ob- 
tained by i rst generating fresh i.i.d. samples�� xjfrom��and 
then pushing them :yj:= T? 
s(�� xj). We use our three costs 
to computeTh,��from these samples, and measure our ability 
to recoverT? 
s fromTh,��using a normalized MSE def i ned as 
1 
nd Pn 
i=1kT 
? s(xi)���Th,��(xi)k2. 
We also measure how well 
our method identif i es the correct support: for each sample, 
we compute the support error as Pd 
i=s+1��� 
2 i/ Pd 
i=1��� 
2 
i 
with���the displacementTh,��(x) ��� x. 
This quantity is be- 
tween0and1and cancels if and only if the displacement 
happens only on the correct coordinates. We then average 
this quantity overall the xi. Figure 5 displays the results as 
d varies and s is i xed. Here, ��stvsperforms better than ���1. 
x-dependent sparsity-pattern.To illustrate the abil- 
ity of our method to recover transport maps whose 
sparsity pattern is adaptive, depending depending on 
the inputx, we extend the previous setting as fol- 
lows.To computeFs(x), we compute i rst the norms 
of two coordinate groups ofx:n1= Ps 
i=1x 
2 
i and 
n2= P2s 
i=s+1x 
2 i. Second, we displace the coordinate 
group with the largest norm: ifn1> n2,Fs(x) = 
[exp(x1),...,exp(xs),xs+1,...,xd], otherwiseFs(x) = 
[x1,...,xs,exp(xs+1),...,exp(x2s),x2s+1,...,xd]. 
Obviously, the displacement pattern depends onx. Figure 6 
shows the NMSE with different costs when the dimensiond 
increases whilesandnare i xed. As expected, we observe 
a much better scaling for our costs than for the standard���2 
2 
cost, indicating that sparsity-inducing costs mitigate the 
curse of dimensionality.
1 / 1
ChatGPT
ChatGPT
The text seems to be part of an academic paper discussing a method for optimal transport mapping in high-dimensional data. Here are the key points for understanding the paper:
The problem:
The authors are trying to solve the problem of optimal transport mapping in high-dimensional data, where the sparsity pattern of the ground truth transport is either constant across all variables (x) or split across two areas.
The experimental setup:
The authors first conduct synthetic experiments to measure their method's ability to recover a sparse transport map. For this, they use a setting where �� = U[0,1]^d. They set �� = T?^s]��, where the map T?^s acts on coordinates independently using a specific formula. They generate 1,000 samples from ��, and �� independently. They use three different costs to compute Th,�� from these samples, and measure their ability to recover T?^s from Th,�� using a normalized Mean Square Error (MSE).
The results:
The authors observed that ��stvs performs better than '1 when the dimension (d) varies and the sparsity (s) is fixed.
Extended experiments:
In a more complex scenario, the sparsity pattern is adaptive and depends on the input. In this case, they displace the coordinate group with the largest norm, causing the displacement pattern to depend on x. When the dimension (d) increases while s and n are fixed, the authors observed much better scaling for their costs than for the standard ���2^2 cost, indicating that sparsity-inducing costs mitigate the curse of dimensionality.
Genomics application:
The authors also mention an application to single-cell genomics, where the assumption of a treatment having a sparse effect on gene activation across 34,000 genes is plausible. The methods were implemented using the Sinkhorn algorithm.
RegTICost:
They introduced a class of regularized translation invariant cost functions, specifying both regularizers �� and their proximal operators. They call such costs RegTICost.
In summary, the authors are using a new cost-based approach for optimal transport mapping in high-dimensional data, where they are assuming the sparsity of data either to be constant or variable. The method showed promising results in synthetic experiments and it was mentioned to be potentially useful in single-cell genomics.
1 / 1
5.2. Single-Cell RNA-seq data. 
We validate our approach on the single-cell RNA sequenc- 
ing perturbation data from (Srivatsan et al., 2020). After 
removing cells with less than200expressed genes and genes 
expressed in less than20cells, the data consists of579,483 
cells and34,636genes. In addition, the raw counts have 
beennormalizedandlog(x+1)scaled. Weselectthe5drugs 
(Belinostat, Dacinostat, Givinostat, Hesperadin, and Quisi- 
nostat) out of188drug perturbations that are highlighted in 
Monge, Bregman and Occam 
50100 
1 2��� 
2 
2estim. 
60 
80 
100 
120 
Elastic 
�����1estim. 
���2 2Sink. div., PCA space 0.500.751.00 
1 2��� 
2 
2estim. 
0.4 
0.6 
0.8 
1.0 
R2, dosage: 10nM 0.00.51.0 
1 2��� 
2 2estim. 
0.0 
0.5 
1.0 
R2, dosage: 10��M 0.250.50 
1 2��� 
2 2estim. 
0.2 
0.4 
0.6 
RBO marker genes 10���1 
6 �� 10���2 
2 �� 10���1 
3 �� 10���1 
4 �� 10���1 
2���320 
Regularization �� 
101 
102 
% non-zero 
in 
disp. 
Hesperadin, MCF7 
�����1 
��-��stvs 
���2 
2 2���320 
Regularization �� 
2300 
2350 
2400 
2450 
2500 
���2 2Sink. 
div., 
gene 
space 
Hesperadin, MCF7 2���320 
Regularization �� 
101 
102 
% non-zero 
in 
disp. 
Givinostat, K562 2���320 
Regularization �� 
2800 
2850 
2900 
2950 
3000 
���2 2Sink. 
div., 
gene 
space 
Givinostat, K562 
Figure 7.Top row: performance, for all 15 experiments, of the elastic-�����1estimator vs. the 
1 
2��� 
2 2entropic map. We consider 6 values 
for��. Each of the15 �� 6crosses denotes the mean, over 10 random 80%,/20% splits of that cell line/drug experiment, of a quantity 
of interest. To facilitate reading, rather than reporting the��value, we report the average percentage of non-zero displacements (using 
np.isclose(.,0), equivalently thresholding values below10���8) across all displaced points in that fold (yellow means40%dense 
displacements, dark-blue displacements only happen on��� 5%of 
genes). While all these maps are estimated in full genes space (��� 34k), 
we provide a simplif i ed measure of their ability to reconstruct the measures by computing a 
1 
2��� 
2 2-Sinkhorn divergence in PCA space. 
This picture shows that one can sparsify signif i cantly 
1 2��� 
2 
2maps and still get a similar reconstruction error. Next, we picture separately 
theR2(see text body for details) computed on marker genes on low (10nM) and high (10��M) dosages of the drug. For low dosages, 
inducing sparsity in displacements seems to help, whereas this may no longer be the case when the effect of perturbations becomes 
large. Finally, the RBO metric shows that sparsity does help to select marker genes based only on map estimation. Bottom row: Close up 
on Hesperadin/MCF7 and Givinostat/K562 experiments. For each, we quantify the sparsifying effect w.r.t��, as well as 
1 2��� 
2 
2-Sinkhorn 
divergence in full gene space. 
Table 1. Per cell line, sample sizes of control + drug perturbation. 
Cont.Dac.Giv.Bel.Hes.Quis. 
A5493274558703669436475 
K5623346388589656624339 
MCF763461562180516848821520 
the original data (Srivatsan et al., 2020) as showing a strong 
effect. We consider3human cancer cell lines (A549, K562, 
MCF7) to each of which is applied each of the5drugs. We 
use our four methods to learn an OT map from control to 
perturbed cells in each of these3 �� 5scenarios. For each 
cell line/drug pair, we split the data into10non-overlapping 
80%/20% train/test splits, keeping the test fold to produce 
our metrics. 
Methods.We ran experiments in two settings, using the 
whole34,000-dgene space and subsetting to the top5k 
highly variable genes using SCANPY(Wolf et al., 2018). 
We consider entropic map estimators with the following cost 
functions and pre-processing approaches: 
1 2��� 
2 2cost; the 
1 2��� 
2 
2 
cost on50-dPCA space (PCA directions are recomputed 
on each train fold); Elastic with�����1; Elastic with��-��svts 
cost. We vary��for these two methods. We did not use the 
k��kovknorm because of memory challenges when handling 
such a high-dimensional dataset. For the non-PCA-based 
approaches, we can also measure their performance in PCA space by projecting their high-dimensional predictions onto the 50-dspace. The��regularization parameter for all these approaches is set for each cost and experiment to10%of the mean value of the cost matrix between the train folds of control and treated cells, respectively.
1 / 1
ChatGPT
ChatGPT
The study expanded to real-world data, analyzing single-cell RNA sequencing perturbation data from (Srivatsan et al., 2020). After pre-processing, the data consisted of 579,483 cells and 34,636 genes. The study focused on five specific drugs that were highlighted in the original data due to their strong effects.
Three human cancer cell lines (A549, K562, MCF7) were considered, and each was subjected to each of the five drugs. This produced 3 �� 5 scenarios for analysis. For each cell line/drug pairing, the data was split into 10 non-overlapping 80%/20% train/test splits.
The study then ran experiments in two settings, one using the whole 34,000-d gene space and one subsetting to the top 5k highly variable genes. Four methods were used to learn an Optimal Transport (OT) map from control to perturbed cells: a standard Euclidean (L2) cost, an L2 cost on a 50-dimension PCA space, Elastic with L1 regularization, and Elastic with tau-SVTs cost. For the last two methods, the gamma parameter was varied.
The research did not use the ovk norm due to memory constraints when dealing with such high-dimensional data. The regularization parameter for all these approaches was set for each cost and experiment to 10% of the mean value of the cost matrix between the train folds of control and treated cells, respectively.
1 / 1
Evaluation. We evaluate methods using these metrics: 
���the���2 2-Sinkhorn divergence (using ��to be 10% of the 
mean of pairwise���2 2cost matrix of treated cells) between 
transferredpoints(fromtestfoldofcontrol)andtestpoints 
(from perturbed state); lower is better. 
���Ranked biased overlap (Webber et al., 2010) withp = 0.9, 
Monge, Bregman and Occam 
between the50perturbation marker genes as computed 
on all data with SCANPY, and the following per-gene 
statistic, computed using a map as follows: average (on 
fold) expression of (predicted) perturbed cells from orig- 
inal control cells (this tracks changes in log-expression 
before/after predicted treatment); higher is better. 
���Coeff i cient of determination (R2) between the average 
ground-truth / predicted gene expression on the50pertur- 
bation markers (Lotfollahi et al., 2019); higher is better. 
These results are summarized in Figure 7, across various 
costs, perturbations and hyperparameter choices. 
Conclusion.We consider structured translation-invariant 
ground costshfor transport problems. After forming an 
entropic potential with such costs, we plugged it in Bre- 
nier���s approach to construct a generalized entropic map. We 
highlighted a surprising connection between that map and 
the Bregman centroids associated with the divergence gen- 
erated byh, resulting in a more natural approach to gradient 
l ows def i ned byWc, illustrated in a simple example. By 
selecting costshof elastic type (a sum of���2 2and a sparsi- 
fying term), we show that our maps mechanically exhibit 
sparsity, in the sense that they have the ability to only impact 
adaptivelyxon a subset of coordinates. We have proposed 
two simple generative models where this property helps 
estimation and applied this approach to high-dimensional 
single-cell datasets where we show, at a purely mechanical 
level, that we can recover meaningful maps. Many natural 
extensions of our work arise, starting with more informative 
sparsity-inducing norms (e.g., group lasso), and a more gen- 
eral approach leveraging the Bregman geometry for more 
ambitious Wcproblems, such as barycenters.
1 / 1
ChatGPT
ChatGPT
The study assessed the performance of the methods using three metrics:
L2-Sinkhorn divergence between transferred points (from the test fold of the control) and test points (from the perturbed state), with lower values being better.
Ranked Biased Overlap (RBO) with p = 0.9, comparing the 50 perturbation marker genes calculated on all data with SCANPY to the average expression of predicted perturbed cells from original control cells. A higher RBO indicates better performance.
Coefficient of Determination (R^2) comparing the average ground-truth and predicted gene expression on the 50 perturbation markers. A higher R^2 value indicates better performance.
The study concluded by highlighting its exploration of structured translation-invariant ground costs for transport problems. By forming an entropic potential with such costs and applying Brenier's approach, the study constructed a generalized entropic map.
An unexpected connection was found between that map and the Bregman centroids associated with the divergence generated by the ground costs. This connection facilitated a more natural approach to gradient flows defined by Wc, which was demonstrated through a simple example.
By selecting costs of an elastic type (a sum of L2 and a sparsifying term), the study showed that the maps exhibit sparsity. This means that they can adaptively impact only a subset of coordinates.
The study proposed two simple generative models that benefited from this property and applied this approach to high-dimensional single-cell datasets. The results demonstrated the ability to recover meaningful maps.
The study concluded by suggesting several potential extensions of the work, such as utilizing more informative sparsity-inducing norms (like group lasso) and leveraging the Bregman geometry for more complex Wc problems, such as barycenters.
1 / 1
Explain the whole article more simply
1 / 1
ChatGPT
ChatGPT
This research article is about a method for understanding changes in gene expression in individual cells, especially when those cells are exposed to various drugs. This type of data is known as single-cell RNA sequencing data, and it allows researchers to look at what's happening in individual cells rather than averaging over many cells, as was done in the past.
The authors used a mathematical technique called Optimal Transport (OT), which is a way of comparing two different distributions of things. In this case, they are comparing the distribution of gene expression in cells before and after drug treatment. By using this method, they can identify what changes have occurred due to the treatment.
They also introduced a way to make their OT method more efficient and capable of handling large datasets by incorporating sparsity, which means that the method ignores some less relevant information and focuses on the most important features. They named this method the "elastic" method, and they tested it against other methods in a variety of scenarios.
Their testing metrics included comparing how close the model's predictions are to the actual results, how well the model can rank genes by their importance, and how well the model's predictions fit the actual results.
Finally, they concluded by suggesting that their method could be adapted to other challenges in the analysis of single-cell data, opening doors for further research and applications in this field.
1 / 1
What is a Wasserstein barycenter
1 / 1
ChatGPT
ChatGPT
A Wasserstein barycenter is a concept that arises in the field of optimal transport, a branch of mathematics that deals with comparing different probability distributions.
A barycenter is a sort of "average" or "center of mass" for a set of points. In a simple Euclidean space, you might calculate a barycenter by adding up all the points and dividing by the number of points. However, when you're dealing with probability distributions instead of simple points, the calculation becomes much more complex.
The concept of a "Wasserstein barycenter" was introduced as a way to define a meaningful "average" of different probability distributions. Given a set of distributions, the Wasserstein barycenter is defined as the distribution that minimizes the sum of its Wasserstein distances to all the other distributions in the set.
The Wasserstein distance, also known as the Earth Mover's distance, is a way of measuring how different two probability distributions are. It essentially measures the "cost" of transforming one distribution into the other, where the cost is defined as the amount of "mass" that needs to be "moved" times the distance it is moved.
So, in simple terms, the Wasserstein barycenter is a sort of "average" distribution that is, in some sense, in the "middle" of a set of other distributions.
1 / 1
Fast Computation of Wasserstein Barycenters
Marco Cuturi, Arnaud Doucet Proceedings of the 31st International Conference on Machine Learning, PMLR 32(2):685-693, 2014.
Abstract
We present new algorithms to compute the mean of a set of N empirical probability measures under the optimal transport metric. This mean, known as the Wasserstein barycenter (Agueh and Carlier, 2011; Rabin et al, 2012), is the measure that minimizes the sum of its Wasserstein distances to each element in that set. We argue through a simple example that Wasserstein barycenters have appealing properties that differentiate them from other barycenters proposed recently, which all build on kernel smoothing and/or Bregman divergences. Two original algorithms are proposed that require the repeated computation of primal and dual optimal solutions of transport problems. However direct implementation of these algorithms is too costly as optimal transports are notoriously computationally expensive. Extending the work of Cuturi (2013), we smooth both the primal and dual of the optimal transport problem to recover fast approximations of the primal and dual optimal solutions. We apply these algorithms to the visualization of perturbed images and to a clustering problem.
1 / 1
ChatGPT
ChatGPT
This paper presents new methods for calculating the average (also known as the barycenter) of a set of empirical probability distributions under the optimal transport metric, a mathematical concept used to compare and contrast different probability distributions. The Wasserstein barycenter of a set of distributions is the distribution that minimizes the sum of its Wasserstein distances to each distribution in the set.
The authors argue that Wasserstein barycenters have certain beneficial properties that make them distinct from other types of barycenters, such as those based on kernel smoothing or Bregman divergences.
The two newly proposed algorithms in the paper involve the repeated calculation of primal and dual optimal solutions of transport problems, which are key components in the theory of optimal transport. However, directly implementing these algorithms is computationally expensive due to the nature of optimal transport problems.
To overcome this challenge, the authors extend previous work to smooth both the primal and dual of the optimal transport problem. This approach provides quicker approximations of the primal and dual optimal solutions, thus making the computation of Wasserstein barycenters more efficient.
Finally, the authors demonstrate the effectiveness of these algorithms through applications in image processing and clustering problems. They visualize perturbed (distorted or changed) images using Wasserstein barycenters and use them to solve a clustering problem, showing the practical implications and benefits of their approach.
1 / 1
Summarize: STOCHASTICSADDLE-POINTOPTIMIZATION FOR THE 
WASSERSTEINBARYCENTERPROBLEM��� 
Daniil Tiapkin 
HSE University, Moscow, Russia 
dtyapkin@hse.ru 
Alexander Gasnikov 
Moscow Institute of Physics and Technology, Moscow, Russia 
Institute for Information Transmission Problems, Moscow, Russia 
HSE University, Moscow, Russia 
Weierstrass Institute for Applied Analysis and Stochastics, Berlin, Germany 
gasnikov@yandex.ru 
Pavel Dvurechensky 
Weierstrass Institute for Applied Analysis and Stochastics, Berlin, Germany 
Institute for Information Transmission Problems, Moscow, Russia 
pavel.dvurechensky@wias-berlin.de 
December 6, 2021 
ABSTRACT 
We consider the population Wasserstein barycenter problem for random probability measures sup- 
ported on a i nite set of points and generated by an online stream of data. This leads to a complicated 
stochastic optimization problem where the objective is given as an expectation of a function given as 
a solution to a random optimization problem. We employ the structure of the problem and obtain a 
convex-concave stochastic saddle-point reformulation of this problem. In the setting when the distri- 
bution of random probability measures is discrete, we propose a stochastic optimization algorithm 
and estimate its complexity. The second result, based on kernel methods, extends the previous one 
to the arbitrary distribution of random probability measures. Moreover, this new algorithm has a 
total complexity better than the Stochastic Approximation approach combined with the Sinkhorn 
algorithm in many cases. We also illustrate our developments by a series of numerical experiments. 
KeywordsStochastic Optimization��Saddle-Point Optimization��Computational 
Optimal Transport�� 
Wasserstein Barycenter Problem 
In this paper we consider stochastic optimization problems, which arise in computational optimal transport when the 
goal is to estimate population Wasserstein barycenter [1] (or Fr��chet mean w.r.t. Wasserstein distance) of a probability 
distribution on the Wasserstein space of probability measures. Wasserstein barycenter problem has recently attracted 
a lot of attention from the machine learning, statistics, and optimization community, see [48,17] and references 
therein. From the computational point of view, approximating population Wasserstein barycenter is a challenging 
optimization problem since it contains several layers of complications. The i rst layer requires to def i ne forp ��� 1the 
p-Wasserstein distanceWp(r,c)between 
two probability measuresr,c, which in this paper we assume to be discrete 
probability distributions that belong to the standard simplex���n.Wp(r,c)is 
def i ned as an optimal value in an optimal 
transport problem, where the goal is to i nd a transport plan between vectorsr,csuch that the total transportation cost 
is minimal (see(1)for a formal def i nition). The next layer is that the measurecis assumed to be random with some 
probability distributionPcon���nand the population barycenterr���minimizes the expected Wasserstein distance, i.e. 
r���= argminr������nEcWp 
p(r,c). This is a stochastic optimization problem, where the objective is def i ned as a solution 
to some other random optimization problem. The third layer of complication is that the dimensionncan be very large, 
���The work of A. Gasnikov in Section 2 was partially supported by the Ministry of Science and Higher Education of the Russian 
Federation (Goszadaniye) 075-00337-20-03, project no. 0714-2020-0005. The work of P. Dvurechensly in Section 3 was funded 
by Russian Science Foundation (project 18-71-10108). The work of D. Tiapkin was prepared within the framework of the HSE 
University Basic Research Program. 
arXiv:2006.06763v3 
[math.OC] 
2 
Dec 
2021 
APREPRINT- DECEMBER6, 2021 
e.g.106. To motivate this setting we refer to [9], where the authors consider a template estimation problem from a 
random sample of its transformations. A particular example of such a template can be images, considered in [15], where 
empirically the Wasserstein barycenter gives the best quality of the template image reconstruction. For a103by103 
image, we obtain that n = 106and the dimension of the transportation plan is then n2= 1012leading to a huge-scale 
optimization problem. 
Similar to other stochastic optimization problems, see e.g. [45], there are two approaches to the population Wasser- 
stein barycenter problem, which we refer to as off l ine (also known as Sample Average Approximation, SAA) and 
online (also known as Stochastic Approximation, SA). The off l ine approach assumes that a number of random 
probability measuresci,i = 1,...,mis sampled in advance and all the measures are stored in memory. Then 
the population Wasserstein barycenter problem is approximated by the empirical Wasserstein barycenter problem 
�� r = argminr������n 
1 
m Pm 
i=1W 
p 
p(r,ci), i.e. the expectation is approximated by the i nite-sum. Finally, it is assumed that 
an optimization algorithm has access to all of the measuresci,i = 1,...,mimmediately, probably in parallel, and the goal 
is to minimize this i nite-sum. So far, nearly all algorithms in the literature for approximating Wasserstein barycenter 
are designed for this i nite-sum problem, to name a few [15,6,54,13,23,59,38,40,22,33,36]. Additionally, in this 
setting, it is possible to use a big arsenal of modern decentralized distributed optimization algorithms, see for example, 
[23,59,52,19,36,29,35,34,30,51,50,27,7], where, in particular, such algorithms are developed with best-known 
complexity bounds for general i nite-sum optimization with cheap dual stochastic oracle, which turns out to be available 
for the Wasserstein distance.
1 / 1
ChatGPT
ChatGPT
In this paper, the authors explore the calculation of population Wasserstein barycenters (averages) in the context of random probability measures, specifically those generated by an online data stream and supported on a finite set of points. This results in a complex stochastic optimization problem, as the objective function is derived from an expectation of a solution to a random optimization problem.
To address this challenge, the authors reformulate the problem into a convex-concave stochastic saddle-point problem. When the distribution of random probability measures is discrete, they propose a novel stochastic optimization algorithm and provide estimates of its computational complexity.
Building upon this, the authors present a second algorithm using kernel methods, extending their approach to arbitrary distributions of random probability measures. Notably, this new algorithm often demonstrates better total complexity than using a combination of the Stochastic Approximation approach and the Sinkhorn algorithm.
The paper also contains numerical experiments illustrating the authors' methodological advancements. The main application of these methods is in computational optimal transport, where they're used to estimate the population Wasserstein barycenter of a probability distribution within the Wasserstein space of probability measures. This is a computationally challenging problem, particularly when dealing with large datasets.
1 / 1
ChatGPT may produce inaccurate information about people, places, or facts.
ChatGPT May 24 Version
ChatGPT





