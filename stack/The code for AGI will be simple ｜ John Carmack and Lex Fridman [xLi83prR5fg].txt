To me, personally, I think it's clear that if AGI were to be achieved, that would change
the course of human history.
AGI-wise, I was making this decision about what do I want to focus on after VR, and I'm
still working on VR regularly.
I spend a day a week kind of consulting with Metta, and Boz styles me the consulting CTO.
It's kind of like the Sherlock Holmes that comes in and consults on some of the specific
tough issues.
I'm still pretty passionate about all of that, but I have been figuring out how to compartmentalize
and force that into a smaller box to work on some other things.
I did come down to this decision between working on economical nuclear fission or artificial
general intelligence.
The fission side of things, I've got a bunch of interesting things going that way, but
it would take, that would be a fairly big project thing to do.
I don't think it needs to be as big as people expect.
I do think something original, SpaceX-sized, you build it, power your building off of it,
and then the government, I think, will come around to what you need to.
Everybody loves an existence proof.
I think it's possible somebody should be doing this, but it's going to involve some politics.
It's going to involve decent-sized teams and a bunch of this cross-functional stuff that
I don't love, while the artificial general intelligence side of things.
It seems to me like this is the highest leverage moment for potentially a single individual,
potentially in the history of the world, where the things that we know about the brain, about
what we can do with artificial intelligence, nobody can say absolutely on any of these
things, but I am not a madman for saying that it is likely that the code for artificial
general intelligence is going to be tens of thousands of lines of code, not millions of
lines of code.
This is code that conceivably one individual could write, unlike writing a new web browser
or operating system.
Based on the progress that AI has machine learning has made in the recent decade, it's
likely that the important things that we don't know are relatively simple.
There's probably a handful of things.
My bet is that I think there's less than six key insights that need to be made.
Each one of them can probably be written on the back of an envelope.
We don't know what they are, but when they're put together in concert with GPUs at scale
and the data that we all have access to, that we can make something that behaves like a
human being or like a living creature and that can then be educated in whatever ways
that we need to get to the point where we can have universal remote workers where anything
that somebody does mediated by a computer and doesn't require physical interaction that
an AGI will be able to do.
We can already simulate the equivalent of the Zoom meetings with avatars and synthetic
deepfakes and whatnot.
We can definitely do that.
We have superhuman capabilities on any narrow thing that we can formalize and make a loss
function for, but there's things we don't know how to do now, but I don't think they
are unapproachably hard.
Now, that's incredibly hubristic to say that it's like, but I think that what I said a
couple years ago is a 50% chance that somewhere there will be signs of life of AGI in 2030,
and I've probably increased that slightly.
I may be at 55, 60% now because I do think there's a little sense of acceleration there.
So I wonder what the, and by the way, you also written that I bet with hindsight, we will find
that clear antecedents of all the critical remaining steps for AGI are already buried
somewhere in the vast literature of today.
So the ideas are already there.
I think that's likely the case.
One of the things that appeals to so many people, including me, about the promise of AGI is we
know that we're only drinking from a straw from the fire hose of all the information out there.
I mean, you look at just in a very narrowly bounded field like machine learning, like you
can't read all the papers that come out all the time, you can't go back and read all the
clever things that people did in the 90s or earlier that people have forgotten about because
they didn't pan out at the time when they were trying to do them with 12 neurons.
So this idea that, yeah, I think there are gems buried in some of the older literature that was
not the path taken by everything.
And you can see a kind of herd mentality on the things that happen right now.
It's almost funny to see, it's like, oh, Google does something, and OpenAI does something,
Meta does something, and they're the same people that all talk to each other, and they're all
one-upping each other, and they're all capable of implementing each other's work given a month
or two after somebody has an announcement of that.
But there's a whole world of possible approaches to machine learning.
And I think that we probably will, in hindsight, go back and see.
It's like, yeah, that was kind of clearly predicted by this early paper here.
And this turns out that if you do this and this and take this result from animal training
and this thing from neuroscience over here and put it together and set up this curriculum
for them to learn in, that that's kind of what it took.
You don't have too many people now that are still saying it's not possible or it's going
to take hundreds of years.
And 10 years ago, you would get a collection of experts, and you would have a decent chunk
on the margin that either say not possible or a couple hundred years, might be centuries.
And the median estimate would be like 50, 70 years, and it's been coming down.
And I know with me saying eight years for something that still puts me on the optimistic
side, but it's not crazy out in the fringes.
And just being able to look at that at a meta level about the trend of the predictions going
down there, the idea that something could be happening relatively soon.
Now, I do not believe in fast takeoffs.
That's one of the safety issues that people say, it's like, oh, it's going to go fume,
and the AI is going to take over the world.
There's a lot of reasons.
I don't think that's a credible position.
And I think that we will go from a point where we start seeing things that credibly
look like animals' behaviors and have a human voice box wired into them.
It's like I tried to get Elon to say it's like you're pig at Neuralink, give it a human
voice box and let it start learning human words.
I think animal intelligence is closer to human intelligence than a lot of people like to
think.
I think that culture and modalities of IO make the Gulf seem a lot bigger than it actually
is.
There's just that smooth spectrum of how the brain developed and cortexes and scaling
of different things going on there.
Cultural modalities of IO, yes, language is sort of lost in translation conceals a lot
of intelligence.
So when you think about signs of life for AGI, you're thinking about human interpretable
signs.
So the example I give, if we get to the point where you've got a learning disabled toddler,
some kind of real special needs child that can still interact with their favorite TV
show and video game and can be trained and learned in some appreciably human-like way,
at that point, you can deploy an army of engineers, cognitive scientists, education
developmental education people, and you've got so many advantages there, unlike real
education where you can do rollbacks and A.B. testing and you can find a golden path through
a curriculum of different things.
If you get to that point, learning disabled toddler, I think that it's going to be a
done deal.
But do you think we'll know when we see it?
So there's been a lot of really interesting general learning progress from DeepMind, opening
eye a little bit too.
I tend to believe that Tesla autopilot deserves a lot more credit than it's getting for making
progress on the general, sort of on doing the multitask learning thing and increasing
the number of tasks and automating that process of sort of learning from the edge, discovering
the edge cases and learning from the edge cases.
That is, it's really approaching from a different angle, the general learning problem of AGI,
but the more clear approach comes from DeepMind where you have these kind of game situations
and build systems there, but I don't know.
People seem to be quite ...
Yeah, there will always be people that just won't believe it and I fundamentally don't
care.
I mean, I don't care if they don't believe it.
And it starts doing people's jobs and I don't care about the philosophical zombie argument
at all.
Absolutely, absolutely.
But do you think you will notice that something special has happened here?
Because to me, I've been noticing a lot of special things.
I think a lot of credit should go to DeepMind for AlphaZero.
That was truly special.
Self-play mechanisms achieve sort of solve problems that used to be thought unsolvable
like the game of go.
Also, protein folding, starting to get into that space where learning is doing ... At
first, it wasn't end-to-end learning.
Now it's end-to-end learning of a very difficult previously thought unsolvable problem of protein
folding.
And so, yeah, where do you think would be a really magical moment for you?
There have been incredible things happening in recent years.
Like you say, all of the things from DeepMind and OpenAI that have been huge showpiece things.
But when you really get down to it and you read the papers and you look at the way the
models are going, it's still like a feedforward.
You push something in, something comes out on the end.
Maybe there's diffusion models or Monte Carlo tree rollouts and different things going on,
but it's not a being.
It's not close to a being that's going through a lifelong learning process.
Do you want something that kind of gives signs of a being?
What's the difference between a neural network, a feedforward neural network, and a being?
So, fundamentally, the brain is a recurrent neural network generating an action policy.
I mean, it's implemented on a biological substrate.
And it's interesting thinking about things like that where we know fundamentally the brain
is not a convolutional neural network or a transformer.
Those are specialized things that are very valuable for what we're doing,
but it's not the way the brain's doing.
Now, I do think consciousness and AI in general is a substrate independent mechanism
where it doesn't have to be implemented the way the brain is,
but if you've only got one existence proof, there's certainly some value in caring about
what it says and does.
And so, the idea that anything that can be done with a narrow AI that you can quantify up a loss
function for a reward mechanism, you're almost certainly going to be able to produce something
that's more resource effective to train and deploy and use in an inference mode.
Train a whole lot using an inference, but a living being is going to be something
that's a continuous lifelong learned task agnostic thing.
So, the lifelong learning is really important, too, and the long-term memory.
So, memory is a big weird part of that puzzle.
And we've got, again, I have all the respect in the world for the amazing things that are
being done now, but sometimes they can be taken a little bit out of context with things like
there's some smoke and mirrors going on, like the GATO, the recent work, the multi-task learning
stuff. It's amazing that it's one model that plays all the Atari games, as well as doing all
of these other things, but it didn't learn to do all of those. It was instructed in doing that by
other reinforcement learners going through and doing that. And even in the case of all the games,
it's still going with a specific hand-coded reward function in each of those Atari games
where it's not that, you know, how does it, it just wants to spend its summer afternoon playing
Atari because that's the most interesting thing for it. So, it's, again, not a general,
it's not learning the way humans learn. And there's, I believe, a lot of things that are
challenging to make a loss function for that you can train through these existing conventional
things. We're going to chip away at all the things that people do that we can turn into
narrow AI problems. And billions of, probably trillions of dollars of value are going to be
created by that. But there's still going to be a set of things. And we've got questionable cases
like the self-driving car where it's possible, it's not my bet, but it's plausible that the
long tail could be problematic enough that that really does require a full-on artificial general
intelligence. The counter argument is that data solves almost everything. Everything is an
interpolation problem if you have enough data and Tesla may be able to get enough data from all
of their deployed stuff to be able to work like that, but maybe not. And there are all the other
problems about, like, say you want to have a strategy meeting and you want to go ahead and
bring in all of your remote workers and your consultants, and you want a world where some
of those could be AIs that are, you know, that are talking and interacting with you in an area
that is too murky to have a crisp loss function, but they still have things that on some level,
they're rewarded on some internal level for building a valuable to humans kind of life
and ability to interact with things. See, I still think that self-driving cars solving that problem
will take us very far towards AGI. You might not need AGI, but I am really inspired by what
autopilot is doing. Waymo, so some of the other companies, I think Waymo leads the way there,
is also really interesting, but they don't have quite as ambitious of an effort in terms of learning-based
sort of data-hungry approach to driving, which I think is very close to the kind of thing that
would take us far towards AGI. Yeah, and it's a funny thing because as far as I can tell,
Elon is completely serious about all of his concerns about AGI, you know, being an existential
threat, and I tried to draw him out to talk about AI and he just didn't want to, and I think that,
you know, I get that little fatalistic sense from him. It's weird because his company
could very well be the leading company leading towards a lot of that, where Tesla being a super
pragmatic company that's doing things because they really want to solve this actual problem.
It's a different vibe than the research-oriented companies, where it's a great time to be an AI
researcher. You've got your pick of trillion-dollar companies that will pay you to kind of work on
the problems you're interested in, but that's not necessarily driving hard towards the core problem
of AGI as something that's going to produce a lot of value by doing things that people currently do
or would like to do.
