Topo Deep Learning Geometric
Open sidebar
Topo Deep Learning Geometric
GPT-4
Summarize: hello and welcome

it's July 17 2023. we're here in active

inference math stream number 5.1 with

Chris Bodner on topological deep

learning graphs complexes and sheaves so

thank you for joining Chris looking

forward to your presentation and

discussion

yeah thanks thanks all for uh having me

so yeah as I was just saying

um this is um my my PhD pieces which

I've finished a couple of months ago

it's also kind of a public online so you

know if you want to go into the details

just kind of look this up on the

internet and uh you should be able to

find it easily uh obviously there's lots

of stuff in there so I kind of try to

give an overview uh today and maybe also

go in a little bit more detail in

certain aspects

um since there's you know not a lot of

time to go through everything

um

so yeah now I'm on my Microsoft research

so this is some basically all work that

I did in the past then when I was at the

University of Cambridge

um all right so let's let's get uh

started

um right so let's let's start very

easily

um now I'm actually not sure exactly

what's kind of the background of the

people who are watching uh but uh in

machine learning there's uh there's all

these kind of subfield that emerged a

few years ago which is called geometric

deep learning

um

which is essentially looking at how to

apply this kind of deep learning neural

network architectures on data living on

all sorts of

um

you know kind of structures or

geometries or or spaces if you want

um and this has a lot of applications

especially in the Life Sciences and

there's kind of been a lot of instances

of this in kind of you know very famous

Publications and all which you see here

but you know just to give some examples

for instance if you if you have proteins

or molecules or things like that they

usually represent it as graphs

um and you kind of have some data living

on these graphs like kind of the

properties of certain atoms and and so

on

um so so these kind of things

um and so far this kind of

um

species or this kind of problems

learning problems if you want they have

been approached mostly kind of with a

geometrical mindset as the kind of name

of the of the subfield

um also mentions

um but

um something that you know I would argue

is that geometry is not everything that

you need and there's kind of other

non-geometrical aspects when you are in

such a setting
1 / 1
ChatGPT
ChatGPT
This transcript captures a presentation given by Chris Bodner on July 17, 2023, in the "Active Inference Math Stream 5.1" concerning his PhD research on topological deep learning, graph complexes, and sheaves. He explains his research, now available online, was completed a couple of months ago, prior to his move to Microsoft Research.
Bodner provides an overview of his work in the field of geometric deep learning, which deals with the application of neural network architectures on data living in various structures, geometries, or spaces. The subfield has become significant over recent years, with numerous applications in life sciences.
Bodner uses proteins and molecules as examples, explaining that they're generally represented as graphs with various properties and data. Most of the learning problems in this field have been approached using a geometrical mindset. However, he asserts that there are other non-geometrical aspects to consider in such a setting.
1 / 1
and this is kind of quite obvious once

you realize that the spaces that kind of

show up in in the field and in many

applications uh they are very

heterogeneous so as I mentioned for

instance you could have graphs that you

know could represent anything in this

case it's the caffeine molecule that you

see here on the left and you want to you

know have some models that predict

certain properties of this molecule and

so on uh but for instance you can have

grits and we see grids all the time and

data living on grades and I'm referring

to images videos they are all kind of

you know pixels living on a grid

um and then you can have more

sophisticated things you could have some

meshes for instance uh they're all over

in computer graphics and then you could

have some sort of manifold so for

instance if you're doing maybe weather

modeling or something you you know we

live on a sphere topologically speaking

um so you might want to model your data

as living on a fear and and so on

um

but nonetheless uh even if these places

are kind of

geometrically kind of heterogeneous and

some of them not don't even have a

geometrical structure in kind of a

strict mathematical sense they all have

What's called the topological structure

which is kind of like kind of a weaker

kind of structure uh but it's it's kind

of more General

um and I'm going to talk a bit uh in a

few seconds about what that means but in

general you do kind of mathematical

physics you kind of have a ladder of

structures where kind of uh you know you

kind of keep building on top and the

more structure you have the more

sophisticated things you can do and so

on and you know at the basis of this

diagram just have sets it's just kind of

a collection of elements with no kind of

extra structure uh and then you kind of

keep going up in this uh leather and you

you add stop stop on top of sets and so

on

um and as I was saying kind of most of

the work is kind of focused on maybe the

top levels of this hierarchy but it's

kind of topological level it's a kind of

uh you know the weakest kind of level

you can add on top of that has kind of

been neglected to a large extent

um and part of what I've been doing in

my PhD thesis was essentially looking at

this kind of learning problems on these

kind of spaces from a more topological

perspective and you know kind of Fight
1 / 1
ChatGPT
ChatGPT
Bodner continues to elaborate on his research, pointing out the heterogeneity of the spaces used in the field, which can include graphs, grids, meshes, and manifolds. These structures represent a wide array of applications, from molecular properties, computer graphics, to weather modelling.
While these spaces might be geometrically diverse, and some may lack a geometric structure in a strict mathematical sense, they all share a topological structure, a broader, less restrictive structure that still conveys information about the space's characteristics.
The mathematical physicist's approach involves a "ladder of structures," each level adding more sophisticated elements and structures on top of the last. Sets, simply a collection of elements, form the base level of this hierarchy. Most work in the field focuses on the top levels of this ladder, with the topological level - the simplest addition beyond basic sets - largely neglected.
Bodner's PhD research sought to address this gap by studying learning problems on these spaces from a topological perspective.
1 / 1
The Filling uh these blanks

um so this is kind of the overview and

now okay so if we are to adopt this

topological perspective what would that

actually mean or how would that look

like

um I guess it could look in different

ways but in what I've done in my faces

it looks kind of like this so we have

horizontally we kind of have a space

um that could be anything it's just kind

of an abstract space you know could be a

grid or things like we've seen uh in the

previous example and kind of vertically

attached to the regions of the space we

have data

um so data is kind of this vertical

component and you kind of see these uh

Flags kind of being you know kind of

anchored in these regions so that kind

of signifies you have some data that's

kind of associated with that region

um so that's kind of the high level

perspective

um and I'm going to make that a bit more

concrete a bit later

um and there's kind of two essential

things about this picture

uh first thing is locality so the data

is uh attached to some regions of this

space this topological space

um and and in that sense it's local so

it's kind of associated with the region

and maybe to give a concrete example if

you kind of have a temperature sensor

somewhere in space right you could think

of whatever that sensor is measuring is

kind of a property of kind of the

immediate surrounding of of you know

around that sensor so it's kind of

describing some property of of a region

in space

um so so that'll be kind of a good

example
1 / 1
ChatGPT
ChatGPT
As he further expounds on the topological perspective, Bodner provides a representation of how this perspective functions. In this conceptualization, the space, which can be any abstract structure like a grid, is represented horizontally. The data related to each region of the space is vertically associated with the respective region. This conceptual setup denotes two essential characteristics.
The first characteristic is locality. The data attached to specific regions of the space are local to that region. An illustrative example given is a temperature sensor located somewhere in the space. The temperature recorded by the sensor can be considered as a property of the immediate surroundings of the sensor, hence reflecting the concept of locality.
1 / 1
um and another kind of Axiom uh that

that uh we're starting with is that

um the space has structure

um so the space has kind of it's kind of

made up of various regions and these

regions intersect in various ways

um

so the and and that implicitly also

makes our data structure because the

data is attached to these regions so so

there is kind of some some structure in

in the data

um all right so that's kind of the

picture and actually many of these

things relate to category Theory I'm not

gonna go in depth into this because it's

kind of sophisticated and I'm not an

expert myself in category Theory

um but kind of the the high level here

is that category theory is kind of a

nice way to uh translate between

different structure in mathematics and

kind of you know discuss about

properties of certain kinds of objects

and translate that to some different

kinds of objects and find all these kind

of relations and connections and a

concrete example is for instance if you

want to study some manifolds on surfaces

you could associate some groups to these

surfaces and then any sorts of relations

between this kind of surfaces also

translating some relations about these

groups or some other algebraic

structures so you could study these

manifolds by doing algebra instead of

doing you know geometry or topology or

something else
1 / 1
ChatGPT
ChatGPT
The second principle in Bodner's topological perspective is that the space itself has structure. This space comprises various regions, and these regions intersect in diverse ways. Since the data is linked to these regions, this inherent structure in the space translates into structure in the data as well.
He also hints at connections between his work and category theory, a branch of mathematics that provides a unified framework for understanding and translating between different structures. It enables the exploration of relationships between various mathematical objects and translation between them.
As a concrete example, he explains how one can study manifolds or surfaces by associating them with certain groups. Any relations between these surfaces translate into relations about these groups or other algebraic structures. This way, one could use algebra as a tool to study these manifolds instead of relying solely on geometry or topology.
1 / 1
so also in this case this kind of

manifests in the fact that we kind of

translate these

um from spaces to data so kind of

because we associate the regions in in a

space certain kinds of data this is kind

of how these translation manifests in in

what I've just described in the previous

picture so you could think of this as

some sort of translation or mapping from

from spaces and regions in that space to

kind of data attached to that space

um but yeah I'm not going to go in a lot

of detail into this but it's just kind

of to you know to keep in the back of

your mind

um that there's there's this stuff

lurking in the background

um all right so this is I I promise this

is the early math definition I'm giving

in this talk and then and then I'll stop

um but just because I'm mentioned in

topological space spaces quite often I

just wanted to kind of

um give this axiomatic definition

um which might sound sophisticated but I

have a picture at the end and hopefully

it'll be clear

um so yeah so what is it um so it's just

a set as I was saying we start with set

and we put stuff on top right so you

start with a set and then you also have

a collection of subsets of this set

um called the open sets that need to

satisfy a certain axioms so you could

think of these open settings kind of

regions of the space very informally

speaking

um so something that uh kind of has to

be satisfied is that well the empty set

and the set itself

um need to be open set so in in some

sense you could think of this as saying

the set itself is a region of that space

uh very informally

um which you know it's a kind of uh say

obvious and then there's some some

kind of constraints about intersecting

and taking units of this region so uh if

we take the intersection of two regions

we should get another one of these

regions

um and if we take a union of these

regions we should take we should get

another region
1 / 1
ChatGPT
ChatGPT
Bodner delves into the formal definition of a topological space. At its most basic, a topological space is a set where certain subsets, referred to as open sets, obey specific axioms that reflect their nature as "regions" of the space.
The axioms that these open sets must satisfy include:
The set itself and the empty set must be considered open sets, informally interpreted as the idea that the set itself is a region of the space.
Intersections and unions of open sets must also be open sets. This axiom means that if two regions (open sets) intersect, the intersection itself forms another region. Similarly, if you combine (union) multiple regions, the resulting entity should also be a region of the space.
These axioms and the idea of a topological space form a basis for more complex discussions in topology and the related mathematical frameworks that Bodner is discussing.
1 / 1
there is some constraints

again like okay how big this

intersection should be there should be

finite intersections but you could have

infinite unions but that's a

technicality and we can just skip

anyways but to see a picture so on the

left you just see the set X itself and

here I put like a potential neighborhood

structure kind of like open set

structure on this space so we have an

open set here another open set V by this

Axiom the intersections would also be an

open set so yes it is intersection in

the middle being another opposite and

then the set itself is another open set

so it's just kind of a splitting stuff

into regions kind of

um you could think of it like that

um all right so so this is this is a top

logical space

um and now let's add data right so we

we mentioned that we have data and then

we put we sorry we have a space and then

we add data on top uh so so far we've

seen how a topological space looks like

now let's add this kind of vertical

stuff these flags that you saw before we

we just put some data on top of these

regions

um and if we put data on all the regions

of the space on all the open sets

um we get these uh

structures that in categories here like

in algebraic topology also geometry they

are called pre-shapes

um which which sounds very fancy but all

it's just kind of a definition of uh

what I was already describing is

actually you have some data for each

region these these F of so for instance

for uh bridge and U you have this F of U

which is kind of the data attached to

region you so you can think of f of U

some some set with with describing the

data that lives there in that region

um but there's also an extra thing you

you have some some sort of maps going

between these kind of uh pieces of data

um and these are called restriction maps

and and why is that it's kind of they

provide you a way to uh kind of zoom in

if you want right like you have you have

the data attached to the whole set X and

then you could think okay how do I take

this data how do I go from this data to

data on a smaller region on on X so it's

kind of a way to zoom in on that data in

uh essentially and I'm going to show

some some example in a second so these

are called

um preaches and just to see an example

our space here is kind of one of the

simplest kind of space you could think


of just 1D or horizontal line right this

is the real line

um and then you have some regions which

are just given by open intervals uh on

on these uh

uh yeah on the Via line and then some

pieces of data could be functions like

continuous functions on on those regions

so here is like some sample data on on

this first interval here's another

function on the second interval here's

some data on this third interval and

actually in this case it happens that

all these

um functions agree on the overlap so

where these regions overlap they take

the same values and we can actually glue

them together on a single uh in a single

function over the entire region right
1 / 1
ChatGPT
ChatGPT
Bodner explains the concept of pre-sheaves, which in algebraic topology is a way of associating data with each region of a topological space. This pre-sheaf structure involves not only assigning a set of data (e.g., a function) to each open set (or region) in the space, but also includes a system of "restriction maps" that allows you to "zoom in" on the data associated with a smaller region nested within a larger one.
He then gives a practical example using a simple 1D real line as the topological space. Each region of the space is defined by an open interval, and the associated data could be continuous functions defined on those intervals. He demonstrates three different regions (intervals) each with its own function.
The key point here is that on the overlapping sections of these intervals, the functions agree — they yield the same values. This agreement allows us to "glue" these separate pieces of data together into a single function defined over the entire space.
This example helps visualize how pre-sheaves can associate data with specific regions of a space and how that data can be integrated across the entire space when certain conditions are met.
1 / 1
so this is just an example of

um of a pre-shift and it's called the

pre-shift of continuous functions

um so our data in this case is

continuous functions and the space is

just the real line

um and we put these functions on top to

realign

um and but it turns out because of this

kind of special property uh that we can

kind of glue data and we uniquely get

some other piece of data right we we can

take these three pieces it's just

exactly like a like a puzzle right we

put this thing through things together

and we get a fourth thing which is kind

of a single function uh where we just

overlapped these these functions right

um and

uh these uh appreciates that satisfy

this kind of properties where you can

kind of glue them to get a unique piece

of data they are called sheaves

um and uh basically the pre-shift of

continuous functions is actually a shift

um so this is kind of a way to formalize

data attached to these things

um it's gonna get less technical in a

second

um so just to give more examples so for

instance in this way we could describe

data on a sphere and let's say if this

data is just some Vector field on this

here so let's see if this is earth right

this could be some wind Vector field

right like if we do weather modeling or

something you just have a vector field

describing the wind on on the surface of

Earth right

um and you might want to do some machine

learning on top of this where this kind

of vector field has a sheath structure

and you could think of it as a chief

because you know if you have like some

Vector field on the red region a vector

field on the yellow region I can kind of

glued them together uniquely if they

overlap if they agree on the overlap and

we get the vector field on this bigger

region
1 / 1
ChatGPT
ChatGPT
Bodner provides another example of a sheaf, this time using a vector field on a sphere to represent wind on Earth's surface. In this context, each region of the sphere (Earth) is associated with a vector field (wind data).
He explains that if you have data for two overlapping regions (say, represented by the colors red and yellow), you can 'glue' them together to form a single vector field for the larger region that includes both. This is possible because the data from the two regions agree on the overlapping part, hence creating a coherent and continuous vector field over the larger region.
This example serves to illustrate that sheaves can be applied in practical situations to unify data from overlapping regions into a larger, consistent dataset. This structure can be especially helpful in situations where you want to merge local data to make global predictions or understand larger-scale behavior, such as in climate modeling or machine learning tasks related to geospatial data.
1 / 1
but something that's quite nice

is that even if we have a very very

different kind of space namely a graph

which is very different from a sphere in

in all points of view and we can still

apply the exact same kind of axioms and

terminology and and kind of definitions

and we can have a shift over a graph so

in this way we could have for instance

some features Associated to the nodes of

the ground some features associated with

the edges of the graph and there's

another node which has its own features

and this is actually the exact same

thing we have in graph machine learning

um so this is quite nice for this kind

of topological perspective allows us to

do is you know we we kind of have a

unified way of thinking if you want

about very kind of heterogeneous spaces

and we can model on all of them data

attached to them by using this kind of

shift terminology and other ways as well

but I'm not going to that in this talk

um all right so this is kind of an

overview of what I've been doing in my

thesis

um and just to kind of dive a bit deeper

into this

um I just yeah I wanted to go in uh into

one paper that we did at Europe's uh

last year yeah so this was last year

um on uh what's called shift diffusion

so essentially how can we use what I've

just described to do some useful stuff

when doing machine learning on graphs

um all right and this was a

collaboration with Francisco de Giovanni

but in Chamberlain uh Pietro Leo my

advisor and Michael bronstein

um okay so before I dive into this I

just want to give some background in

case people are not familiar with this

um so the kind of

favorite architecture of people doing

machine learning on graphs these days

are these things called graph neural

networks

um which are actually very simple kind

of models so

in this setting you you have some

features so each node in your graph will

have some features this is what is h

Vector here denote so it's

um the vector H associated with node a

at layer or time D whatever so you have

some features for each of these nodes

and what you're doing each of if you are

at a certain note you want to kind of

compute a new representation or a new

new features for for this now that the

next layer

um

so since you're learning representations

um and what the graphene networks are

doing this node will receive a message

from all the other

nodes that our neighbors with this node

um and this these uh message can also be

passed you know can use on neural

networks in there but essentially some

processing of these features of the

neighbors and these are aggregated into

this message so here in in green

um and then there is this is passed

through some update function

um that combines the message from the

neighbors with the old representation of

this node and it gives you a new

representation kind of the next layer

um and this happens for all the nodes

right so then you get some new

representations for this node and this

is one layer and then you kind of keep

repeating this for as many layers as you

like

um so this is kind of how you do deep

learning on on graphs
1 / 1
ChatGPT
ChatGPT
The speaker then moves on to discuss Graph Neural Networks (GNNs), which are a popular model in machine learning for processing graph data.
In a GNN, each node in the graph is associated with a set of features, which are represented as a vector. The fundamental operation of GNNs is the message-passing mechanism, which is used to update the feature vectors of nodes.
For a given node, this mechanism operates as follows:
The node receives "messages" from all its neighboring nodes. These messages are often based on the feature vectors of the neighboring nodes and are processed (e.g., transformed via a neural network or some other function).
These messages are then aggregated to form a single message.
This aggregated message is combined with the node's current feature vector to create a new feature vector. This process is typically achieved through an update function, which might be another neural network.
This process is repeated for each node in the graph, and it can be performed for several "layers" or iterations to propagate information across the graph.
By iteratively updating the feature vectors of the nodes based on their neighbors' features, GNNs can learn meaningful representations for the nodes and the graph as a whole, making them a powerful tool for tasks like node classification, link prediction, or graph classification.
1 / 1
it's kind of a very very simple uh

recipe and most models actually vary in

the way they kind of compute these

messages messages and in the way these

update function is designed uh but

that's kind of the parameter is the most

of these models use otherwise they all

kind of respect this framework and work

in this kind of particular way

um and to give you maybe an example for

why you would want to do this

uh you might want to do note

classification this is kind of a classic

problem in graph machine learning there

are others but I'm just going to talk

about this because it's easier

um so you have a graph and this graph

um has nodes that have different labels

here there's just two kinds of labels

these orange and and blue

um

and you you have some edges between uh

these nodes and what you want is you

want to do this kind of a message

passing that I was describing to compute

some representations for these nodes

where you can easily classify you can

easily classify the blue and the orange

notes

um now something that's quite

interesting is that for many kind of

graph neural networks

um

depending on the properties of these

graphs and how these kind of different

nodes are connected their performance

might vary quite a lot

um so in particular they're affected by

this property called heterophili

um so this kind of a measure of how much

uh opposites attractive you want right

so uh it has a very simple formula

um basically you take the number of

edges between orange and blue notes and

you divide by the total number of edges

right so so basically you kind of check

how many connections we have in this

graph between things that are quite you

know they are opposite to each other

versus you know connections that are

between similar kind of nodes

um so if you have this kind of a lot of

these kind of heterogeneous connections

then you have very high heteropoly

um and it turns out that many Graphics

are not exactly struggle in that setting

it's very hard to to classify things in

that setting

um and iteratively you could kind of

also you know figure out why because you

know you you could easily apply to some

kind of reasoning where you know oh this

note looks a lot like these other node

is connected to so they kind of must be

in you know in the same Community if you

want or in the same label but it's much

harder to do that when you know all

things are kind of different from

um from each other

um and and this community is kind of

don't form right even visually if you if

you see a graph and it has a nicely

clustered communities you know it's

quite easy to draw a line between those

and say oh this is uh communities

another Community but if things are very

mixed

um then it's quite challenging and it

turns out it's also challenging for

these models uh not just for kind of our

intuition when when you know we would

have to do this

um so this is kind of some problem where

um this perspective this topological

perspective I was mentioning uh will be

used to to you know do some useful stuff
1 / 1
ChatGPT
ChatGPT
The speaker discusses a common application of Graph Neural Networks (GNNs), which is node classification. In a graph, nodes may represent different categories, like orange and blue nodes in the example. GNNs can be used to learn meaningful representations of these nodes to classify them effectively.
The performance of GNNs can vary significantly based on certain properties of the graph. One such property is 'heterophily', which measures the proportion of connections (or edges) in the graph that connect nodes of different categories (or labels). High heterophily means that there are many connections between different types of nodes, which can make classification more challenging.
Traditional GNNs can struggle in situations with high heterophily, as they rely on the principle that connected nodes are likely to have the same label, which might not be true in high heterophily scenarios. The speaker implies that a topological perspective, one that takes into account the structure of the space (the graph) along with the data (the node features and labels), might be useful in such situations.
1 / 1
okay so coming back to toshibs on graph

um and at this point I think you can

largely forget what I mentioned in your

introduction uh or if there's something

give misunderstood there uh you know we

kind of start from zero bit here so

um there's no problem so

on the left

um you just have a graph

um which is kind of the incident

structure of a graph I just don't hear

the simplest possible graph that has two

nodes V and U and then there's an edge

between them so this is just a graph

with one Edge then that's all that's

going on here and I've just represented

it by kind of in this kind of incidence

structure kind of way right node V is

incident to node e and node uh sorry

Edge e and node U is incident to h e so

this is just an incident structure and

what is kind of triangle symbol we're

showing is just these incidence

structure is just the way to symbolize

this incidence relation if you want

um okay so this is just a graph right

and a way we can kind of think of of

fields on graphs is just mapping these

graph structures so this is kind of this

categorical Theory translation we

translate this graph into something else

which looks you know very similar the

structure is kind of the same it's just

kind of the meaning of these uh things

change so for each node V we have

um here this will be a vector space so F

of e is a vector space for each node

here we have F of U which is another

Vector space uh for each Edge e we have

this Fe which is another Vector space

um so all nodes have their own Vector

spaces and the features Associated to

those nodes leaving those Vector spaces

so basically for each node we have a

vector space of features that that's all

that's going on so far and also these

arrows that these incident relations

also translate into something and they

translate into the obvious thing linear

Maps so if these are vector spaces then

these things should be linear Maps or

just some matrices essentially right so

for each Arrow you see here we have a

matrix

and

um

uh see yeah and something uh that I'll

I'll argue and show in a few slides is

that basically message passing

um

uh is very similar on graphs it's very

similar with group actions in in group

Theory
1 / 1
ChatGPT
ChatGPT
The speaker provides a more detailed explanation of sheaves on graphs. They begin by using the simplest possible graph as an example, one with only two nodes (V and U) connected by an edge (E). This graph is presented in an incidence structure, where nodes V and U are connected to edge E, represented by the triangular symbol.
The speaker then applies a categorical theory translation to the graph, converting the nodes and edges into vector spaces. This means that each node (V and U) is associated with its own vector space, F(V) and F(U), and the edge E is also associated with a vector space, F(E). In this context, the features associated with each node live in these vector spaces.
The incidence relations, or the connections between the nodes and edges, are translated into linear maps (matrices). This transformation creates a linkage between these vector spaces and reflects the structure of the original graph.
The speaker suggests that message passing in graph theory, a critical process in graph-based machine learning models such as Graph Neural Networks (GNNs), is quite similar to group actions in group theory. They promise to further elaborate on this connection in the upcoming slides.
1 / 1
A list of everything discussed so far.
1 / 1
ChatGPT
ChatGPT
Sure, here's a summary of everything we've discussed so far:
Pre-sheaf and Sheaf Concepts
: We began with a basic explanation of the pre-sheaf and sheaf concepts. A pre-sheaf of continuous functions involves applying these functions to the real line, realigning and modifying the data. If these functions have certain properties, such as the ability to "glue" together to yield unique pieces of data, they can form a sheaf. The pre-sheaf of continuous functions is a sheaf.
Sheaf Structures in Diverse Spaces
: We discussed how the sheaf structure can be used to describe data on different types of spaces, such as a sphere (like Earth) with a wind vector field or a graph. For instance, in the case of a graph, different features are associated with the nodes and edges, similar to graph machine learning.
Machine Learning on Graphs
: We delved into the subject of machine learning on graphs, where Graph Neural Networks (GNNs) are popular models. GNNs work by passing messages between nodes and updating their representations based on these messages. A common application of this process is node classification.
Heterophily Impact
: We discussed the impact of heterophily, or how nodes with different labels are connected, on the performance of many GNNs. High heterophily, which corresponds to numerous connections between dissimilar nodes, can pose a significant challenge to these models.
Sheaves on Graphs
: We began to explore the concept of sheaves on graphs, using the simplest possible graph (two nodes connected by one edge) as an example. We applied a categorical theory translation to the graph, transforming the nodes and edges into vector spaces and the incidence relations into linear maps (matrices).
Correlation between Message Passing and Group Actions
: Finally, we touched on the idea that message passing in graph theory (which underpins machine learning on graphs) is akin to group actions in group theory.
The discussion is ongoing, and more points will be expanded upon in the upcoming part of the talk.
1 / 1
let me explain exactly why so we

kind of can think of the what we have on

the left the these arrows in from the

incidence relation they could think of

these errors as kind of uh some patterns

we can press so what do I mean by that

so if we have this node V on the left

right and this e now if we have some

features some feature living in fov

right we could just kind of press this

arrow button here and then if we

multiply this matrix by this feature we

will get an edge feature

um so it's kind of like

um if you go along this Arrow this

Matrix will multiply this feature and

this vertex feature and it will give you

an edge feature

um so if you think of these arrows as

kind of giving you some sort of actions

that you can play with to move features

from vertex to Edge and and Edge to

vertex so in this case it's kind of a

left action right so this is what I'm

saying I'm taking this R which is uh

this one here and I act on some features

of node V so this HOV that lives here

right

um and how I do that is I just take this

Matrix this Matrix associated with this

arrow and I multiply this Vector HOV so

just Matrix times Vector that's all and

then we get an edge feature so this is

just kind of a way to to move from here

to here so this already kind of looks a

bit like message passing right we're

kind of passing a message from this

vertex to this Edge

um but now we also need to pass a

message from this Edge to these other

vertex used so we kind of you know we

need to get from V to U

um so and we did that by passing through

e so by doing that we could do that by

kind of going in reverse so we could

have a right accent

um where instead of applying this Matrix

we applied adjoint Matrix so it doesn't

the transpose Matrix so in instead if we

want to go from here to here

um instead of applying this Matrix we

apply its transpose because we want to

go the other way around

um so this is this is a if we compose

these things then we can move features

from V to U so this is just the way

uh that kind of we can apply these

actions uh to to do message passing and

these are called shift actions or

pre-shift actions and uh I'm gonna now

say what's kind of the relation with

between this and what we have in group

uh Theory

um so one way to

represent the group is by kind of having

some sort of graph

um like here on on the left so

uh we kind of have some star object it

is just kind of a dummy thing there uh

but all the group structure is in these

arrows right so for each group element

so let's say this G is a 90 degree

rotation for instance that that's uh

let's say we do have a group of

rotations that just to have some

concrete example these uh

so these Arrow could correspond to a 90

degree rotation we have another arrow

that does the opposite minus kind of 90

degree rotation that's the inverse of

that transformation

um so this is kind of the structure of

this group and if we have we also do

this kind of similar kind of translation

as we've just seen

um so basic basically we Define a

pre-shift on this group uh we map this

star to a vector space so the star kind

of replace is the vertex uh we had

before now we just have a single vertex

uh and it's just these arrows we have um

so the star is not this Vector space

that you showed here in in blue right

um and now if we actually do group

actions which are kind of a very well

established Concept in group Theory

um

well for instance if you want to act on

this vertex oh sorry not vertex on this

vector v right here you have a vector in

this Vector space and you want to act on

it by this group transformation G so

essentially you want to press this Arrow

so you you apply some action on it

um then what you do is well because of

this translation this G has been mapped

to some Matrix which is the rotation

Matrix the corresponding rotation Matrix

and you apply this rotation Matrix on V

and you get like a 90 degree rotation

here

um so this is what what's going on this

kind of vertical Vector is showing the

rotated Vector

um

so this is completely analogous with

what we've seen on the previous slide

um is uh this is how kind of chiefs

connect these kind of actions so

essentially what you could think of as

message passing is same as group actions

in group Theory but you just replace

this group with a graph so it's kind of

analogous to that right so uh it's just

kind of a different kind of translations

where we replace the object on the left

uh now it looks like this is a graph

this is a group right so but kind of the

rest stays exactly the same so

um you could and this kind of gives us a

way to formalize in a way by looking in

this topological perspective to connect

all these kind of symmetries and things

like that that have been explored quite

a lot in machine learning through

message passing on graphs and to see one

way in which in which they are related

um okay so now in my circuit this was

all very sophistically and nice but you

know what what is this going anywhere

basically

um and I'm just going to show you kind

of a very forward example there there's

more about you know the time is limited

um and something we we showed is that

uh as I was saying in the beginning many

graphene networks kind of struggle in

these heterophilic graphs and what we've

shown is that

um no matter how heterophilic or kind of

weird your your graph is uh you can

always kind of find some shift structure

essentially kind of a message passing

neural network that

if you use sufficient layers it will be

able to disentangle the classes of the

of the nodes right so just to show you

in this picture what you have here on

the very far left

um the colors of the notes they they

show the clasp so we have three there's

three colors here so three classes right

um and this is kind of the graph in the

beginning and the position of the nodes

in this box denotes the features so

that's a way to kind of just to

visualize the features

um the the position the 2D position is

actually the 2D feature Vector of each

node

um and you can see in the beginning

everything is kind of super messy and

entangled right like if you want to

classify these notes it's kind of very

hard because the representations the

initial representations are are very

messy and kind of uh intertwined

um but as we stack more layers of a

particular kind of key for a message

passing model

um you see how progressively these

classes get you know kind of more decent

angle and more

at each new layer and so that kind of

this representations kind of collapse

and they form this kind of clusterings

right and then when you get with

something like at the end you can kind

of see these three communities very

clearly and it's extremely easy to

separate

um and kind of the essence behind this

results was we we showed for different

kinds of problems what sorts of Chief or

message passing models you need by using

the theory to solve kind of problems and

um this is quite important because it

kind of shows you some important bits

and pieces in the architecture that you

might want to to kind of change or or

use in order to solve certain kinds of

problems and we also had some sort of

impossibility results so if you use a

graphing a network of some kind you

can't solve this problem or you'll

struggle to solve this problem and we

also saw okay if you use some more

general ones then you might have a

chance

so this is kind of some the you know

high level view behind

um these theoretical stuff

um and what we're actually doing in

practice

um is to essentially learn these message

passing functions or to learn the sheet

or these matrices so in in practice like

when someone gives you a node

classification tasks is very hard to

know beforehand what what exactly is the

right

um the right C for the right message

passing model uh to sort that task

um and what we do is actually we learn

that from data so we learned these

matrices that do the message passing we

learn them from data

um by passing some using some neural

networks which are shown here in red

um and and then you learn how to kind of

transfer features between these Vector

spaces and kind of move them around so

this is just showing how these vectors

which are features of these nodes and

edges how they kind of moved around by

depends going through the Via this

matrices just some Matrix

multiplications
1 / 1
ChatGPT
ChatGPT
The conversation so far has gone into deeper details of sheaves on graphs and how they relate to group actions and message passing in Graph Neural Networks (GNNs). Here are the points covered:
Sheaves and Actions on Graphs
: The speaker elucidated how actions can be applied on features of nodes and edges in graphs. An action is a matrix multiplication applied to a feature vector that transforms it from one space to another. This operation can be thought of as 'pressing an arrow button', which moves features from vertex to edge and edge to vertex. These movements, akin to 'message passing' in graphs, are termed as sheaf or pre-sheaf actions.
Group Theory and Sheaf Actions
: The speaker drew a parallel between sheaf actions on graphs and group actions in group theory. As an example, a group represented by a graph, where each arrow denotes a group element (e.g., a 90-degree rotation), was considered. The actions in this context would be the rotation operations applied to a vector.
Analogous Nature of Message Passing and Group Actions
: The speaker highlighted how message passing in graphs is similar to group actions in group theory. This connection offers a way to formally understand symmetries and other concepts explored in machine learning through message passing on graphs.
Applications and Practicality
: The speaker presented a practical example showing that no matter how heterophilic or complex a graph is, a suitable sheaf structure or message passing model can disentangle the classes of nodes. This operation progressively separates intertwined representations into distinct clusters.
Learning Message Passing Functions
: In practice, it is difficult to pre-determine the most suitable sheaf or message passing model for a given task. So, the speaker's approach involves learning these functions from data using neural networks. These learned matrices then allow for effective transfer of features between vector spaces.
1 / 1
so that's kind of a high

level here behind this model and we

evaluated this on some kind of real

world uh heterophilic data sets

um uh where you you have to classify

nodes based on kind of various

communities or different kinds of labels

and these data sets going from

um right to left they are getting more

heterophilic so in some sense more

challenging for classic architectures

and our models which are kind of

inspired by all this stuff the dimension

they score quite highly in in these

benchmarks

um and at the same time they also

revealed some or Justified some various

choices that other models in this space

have done but maybe they were not so

well Justified or maybe they had

different kind of motivations we also

managed to kind of show why various

things they were already doing why they

made sense from from the point of view

of this kind of uh Theory

um all right well that's it's all um all

I had

um yeah thanks for for listening and

yeah happy to chat more about this and

also have lots of backup slides in case

you know uh depending on how far away

you can Venture offer with these

questions

oh well awesome work thank you for the

presentation

for people who are in the live chat they

can write some questions but there's

many things I think we could talk about

so I want to start with reading a quote

from an abstract of the paper by

uh vanderlar kudal and DeVries just to

kind of ground this in the active

inference context and and really justify

why the message passing approaches that

you are describing are helping in the

active inference modeling so the paper

it's two papers it's called realizing

synthetic active inference agents and

they wrote with a full message passing

account of synthetic active inference

agents it becomes possible to derive and

reuse message updates across models and

move closer to Industrial applications

of synthetic active inference framework

so how does knowing the message passing

structure

help reuse a model across different

settings or like facilitate the

legibility of the model

right um so first of all I'm not super

familiar with the kind of active

inference literature

um so

you know you'll have to help me there a

bit in in for for to Anchor maybe the

discussions a bit more into that

um but I think if I understand correctly

the the kind of question uh you're

you're getting at is basically how how

can kind of message passing help us

generalize you know in kind of various

kinds of settings or maybe you know from

one graph to another and and things like

that and

um these these isn't kind of active area

of of research how exactly this

generalization is happening

um but something you could notice where

is something that

um for instance was shown like these

models are quite good at for instance

plotting patterns or structures uh

depending on how exactly you implement

them but for instance uh you know let's

say you have a triangle in your graph or

you know that'll be kind of a simple

structure right you you have a triangle

or some other kind of kind of Gadgets in

your in your graph like particular

subgraphs right that might show up in in

different kinds of uh you know various

graphs the graphs themselves might look

completely different from each other but

these kind of patterns might kind of be

re-emerging in multiple like local

patterns like three emerging multiple

graphs and that could help your way to

kind of generalize right like you could

see for instance if you have clicks

they're super important and kind of when

you do social network modeling and

things like that because they kind of

show this kind of um you know close

group of friends right they all talk to

each other so they can form a click like

everyone's connected to each other right

and then you you might be able to use

that in general if another completely

different social context

um where you know the these agents are

again kind of communicating in a similar

manner or connected in a similar manner

even if the kind of overall pattern is

quite different

um and it goes Way Beyond just kind of

structural similarities because there's

also features in there so there's

combinations of kind of structural

patterns and features that you know give

your even more complicated patterns

right like you might have a triangle but

then also two of the features in this

triangle are you know look in a certain

way and one that looks in another way so

so that gives you even more kind of

refinement then and uh you know even

kind of richer pattern detection

abilities

um so so you have essentially this you

know ability to kind of spot spot

patterns at multiple scales as well so

you could see this happening at in

multiple skills you could have patterns

of patterns right you could have uh

Community entire communities connected

in various patterns and so on

um and again it's kind of also research

question how do you capture these

hierarchical patterns and so on

um you know in general you have to do

more message passing if you want to

capture things that are further away

from each other because otherwise they

can't talk to each other right

um so yeah I don't know if that that

actually answered your question or if I

was kind of going in the right direction

there

oh it's great it brings up a lot of

different cool ideas like like this

patterns all the way down but totally

agree I I think we can now

perhaps explore some more specific

connections to active inference because

hopefully the the

um listenership or viewership of this

it's kind of like a two-way street like

some people may be coming from more of

your background and then learning about

active inference and generative models

as a specific system of Interest the

first time but also certainly for a lot

of people in the active inference space

like these methods coming from category

Theory have only recently come up

to I guess more prominence in Bayesian

modeling at least where we are so it's

kind of it's a cool connection making

yeah um I think one of the biggest touch

points off the bat was like you

mentioned multiplying a matrix by a

vector and interpreting that as an edge

so just in the in the inference part of

the generative model of about sensory

observations we always talk about the

thermometer observation and then the

underlying hidden State temperature

so that like exactly describes that case

and that's why we can represent the

active inference generative models the

perceptual parts and the action Parts in

terms of matrix multiplication it's why

the Matlab code for generative models

does look mostly like matrix

multiplication and it can all be done

explicitly that way so

how does that feed like are there models

that don't have this feature

or what do we gain by having like all of

our edges defined as

um appreciative action with a matrix and

a vector

in this setting of agent generative

models with perception and action

like once yeah yeah any thoughts on that

yeah I think

essentially kind of the the graph

structure is kind of telling you these

things interacting in some way right so

um there there's some communication

between these vertices if you you know

if we're kind of in a graph setting

right

um and then kind of what the Sheikh is

giving you or or any message passive

model essentially is is expressing a way

that in which way that connection should

manifest in the model or in what way

that connection should be used to

process information

um so in this case I was mentioning okay

we you have like linear Maps because you

could go on the type if your type of

data or vector spaces then this

transformation will be some sort of

linear Maps

um

but it doesn't necessarily have to be so

for instance it could go to any

nonlinear transformation right like if

and this is what's happening in general

in practice

um you know if you if you have a

neighbor the message coming from that

neighbor could be modulated by any sort

of transformation you you want so it

could be linear it could be non-linear

it could be

something I don't know you you can

specify it basically uh but essentially

you could think of this as

you have a structure level telling you

who should communicate to whom

um and then you kind of have some

semantics that is kind of shift is

adding on top uh saying how should these

things communicate right like the first

thing is uh who should communicate or

what should communicate and and then the

semantics we had on top essentially

describe uh how should that

communication uh manifest uh essentially

very cool I think that Maps like exactly

to how we talk about the sparsity of

variables in degenerative model
1 / 1
ChatGPT
ChatGPT
Absolutely! The structure of the graph that you're working with can be seen as reflecting the sparsity of the connections in your model. In other words, if you have a sparse graph (meaning there are not that many edges), this can translate to a model where only a few variables interact with each other directly. This kind of sparsity is very useful when you're dealing with high dimensional data, as it can reduce the complexity of the model and make it easier to understand and to compute.
In active inference models, the connections or edges between nodes could represent the probabilistic dependencies between hidden states and sensory observations or between different hidden states. The process of message passing, where information flows along these edges, can be seen as a way to update beliefs about the hidden states given new sensory data, allowing the model to make predictions and guide action.
It's also important to note that, as you mentioned, the way information flows along these edges is determined by the specifics of the model and the type of transformation applied. In the context of active inference, these transformations would be determined by the particular generative model being used.
It's really quite fascinating to see how these concepts from category theory can be applied to active inference and help us to understand and work with these models in a more abstract, general way.
1 / 1
so here the topology of the nodes in the

graph that we want to do message passing

on are going to be describing like the

agent and the environment or like the

generative model that includes

perception cognition and action it's a

lot of people have proposed different

sparsity architectures for integrated

modeling of perception cognition action

so one example would just be like kind

of Around the Clock like action

influences environment environment

influences perception back to cognition

you could add a self Loop

blanket and different kinds of

connectivities and that defines the

sparsity topologically which is where

you showed the stack and and you were on

the second and the third levels I think

of the stack and then like What flows

it has to be described how it actually

what that edge does so what what is that

what is that that is also being provided

yeah yeah exactly and it could even go

to the extreme where you know does that

edge actually do anything so for

instance if you uh have a matrix that's

just the zero Matrix for instance

Associated to that edge you're just

going to multiply by zero or an item

that gives you zero and it's kind of

essentially pruning that edge right like

I can't get rid of it I don't want that

communication to happen but there's kind

of there's also kind of this possibility

where these kind of semantics they

override the structural level where you

say okay I don't need to communicate you

know with this other agent person or

whatever it depends on what this

vertices actually mean and in what

context in what context you are and then

there's also the case where you could do

some sort of selective pruning where

um these Matrix depending on

um so in in kind of linear algebra The

Matrix has a kernel so it's uh all the

stuff that that Matrix sense to zero so

what vectors are sent to zero right but

not everything will be sent to zero

unless you're the zero Matrix

so depending on the features of the

neighbors you could also just send some

of the neighbors to zero right and that

kind of removes those Neighbors From the

equation they just kind of get you know

you you yeah they're not they're not

factored in anymore so you kind of have

this you know it's a way to get the

sparsity I guess that that they were

also talking about uh

where you know certain only maybe a

small subset of the inputs or kind of uh

yeah only a a subset of the features are

actually kind of doing some meaningful

stuff among the neighbors and everything

else will be kind of zeroed up

yeah that makes me think of the lasso

regression which tries to set most

variables of having an impact of zero so

that a few hopefully important variables

really pop out in the analysis but also

there's there's newer techniques I guess

of attention modeling and maybe waiting

that isn't just like okay set five of

them to one and then the rest and then

the rest of them to zero like more

nuanced um so I think that sparsity with

the expressivity is basically the The

Best of Both Worlds because you do want

to have a situation where there is an

edge but the attention being paid to it

is zero so functionally

that doesn't have an update on the

belief State even though the in

principle The Edge exists

and that's why we can model situations

where like the

agent believes they have impact in the

world but actually just because the edge

in principle exists doesn't mean that it

has any given impact and so that allows

like the articulation of these models

where they factorize and keeps

interpretable motifs in terms of just

Little clusters

of uh motifs here in our case describing

the action perception and cognition

types of systems of interest but people

I believe already implicitly do this

um like they will often add an adjective

and refer to X kind of active inference

so like deep active inference with a

temporal Horizon sophisticated active

inference with this kind of nesting

and those are pointing to a given

feature but of course those features

as we're hoping should be composable

and so

this seems to be bringing

tools that are even more General than

just action perception modeling

because they're at a lower level of

abstraction than like any specific

system of interest but where this work

and kind of Timeless thinking around

cybernetic systems

come together through the active inverse

generative model as a base graph

it gets very exciting

yeah yeah and maybe it's also something

we're emphasizing here is that even if

this kind of semantic level can can get

rid of some edges right by doing this

kind of pruning something it cannot get

rid of is the computation so something

that kind of that

structural graph level forces you to do

it kind of tells you what should you

spend compute time on right because like

if

even if you're going to decide to prune

an edge you still need to decide that

which you know takes compute time so you

still need to look at all your neighbors

if you are not right and and decide what

to prove in or maybe you're done print

anything or whatever but you have to

look at every Edge and one way to look

at this is uh the kind of the graph

structure defines your computational

graph or kind of a computational yeah a

series of computational steps you have

to execute and then the kind of the

shift structure or the message passing

model actually specifies what those

steps are and you know in one particular

way they they look exactly

um and so yeah that's one point and yeah

you also mentioned attention and

actually

yeah I'm glad you did because this is

actually quite related and in in certain

ways more General than attention and

actually maybe going back to this slide

it might be a nice way to

um to see this so here basically what

happens in attention

um instead of learning these matrices

that we learned here uh in attention you

learn attention coefficients here so

just learn a scalar that's the attention

coefficient how much attention should I

pay to essentially well you know this

overall Edge let's say which will be

just a scalar what we do is kind of a

bit more complicated because you just

learn how do I transform these neighbors

so it's kind of a whole Matrix rather

than a single scalar

uh but there's also some subtle

differences but in in a in a follow-up

work we did we also combined this with

with attention and and went a bit more

General and

um that that also worked uh quite well

but the kind of underlying idea is is

very similar you want to modulate the

way it transform information based on

the information itself right so we have

this kind of one level of

um recursivity if you want or kind of

yeah that we're also alluding to in that

you know it happens in active inference

where okay you're you're so if I'm not V

right my neighbor you it has some

features and based on these features

which are XU I'm gonna find out the

Matrix that will be used to process X

here right so it's kind of very

recursive and it's what happens with

attention right based on the features of

node U I'm gonna compute an attention

coefficient that I'm going to apply to

this feature over here right I'm gonna

decide based on this feature how much

attention should I pay to it

um and here we decide how should I

process it more generally uh in a linear

way

um so you have this kind of loopiness

structure embedded in there

awesome all

bring up a few more points because I

think there's there's so many great

pieces so Toby

Sinclair Smythe who we recently

discussed his dissertation in live

stream 54.

introduced a term or Lisa phrasing the

compositional cognitive cartography and

so thinking about the compositionality

of cognitive systems

and I think what you're describing here

with this notion that the mappings are

more General than

the kind of attention mechanisms known

famously today that those represent like

a lower dimensional special case of one

kind of architecture

makes me think about how

the Bayesian graph is kind of semantic

in principle

and can have all of these nice

categorical formalisms around them but

then

and and you can even build the connector

to empirical data with the pre-shief and

the Sheaf which may be news to even many

empirical researchers doing data

analysis certainly was for me but the

message passing provides a rigorous

translation from whatever semantic model

is proposed topologically

to an implementation procedure that can

be planned for

and executed in linear time or at least

with definable characteristics so

message passing plays a really important

part in going from like the abstract

what is possible to the implementations

of any of these actual models and it

does it in a really General way where is

it accurate to say that we hope that

implementation

with message passing compatible

generative models will kind of roll out

better

because we won't have some of the

engineering challenges that less

reusable

abstractions might carry
1 / 1
ChatGPT
ChatGPT
Yes, that's exactly right. Message passing provides a structured way to decompose the computation over the graph. Instead of trying to compute everything all at once, which might be impossible or inefficient, we break the computation down into smaller, manageable pieces that correspond to the edges and nodes of the graph. This provides an efficient way to implement the computations and makes it easier to scale up to larger, more complex models.
Moreover, the design of the computation is guided by the structure of the graph, so we ensure that the computation reflects the structure of the problem we're trying to solve. It also makes the implementation more modular and reusable, as you mentioned. If we have a new problem that can be modeled with a similar graph structure, we can reuse the same message passing routines with minor adjustments.
Additionally, message passing also provides a general framework that can be used to implement a wide range of models, from attention mechanisms to more complex transformations. This generality makes it a powerful tool for implementing a wide range of models and algorithms.
It's also worth mentioning that this approach is not only efficient but also can be more interpretable. By breaking down the computation into smaller parts, we can better understand what each part of the model is doing and how they interact. This can provide valuable insights into the workings of the model and help us improve and refine it.
So, in summary, by using message passing to implement generative models, we hope to overcome some of the engineering challenges associated with less reusable abstractions and provide a more efficient, scalable, and interpretable framework for modeling complex systems.
1 / 1
it's hard to say I think there

there's also certainly some limitations

to this paradigm

as well so just kind of doing this kind

of message passing things you were

mentioning one thing is that it kind of

scales up uh quite easily like linearly

with the size of the graph but that also

come at the cost so there is certain

results showing this has limit in

expressivity

um so if you actually want to to

go beyond this for instance you have to

instead of just looking at pairs of

nodes you have to look at tuples and and

this kind of high order uh groupings of

nodes

um in in order to kind of get higher

exclusivity there's all sorts of

techniques to do that

um but yeah and there's always this kind

of tension between

being more expressive and being

efficient that you know will always be

there in any sort of algorithm or or

method

um

so so it's kind of hard to say I mean

it's definitely we can definitely say

this is kind of not the ultimate

solution let's say if you want to do

things message passing in itself you

know but you know maybe doing some sort

of computations on on on graphs you know

could could be

yeah maybe also something that that's

kind of

maybe missing a bit in in kind of the

graph email setting is you know the

context where you know your your graph

is uh

your assume your graph is known

um and you know you need to have some

graph structure at least the sensible

way to construct it right but for for

many kind of more

um

I don't know like

more obviously I phrase that

um I guess for Less clearly the fine

things like okay if I'm an agent doing

perception in the real world or

something right like

if if I'm trying to create a graph of

the world you know like

um what's what's an object what do I

create what what do I create a note for

right if I want to have like one note or

object and there's some connections

between objects and things like that

just I know it's like some some wild

example

um the custom item if you actually want

to do that but let's say you want right

then there's also like all these kind of

blurry things like what's an object and

what's not an object what's kind of

somewhere in between maybe you know do

you is that a node is that so it's kind

of like I'm trying to say that the graph

structure is kind of very discreet right

like it's kind of the node is either

there is not there and Edge is there

it's not there but then the world is

kind of really fuzzy right so if if you

use graphs as a as a model for your

world then there probably has to be some

decision to be made somewhere about

these kind of fuzzy Concepts uh they

actually translate in a concrete graph

entity like an object an edge or

whatever or not uh based on you know

some kind of inference procedure

um and I don't know if we did that or

not it's kind of humorous as kind of as

intelligent agents uh but that's you

know kind of some some interesting thing

to think about maybe you could also well

maybe one way to solve that there's also

kind of stuff like soft edges and things

like that and in some way you know if

you have attention coefficients it's a

bit like that you know like if an edge

has a weight of 0.001 or something it's

almost like not being there but you know

it's still kind of there so it's uh you

know it's a bit of a soft graph

architecture uh but so I guess at the

edge level you can implement this

softness but I think it's a bit harder

at kind of the note level right like uh

how do you kind of model a node that's

kind of there and not there

um yeah

there's just some random thoughts

that's very interesting about the fuzzy

object identification and kind of

similarities and differences between

nodes and edges even though in some ways

they have some similarities too

um or interoperabilities to

one other point of contact was like

an underlying hidden space

that we understand topologically that

projects a vector space at

in from different places so that could

be a vector of thermometer readings

and we want to have a smooth path within

the homeostatic

range defined up to a boundary Point not

saying that that's the structure of the

world but a structure of a very

heuristic and simple model might be to

aim for continuity and have a defined

hid in the state space that has

continuity underneath

and is able to emit vectors so that kind

of brings some of these classifier type

discussions that you brought up and like

the kind of fundamental impossibility of

geometric classification because you you

are going to end up with gray zones

whereas even if it takes a bit wise

description you can separate the network

so that gives an actual completeness

measure

and that allows

measures like I mean amount of

computational resources or in a more

like statistically principled way like

the Bayesian information Criterion so

how many nodes should we have

we should be on some

trade-off front in some modeling space I

don't know what to tell you it's a map

not the territory and that's because

that's more justifiable and so uh even

lifelike organisms might want to self

evidence

staying emitting from a living state

and so that provides a really simple

graphical architecture to cybernetic

systems and then active inference

explores a lot of different more

specific motifs

within that broader blanket persistence

picture and the path of least action so

that's what enables the physics in that

space and why these methods which as far

as I understand are often used in

quantum mechanics

or being able to come together with

active inference this way

yeah um some something that comes to

mind when you mention this I think

there's also like a

Recent avenir research in this area

where people and it's again kind of

generated by the fact that you don't

know the graph before beforehand many

times and I think kind of the old school

approach was well you you construct it

based on some rules right like you

you're gonna say all these I don't know

some things are similar I'm going to put

an edge between them and you define

similarly in whatever where you like and

so on

um and there was this kind of recent

Trend where what you try to do is kind

of latent graph inference or some people

call it manifold learning if you think

of the graph as some sort of manifold

but this kind of very informally

speaking

and essentially what you would do is

like you would map whatever you try to

learn the raw observations

um into some latent space and that's

where you actually construct the graph

you construct the graph in the latent

space rather than kind of in in the you

know raw space so that might be kind of

a way to deal with fuzziness as well

right like because then I guess you

might lose some

some of these kind of very concrete

one-to-one mappings because you might

learn some note in the latent space that

maybe corresponds to three or four

Concepts kind of uh mixed together in

you know so there's all these kind of

nice uh experiments with neurons in the

networks kind of visualized and you know

they learn maybe a kind of a mixture of

Concepts like like if you see what

actually uh you know activates that

neuron is actually you know maybe a few

classes or different kinds of things

it's not necessarily a single thing

um

so it could be something very similar

here where you have some very entangled

um representations that are kind of

distilled in in this latent graph and

then in a way at least in concert stay

even if like kind of in the latent space

that's still like kind of a very you

know clear combinatorial structure with

respect to kind of your raw observations

um that that structure kind of can still

kind of encode the fuzziness of the

world to some degree because you have

this kind of mixture of Concepts that

got distilled in the same node or things

like that or maybe some Concepts could

be represented by multiple nodes uh

depending on you know in what you see

this Concepts there might be all sorts

of variations or concept or points of

view and so on so

um I think you know kind of latent graph

inference could be uh you know quite an

interesting way maybe to address some of

these

um issues where we were discussing

although I think uh it kind of died off

a bit in in the recent year at least as

far as I've seen there were a few you

know slightly fewer papers um on the

topic

hmm

well certainly the agents

proposed latent structure of the world

the causal structure of the world is

just map not territory and so it enables

maybe some of those core screenings

um could you go to the slide where there

was a mapping between a smooth sphere

and then a regular geometric shape

uh yeah

let's see

in this one yeah just wanted to make one

one point see if you had any um comments

at the heart of some of the

relationships that you're describing and

where you pulled back to and kind in

terms of generalization helps us

understand

this relationship between the the sphere

and the geometry

and one place that that and the

implications for data processing and all

of the computational science areas is if

you are preserving or learning or

analyzing geometry but not topology or

the other way around you might get these

different like data set aberrations like

you might have the topology of the

coffee cup but it looks like something

totally different

and so what we would really want to do

would be understand the relationship

between geometry and topology

because if we could understand it in

principle like you have it on the left

side and then in practice with the data

scheme on the right side or or insert

your own left and right side there

then we'd be able to do data

analysis in a way that respected slash

preserved

both the topology and the geometry so

it's like two compatible perspectives

that have their different like strengths

and weaknesses and heuristics and so

understanding that relationship between

geometry and topology

and the implicit

um spaces that geometry requires and so

on that has tremendous

use

and it just in closing reminds me of

Buckminster Fuller's synergetics which

uses a close packing

architecture and a tetrahedron Centric

model of coordinates

to find more continuity between surface

area and volume and between the smooth

surfaces and the great circles on them

and like the points of connectivity on

shapes so I think it's an incredibly

deep area it really has fundamental

impact inactive inference helps us think

about our models in this way kind of

like the inflated balloon and with the

fuzziness and the architecture

and the finiteness

it really brings a lot to active

inference and so I appreciate you

sharing the work with us today and

continuing to work in this way

thanks out

any last thoughts

yeah I think

um yeah what what you mentioned I think

it's been all over my thesis this kind

of tension between between topology and

and geometry and maybe what I want to

emphasize is that you know I'm not

saying kind of the the previous

perspective of looking maybe more

geometrically of things was wrong in any

way and on the contrary actually there's

lots of interesting places where these

things intersective and in kind of this

Chief uh paper I briefly went there like

if you actually read the paper there's

uh there's a lot of beautiful

intersections actually my my main

collaborative Francesco he he's a

differential geometer so

um he actually had lots of kind of

inputs from from that side and

um indeed I think you know we should try

to use all these kind of layers of

structure uh you know in the best way

possible for our methods

awesome all right thank you until next

time thanks for having me
1 / 1
ChatGPT
ChatGPT
My pleasure, thanks for sharing your insights on this complex topic. It's always fascinating to delve into the world of graph theory, machine learning, and topology, and how they intersect and inform each other. Until next time!
1 / 1
A summary of the whole talk: ActInf MathStream 005.1 ~ Cristian Bodnar "Topological Deep Learning: Graphs, Complexes, Sheaves" opological Deep Learning: Graphs, Complexes, Sheaves
Authors
Bodnar, Cristian 
DOI
https://doi.org/10.17863/CAM.97212
Loading...
Files
Thesis (14.28 MB)
Type
Thesis
Abstract
The types of spaces where data resides - graphs, meshes, grids, manifolds - are becoming increasingly varied and heterogeneous. Therefore, translating ideas, models, and theoretical results between different domains is becoming more and more challenging. Nonetheless, two fundamental principles unite all these settings. The first states that data is localised, meaning that data is associated with some regions of the underlying space. The second says that data is relational, and this relational structure reflects how the various regions of the space overlap.

It is natural to formalise these axioms using algebraic topology. The "space'' in question is a topological space - a set with a neighbourhood structure - and the data attached to its neighbourhoods are algebraic objects like vector spaces. Since graphs, manifolds and everything in between is a topological space, we adopt this mathematical viewpoint to smoothly transition between domains, improve our theoretical understanding of existent models and design new ones, including for spaces that are yet to be explored in Machine Learning. Guided by this perspective, this work introduces Topological Deep Learning, a research programme studying (deep) models performing inference on data glued to a topological space.

This thesis includes four research works expanding upon the directions outlined above. The first work proposes Message Passing Simplicial Networks (MPSNs), a family of models operating on simplicial complexes, a higher-dimensional generalisation of graphs coming from algebraic topology. We study the symmetries these models must satisfy, the topological invariants that describe their behaviour, and how they can learn representations based on discrete differential forms. The second work takes this generalisation further to cell complexes, a class of spaces that also subsume simplicial complexes. We show their additional flexibility benefits molecular applications, where the resulting models outperform prior art on molecular property prediction tasks. The third work proposes a general topological framework for constructing graph coarsening (aka pooling) operators in a way that naturally generalises existing pooling approaches in computer vision. We show that this framework can be used to construct graph-based hierarchical models and visualise attributed graphs. Finally, the last work introduces a new perspective on graph models based on sheaf theory, a subfield of algebraic topology. Sheaves, which are mathematical data structures that naturally store the data attached to a topological space and its relational structure, faithfully realise the axiomatic principles of Topological Deep Learning. We show that sheaf structures on graphs are intimately connected with the asymptotic behaviour of message passing graph models and exploit these connections to design new sheaf-based convolutional architectures. We demonstrate that these models can cope with the challenges of oversmoothing and heterophilic graphs, which affect many existent graph models.

Overall, this thesis introduces a novel topological perspective on deep learning for structured data, whose ramifications establish many new connections with algebraic topology.
1 / 2
ChatGPT
ChatGPT
"Topological Deep Learning: Graphs, Complexes, Sheaves" is a thesis by Cristian Bodnar. The focus is on understanding the space where data resides - increasingly varied forms such as graphs, meshes, grids, manifolds - and how to translate ideas, models, and results across these diverse domains. Bodnar notes that data is both localized (associated with regions of the space) and relational (reflecting the overlap of various regions), and these principles can be formalized using algebraic topology. The concept of "space" is viewed as a topological space, a set with a neighborhood structure, and data attached to its neighborhoods are algebraic objects like vector spaces.
The thesis introduces "Topological Deep Learning", which studies models performing inference on data attached to a topological space. It includes four research works:
Message Passing Simplicial Networks (MPSNs): Models operating on simplicial complexes, a higher-dimensional generalization of graphs from algebraic topology. MPSNs' symmetries, topological invariants, and their ability to learn representations based on discrete differential forms are explored.
Generalization to cell complexes: This part of the work applies topological learning to cell complexes, spaces that include simplicial complexes. It shows that cell complexes' additional flexibility benefits molecular applications, outperforming previous methods in predicting molecular properties.
Topological framework for graph coarsening: It presents a topological approach for creating graph coarsening (pooling) operators that naturally extends existing pooling methods in computer vision. This framework can be used to construct graph-based hierarchical models and visualize attributed graphs.
Graph models based on sheaf theory: The thesis introduces a new perspective on graph models using sheaf theory, a subfield of algebraic topology. Sheaves are data structures that naturally store data attached to a topological space and its relational structure. The author demonstrates that sheaf structures on graphs are connected with the asymptotic behavior of message passing graph models and uses these connections to design new sheaf-based convolutional architectures.
Overall, the thesis presents a topological perspective on deep learning for structured data, establishing many new connections with algebraic topology. It demonstrates new methods and models that address challenges in current graph models, such as oversmoothing and heterophilic graphs.
1 / 1
Graph neural networks (GNNs): We discussed the role and significance of GNNs in machine learning and AI, particularly their capability to operate over graph structures, capturing relationships and structures that other models may miss. GNNs propagate and aggregate information from connected nodes, which is crucial for tasks involving relational data.

Combinatorial Generalization: We talked about the idea of combinatorial generalization and how it allows the creation of novel combinations from known components. This property is essential for adaptive and flexible reasoning, enabling us to infer and predict outcomes in novel situations.

Geometric vs. Topological Perspective: We highlighted the distinction between a geometric perspective, which emphasizes measures and distances, and a topological perspective, which is more concerned with connectivity and invariance under continuous transformations. Both perspectives have their unique strengths and potential applications, but GNNs and the combinatorial generalization paradigm align more with a topological perspective.

Expressivity and Efficiency Tension: We addressed the trade-off between the expressivity of a model (its ability to capture complex patterns and relationships) and its computational efficiency. This tension arises in any algorithm or method, including GNNs.

Limitations of the Graph Paradigm: We touched on some limitations of the graph paradigm, including the difficulty of dealing with "fuzzy" real-world concepts that do not easily map onto discrete graph entities. We discussed the possibility of addressing these limitations through soft graph architectures and other techniques.

Latent Graph Inference: We discussed the idea of constructing graphs in a latent space rather than in the raw observation space, which may allow the graph to capture more nuanced and complex structures in the data.

Topology and Geometry Relationship: Towards the end, we delved into the need to understand the relationship between geometry and topology for effective data analysis that respects and preserves both dimensions.

Buckminster Fuller's Synergetics: Lastly, we brought up Buckminster Fuller's synergetics as a framework that seeks to find more continuity between surface area, volume, and points of connectivity on shapes.
2 / 2
ChatGPT
ChatGPT
This summary appears to be a continuation of a discussion on the topics mentioned in Cristian Bodnar's thesis "Topological Deep Learning: Graphs, Complexes, Sheaves" that I summarized earlier. The central themes discussed include:
Graph Neural Networks (GNNs):
The conversation emphasized the significant role GNNs play in AI and machine learning, particularly their capability to operate over graph structures and capture relationships that other models may miss. GNNs' ability to propagate and aggregate information from connected nodes is crucial for tasks involving relational data.
Combinatorial Generalization:
The discussion centered around the concept of combinatorial generalization, which allows for the creation of new combinations from known components. This concept is crucial for adaptive and flexible reasoning, enabling us to infer and predict outcomes in novel situations.
Geometric vs. Topological Perspective:
A key part of the conversation was the distinction between a geometric perspective, which emphasizes measures and distances, and a topological perspective, focused on connectivity and invariance under continuous transformations. While both perspectives have their unique strengths and potential applications, GNNs and the combinatorial generalization paradigm align more with a topological perspective.
Expressivity and Efficiency Tension:
The trade-off between a model's expressivity (its ability to capture complex patterns and relationships) and its computational efficiency was discussed. This tension is a universal concern in any algorithm or method, including GNNs.
Limitations of the Graph Paradigm:
The discussion touched on some limitations of the graph paradigm, such as the difficulty of dealing with "fuzzy" real-world concepts that do not easily map onto discrete graph entities. Possible solutions to these limitations through soft graph architectures and other techniques were explored.
Latent Graph Inference:
The concept of constructing graphs in a latent space, rather than in the raw observation space, was discussed. This approach might allow the graph to capture more nuanced and complex structures in the data.
Topology and Geometry Relationship:
The conversation highlighted the need to understand the relationship between geometry and topology for effective data analysis that respects and preserves both dimensions.
Buckminster Fuller's Synergetics:
Finally, the conversation mentioned Buckminster Fuller's synergetics as a framework that seeks to find more continuity between surface area, volume, and points of connectivity on shapes.
1 / 1
A summary of the whole conversation.
1 / 1
ChatGPT
ChatGPT
In this conversation, we explored various concepts related to graph neural networks (GNNs) and topological deep learning as part of a larger discussion on Cristian Bodnar's thesis, "Topological Deep Learning: Graphs, Complexes, Sheaves."
Starting with the discussion of
Graph Neural Networks (GNNs)
, we underscored their significant role in artificial intelligence and machine learning. GNNs are unique in their ability to operate over graph structures, capturing relationships and structures that traditional models might overlook. The discussion highlighted how GNNs can propagate and aggregate information from connected nodes, a characteristic essential for tasks involving relational data.
We also talked about the concept of
Combinatorial Generalization
. This principle allows for the creation of new combinations from known components, a crucial aspect for adaptive reasoning. It enables models to infer and predict outcomes in novel situations, demonstrating a high level of flexibility.
The conversation shifted between two perspectives:
Geometric and Topological
. A geometric perspective emphasizes measurements and distances, while a topological perspective focuses on connectivity and invariance under continuous transformations. We pointed out that the topological perspective is more in line with the principles of GNNs and combinatorial generalization.
The
trade-off between a model's expressivity and computational efficiency
was another key point of the discussion. Expressivity refers to the model's ability to capture complex patterns and relationships, while efficiency relates to the computational cost of running the model. This tension is an overarching issue in all machine learning algorithms, including GNNs.
The
limitations of the Graph Paradigm
were explored, such as the difficulty in dealing with 'fuzzy' real-world concepts that don't easily map onto discrete graph entities. However, we suggested that these limitations could potentially be addressed through soft graph architectures and other techniques.
Another concept we delved into was
Latent Graph Inference
, the idea of constructing graphs in a latent space rather than the raw observation space. This could potentially capture more nuanced and complex structures in the data.
We also emphasized the need to understand the
relationship between geometry and topology
for effective data analysis that respects and preserves both dimensions.
Finally, the conversation touched on
Buckminster Fuller's Synergetics
as a framework that seeks to find more continuity between surface area, volume, and points of connectivity on shapes. This approach provided a novel perspective on the geometric and topological understanding of shapes and structures.
Overall, the conversation provided a deep dive into topological deep learning, exploring its principles, challenges, and potential solutions, illuminating new pathways and perspectives in machine learning and artificial intelligence research.
1 / 1
Abstract
Topological Deep Learning: Graphs, Complexes, Sheaves
Cristian Bodnar
The types of spaces where data resides — graphs, meshes, grids, manifolds — are
becoming increasingly varied and heterogeneous. Therefore, translating ideas, models,
and theoretical results between different domains is becoming more and more challenging. Nonetheless, two fundamental principles unite all these settings. The first
states that data is localised, meaning that data is associated with some regions of the
underlying space. The second says that data is relational, and this relational structure
reflects how the various regions of the space overlap.
It is natural to formalise these axioms using algebraic topology. The "space" in
question is a topological space — a set with a neighbourhood structure — and the data
attached to its neighbourhoods are algebraic objects like vector spaces. Since graphs,
manifolds and everything in between is a topological space, we adopt this mathematical
viewpoint to smoothly transition between domains, improve our theoretical understanding of existent models and design new ones, including for spaces that are yet to
be explored in Machine Learning. Guided by this perspective, this work introduces
Topological Deep Learning, a research programme studying (deep) models performing
inference on data glued to a topological space.
This thesis includes four research works expanding upon the directions outlined
above. The first work proposes Message Passing Simplicial Networks (MPSNs), a family
of models operating on simplicial complexes, a higher-dimensional generalisation of
graphs coming from algebraic topology. We study the symmetries these models must
satisfy, the topological invariants that describe their behaviour, and how they can
learn representations based on discrete differential forms. The second work takes this
generalisation further to cell complexes, a class of spaces that also subsume simplicial
complexes. We show their additional flexibility benefits molecular applications, where
the resulting models outperform prior art on molecular property prediction tasks. The
third work proposes a general topological framework for constructing graph coarsening
(aka pooling) operators in a way that naturally generalises existing pooling approaches
in computer vision. We show that this framework can be used to construct graph-based
hierarchical models and visualise attributed graphs. Finally, the last work introduces a
new perspective on graph models based on sheaf theory, a subfield of algebraic topology.
Sheaves, which are mathematical data structures that naturally store the data attached
to a topological space and its relational structure, faithfully realise the axiomatic
principles of Topological Deep Learning. We show that sheaf structures on graphs are
intimately connected with the asymptotic behaviour of message passing graph models
and exploit these connections to design new sheaf-based convolutional architectures.
We demonstrate that these models can cope with the challenges of oversmoothing and
heterophilic graphs, which affect many existent graph models.
Overall, this thesis introduces a novel topological perspective on deep learning for
structured data, whose ramifications establish many new connections with algebraic
topology.
1 / 1
ChatGPT
ChatGPT
Cristian Bodnar's thesis, "Topological Deep Learning: Graphs, Complexes, Sheaves," focuses on the increasingly diverse and heterogeneous spaces where data resides, including graphs, meshes, grids, and manifolds. The challenge of translating ideas, models, and results between these different domains is growing, yet two fundamental principles unite these contexts: the localization and relational nature of data.
Localization signifies that data is associated with specific regions of the underlying space, while relationality reflects how these regions overlap. These principles are naturally formalized using algebraic topology, considering the 'space' as a topological one—a set with a neighborhood structure—with data attached to its neighborhoods in the form of algebraic objects like vector spaces.
The thesis introduces Topological Deep Learning, a research program studying models performing inference on data attached to a topological space. This approach allows for a smooth transition between different domains, enhancing the theoretical understanding of existing models and facilitating the design of new ones, even for spaces yet to be explored in Machine Learning.
The thesis encompasses four research works:
The first proposes Message Passing Simplicial Networks (MPSNs), a model family operating on simplicial complexes, a higher-dimensional generalization of graphs from algebraic topology. The research explores the symmetries these models must satisfy, the topological invariants that describe their behavior, and how they can learn representations based on discrete differential forms.
The second work generalizes to cell complexes, a class of spaces that also include simplicial complexes. This flexibility benefits molecular applications, with the resulting models outperforming previous approaches in molecular property prediction tasks.
The third work presents a general topological framework for constructing graph coarsening (pooling) operators. This framework naturally generalizes existing pooling approaches in computer vision and can be used to construct graph-based hierarchical models and visualize attributed graphs.
The final work offers a new perspective on graph models based on sheaf theory, a subfield of algebraic topology. Sheaves are mathematical data structures that store the data attached to a topological space and its relational structure, thereby embodying the axiomatic principles of Topological Deep Learning. The research shows that sheaf structures on graphs are closely linked with the asymptotic behavior of message-passing graph models and uses these connections to design new sheaf-based convolutional architectures. These models can handle challenges like oversmoothing and heterophilic graphs, which affect many existing graph models.
In summary, the thesis introduces a novel topological perspective on deep learning for structured data, setting up many new connections with algebraic topology.
1 / 1
Chapter 1
Introduction
Topology! The stratosphere of human
thought! In the twenty-fourth century,
it might possibly be of use to
someone.
Aleksandr Solzhenitsyn,
In the First Circle (1968)
The types of spaces that machine learning (ML) models can handle have continuously
evolved over the years. The rise of deep learning models in the early 2010s has been
driven by data supported on Euclidean spaces such as images, video, text, and audio.
This first wave of success for deep learning was naturally followed by models operating
on non-Euclidean domains such as graphs, manifolds, meshes, and vector bundles. To a
large extent, this second generation of models was driven by the myriad of applications
(especially in the life sciences) where these spaces are present. The many milestones
in this area include de-facto solving protein folding (Jumper et al., 2021), proving old
conjectures in pure mathematics (Davies et al., 2021), and improving ETA prediction
on Google Maps (Derrow-Pinion et al., 2021).
As the family of spaces and data living on them is becoming increasingly diverse, a
unified mathematical perspective from first principles is increasingly necessary. However, efforts in this direction are hindered by the heterogeneous nature of these domains,
which range from discrete combinatorial spaces like graphs to highly-structured and
continuous spaces like Riemannian manifolds. So far, the unification attempts have
been exclusively geometric in nature. The Geometric Deep Learning (GDL) paper by
Bronstein et al. (2017), which has been particularly influential in popularising the
geometric viewpoint, provided an overview of how deep learning models can be generalised from grid-like domains to non-Euclidean spaces using ideas from differential
geometry.
15
Figure 1.2: A topological space (left), a topological manifold (middle) and a Riemannian
manifold (right). The topological space on the left fails to be a manifold because the
neighbourhood U around p does not "look flat" or Euclidean. The second space is a
topological manifold because each point p has a neighbourhood U that can be stretched
and bent to look like R
2
. Finally, a differentiable manifold M has at each point q a
tangent space TqM containing all the vectors that are tangent to the surface of the
manifold at that point. Riemannian manifolds additionally equip this tangent space
with an inner product gq between tangent vectors.
Sets
Topological Spaces
Topological Manifolds
Smooth Manifolds
Riemannian Manifolds
Figure 1.1: The structure hierarchy.
Nonetheless, differential geometry is only a
part of the story of mathematical spaces, described
by an entire ladder of structures (Figure 1.1). At
its foundation, we find sets, just a collection of
elements. First, sets can be endowed with a topological structure, which adds a notion of neighbourhood around a point. The resulting object, formed
of a set together with a neighbourhood structure, is
called a topological space. Next, some topological
spaces "look like" a Euclidean space in a neighbourhood around each point. These are called topological manifolds. At this stage, we
can further equip topological manifolds with a differentiable (smooth) structure that
allows us to generalise calculus to these non-Euclidean spaces. Finally, the ladder of
structures culminates (for our purposes) with Riemannian manifolds, differentiable
manifolds that also come equipped with an inner product between tangent vectors.
At the very top of this ladder, we now have the required structures to define familiar
geometric concepts such as lengths, angles, area or curvature in a non-Euclidean setting
(Figure 1.2).
Since not all spaces are or can be equipped with all these layers of structures, it is
natural to adopt instead the viewpoint allowed by the most general type of structure we
can impose on any set: the topological structure. This thesis relies on the fundamental
perspective that data lives on the neighbourhoods of a topological space and that the
data has a relational structure induced by the way the various neighbourhoods overlap.
16
Since any space, whether discrete or continuous, is a topological space, one can easily
transition between various spaces while always having the powerful tools of algebraic
topology at their disposal.
As we shall see in the following chapter, the notion of "non-Euclidean space",
which initially motivated the term "Geometric Deep Learning", can be interpreted
topologically without any geometric structure. Many non-Euclidean spaces, such as
graphs, one of the main spaces of interest in this thesis, discourage the geometric
viewpoint. Unlike other discrete structures in machine learning (e.g. meshes), graphs
are not even topological manifolds (in general). Therefore, models developed on
sequences, images, meshes or manifolds often cannot be adapted to graphs because
the tools of (discrete) differential geometry are not immediately applicable without
friction. Graphs are not the only example, and almost the entire thesis is dedicated to
geometrically "ill-behaved" spaces (i.e. non-manifolds) like the one shown on the left
side of Figure 1.2.
More recently, Felix Klein's generalised notion of geometry, seen as the understanding of a symmetry group's invariants, has dramatically influenced the deep learning
community. This perspective has become foundational for GDL, which has been described as the "Erlangen Program of ML" (Bronstein et al., 2021). Analogously to Klein's
Erlangen Program (Klein, 1893), which classified the various geometries that appeared
before his time based on their symmetries, GDL has become an inspiring attempt to
classify deep learning models based on the symmetries that they must satisfy. This
perspective is not remote from the topological viewpoint we adopt in this thesis. In
Analysis Situs (Poincaré, 1895), the work which marks the birth of topology, Poincaré
defines topology as the study of the group of smooth deformations of a space and, consequently, of those properties that remain invariant under such deformations (Bartocci,
2013; McCleary, 2013). Therefore, in Poincaré's view, topology was a branch of Klein's
geometry1
.
Despite these conceptual overlaps, the group of deformations is very different from
the so-called Lie groups (i.e. groups that are also differentiable manifolds) that describe
the symmetries of a space in Klein's geometry and machine learning applications more
generally. Furthermore, it is generally unreasonable to design models invariant to such
deformations, which can heavily distort the input space. Instead, we aim to study how
topological invariants emerge naturally in deep learning models applied to topological
spaces and how they can use them to their advantage.
Finally, while it is beyond question that the symmetry-based geometric viewpoint is
compelling and spans all the levels of the structure ladder from Figure 1.1, it can obscure
1Remarkably, as noted by Bartocci (2013), Poincaré established independently of Klein's Erlangen
Program that geometry is "the study of a group of operations comprised of the displacements to which a
figure can be subject without deforming it".
17
the interactions and inner workings of all these mathematical layers. Herefore, the
topological approach we adopt in this thesis can provide a new foundational description
of the field that is orthogonal to previous geometric attempts.
1 / 1
ChatGPT
ChatGPT
In the introductory chapter of his thesis, Cristian Bodnar presents the backdrop against which his work on Topological Deep Learning is set. The landscape of spaces that machine learning (ML) models can handle has continuously evolved, with deep learning models originally designed for Euclidean spaces such as images, text, audio, and video. A second wave of models then followed that were capable of operating on non-Euclidean domains like graphs, manifolds, meshes, and vector bundles. This shift was primarily driven by the wide range of applications in the life sciences where these spaces are prevalent.
However, as these spaces and their data become more diverse, the need for a unified mathematical perspective based on first principles grows more apparent. Current efforts towards this unification are complicated by the heterogeneous nature of these domains, which range from discrete spaces like graphs to highly-structured, continuous spaces like Riemannian manifolds.
Previous attempts to unify these spaces have been geometric in nature, most notably the Geometric Deep Learning (GDL) paper by Bronstein et al. (2017). However, Bodnar argues that not all spaces can be equipped with these geometric layers of structures and that the most general type of structure we can impose on any set is a topological one. Therefore, he advocates for a topological approach where data is viewed as living on the neighborhoods of a topological space, with the relational structure of the data being induced by the overlapping of these neighborhoods.
The concept of "non-Euclidean space", which originally motivated the term "Geometric Deep Learning", can be interpreted topologically without any geometric structure, according to Bodnar. This is particularly relevant to spaces like graphs that do not easily lend themselves to a geometric viewpoint.
Bodnar also discusses how Felix Klein's general notion of geometry, viewed as the understanding of a symmetry group's invariants, has heavily influenced the deep learning community. However, he posits that while this symmetry-based geometric viewpoint spans all the levels of the structure hierarchy, it can obscure the interactions and inner workings of these mathematical layers. He proposes the topological approach as a new foundational description of the field, orthogonal to previous geometric attempts.
1 / 1
1.1 Two topological postulates
In this section, we provide an overview of Topological Deep Learning and the various
advantages that accompany our topological approach. The entire thesis is rooted in two
postulates, which will form the fundamental blocks of our discussion (Figure 1.3). Until
the next chapter, when we formally define topological spaces, one should informally
think of the neighbourhoods of a topological space simply as regions of the space. For
instance, when the space is R, the open interval (−4,1) is a neighbourhood around 0 ∈ R.
Locality Postulate. The data is attached to neighbourhoods of a topological space.
The first postulate states that data is locally defined. That is, for some neighbourhood
U ⊆ X of a space X we have a set of possible features DATA(U) associated with U. A
more formal description of this postulate will be provided in the next chapter but, for
now, it is better to give some examples. For instance, the temperature readings of a
sensor can be associated with a particular region of the space surrounding that sensor
for which that measurement is accurate enough. Thus, the data describes a property of
that region (i.e. its temperature). The topological space might not necessarily represent
a physical space but could also describe a more abstract space. For instance, in a social
network, a neighbourhood might be formed by a user and its 1-hop neighbours in the
social graphs. The data attached to this neighbourhood might consist of the ages of the
people within it and other such demographics. Another example could be a region of a
molecule (e.g. a functional group), and the data could consist of various chemical and
physical properties associated with that region.
A space generally contains many such regions, which naturally overlap. Therefore,
it is reasonable to assume that some relation between the data attached to overlapping
regions exists.
Structure Postulate. The data has a relational structure, which is induced by the
overlaps between the neighbourhoods of the topological space.
This second axiom essentially says that data is structured, and models should exploit
this structure, which is induced by the set relationships (e.g. set inclusion, overlap)
between neighbourhoods. For example, two nearby sensors will have similar readings.
Thus, the spatial relation induces a correlation between the temperature readings. In
18
Figure 1.3: The viewpoint we adopt in this thesis is that data is attached to some neighbourhoods of a topological space. The relational structure arises from the intersections
between these neighbourhoods. These principles reflect that data is always localised
and structured in nature.
general, this relationship does not necessarily have to be linear.
These two axioms complement the geometric viewpoints discussed in the previous
section. For instance, when the space of interest is a geometric object such as a smooth
manifold, the data attached to the neighbourhoods of the manifold could also be geometric (e.g. a vector field). Because all differentiable manifolds have a topological structure,
our axioms are still well-defined and can describe such situations. Additionally, the data
attached to the neighbourhoods can still be studied through the lens of its symmetries.
In Chapter 2 we will progressively formalise and refine these axioms, starting
(almost) from scratch. In the meantime, we proceed to summarise in more detail the
benefits of this perspective, which we partially alluded to in the previous section.
A unified view. Any space of interest can be treated from a topological perspective,
and, therefore, a topological viewpoint provides a unified view of different models,
potentially operating on different types of spaces: graphs, grids, meshes, and differentiable manifolds (see Figure 1.4). This is made possible because topology offers a
smooth transition from continuous to discrete spaces by selecting a sufficient fine or
coarse-grained neighbourhood structure on the space of interest. Similarly, the data
attached to these spaces can take a range of forms: from node features on a graph to
vector fields on a sphere.
A bottom-up approach. Any geometric approach is naturally top-down in accordance
with the structure ladder from Figure 1.1. For instance, early attempts to generalise deep
learning models to non-geometric objects such as graphs relied on often unmotivated
geometric discretisations such as seeing the edges of the graph as a proxy for the tangent
space of a manifold. Instead of trying to enforce a geometric structure on spaces that do
19
Figure 1.4: Data living on graphs, grids, meshes and manifolds can be seen through the
lens of some topological structure.
not naturally have one, it is easier to start from the structure these spaces do have (i.e.
the topological one) and go upwards. As we shall see later, this bottom-up approach
even allows us to recover structures typical of Riemannian manifolds on graphs without
friction.
Topological invariants. Topological invariants are those properties of a space that
remain invariant under deformations like bending and stretching, called homeomorphisms. For instance, one can find a homeomorphism between a doughnut and a mug
(Figure 1.5), showing that these two spaces are equivalent topologically. From a machine
learning point of view, it is clear that topological invariants are incredibly robust to (potentially adversarial) deformations of the space in question. Therefore, understanding
the topological invariants that models can compute and other associated phenomena
that are invariant under homeomorphisms can provide a better understanding of their
robustness. At the same time, we will explore how models can use topological invariants
to their advantage.
Figure 1.5: A homeomorphism between a mug and a doughnut. From a topological
perspective, all the spaces in this image are equivalent.
Categorical foundations The field of algebraic topology is all about associating algebraic objects (such as groups or vector spaces) with topological spaces in such a way
that functions between these spaces become algebraic relations between groups (Figure
1.6). In this way, one can study topological spaces with the help of algebra. Category
theory (Eilenberg and MacLane, 1945; Mac Lane, 2013), a general theory of mathematical spaces, was precisely invented to study such translations between structures of
different types (e.g. topological spaces and groups) and provides a valuable language
20
Figure 1.6: Algebraic Topology associates algebraic structures to topological spaces and
studies how relationships between spaces translate into relationships about algebraic
objects such as groups. The language of category theory was born inside algebraic
topology as a tool for formalising and generalising these sorts of translations.
for such endeavours. In fact, category theory was first applied and developed within
algebraic topology (Eilenberg and Steenrod, 1952; Eilenberg and Cartan, 1956), which
has provided many prototypical uses-cases.
The categorical perspective also emerges naturally within our framework, and the
reader familiar with category theory might have noticed that the postulates have a
categorical flavour. Analogously to how algebraic topology translates from topology
to algebra, the axioms described above describe a translation from neighbourhoods
and set relations to data spaces and functions. This is more formally described by
the categorical and topological notion of a (pre)sheaf, which we encounter in the next
chapter.
1 / 1
ChatGPT
ChatGPT
This passage discusses the two foundational postulates of Topological Deep Learning, the Locality Postulate and the Structure Postulate, and how they form the basis of the methodology applied in the thesis.
Locality Postulate: The postulate assumes that data is locally defined and attached to specific neighbourhoods within a topological space. Each neighbourhood is associated with a set of potential features or data, for example, sensor readings associated with a specific region in space or demographic data associated with a particular group in a social network.
Structure Postulate: According to this postulate, data has a structured relationship induced by overlaps or set relationships between different neighbourhoods in the topological space. This means that there should be a correlation or relationship between data attached to overlapping regions.
The passage emphasizes the advantages of this topological perspective:
Unified View: Topology allows for a uniform understanding and representation of various models operating on diverse types of spaces, such as graphs, grids, meshes, and differentiable manifolds. Data associated with these spaces can take many forms, from node features on a graph to vector fields on a sphere.
Bottom-Up Approach: Instead of enforcing a geometric structure on spaces that do not naturally possess one, a topological approach advocates starting from the inherent topological structure of the spaces. This approach enables recovery of structures typical of Riemannian manifolds on graphs smoothly.
Topological Invariants: Topological invariants are properties of a space that remain constant even under homeomorphisms, providing robustness against deformations. In machine learning, understanding these invariants can provide insights into the robustness of models.
Categorical Foundations: Algebraic topology associates algebraic structures to topological spaces, allowing for the study of topological spaces using algebra. The language of category theory, a general theory of mathematical structures, is valuable for such translation tasks.
The concepts and principles introduced in this section are stated to be more formally defined in subsequent chapters.
1 / 1
1.2 A brief history of topology in ML
This thesis is by no means an isolated effort, and many other works at the intersection
of deep learning and topology exist. Here, we provide a brief (but incomplete) history
of topology in machine learning, highlighting some of the most representative papers.
Finally, we briefly compare this work with these approaches.
The idea of exploiting topological patterns in data originates in the field of Topological Data Analysis (TDA) (Chazal and Michel, 2021), which effectively started with
the seminal works of Edelsbrunner et al. (2000); Zomorodian and Carlsson (2004) on
persistent homology (PH). Today, TDA is almost synonymous with this term, which
refers to a set of tools trying to capture the topology of the data at multiple scales. As
shown in Figure 1.7, given a set of points in a metric space, PH considers an open ball of
growing radius around each point. The intersections between these open balls generate
a space (shown in blue): two intersecting balls form an edge, three intersecting balls
21
Figure 1.7: An overview of persistent homology: the point cloud that the method takes
as input and the spaces obtained for three different values of the radius of the open balls.
The method keeps track of the connected components, cycles and higher-dimensional
holes that appear and disappear as the space changes with the radius. The code to
generate this diagram has kindly been provided by Bastian Rieck.
form a triangle, four intersecting balls form a tetrahedron and so on. As the radius of
the balls increases, the space generated by these intersections evolves. As that happens,
PH keeps track of the radius at which the holes in this space (a topological invariant)
appear and disappear. This information provides a topological description of the data
that can later be integrated with a machine learning model. While the origins of this
procedure can be traced as far back as the work of Vietoris (1927), these ideas had to
wait for the power of modern computing to be applicable.
With the deep learning revolution in machine learning, persistent homology has
found a new life with an entirely new set of applications. It has been primarily employed to enhance deep learning models, often by back-propagating through the PH
computations (Brüel-Gabrielsson et al., 2019). The main methods in this space used PH
to impose topological constraints on neural representations (Hofer et al., 2019, 2020a;
Moor et al., 2020; Chen et al., 2019a) and to increase the expressive power of GNNs and
other geometric models (Hofer et al., 2017; Zhao and Wang, 2019; Rieck et al., 2019;
Hofer et al., 2020b; Zhao et al., 2020; Horn et al., 2022), typically by augmenting them
with (differentiable) topological signatures. Besides these applied directions, PH has
also proven to be a helpful tool for analysing the behaviour of deep neural networks
(DNNs). Ramamurthy et al. (2019) propose a PH-based measure of topological complexity for the decision boundary of deep classifiers and show how it can be used for
model selection. Another measure of topological complexity for DNNs was proposed
by Rieck et al. (2018), who used PH to examine the weights of MLPs. Finally, Naitzat
et al. (2020) used PH to study the activations of DNNs at each layer and show that they
are performing a progressive topological simplification of the data.
Returning to the present work, we want to highlight from the outset that this thesis
is not about persistent homology, on which most of the papers at the intersection of
deep learning and topology have already focused so well. In fact, despite using other
topological theories, tools and techniques, this thesis is not about any single one of
them either. Instead, these serve the purpose of mathematically carrying forward the
22
fundamental perspective we started with: data lives on a topological space.
1.3 Thesis outline
This section presents an overview of the following chapters of the thesis.
Chapter 2: Fundamentals
We formalise the main ideas of Topological Deep Learning while introducing the
fundamental mathematical notions in algebraic topology, differential geometry,
sheaf theory, category theory and graph neural networks. The chapter culminates
with a new perspective of message passing graph neural networks via sheaf and
cosheaf actions.
Chapter 3: Message Passing Simplicial Networks
We propose Message Passing Simplicial Networks (MPSNs), a generalisation of
GNNs to oriented simplicial complexes, higher-dimensional generalisations of
graphs from algebraic topology. The features living on these complexes can be
seen as discrete differential forms, signed volume elements that can be integrated.
These forms have strong connections to the topology of the input complex and
come equipped with a new set of symmetries compared to the common node
features that GNNs use. We show how MPSNs can satisfy these symmetries and
use these forms to extract information about the topology of the complex. Finally,
we apply this family of models to edge-flow classification problems.
Chapter 4: CW Networks
This chapter introduces CW Networks, a family of message passing models that
operates on CW Complexes, a type of topological space that further generalises
simplicial complexes and graphs. We show how these models can be applied
to graph ML problems by "lifting" the input graph into a higher dimensional
CW Complex (also called cell complex) and demonstrate that the resulting space
provides an improved computational fabric for learning from structured data.
This proves particularly useful for molecular modelling, where CW Networks
significantly outperform prior art. From a theoretical point of view, we analyse the
expressive power of CW Networks via a cellular generalisation of the WeisfeilerLehman test for graph isomorphism. We use this to prove that with particular
"lifting" procedures, CW Networks are more expressive than message passing
GNNs.
23
Chapter 5: Topological Pooling
One of the first dilemmas in the early days of graph ML was to determine how
pooling operators from convolutional neural networks can be adapted to GNNs.
In this chapter, we argue that pooling can be understood through the topological
notion of homotopy equivalence between spaces. Using this perspective, we
propose a new topologically-inspired graph pooling algorithm called Deep Graph
Mapper that generalises many existent graph coarsening approaches. Additionally,
we show how this method can be used for GNN-powered graph visualisations.
Chapter 6: Neural Sheaf Diffusion
Sheaves of vector spaces on a graph induce a hierarchy of diffusion-based message passing models, including Graph Convolutional Networks. By studying
this hierarchy using spectral sheaf theory, we show how sheaves are intimately
linked to the poor performance of certain GNNs in heterophilic settings and their
oversmoothing behaviour in the limit of many layers. By learning sheaves from
data, we obtain a set of sheaf diffusion models that address these two problems
leading to solid results in heterophilic node classification tasks.
Chapter 7: Conclusion and Future Directions
This last chapter reviews the contributions proposed in this thesis and discusses
future directions for Topological Deep Learning.
1 / 1
ChatGPT
ChatGPT
1.2 A Brief History of Topology in ML
The intersection of deep learning and topology originates from the field of Topological Data Analysis (TDA), with the seminal works of Edelsbrunner et al. and Zomorodian and Carlsson on persistent homology (PH) serving as a foundation. PH works by capturing the topology of data at multiple scales, tracking the appearances and disappearances of holes in a generated space. With the rise of deep learning, PH has been employed to enhance machine learning models and impose topological constraints on neural representations. It has also proven instrumental in analysing the behaviour of deep neural networks.
However, this thesis does not solely focus on persistent homology, despite its common use in topology-intersecting deep learning research. Instead, various topological theories, tools, and techniques are leveraged to uphold the perspective that data inhabits a topological space.
1.3 Thesis Outline
The thesis is organized into several chapters:
Chapter 2 lays the groundwork for Topological Deep Learning, covering essential mathematical concepts, including algebraic topology, differential geometry, sheaf theory, category theory, and graph neural networks.
Chapter 3 introduces Message Passing Simplicial Networks (MPSNs), a generalisation of Graph Neural Networks (GNNs) to oriented simplicial complexes, demonstrating their application to edge-flow classification problems.
Chapter 4 discusses CW Networks, which operate on CW Complexes, a type of topological space that extends simplicial complexes and graphs. Their application in graph ML problems, particularly molecular modelling, is presented.
Chapter 5 approaches pooling through the topological concept of homotopy equivalence between spaces, introducing a new topologically-inspired graph pooling algorithm called Deep Graph Mapper.
Chapter 6 investigates sheaves of vector spaces on a graph and how they relate to the performance of certain GNNs. By learning sheaves from data, a set of sheaf diffusion models is developed to address these issues.
Chapter 7 concludes the thesis, summarising the contributions and suggesting future directions for Topological Deep Learning.
1 / 1
Chapter 2 introduces the fundamentals of mathematics required for understanding Topological Deep Learning. It starts by formalizing the idea of a space through the notion of a topological space, which consists of a set and a collection of subsets called open sets that satisfy specific axioms. These axioms ensure that the open sets capture the intuitive notion of points being "near" or "close" to each other.

The chapter then discusses various examples of topological spaces, including finite sets, R^n with its Euclidean topology, and the discrete and trivial topologies. It also introduces the concept of a topological basis, which is analogous to the concept of a basis in linear algebra.

Continuing with the discussion of topological spaces, the chapter defines continuous functions between topological spaces. These functions preserve the "closeness" of points and are characterized by the property that the preimage of an open set under the function is also open.

The notion of open covers is introduced as well, providing a way to understand how open sets cover the entire topological space. Using open covers, the postulates from the introductory chapter are refined and made more precise.

In Chapter 2, the author continues to explore the fundamentals of mathematics relevant to Topological Deep Learning. They introduce Postulate 2, which states that the structure of the data is determined by the overlaps between the elements of the cover. This postulate allows for flexibility in data representation, as the presence of data in open sets can be tailored to the problem at hand.

The concept of homeomorphism is discussed as a stronger requirement for continuous functions between topological spaces. A homeomorphism is a bijective map that preserves the topology of the input space, meaning that both the function and its inverse are continuous. Homeomorphic spaces are those that can be transformed into each other through bending and stretching, without changing their topological properties.

Additionally, the notion of Euclidean spaces is introduced. A space is considered Euclidean if it can be homeomorphic to some Euclidean space R^n. Conversely, a space is non-Euclidean if it has no homeomorphism with any Euclidean space.

The idea of homotopy equivalence is presented as a weaker notion of equivalence between spaces. A homotopy equivalence between two spaces X and Y consists of a pair of continuous functions, one from X to Y and the other from Y to X, such that their composition with each other is homotopic to the identity functions of X and Y, respectively. Homotopy equivalence allows for more general transformations, including shrinking and expanding, in addition to bending and stretching.

The chapter also introduces the concept of homotopy between functions. A homotopy between two functions f and g is a continuous function that smoothly interpolates between f and g as a parameter (often represented as time) varies from 0 to 1. This idea is illustrated with an example of homotopy between two paths or trajectories.

Overall, Chapter 2 provides the foundational mathematical concepts necessary for understanding the subsequent discussions on Topological Deep Learning. It introduces the notion of topological spaces, continuous functions, homeomorphisms, and homotopy equivalence, which are essential for exploring the applications of topology in machine learning.

The notion of simplicial complexes is introduced in this section. An abstract simplicial complex is a collection of non-empty subsets of a vertex set, called simplices, satisfying two conditions: it contains all singleton subsets of the vertex set, and it is closed under taking subsets. Simplices are interpreted as geometrical objects, such as points, lines, triangles, etc., based on their cardinality.

The concept of orientation for simplices is introduced in the context of algebraic topology. An oriented simplex is a simplex with a specified order of its vertices, represented as a walk on the simplex. An oriented simplicial complex is a simplicial complex with orientations assigned to its simplices. The choice of orientation is arbitrary but crucial for performing algebraic operations on simplicial complexes.

To work with simplicial complexes algebraically, vector spaces with real coefficients are constructed, where the basis consists of the oriented simplices. This vector space is known as the space of k-chains, denoted as Ck(K, R), where k denotes the dimension of the simplices. The boundary operator (∂k) is defined as a linear map that computes the boundary of a region described by a k-chain. It does so by summing up the oriented (k-1)-dimensional simplices on the boundary of the k-dimensional region. The operator has the property that applying it twice results in zero, known as the boundary of a boundary being zero.

The boundary operator's null space (ker ∂k-1) contains (k-1)-dimensional cycles, and the image of the boundary operator (im ∂k) consists of k-dimensional boundaries. This relationship suggests that the boundary operator captures the algebraic structure of the simplicial complex and allows for the computation of cycles and boundaries. These concepts are fundamental for the study of homology and other topological invariants.
1 / 1
ChatGPT
ChatGPT
Building upon the notions of simplicial complexes, chains, and the boundary operator, the chapter introduces homology, a powerful tool in algebraic topology for quantifying the "holes" in a topological space. The k-th homology group of a simplicial complex K, denoted as Hk(K, R), is defined as the quotient group of the k-th cycle group by the k-th boundary group, i.e., Hk(K, R) = ker ∂k / im ∂k+1. Elements of the homology group, called homology classes, represent different types of "holes" in the simplicial complex.
The chapter explains that homology is a topological invariant, meaning it does not change under homeomorphisms. This makes it a useful tool for comparing topological spaces. Homology groups also have an additive structure, which allows the counting of holes of different dimensions. However, it is pointed out that homology groups can't distinguish between different types of holes of the same dimension.
The discussion then moves to the concept of cohomology, a dual concept to homology. Cohomology is defined using cochains, which are dual to chains, and the coboundary operator, which is dual to the boundary operator. The coboundary operator acts on cochains to produce another cochain, similar to how the boundary operator acts on chains. The cohomology group is defined as the quotient group of the coboundary operator's null space by its image, mirroring the definition of the homology group.
The chapter also introduces sheaf theory, which provides a framework for systematically studying the local-to-global principles underpinning the concepts discussed in the chapter. A sheaf is a mathematical structure that encodes how data varies when we restrict it to smaller subsets of the topological space. The chapter discusses sheaf morphisms, operations on sheaves, and how sheaves can be used to model neural networks.
The chapter then introduces the concepts of category theory necessary for understanding the formalism of Topological Deep Learning. Category theory is a mathematical framework that captures the essence of mathematical structures and their relationships. The chapter discusses the concepts of categories, objects, morphisms, functors, and natural transformations. These concepts are used to formalize the idea of sheaf neural networks and their properties.
Finally, the chapter discusses Graph Neural Networks (GNNs) as a specific instance of the general mathematical framework developed throughout the chapter. GNNs are machine learning models that operate on graph-structured data. They propagate information across the graph by applying transformation functions to the nodes and edges of the graph, resulting in a high-level representation of the entire graph. The chapter shows how the concepts of sheaves and cosheaves can be used to provide a novel perspective on GNNs. The chapter ends with the definition of sheaf and cosheaf actions, which are functions that process and propagate information on a graph in a way that respects the graph's topological structure.
1 / 1
A list of everything we've talked about so far.
1 / 1
ChatGPT
ChatGPT
Here's a summary of the key topics we've covered from the book "Topological Deep Learning":
Topological spaces
: The chapter starts by defining a topological space, which is a set and a collection of subsets called open sets that satisfy specific axioms. These axioms provide a foundation for the concept of "closeness" or "nearness" between points in the set.
Topological basis
: This concept is similar to the concept of a basis in linear algebra. It serves to generate all open sets in a topology.
Continuous functions
: These are functions between topological spaces that preserve the property of "closeness". They are characterized by the property that the preimage of an open set under the function is also open.
Open covers
: These are collections of open sets that cover a topological space. They help to refine and make more precise the postulates introduced in the first chapter.
Postulate 2
: The structure of data is determined by the overlaps between elements of the cover.
Homeomorphism
: This is a bijective map that preserves the topology of the input space, meaning that both the function and its inverse are continuous.
Euclidean and non-Euclidean spaces
: A space is considered Euclidean if it is homeomorphic to some Euclidean space R^n. Conversely, a space is non-Euclidean if it cannot be transformed into any Euclidean space through a homeomorphism.
Homotopy equivalence and Homotopy
: Homotopy equivalence is a weaker equivalence relation between spaces than homeomorphism. Homotopy is a concept that allows us to interpolate between two functions in a continuous way.
Simplicial Complexes
: These are geometric structures made up of vertices, edges, faces, and higher-dimensional counterparts.
Orientation of Simplices
: An ordered way to traverse the vertices of a simplex, which is used in defining algebraic operations on simplicial complexes.
Chains and Boundary Operators
: Chains are formal sums of simplices, and the boundary operator computes the boundary of a chain.
Homology
: This is a method in algebraic topology used to associate a sequence of abelian groups or modules with a given mathematical space. It provides a way to classify and count the number of "holes" in a space.
Cohomology
: This is a tool in algebraic topology that can be viewed as a method of assigning richer algebraic structures to a space than does homology.
Sheaf Theory
: Sheaves provide a way to systematically study local-to-global principles. They allow for a deep understanding of how information can be smoothly distributed across a space.
Category Theory
: A branch of abstract mathematics that provides a unified framework to describe different mathematical structures and transformations between them.
Graph Neural Networks (GNNs)
: A type of machine learning model that operates on graph-structured data. They are used as a specific instance of the general mathematical framework developed in the book.
Sheaf and Cosheaf Actions
: These are functions that process and propagate information on a graph, respecting the graph's topological structure. They offer a novel perspective on GNNs.
Each of these topics provides a key piece of the mathematical foundation for understanding the applications of topology to machine learning, as outlined in "Topological Deep Learning".
1 / 1
ChatGPT may produce inaccurate information about people, places, or facts.
ChatGPT July 20 Version
ChatGPT



