Excuse me, sir.
How are you doing?
Oh, not bad.
Maybe I should get set up to get on there.
Yeah.
Maybe I should put it on there.
What time do I start?
You've got, I mean, you've got two or five minutes to set up.
Okay.
We've covered a lot the last two days, what do you think?
Oh, geez.
I don't know.
We're jouncing for a drink.
I mean, it's just like you're crying to us, Amanda.
I'm like, oh, geez, it's five o'clock.
It's a good time.
I didn't make the schedule.
All right, Lou.
Do you want me to introduce myself?
I got a joke planned.
Oh, I got a joke.
Well, I had jokes planned, too, so.
You can speak up.
Well, okay, you just do your thing and I'll do mine.
All right.
Thanks, sir.
So, ladies and gentlemen, our next presenter last year
graduated with his NA here from the University of Georgia.
He's currently doing his PhD work at the University of Arizona.
But we've all been really excited to have him back this week,
all of us the sugar members.
It's been really great to see him.
And I know that at least Rachel Pym is very excited
for a temporary visit.
Thank you.
Great.
Well, it's nice to finally, for those of you who don't know,
I was the guy emailing you.
So, you know, I'm that Lou just in case, you know, now you know me.
Yeah, yeah, that too.
So anyway, what I want to talk about is, so I'm told I'm a syntactician.
I don't have handouts, but I'm told that I'm one.
So I'm going to talk sort of about syntax,
but really about how much I don't like it.
So we'll just start.
Actually, let's, Vera and Tim aren't here, right?
Okay, good.
Okay, quick poll.
Who here, whose first experience with syntax in a syntax class
was an enjoyable experience which enlightened their view of language.
And you feel like you actually learned.
Okay.
You got some people in here.
That's more than I expected.
Other question.
How many people actually can look at a syntax article
and actually understand what's going on without reading it like five times?
Okay, okay.
Well, several liars.
So anyway, so, you know, you get into syntax class
and you see all these magical trees and you have all these magical concepts
like functional categories, covert movements, specifiers.
I don't even know what a specifier is.
No one does.
C commanding A bar movement, quidditch, horcruxes, feature checking, death eaters.
And you start wondering, like, what is science and what is, you know, like,
when you step in a syntax class, it's like stepping into some sort of seance
or something like that.
So that's the feeling I get, at least.
Now, one of the problems, I think, is that syntax has been burdened
with some very unfortunate books like syntactic structures, aspects of ethereal syntax,
Cartesian linguistics, and a couple others.
Now, I have no idea what the common denominator behind all of these books are,
but I think they all share the same sort of muddy headedness
and I'd like to sort of elucidate what I think is the problem with all of them.
So that's what I'm going to do here.
So the flaw.
What's the flaw?
So there's this diagram that I show in every presentation
because it's like a really annoying one.
And it's called the reverse Y model of grammar.
And basically the idea is that syntax is the core of language.
Syntax produces a bunch of strings in language.
And it becomes sound or meaning.
On one side, it becomes the phonetic string.
It goes to the motor system and it becomes phonetic, what you enunciate.
On the other hand, it goes into part of the conceptual system.
It becomes meaning.
Now, the problem with this, I'm not going to even talk about sound and meaning here.
The problem is that there's nothing up here.
That is, there's nothing that feeds into syntax.
And that's a huge problem that you might not necessarily realize.
So classic generative grammar models language as a random generator.
That's to say that they think of it as a machine or it's not even people think about it.
It's just it's modeled as a machine that produces random sentences from a grab bag of words.
And the objective is to make a system that can produce all grammatical utterances of a language
and rule out all ungrammatical utterances.
So first off, this is obviously not how the mind works.
So what happens when you produce language is that you have an idea
and your language faculty has some way of embedding this in syntax and eventually enunciated words.
But if you model it like this, you end up with a bunch of problems that you don't actually need to solve.
Just to run through a couple of them, there are these things called selectional constraints
like the harvest was clever to agree, the boy elapsed sentences like this.
Traditionally, these were huge problems in syntax because no one really knew
how you're supposed to solve for them in generative grammar because of syntax is the very core.
How do you rule out anomalous sentences?
And the idea was, well, maybe there's this part of the syntax called selectional constraints
that weaved them out later in the derivation or something like that.
So the actual answer is that no one says these sentences because they don't make any sense.
Why would you try to produce a sentence like this?
There's not a derivation that's beginning and crashing.
It's that it can't even start because no one wants to say this.
Also, if you look at another Smith, not me,
one of my colleagues at Arizona has done this work on C-selection,
saying that basically C-selection, which is thought to be this arbitrary aspect of syntax,
can be boiled down basically entirely to semantics and other things like that.
So you don't need all of this sort of machinery.
And syntax ends up just recapitulating parts of meaning.
So what ends up happening is that you have this semantic,
syntax does stuff that other parts of the mind are supposed to do.
Like, you know, make sense of the world.
So to isolate the error, there's a very good article, very clear article called,
well, it's wrong but it's still clear by Hauser-Chomsky and Fitch 2002.
This is the language evolution article.
And it does well to distinguish what's called the broad language faculty from the narrow language faculty.
So the broad narrow language faculty is everything that goes into play in our production of language,
like our vocal apparatus or, you know, understanding of events, you know, everything we need in language.
Well, the narrow language faculty is that extra element that makes humans capable of producing, you know,
sentences in the way, ways animals can't.
So biolinguistics and syntax are sort of in the search for whatever that narrow language faculty is,
what that extra element is.
So it's just an issue of, you know, so you have a chimpanzee,
what do you add to it to create a human besides the bowtie?
Like what extra mental element do you need to create sort of a human-like cognition?
So, and Hauser-Chomsky and Fitch identified this as what's called recursion or merge or generativity, a bunch of different words.
And their idea is this element called merge evolved in the human psyche or something like that.
And what merge does is it combines different mental elements.
So you have conceptions of apples and sally and eating.
And so monkeys presumably have concepts of those things too.
You do as well, but the unique thing about humans, according to Chomsky,
is that there's this unique way of binding them together to produce sort of new sentences, new expressions.
And it's not just as soon as 2002, if you look back, this is from Cartesian linguistics.
Chomsky says sort of the same thing.
Linguistic and mental processes are virtually identical.
Language providing the primary means for free expression and thought and feeling,
as well as for the functioning of creative imagination.
So the idea is basically everything in our conscious mind comes about from this operation merge, and merge is tied into language.
So anyway, why is this obviously wrong? That's the main question.
Now they can't tell us, but pretty much all evidence shows that animals do have merge-like thought.
And C.R. Gallistel has interesting work in this, showing that really animals do have a conception of argument structure,
of direct and direct objects, patients, stuff like that, of dikesis, of basic numeracy.
They're at least equivalent to people who have languages with the primitive quote-unquote numerals.
And in fact, when you think about it, what's the use of having concepts of things if you can't merge them together?
So if I have a concept of Lisa, what use is that concept if I can't think of Lisa eating something or doing something?
You know, to make use of concepts, you have to be able to combine them together.
Oh yeah, I like this example.
So generativity is in all human behaviors.
So if you ever work at Waffle House, you don't actually have to be taught the 22 million ways to make a burger at Waffle House.
What you're taught is a generative operation.
You perform a set of tasks in a recursive way to produce this hamburger to its specifications.
And animals can do this too.
I mean, maybe they can't make hamburgers, but they can do similar generative things.
They can understand argument structure, etc., etc.
And one of the things I like talking about is how syntactic structure mirrors semantic structure.
So this is something really weird if you assume merge.
Because merge, so it combines linguistic elements, but it doesn't specify how they have to be combined.
But there's actually a lot of rules as to what can be combined in language.
So the weird thing is that the structure of syntactic and semantic derivations are eerily similar, as Barbier puts it.
There's one property that makes generative grammar particularly un-economical.
Namely, the fact that X bar structure itself does not contribute to the semantic interpretation.
So the idea is, well, we have all the structure and it looks like a semantic derivation.
Why don't we just put semantics in it?
Because when you take a semantics class and you do these relative clauses and dependencies, they look exactly like syntactic dependencies.
Why don't we think of these as the same thing?
Well, they are the same thing.
So let's talk about cartography, which is similar.
So, of course, Qin Kui and other linguists have gone into the sort of semantic structure behind languages.
And pretty much all of them show the same things.
There are very detailed semantic categories that appear in particular orders in all human languages.
So what was traditionally called CP and TP are really very expounded categories that have very specific meanings in them.
Noun phrases in adjective orders have very specific orderings, prepositional structures, same thing.
And if you assume merge just puts things together in whatever order, this is a really weird coincidence.
So, oh, well, here, this is just cartographies.
So these are like adverbs are in this particular hierarchical order.
Manor adverbs are going to be lower in the structure than perspective adverbs, stuff like this.
And this holds, you know, Qin Kui's original stuff is, you know, on like 75 different languages,
and they all show the same order and stuff like that.
And you have for argument structure, et cetera, et cetera.
So the problem is that merge recursion as the narrow language faculty is too powerful and too complex.
It implies that animals basically have to be creatines, that they don't do anything,
like they can't really have any kind of thought behind them.
And as Qin Kui notes, he used, I hate this word universal grammar because no one understands what it means.
But we'll just say the language faculty has a relatively rich content, far richer,
in fact, than most people are used to and perhaps willing to assume.
And he's referring to this complexity here.
There's an enormous amount of complexity that seems to have come from nowhere if you assume that merge generates it all.
So merge can't, merge doesn't need, just need to combine.
It has to know all of these orders.
So it's not just one operation, it's actually a very complex operation.
So my answer, and I think the answer of a lot of people, but to put it in words is this,
is that you don't need an operation called merge, you need an operation called externalize.
That's to say that we should assume that these semantic structures that we use to analyze the world
exist not only in humans, but in non-human animals that are cognitively similar to us.
So they have recursive ability to understand events, relative clauses.
They don't have them explicitly, but they have mental lionelogs.
And it's an epiphenomenon of how their brains are arranged.
This is just speculation, but it's speculation we're working into.
But what's unique about human cognition is that we can externalize the structure.
So our brains have been arranged in this way.
What the language faculty is, is a mechanism which links these lower mental processes
into the conscious mind so we can externalize them and stuff like that.
So time for the green smiley faces, which half of you guys have seen already.
So my idea is basically that there are cartographies in your brain,
and when you want to enunciate a DP or something like that,
you really just have this externalized operation just go up it and say,
okay, what do I want to say in all of this?
So it goes up and the adjectives are arranged in a particular order depending on what they are.
Which ones you need, whatever.
And again, the same thing is true in morphology.
This is a Korean example.
If you have a glutinative morphology, you see the exact same thing, same order.
It's just an issue of an externalization.
So in a nutshell, all animals have evolved a series of mental heuristics
for making cognitive distinctions and recursion.
So I'm just going to say that animals are not stupid.
Animals have basically similar cognitive apparatus.
It's a fourth-thick convention, isn't it?
Apparatus, that would be the proper plural.
Anyway, I think.
So animals pretty much have the same cognitive repertoire as us.
The big difference, and this repertoire is mostly non-conscious,
when they analyze events and things, their mind of course addresses them in this way
and they make decisions at a non-conscious level and react to them.
But our language faculty is an externalization scheme.
And I guess it's sort of like what you would call synesthesia.
So synesthesia is where you hear colors or smell tastes or something like that.
It's a merger of this sort of cognitive processing system with motor and naming systems.
So you sort of have this merger of these two systems.
And what you get is that this lower portion of cognition sort of bubbles up into the conscious mind
and we can actually externalize it.
So this allows otherwise non-conscious thoughts to be externalized into the world
and importantly in our brain.
So we can actually have like a metacognition and things like this.
That's one of the huge advantage.
As Chomsky notes, this is one of the huge advantages to language.
We can actually think about language in our head, contemplate events
and things like that and that's a huge benefit of it.
So the end result is that I feel like this sort of reconception
has pretty much all the desiderata of a theory of grammar.
Again, the narrow language faculty is mechanistically small,
meaning it's something that can actually have evolved over the past five million years or so that we need.
It's not like merge, which has to be incredibly complex
and has to basically have all the cartographies embedded in it implicitly, stuff like that.
And it also implies that animals are sort of stupid.
And syntax, I think in my idea, syntax becomes the study of the cartography structure.
So like what order do you have, you know, these kind of adverbs, prepositions, etc., etc.
And importantly, in some way, this is a window into human cognition
because it's showing us that our mind has a set way of analyzing events
and that can conceivably tell us how mental algorithms have evolved to address the external world.
So if you go back to the cartography, which I'll do later,
if you go back to the cartographies, there might be a particular reason that they are in a certain order.
It might be because they have evolved in a particular way
or because of natural law constraints or something like that.
And one of my favorites is that because syntax is tied to a set structure,
it suddenly becomes falsifiable.
That means you can actually make scientific statements about syntax.
You can't just say, oh, well, this doesn't work.
Let's just, oh, AP, this is an AP, this is an XP, this is an FP.
All these structures that are just mean nothing.
So that's a nice benefit, like actual falsifiability that's new.
So that's, yeah, so that's the, how much time do I have?
Okay, well, fine.
So that's about it.
I was going to go into more, but yeah, so it's good to quote your colleagues when they cite you.
I'm going to do that, you know, merchant yesterday said, hey, Luke,
we're going to be moving to my house.
Stop working on your presentation and come join us.
So I'm going to leave the rest of the time for questions because, you know,
that's all I got.
And I hope that made at least a modicum of sense.
So that's it.
Yeah.
So I don't completely disagree.
I'm sort of, I guess I'm wondering on the, yeah.
Yeah.
So as far as the, the cartography of the derivation, one of the motivations behind the
viral enlisting enterprise is that any difference that there is between humans and animals
has to be a very simple one and that would merge.
Yeah.
And things making it through the interfaces.
So I guess are you claiming that the same cartographic hierarchy is also present in all animals?
And if so, is it the same one?
Basically, yes.
So if it's not clear, what I was trying to say is that over the course of bajillions of years of evolution,
we have developed algorithms for addressing these kind of semantic things.
And the order we find them in is just a sort of an emergent property of the fact they're being externalized
based on their sort of ordering, if that makes sense.
Is there any sort of evidence that the same hierarchy exists in animals as well?
That would be very hard to find evidence for.
In terms of like the actual ordering.
But I think, again, Gallister's work, which he has a lot of interesting stuff, he can't, I mean,
I don't know how you would do actual ordering here, but it is sort of clear that they do have things like argument structure
and they might have similar categories.
So I don't know how to test that unless you have some brilliant idea.
So.
Mr.
Okay, Doug, yeah.
So they lack externalization.
So, yeah.
So they.
So yeah, well, there are two ways you can interpret that.
You can say on one sense, their reactions are just behavioristic.
They're just learning, okay, I make these signs and, you know, I get a response.
That's one possibility.
Another is that they have, they can develop gradually this sort of relationship between meaning and externalization.
But it's not, or, you know, some kind of phonetic realization.
But it's not like there's an actual algorithm like the green smiley face that's going through all of the structure, you know, forcing it out.
So it's not, it's.
Okay, but if they can learn some words.
If they can.
Yeah.
If they can learn some words as they could be able to.
Yeah.
Then what's preventing them from forming bird structures if not merged?
Well, if you.
Why is it that we doubt that Washow said water bird getting done?
Well, because if you don't have it externalizing, you know, sort of algorithmically, you can't have, I mean, you can't have particular orders of objects and direct objects and, you know, stuff like that.
You can't really have, you could theoretically have the externalization of individual words, but it wouldn't necessarily have any rhyme or reason to it.
It just sounds like you're saying that they lack syntax.
That's what it sounds like you're saying, even though you're saying that they actually lack syntax.
I mean, okay, there, well, let me put it this way.
The brain does a lot of things and we're not always consciously aware of them.
You know what I mean?
So my whole point is that this is something, you know, this sort of quote unquote, syntactic processing is happening in all animals.
But we just have this extra algorithm that, you know, computationally externalizes them.
Okay, so in the same way, it's like the difference between a first and second language learner.
There are different cognitive processes that are working on this to address similar problems and the output is totally different.
And it's a lot more salient in this example.
Yeah.
I just kind of wanted to run your comment.
Yeah.
Yeah.
Yeah.
Yeah.
Yeah.
Yeah.
Yeah.
Yeah.
Yeah.
Yeah.
Yeah.
Yeah.
Yeah.
Yeah.
Yeah.
Yeah.
Yeah.
Yeah.
Yeah.
Yeah.
Yeah.
Yeah.
Yeah.
Yeah.
So are you thinking of vervet monkeys?
Is that what you're thinking of?
I don't remember.
Okay, yeah.
So vervet monkeys, vervet monkeys, you know, have set calls for predators depending on their location.
And I would say sort of what I said, the dugs, that's probably a behavioristic response.
It's not like their calls have specific truth functional, you know, content, you know what I mean?
So it's not really the same thing.
It's something that looks like language to us because we're humans.
But it's probably just some other, I mean, it's as related as B communication, you know what I mean?
So is there a human difference just in these conditions?
Well, I mean, the human difference is we can externalize the, I mean, I would say that what they're doing is not related to this at all.
I would say it's just like, you know, they have a reef, they develop a reflex, sort of cultural reflex based on, you know, sort of behavioristic things.
So it's not really the same.
Yeah, Jason.
It might not be.
Yeah, yeah.
So I will say, you know, one of the big differences for Chomsky, you know, Chomsky's idea is that language exists not for communication.
He gets really pissy when people say that.
So language is the organized thought.
And even my model does the same thing because, yes, you have this semantic processing, but it's actually bringing it into the realm of like metacognition.
You can actually think, you can analyze your own thoughts and sort of think of them again.
So that's what you mean.
Yeah.
And then with this hierarchy system, you mentioned that you use dollars.
It's very vague on whether or not all languages have this hierarchy.
Who is?
The G-Ga that you were referring to.
Oh, Galastal?
Galastal.
No, it has nothing to do with cartography.
He was the one who said, you know, basically animals have merged like structure.
It's like, specifically, like Chomsky said, okay, the language faculty basically gives us the ability to count, argument structure, things like this.
Galastal says, the empirical evidence says that's not true.
So this is Qin Kui.
This is like Qin Kui, Ritzi.
I will yap on to you for ages about this if you ask me later.
Sweet.
Yeah.
Well, you know, parts of speech might be totally epiphenomenal.
I don't believe in grammar.
Yeah, the why?
Yeah, yeah.
Yeah.
Yeah.
Yeah.
Yeah.
Yeah.
The lexicon.
Yeah.
So the lexicon is an input to syntax.
It's something feeding into it.
But I mean, like in the sense that syntax, in this model, syntax is the thing that generates what you want to say, basically.
So what I'm saying is that what you really need is a, I mean, I mean, think about it.
Whenever you say something, you're saying it for a reason.
You don't just mumble a bunch of sort of like postmodern diatribes of sentences.
You say thanks with particular reasons.
And Chomsky models this the wrong way because he's sort of implying that meaning is generated after syntax or something, which doesn't even make sense.
So yeah, there is something visually above syntax, but that that's something different.
That's just like feeding into the syntax system.
Yeah.
Yeah.
Yeah.
Yeah.
Yeah.
Yeah.
Yeah.
Yeah.
Well, my point was simply their model has to account for why the harvest was clever to agree is grammatical.
But in reality, you don't even need that because you wouldn't even want to say something like that.
You know, and the problem with the sentence isn't C selection or something.
It doesn't make sense.
It's semantics, not syntax.
You have any questions, Mike?
I just wanted to say groovy.
Oh, yeah.
No, it's fun to have Luke back for a few days.
Thanks a lot.
Yeah, it's a little night.
So I just want to tell you, I had the great fortune to see you 100 times and speak.
I was sending a statement and he came on stage with a bottle of satchel.
It's okay.
And he just rumbled.
It's wonderful.
It's wonderful.
It's very engaging and what I would like to tell you is that you're wonderful.
Sure.
Just joking.
