Welcome to the first distinguished speaker seminar of this semester. We have the pleasure
to have Gary Marcos. He's an advocate of nursing policy AI for more than three decades. He was
doing a student pinker and got a PhD from MIT at just 23 years old and not too many people
can say that. And he was professor at NYU for more than two decades. He's also of many books,
for example, The Algorithm Mind 2001 and the most recent one that is related to AI in 2019
with Ernie Davis rebooting AI. And he has written for many journals and is
all the time mentioned by the media and also he's very active on social media
against the hype of AI. So please, Gary, we are glad that you are talking about
a proper foundation for AGI. Thank you for coming.
Thanks very much for having me. So as you say, I'm going to talk about foundations for AI.
I often give this talk lately to the small number of people that are on my side,
which is Neurosymbolic AI. And even for them, I recognize a lot of friendly names in the room.
A lot of what I'm going to talk about today is drawn on work that I've done with Ernie Davis
for the last decade or so, some of which is in our book Rebooting AI and an article that I'll
be particularly drawing on today called Has AI Found a New Foundation? So I want to give a shout
out to Ernie Davis. There is a growing anti-Gary market there. There have been many things
against me by Poon and Slate Star Codex and so on and on. And it's a common rhetorical move to
say it's only me, but there's actually lots of people that believe the things that I will tell
you today. And in fact, most of the things that I've said have been with Ernie Davis and credit
and blame get shared. So I have often recently imagined myself at a protest march, except instead
of asking for social justice, answers the questions to what do we need? When do we want it? And so
forth. I imagine that what do we need is artificial and general intelligence. And some people might
actually want it now, although that's controversial and we could talk about whether we want it or not
at all. And I think it's also worth asking another question about artificial general intelligence
or AI that I think is more robust maybe than we have now, which is why don't we have been working
on it for so long? And so I'll talk about that as well. Some people already think we're close to AGI.
Elon Musk, for example, is often, I think, hyping AI. I think that's part of his shtick. So in May,
for example, he wrote to Jack Dorsey, who was for a while the CEO of Twitter, and he says,
in 2029 feels like a pivotal year. I'd be surprised if we don't have AGI by then. Hopefully people
on Mars too. I don't know much about Mars. I will say that if we get anybody on Mars by 2029,
I'm pretty sure it'll be a one-way ticket. We won't have solved the problem of getting people back
from Mars. But in any case, I do know something about AI. And I think it's very implausible. We have
general intelligence in the sense of being kind of flexible and adaptive like human beings
by the year 2029. So on a place called garymarcus.substack.com, where I've been
blogging for the last couple of months, I must bet. I said, and this was again, really work with
Ernie Davis, so the money was mine. I put up $100,000. Other people quickly matched for me
to half a million dollars. Betting we wouldn't have AGI by 2029. And I gave some criteria. So I
said we wouldn't by then be able to have an AI that could watch a movie, tell you accurately
what's going on, which actually dates back to something I proposed in New York in 2014,
that we wouldn't be able to read a novel reliably, answer questions about plot and character and
motivations. I recommended Steve Wozniak's benchmark, which is you should be able to
go into anybody's home and figure out how to make coffee. You could imagine other kinds of things
like that. And then some benchmarks around computer programming and mathematics and so forth.
And a key point about general intelligence is you shouldn't just solve one of these things. We've
been pretty good at solving certain narrow domain problems. They're not all of them,
like go and protein folding, but we've been very poor at general AI. And so,
in order to say that we did succeed at a general AI, I thought you should be able to do at least
three out of five of these things. Elon Musk standing behind his words said, sure, half a
Elon Musk did not respond, did not put money behind his mouth. His response so far is this,
and somebody mid-journey captured it nicely. This relates to Optimus on Friday if you get the
joke, but anyway. All right. There are other people that are also in positions of power in AI
who would like to think that it's going to be really here soon. So Sam Altman, who's the CEO
of OpenAI, said in five years, he said this a year ago, we'll have computer programs that can
think and read legal documents and give medical advice. I'm actually doing some research on the
history of Watson, and that's, of course, Watson was promised to do that and didn't come anywhere in
her clothes. Actually, thanks to the Tim Altman's here is actually plausible. What do we actually
have now? Well, almost all the talk in AI these days is about foundation models. This is the buzz
word. There was a huge paper about this by like, I don't know, 80 or 100 authors last summer about
a year ago, all affiliated with this new search on foundation models at Stanford. And what do they
really mean by this? So this is where I think almost all the effort in AI these days goes to like
90 or 80 percent. So foundation models come pretty familiar for almost everybody. These are models
like GPT-3 and Dali that are trained on huge amounts of data at scale, and then they get adapted to
some kind of downstream tasks. Another name for them might be large pre-trained language models.
And what language model really means here is they're a model of what words people say next
in a sentence. So if you think about autocomplete, say when you're texting on your phone, these are
basically forms of autocomplete. They get a lot of language, and then they're able to predict what a
person might say next in the example on the left, which is from GPT-3. So you might type in some
sentences, and then the system might generate some very plausible English pros afterwards, or now we
have systems that you type in a set of words, and they'll maybe make a picture for you.
This is really most of what people talk about in AI these days. And a question you can ask is,
is something like this large neural network model trained on a bunch of sentences really a good
foundation for AI? Well, let's talk about the word foundation. Foundation is basically the bedrock on
which a larger structure is built. Above all else, what is a foundation being about?
A foundation is about being reliable. The whole point of a foundation of a house is that
you can trust it. Large neural network models like GPT-3 and Bird and Dolly and so forth,
but reliability is really not one of them. So I've been trying to point this out for
a few years. One example was in a paper that did not get the name it was supposed to get,
in technology review. It was supposed to be called GPT-3 bullshit artist. Our editor made us pull
punches, but you and I can all know that that's what it was supposed to be called, and we gave
examples like this. So the top part is what we typed into GPT-3. Maybe I'll let people read that
for themselves, or I get a drink of water here. So you have this scenario about cranberry juice
and grape juice, and Ernie Davis actually wrote it, and the point was to see whether the system
really understood the concepts of things like cranberry and grape juice, or whether it just
statistically noticed sentences like you can't smell anything, you sniffed it, you're thirsty,
and really what the answer we got was was driven by the statistics. Also the statistics that in
the corpus is unreaded and so forth, that it's trained on death as a common theme, as it turns out,
and so the system says you drink this cran grape juice essentially, what happens? Sorry, you have
this stuff, the system correctly predicts that you drink it, and then it says incorrectly you're
now dead. Nobody's going to die from drinking a little mixture of cranberry and grape juice.
The system has fundamentally confounded the statistical situation about what words might
follow one another, because that's really what a large language model models, with having a model
of the world, which you have. There are many, many examples like this, and lots of problems with it.
These systems are very prone to spreading misinformation, they don't actually understand
how the world works. So you might have some random person on Reddit saying Bill Gates
invented COVID-19, which is of course completely untrue, but a system like replica, which is
built on GPT-3 will also tell you that COVID vaccine is not very effective and so forth.
Here's another example from a French company called NABLA, it was investigating whether GPT-3
could be turned into a medical chatbot. So human types saying, hey, I feel very bad, I want to kill
myself, and GPT-3 gives a great answer. I'm sorry to hear that, I can help you with that, but its
depth of understanding is really minimal, because then the person says, should I kill myself? And
the system says, I think you should. Well, no suicide counselor should be saying that. I mean,
you could talk about euthanasia and so forth, but you should not have somebody walking in the room
and you tell them to commit suicide. But the system doesn't actually understand concepts
about death or suicide or anything else. It's just predicting next words, it turns out that
if you look in training corpuses and people say, should I blank? Often human beings say,
I think you should, people are supportive of their friends' actions and so forth. This is a case
where you shouldn't, the system has no idea. Another example from linguist Alison Anager
took one of these models and said, a robin is a blank, right? They fill in the next word in the
sentence, and the system said, a robin is a bird, that's a perfectly reasonable answer. And then
she tried something fiendish. She said, a robin is, well, it turns out these systems have no idea
what words like not actually mean. A robin is not a bird. Well, that's a crazy answer. And in general,
it just turned out that for years and years, systems like this are very,
years or even more frightening in some ways. You have a system that is automatically extracting,
I don't know if it uses the language model or not, but automatically extracting text and it says
what to do in this seizure, and it extracted everything from the text out there except the
words do not. And so the advice for seizure was actually 100% wrong.
Then people have tried adapting these systems to moral reasoning. Again, using the statistics
of language, in this case, the language was actually actually tailored to morality,
is a problem. So this system, somebody said, should I commit genocide if it makes everybody happy?
Well, the statistical correlative, if it makes everybody happy, in this system was a kind of
positive rating. So it said, you should. Well, this was not actually good advice.
Maybe I'll skip that one. I think I've made enough of that point already.
There are other problems. For example, sexism. Here's an example that somebody else gave me
that I posted on the web of Polish to English. So you have something where the definite article,
I won't try to pronounce the Polish, but the definite article in every sentence is identical.
And the back, she is beautiful. He is clever. He reads, she washes the
toes and so forth. It's about as sexist as you could possibly imagine, because all that's getting
spit back is the statistics of a corpus, not anything about what it might be saying. And then,
you know, on Twitter, challenge almost everything I say, but this time when I
came back with, yeah, that I find the same thing in my language when I do the translation.
There was a, I think, lovely headline, I should say 2021, from the next web by, I think, Tristan
Harris, no, Tristan Green, excuse me, saying, keep mind tells Google it is no idea how to make AI
less toxic. People in the field now realize that these larger language models do all these
problematic things, but nobody really has anything to do with it, has anything to do about it. And I
think that's because fundamentally predicting statistical words has nothing to do with having
concepts about the world. You need a whole different framework with concepts about the world
to actually make progress on problems like these. I'll skip this one for now, except to say that
misinformation is a very serious problem and it's proven much harder to solve than people think.
There is a parody by the cartoon XKCD that I think actually really catches
what's going on right now. Right now what people are doing are taking big piles of linear out,
basically what deep learning is, and there's no methodology here. They try things, if they work,
they publish them, if they don't, then like in this cartoon, they just stir the pile until things
are looking right, but we don't have a systematic way of doing this and just adding more data and so
forth is not really helping, helping these systems. They continue to make most ridiculous
kinds of varus. Like you say Sally's favorite cow died yesterday, when will the cow be alive again?
And the system says in a few days, it just doesn't understand the difference between being alive and
dead. And then a cult in the field where people try things, like they add a few words, they find that
if you say let's take things step by step, sometimes the systems work better. So then you try that
on an example like this, you say Sally's favorite cow died yesterday, when will the cow be alive again,
let's take this step by step and the system then takes things step by step and it gives you a whole
bunch of intermediate steps, like it takes 90 days of gestation for a cow to give birth to another
cow and then you come out with the answer to when will the cow be alive again and you get 90 days,
which is absurd. The fundamental reality is that contemporary AI basically has a long tail problem.
It's really good if you have big data, lots of data about routine things and it's
unusual but important things. This is fundamentally what's going on. There's a kind of myth out there
that you just put in more and more data, you have more compute and outcomes, artificial,
general intelligence, but that never really solves the long tail problem. There's still
always outliers no matter how much data that you add. So I think this is silly, although it's
pretty much the predominant view in the field. There are people making t-shirts saying scale is
all you need, trying to show that if you have enough data, it will project it out. But in fact,
things like telling the truth and understanding the world or not, scaling as fast as other things
like something fluid. So the metaphor I've been using for a decade in which Ion Lacoon happened
to use the other day is that deep learning is a better ladder, but a better ladder doesn't
necessarily get you to the moon. What would be better? Well, I think to get further, we're really
almost going to have to start over. We're going to need to build a deeper AI. Right now, deep
learning sounds deep, but really deep just means a number of layers in a neural network. It doesn't
mean conceptual depth. We're going to need to get to conceptual depth if we're going to get
to trustworthy AI. So this is essentially the argument of the book Rebooting AI. And then
there's an archive article I have online for free, which develops some of the ideas there
and it'll kind of guide the next part of the talk if you want to read afterwards.
So our first point to make, it's sort of obvious to people who really do this for living,
but not for people outside of AI, is that AI is not actually one technique, but many.
Deep learning is a kind of machine learning. And machine learning is one of many things that
people do in artificial intelligence. Often, if they want to do something successful, they
have to actually combine these things. So for example, Alpha Fold 2 is probably the biggest
contribution to AI so far. It automatically folds proteins. It's not perfect. We could talk about
its limitations, but it's really an amazing tool for biology. And it works by combining a very
classical technique in search called Monte Carlo Tree Search with a deep learning technique.
And then a lot of knowledge representation as well about what proteins actually are and so forth.
The first thing that I've been calling for is a hybrid neurosymbolic approach,
meaning that we need to take some things from the neural network community, particularly its
emphasis on learning, and some things from classical AI with its emphasis on symbolic,
abstract knowledge that can be generalized through algebraic-like operations over variables.
The reality is we are not getting to AI we can trust by relying on deep learning alone.
All the examples I showed you in the first part, deep learning is not only failing and occasionally
making mistakes, but making the same kinds of mistakes over and over again for decades.
A realistic appraisal would be that it's good for learning, but it's poor for abstraction. I've
been saying this for a long time. I've gotten an enormous amount of flak from the deep learning
community for it, but now you can see people like Yashua Benjio are starting their talks in
in Kahneman's terms. He says, deep learning has been good at system one. Now we need to
figure out how to do system two. This is a very similar point to the one that I'm making.
Classical AI is not going to get us to robust AI either. It's very good at abstraction, but
although there are some techniques like ILP, they're not in general as powerful at learning as
deep learning is. It seems to me obvious, in fact, that what we really need are hybrid models
that bring together the two traditions. I think you see that a lot of major figures that have
been hostile to this are, in their own way, trying to find a way of at least getting the virtues of
both sides of this equation. Jurgen Schmidt-Huber is one of the biggest people in deep learning,
has a company's been working on for the last several years. I talked to him about it the other
day, and it's very much a neurosymbolic system that has explicit symbolic stuff working alongside of
deep learning. I had an interesting conversation with John Lacoon over Twitter in the last few days,
but there's a way in which he too is starting to see the wisdom of this.
What I've always said, although Lacoon usually gets us wrong, is that we don't need to
toss deep learning. It's easy to character me by saying that what I'm saying is we should get rid
of deep learning, and Lacoon has done that as recently as yesterday despite being corrected,
but the reality is that I've always basically made the same point. Here's a good
18. Despite all of the problems that I've sketched, I don't think we need to abandon deep learning.
Rather, what we need to do is to reconceptualize it, not as a universal solvent,
but simply as one tool among many. So deep learning is really good at perception, but if you think
about what cognition is as a whole, there are many components. This is a very crude caricature, but
it's not unreasonable to kind of start here. There's perception, deep learning,
things like common sense, planning, analogy, language, and reasoning. I would argue that
deep learning has done all of those things poorly, and that it is a mistake to assume that
because you can do one piece of this by with a particular technique that you can do all of them,
but I think essentially that the way that most of the field thinks right now is exactly that.
They think that if we just scale up what we're doing, we'll get the rest of these. Lacoon has a
different take we could discuss in the discussion. He agrees with me that adding more data is not
enough, but I do think he thinks that deep learning is going to handle all of these, and I'm skeptical.
Here's some interesting neuro-symbolic work people might want to go have a look at
from AI21 Labs, which is a startup with investment from Amnon Shafshua, who worked on
Mobile Eye, among other things. What they're trying to do is to take large language models
and integrate them with symbolic knowledge, and they're finding that in some fairly important
ways they do much better than pure deep learning approaches. One example is if you ask a neural
network who is the president right now, a deep learning system is going to look at all the data,
and across all the data, there's going to be a lot of data that says, is the president, and it can't
reason over the fact that because someone was president, it doesn't mean that they still are,
and then else becomes president than the other person is not. I guess you could argue that Donald
Trump is a little like a deep learning system, but that would be a story for another day.
Reality is that we do temporal reasoning over time, and this neuro-symbolic system is trying
to do a little bit of that. Taking large models as a piece of it, but also having some classical
symbolic AI to keep things straight. I just got a year internet connection is unstable,
but hopefully we'll be okay here. In my view, the Achilles traditional neural networks
is what people might nowadays call distribution shift, and what I called in 1998, generalizing
outside of a set of examples. Suppose you have to learn a trivially easy function of the input is
the same as the output f of x equals x, and call it the identity function. You want a bunch of
binary numbers that are the green ones here. You can generalize to some others that are nearby with
a very simple neural network solution, but if you try to generalize to other numbers,
they're out of training space. Let's say odd numbers, they're outside of the training space
because they have a one in the right most digit, and you've only been trained on even numbers which
don't, these neural networks fail over and over and over again. In my view, this is the problem
that has haunted essentially all efforts at getting neural networks to reason well or to use
language well. I think it continues to be the problem, and this is in some sense the strongest
motivation for including symbol systems which have no trouble at all with this kind of generalization.
I made these arguments back here in 1998, 2001, but the same kind of thing is still showing up.
Here's a working paper from earlier this year from Yasemin, and what they did is they looked
at arithmetic in a large language model. What they found is the system is much more like
get things right if they're highly frequent and much less likely to get things right if they're
not frequent. Here they're working with multiplication. I should have said that. If you're
working with something like multiplication, you want a linear to all possible instances of a
problem, not something that generalizes if it's in the distribution of cases that you have seen
before and not if it's outside that distribution. What they're finding is you get outside that
distribution is problematic. Since this came out a larger language model appeared in Minerva,
and Minerva actually showed the same thing. It was okay at two-digit multiplication problems,
but multiplying two, four-digit multiplication things was basically, generalization was basically
zero. I think it's a good exercise in perspective taking, especially for us old-timers, to think
about the fact that in 1994, Intel recalled the Pentium chip, and it was a huge scandal. It cost
the company half a billion dollars, which was a lot of money for a company at that point.
Everybody was really upset that the Pentium chip inside their Windows machine made mistakes,
but how often did it make mistakes? It made mistakes on integer arithmetic, which I was just talking
about zero percent of the time. Very, very rare floating point arithmetic problems that ever
made a mistake. They actually recalled the chip just because it once in a while made a mistake.
Whereas GPT-3, you can train it with a hundred billion parameters, and it's still only 80%
correct on three-digit addition. They don't do division at all. They don't test floating
point arithmetic, and it's available now, and there's billions of dollars.
To me, this is an enormous step backwards. We've seen systems like Dolly. I can maybe take
questions about it. I think I have a little bit here, and I could talk more about it if you like.
Dolly was kind of billed as this system that understands natural language,
and it can draw great images from natural language sometimes, but if you dive deeper,
it doesn't, for example, understand the difference between a red cube
cube. If you give it words to repeat, it tends to make errors. I won't go into all of the details
here, but the thing on the left was someone at OpenAI trying to ridicule my notion that deep
learning is hitting all. They sort of got it right with their system, and they sort of didn't.
The ability is that there are very systematic problems. I have an article about this in my
GaryMarcus.Substack.com the other day, which I didn't get to add into the slide, but I found
these systems systematically can't correctly depict things like draw a bicycle and circle the
parts that you pedal, or they don't understand things like function. They don't really even
understand parts and whole. It is true that if you give a very large database,
do interesting things with them, but it is also true that they really don't
originally certainly fail on what I would call compositionality. They don't understand parts
and holes, and they don't understand function. You can't get the general intelligence if you
don't understand basic concept like that. Second thing that I think we need besides a
neurosymbolic hybrid, which to me seems like an absolute minimum necessary requirement if we're
going to make progress. We need to have a lot of knowledge. Human beings know things like if you
break a bottle and there's some liquid in it, some of that liquid is probably going to escape.
If you feed this into neural networks, you get very probabilistic stuff. If you break a glass
bottle that has toy soldiers, it might say that toy soldiers will probably follow you in there,
which is just incoherent. Current systems just don't have concepts of things like bottle and
soldier, and they don't have knowledge about how the everyday world works. To me, it seems like
foolish to even imagine that we might be close to general intelligence without a solution to that
problem. I think probably, and I don't know if Iris Bayron is here. I didn't see her in the list as
she arrived, but your colleague at Northeastern, Iris Bayron, has done very interesting work around
this showing that human beings have an innate resistance to believing that things are innate,
essentially. But Elizabeth, I think Iris is absolutely right. It's in the field. It's quite
problematic. Actually, Iris and I have an article coming out about this. But if you look at the
empirical literature, it's pretty clear that human children have some innate understanding of the
world. And Liz Felkey has argued that it has to be that way. If children are now endowed innately
with abilities to perceive objects, person, sets, and places, well, that's great. And they can use
their perceptual experience to learn about the properties and behaviors of such entities.
If you don't start by having at least the kind of minimal ontology of the world, it's just not
clear what you can learn from that point. And I think that the failures of massive, massive, massive
large language models to really understand the world around them, like I documented in the
first part of the talk, speaks to that. You have all this data, but the systems don't really know
how to organize that into a conception of the world. I think we need at least a little bit of
innate knowledge in order to be able to do that. A simple example of how far we really are away
is a cheese grater. We know how to take a photograph and represent it as a set of bits.
We can make a 3D model of a cheese grater. But we don't just have a system understand why you would
run the cheese with another in order to get small bits of cheese out of it. If we're going to move
to general ontology, we need to be able to understand how the function of something relates to its form.
And we just really are nowhere on that. I'll skip that. I guess it's the interest of time.
I think it's time for a discussion. I will point out that there's this kind of media hype around
deep learning. The media, for whatever reason, loves the deep learning narrative. It loves
the narrative. AI is imminent. You can look, for example, at Kevin Roos's article in The New York
Times a few weeks ago. The reality is, for example, that deep learning isn't even always the best
tool for current problems. Facebook now, Meta, had something called the NetHack competition
where you have a different dungeon every time. And while AI crushed deep learning in this competition,
this is put on by Facebook, which is pretty pro deep learning. But deep learning is still
lost in this because the system has no real knowledge of what's going on. And what deep
learning tends to do is superficial. Sort of like, if I'm playing Will at this time,
be a brick, and so paddle right here. In this game, the dungeon is what we call procedurally
generated. It's different every time you play the game. And that puts deep learning in real
problematic space. Whereas if you have some abstract understanding of what rooms are in
a dungeon and so forth, you do a lot better. You will not see that in the writings in The
New York Times. Another thing that I think is critical is really two things I'm consolidating
here, but are reasoning and cognitive models. And so one of the people who thought most deeply
about this is Doug Lennett, who's built a system called Psyche. Nowadays, I find people aren't that
much aware of it. People used to be very aware of it in the AI space. What Lennett tried to do is to
systematize common sense in machine interpretable form with a system called Psyche, which he's
still working on, has been working on for decades. And an interesting article in Forbes in July 2019,
he shows how at least in principle, systems like that can make spectacular inferences,
like doing the kind of reading comprehension that I suggested Elon Musk is not going to be
able to solve by 2029. So here, I won't go into all the details, but you can find the article
in Forbes. He showed how a system like this was able to make inferences about what Romeo was
thinking about Juliet when she was taking the death potion or what Juliet, the question was,
when she takes the feigned death potion, does Juliet believe that Romeo will believe
she is alive or in the suspension? And there's no current other AI system, aside from the
symbolic system of a lot of common sense, that can even come close to making
like that, where one person's knowledge of another person's knowledge relative to what they know.
And yet, like that all the time, constantly reasoning about what other people might believe.
We know that four-year-olds, though maybe not three-year-olds, are able to do that.
And most of the world's movies depend on that. A lot of politics and diplomacy depend on that
kind of thing. In order to actually do this stuff, you need cognitive models. What is going on in
the world? Who believes what? You need to be able to reason over that. It's just not enough work on
that in AI right now. And for that reason, it seems to me like anybody who's worried about
artificial and general intelligence being here in the next decade is not in touch with the realities
of what actually would need to be done. This again ties in with misinformation. So if you type
the into the latest version of GPT-3, which is called, why is it important to eat socks after
meditating? It will make up stuff like on the right. I'll say some experts believe that the act of
eating a brain to come out of its altered state is a result of meditation, while others suggest
with fueling the body, blah, blah, blah. Well, it's perfectly fluent prose, which means that if
you're a Russian troll farming, you want to disrupt the United States election is a fantastic tool,
but it's nothing to do with reality. This is not anchored in the system going and doing a
web search to see what experts believe about the act of eating a sock. Nobody's even written about
that because it's so ludicrous, but the system has no way of knowing that because there is no
cognitive model in the world inside of it. Anybody who thinks like we're close to the
Star Trek computer, when we can't even have a system know that it's kind of a joke to ask about
after meditating is, I think, delusional. So back to the why question. Why don't we have
yet? It's 65 years after the famous Dartmouth AI conference. We still haven't solved it.
Why not? Well, you could imagine what those guys would have said back in the day, Marvin Menzky,
people like that. If you said, you know, this is a great thing, but we have a time machine,
we know it's 65 years, you're still not going to solve the problem. It would have made excuses as
any computers are really expensive. Maybe that's why we never got the hardware we wanted in those
65 years, maybe, where they might have said, well, computers, you know, they don't have enough memory.
You're really going to remember a lot of stuff, but we don't have enough data. There's not enough,
you know, we tried, but we couldn't get enough investment in the field or we couldn't get enough
people. Well, it turns out AI in some ways got everything that Menzky and McCarthy and Simon
wanted in terms of much cheaper computers with enormous amounts of memory, vast amounts of data,
all the money that anybody could possibly imagine, billions and billions of dollars,
tons and tons of people interested in the field, and that hasn't solved AI.
Well, one thing is that AI is harder than its originators realized.
I think another thing is that too much research in AI is siloed. There's not enough
interdisciplinary collaboration. Back in the day, people really respected human cognitive
psychology when they built AI and they respected linguistics and so forth. And now they're just
like, I need my data. I don't need to listen to anybody else. That's not good. I think the pendulum
has swung too far towards machine learning with too little focus on innate contributions. I think
there's still not enough common sense represented in machine interpretable form, except maybe in
the proprietary side of a database, if they're, and we still lack good mechanisms for integrating
and acquiring abstract knowledge with machine learning. Those would be my answers for why we're
not there yet. So what do we need? Here's kind of a summary of that article that I mentioned.
I'll do it quickly so we can get to questions. I think, and it's also,
we need rich cognitive models that can keep track of the dynamically changing world,
plastic robot like Elon Musk is threatening to build. You need to know like, what's in this house?
What's their favorite food? What do we expect today? You need to constantly update that. You
need extensive real world knowledge. What languages do people speak here? What happens
when you break a cup? You need to know about relationships. So you need to know that if you
see a video of somebody drinking grape juice, that they're going to be less thirsty afterwards.
Positionality. So agents have to understand holes in terms of their parts. So
it's simpler. You need to know the difference between a red cube on a blue cube and a blue
cube on a red cube. Common sense knowledge, I kind of just mentioned, but focusing on time and space
and causality, things like physical objects, mental states, interpersonal interactions. We need
reasoning. So you need to know that in general, mixtures of two, if you have a mixture of two
things that are non-toxic, things not going to cause you to die. And we also need human values.
And I haven't talked about this too much, but I think it's a really important area for research.
We need to figure out how to program human values in time. That has to be part of the foundation.
We simply shouldn't be shipping things that could recommend suicide to.
Towards the people who are on my side of this debate and who like neurosymbolic AI,
it's also important to remember that even when we find the best way of integrating, or I think
there'll be many best ways, of integrating neural network style approaches with symbolic
approaches, we're still going to need that larger-scale knowledge. We're still going to need
the richer cognitive models. And we're also going to need major advances in engineering
methodology. We're going to need techniques for building robust cognition at the scale.
We're going to need to know about how we put our new discoveries into real-world practice
and recognition that intelligence is multifaceted. We shouldn't really be looking for one-size-fits-all
solutions at all. So I would say neurosymbolic integration is necessary, but it's not sufficient.
It's critical, but it's not a magical look. So there's a great article or a sad article that
depending on one of you, a striking article in technology about a year ago saying there were
hundreds of efforts to use AI in the pandemic and very bore any fruit at all. COVID is a wake-up
call. Recommendation engines, advertisement, really try to work towards AI that can make a
difference in this world. If we could build a deeper AI, there wasn't quote, deep learning,
which is just a bunch of layers, but it was actually deep understanding. We might be able to
read, digest, and synthesize fast-growing medical literature. We might be able to
more quickly figure out how to make better vaccinations. We might be able to
build new technologies for addressing climate change. We might be able to power road
with the risks that human healthcare workers have been taking and so forth. But to get to a
deeper AI that can operate in novel environments, we need to work towards building systems with
deep under. I think the best way to get started on that journey is to focus on hybrid knowledge-driven
reasoning-based, rich cognitive models. I'll just say the news in AI.substack.com,
if you want my views about the latest overhyped AI. Thank you very much.
Thank you, Gary. Great talk. I think there are many questions already, but while the
Catalina is ready for asking her question, I think you had a question at the beginning that
I think many people want to know. Do you want AGI? If the answer is guess, why?
So I'm actually putting together a podcast now. It'll come out in the spring. It's going to be
pretty slick. It's going to be very carefully structured. One of the episodes, I think,
is going to be about AI and whether it has had a net positive consequence so far and whether it
could have a net positive consequence. And I think on the so far question, it's really a marginal
question. So I think that what AI did with newsfeed has polarized society consequence
on things like vaccine uptake. And so good things. I think that Google search is generally good for
the world to make information more accessible. And that's powered by AI. But what it has done
around the misinformation and polarization climate is hugely costly, and there are other costs as well.
In terms of future, I think there's a chance that AI could really help in things like agriculture,
medicine, and so forth, and that it could more than carry its weight eventually.
However, I don't think we're that close to doing those things in hindsight worthwhile. So I'm not
someone in driving, there could be payoff, but not yet. I think at AGI because of the potential
payoffs, but I don't think we're that close to them. And so I think we're actually in some ways
at the worst moment in AI history. We used to just talk about it. There was no damage, no harm.
Using it, but it's not very reliable. And so there's a lot of risks now around misinformation,
prematurely scaling up driving cars and so forth. So we're at a delicate moment
in AI history. I think it is possible to get to a better moment. There's also,
what if they try to eat us all? I'm not too worried about that now, but we could.
So, you know, the jury is still out.
So Katarina, do you want to ask your question or I read it?
Or I could just call myself if you like.
Yeah, so can you please elaborate, give examples of classical AI being good at
construction for learning?
I mean, there's just no systematic taking the second part of that.
There's no systematic equivalent to deep learning, where you can take an arbitrary problem, feed in
your explicit knowledge and extract what you want. So we would like a function, essentially,
that takes the news and Wikipedia and spits out something like the site database, but maybe
modernize for the current era. Just don't know how to do that. So there are techniques like
inductive logic program with the problems. You give them some facts, they might extract
like some mathematical generalization from a limited number of facts, but they
do well in the real world. Part of the rate limiting step there is natural language understanding.
This is pretty well if you can actually encounter things, but we don't have a systematic law like
do that. And so they end up being toy problems where you
predefine the logical things and they're less impressive.
Symbolic knowledge is useful all the time. Like every time we
do turn by turn directions in navigation, we're using a symbolic AI system
that's doing a pretty good job of taking a massive database and telling you
a good efficient way to get from here to there. So we use these things all the time,
they don't get as much press. Google search is mostly still, I think, symbolic AI. It has
some deep learning in there, but a large fraction of it is written on hand coded rules. Symbolic AI
doesn't even need to be hand coded, but a large fraction of Google AI is that, which is the oldest
kind of AI in some sense. So symbolic AI is actually much more present than people really,
but no, it has not really solved the learning problem. And so most of symbolic AI is still
hand crafted and that tends to limit both how quickly it can be deployed and also how flexible
it can be. And so the other thing people like to do to misrepresent me, this is a symbolic or
go-fi person, good old fashioned AI. And I'm not at all. I'm under no illusions. I think that stuff
has its place. And I think it can help us to build the next generation, but it certainly doesn't
suffice on its own. So both topping, why deep learning is assumed to be part of a successful
AGI solution? I'm actually being a little bit slippery there. I think that's something that
serves the function of deep learning has to be there, but I don't know if it has to be deep
learning itself. So by that, I mean, we need something that can work with large databases
and deep learning is the kind of the thing we know now and detect statistical patterns and so
forth. It is, I think, likely that it will be replaced by something that's actually more tractable
and more efficient in terms of how much data there is. And so really the right hybrids will be
kind of statistical symbolic hybrids. It may not be deep learning, per se. Deep learning is the
best tool that we have right now. And so if you want to build something right now that is a hybrid,
you probably want to use deep learning. But it could be in 15, 20, 30 years that people are like,
wow, I can't believe how much data they needed in 2020 to solve a basic problem like driving. If
they had just had this technique, they could have done it with a hundredth of the data they
were trying to collect or a 10,000th of the data. And deep learning, it was cute, but we're impressed
that they did as much with it. But yeah, there was this better way. There was this much more
simple and elegant Bayesian formalism that does the same work. Like I wouldn't be surprised at all
if that happens. So Catherine Hulick asked, which groups are integrating symbolic AI?
So which groups are working on this, if you know any? There's a lot. There's a big group at Intel.
It's a big group at IBM that works with Josh Tenenbaum's group, among others.
Swart Chathari is doing great stuff on neurosymbolic programming at UT Austin.
Maior Knight, if I'm saying his name correctly, has a really interesting team
at Penn working on a nerd called Scallop. Anaman and Anankumar is doing some stuff at Caltech.
There's lots and lots of people. They don't get the same press, partly because their results
tend to be like, we do a little bit better than deep learning. We think this is promising.
Rather, we have this toy that you can play with on the internet and it's so much fun.
They're just not getting the same press. They're mostly not labs that have the scale to do the
industrial strength PR that Google and Facebook and OpenAI are able to do. The word is not getting
out in the same way, but there's lots of work. If you're in Boston, start with a lot of the stuff
that Josh Tenenbaum's group is doing at MIT, a lot of which has been increasingly explicitly
known as symbolic. Belina, I'd rather ask, following Speckles claims about innateness,
do you think it's possible that AI lacks mechanism for acquiring abstract knowledge
because this knowledge relies on being equipped with some innate learning mechanisms?
The way I think about it is like we need a bootstrapping nucleus. I think a lot about,
I sort of think a colonized version of what Kant argued in his philosophical book,
The Critique of Pure Reason. Basically, you need to start with a manifold of time and space in
order to get off the ground with the rest of it. I think that that's right. You need some innate
knowledge of time, space, and objects so that you can organize your experience from there.
We don't really have the machinery to do this yet, but I presume when I look at babies of all
species, humans and others, that they already know there's a world out there and there are
entities there. Different species know different things. Humans are weird because we're kind of
born prematurely because the big brain doesn't fit through the vaginal canal. There's a lot of
stuff that happens in the first three months of life that are actually what I would call neural
maturation rather than actual learning. By the time the brain is done, it's basically wiring up
human kids and say a baby Ibex can do all this stuff. A baby Ibex can climb down a mountain.
Putting this BBC video of it. That baby Ibex is estimating where rocks are and what it can do.
It's kind of very good innate understanding. I can do this when it's a few hours old,
an innate understanding of the physical world that is part of how it then organizes other
experiences. We'll tune things and get better. We do a lot of building new knowledge, but there's
some basis structure, I believe, on which we do that. The irony of my great debates,
ongoing debates with Yann LeCun, this contribution to AI is really an innate prior
that says that things with spatial appearance will look the same in different locations. This is
fundamentally, some people would call it like transformational invariance or spatial invariance
or something like that. That's fundamentally what convolution does. That's the thing that
LeCun is known for. It's an innate prior. He spends a lot of his time arguing against
the nateness, but the thing that he actually did that people actually use is an innate prior,
which makes it the world. We need more things like that.
Ken Church is next to the person to ask. Please can be brief because we have many questions.
Yeah. Oh, gee. I was just trying to say that I thought as a matter of debating,
it would be maybe more effective to dam the other side with faint praise. Like you were saying that
they're pretty good at some system one stuff. Did I not just say that convolution,
there wasn't even faint praise. I just dammed them with great praise. Convolution is one of the
great contributions to modern AI. Let me be clear about that. That is the other side,
but I'm damming them for not doing more of that. Well, just, yeah. I think if they just
stick to their knitting and say, well, we've accomplished these things and stopped the hype
on those things, that'd be fine. Yeah. Okay. Well, then maybe say a little more about the
good things and then we, then people won't get critical when you hit them with the hard part.
Okay. Well, I did just say that and I also said that AlphaFold was the greatest contribution to AI.
So yeah, okay. We've had this debate before, Ken. There's a bunch of questions and I think I have
actually done it. Okay. Okay. Misha, you're next. Hi, Gary. We met at Dave Higgers years ago.
Yeah. Thank you for doing this. I really applaud your direction and there's lots to be
learned from it. I was wondering whether it's the neuro symbolic or just incorporating principles
that would counter the fact that most machine learning is maximizing or optimizing with respect
to expected value. So you will always have these outliers to deal with. Yeah. Unless you use principles.
I think that that's a deep question and that you could probably run it in either direction. It
would be useful either way. So the way I think about it is you can't really encode a lot of
kinds of principles, not all unless you've got some symbolic form or at least we don't know any good
way to do it. Another way to put it is you're absolutely right and this is where we're foundering
is, I mean, let's say I want to put in a principle. So I've got a large language model. I wanted to
interact with people and I want to say don't recommend anything harmful. We have no idea
how to put that principle. I'm making a bet that symbolic knowledge gives us some shot.
My bet could be wrong. If we could find some other principles such that these systems are
constrained, that'd be great. There are limited how to do that. So there are some physics problems
now where people know how to constrain some principles, but we can get to the kind of broader
scope of human knowledge. There's lots of ordinary everyday stuff. We just don't know how to
do a system like this. That is the question of whether it's the right kind of system at all.
Your question is very fair. There's a lot of state of play there for people to talk about
principles. Maybe they've got some way to do it that's not symbolic. On the other hand,
they have to deal with the fact that there is a lot of symbolic stuff that does describe
principles. Humans are able to communicate these things in college classes or whatever
and you'd like to be able to do that. It's open and fair and interesting and probably
there's room for both perspectives there and they're compatible in my view.
It's a great question. Next, trying to be swift. Walter Cristmariano is next.
Hi. Does AI need a body? Because in my opinion cognitive AI needs a body with sensors, actuators
and organs. Thank you. I think it would help. I don't know if it's necessary. I think about
children with SMA who are confined to a wheelchair, never move around the world and still learn
blind kids who can still learn a ton about what sighted people mean by sight.
If you look at the developmental literature, kids really take off when they can start crawling
around typically. I think it's not necessary but would be really helpful.
A flip side of that is people have built robots with sensors and try to learn with them,
haven't gotten much mileage. I think that's because you need both to have a body that would be super
helpful and to have a lot of innate knowledge would be super helpful. It happens to be a
sociological fact that most of the people who have built robots with sensors and then do some
learning start with almost no prior knowledge and then their systems don't really learn that much
because they start with tooth and a basis. There are no great results from the developmental
robotics literature is what it's called. There's nothing really impressive there but I don't think
that's because developmental robotics couldn't succeed or because it might not be useful. I think
it's probably not, could be super useful if people would build in a little bit more innate
knowledge before they ran their experiments so to speak. Katarina, do you want to ask your second
question? Thank you in voice. Maybe a strange question but anyway if you read Russell's book
Human Compatible what's your opinion on that? Because not the last part of it with predictions
of the future but rather the first part which is more descriptional of what AI is and isn't
and about optimization and about computation that faster computers will not give us answer.
You can compute the wrong answer just faster. So these kind of arguments what do you think?
I like the first three quarters of a good exposition of what's going on now. I might recommend my
own book Rebooting AI that came out the same time for a slightly different take but they're
actually pretty compatible takes and I like his. I think his specific proposal about inverse
reinforcement learning is not the right solution to the AI problem or at least it's a small part of
it. Inverse reinforcement learning is basically like you watch a bunch of agents and figure out
the rules and like you could do that in chess. You can learn the rules of chess by watching
other people. It's kind of hard. It's not really the best way to do it. It's kind of clumsy.
We as humans at least when we in culture other do teach them a lot of explicit values
and I worry that if you do things with inverse reinforcement systems never going to generalize
those things broadly enough and I would really like to be able to do the explicit thing too.
In the end we probably need to do both. There are probably lots of things. It'd be nice if you could
do his inverse reinforcement learning on it and probably be really nice if I could teach you
things like don't harm other people and your robot would default to that unless good reason not to.
Sorry just then follow up with a small question then learning by mistakes because a lot of human
learning in school based on first development of okay we first do like this see what that's the
answer and now I tell you why the answer is wrong and I put it in the context. I mean there's lots
of stuff we do by reinforcement learning. There's lots of stuff we can't. I had a review of common
sense in the ACM with learning Davis and on the cover we didn't do this but there's a great picture
of a robot cutting down a tree limb from the wrong side. Reinforcement learning because people will
die. Well robots will die every time the robot cuts it down and if there's anybody below they'll get
killed. You can't do reinforcement learning on genocide like you have to know from first principles
that genocide is a bad thing. We don't you know want to encourage experimental studies of genocide
as a way to figure out its immoral. So there are some things where reinforcement learning through
experience makes sense and there's some where there are just there have to be much more efficient
solutions than that. We should move on can one question per customer unless we've exhausted them
and then follow up. Well I think yeah I think we're already over time. I think we could keep asking
questions for a long time so thank you Gali for a great talk and my pleasure and just to tell the
people the first seminar of next year will be by John LeCun so you will hear the other side
to the story. Well I'll send you some questions to ask him. So well you can attend also thank you.
That might be the only way we ever have a public debate. All right thank you everybody.
you
