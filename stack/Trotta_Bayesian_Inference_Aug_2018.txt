Bayesian inference: 
Principles and applications
Analytics, Computation and Inference in Cosmology 
Cargese, Sept 2018
Roberto Trotta - www.robertotrotta.com
@R_Trotta
To Bayes or Not To Bayes 
The Theory That 
Would Not Die 
Sharon Bertsch McGrayne 
How Bayes' Rule Cracked the 
Enigma Code, Hunted Down 
Russian Submarines, and 
Emerged Triumphant from Two 
Centuries of Controversy
Probability Theory: 
The Logic of Science  
E.T. Jaynes  
Information Theory, 
Inference and 
Learning Algorithms  
David MacKay   
Roberto Trotta 
Expanding Knowledge 
“Doctrine of chances” (Bayes, 1763)
“Method of averages” (Laplace, 1788)
Normal errors theory (Gauss, 1809)
Bayesian model comparison (Jaynes, 
1994)
Metropolis-Hasting (1953)
Hamiltonian MC (Duane et al, 1987)
Nested sampling 
(Skilling, 2004)
Category
# known
Stars 
455,167,598
Galaxies 
1,836,986
Asteroids
780,525
Quasars
544,103
Supernovae
17,533
Artiﬁcial satellites
5,524
Comets
3,511
Exoplanets
2564
Moons
169
Black holes
62
Solar system large 
bodies
13
“Bayesian” papers in 
astronomy (source: ads) 
2000s: The age of Bayesian 
astrostatistics 
SN discoveries 
Exoplanet discoveries
Roberto Trotta 
Bayes Theorem
• Bayes' Theorem follows from the basic laws of probability: For two propositions A, B 
(not necessarily random variables!)
P(A|B) P(B) = P(A,B) = P(B|A)P(A)
P(A|B) = P(B|A)P(A) / P(B)
• Bayes' Theorem is simply a rule to invert the order of conditioning of 
propositions. This has PROFOUND consequences! 
Roberto Trotta 
prior
posterior
likelihood
θ
Probability density
The equation of knowledge
P(θ|d, M) = P (d|θ,M)P (θ|M)
P (d|M)
Consider two propositions A, B. 
A = it will rain tomorrow, B = the sky is cloudy 
A = the Universe is ﬂat, B = observed CMB temperature map
P(A|B)P(B) = P(A,B) = P(B|A)P(A)
Bayes’ Theorem  
Replace A → θ (the parameters of model M) and B → d (the data):
posterior  = likelihood x prior 
evidence
information 
from the data
state of knowledge 
before
state of knowledge 
after
Roberto Trotta 
Why does Bayes matter?
 P(hypothesis|data)  
This is what our scientiﬁc 
questions are about 
(the posterior)
This is what classical 
statistics is stuck with 
(the likelihood)
≠ 
P(data|hypothesis)
Example: is a randomly selected person female?  (Hypothesis)
Data: the person is pregnant (d = pregnant)
P(pregnant | female ) = 0.03 
P(female | pregnant ) = 1 
“Bayesians address the question everyone is interested in by using 
assumptions no–one believes, while frequentists use impeccable 
logic to deal with an issue of no interest to anyone”  
Louis Lyons 
Roberto Trotta 
Bayesian methods on the rise
Roberto Trotta 
The real reasons to be Bayesian... 
• Eﬃciency: exploration of high-dimensional parameter spaces (e.g. with 
appropriate Markov Chain Monte Carlo) scales approximately linearly with 
dimensionality. 
• Consistency: uninteresting (but important) parameters (e.g., instrumental 
calibration, unknown backgrounds) can be integrated out from the posterior with 
almost no extra eﬀort and their uncertainty propagated to the parameters of 
interest. 
• Insight: having to deﬁne a prior forces the user to think about their assumptions! 
Whenever the posterior is strongly dependent on them, this means the data are 
not as constraining as one thought. “There is no inference without assumptions”. 
... because it works! 
Roberto Trotta 
The matter with priors 
• In parameter inference, prior dependence will in principle vanish for strongly 
constraining data.  
A sensitivity analysis is mandatory for all Bayesian methods! 
Priors 
Likelihood (1 datum) 
Posterior after 1 datum Posterior after 100 data 
points 
Prior 
Likelihood 
Posterior 
Data 
Roberto Trotta 
All the equations you’ll ever need!  
P(A|B) = P(B|A)P(A)
P(B)
P(A) =
X
B
P(A, B) =
X
B
P(A|B)P(B)
(Bayes Theorem)
“Expanding the 
discourse” or 
marginalisation rule
Writing the joint in 
terms of the 
conditional 
Roberto Trotta 
What does x=1.00±0.01 mean?
• Frequentist statistics (Fisher, Neymann, Pearson):  
E.g., estimation of the mean μ of a Gaussian distribution from a list of observed 
samples x1, x2, x3... 
The sample mean is the Maximum Likelihood estimator for μ: 
 
μML = Xav = (x1 + x2  + x3 + ... xN)/N 
• Key point: 
in P(Xav), Xav is a random variable, i.e. one that takes on diﬀerent values across an 
ensemble of inﬁnite (imaginary) identical experiments.  Xav is distributed according to 
Xav ~ N(μ, σ2/N) for a ﬁxed true μ 
The distribution applies to imaginary replications of data.
P(x) =
1
⇥
2⇥⇤exp
!
−1
2
(x−µ)2
⇤2
⇥
Notation :
x ∼N(µ, ⇥2)
Roberto Trotta 
What does x=1.00±0.01 mean?
• Frequentist statistics (Fisher, Neymann, Pearson):  
The ﬁnal result for the conﬁdence interval for the mean 
 
P(μML - σ/N1/2 < μ < μML + σ/N1/2) = 0.683 
• This means:  
If we were to repeat this measurements many times, and obtain a 1-sigma distribution 
for the mean, the true value μ would lie inside the so-obtained intervals 68.3% of the 
time 
• This is not the same as saying: “The probability of μ to lie within a given interval is 
68.3%”. This statement only follows from using Bayes theorem.
Roberto Trotta 
What does x=1.00±0.01 mean?
• Bayesian statistics (Laplace, Gauss, Bayes, Bernouilli, Jaynes):  
 
After applying Bayes therorem P(μ |Xav) describes the distribution of our degree of 
belief about the value of μ given the information at hand, i.e. the observed data.  
• Inference is conditional only on the observed values of the data.  
• There is no concept of repetition of the experiment. 
Roberto Trotta 
Inference in many dimensions
Marginal posterior:
P(θ1|D) =
!
L(θ1, θ2)p(θ1, θ2)dθ2
Proﬁle likelihood: 
L(θ1) = maxθ2L(θ1, θ2)
Usually our parameter space is multi-dimensional: how 
should we report inferences for one parameter at the 
time?
FREQUENTIST
BAYESIAN
Roberto Trotta 
The Gaussian case 
• Life is easy (and boring) in Gaussianland: 
Proﬁle likelihood
Marginal posterior
Roberto Trotta 
The good news 
• Marginalisation and proﬁling give exactly identical results for the linear Gaussian 
case.  
• This is not surprising, as we already saw that the answer for the Gaussian case is 
numerically identical for both approaches 
• And now the bad news: THIS IS NOT GENERICALLY TRUE! 
• A good example is the Neyman-Scott problem:  
• We want to measure the signal amplitude μi of N sources with an uncalibrated 
instrument, whose Gaussian noise level σ is constant but unknown.   
• Ideally, measure the amplitude of calibration sources or measure one source 
many times, and infer the value of σ 
Roberto Trotta 
Neyman-Scott problem
• In the Neyman-Scott problem, no calibration source is available and we can only 
get 2 measurements per source. So for N sources, we have N+1 parameters and 
2N data points.  
• The proﬁle likelihood estimate of σ converges to a biased value σ/sqrt(2) for N → 
∞ 
• The Bayesian answer has larger variance but is unbiased
Roberto Trotta 
Neyman-Scott problem
Tom Loredo, talk at Banff 2010 workshop:
true value
Bayesian marginal
Proﬁle likelihood
σ
μ
Joint posterior
Roberto Trotta 
Conﬁdence intervals: 
Frequentist approach
• Likelihood-based methods: determine the best ﬁt parameters by ﬁnding the 
minimum of -2Log(Likelihood) = chi-squared  
• Analytical for Gaussian likelihoods  
• Generally numerical  
• Steepest descent, MCMC, ...   
• Determine approximate conﬁdence intervals:  
Local Δ(chi-squared) method
θ
χ2
∆χ2 = 1
≈ 68% CL
Roberto Trotta 
Credible regions: 
Bayesian approach
• Use the prior to deﬁne a metric on parameter space.  
• Bayesian methods: the best-ﬁt has no special status. Focus on region of large 
posterior probability mass instead.  
• Markov Chain Monte Carlo (MCMC)  
• Nested sampling 
• Hamiltonian MC  
• Determine posterior credible regions:  
e.g. symmetric interval around the  
mean containing 68% of samples 
SuperBayeS
500
1000
1500
2000
2500
3000
3500
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
m1/2 (GeV)
Probability
68% CREDIBLE REGION
Roberto Trotta 
Marginalization vs Proﬁling
• Marginalisation of the posterior pdf (Bayesian) and proﬁling of the likelihood 
(frequentist) give exactly identical results for the linear Gaussian case.  
• But: THIS IS NOT GENERICALLY TRUE! 
• Sometimes, it might be useful and informative to look at both.
Roberto Trotta 
Marginalization vs proﬁling (maximising) 
Marginal posterior:
P(θ1|D) =
!
L(θ1, θ2)p(θ1, θ2)dθ2
Proﬁle likelihood: 
L(θ1) = maxθ2L(θ1, θ2)
θ2
θ1
Best-ﬁt  
(smallest chi-squared)
(2D plot depicts likelihood contours - prior assumed ﬂat over wide range)
⊗
Proﬁle  
likelihood
Best-ﬁt 
Posterior  
mean
Marginal posterior
}
Volume effect
Roberto Trotta 
Marginalization vs proﬁling (maximising) 
θ2
θ1
Best-ﬁt  
(smallest chi-squared)
(2D plot depicts likelihood contours - prior assumed ﬂat over wide range)
⊗
Proﬁle  
likelihood
Best-ﬁt 
Posterior  
mean
Marginal posterior
}
Volume effect
Physical analogy:  (thanks to Tom Loredo) 
P ∝
!
p(θ)L(θ)dθ
Q =
!
cV (x)T(x)dV
Heat: 
Posterior: 
Likelihood  = hottest hypothesis 
Posterior = hypothesis with most heat
Markov Chain Monte Carlo 
Roberto Trotta 
Exploration with “random scans”
• Points accepted/rejected in a in/out 
fashion (e.g., 2-sigma cuts) 
• No statistical measure attached to 
density of points: no probabilistic 
interpretation of results possible, 
although the temptation cannot be 
resisted... 
• Ineﬃcient in high dimensional 
parameters spaces (D>5)  
• HIDDEN PROBLEM: Random scan 
explore only a very limited portion of 
the parameter space! 
One example:  
Berger et al (0812.0980) 
pMSSM scans  
(20 dimensions)
Roberto Trotta 
Random scans explore only a small fraction of the 
parameter space
• “Random scans” of a high-
dimensional parameter space only 
probe a very limited sub-volume: 
this is the concentration of 
measure phenomenon. 
• Statistical fact: the norm of D 
draws from U[0,1] concentrates 
around (D/3)1/2 with constant 
variance 
1
1
Roberto Trotta 
Geometry in high-D spaces
• Geometrical fact: in D dimensions, most of the volume is near the boundary. The 
volume inside the spherical core of D-dimensional cube is negligible. 
Volume of cube
Volume of sphere
Ratio Sphere/Cube
1
1
Together, these two facts mean that random scan only explore a very small 
fraction of the available parameter space in high-dimesional models.
Roberto Trotta 
Key advantages of the Bayesian approach
• Eﬃciency: computational eﬀort scales ~ N rather than kN as in grid-scanning 
methods. Orders of magnitude improvement over grid-scanning. 
• Marginalisation: integration over hidden dimensions comes for free.   
• Inclusion of nuisance parameters: simply include them in the scan and 
marginalise over them.  
• Pdf’s for derived quantities: probabilities distributions can be derived for any 
function of the input variables
Roberto Trotta 
The general solution
• Once the RHS is deﬁned, how do we evaluate the LHS? 
• Analytical solutions exist only for the simplest cases (e.g. Gaussian linear model) 
• Cheap computing power means that numerical solutions are often just a few clicks 
away!  
• Workhorse of Bayesian inference: Markov Chain Monte Carlo (MCMC) methods. A 
procedure to generate a list of samples from the posterior. 
P(θ|d, I) ∝P(d|θ, I)P(θ|I)
Roberto Trotta 
MCMC estimation
• A Markov Chain is a list of samples θ1, θ2, θ3,... whose density reﬂects the 
(unnormalized) value of the posterior  
•  A MC is a sequence of random variables whose (n+1)-th elements only depends on 
the value of the n-th element  
• Crucial property: a Markov Chain converges to a stationary distribution, i.e. one that 
does not change with time. In our case, the posterior.  
• From the chain, expectation values wrt the posterior are obtained very simply: 
P(θ|d, I) ∝P(d|θ, I)P(θ|I)
⇥θ⇤=
⇥
dθP(θ|d)θ ≈1
N
"
i θi
⇥f(θ)⇤=
⇥
dθP(θ|d)f(θ) ≈1
N
"
i f(θi)
Roberto Trotta 
Reporting inferences
• Once P(θ|d, I) found, we can report inference by:  
• Summary statistics (best ﬁt point, average, mode) 
• Credible regions (e.g. shortest interval containing 68% of the posterior probability 
for θ). Warning: this has not the same meaning as a frequentist conﬁdence interval! 
(Although the 2 might be formally identical) 
• Plots of the marginalised distribution, integrating out nuisance parameters (i.e. 
parameters we are not interested in). This generalizes the propagation of errors: 
P(θ|d, I) =
!
d⇥P(θ, ⇥|d, I)
Roberto Trotta 
Gaussian case
Roberto Trotta 
MCMC estimation
• Marginalisation becomes trivial: create bins along the dimension of interest and 
simply count samples falling within each bins ignoring all other coordinates  
• Examples (from superbayes.org) : 
2D distribution of samples  
from joint posterior
SuperBayeS
500
1000
1500
2000
2500
3000
3500
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
m0 (GeV)
Probability
SuperBayeS
500
1000
1500
2000
2500
3000
3500
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
m1/2 (GeV)
Probability
1D marginalised  
posterior  
(along y)
1D marginalised  
posterior  
(along x)
SuperBayeS
m1/2 (GeV)
500
1000
1500
2000
500
1000
1500
2000
2500
3000
3500
Roberto Trotta 
Non-Gaussian example
Bayesian posterior 
(“ﬂat priors”)
Bayesian posterior 
(“log priors”)
Proﬁle likelihood
Constrained Minimal Supersymmetric Standard Model (4 parameters)  
Strege, RT et al (2013)
Roberto Trotta 
Fancier stuff 
SuperBayeS
500
1000
1500
2000
500
1000
1500
2000
2500
3000
3500
m1/2 (GeV)
 
 
tan β
10
20
30
40
50
Roberto Trotta 
The simplest MCMC algorithm
• Several (sophisticated) algorithms to build a MC are available: e.g. Metropolis-
Hastings, Hamiltonian sampling, Gibbs sampling, rejection sampling, mixture 
sampling, slice sampling and more...  
• Arguably the simplest algorithm is the Metropolis (1954) algorithm:  
• pick a starting location θ0 in parameter space, compute P0 = p(θ0|d) 
• pick a candidate new location θc according to a proposal density q(θ0, θ1) 
• evaluate Pc = p(θc|d) and accept θc with probability 
• if the candidate is accepted, add it to the chain and move there; otherwise stay 
at θ0 and count this point once more.
α = min
!
Pc
P0 , 1
⇥
Roberto Trotta 
Practicalities 
• Except for simple problems, achieving good MCMC convergence (i.e., sampling 
from the target) and mixing (i.e., all chains are seeing the whole of parameter space) 
can be tricky 
• There are several diagnostics criteria around but none is fail-safe. Successful 
MCMC remains a bit of a black art!  
• Things to watch out for: 
• Burn in time 
• Mixing  
• Samples auto-correlation 
Roberto Trotta 
MCMC diagnostics 
Burn in 
Mixing
Power spectrum
10
−3
10
−2
10
−1
10
0
10
−4
10
−2
10
0
k
m1/2 (GeV)
P(k)
(see astro-ph/0405462 for details)
Roberto Trotta 
MCMC samplers you might use 
• PyMC Python package: https://pymc-devs.github.io/pymc/  
Implements Metropolis-Hastings (adaptive) MCMC; Slice sampling; Gibbs sampling. 
Also has methods for plotting and analysing resulting chains.  
• emcee (“The MCMC Hammer”): http://dan.iel.fm/emcee  
Dan Foreman-Makey et al. Uses aﬃne invariant MCMC ensemble sampler.  
• Stan (includes among others Python interface, PyStan): http://mc-stan.org/  
Andrew Gelman et al. Uses Hamiltonian MC. 
• Practical example of straight line regression, installation tips and comparison 
between the 3 packages by Jake Vanderplas: http://jakevdp.github.io/blog/
2014/06/14/frequentism-and-bayesianism-4-bayesian-in-python/  
(check out his blog, Pythonic Preambulations) 
Roberto Trotta 
Bayesian hierarchical models
“True” values of 
observables
Population parameters 
Prior
Parameters of 
interest
Prior
INTRINSIC VARIABILITY
NOISE, SELECTION EFFECTS
Nuisance  
parameters
Latent variables
Data
Observed values
Calibration data
See Daniela Huppenkothen’s lecture on Wed! 
Roberto Trotta 
Why “Hierarchical”? 
• In cosmology, we have many problems of interest 
where the “objects” of study are used as tracers for 
underlying phenomena 
• Eg:  
• SNIa’s to measure d_L 
• Galaxies to measure velocity ﬁelds, BAOs, growth 
of structure, lensing, …  
• Galaxy properties to measure scaling 
relationships  
• Stars to measure Milky Way gravitational 
potential/dark matter  
• … 
• In many cases, we might or might not be interested 
in the objects themselves — insofar as they give us 
accurate (and unbiased) tracers for the physics we 
want to study 
Parameters
DATA
Roberto Trotta 
Why “Models”? 
• By “model” in this context I mean a probabilistic representation of how the 
measured data arise from the theory 
• We always need models: They incorporate our understanding of how the 
measurement process (and its subtleties, e.g. section eﬀects) “ﬁlters” our view of 
the underlying physical process  
• The more reﬁned the model, the more information we can extract from the data: 
measurement noise is unavoidable (at some level), but supplementing our inferential 
setup with a probabilistic model takes some “heavy lifting” away from the data  
• The key is to realise that there is a diﬀerence between “measurement noise” and 
intrinsic variability — and each needs to be modelled individually 
Params
Objects 
Data 
N 
O 
D 
In general:
N + O > D  
Roberto Trotta 
Mathematical formulation
p(params | data) ∝ p(data | params)p(params)
p(data|params) ∝ ∫p(data, true, pop | params) dtrue dpop
                          =  ∫p(data | true) p(true | pop) p(pop) dtrue drop
Measurement errors
Intrinsic variability
Population-level priors
The posterior distribution can be expanded in the usual Bayesian way:
Roberto Trotta 
Gaussian linear model 
• Intuition can be gained from the “simple” problem of linear regression in the 
presence of measurement errors on both the dependent and independent variable 
and intrinsic scatter in the relationship (e.g., Gull 1989, Gelman et al 2004, Kelly 
2007):  
yi = b + axi
xi ⇠p(x| ) = Nxi(x?, Rx)
POPULATION 
DISTRIBUTION
yi|xi ⇠Nyi(b + axi, σ2)
INTRINSIC VARIABILITY
ˆxi, ˆyi|xi, yi ⇠Nˆxi,ˆyi([xi, yi], ⌃2)
MEASUREMENT ERROR
Model: unknown 
parameters of 
interest (a,b) 
Roberto Trotta 
Malmquist bias revisited 
• Malmquist (1925) bias: intrinsically brighter objects are easier to detect, hence 
quantities derived from a magnitude (brightness) limited sample are biased high.
log(distance)
log(Luminosity)
Unobservable
1. Observed objects 
have mean luminosity 
biased high 
2. Noise more likely to 
up-scatter lower 
luminosity object into 
detection threshold 
than vice-versa (as 
less luminous objects 
are more frequent)
☆
☆
☆
☆
☆
☆
☆
☆
☆
☆
☆
☆
☆
☆
log(frequency)
latent x
latent y
INTRINSIC VARIABILITY
observed x
observed y
+ MEASUREMENT ERROR
observed x
Kelly (2007)
latent  
distrib’on
PDF
• Modeling the latent distribution of the 
independent variable accounts for “Malmquist 
bias” of the second kind  
• An observed x value far from the origin is more 
probable to arise from up-scattering of a lower 
latent x value (due to noise) than down-
scattering of a higher (less frequent) x value 
Flux 
limit
TRUE VALUES
“SMALL” ERRORS
“LARGE” ERRORS
The key parameter is noise (σx) to population (Rx) 
characteristic variability scale ratio 
σx/Rx <<1
Bayesian (black) marginal 
posterior identical to Chi-
Squared (blue)
true
σx/Rx ~1
Bayesian marginal posterior 
broader but less biased than 
Chi-Squared
March, RT et al (2011)
true
yi = b + axi
σx
σx
Rx
Rx
Roberto Trotta 
Slope reconstruction
Rx = σx2/Var(x): ratio of the covariate measurement variance to observed variance  
Kelly, Astr. J., 665, 1489-1506 (2007)
Ordinary Least 
Square
Maximum 
Likelihood
BIASSED 
LOW
BIASSED 
HIGH
Chi-Square 
incl variance
APPROX 
UNBIASSED
Roberto Trotta 
Why should you care? 
Rx = σx2/Var(x) = 1 in this example: Comparing the MLE (dashed) with  
the Bayesian Hierarchical Model Posterior (histogram)
Kelly, Astr. J., 665, 1489-1506 (2007)
Slope
True 
Bayesian 
MLE 
Standard MLE (or 
Least Squares/
Chi-Squared) ﬁts 
are biased! 
(even if you 
artiﬁcially inﬂate the 
errors to get Chi-
Squared/dof ~ 1)
Roberto Trotta 
ISBA 2012 meeting, Kyoto
Supernovae Type Ia Cosmology example
• Coverage of Bayesian 1D marginal posterior CR and of 1D Chi2 proﬁle likelihood CI 
computed from 100 realizations 
• Bias and mean squared error (MSE) deﬁned as 
 
   is the posterior mean (Bayesian) or the  
   maximum likelihood value (Chi2).
ˆ✓
Coverage
Red: Chi2
Blue: Bayesian
Results: 
Coverage: generally improved 
(but still some undercoverage 
observed) 
Bias: reduced by a factor ~ 2-3 
for most parameters 
MSE: reduced by a factor 1.5-3.0 
for all parameters
Roberto Trotta 
Adding object-by-object classiﬁcation
• “Events” come from two diﬀerent populations (with diﬀerent intrinsic scatter around 
the same linear model), but we ignore which is which: 
LATENT
OBSERVED
Roberto Trotta 
Reconstruction (N=400)
Parameters of interest
Classiﬁcation of objects
Population-level properties
