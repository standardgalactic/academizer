Thank you so much for inviting me for this talk.
And thank you so much for being so numerous today.
I have a lot of things to talk about.
But what's interesting in my mind
is that these things are linked together
in a really interesting way.
At least that's what I'll try to convince you about.
So the title talks about system two cognition, which
is about high-level cognition.
And that's connected to consciousness, as I'll explain.
It's also connected to issues that
are very important today in machine learning about generalization
out of distribution.
And it's also connected to the notion of agency
and reinforcement learning and how agents face problems
that standard static machine learning
hasn't been looking at enough already.
So deep learning has made amazing progress
in the last few years and decades.
And some people think that it might
be enough to take what we have and just
grow the size of the data sets, the model sizes,
computer speed, just get a bigger brain.
But I'm not quite of this opinion.
I think that we're missing out in qualitative ways
in order to approach human level AI.
We have currently machines that learn in a very narrow way.
They need much more data to learn a new task than our human
examples of intelligence.
They need humans to provide high-level concepts
through labels.
And they still make really stupid mistakes.
They're not very robust to changes in distribution.
They're adversarial examples and things like this.
And so some people, contrary to the first ones
that I mentioned, think that we have to start afresh
and invent something completely new to face these challenges
and maybe go back to classical AI
to deal with things like high-level cognition.
What I'll try to tell you about today
is that I think there is a path to go from where we are now,
expanding the abilities of deep learning
to approach these kinds of high-level cognition
questions of system 2.
So I talked about these two things, system 1, system 2.
Let me try to be a bit more precise.
I really got introduced to these concepts
by reading the amazing book of Daniel Kahneman, which I suggest
you to look at, Thinking Fast and Slow.
And he talks about these two kinds of cognitive tasks.
System 1 are the kinds of things that we do intuitively
unconsciously that we can't explain verbally
and that we can, in the case of behavior, that are habitual.
This is what current deep learning is good at.
So if you're driving a car and going back home
to a place you know, you don't need
to pay a lot of attention to the road.
You can talk to the person near you.
On the other hand, if you're in a new city
and you don't know the way, maybe somebody told you
directions, but you have to really pay attention
to every corner.
You have to read signs.
You have to be on the lookout.
And if somebody tries talking to you,
you will ask them to please let me drive.
So what's going on there is that you're
generalizing in a more powerful way.
And you're doing it in a conscious way
that potentially you could explain.
So if I ask you to add 34 and 56,
you can do it in your head, but it's
going to be sequential and slow compared to a calculator.
The kinds of things that we do with system 2
include programming.
So we come up with algorithms, recipes.
We can plan.
We can reason.
We can use logic.
Usually these things are very slow
if you compare to what for some of these problems
computers could do.
And these are the things that I'm
going to argue that we want future deep learning to do as well.
So here are some things that I think
we are missing in order to approach human level AI
with deep learning.
We need to work better on out of distribution generalization
and transfer.
So when you learn a new task, you
want to be able to do it with very little data.
We want to be able to handle these high level cognitive tasks
that I mentioned, including using these tasks to force
the learner to discover the kind of high level
representations, which deep learning was supposed
to be about in the first place.
Deep learning was about learning multiple levels
of representation, where the highest level supposedly
corresponding to the kind of concepts
we manipulate with language.
And many of these concepts have to do with causality.
The kinds of concepts we manipulate with words
tend to be cause or effect.
And once you start talking about causality,
you have to also talk about the agent perspective, something
that the mainstream of machine learning
hasn't paid enough attention to, not just because we
want to solve reinforcement learning problems,
but because we want to have machines that
understand the world, that build good world models,
that understand cause and effect,
and that can act in the world in order to acquire knowledge.
So a big theme of my presentation today
is that these different questions are not
independent research directions.
They're connected.
When you make progress in one, you
can make progress on the other.
And understanding how they are linked
can help us plan a path for our research.
So let me talk a little bit about consciousness
and the kind of functionalities that I think
we need to add to machine learning.
So this is the roadmap for this talk.
I'm going to start by talking about the need
for having machine learning systems that
can handle changes in distribution
and why it's particularly important for agents
because of non-stationaries in their environment.
Then I'm going to talk about the building blocks
that I think can help us get there.
In the last few years, maybe unnoticed,
we have added things to the deep learning toolbox,
in particular, attention mechanisms, which I think
are actually the key to move to the next stage that I'm
talking about.
And in this part of the talk, I'll also
tell you a little bit about how much work has been
done by our colleagues in cognitive neuroscience
to better understand the human side of the equation.
Then I'll talk about several priors that may be linked
to consciousness.
So the main theme here is that there
is an advantage for human beings to have these high-level
cognitive abilities.
And so these advantages, you can think of them
as assumptions about the world.
So usually we talk about priors, or you
can think of them as regularizations.
So the first one that I'll talk about
is the idea that the joint distribution
between the high-level concepts can be represented
by a sparse factor graph.
Any joint distribution can be represented by a factor graph,
but the sparse one is a different story.
And then I'll talk about another prior
that has to do with how the world changes.
And this is going to connect with the notion of agency.
So the fact that most of the time, when things change
in the world, it's because agents, like people, do something.
So we intervene.
We do things.
And the hypothesis that I'm proposing with other people
is to consider those changes as localized
in some abstract space.
And if we use these hypotheses I'm
going to show through some of the recent work we've done,
you can actually discover relationships
that are just causal relationships between variables.
Yeah, I'm going to also talk about metal learning, which
is connected to the problem of learning out of distribution.
And the final bit of my presentation
is going to be more about the detailed architectural
structures that we should explore
to introduce the kind of compositionality
that system-to-processing requires.
And in particular, I think we should
be moving towards neural nets that can not just
operate on vectors, but also operate
on sets of elements, sets of vectors, sets of objects
that are pointable, that can be referred,
and operated on by dynamically recombined modules.
So that's it.
That's the end of my talk.
This is the talk, right?
Now I'm going to go a little bit more detail
on each of these points.
Let's start with changes in distribution.
So the classical framework for machine learning
is based on the hypothesis of IID, independently
and identically distributed data.
That means the test data has the same distribution
as the training data.
And it's very important, because if we didn't have
that assumption, then we would not
be able to say anything about generalization.
Why would a function learned on some data
work on some other data, right?
Now, unfortunately, this assumption is too strong.
And reality is not like this.
Most of the data we get isn't IID.
And so in practice, what many people
do in industry or in academia is they take,
when they collect data, they shuffle it.
And here, to make it IID, and here I
want to quote my friend Leon Boutou, who in his ICML keynote
this year said something like, nature
does not shuffle data.
We shouldn't.
And the reason why we shouldn't is because when we do that,
we destroy important information about those changes
in distribution that are inherent in the data we collect.
And instead of destroying that information,
we should use it in order to learn how the world changes.
So this is important, because there
are, for example, rare events like Black Swan events that
are highly unlikely to happen, but could
have severe consequences.
And this question of out-of-distribution
generalization really breaks the IID hypothesis.
Now, the IID hypothesis was good, but if we discard it,
we need to replace it by something else.
And that's why I've been talking about priors.
And one of the important ones is going to be, well,
how is the test distribution going
to be related to our training distribution
if it's not the same?
OK, so let's talk about out-of-distribution generalization.
What I mean by out-of-distribution generalization
is essentially the phenomenon of a learner being
able to generalize in some way to a different distribution.
It doesn't say anything about how we do it.
But let's see why we need this.
Well, if you are a learning agent, so agent means actions,
so it's a learning system that's embedded in some environment,
you're almost always facing non-stationaries
for several reasons.
There are changes in distribution
due to the actions of the agents.
I move to a different place.
The actions of other agents, the fact
that I'm moving, that we're looking at different times,
different sensory signals, different sensors,
different actuators, different goals, different policies,
all kinds of things are changing.
And as you heard from Blaise thought this morning,
once we start looking at multi-agent systems,
it gets even more complicated.
You can't even talk about optimization
as he was explaining.
But certainly from the point of view of each agent,
you don't have a stationary distribution anymore.
And if you think about a child learning, for example,
their world is changing all the time,
their body is changing all the time.
And so we need systems that are going
to be able to handle those changes
and do things like continual learning,
lifelong learning, and so on.
This has been a long-standing goal for machine learning,
but I think we haven't yet built the solutions to this.
And one of the crucial elements, according to me,
in order to be successful in this,
and I'll come back at the end of this presentation
much more on this, is introducing more forms
of compositionality.
So what does that mean?
It means being able to learn from some finite set
of combinations about a much larger set of combinations.
We already get that from distributed representations.
Distributed representations are really at the heart
of why neural nets are working.
They were introduced by Jeff Hinton in the early 80s.
And in the last few years, we actually have theory
that helps us see why you get an exponential advantage,
potentially at least, if you make the right assumptions,
if we're making the right assumptions about the world
in terms of compositionality, in terms of being explained
by a number of different variables and factors,
then distributed representations
can be exponentially advantageous.
Because essentially, once you've trained a bunch of features,
you can generalize to new combinations of those features.
This is what a single hidden layer already gives you.
Now, if you have a deeper network,
you also get compositionality
because each layer gets composed with the next one.
And we've also shown that gives
another exponential advantage.
And now, I think there are other forms of compositionality.
And the one that we know best
is the one you find in language.
And linguists have been talking about this for a long time.
They call it systematicity or systematic generalization.
And I'll talk a lot more about that.
So, this opens the door to better powers of analogy,
of abstract reasoning,
and that remains to be done in machine learning, I think.
So, systematic generalization really is aiming
at out-of-discretion generalization and fast transfer,
but it's about how we do it.
It's about the idea that we can get that
by dynamically recombining existing concepts.
So, it could be language, but it could be in other settings.
Like in the picture here from Lake Adal 2015,
we invent a new type of vehicle
by combining properties of different vehicles.
But what's interesting about this
is that systematic normalization allows potentially
to generalize to combinations
that have zero probability under the training distribution.
It's not just that they're not present in the training data,
then they would have zero probability
under the training distribution.
So, as an example, if I tell you a science fiction scenario,
clearly you haven't lived something like this in your life,
but you can still imagine it.
If you drive, if you've been living all your life
in some continent or some city
and then you move to a completely different place
and you have to drive in this unknown city,
as the example I gave earlier,
you are also doing a form of systematic normalization.
Unfortunately, current neural net architectures
are not that good at doing that.
And it's been shown through several papers and experiments,
starting in particular with the work of Lake and Baroni.
More recently, the work we did at Mila,
led by Badano and collaborators, presented that air clear.
Currently, we have a paper
that's gonna be on archive probably tomorrow or the day after,
on Clever, which is a data set for visual question answering,
where there are combinations of the linguistic concepts
that are present in the questions
that just don't come up in the way
that the data is generated.
And the current methods,
when you ask them to answer these kinds of questions,
fail completely, whereas the human wouldn't,
we don't even realize that these combinations
are not present in the data.
Okay, and then one question people may come up with
is how what you're proposing is different
from the classical AI program of symbolic logic and so on.
So that's a good question.
Well, I think there are a number of reasons
why these classical AI programs had trouble.
And in the work that we need to do
in order to achieve system two performance,
we want to avoid some of these pitfalls.
We want to make sure that those systems
will be able to generalize efficiently in large scale.
The concepts that we want to learn
need to be grounded with system one
in low level perception and action.
We want to keep the power of generalization
of the distributed representations.
We want to make sure that the kind of search
that is involved in things like reasoning and planning
can be done efficiently.
Whereas the classical approaches require
a huge number of explorations of many trajectories
of how things can unfold
or how you could combine concepts rules and so on.
And finally, we need to make sure we build systems
that can properly handle uncertainty in the world.
And machine learning has been doing a pretty good job
at that up to now.
But we want to achieve these extra goals
that we're not doing very well at,
like systematic realization that I explained,
factorizing the knowledge or at least some of it
that the learner is acquiring in small exchangeable pieces
to get this combinatorial advantage
that I've been talking about.
And that includes being able to manipulate variables,
things that you do naturally in programming
and in logic formulations,
dealing with instances that are associated
with more general categories if you want
and having references and in direction.
Things that don't seem natural in the neural net world,
but as I'll try to convince you,
we now have actually built the tools for doing that
in deep learning using attention mechanisms.
Okay, so let's talk about attention and consciousness.
So what is attention?
Attention is about doing computation in a focused way.
We're gonna sequentially focus computation
on one or a few elements at a time.
And we realized in around 2014
that this was extremely powerful
and was the reason why we were able to get a breakthrough
in machine translation.
When you produce the next word in English,
when you're trying to translate from French, say,
you wanna really focus the computation
on just the right few words in the French sentence
that are relevant to do the translation.
So we introduced a particular form of attention
called content-based soft attention,
which is very convenient because you can backdrop through it
and so you can learn it.
In other words, we can learn where to attend.
And so the way it works is the computation being done
at the next level is gonna use as input
a selection from the previous level of computation.
And that selection is gonna be a soft selection.
So we're gonna take a convex weighted combination
of value vectors from the previous level.
And these convex weights are coming through a softmax
that is conditioned on each of the elements.
So for each of the elements,
we're gonna see how well they match the context
to see on which one the attention should be focused.
So in a way, attention is parallel.
We're considering all the possible elements in some set
and we're computing a score for each of them
in order to decide on which one or which ones
we're gonna put attention.
And there's been recent work in cognitive neuroscience
showing that attention should be thought of
as an internal action.
So the way that your brain is attending
is very similar to the way that your motor system
is deciding to move your arm.
And so we wanna learn these attention policies.
So attention has been very, very useful.
I mentioned machine translation,
but essentially today's NLP systems,
state-of-the-art systems all rely on attention,
look at all the work based on transformers
and their variations.
They're also at the heart of memory extended neural nets.
We had paper last year and more ongoing work
on how attention connected to memory
can also unlock the problem of vanishing gradients.
And as I'll mention, attention also allows to change
neural nets from machines that are processing vectors
to machines that are processing sets.
And in particular, sets of key value pairs.
Okay, so let's see this picture again.
We can think of attention as creating a dynamic connection
between two layers.
Whereas in a traditional setting,
the connections are fixed.
Here we kind of pick which of the inputs
is gonna be sent to the whatever module
we're considering that uses an attention mechanism.
Now this is great, but from the point of view
of the receiving module, there is a problem.
It gets this value, which is one of those
in the set of input elements,
but it doesn't know where it's coming from, right?
It's the value of what?
And so what we're doing with attention mechanisms
is in addition to the value, we have some concept of key.
In other words, a kind of identifier
for where the value is coming from.
Currently we're using those keys
to decide which element should get the attention.
But that key is also sent to the next level.
And so downstream computation can know
what the value it's getting is what it is,
what it's coming from, what kind of object it is,
what kind of type it is.
So you can think of this as creating a name
for these objects and creating a form of indirection.
And as I said, now we have these systems
of operating on sets.
Why is it a set?
Because the attention mechanism doesn't care of the order
in which we're putting these elements in the first layer.
It just picks one according to how it matches
the some kind of learned policy,
but the information about the relative position
of these elements in your brain
or in a new owner architecture, it doesn't matter anymore.
Okay, now let's connect to what are friends
in your science and cognitive science are doing.
In our community, the C word, consciousness,
is still kind of taboo,
but in their community it's not anymore.
And that's great.
They've made a lot of progress
in understanding several aspects of consciousness.
And there are a number of theories,
but many of them are related to what's called
the global workspace theory,
which originated from bars.
And there's a lot of very good, important improvements
to it from stand the hand and collaborators.
And what this theory says is that what's going on
with consciousness is there's a bottleneck of information.
The some elements of what is computed in your brain
gets selected and then broadcast to the rest of the brain
and influencing the rest of the brain.
So this is related to short-term memory
where the things that have been selected are available
and conditions heavily perception and action.
So it also gives rise naturally
to the kind of system two abilities
that I've been talking about.
All right, so how is machine learning going to do something?
I mean, what's the connection with machine learning?
Machine learning could be used to help brain sciences
better understand consciousness,
but what we are understanding of consciousness
could also help machine learning develop better abilities.
So first of all, the work that's been done in neuroscience
in general is based on fairly qualitative descriptions
of these functionalities
that we think are associated with consciousness.
And what machine learning can do is help us formalize
in a way that's more mechanistic,
what these exactly means.
And then that could feedback the research in neuroscience
in order to provide more specific tests
that could be done of these theories.
Of course, for me at least one of the motivations
is also to get rid of the sort of fuzziness and magic
that surrounds the word consciousness.
And I think machine learning is in a good position
to provide a justification
for these particular forms of computations
in the sense of why have they evolved?
What kind of computational and statistical advantage
are coming with these particular forms of computations?
And of course, once we understand these things,
we also want them in our learning agents.
So consciousness is closely related to language
because the way that we know that somebody is conscious
is by asking them to report what they're thinking about.
So this is the direct way that we know about consciousness.
And that means that there's a very strong link
between your thoughts,
the things that you're conscious of and language
that one can be translated to the other fairly easily,
although there's a loss of information
when you go from your thoughts to language.
It also means that there's a connection
between system one and system two here
because those high level concepts
that we communicate with language
are anchored in the system one sort of intuitive system
that connects your brain to the rest of the world
through perception and action.
And I think that's one of the big important direction
for natural language research.
We really want to explore things
like grounded language learning
where we don't just learn from texts,
we learn from environments which involve perception, action,
the perception action loop through the environment
and allow a learner not just to get sort of patterns
of sequence of words,
but also what they refer to
is its understanding potentially implicit
of how the world works.
And I refer you here to some work we've done recently
published at The Last Eye Clear
on grounded language learning,
something that I call baby AI.
Okay, so now we're at the part of my presentation
when I'm gonna talk about the kinds of priors,
the kind of structures, the kind of assumptions
and regularization that we could use
in order to encourage our learning systems
to do a good job out of distributionalization
and the kind of conscious processing abilities
that I've been talking about.
And the first one I wanna talk about
is the sparse factor graph assumption I mentioned.
I called that in a 2017 paper, The Consciousness Prior.
So in this paper I talk about the way
that cognitive neuroscience has been understanding
conscious processing, the fact that we form
these low dimensional conscious thoughts
that are obtained by selecting elements
from a larger unconscious state.
So now instead of having a single top level representation
like we usually have in our systems, we have two.
There's a very high dimensional unconscious state
which contains all of the things you could think about.
And then there's a tiny, tiny one
which only contains those that went
through the bottleneck recently.
That's the conscious state.
And then attention of course is used to select
from the first one, what goes in the second one.
And attention is also used in a top down way
to condition further processing
in the system one's computation.
So now that way of thinking about conscious processing
is about the sort of computation we're doing
but in machine learning, we're also used to think about
what do these computations mean?
And often they mean some kind of inference
with respect to some model of the world
which might be implicit.
And a good way to think about a model of the world
includes what kind of joint distribution
between the high level concepts are we talking about?
And so the consciousness prior proposes
that it's a sparse factor graph.
So a factor graph is just a particular representation
of joint distributions.
You can represent any joint distribution in this way.
It has these squares that correspond to factors
that are kind of relations between variables.
The nodes are the variables.
And sparsity here just has to do with how many neighbors
each node has.
So why do I think that it's a good hypothesis?
Well, think about the kind of statements
we can make with natural language.
Like when I say, if I drop the ball,
it will fall on the ground.
Now this is a very powerful statement
because it's true with high probability.
And yet it involves very few variables.
Just the words that are there.
And so the fact that it uses very few words
means that the relationships that are being described
can tightly capture some element of the joint probability
through very few variables at a time.
So you can think of like each of these black squares,
each factor corresponding to the relationship
between a small set of variables
of the kind you could produce with natural language.
Now this is different from the usual assumption
that you find in many papers these days
that study high level representations
that are supposed to be in,
where the variables are supposed to be independent
of each other.
This so-called disentangling work,
which I think is a misunderstanding
of the goals of deep learning,
which was to learn these high level representations.
And so instead of thinking of these variables
as completely independent at the top level,
we think of them as having this very, very structured
joint distribution.
And they have to be like this
because high level concepts like the ball and hen
that I just used are not independent.
They come in these very powerful
and strong relationships as I showed.
So then what I'm saying is instead of imposing
a very strong prior of complete marginal independence
at the high level,
we can have this slightly weaker prior,
but still very strong prior
that the joint is represented by a sparse factor of.
The reason I say still very strong is that this prior,
for example, would not work in the space of pixels.
You can't find easily a small group of five pixels
in images such that you can predict one of the pixels
with high accuracy given the four others.
But we can do that with natural language
when we express those statements at this high level.
So factor graphs are expressed by writing the joint
as a product of these partition functions
like the phi k here,
where each of these partition functions
involves only a few subsets of the variables.
And so what it means,
if we were to impose something like this,
is it puts pressure on the end quarter or say
that maps the low level data to the high level representations
to have the property that the factor graph is sparse.
Okay, so next I want to talk about the
meta-learning aspect and another hypothesis
that's important to deal with how the world changes.
So meta-learning is something really hot and cool these days,
but actually started several decades ago.
My brother and I have been working on this in the early 90s
and actually was Sammy's PhD subject.
And what it's about really is about having multiple timescales
of learning or multiple timescales
of iterative optimization like computation.
So typically you would have an inner loop like normal learning
and outer loop like evolution,
which optimizes whatever the inner loop is producing.
And so in this way we can talk about the evolution outer loop
with individual learning in the loop,
or we can talk about in the life of an individual,
lifetime learning as the outer loop
and individual adaptation to a new environment
as the fast timescale.
And the thing that's cool about meta-learning
is it allows us to explicitly optimize for generalization.
And in particular,
it can be used to explicitly optimize
for out of distribution generalization, right?
If the agent sees multiple environments,
we can train its slow timescale meta-parameters
so that it will generalize well to new environments.
Now, there's the question that I mentioned earlier,
what kind of hypothesis can we make
about these changes in distribution?
And because underlying physics means
that typically an action by an agent
is gonna be localized in space and time,
we can assume that the changes in distribution
correspond to or are caused by,
or the consequence of an intervention by an agent
that acts on just a few causes or a few mechanisms
that relate variables with each other.
So this extends a hypothesis that was proposed
by Shropkofen collaborators of independent mechanisms.
And what they mean here is informationally independent,
meaning that the relationships between variables,
the mechanisms, the conditional distributions
are independent in a sense that what you learn from one
doesn't tell you anything about another one.
And so if something changes in the world,
one of these mechanisms changes,
one of the conditional distributions
corresponding to say one of the nodes in this graph,
it's like a graphical model, for example,
if something like this changes due to an intervention,
you only need to adapt the part of the model
that corresponds to the change.
So for example, if I put on some sunglasses,
the data that I'm getting on my retina
is very, very different,
but it can be explained by a tiny change,
which is this one variable that changes its value
from zero to one.
So that's really interesting because if we have
such a hypothesis and we have a good representations
of the interactions between all those high-level variables,
then very few bits would be needed
to account for those changes.
And there's very few observations needed to adapt
or infer what has happened.
And thus we can get good out-of-distriction generalization.
So the idea is since out-of-distriction generalization
can be obtained if we have a right decomposition
of knowledge, we can use out-of-distriction performance
as a training signal for factorizing knowledge.
Okay, so we did some work in that direction.
There's a paper called the Metatransfer Objective
for Learning to Disentangle Causal Mechanisms,
where we applied this idea in a very simple settings
where you have just two variables, A and B,
or say four variables, A, B, X, Y,
and A is the cause of B, but the learner doesn't know,
and you might observe just X and Y,
which are simple like rotations of A and B.
And it turns out that if you have to write decomposition,
if you look at the right way of factorizing
the joint distribution between A and B,
the one in which we have P of A, the cause,
multiplied by P of B given A for the effect given the cause,
then when there's a change in distribution
due to an intervention on the cause,
the learner that has the right model will adapt much faster,
doesn't need as much data, so the X axis on this thing
is the amount of data that the learner needs
to recover from a change in P of A.
And it turns out that you can also use this
to learn about how to map the X, Y,
which is things like the pixels,
that don't have a causal structure,
to the A's and B's that are the high level variables
that have causal structure.
Because again, the same remark as I did earlier
with the consciousness prior,
which does not apply on the pixel level,
the assumption that these high level variables are causal
does not work on things like pixels.
And you can't really find a pixel
that's a cause of another pixel, right?
So it's not the right space for doing things.
We had a more recent paper called
Learning Neural Causal Models from Unknown Interventions
where we extend these ideas to larger graphs
in such a way that we can avoid the exponential explosion
of the number of graphs that need to be considered
by parametrizing in a factorized way
the distribution over graphs.
And one of the things we find is that
in order to really facilitate
the learning of the causal structure,
the learner should try to infer
what was the intervention
on which variable was the change performed.
And that's something we do all the time.
Like most of the time, which is my brain,
is trying to figure out what was the cause
of what I've observed or that explained the change
that I am seeing.
So we tested these ideas on various small graphs
and we were able to find that actually
this works quite well and better than the commonly used
causal induction methods.
But more importantly, the way that we're attacking
this problem is something that's very deep learning friendly
in the sense that we just define an overall objective,
some regularization terms, usual things,
and we do green descent on it.
Okay, the last bit of my presentation
is about operating on sets of objects
that may be pointable using dynamically recombined modules,
which I promised at the beginning.
And so we have another recent paper called RIMS
for recurrent independent mechanisms.
And it's about modularizing the computation
and operating on these sets of named and typed objects,
but in a deep learning way, as you'll see.
So we apply this idea to recurrent nets.
The state of the recurrent net is broken into pieces.
Let me see if this works.
All right, so each of these boxes represent
a small recurrent net that is updated based
on its previous state, but it can also be updated
based on the state of other subnetworks,
but we are constraining the way that these subnetworks
are talking to each other so that it's gonna be sparse
and it's gonna be done in a dynamic way.
So what it means is that using attention mechanisms,
the connectivity pattern between those modules
is changed and computed on the fly.
Also, we use the attention mechanisms
to select a subset of the modules
that are gonna be activated.
So this is the idea that there's a bottleneck
that dominates the computation.
And the other important element is that
the things that are communicated
between those subnetworks are not the usual
standard vectors that we use in recurrent nets.
They are these sets of key value pairs.
And so what it means is that what these networks
are exchanging is you can think of them as variables
along with something like their type.
And what we found is that this leads to better
out of distribution generalization
than standard methods that don't use these kind of structures.
We've also tested this in reinforcement learning setups
where we just replaced LSTMs that were used
in a PPO baseline for Atari games
and found that it helped on the majority of the Atari games.
Okay, so we're close to the end of my presentation.
Let me recap some of the hypotheses
for conscious processing by agents
and systematic generalization
that I'm proposing we pay more attention to.
So I mentioned that the conscious supplier,
this idea that there would be a sparse factor graph
relating the joint distribution
between the high-level semantic variables.
I suggested that these high-level variables
for the most part can be considered as causal variables
that they can be causal effect of each other.
And what it means really is that they are about agents,
they are about intentions,
and they are about objects that are controllable
and their attributes.
One thing I didn't talk about is that those relationships
between variables,
they don't have their own parameters
for each factor of the graph
or each potential function.
Instead, they should be shared modules
that can be reused across different tuples.
So the fact that we have these key value pairs
that means that a particular subnetwork
is gonna receive input that's gonna be different
depending on the context.
And so it's like if it's a little rule,
but of course it's a neural net
that is gonna be applied to different instances.
So the graphical model that I'm talking about
is more like a dynamic base net
where the same parameters can be reused
in many, many places.
This is also connected to Markov logic networks
that Domingo's proposed a long time ago.
Another really important hypothesis
that I spend time on is the idea
that the changes in distribution
are mostly localized
if you represent information in the right way.
This is a semantic space.
So that's another property of that semantic space.
And one thing I didn't spend much time on,
but that for example, Arjavski and Butsu
have talked about in their recent paper
is that the things that are preserved
across those changes in distribution
have to do with the stable properties of the world.
That could be for example, grounded by an encoder,
a system one aspect that capture the stable
and robust aspects of the world.
So to conclude, I think after decades of work
on consciousness by contemporary science,
it's time that machine learning looked at these questions
and explore the functionalities
and the justifications
for having these kinds of capabilities.
And I think that this would bring new priors
that could help systematic generalization
and out-of-distribution generalization.
It could benefit of course, neuroscience
because we would be able to provide more detailed account
of these functionalities that can then be tested
in real people or animals.
And it would allow to expand deep learning
from system one to system two.
So thank you.
Thank you very much, Joshua, for this great talk.
So please, if you have questions,
come on the left side or the right side,
close to the mic.
Hi.
Hey, so you're talking about consciousness
and this is something I think that's really interesting.
I think there's actually pretty broad consensus
among moral philosophers that having conscious experience
is an important part of what makes one a moral patient
in other words, deserving of moral consideration.
And then philosophers also like to talk about
the easy problem of consciousness
versus the hard problem of consciousness.
And I'm kind of just wondering what your thoughts are
on the moral implications of building machines
that may be conscious.
And if you think the way that you're talking
about consciousness has any relevance
to that kind of question.
And also if you think that there's a way that you see
for us to determine whether or not AI systems
that we're building are having subjective experiences
that make them relevant moral patients.
So today I've only talked about
the easy problem of consciousness.
There is the question of subjective experience
which I haven't talked about
and deserves much more work and attention.
But on the neuroscience side,
people have been thinking about this for much longer.
And there are some interesting theories
which I think connect to issues of self-knowledge
and predicting our own actions
which might explain the impression
of subjective experience.
So over here.
The other major theory of consciousness
is of course integrated information theory
which measures consciousness by this phi quantity
which is essentially a measure of the mutual information
of the parts of the system.
And the higher the mutual information,
the more consciousness you have.
Which seems like the polar opposite
of your sparse factor graph hypothesis.
How do you reconcile the two?
I don't.
So I think the IIT theory is
more on the mystical side of things
and attributes consciousness to any atom in the universe.
And I'm more interested in the kind of consciousness
that we can actually see in brains.
No, but they measure consciousness by this phi measure
in brains and in computers.
Yes, there's a quantity that is being measured
but I don't think it's related to the kind of
computational abilities that I've been talking about.
Hi, here.
Thank you for the very inspirational talk.
My question is regarding,
so the topics that you touched upon,
the prior part and also the factorization part.
So in recent cognition work,
it has also been shown that the human mind
kind of uses the spatial world where we navigate
as kind of a prior to sort our thoughts.
This has been recently summarized also in the book
by Bob Raczewski, if I don't know if you're familiar with that.
But my question is do you see like a role
for this kind of like spatial prior
or like a way to organize certain concepts?
How could we go about this?
Yeah, yeah, possibly.
I think one of the big lessons of the last two decades
of machine learning is we need all kinds of priors
in order to encourage good solutions
to the problems we're looking at.
And clearly the brain is exploiting the fact
that a very important aspect of the world
is the spatial structure.
Of course, some of our models do,
but I think on the conscious side and memory side,
these are also important, but I haven't looked at that.
Thanks.
Thank you.
Hi, over here.
Yeah.
I'm inspired and also heartened to see that you're exploring
how to integrate some aspects of what you put
the symbolic program into deep learning and so on.
I guess my question is how far you anticipate that going?
There's been a lot of work, for example,
on extending logic programming, which
is far more powerful than just knowledge graphs or graphs
or hypergraphs to have numerical uncertainty,
including scalably, computationally scalably.
So that might involve interleaving, fine-grain
these kinds of symbolic reasoning techniques
with neural network type learning and inference,
as it's usually conceived of in neural net or machine
learning terms.
So how much of that, how soon do you think there ought to be?
Well, so indeed there are a lot of people
who have been exploring how we could kind of paste
the symbolic logic tools on top of the neural net computation.
I don't think this will work.
What I'm talking about is interleaving it more tightly,
as opposed to just bolting it on before or after.
But so what I'm talking about is different.
And of course, I don't know what will work.
I don't have a crystal ball.
But what I'm talking about is more
implementing some of the functionalities of logic
and reasoning with the neural net machinery
in the hope of keeping both properties.
And in one place, I didn't have time
to talk about where this matters is in the search.
So in classical AI, a big obstacle
is we have to look through a potentially
exponentially large number of potential trajectories
or combinations of things.
And that's where system one, which
is the neural net thing, comes in.
So we learn what to attend to, what to think about.
But it happens out of our consciousness.
We don't know why we think about something.
But that's crucial to enable efficient reasoning and search
and planning and so on.
Well, thanks.
We can talk more about it later.
Thanks, Yasha, for great talk.
I'm curious to hear your thoughts on what
is a data distribution and how we know if we're in it
or out of it and how the data distribution, what
is the relationship between the data distribution
and the empirical distribution?
How, given a training set, can we exactly characterize
what is the underlying distribution of that data?
Well, obviously, we don't have access
to the data distribution, except if we're
doing machine learning experiments
and we sort of cheat and synthesize the data
from some handmade distribution.
And so we can do it mathematically.
We can do it in simulations where we know what it is
and it's very useful.
But I think we also have to think about maybe
how that distribution came about as a non-stationary process
with where things change in the world.
And sometimes what looks like a distribution
can be broken down into sub-distributions
that are related.
Think about data coming from different places
or different times, for example.
And we don't pay enough attention to that right now.
Hi, quick question.
So you made a call to move beyond just perception
to high-level reasoning and consciousness.
And the question is, how do we actually measure progress
towards that goal?
So I talked about out-of-distribution
realization, right?
And things like transfer learning,
few-shot learning, continual learning, and so on,
have benchmarks, which I think is a good starting point.
But in general, especially if we introduce the agent aspect,
I think we have to start thinking about building environments
in which we're going to test how well our learner can cope
with those changes in distribution
when those changes are not like a simple set of distributions
like we do right now in standard meta-learning benchmarks,
for example.
Hi, I'm here.
So you said these high-level semantic variables.
You said, at one point, you said they're in a factor graph.
And then you also said they're causal.
But usually the causal variables we put them into DAGs.
So can you say something on the relation?
Yeah, just put arrows on the factor graph.
And that works.
Well, I mean, so you have the joint distribution structure.
And the causality is just another thing on top,
which provides extra information.
And these are the arrows.
I mean, there could be a longer answer,
but that's the short answer.
Hi.
Over to the right.
Hi, so great talk.
Thank you.
My question is, what relationship
you see between sparse factor graphs
and relational and associative memory supported
by the hippocampus in humans, given
that humans with bilateral hippocampal removal
appear to be able to be conscious, but not flexibly
use relational associations?
That's interesting.
I didn't know about it.
So I don't know.
I guess maybe you need that attention going back and forth
between the two sides of the brain
in order to coordinate the different elements of,
say, our relation.
So one thing maybe that has been misleading in my picture
is I've drawn the conscious state as if it was something
that was physically separate from the unconscious state.
But in the brain, that's not how it is.
It's more like within the brain, the unconscious part,
that some subset of neurons become excited a lot more
and others become inhibited.
And of course, that goes through a global communication.
So if you break the global communication,
I could imagine it would hurt the whole thing.
Thank you.
OK, so I propose you ask your questions during the coffee
break.
And we are going to thank you again, Yosha.
Thank you.
Thank you.
Thank you.
Thank you.
Thank you.
Thank you.
Thank you.
Thank you.
Thank you.
Thank you.
Thank you.
Thank you.
Thank you.
Thank you.
Thank you.
Thank you.
Thank you.
Thank you.
Thank you.
Thank you.
Thank you.
Thank you.
Thank you.
Thank you.
Thank you.
Thank you.
Thank you.
