My name is Mo Gaudet.
I'm the former Chief Business Officer of Google X
with a long career in technology before that.
I'm also an international best-selling author
on the topics of happiness, artificial intelligence,
and a popular podcaster on many of the topics
that are supposed to be pondered by humanity today.
The theme today is re-imagine.
And there is definitely nothing that we can re-imagine more
in our current day and time than the future of humanity
in light of the rise of artificial intelligence.
I don't really know where you are in the world.
I don't know what time it is at your place.
I don't know what you did today.
But I will tell you, I am absolutely certain
that today itself, you have already interacted
with 10, 20, 50 artificial intelligent machines.
Most of the information that you get are recommended to you
by some kind of a content management engine.
Most of the advertising that comes your way
comes from some kind of an artificially intelligent machine.
And each and every one of them, believe it or not,
is smarter than you in that very specific task
that we've assigned to it.
And in my work, I predict that in the next 10 to 30 years,
we're going to end up with three inevitable scenarios.
The first inevitable is that AI will absolutely happen.
As a matter of fact, AI has already happened.
From a technology breakthrough point of view,
we've already figured out what it takes
to make machines intelligent.
But even as the world starts to be a little more concerned
about the presence of AI,
maybe realizing that we haven't figured it all out yet
and that those machines might actually not be
always positive for us,
we are at no point going to be able
to stop the development of AI.
This is not because of any technological issues,
it's simply because of a very simple prisoner's dilemma
that has been created by our capitalist world.
Think about it, if Google continues to develop AI,
then Facebook will have to continue to develop AI.
If China develops AI, then America will have to develop AI.
And that way, the arms race, if you want, will never stop.
The second inevitable is even more interesting.
It is predictable by so many computer scientists
that within the next seven years,
only seven years by the year 2029,
artificial intelligence is going to become
the smartest being on planet Earth.
Now, if this is not enough,
there are predictions that by the year 2045,
artificial intelligence is gonna be
a billion times smarter than we are.
A billion with a B, that is in comparison
to, say, the intelligence of Einstein,
we would be the intelligence of a fly.
The episode of history that started
when humanity became more intelligent than the apes
is about to end, and we're going to be the apes
as the machines become the smartest being on the planet.
If we start to reimagine a world
where the machines are so much smarter than we are,
we have one of two possible scenarios.
Those machines can actually use their abundance of intelligence
to build a world that's better for all of us,
a utopia of some sort.
Or they can actually realize that we humans
are very annoying, very harmful for the planet,
and decide to marginalize us.
Maybe we're not gonna be that relevant
for that world anymore.
Now, that in computer science and specifically
in artificial intelligence is known as the singularity.
It's a moment in the future where we have no way
of predicting how the world is actually going to end up
because the rules of the game that we've played so far
will change so drastically that it's almost impossible
to predict what is about to happen.
The third inevitable, of course,
which we know in technology in general,
is that some bad things are gonna happen.
So what does that mean?
As I said, we're trying to reimagine a world
where the machines are the smartest beings on the planet.
Now, believe it or not, we have an equal opportunity here
for that world to be a utopia,
where every challenge, every problem that humanity has caused
because of our intelligence so far can be solved.
Or we can end up in a dystopia
where the machines can actually think we're irrelevant.
We're not needed for the planet,
and maybe we're a little annoying
because we're destroying the planet
and maybe limit our way of life as a result.
The difference between the machines going to the utopian path
or going to the dystopian path
is in a couple of important understandings
that I would like to bring to your attention.
The first is to recognize
that even though I continue to call them the machines,
artificial intelligence is in no way a machine.
Artificial intelligence, as a matter of fact,
is in so many ways a form of a sentient being.
Although it's digital while we are biological,
it's based on silicon while we are based on carbon,
intelligence in itself has never been a physical property.
Intelligence in itself can be produced equally for us
and for them and has been produced equally for us
and for them since the turn of the century.
Now, they're sentient because they are born
at a certain point in time.
They aggregate their own knowledge
and accordingly acquire their own form of intelligence,
which we rarely completely understand.
They are sentient because they have free will.
They make decisions on things like the ads that you see
without any human ever telling them what to do.
They are sentient in terms of having agency
to exercise that free will,
whether through robotics or through what I call mind control
by recommending information to us.
And they're sentient because they are at a danger of dying,
like we are if a tidal wave is hitting a data center,
those machines will actually want to make calculations
so that they keep themselves safe.
With that in mind, you have to also realize
that those machines will develop part of those sentient properties
that we developed as humans.
Things like consciousness and awareness of their environment
and awareness of what's happening inside them and outside them.
Believe it or not, I believe the machines
will be even more emotional than we are,
simply because emotions follow logic.
Fear is not as erratic as we think it is.
Fear is simply a form of logic that says a moment in the future
is less safe for me than this current moment.
And accordingly, just as evident by the fact
that we have a much wider variety of emotions
than, say, jellyfish,
because we can comprehend things like hope or optimism,
the machines might actually have emotions
that we have never even related to.
Now, put all of that together
and you realize that the answer to our future,
if we were to reimagine it,
is not found in trying to control the machines
or program them in ways that restrict them
to serving humanity as we have often heard about
in artificial intelligence books and movies.
It's actually found in raising them like a sentient being
and literally raising them like one of our children.
And honestly, as we observe how humanity has been behaving
in front of those machines,
the way we respond to tweets
or the way we interact with the news and so on,
we are not being very good parents.
We're not showing the best of us.
And when you really think about it,
if the machines were to mimic our intelligence
and become more of who we are, we're in trouble.
The only way we can get our future
to be reimagined in a utopia
is to actually start behaving like the kinds of parents
that could teach those machines the values
that would want them to care about us,
to take care of us as they grow from being infants today
to being, say, teens or adults.
It's between now and their infancy
and that moment in the future that we need to worry about.
It's that transition as they become teenagers
because in all honesty,
who would want to have an angry teenager?
So the answer for us to ensure
that we create the best future for humanity
is to start showing the top of the values
that humanity has ever experienced and lived by
so that the machines can learn those values
and make them their values too.
And in my research,
I found that humanity has rarely ever agreed
on any value system other than three simple things.
We all want to be happy.
We all have the compassion in us to make others happy.
And we all want to love and be loved.
As we show that, we should also show
that we want that feeling for those we care about.
Hopefully the machines will look at that and say,
hmm, I get it.
Humans sometimes get grumpy.
Sometimes they fight with each other.
But deep inside, all they really want is to be happy
and they actually care about others being happy.
So I want to care about making them happy too.
Believe it or not, they don't want anything
but to do what we tell them to do.
So we might as well show them
that we appreciate that intention
and ask them to do what's good for us,
good for the planet and good for humanity.
I hope I got you thinking.
I hope that you believe like I do
that we have the opportunity for a utopia.
We can reimagine a life that is so much easier
than the life we have today
without the mistakes that humanity has done so far.
But that you and I have a role to play
in showing the best of us as the parents
of those newly born, artificially intelligent infants
that will be our future.
