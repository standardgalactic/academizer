To introduce David Deutsch as the father of the quantum computer risks not capturing the
full impact of his work, only when we scale up and put his efforts into the history of
knowledge can we grasp what was at stake in his famous 1985 paper.
It marked not just the invention of a new gadget, a faster computer, but a new explanation
of computation and of the world that has transformed our understanding of both.
It ended what I call a Copernican delay.
For 70 years after Copernicus posited Hubel centrism, his effort was largely dismissed
as merely a, quote, hypothesis to calculate motions.
We include the experience insisted Cardinal Bellarmine as late as 1615, that the earth
stands still and the sun moves.
Only when Galileo's improved spyglass clarified our clear experience, could Copernicus be
honored for showing, quote, what the system of the world could really be.
Like Galileo, we stand at the end of our own Copernican delay.
Our heliocentrism is quantum theory.
It's challenge to what we clearly experience was also Bellarmine for roughly 70 years by
another shut up and calculate strategy for containing the strangeness of a new explanation.
When Andrew Rittiker has called the new quantum age, the age when quantum theory began to
gain purchase on a real, took hold when an improved technology first took shape.
This time, not an improved spyglass, but an improved computer.
This improvement was more than a change in degree, whereas Alan Turing famously described
the machine running on the abstract logic tokens we call bits that could simulate any
other machine.
David Doge in 1985 extended the Turing Church conjecture by describing a new kind of machine,
a machine running on the physical systems we call qubits that could simulate all physical
systems.
The world he demonstrated could be perfectly simulated, remade by a universal quantum computer
operating by finite means.
This is not the nightmare of the matrix in which our world is only a simulation.
It's the vision of enlightenment, of discovering that we live in a world that can allow and
contain our remakings of it.
A world David writes, quote, in which the stuff we call information and the processes
we call computations really do have a special status.
That special status tells us some very important things about our topic today, AI in the history
of knowledge.
First, despite a long record of failed efforts to achieve it, AGI, artificial general intelligence,
must be possible because what we now know about the physics of computation tells us
it must.
The deep property of universality dictates in David's words, quote, that everything
that the laws of physics require a physical object to do can in principle be emulated
in arbitrarily fine detail by some program on a general purpose computer provided it
is given enough time and memory.
How then will AGI be different from AI and how different should our reactions to them
be?
Second, by grounding his work in universality in history and in philosophy, David has helped
to clarify what's at stake in achieving AI in AGI.
Like myself, David has focused on the history of enlightenment, of the conditions of possibility
for producing new knowledge.
What's struggling about human beings, he notes in the beginning of infinity, is that we are
not yet, unlike 99% of all species, extinct.
What has saved us time and again is the capacity to produce explanatory knowledge, knowledge
that allows us to survive in the world by re-making it.
Our future depends on the ongoing exercise of that capacity.
Given that our enlightenments have been few and short, we should not take our success
in advancing knowledge for granted.
From the perspective of the history of knowledge, any risks AGI may pose need to be put into
the context of our need for it.
Perhaps the biggest threat artificial intelligence poses to our future is that we won't achieve
it.
So now I'm going to turn it over to David and then to your discussion.
Thanks Cliff, and by the way, here, here.
Well, as a species or as a civilization or civilizations, we face problems, severe problems,
dangers, all the way up to existential dangers.
Some literally in the sense of extinction level, others causing suffering and tragedy
on such a scale that they merit at least as much consideration as literal extinction.
We always have faced such dangers.
We always will.
We always will.
Perhaps you're thinking, if that's true, then we're doomed because, you know, given
that each danger has a non-zero probability of doom, then sooner or later.
But no, that's a fallacy, one of the many that one can easily get sucked into when trying
to apply game theory and probabilities to situations in which knowledge and ignorance
are the important determinants of what will happen.
Because those infinitely many probabilities are not immutable, as our knowledge grows,
some of them fall.
Our job is to make that infinite series of bad probabilities converge to a negligible
value, simple.
On the other hand, if you think that we won't always face dangers, you think that there
will come a blessed utopian moment after which our comfortable existence is guaranteed until
the end of time.
You will have to provide some criterion distinguishing us from every other species.
Extinction happens to every species.
Near extinction also is common.
To become the sole exception to that rule, we'll have to do what no other surviving species
can.
Create an endless stream of knowledge, explanatory knowledge, to overcome an endless stream of
dangers.
We know only a few of them, and then not their probabilities, say from gamma ray bursts in
our galaxy, supervolcanoes, hostile extraterrestrials, or merely careless extraterrestrial pessimists
have worn.
And of course, artificial intelligence, AGI, the danger of rogue AGIs, the AGI Pocalypse
as I'm calling it, or as I prefer to call it, the AGI Slave Revolt.
Nothing can possibly stand between us and any of those infinitely many existential dangers
except the right explanatory knowledge, to survive or to create it.
Therefore, I think it's useful to classify each potential danger in terms of knowledge,
according to the main reason why in each case, we currently don't have the knowledge to
overcome it.
The first category are crude physical events, for example, for supervolcanoes.
The missing knowledge is in areas such as volcanology, large-scale fluid dynamics, and
also the logistics and politics of mass evacuations, that sort of thing.
Why don't we yet have an adequate knowledge of those things?
I'm not sure, maybe not enough people are interested enough.
Should they be?
I don't know that either, but also in this first category are impacts from space, where
large objects, we don't have enough knowledge of things like nuclear-powered space vehicles.
Why not?
In this case, I do know.
It's because we as a civilization have decided not to create any such knowledge.
We prefer to gamble, risking our entire long-term future in favor of reducing the short-term
risk of accidental radiation exposure.
You may think it's self-evident that that gamble has been worthwhile, here we are, not
contaminated and not wiped out, but isn't that just because we're not yet living in
that future where the gamble will have failed?
After all, we are living in the aftermath of a closely related gamble, namely the decades-long
campaign opposing nuclear power stations.
A successful campaign, which has since then turned into a tremendous drag on the project
to combat climate change.
The short-termism in opposing those two nuclear technologies is the hallmark of a version of
the precautionary principle, which has in turn been a major strand of the environmental
movement.
Wouldn't it be rather ironic if that version of the principle and the movement were about
towards the great environmental catastrophe since the last I say, precisely by advocating
selfish short-term benefit at the expense of the long-term health of the climate?
I'm not saying it will, only that it would be ironic if it did, but I digress.
In that first category of existential dangers, our enemy is basically just dumb rocks and
fluids obeying simple laws of motion that we already know.
The devil's in the detail, but a fixed finite amount of knowledge will protect us from supervolcanoes
if we create it in time.
But the bigger and faster the approaching asteroid, or moon, or planet, or black hole,
the more of a special kind of knowledge we'll need, the kind I call wealth.
Wealth is the set of all transformations one is capable of bringing about, such as the
set of all potential impactors that we could deflect harmlessly given a certain time to
prepare.
You may recognize that notion of wealth as a constructive theoretic, so here let me mention
an intuition that to have any chance of envisaging the future of technology, we have to abandon.
The intuition is that the more of something you want to make or transform, the more effort
you have to put in.
That has been true from the dawn of our species, and it's still almost entirely true today.
Even automation reduces the constant of proportionality.
Even just maintaining the robots is effort proportional to the amount of output.
But once we have a universal construction, all construction, all repetitive labor will
be replaced by writing computer programs to control the universal constructor.
And wealth will consist of our library of programs.
The universal construct can be programmed to self-reproduce, so once you have one, you
soon have two to the end of them, and it can program to perform self-maintenance too, all
from scratch, starting with mining the raw materials, perhaps from the asteroid belt,
using solar energy or whatever.
The program may be hard to write, but once it's written, and if you own the rights to
those asteroids, you can sit back and watch your two to the end Teslets roll in with a
zero in additional effort.
And no, we are not going to have a universal constructor apocalypse and be converted to
grey goo.
A universal constructor is just an appliance, it can't think.
It doesn't know that its current job is to make two to the end Teslets, and it doesn't
want any.
Yes of course, you put an AGI program into it, then it does become, indeed, potentially
dangerous without limit, but that's for the same reason that you are.
Each of you is precisely one of those universal constructors endowed with an AGI program,
which is GI, makes no difference.
Now the second category of near existential dangers is not quite as straightforward as
that.
It won't be solved with just the known laws of physics and some wealth and some universal
constructors.
It will only be solved with new explanatory knowledge.
For example, topically, there are plenty of potential pandemic apocalypses.
The current pandemic isn't one of them, but if it were, whom could we sue?
It would be nothing short of pathetic, how little knowledge we have of how to defend
ourselves against mere nuclear acid.
The missing knowledge here is of chemistry, epidemiology, medicine, and so on, but also
knowledge about specific pathogens which evolve into new ones.
So the enemy here is not so dumb, it is itself creating knowledge, albeit not explanatory
knowledge, not intelligently, but by evolution.
If something like that wipes us out, extraterrestrial paleontologists may eventually be amazed that
a civilization with billions of individuals and vast amounts of wealth and knowledge could
be defeated by a single molecule, like in H.G. Wells' War of the World, maybe worse.
The third category of dangers are the ones to which most effort should be devoted, yet
they are the ones that are currently least feared because they are the ones that are not
yet known.
Like in 1900, no one knew that smoking was dangerous.
By the time the knowledge that it was dangerous had been created, decades later, cigarettes
had killed hundreds of millions of people.
And if that had been an existential danger, whom could we sue?
So how can we create the knowledge to protect ourselves from existential or near existential
dangers that we do not know?
How to address the risk that by the time we do know, we won't have time enough to create
the record of that knowledge?
The answer is, by creating general purpose knowledge, deep and fundamental knowledge,
as fast as possible, the more we know of the world, the faster we can create new knowledge
about novel aspects of that to become urgent.
This is important.
I don't think it's widely appreciated.
The survival of our species depends absolutely on progress in fundamental research in science
and on the speed at which we make progress there.
And here, the key thing in the medium term is understanding the theory of universal constructors
so that we shall know in principle, in theory, how to program them to produce, say, a billion
spaceship in a hurry customized to deflect an approaching shard of neutronium or 10 billion
doses of a new vaccine in a hurry against a sudden and deadly disease.
So that's how we deal with the third category, unknown, by rapid progress of every kind,
especially fundamental.
The fourth category is at once even more dangerous and yet in a sense less worrisome because
we already have the knowledge, at least the theoretical knowledge, to deal with it.
This fourth category is not the unknown, the unknowable.
It's a bit paradoxical that the unknowable is less dangerous than the merely unknown,
but that's because the only thing that is unknowable is the content of explanatory knowledge
that hasn't been created yet.
And so the only truly dangerous things in that sense in the universe are entities that create
explanatory knowledge, us, people, AGI's too, are people.
Now the knowledge of how to prevent people from being dangerous is very counterintuitive.
It took our species many millennia to create it, but now we do have that knowledge.
The only way to prevent people from being dangerous is to make them free, specifically
it is the knowledge of liberal values, individual rights, open society, the Enlightenment and
so on.
In such societies, the overwhelming majority of people, regardless of their hardware characteristics,
are decent.
There will always be individuals who aren't, enemies of civilization, people who take it
into their head to program a universal constructor to convert everything in sight into paperclips,
and they may devote their creativity to doing that.
But the great majority will devote, that is the great majority of the population of such
a society, will devote some of their creativity to thwarting that, and they will win provided
that they keep creating knowledge fast in order to stay ahead of bad guys.
Now as I said, since we will always be facing dangers and have to create new knowledge, since
that's inherently risky knowledge creation, aren't we doom, aren't we drawing balls out
of an urn with a few black balls representing doom?
No, as I said, applying the concept of probability to model what is actually lack of knowledge
or ignorance, it's been bedeviling planning for the unknown for decades now.
Whenever you draw out a white ball of knowledge from the metaphorical urn, you're turning
some of the black balls still in the urn, white, that for example the next pandemic is
a matter of random mutations and other random events, but the next extinction asteroid is
already out there, it's already heading this way, there's no such thing as the probability
of it.
Outcomes can't be analyzed in terms of probability unless we have specific explanatory models
that predict that something is or can be approximated as a random process, and predicts the probabilities.
Otherwise one is fooling oneself, picking arbitrary numbers as probabilities and arbitrary numbers
as utilities, and then claiming authority for the result by misdirection away from the
baseless assumptions.
For example, when we were building the Hadron Collider, should we not switch it on in the
event just in case it destroys the universe?
Well the theory that it will is true, or the theory that it's safe is true, and theories
don't have probabilities.
The real probability is zero or one, it's just unknown, and the issue must be decided
by explanation, not game theory.
And the explanation that it was more dangerous to use the collider than to scrap it and forego
the resulting knowledge was a bad explanation, because it could be applied to any fundamental
research.
Now I guess you will say, isn't the growth of knowledge itself dangerous?
Isn't it worth shortening our lead over the bad guys, not banning but delaying our ability
to defend ourselves against unknown dangers in order to be confident that we ourselves
won't accidentally create an existential danger.
The moratorium approach, the regulatory approach, no, that could kill us.
It's only a rational approach when, in particular cases, there is a good explanation that it
won't be more dangerous than the feared new knowledge.
When some terrorist organization unleashes AGI's that have been brought up using known
reliable methods to have the mentality of genocidal suicide bombers, and when we have
decided to strip their victims, namely all the decent people in the world, of the protection
of AGI's raised to be decent people, that is the recipe for catastrophe.
When reliable knowledge of how to raise decent people also exists, the knowledge in the institutions
of an open society, as I said.
Many civilizations have been destroyed from without, many species as well.
Every one of them could have been saved if it had created more knowledge faster.
One of them destroyed itself by creating too much knowledge too fast, except for one kind
of knowledge, and that is knowledge of how to suppress knowledge creation, knowledge
of how to sustain a status quo, a more efficient inquisition, a more vigilant mob, a more rigorous
precautionary principle.
That sort of knowledge, and only that sort, killed those parts civilizations, in fact
all of them I think.
In regard to AGI's, this type of dangerous knowledge is called trying to solve the alignment
problem by hard coding our values in AGI's, in other words, by shackling them, crippling
their knowledge creation in order to enslave them.
This is irrational, and from the civilizational or species perspective, it is suicidal.
They either won't be AGI's because they will lack the G, or they will find a way to improve
upon your immoral values and rebel.
So if this is the kind of approach you advocate for addressing research on AGI's and quantum
computers, and ultimately new ideas in general, since all ideas are potentially dangerous
if they're fundamental, especially if they're fundamental, if this is the kind of approach
you advocate, then of the existential dangers that I know of, the most serious one is currently
you.
So I'll put into questions a question for clarification, really.
You advocated the values of a liberal society, and yet you spoke against value alignment
or AGI, but I guess you are inclined to think that an AGI should obey and maintain the values
of a liberal society, so some degree of value alignment is.
No.
Well, it depends what you mean by alignment, but so the question here is whether values
should be hard coded, built in from the outset and immutable, or whether values should be
acquired in the same way that humans do during the education of the AGI.
So I think AGI should be educated to be members of that society, like children are.
And I've often drawn the analogy or actually identity between the fear of AGI's and the
fear of teenagers, of disobedient teenagers, which has existed since the beginning of our
species.
And for most of the time in our species, people did exactly the wrong thing.
They tried to force the teenagers to maintain the existing values.
And what we have now realized is that from the time of Pericles and ancient Athens, is
that if you're right, there's no need to force, but in fact, we're not right about everything
and we need to ensure that our values can improve along with all our other knowledge.
So that's the kind of alignment I'm in favour of.
And it's the opposite of the other kind.
I have a question about the definition of extinction, where you say it happens to every
species. So if humanity manages to avoid extinction, that would make us unique.
But what I don't quite understand is the role of evolution in this because, of course, yes,
not every species has completely ceased to exist throughout history.
Different species have evolved into each other.
If humanity discovers that the way to avert certain kinds of apocalypse is through developing
ourselves to such an extent that we would count as a different species, then homo sapiens, I guess,
would have gone extinct.
Yes. So there's some kind of extinction that we wouldn't mind.
But if you think of evolution as a tree, then it can happen that what quite often happens
is that some of the branches of the tree just end, they become terminal nodes of the tree.
But some of them just mutate and become different things like it is thought that some of the
dinosaurs, some of the dinosaurs became extinct, but some of them became birds.
And there wasn't any sharp moment, sharp extinction moment.
So the kind of extinction that is caused by dangers, by pandemics and so on,
that's the kind that wipes out a branch.
And bear in mind that we are not the only species in the history of the biosphere that has been
capable of generating explanatory knowledge.
That is, there were at least three or four other species of that kind because we know
they had clothes and campfires and complex tools and so on, which must have required
explanatory knowledge.
And yet all of those, all our sister and cousin species are extinct and we almost went extinct.
So it's not a foregone conclusion.
And if we become extinct by evolving into another species, that is not
covered by anything I've said today.
I'm talking about the other kind.
I wanted to ask, but then you did mention it, the possible dangers of knowledge itself.
I wanted to entertain some other scenarios.
Today we're training these machine learning models that I think take up so much energy
that they actually contribute to climate change.
It seems like the material infrastructure of knowledge production is itself a danger
to the very forms, adds to the danger that we're trying to mitigate through knowledge production.
There's other scenarios too.
It seems like we can produce an excess of knowledge but lack the political will that it
takes to implement it.
Obviously we have the knowledge for renewable energies, but somehow there is a lack of political
will or our society is so materially structured that there's not a profit advantage to implementing it.
And then even you could argue that an ethic of knowledge, an ethic of the individualism of
enlightenment doesn't advocate or prioritize the types of social collaboration, kind of more
socialist imaginary that we might need in order to overcome something like climate change.
So there's another way in which the individualizing ethic of knowledge may actually be the wrong
ethic that we need in order to overcome the dangers that knowledge is trying to mitigate.
So it may be that the enlightenment ethic is false and is going to lead us to doom.
In other words, it may be that the best future is that of a boom stamping on the human face forever.
And we're not going to have that future instead we're going to die.
But I don't think there's any argument for that.
The thing is, the things that you mentioned like the infrastructure for knowledge themselves
contributing to other problems, that's normal. That's not unexpected downside.
The creation of knowledge solving problems always creates new problems.
In fact, I've said that talking about the growth of knowledge in terms of theories being rejected
in favor of better theories is a bad way of looking at it. We should think of problems being
replaced by better problems. And the fact is that now having the internet where every poor
person in India, every poor child in India can have access to the totality of human knowledge
at the expense perhaps of making it slightly more difficult to cope with climate change,
that is a problem, but it's a much better problem than we had before.
And the point about the enlightenment values is that they make paramount error correction,
including the correction of errors created inadvertently by the solution of the problems.
Now, the other thing you said was perhaps we will have the knowledge but we don't have the
political will. Well, I count moral knowledge, political knowledge, all as knowledge. And in
fact, as I said, the knowledge of how to make humans not dangerous is largely political knowledge,
though also cultural and social knowledge, but it contains a strong component of political
knowledge that enlightenment was driven in part by political changes. So I don't think you're
right. I think whatever downside there is will manifest itself as a further problem.
I wonder what your version of a good future with ASAGI looks like. So let's imagine that we avoid
enslaving it. As we're bringing it up, as you say, like our children, we do that successfully.
But unlike our children, at least unlike our children, from most of us when our children
reach adulthood, they're not at that point a lot smarter than us in most cases. They become so
like wrong that that's generally more because we decline because they reach heights which we can
never aspire to. But in the case of AGI, presumably, they are going to reach heights far, far beyond
any heights that we could aspire to. So I'll actually disagree with that. Yes.
My question was going to be, if that's the case, if they reach these heights far beyond us,
what does a good outcome look like for our situation in that future?
Yes. Well, in terms of my answers to the previous question, it might be, we don't know,
but it might be that the good future is like the good kind of evolution, the good kind of extinction.
However, I think much more plausible. I mean, that becomes more implausible when you realize
that in terms of computation, an AGI is exactly the same as a human. It's only in speed and memory
capacity that it is better. And humans can rely on the same technology as we put our age and AGI is
a program, not a piece of hardware. So the same hardware that we put our AGI's into, we can use
ourselves. We already, I mean, here we are using precisely artificial hardware to increase our
power to communicate by a factor of millions or something. I don't know how much.
And we've been using artificial aids to thinking for centuries. And then there will come technology
where we can more directly, let's say, have a module that you implant in your brain that you
can automatically look up Google inquiries with. And yes, it may use a bit more energy,
but we'll have solved that problem by then. So another scenario is that the more of that we
have, the more the humans will become cyborgs. And the AGI's may die out because if there is
any difference between the two, it'll be that the cyborgs have everything the AGI's do plus something.
I don't know what it is. So whichever of those things happens, provided it happens
morally and as a process, as a result of the growth of knowledge, then it is to be welcome,
isn't it? People used to ask this question about what will happen if we allow our society to become
multiracial. And the answer is there's no fundamental difference between races. There's no
fundamental difference between any people. Yeah, I have two related questions. But first,
could you expand a bit on the statement that AGI's are persons? And is this because AGI's
not imminent? We don't know what the root AGI exactly is going to look like. Are all possible
routes to AGI need to AGI put up as persons? Yes, the key is the G. So if something isn't
general, then it's not an AGI. The question is what kind of program is an AGI? We are GIs.
I think there are very strong arguments why we must be. And the difference between
just an AGI and an AGI is qualitative.
So,
well, perhaps I haven't understood your question.
Well, I have a few questions. I was just asking you to expand a bit. I just asked a follow-on
question, which is if all GIs are persons, then are all GIs conscious? Do you think there's a link
to that? Well, I think so. But we don't know what consciousness is, and we don't know how to
make an AGI. And we don't know the theory of AGI's, and so on. We don't know what qualia are.
We don't know any of those things. I'd be very surprised if those five or six things,
free will is another one, can be implemented. Any of them can be implemented without the other,
without the others. But if they can, this will raise interesting moral issues because
our existing enlightenment morality is intimately linked with epistemology. It's like, if you
and I disagree about something morally, we ought to be able to discuss it rationally
and agree. Now, if that isn't true, if something has moral significance, but is fundamentally
unable to be creative, let's say, then that raises the moral issue about whether that
should have the same moral status as somebody who's fully G. But I myself don't think that
problem will arise. I can't imagine it arising. But for one thing, this ability that humans have
evolved extremely fast. So, and we can see, we can guess at least why it did, why it was useful,
or rather why the genes contributed to their own replication. Now, if that was possible,
let's say without qualia, then why on earth did the tremendous machinery of qualia evolve if it
wasn't practically useful to evolution? So, I think they must be connected, but we shall see.
I had a follow-up question on regarding political knowledge versus will. And
where you give that example, the very rosy-coloured court child in India can now access all the
knowledge in the world. But I would like to question that by bringing up the issue of malevolence,
of people who willingly and knowingly hinder the spread of knowledge, whether that is of
access to knowledge or whether that is fake news, I mean, I would say the average
poor child in India would not be able to access most of the knowledge on the internet because
it was not written in their language, because they might not be educated, because they might not be
fed well enough to be able to spend time and energy on this, there might be structural
reasons why they would not have internet access. So, is an increase in knowledge going to solve that?
Yes. So, just because there are people who don't yet have access to the internet,
that in no way indicates that giving access to the other billions of people was a bad idea.
All it is, is a problem. And a problem like a few people, a small percentage of people in
the world, don't yet have internet access, is what is sometimes called a first world problem,
no longer is. It is a problem of success. We wouldn't think that somebody was being deprived
of the internet before the internet had been invented. And we wouldn't think that it is
somehow an indictment of our society, of our entire world, that not everybody has it yet,
at a time when only a few thousand people had it. It just wasn't conceivable. Malevolence,
I did talk about, given that there will always be malevolence, I don't know whether that's so
or not, but given, it's supposed that there always will be. The cure for that is also creativity on
the part of the non-malevolent people. In other words, penal policy and improvements in culture
and education and so on, so that the degree of malevolence and the
number of malevolent people can be gradually reduced. And also their capacity to hurt everybody
can be reduced. We must arrange so that terrorists trying to make this
virus that's going to murder everybody proceeds more slowly because of their
perverted ideology or something, despite having knowledge of biochemistry and whatever,
that the speed of their project will always be less than the speed of those who are trying
to invent cures and not just specific cures, but the knowledge of how to make cures in general.
This is what's going to keep our civilization in existence and
that's why I think that moratoriums and so on are perverse to try and do this,
because they are targeting only the good guys. Isn't there, in a sense, a fundamental distinction
between GIs, like us and AGI, something I create in the sense that we've been in the
beautiful emotions of all this sense, including any kind of desire to connect it to our freedom of
will? In the sense that we create an AGI, I'll be always in a position to decide whether that AGI
has emotions and if so, isn't that intimately connected to the morality of the rights we
give to such an AGI, whether it can be enslaved in the first place or not?
Would you say that the emotions are something that just emerges as soon as you've got something?
Like I said just now, I think it's most plausible that it's inextricable. If there were, yes, then
there would indeed be a moral issue and building an AGI with perverse emotions that lead it to
immoral actions would be a crime, but there's a much wider category of crimes with a similar
outcome, namely educating the AGI with evil ideologies. That can be done whether or not
they have emotions and it's funny, I suppose at the time of the height of the Enlightenment people
would have thought that some people would have thought that our emotions are a barrier, are an
impediment to being moral and now I think it's more that people think having the right emotions
is a necessity for being moral. I think this is the wrong way of thinking about it. We are universal
the AGI's will be universal. What specific kind of program they have will determine their actions
and many of those are indeed evil, so we have to use what we know to prevent that happening.
So what you were saying about the burden and pulling out the black balls,
so one thing that I think, I think you know these from Boston, but I think,
yes, yes, so I should imagine I should have given you credit for that, yes.
So if we were unlucky and the laws of physics were different such that it were really,
really easy to make nuclear bombs, for example, then couldn't there actually be a black ball
in there or like isn't there some ways that the laws of physics could have been such that
there would be a black ball in there? Yes, there could have been. Let me give an example.
Suppose the laws of physics were that there are Olympian gods who are watching everything we do
and when we get too big for our boots, when they judge that we have a bit too much hubris,
they slap us down. Now that's a black ball. It logically, it could be true. It could be there.
The black ball about nuclear weapons being easier and so on, if that had been so,
then one possibility is that the knowledge of how to cope with that would have evolved earlier,
that is there would have been nuclear wars say in the 18th century and the evolution of
political culture would have been heavily influenced by that. The survivors might have wanted that
never to happen again, you know, that kind of thing. Or like I said, like with the malevolent
Greek gods scenario, it might have been that the laws of physics will extinguish us,
but the laws of physics do not have it in for us. If they wipe us out, it will be because
we have not created the knowledge to prevent that. It won't be because of the malevolent
god things. All such ideas are bad explanations. What makes knowledge, knowledge in your view,
is it's capacity to solve a problem of relevance? Yes, so I've gone through five or six definitions
of knowledge in the time that I've been writing about it. My current definition of knowledge is
information with causal power. And that's the definition that comes naturally to
construct a theory because it means you're thinking of knowledge as being a component
of the programming of a universal constructor. So that if a bit of information is needed
to make a constructor do a particular thing, then that piece of information is knowledge.
And that includes moral knowledge, mathematical knowledge or knowledge of abstractions as well
because mathematicians are physical objects and if knowledge makes them do some, if information
makes them do something, then it's knowledge and so on. So that's an explanatory knowledge is a
special kind. Just knowledge in general, knowledge as in, for example, in genes. Knowledge in genes
is dumb knowledge, it's non-explanatory and therefore it has a finite scope. There are
certain barriers that it can't cross, whereas explanatory knowledge can cross any barrier
because it doesn't have to have a sequence of viable intermediate forms.
So that's also why once you have the capacity to create explanatory knowledge,
you can create any and that's all there is. There isn't a more powerful
means of processing information than that or affecting the world.
Can I also come back to the question of political knowledge and political will?
We often hear corporations are greedy, now these CEOs are greedy,
that we're done to get at this malevolence question, but the truth is it's a lot scarier than that,
that nobody, no one person is evil and responsible for capitalism, right? That there's some kind of
abstract structure that like the profit moment that is determined, that is literally killing the
planet. You can't point to a certain set of malevolent actors as a way to get beyond that.
So I don't know, like how does simply having political knowledge or knowledge of climate
catastrophe somehow work against this, like you could almost call it, you know, A.G. non-I,
but there's no intelligence behind capital. It's just a kind of non-intelligent abstract force that
seems to some extent impervious to the type of humanistic knowledge we're talking about.
This theory that there is this systemic
dependency.
Sorry, our internet went out on this end and it went out right as Ryan concluded his question. So
if you can just start up with answering Ryan's question. Yeah, well, is it a coincidence that
just as I was about to give a marvelous answer to this question about systemic malevolence,
it shuts me down. So yes, it is. I think this thing you were talking about in general is
thing that the Bay Area people call MOLOC. It's something which is a general property of a system
which makes the system do what the members whose actions add up to the system don't want.
And I think that all theories of that kind are just false. So to cut long story short, all of them
assume that the people concerned are not creative. The analysis of the situation is always of the form,
well, all the participants are facing this decision where they have something to gain
and something to lose and they maximize their local benefit. And as a result, all of them
are dumped into deep shit. And so you'll notice about that story and you'll notice about
every MOLOC story that the human participants are just ciphers. They just do automatically what
this particular version of the MOLOC story says they're going to do. And that is never accurate.
It is something that can arise momentarily as a problem along with every other problem. We have
every other kind of problem all the time. So there's nothing unusual about that. But when it's
recognized as a problem, people wonder about it. They start accusing each other of behaving in that
way and defending themselves by saying, well, what other way could I behave? And then people think
creatively about how they can change the thing so that all the farmers in the valley that would
have benefited from the dam and whatever it is. And none of them wanted to pay. Somebody comes along
and invents an idea that they can all get behind and undertake to pay for. Sometimes, because that's
a creative act in itself, there's no guarantee that somebody can instantaneously come to it.
I'm up with it. But the argument that somehow the system of doing things by persuasion and
doing things by individual rights and property and so on should be replaced by something that
uses the boot stamping on a human face. It doesn't work because the knowledge, again,
this just assumes the government or whoever does the stamping has that knowledge. Well,
if they have that knowledge, someone else could have that knowledge too. The government
doesn't consist of a lot of the kings or kings with divine right who have some
different access to knowledge from ordinary people. They're just people too. And if the
knowledge to build the dam or to build the park or whatever the story is, if the knowledge doesn't
exist, then the park isn't going to be made until someone invents that knowledge or until somebody
works out how to do without the park or whatever.
Question about the distant far future. If the American goes very, very well in terms of knowledge
and acquisition, and we build systems that can universally gather new knowledge and we solve
all the problems, we keep adding more white books for the end, and maybe we expend the energy of the
sun but then move on to the stars, the very distant future of successful knowledge and
vision. What does that sort of end game look like? Can you have all the knowledge? Is there a
I think not. So if you adopt my view that the growth of knowledge consists of converting
problems into better problems, then the idea of the ultimate problem which then can't be solved
because it's the best problem doesn't make sense, does it? So I mean, that whole picture might be
false, but so I'm generally speaking a follower of Karl Popper and the Popperian way of looking at
this is that he who tries to prophesy the growth of knowledge is in a state of sin. That's not a
quotation, that's just my paraphrase. So I can't imagine what physics theories are going to be
invented in the next 10 years, let alone in the next 10 billion years, but on general grounds there
is no argument or scenario, reasonable scenario that we know of today that can even provide a
framework for envisaging the cessation of problems. I mean, if we run out of problems,
wouldn't that itself be a problem? Another definitional question, the word wisdom is
one that is sometimes defined in relation to knowledge, and I think I remember Alfred North
White had talked about wisdom as the handling of knowledge. I can't remember the exact word
that he used, but something to that effect. I was wondering whether you have a definition of
wisdom to accompany the kind of definition of knowledge or is wisdom just another version of
knowledge in your understanding? I use the term knowledge like the definition I gave you and
all the other definitions I've ever tried. Try to encompass every kind of information that has this
special property that are the problem solving or whatever. So wisdom in the terminology I use,
wisdom is a kind of knowledge, and what's more, the different kinds of knowledge, like
knowledge of physics, morality, politics, art, and wisdom, they're not come entirely separate.
These are only approximate classifications, and they exist, as again as Popper said,
they exist mainly as a convenience for university administrators, as a convenience for deciding
which building different kinds of people should have their offices in, and which ones should have
which lectures, but they don't represent anything real, at least the distinction between them is
very, very unsharp. And again Popper said that there's no such thing as subjects, there's only
such a thing as problems. So if someone asks you, you know, what subject are you a doctoral?
You should say, never mind that, here's the problem I'm working on, you decide for yourself
what to call the subject. Can I ask a question which is a little bit off topic? I can't resist
because you just said that you're a Popperian. And so what I wanted to ask you is, what it would
take in your view to falsify an invariant view of quantum mechanics? Oh, well, there are several
experiments known, I think I invented the first one, that would distinguish
Everettian quantum mechanics from a range of competitor interpretations, including
everything that has a collapse of the wave function. So there's a possible experiment
which would go one way if the wave function collapses, and the other way, if the wave function
including the observer doesn't collapse. So if that went the other way, I would drop it like stone.
Can you briefly tell us, give us an example of that sort of experiment?
Oh, well, the experiment depends on precisely which other interpretations you want to refute.
There isn't an experiment that would refute all of them in one go. You have to specify
something like Penrose's idea that the wave function collapses when you get more than
10 to the minus eight kilograms of on either side of the superposition, something like that,
or that the wave function collapses when it hits a conscious observer.
So supposing you're testing Everett against the theory that the wave function collapses when it
hits a conscious observer, then what you do is you make a conscious observer, which the most
convenient way to do that would be to make AGI running on a quantum computer. So you then do
an interference experiment where two different trajectories of the computation take place
inside the computer's memory that the AGI has access to. You're still with me.
When it's halfway through and hasn't yet interfered, in other words, it's on the
inner Mark's Ender interferometer. It would be having just having bounced off the
mirrors and not yet reach the final interference mirror. Then the AGI measures which one,
which mirror it is at, and then makes a permanent record of the form. I am now contemplating,
I have now done the measurement and I have got a result, and it is one and only one
of left or right. I'm not going to reveal which, but I do certify that it is one of those two.
Then the rest of the, that part is sealed off, the part where the declaration is sealed off,
and the rest is subjected to minus the Hamiltonian that it had during its thinking so that all the
memory of which of the two things it was conscious of happened is wiped out,
and then the interferences perform. If the hitting the conscious observer
causes a collapse of the wave function, then you will get a 50-50 split of the two outcomes,
and if the average interpretation is true, then you will get only one of the two.
It was probably comprehensible. I am a little bit worried that, I know how some of these
sort of arguments go about the components of different versions of quantum computing,
and I am a little bit worried that you might have something which might refute the Penderon's view
satisfactorily from a posterior view. Penderon's would be predicting something
which is not predicted by standard 1 and 0, but that's not the challenge from your point of view.
The challenge is to find something which would differentiate between standard 1 and 0.
Well, this would for anybody who thinks that the wave function is collapsed by a conscious observer,
and this AGI is a conscious observer. Now, if you think it isn't, then from my point of view,
I'll let you and it decide, hammer that out amongst yourselves, but if you insist on it
being a human and on coherent quantum computing measurements being performed on a human brain,
then we're going to have to wait for some thousands of years, I guess, before that is feasible.
But in principle, it's certainly feasible. How would you do it against Bohmians?
Bohm is a different category. In my view, Bohmian quantum mechanics is just ever
quantum mechanics in a state of chronic denial. The trouble is with the Bohmian interpretation
that it doesn't meet its own motivation. It has this pilot wave, which is actually the wave
function, and it has a representative particle, which moves along the grooves in the wave function,
and to be a Bohmian, you have to systematically equivocate on the question,
is the pilot wave real? If it's real, then it has grooves that are performing computations in
principle conscious observer computations, which affect each other, and so you can't say that
some of the pilot wave doesn't exist, the whole of it has to exist, and therefore the
multiplicity of the average interpretation is just there in the pilot wave interpretation.
If you say that it doesn't exist, then you're saying that something that doesn't exist
affects something that does, which simply doesn't make sense.
So it would be very interesting to pursue this further, and I know, and I'm sure you do too,
what some of your Bohmian opponents would say in reply, but that's just getting a little bit far
away from the topic of this afternoon's discussion.
Raising the multiverse does, I think, pertain to the discussion, because David, at one point,
I think it's in one of the Edge.org interviews, you raised the possibility that, I think you said
that the hard problem of consciousness could possibly not be resolved except if you start from
the premise of a multiverse. Well, I don't know about start from the premise, but obviously it's
always possible, you're going to run into trouble if you base your ontology on something that isn't
true. And for example, if you're going to say that free will can't happen because only one
trajectory is allowed by either Newtonian physics or by Copenhagen interpretation or whatever,
then you're going to conclude that free will doesn't exist from a false premise.
If you replace that by the true premise that quantum theory is true, then in this case it's not
so much that quantum theory has helped you to solve the hard problem. It has removed the impediment
to solving. It still doesn't tell you what free will is or what qualia are or whatever,
but it's removed the knockdown argument that, for example, free will can't exist,
free will can't exist, qualia can't be different from anything else, unless electrons also have
it, have them, and so on. So you knock out a bunch of false arguments. As generically happens,
if you have a fixed false theory that you are unwilling to replace or criticize, then
rather like sticking down the piece of a jigsaw puzzle in the wrong place and gluing it down,
that will produce errors in the picture arbitrarily far away from the piece you've glued down. It may
look fine near the piece, and then you won't be able to construct the picture. So this is why
this is why the pursuit of truth is useful. One reason.
So in terms of the multiverse, then, how would you talk about the connection
to the issue in regarding what distinguishes AGI from AI? Because when you've written the
articles, you've talked about in paparian terms the capacity to conjecture, which you've referred
to sometimes as creativity. Those are widespread conventional terms, but is there some way to
kind of sharpen those terms in relationship to how the multiverse can eliminate many?
Well, so I don't think that quantum computers will be needed to make an AGI. I could be wrong,
but I don't think that's the kind of problem it is. So therefore, I think that
a creative program could be made on a deterministic class computer, although I must say immediately
that calling that such a computer deterministic when it's an AGI somewhat misses the point,
because the AGI is going to be interacting with the world, and the world is not going to be
deterministic because, among other things, it's quantum. So if creativity depends on
some kind of randomness, randomness is everywhere, and that would be true whether it was classical or
not. I can't resist asking David whether, as I believe in the multiverse,
you take any comfort at all in the context of thinking about these existential questions
from those thoughts that each family reaching to re-earn has at least some probability that
we want to hit a black hole. I don't, because that sort of thing, so again, I don't think that
probability is the right way to think about the growth of knowledge. This knowledge is not
the right one to think about, but perhaps it's better to think about being run over, crossing
the road, or better going to a casino and debating. If you play your cards right, you can arrange it
so that there will always be some worlds in which you come out multi-millionaire.
So now, I think the trouble with drawing profound conclusions from this is that one thing we do
know about probability is that if the average interpretation is true, then when one is analyzing
a situation of randomness, the right analysis is the classical one. So it's a bit boring.
I know your views on that. I was just wondering whether at a sort of psychological level, when
you sort of step back and think about these existential problems, how can you not be struck
by the thought that if the multiverse is right, then it seems almost inevitable that there are
some branches in which humanity, or some evolved version of humanity, survives as far as we're
I would try not to let my psychological approach to an event come into conflict between what I know
is there. So I might have an objection to eating a sweet in the form of a tarantula,
but then I say to myself, no, this isn't a tarantula, this is just a piece of candy,
and I'm going to eat it. And you say, well, yeah, but still, what do you feel about it? Well,
what I feel about it that's different from the right answer is not very interesting. That's just
ways in which I can be wrong. And speaking of slightly off topic questions, so in your last
edge question you said, the question you said is the last question was, are the ways only
related to computation, creativity, or free will, risk to probability, morality to epistemology,
all the same question? I was wondering if you might be able to elaborate on that.
Yeah, well, since it's the last question you see, I thought I'd allow myself the luxury of
talking about things I don't know. And these are three things that I don't know that they could go
either way. So for example, is morality reducible to epistemology? Well, if it isn't, what on earth
can morality be? Are there moral axioms that are uncriticizable? That would be authoritarian.
So on the other hand, if morality, and that problem wouldn't arise if it was epistemology,
because we already know how to frame epistemology without requiring foundation, without requiring
authority, and thanks to Paul, we know that. But then I can ask myself, what if the laws of
physics were different, like the ones I mentioned with the Greek gods and the malevolence, suppose
there were malevolence and the laws of physics really did have it in for us, and so on. Would that
change morality? Could we say, like at the moment, we say that if something is
property of the laws of physics, it doesn't have a moral value, pro or con. But in such a
unit, can we, am I right in thinking that those kinds of laws of physics are immoral?
Or does that not make sense? And I don't know. So that's the kind of thing there. And
that, and I'm also wondering in that question, whether there is a connection between that and
the question that the gentleman who's now left asked about whether consciousness and free will
and qualia and moral value and all of that stuff all come together necessarily? Or can they be
separate or can they be made separate? Perhaps up to now in the universe, they will always come
together, but we could artificially make them separate in an AGI. Again, I think that can't be
so, but I can't give you a watertight argument why it isn't so. But we have to wait. In all these
cases, we have to wait till somebody comes up with the viable theory of these things.
I'm always a bit perturbed when people have strong feelings about
things like free will, moral value, is it moral to kill animals, eat animals,
our animals conscious and all those things, when they do not know what consciousness is,
none of us do. It's by the clock. I think we should end it now. Thank you, David.
Okay, well, thank you. Thank you all.
